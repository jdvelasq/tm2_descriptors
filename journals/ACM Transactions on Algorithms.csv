Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Sparse sums of positive semidefinite matrices,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954315883&doi=10.1145%2f2746241&partnerID=40&md5=f054630e03e1e3abbbaeec2ad853a549,"Many fast graph algorithms begin by preprocessing the graph to improve its sparsity. A common form of this is spectral sparsification, which involves removing and reweighting the edges of the graph while approximately preserving its spectral properties. This task has a more general linear algebraic formulation in terms of approximating sums of rank-one matrices. This article considers a more general task of approximating sums of symmetric, positive semidefinite matrices of arbitrary rank. We present two deterministic, polynomial time algorithms for solving this problem. The first algorithm applies the pessimistic estimators of Wigderson and Xiao, and the second involves an extension of the method of Batson, Spielman, and Srivastava. These algorithms have several applications, including sparsifiers of hypergraphs, sparse solutions to semidefinite programs, sparsifiers of unique games, and graph sparsifiers with various auxiliary constraints. Copyright © 2015 ACM.",Derandomization; Laplacian matrix; Positive semidefinite matrices; Randomized algorithms; Spectral sparsifiers,Algorithms; Application programs; Graph theory; Polynomial approximation; Derandomization; Laplacian matrices; Positive semidefinite matrices; Randomized Algorithms; Spectral sparsifiers; Matrix algebra
Improved approximation algorithms for relay placement,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954313620&doi=10.1145%2f2814938&partnerID=40&md5=ae093bfb91c1a5c968af7dd17c4dc5e4,"In the relay placement problem, the input is a set of sensors and a number r ≥ 1, the communication range of a relay. In the one-tier version of the problem, the objective is to place a minimum number of relays so that between every pair of sensors there is a path through sensors and/or relays such that the consecutive vertices of the path are within distance r if both vertices are relays and within distance 1 otherwise. The two-tier version adds the restrictions that the path must go through relays, and not through sensors. We present a 3.11-approximation algorithm for the one-tier version and a polynomial-time approximation scheme (PTAS) for the two-tier version. We also show that the one-tier version admits no PTAS, assuming P ≠ NP. Copyright © 2015 ACM.",Approximation algorithms; Polynomial-time approximation scheme (PTAS); Relays; Sensor networks; Steiner minimum spanning tree; Wireless networks,Algorithms; Approximation theory; Polynomial approximation; Sensor networks; Trees (mathematics); Wireless networks; Wireless sensor networks; Communication range; Polynomial time approximation schemes; Relay placements; Relays; Steiner minimum spanning trees; Two tiers; Approximation algorithms
Segmentation of trajectories on nonmonotone criteria,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954314158&doi=10.1145%2f2660772&partnerID=40&md5=07fdc71d616e4c6ccef3ce0aa8235480,"In the trajectory segmentation problem, we are given a polygonal trajectory with n vertices that we have to subdivide into a minimum number of disjoint segments (subtrajectories) that all satisfy a given criterion. The problem is known to be solvable efficiently for monotone criteria: criteria with the property that if they hold on a certain segment, they also hold on every subsegment of that segment. To the best of our knowledge, no theoretical results are known for nonmonotone criteria. We present a broader study of the segmentation problem, and suggest a general framework for solving it, based on the start-stop diagram: a 2-dimensional diagram that represents all valid and invalid segments of a given trajectory. This yields two subproblems: (1) computing the start-stop diagram, and (2) finding the optimal segmentation for a given diagram. We show that (2) is NP-hard in general. However, we identify properties of the start-stop diagram that make the problem tractable and give a polynomial-time algorithm for this case. We study two concrete nonmonotone criteria that arise in practical applications in more detail. Both are based on a given univariate attribute function f over the domain of the trajectory. We say a segment satisfies an outlier-tolerant criterion if the value of f lies within a certain range for at least a given percentage of the length of the segment. We say a segment satisfies a standard deviation criterion if the standard deviation of f over the length of the segment lies below a given threshold. We show that both criteria satisfy the properties that make the segmentation problem tractable. In particular, we compute an optimal segmentation of a trajectory based on the outlier-tolerant criterion in O(n2 log n + kn2) time and on the standard deviation criterion in O(kn2) time, where n is the number of vertices of the input trajectory and k is the number of segments in an optimal solution. Copyright © 2015 ACM.",Dynamic programming; Geometric algorithms; Segmentation; Trajectory,Algorithms; Dynamic programming; Image segmentation; Optimization; Polynomial approximation; Statistics; Attribute functions; Disjoint segments; Geometric algorithm; Optimal segmentation; Optimal solutions; Polynomial-time algorithms; Standard deviation; Trajectory segmentation; Trajectories
Linear kernels and single-exponential algorithms via protrusion decompositions,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954322548&doi=10.1145%2f2797140&partnerID=40&md5=79f0bb0862b15b9f45cda5607479a9d3,"We present a linear-time algorithm to compute a decomposition scheme for graphs Gthat have a set X ⊆ V(G), called a treewidth-modulator, such that the treewidth of G-X is bounded by a constant. Our decomposition, called a protrusion decomposition, is the cornerstone in obtaining the following two main results. Our first result is that any parameterized graph problem (with parameter k) that has a finite integer index and such that YES-instances have a treewidth-modulator of size O(k) admits a linear kernel on the class of Htopological-minor-free graphs, for any fixed graph H. This result partially extends previous meta-theorems on the existence of linear kernels on graphs of bounded genus and H-minor-free graphs. Let F be a fixed finite family of graphs containing at least one planar graph. Given an n-vertex graph G and a non-negative integer k, PLANAR-F-DELETION asks whether G has a set X ⊆ V(G) such that |X| ≤ k and G-X is H-minor-free for every H ∈ F. As our second application, we present the first single-exponential algorithm to solve PLANARF-DELETION. Namely, our algorithm runs in time 2O(k)·n2, which is asymptotically optimal with respect to k. So far, single-exponential algorithms were only known for special cases of the family F. Copyright © 2015 ACM.",Algorithmic meta-theorems; Graph minors; Hitting minors; Parameterized complexity; Sparse graphs,Algorithms; Clustering algorithms; Graphic methods; Parallel processing systems; Graph minors; Hitting minors; Meta-theorems; Parameterized complexity; Sparse graphs; Graph theory
Fast zeta transforms for lattices with few irreducibles,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954312180&doi=10.1145%2f2629429&partnerID=40&md5=c3e041110869a0ea270461d257cef9f2,"We investigate fast algorithms for changing between the standard basis and an orthogonal basis of idempotents for Möbius algebras of finite lattices. We show that every lattice with v elements, n of which are nonzero and join-irreducible (or, by a dual result, nonzero and meet-irreducible), has arithmetic circuits of size O(vn) for computing the zeta transform and its inverse, thus enabling fast multiplication in the Möbius algebra. Furthermore, the circuit construction in fact gives optimal (up to constants) monotone circuits for several lattices of combinatorial and algebraic relevance, such as the lattice of subsets of a finite set, the lattice of set partitions of a finite set, the lattice of vector subspaces of a finite vector space, and the lattice of positive divisors of a positive integer. Copyright © 2015 ACM.",Arithmetic circuit; Fast multiplication; Lattice; Möbius inversion; Möbius transform; Semigroup algebra; Zeta transform,Algebra; Algorithms; Integrating circuits; Logic circuits; Reconfigurable hardware; Set theory; Vector spaces; Arithmetic circuit; Circuit construction; Fast multiplication; Lattice; Monotone circuit; Orthogonal basis; Positive integers; Semi-group; Lattice constants
A new approach to incremental cycle detection and related problems,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954199041&doi=10.1145%2f2756553&partnerID=40&md5=b53dee872b839bb7a6fd591ddb864aed,"We consider the problem of detecting a cycle in a directed graph that grows by arc insertions and the related problems of maintaining a topological order and the strong components of such a graph. For these problems, we give two algorithms, one suited to sparse graphs, the other to dense graphs. The former takes O(min{m1/2, n2/3}m) time to insert m arcs into an n-vertex graph; the latter takes O(n2 log n) time. Our sparse algorithm is substantially simpler than a previous O(m3/2)-time algorithm; it is also faster on graphs of sufficient density. The time bound of our dense algorithm beats the previously best time bound of O(n5/2) for dense graphs. Our algorithms rely for their efficiency on vertex numberings weakly consistent with topological order: we allow ties. Bounds on the size of the numbers give bounds on running time. © 2015 ACM.",Cycle detection; Incremental data structure; Strongly connected components; Topological ordering,Algorithms; Directed graphs; Graphic methods; Topology; Cycle detection; Incremental data; N-vertex graph; New approaches; Sparse algorithms; Strongly connected component; Time algorithms; Topological order; Graph theory
Label cover instances with large girth and the hardness of approximating basic k-Spanner,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954223441&doi=10.1145%2f2818375&partnerID=40&md5=73f2de77a547f1f1568ef5c282e30da9,"We study the well-known Label Cover problem under the additional requirement that problem instances have large girth. We show that if the girth is some k, the problem is roughly 2(log1-∈ n)/k hard to approximate for all constant ∈ > 0. A similar theorem was claimed by Elkin and Peleg [2000] as part of an attempt to prove hardness for the basic k-spanner problem, but their proof was later found to have a fundamental error. Thus, we give both the first nontrivial lower bound for the problem of Label Cover with large girth as well as the first full proof of strong hardness for the basic k-spanner problem, which is both the simplest problem in graph spanners and one of the few for which super-logarithmic hardness was not known. Assuming NP ⊈ BPTIME(2polylog(n)), we show (roughly) that for every k ≥ 3 and every constant ∈ > 0, it is hard to approximate the basic k-spanner problem within a factor better than 2(log1-∈ n)/k. This improves over the previous best lower bound of only Ω(log n)/k from Kortsarz [2001]. Our main technique is subsampling the edges of 2-query probabilistically checkable proofs (PCPs), which allows us to reduce the degree of a PCP to be essentially equal to the soundness desired. This turns out to be enough to basically guarantee large girth. © 2015 ACM.",Graph spanners; Probabilistically checkable proofs,Algorithms; Cover problem; Graph spanners; Lower bounds; Probabilistically checkable proof; Problem instances; Hardness
Simultaneous PQ-ordering with applications to constrained embedding problems,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954200442&doi=10.1145%2f2738054&partnerID=40&md5=36f38f985ed25728302d1eadc7c12313,"In this article, we define and study the new problem of SIMULTANEOUS PQ-ORDERING. Its input consists of a set of PQ-trees, which represent sets of circular orders of their leaves, together with a set of child-parent relations between these PQ-trees, such that the leaves of the child form a subset of the leaves of the parent. SIMULTANEOUS PQ-ORDERING asks whether orders of the leaves of each of the trees can be chosen simultaneously; that is, for every child-parent relation, the order chosen for the parent is an extension of the order chosen for the child. We show that SIMULTANEOUS PQ-ORDERING is NP-complete in general, and we identify a family of instances that can be solved efficiently, the 2-fixed instances. We show that this result serves as a framework for several other problems that can be formulated as instances of SIMULTANEOUS PQ-ORDERING. In particular, we give linear-time algorithms for recognizing simultaneous interval graphs and extending partial interval representations. Moreover, we obtain a linear-time algorithm for PARTIALLY PQ-CONSTRAINED PLANARITY for biconnected graphs, which asks for a planar embedding in the presence of PQ-trees that restrict the possible orderings of edges around vertices, and a quadratic-time algorithm for SIMULTANEOUS EMBEDDING WITH FIXED EDGES for biconnected graphs with a connected intersection. Both results can be extended to the case where the input graphs are not necessarily biconnected but have the property that each cutvertex is contained in at most two nontrivial blocks. This includes, for example, the case where both graphs have a maximum degree of 5. © 2015 ACM.",Interval graphs; Ordering problem; Planar embeddings; PQ-trees; Simultaneous embedding,Clustering algorithms; Forestry; Graphic methods; Interval graph; Order problems; Planar embeddings; PQ-tree; Simultaneous embedding; Trees (mathematics)
Approximation algorithms and hardness of the k-route cut problem,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954200150&doi=10.1145%2f2644814&partnerID=40&md5=0b36cc78506231fad00994113228dd06,"We study the k-route cut problem: given an undirected edge-weighted graph G = (V, E), a collection {(s1, t1), (s2, t2), ⋯, (sr, tr)} of source-sink pairs, and an integer connectivity requirement k, the goal is to find a minimum-weight subset E′ of edges to remove, such that the connectivity of every pair (si, ti) falls below k. Specifically, in the edge-connectivity version, EC-kRC, the requirement is that there are at most (k. 1) edge-disjoint paths connecting si to ti in G\ E′, while in the vertex-connectivity version, VC-kRC, the same requirement is for vertex-disjoint paths. Prior to our work, poly-logarithmic approximation algorithms have been known for the special case where k ≤ 3, but no non-trivial approximation algorithms were known for any value k > 3, except in the single-source setting. We show an O(klog3/2 r)-approximation algorithm for EC-kRC with uniform edge weights, and several polylogarithmic bi-criteria approximation algorithms for EC-kRC and VC-kRC, where the connectivity requirement k is violated by a constant factor. We complement these upper bounds by proving that VC-kRC is hard to approximate to within a factor of k∈ for some fixed ∈ > 0. We then turn to study a simpler version of VC-kRC, where only one source-sink pair is present. We give a simple bi-criteria approximation algorithm for this case, and show evidence that even this restricted version of the problem may be hard to approximate. For example, we prove that the single source-sink pair version of VC-kRC has no constant-factor approximation, assuming Feige's Random κ-AND assumption. © 2015 ACM.",Approximation algorithm; K-route cut problem,Algorithms; Graph theory; Constant factor approximation; Edge connectivity; Edge disjoint paths; Edge-weighted graph; Logarithmic approximation; Source-sink pairs; Vertex connectivity; Vertex disjoint paths; Approximation algorithms
Faster spectral sparsification and numerical algorithms for SDD matrices,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954203759&doi=10.1145%2f2743021&partnerID=40&md5=cd079735f271f872253f2322559372bd,"We study algorithms for spectral graph sparsification. The input is a graph G with n vertices and medges, and the output is a sparse graph G that approximates G in an algebraic sense. Concretely, for all vectors x and any ∈ > 0, the graph G satisfies (1-∈)xT LGx ≤ xTLGx ≤ (1 + ∈)xT LGx, where LG and LG are the Laplacians of G and G, respectively. The first contribution of this article applies to all existing sparsification algorithms that rely on solving solving linear systems on graph Laplacians. These algorithms are the fastest known to date. Specifically, we show that less precision is required in the solution of the linear systems, leading to speedups by an O(log n) factor. We also present faster sparsification algorithms for slightly dense graphs: -An O(mlog n) time algorithm that generates a sparsifier with O(nlog3 n/∈2) edges. -An O(mlog log n) time algorithm for graphs with more than nlog5 nlog log n edges. -An O(m) algorithm for graphs with more than nlog10 n edges. -An O(m) algorithm for unweighted graphs with more than nlog8 n edges. These bounds hold up to factors that are in O(poly(log log n)) and are conjectured to be removable. © 2015 ACM.",Spectral sparsification; Symmetric diagonally dominant (SDD) matrices,Graph theory; Graphic methods; Linear systems; Matrix algebra; Diagonally dominant; Graph laplacians; Graph sparsification; Numerical algorithms; Sparse graphs; Sparsification; Time algorithms; Unweighted graphs; Algorithms
Optimal partitioning for dual-pivot quicksort,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947911347&doi=10.1145%2f2743020&partnerID=40&md5=c9a82992d131ec42eae434dd8d55dd6f,"Dual-pivot quicksort refers to variants of classical quicksort where in the partitioning step two pivots are used to split the input into three segments. This can be done in different ways, giving rise to different algorithms. Recently, a dual-pivot algorithm due to Yaroslavskiy received much attention, because it replaced the wellengineered quicksort algorithm in Oracle's Java 7 runtime library. Nebel and Wild (ESA 2012) analyzed this algorithm and showed that on average it uses 1.9nln n + O(n) comparisons to sort an input of size n, beating standard quicksort, which uses 2nln n+ O(n) comparisons. We introduce a model that captures all dual-pivot algorithms, give a unified analysis, and identify new dual-pivot algorithms that minimize the average number of key comparisons among all possible algorithms up to a linear term. This minimum is 1.8nln n + O(n). For the case that the pivots are chosen from a small sample, we include a comparison of dual-pivot quicksort and classical quicksort. Specifically, we show that dual-pivot quicksort benefits from a skewed choice of pivots. We experimentally evaluate our algorithms and compare them to Yaroslavskiy's algorithm and the recently described 3-pivot quicksort algorithm of Kushagra et al. (ALENEX 2014). © 2015 ACM 1549-6325/2015/11-ART18 $15.00.",Dual-pivot; Quicksort; Sorting,Algorithms; Sorting; Average numbers; Dual-pivot; Optimal partitioning; Partitioning step; Pivot algorithms; Quicksort; Run-time library; Unified analysis; Mathematical techniques
On the k-independence required by linear probing and minwise independence,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947920506&doi=10.1145%2f2716317&partnerID=40&md5=38cdcbcc7d230c49896b31a1119097a0,"We show that linear probing requires 5-independent hash functions for expected constant-time performance, matching an upper bound of Pagh et al. [2009]. More precisely, we construct a random 4-independent hash function yielding expected logarithmic search time for certain keys. For (1 + ε)-approximate minwise independence, we show that Ω(lg1ε)-independent hash functions are required, matching an upper bound of Indyk [2001]. We also show that the very fast 2-independent multiply-shift scheme of Dietzfelbinger [1996] fails badly in both applications. © 2015 ACM.",k-Independent hashing; Linear probing; Minwise independence,Algorithms; Mathematical techniques; Constant time; Linear probing; Logarithmic Search; Minwise independence; Upper Bound; Hash functions
Dominator tree certification and divergent spanning trees,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947931977&doi=10.1145%2f2764913&partnerID=40&md5=19d3d44d624bbcf91e9bc4b2bda71bb0,"How does one verify that the output of a complicated program is correct? One can formally prove that the program is correct, but this may be beyond the power of existing methods. Alternatively, one can check that the output produced for a particular input satisfies the desired input-output relation by running a checker on the input-output pair. Then one only needs to prove the correctness of the checker. For some problems, however, even such a checker may be too complicated to formally verify. There is a third alternative: augment the original program to produce not only an output but also a correctness certificate, with the property that a very simple program (whose correctness is easy to prove) can use the certificate to verify that the input-output pair satisfies the desired input-output relation. We consider the following important instance of this general question: How does one verify that the dominator tree of a flow graph is correct? Existing fast algorithms for finding dominators are complicated, and even verifying the correctness of a dominator tree in the absence of additional information seems complicated. We define a correctness certificate for a dominator tree, show how to use it to easily verify the correctness of the tree, and show how to augment fast dominator-finding algorithms so that they produce a correctness certificate. We also relate the dominator certificate problem to the problem of finding divergent spanning trees in a flow graph, and we develop algorithms to find such trees. All our algorithms run in linear time. Previous algorithms apply just to the special case of only trivial dominators, and they take at least quadratic time. © 2015 ACM.",Connectivity; Depth-first search; Directed graph; Dominators; Dynamic list; Flow graph; Global code optimization; Program certification,Directed graphs; Flow graphs; Graph algorithms; Graphic methods; Code optimization; Connectivity; Depth first search; Dominators; Program certification; Trees (mathematics)
Sorting and selection with imprecise comparisons,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947943893&doi=10.1145%2f2701427&partnerID=40&md5=c1732df7edeb69e299c39b3d98b6e7f6,"We consider a simple model of imprecise comparisons: there exists some δ > 0 such that when a subject is given two elements to compare, if the values of those elements (as perceived by the subject) differ by at least δ, then the comparison will be made correctly; when the two elements have values that are within δ, the outcome of the comparison is unpredictable. This model is inspired by both imprecision in human judgment of values and also by bounded but potentially adversarial errors in the outcomes of sporting tournaments. Our model is closely related to a number of models commonly considered in the psychophysics literature where δ corresponds to the Just Noticeable Difference (JND) unit or difference threshold. In experimental psychology, the method of paired comparisons was proposed as a means for ranking preferences among n elements of a human subject. The method requires performing all 2 comparisons, then sorting elements according to the number of wins. The large number of comparisons is performed to counter the potentially faulty decision-making of the human subject, who acts as an imprecise comparator. We show that in our model the method of paired comparisons has optimal accuracy, minimizing the errors introduced by the imprecise comparisons. However, it is also wasteful because it requires all(N2). We show that the same optimal guarantees can be achieved using 4n3/2 comparisons, and we prove the optimality of our method. We then explore the general tradeoff between the guarantees on the error that can be made and number of comparisons for the problems of sorting, max-finding, and selection. Our results provide strong lower bounds and close-to-optimal solutions for each of these problems. © 2015 ACM 1549-6325/2015/11-ART19 $15.00.",Faulty comparisons; Just Noticeable Difference; Maximum; Paired comparisons; Selection; Sorting; Tournament king; Weber's law,Decision making; Sorting; Faulty comparisons; Just-noticeable difference; Maximum; Paired comparison; Selection; Tournament king; Weber's law; Errors
Approximating the diameter of planar graphs in near linear time,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947915068&doi=10.1145%2f2764910&partnerID=40&md5=f593f1e04b75d86bf3518b81ed7c067f,We present a (1 + ε)-approximation algorithm running in O(f(ε). nlog4n) time for finding the diameter of an undirected planar graph with n vertices and with nonnegative edge lengths. © 2015 ACM.,All pairs shortest paths; Diameter; Distance oracles; Planar graphs,Approximation algorithms; Graph algorithms; Graph structures; Graphic methods; All pairs shortest paths; Diameter; Distance oracles; Near-linear time; Nonnegative edge lengths; Planar graph; Running-in; Graph theory
Simple deterministic algorithms for fully dynamic maximal matching,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947916322&doi=10.1145%2f2700206&partnerID=40&md5=6a52ce431f8bf8a2c3848326ddf24fac,"A maximal matching can be maintained in fully dynamic (supporting both addition and deletion of edges) nvertex graphs using a trivial deterministic algorithm with a worst-case update time of O(n). No deterministic algorithm that outperforms the naïve O(n) one was reported up to this date. The only progress in this direction is due to Ivković and Lloyd, who in 1993 devised a deterministic algorithm with an amortized update time of O((n + m) √2/2), where m is the number of edges. In this article, we show the first deterministic fully dynamic algorithm that outperforms the trivial one. Specifically, we provide a deterministic worst-case update time of O(√m). Moreover, our algorithm maintains a matching, which in fact is a 3/2-approximate maximum cardinality matching (MCM). We remark that no fully dynamic algorithm for maintaining (2 - ε)-approximate MCM improving upon the näve O(n) was known prior to this work, even allowing amortized time bounds and randomization. For low arboricity graphs (e.g., planar graphs and graphs excluding fixed minors), we devise another simple deterministic algorithm with sublogarithmic update time. Specifically, it maintains a fully dynamic maximal matching with amortized update time of O(logn/loglogn). This result addresses an open question of Onak and Rubinfeld [2010]. We also show a deterministic algorithm with optimal space usage, which for arbitrary graphs maintains a maximal matching in amortized O(√ m) time and uses only O(n + m) space. © 2015 ACM.",Dynamic algorithms; Maximal matching; Maximum matching,Graphic methods; Amortized time; Arbitrary graphs; Cardinality matching; Deterministic algorithms; Dynamic algorithm; Fully dynamic algorithms; Maximal matchings; Maximum matchings; Graph algorithms
Robust and maxmin optimization under matroid and knapsack uncertainty sets,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947923518&doi=10.1145%2f2746226&partnerID=40&md5=d9ac649dbb57612a063105d1486703c4,"Consider the following problem: given a set system (U, Omega;) and an edge-weighted graph G = (U, E) on the same universe U, find the set A ∈ Ω such that the Steiner tree cost with terminals A is as large as possible-""which set in Omega; is the most difficult to connect up?"" This is an example of a max-min problem: find the set A ∈ Ω such that the value of some minimization (covering) problem is as large as possible. In this article, we show that for certain covering problems that admit good deterministic online algorithms, we can give good algorithms for max-min optimization when the set system Ω is given by a p-system or knapsack constraints or both. This result is similar to results for constrained maximization of submodular functions. Although many natural covering problems are not even approximately submodular, we show that one can use properties of the online algorithm as a surrogate for submodularity Moreover, we give stronger connections between max-min optimization and two-stage robust optimization, and hence give improved algorithms for robust versions of various covering problems, for cases where the uncertainty sets are given by p-systems and knapsack constraints. © 2015 ACM.",Approximation algorithms; Online algorithms; Robust optimization; Submodularity,Combinatorial optimization; Online systems; Optimization; Set theory; Trees (mathematics); Deterministic online algorithms; Edge-weighted graph; Knapsack constraints; Max-min optimizations; On-line algorithms; Robust optimization; Submodular functions; Submodularity; Approximation algorithms
Quasi-polynomial local search for restricted max-min fair allocation,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947911532&doi=10.1145%2f2818695&partnerID=40&md5=a58728900d08e0d11e235a4e48d2754e,"The restricted max-min fair allocation problem (also known as the restricted Santa Claus problem) is one of few problems that enjoys the intriguing status of having a better estimation algorithm than approximation algorithm. Indeed, Asadpour et al. [2012] proved that a certain configuration LP can be used to estimate the optimal value within a factor of 1/(4 + ε), for any ε > 0, but at the same time it is not known how to efficiently find a solution with a comparable performance guarantee. A natural question that arises from their work is if the difference between these guarantees is inherent or results from a lack of suitable techniques. We address this problem by giving a quasi-polynomial approximation algorithm with the mentioned performance guarantee. More specifically, we modify the local search of Asadpour et al. [2012] and provide a novel analysis that lets us significantly improve the bound on its running time: from 2O(N) to nO(LOGN). Our techniques also have the interesting property that although we use the rather complex configuration LP in the analysis, we never actually solve it and therefore the resulting algorithm is purely combinatorial. © 2015 ACM 1549-6325/2015/11-ART13 $15.00.",Approximation algorithms; Linear programming; Local search; Max-min fair allocation,Linear programming; Local search (optimization); Polynomial approximation; Complex configuration; Estimation algorithm; Fair allocation; Local search; Optimal values; Performance guarantees; Quasi-poly-nomial; Running time; Approximation algorithms
A simplified 1.5-approximation algorithm for augmenting edge-connectivity of a graph from 1 to 2,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947911898&doi=10.1145%2f2786981&partnerID=40&md5=09612292563e860ace55a0fa819922d3,"The Tree Augmentation Problem (TAP) is as follows: given a connected graph G = (V, ε) and an edge set E on V, find a minimum size subset of edges F ⊆ E such that (V, ε cup F) is 2-edge-connected. In the conference version [Even et al. 2001] was sketched a 1.5-approximation algorithm for the problem. Since a full proof was very complex and long, the journal version was cut into two parts. The first part [Even et al. 2009] only proved ratio 1.8. An attempt to simplify the second part produced an error in Even et al. [2011]. Here we give a correct, different, and self-contained proof of the ratio 1.5 that is also substantially simpler and shorter than the previous proofs. © 2015 ACM 1549-6325/2015/11-ART23 $15.00.",Edge connectivity; Laminar set family; Tree Augmentation,Approximation algorithms; Forestry; Graph algorithms; Connected graph; Edge connectivity; Edge-sets; Laminar set family; Tree augmentation; Trees (mathematics)
An improved competitive algorithm for reordering buffer management,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932651472&doi=10.1145%2f2663347&partnerID=40&md5=a0f4b3dfaa3fef0cd43f245694c05761,"We design and analyze an online reordering buffer management algorithm with improved O(logk/loglogk) competitive ratio for nonuniform costs, where k is the buffer size. This improves on the best previous result (even for uniform costs) of Englert and Westermann (2005) giving O(log k) competitive ratio, which was also the best (offline) polynomial time approximation guarantee for this problem. Our analysis is based on an intricate dual fitting argument using a linear programming relaxation for the problem that we introduce in this article.",Online computing; Reordering buffer management,Algorithms; Linear programming; Social networking (online); Buffer management; Buffer sizes; Competitive algorithms; Competitive ratio; Linear programming relaxation; Offline; Online computing; Polynomial time approximation; Polynomial approximation
Rank-balanced trees,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931270631&doi=10.1145%2f2689412&partnerID=40&md5=4d2d2f54a2e2f0accd5a3b3060b5d8b8,"Since the invention of AVL trees in 1962, many kinds of binary search trees have been proposed. Notable are red-black trees, in which bottom-up rebalancing after an insertion or deletion takes O(1) amortized time and O(1) rotations worst-case. But the design space of balanced trees has not been fully explored. We continue the exploration. Our contributions are three: We systematically study the use of ranks and rank differences to define height-based balance in binary trees. Different invariants on rank differences yield AVL trees, red-black trees, and other kinds of balanced trees. By relaxing AVL trees, we obtain a new kind of balanced binary tree, the weak AVL tree (wavl tree), whose properties we develop. Bottom-up rebalancing after an insertion or deletion takes O(1) amortized time and at most two rotations, improving the three or more rotations per deletion needed in all other kinds of balanced trees of which we are aware. The height bound of a wavl tree degrades gracefully from that of an AVL tree as the number of deletions increases and is never worse than that of a red-black tree. Wavl trees also support top-down, fixed look-ahead rebalancing in O(1) amortized time. Finally, we use exponential potential functions to prove that in wavl trees rebalancing steps occur exponentially infrequently in rank. Thus, most of the rebalancing is at the bottom of the tree, which is crucial in concurrent applications and in those in which rotations take time that depends on the subtree size. © 2015 ACM.",Amortized complexity; AVL trees; Balanced binary trees; Data structures; Exponential potential function; Red-black trees; Search trees,Bins; Data structures; Rotation; Trees (mathematics); Amortized complexity; AVL tree; Balanced binary tree; Potential function; Red black tree; Search trees; Binary trees
Testing planarity of partially embedded graphs,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928239369&doi=10.1145%2f2629341&partnerID=40&md5=dfa6c1a7b63236de2203ae9c634eeab8,"We study the following problem: given a planar graph G and a planar drawing (embedding) of a subgraph of G, can such a drawing be extended to a planar drawing of the entire graph G? This problem fits the paradigm of extending a partial solution for a problem to a complete one, which has been studied before in many different settings. Unlike many cases, in which the presence of a partial solution in the input makes an otherwise easy problem hard, we show that the planarity question remains polynomial-time solvable. Our algorithm is based on several combinatorial lemmas, which show that the planarity of partially embedded graphs exhibits the 'TONCAS' behavior ""the obvious necessary conditions for planarity are also sufficient."" These conditions are expressed in terms of the interplay between (1) the rotation system and containment relationships between cycles and (2) the decomposition of a graph into its connected, biconnected, and triconnected components. This implies that no dynamic programming is needed for a decision algorithm and that the elements of the decomposition can be processed independently. Further, by equipping the components of the decomposition with suitable data structures and by carefully splitting the problem into simpler subproblems, we make our algorithm run in linear time. Finally, we consider several generalizations of the problem, such as minimizing the number of edges of the partial embedding that need to be rerouted to extend it, and argue that they are NP-hard. We also apply our algorithm to the simultaneous graph drawing problem SIMULTANEOUS EMBEDDING WITH FIXED EDGES (SEFE). There we obtain a linear-time algorithm for the case that one of the input graphs or the common graph has a fixed planar embedding. © 2015 ACM.",Algorithms; Theory,Algorithms; Clustering algorithms; Drawing (graphics); Dynamic programming; Embeddings; Graph algorithms; Polynomial approximation; Decision algorithms; Following problem; Linear-time algorithms; Minimizing the number of; Partial embedding; Planar embedding; Simultaneous graph drawing; Theory; Graph theory
The discrete and semicontinuous Fréchet distance with shortcuts via approximate distance counting and selection,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928237747&doi=10.1145%2f2700222&partnerID=40&md5=13f6202a9f46b867903fb4e646dee264,"The Fréchet distance is a well-studied similarity measure between curves. The discrete Fréchet distance is an analogous similarity measure, defined for two sequences of mand n points, where the points are usually sampled from input curves. We consider a variant, called the discrete Fréchet distance with shortcuts, which captures the similarity between (sampled) curves in the presence of outliers.When shortcuts are allowed only in one noise-containing curve, we give a randomized algorithm that runs in O((m+ n)6/5+ε) expected time, for any ε > 0. When shortcuts are allowed in both curves, we give an O((m2/3n2/3 + m+ n) log3(m+ n))-time deterministic algorithm. We also consider the semicontinuous Fréchet distance with one-sided shortcuts, where we have a sequence of mpoints and a polygonal curve of n edges, and shortcuts are allowed only in the sequence. We show that this problem can be solved in randomized expected time O((m+ n)2/3m2/3n1/3 log(m+ n)). Our techniques are novel andmay find further applications. One of themain new technical results is: Given two sets of points A and B in the plane and an interval I, we develop an algorithm that decides whether the number of pairs (x, y) ∈ A × B whose distance dist(x, y) is in I is less than some given threshold L. The running time of this algorithm decreases as L increases. In case there are more than L pairs of points whose distance is in I, we can get a small sample of pairs that contain a pair at approximate median distance (i.e., we can approximately ""bisect"" I). We combine this procedure with additional ideas to search, with a small overhead, for the optimal one-sided Fréchet distance with shortcuts, using a very fast decision procedure. We also show how to apply this technique for approximating distance selection (with respect to rank), and a somewhat more involved variant of this technique is used in the solution of the semicontinuous Fréchet distance with one-sided shortcuts. In general, the new technique can be applied to optimization problems for which the decision procedure is very fast but standard techniques like parametric search makes the optimization algorithm substantially slower. © 2015 ACM.",Algorithms; Theory,Algorithms; Mathematical techniques; Decision procedure; Deterministic algorithms; Optimization algorithms; Optimization problems; Polygonal curve; Randomized Algorithms; Similarity measure; Theory; Genetic algorithms
On the performance of smith's rule in single-machine scheduling with nonlinear cost,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928229688&doi=10.1145%2f2629652&partnerID=40&md5=d2bc00ba7021cc0247f0a9b72d538b1d,"We consider a single-machine scheduling problem. Given some continuous, nondecreasing cost function, we aim to compute a schedule minimizing the weighted total cost, where the cost of each job is determined by the cost function value at its completion time. This problem is closely related to scheduling a single machine with nonuniform processing speed. We show that for piecewise linear cost functions it is strongly NP-hard. The main contribution of this article is a tight analysis of the approximation guarantee of Smith's rule under any convex or concave cost function. More specifically, for these wide classes of cost functions we reduce the task of determining a worst-case problem instance to a continuous optimization problem, which can be solved by standard algebraic or numerical methods. For polynomial cost functions with positive coefficients, it turns out that the tight approximation ratio can be calculated as the root of a univariate polynomial. We show that this approximation ratio is asymptotically equal to κ (κ-1)/(κ+1), denoting by κ the degree of the cost function. To overcome unrealistic worst-case instances, we also give tight bounds for the case of integral processing times that are parameterized by the maximum and total processing time. © 2015 ACM.",Algorithms; Performance,Algorithms; Cost benefit analysis; Machinery; Numerical methods; Optimization; Piecewise linear techniques; Polynomial approximation; Scheduling; Continuous optimization problems; Performance; Piecewise linear cost functions; Positive coefficients; Single machine scheduling problems; Single-machine scheduling; Total processing time; Worst-case instances; Cost functions
Optimal lower and upper bounds for representing sequences,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928251823&doi=10.1145%2f2629339&partnerID=40&md5=56636fd85fe69dac85f5b875a2592d12,"Sequence representations supporting the queries access, select, and rank are at the core of many data structures. There is a considerable gap between the various upper bounds and the few lower bounds known for such representations, and how they relate to the space used. In this article, we prove a strong lower bound for rank, which holds for rather permissive assumptions on the space used, and give matching upper bounds that require only a compressed representation of the sequence. Within this compressed space, the operations access and select can be solved in constant or almost-constant time, which is optimal for large alphabets. Our new upper bounds dominate all of the previous work in the time/space map. © 2015 ACM.",Algorithms,Algorithms; Constant time; Large alphabets; Lower and upper bounds; Lower bounds; Upper Bound; Mathematical techniques
Directed Subset Feedback Vertex Set is Fixed-Parameter Tractable,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928235454&doi=10.1145%2f2700209&partnerID=40&md5=a50298b33785c320e301f3df21ae2b4f,"Given a graph G and an integer κ, the FEEDBACK VERTEX SET (FVS) problem asks if there is a vertex set T of size at most κ that hits all cycles in the graph. The first fixed-parameter algorithm for FVS in undirected graphs appeared in a monograph of Mehlhorn in 1984. The fixed-parameter tractability (FPT) status of FVS in directed graphs was a long-standing open problem until Chen et al. (STOC '08, JACM '08) showed that it is fixed-parameter tractable by giving a 4κ κ! · nO(1) time algorithm. There are two subset versions of this problems: We are given an additional subset S of vertices (resp., edges), and we want to hit all cycles passing through a vertex of S (resp., an edge of S); the two variants are known to be equivalent in the parameterized sense. Recently, the SUBSET FVS problem in undirected graphs was shown to be FPT by Cygan et al. (ICALP'11, SIDMA'13) and independently by Kakimura et al. (SODA '12). We generalize the result of Chen et al. (STOC '08, JACM '08) by showing that a SUBSET FVS in directed graphs can be solved in time 2O(κ 3) · nO(1) (i.e., FPT parameterized by size κ of the solution). By our result, we complete the picture for FVS problems and their subset versions in undirected and directed graphs. The technique of random sampling of important separators was used by Marx and Razgon (STOC '11, SICOMP '14) to show that UNDIRECTED MULTICUT is FPT, and it was generalized by Chitnis et al. (SODA '12, SICOMP '13) to directed graphs to show that DIRECTEDMULTIWAY CUT is FPT. In addition to proving the FPT of a DIRECTED SUBSET FVS, we reformulate the random sampling of important separators technique in an abstract way that can be used with a general family of transversal problems. We believe this general approach will be useful for showing the FPT of other problems in directed graphs. Moreover, we modify the probability distribution used in the technique to achieve better running time; in particular, this gives an improvement from 22O(κ) to 2O(κ 2) in the parameter dependence of the DIRECTED MULTIWAY CUT algorithm of Chitnis et al. (SODA '12, SICOMP '13). © 2015 ACM.",Algorithms; Theory,Algorithms; Graph algorithms; Graphic methods; Importance sampling; Parameter estimation; Probability distributions; Separators; Set theory; Feedback vertex set; Fixed-parameter algorithms; Fixed-parameter tractability; Parameter dependence; Random sampling; Theory; Time algorithms; Undirected graph; Directed graphs
Mapping simple polygons: The power of telling convex from reflex,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928266429&doi=10.1145%2f2700223&partnerID=40&md5=3854c612f9ad8f7c068f831808016188,"We consider the exploration of a simple polygon ρ by a robot that moves from vertex to vertex along edges of the visibility graph of ρ. The visibility graph has a vertex for every vertex of ρ and an edge between two vertices if they see each other-that is, if the line segment connecting them lies inside ρ entirely. While located at a vertex, the robot is capable of ordering the vertices it sees in counterclockwise order as they appear on the boundary, and for every two such vertices, it can distinguish whether the angle between them is convex (≤π) or reflex (>π). Other than that, distant vertices are indistinguishable to the robot.We assume that an upper bound on the number of vertices is known. We obtain the general result that a robot exploring any locally oriented, arc-labeled graph G can always determine the base graph of G. Roughly speaking, this is the smallest graph that cannot be distinguished by a robot from G by its observations alone, no matter how it moves. Combining this result with various other techniques allows the ability to show that a robot exploring a polygon ρ with the preceding capabilities is always capable of reconstructing the visibility graph of ρ. We also show that multiple identical, indistinguishable, and deterministic robots of this kind can always solve the weak rendezvous problem in which they need to position themselves such that they mutually see each other-for instance, such that they form a clique in the visibility graph. © 2015 ACM.",Algorithms; Theory,Algorithms; Geometry; Graph structures; Robots; Visibility; Labeled graphs; Line segment; Rendezvous problems; Simple polygon; Theory; Upper Bound; Visibility graphs; Graph theory
Computing shortest paths among curved obstacles in the plane,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928229404&doi=10.1145%2f2660771&partnerID=40&md5=14b3e389f3fa653a5905cb8834dc5e04,"A fundamental problem in computational geometry is to compute an obstacle-avoiding Euclidean shortest path between two points in the plane. The case of this problem on polygonal obstacles is well studied. In this article, we consider the problem version on curved obstacles, which are commonly modeled as splinegons. A splinegon can be viewed as replacing each edge of a polygon by a convex curved edge (polygons are special splinegons), and the combinatorial complexity of each curved edge is assumed to be O(1). Given in the plane two points s and t and a set S of h pairwise disjoint splinegons with a total of n vertices, after a bounded degree decomposition of S is obtained, we compute a shortest s-to-t path avoiding the splinegons in O(n+ h log h+ κ) time, where κ is a parameter sensitive to the geometric structures of the input and is upper bounded by O(h2). The bounded degree decomposition of S, which is similar to the triangulation of the polygonal domains, can be computed in O(n log n) time or O(n+ h log1+ε h) time for any ε > 0. In particular, when all splinegons are convex, the decomposition can be computed in O(n+ h log h) time and κ is linear to the number of common tangents in the free space (called ""free common tangents"") among the splinegons. Our techniques also improve several previous results: (1) For the polygon case (i.e., when all splinegons are polygons), the shortest path problem was previously solved in O(n log n) time, or in O(n+h2 log n) time. Thus, our algorithm improves the O(n+h2 log n) time result, and is faster than the O(n log n) time solution for sufficiently small h, for example, h = o(√ n log n). (2) Our techniques produce an optimal output-sensitive algorithm for a basic visibility problem of computing all free common tangents among h pairwise disjoint convex splinegons with a total of n vertices. Our algorithm runs in O(n+h log h+κ) time and O(n) working space, where κ is the number of all free common tangents. Note that κ = O(h2). Even for the special case where all splinegons are convex polygons, the previously best algorithm for this visibility problem takes O(n+ h2 log n) time. (3) We improve the previous work for computing the shortest path between two points among convex pseudodisks of O(1) complexity each. In addition, a by-product of our techniques is an optimal O(n+ h log h) time and O(n) space algorithm for computing the Voronoi diagram of a set of h pairwise disjoint convex splinegons with a total of n vertices. © 2015 ACM.",Algorithms; Theory,Algorithms; Computational geometry; Visibility; Combinatorial complexity; Euclidean shortest path; Geometric structure; Obstacle-avoiding; Output-sensitive algorithm; Shortest path problem; Theory; Voronoi diagrams; Graph theory
Fixed-parameter algorithms for minimum-cost edge-connectivity augmentation,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928254149&doi=10.1145%2f2700210&partnerID=40&md5=44510e62aee77516f79581a1ef6bf90a,"We consider connectivity-augmentation problems in a setting where each potential new edge has a nonnegative cost associated with it, and the task is to achieve a certain connectivity target with at most p new edges of minimum total cost. The main result is that the minimum cost augmentation of edge-connectivity from κ-1 to κ with at most p new edges is fixed-parameter tractable parameterized by p and admits a polynomial kernel. We also prove the fixed-parameter tractability of increasing edge connectivity from 0 to 2 and increasing node connectivity from 1 to 2. © 2015 ACM.",Algorithms; Theory,Algorithms; Connectivity augmentation problems; Edge connectivity; Fixed-parameter algorithms; Fixed-parameter tractability; Node connectivity; Parameterized; Polynomial kernels; Theory; Mathematical techniques
A bounded budget network creation game,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928253635&doi=10.1145%2f2701615&partnerID=40&md5=13af30dad85b280811bfe0b38f94d61b,"We introduce a network creation game in which each player (vertex) has a fixed budget to establish links to other players. In this model, each link has a unit price, and each agent tries to minimize its cost, which is either its eccentricity or its total distance to other players in the underlying (undirected) graph of the created network. Two versions of the game are studied: In the MAX version, the cost incurred to a vertex is the maximum distance between the vertex and other vertices, and, in the SUM version, the cost incurred to a vertex is the sum of distances between the vertex and other vertices. We prove that in both versions pure Nash equilibria exist, but the problem of finding the best response of a vertex is NP-hard. We take the social cost of the created network to be its diameter, and next we study the maximum possible diameter of an equilibrium graph with n vertices in various cases. When the sum of players' budgets is n-1, the equilibrium graphs are always trees, and we prove that their maximum diameter is ⊖(n) and ⊖ (log n) in MAX and SUM versions, respectively. When each vertex has a unit budget (i.e., can establish a link to just one vertex), the diameter of any equilibrium graph in either version is ⊖ (1).We give examples of equilibrium graphs in the MAX version, such that all vertices have positive budgets and yet the diameter is ω(√ log n). This interesting (and perhaps counterintuitive) result shows that increasing the budgets may increase the diameter of equilibrium graphs and hence deteriorate the network structure. Then we prove that every equilibrium graph in the SUM version has diameter 2O(√ log n). Finally, we show that if the budget of each player is at least k, then every equilibrium graph in the SUM version is κ-connected or has a diameter smaller than 4. © 2015 ACM.",Algorithms; Design; Theory,Algorithms; Budget control; Computer networks; Design; Trees (mathematics); Best response; Maximum distance; Network creation; Network structures; Pure Nash equilibrium; Sum of distances; Theory; Total distances; Graph structures
Faster fully compressed pattern matching by recompression,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944886866&doi=10.1145%2f2631920&partnerID=40&md5=86485db1e9364f76f01063fc50cbb34a,"In this article, a fully compressed pattern matching problem is studied. The compression is represented by straight-line programs (SLPs)-that is, context-free grammars generating exactly one string; the term fully means that both the pattern and the text are given in the compressed form. The problem is approached using a recently developed technique of local recompression: the SLPs are refactored so that substrings of the pattern and text are encoded in both SLPs in the same way. To this end, the SLPs are locally decompressed and then recompressed in a uniform way. This technique yields an O((n+ m)log M) algorithm for compressed pattern matching, assuming that M fits in O(1) machine words, where n (m) is the size of the compressed representation of the text (pattern, respectively), and M is the size of the decompressed pattern. If only m + n fits in O(1) machine words, the running time increases to O((n+m)log Mlog(n+m)). The previous best algorithm due to Lifshits has O(n2m) running time. © 2015 ACM 1549-6325/2015/01-ART16 $15.00.",Algorithms for compressed data; Compressed pattern matching; Lempel-Ziv compression; Pattern matching; Straight-line programs,Algorithms; Context free grammars; Compressed datum; Compressed pattern matching; Recompression; Running time; Straight line program; Sub-strings; Pattern matching
A polynomial-time approximation scheme for Euclidean Steiner forest,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950249313&doi=10.1145%2f2629654&partnerID=40&md5=640b754106f791f851f78286dfb3c9e0,"We give a randomized O(n polylogn)-time approximation scheme for the Steiner forest problem in the Euclidean plane. For every fixed e > 0 and given n terminals in the plane with connection requests between some pairs of terminals, our scheme finds a (1 + ∈) approximation to the minimum-length forest that connects every requested pair of terminals. © 2015 ACM 1549-6325/2015/01-ART16 $15.00.",Approximation algorithms; Euclidean plane; Steiner forest,Approximation algorithms; Approximation theory; Forestry; Geometry; Approximation scheme; Euclidean; Euclidean planes; N-terminals; Polynomial time approximation schemes; Steiner forest problem; Steiner forests; Polynomial approximation
A quasi-polynomial time partition oracle for graphs with an excluded minor,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950243577&doi=10.1145%2f2629508&partnerID=40&md5=a8f7853a8b6f0fc7f8bcffd34fafe916,"Motivated by the problem of testing planarity and related properties, we study the problem of designing efficient partition oracles. Apartition oracle is a procedure that, given access to the incidence lists representation of a bounded-degree graph G = (V, E) and a parameter e, when queried on a vertex v ∈ V, returns the part (subset of vertices) that v belongs to in a partition of all graph vertices. The partition should be such that all parts are small, each part is connected, and if the graph has certain properties, the total number of edges between parts is at most ∈ | V |. In this work, we give a partition oracle for graphs with excluded minors whose query complexity is quasi-polynomial in 1/∈, improving on the result of Hassidim et al. (Proceedings of FOCS 2009), who gave a partition oracle with query complexity exponential in 1/∈. This improvement implies corresponding improvements in the complexity of testing planarity and other properties that are characterized by excluded minors as well as sublinear-time approximation algorithms that work under the promise that the graph has an excluded minor. © 2015 ACM 1549-6325/2015/01-ART16 $15.00.",Minor-free; Partition oracle; Planarity; Property testing,Approximation algorithms; Polynomial approximation; Bounded degree graphs; Minor-free; Other properties; Planarity; Property-testing; Quasi-poly-nomial; Quasi-polynomial time; Query complexity; Graph theory
Minimum makespan multi-vehicle Dial-A-Ride,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950298556&doi=10.1145%2f2629653&partnerID=40&md5=e171f6146dbdf6316cf41c0c2bd8e5c4,"Dial-A-Ride problems consist of a set V of n vertices in a metric space (denoting travel time between vertices) and a set of m objects represented as source-destination pairs {(si, ti)}mi = 1, where each object requires to be moved from its source to destination vertex. In the multi-vehicle Dial-A-Ride problem, there are q vehicles, each having capacity k and where each vehicle j ∈ [q] has its own depot-vertex rj ∈ V. A feasible schedule consists of a capacitated route for each vehicle (where vehicle j originates and ends at its depot rj) that together move all objects from their sources to destinations. The objective is to find a feasible schedule that minimizes the maximum completion time (i.e., makespan) of vehicles, where the completion time of vehicle j is the time when it returns to its depot rj at the end of its route. We study the preemptive version of multivehicle Dial-A-Ride, in which an object may be left at intermediate vertices and transported by more than one vehicle, while being moved from source to destination. Our main results are an O(log3n)-Approximation algorithm for preemptive multi-vehicle Dial-A-Ride, and an improved O(log t)-Approximation for its special case when there is no capacity constraint (here t ≤ n is the number of distinct depot-vertices). There is an Ω(log 1/4-∈ n) hardness of approximation known even for single vehicle capacitated Dial-A-Ride [Gørtz 2006]. For uncapacitated multi-vehicle Dial-A-Ride, we show that there are instances when natural lower bounds (used in our algorithm) are Ω(log t) factor away from the optimum. We also consider the special class of metrics induced by graphs excluding any fixed minor (e.g., planar metrics). In this case, we obtain improved guarantees of O(log2n) for capacitated multi-vehicle Dial-A-Ride, and O(1) for the uncapacitated problem. © 2015 ACM 1549-6325/2015/01-ART16 $15.00.",Approximation algorithms; Scheduling; Vehicle routing,Algorithms; Approximation algorithms; Scheduling; Scheduling algorithms; Travel time; Vehicle routing; Capacity constraints; Completion time; Dial-a-ride problem; Feasible schedule; Hardness of approximation; Minimum makespan; Natural lower bounds; Source-destination pairs; Vehicles
Min st-cut oracle for planar graphs with near-linear preprocessing time,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950288646&doi=10.1145%2f2684068&partnerID=40&md5=7852ab6f0ec3f9c826db195242d0347b,"For an undirected n-vertex planar graph G with nonnegative edge weights, we consider the following type of query: given two vertices s and t in G, what is the weight of a min st-cut in G? We show how to answer such queries in constant time with O(nlog4 n) preprocessing time and O(nlogn) space. We use a Gomory-Hu tree to represent all the pairwise min cuts implicitly. Previously, no subquadratic time algorithm was known for this problem. Since all-pairs min cut and the minimum-cycle basis are dual problems in planar graphs, we also obtain an implicit representation of a minimum-cycle basis in O(nlog4 n) time and O(nlogn) space. Additionally, an explicit representation can be obtained in O(C) time and space where C is the size of the basis. These results require that shortest paths are unique. This can be guaranteed either by using randomization without overhead or deterministically with an additional log2 n factor in the preprocessing times. © 2015 ACM 1549-6325/2015/01-ART16 $15.00.",Minimum cut; Minimum cycle basis; Planar graphs,Graphic methods; Query processing; Explicit representation; Implicit representation; Minimum cut; Minimum cycle basis; Planar graph; Preprocessing time; Shortest path; Time algorithms; Graph theory
Average case and distributional analysis of dual-pivot Quicksort,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947902964&doi=10.1145%2f2629340&partnerID=40&md5=9af67210b7dde906466b128a8a9948d0,"In 2009, Oracle replaced the long-serving sorting algorithm in its Java 7 runtime library by a new dual-pivot Quicksort variant due to Vladimir Yaroslavskiy. The decision was based on the strikingly good performance of Yaroslavskiy's implementation in running time experiments. At that time, no precise investigations of the algorithm were available to explain its superior performance-on the contrary: previous theoretical studies of other dual-pivot Quicksort variants even discouraged the use of two pivots. In 2012, two of the authors gave an average case analysis of a simplified version of Yaroslavskiy's algorithm, proving that savings in the number of comparisons are possible. However, Yaroslavskiy's algorithm needs more swaps, which renders the analysis inconclusive. To force the issue, we herein extend our analysis to the fully detailed style of Knuth: we determine the exact number of executed Java Bytecode instructions. Surprisingly, Yaroslavskiy's algorithm needs sightly more Bytecode instructions than a simple implementation of classic Quicksort-contradicting observed running times. As in Oracle's library implementation, we incorporate the use of Insertionsort on small subproblems and show that it indeed speeds up Yaroslavskiy's Quicksort in terms of Bytecodes; but even with optimal Insertionsort thresholds, the new Quicksort variant needs slightly more Bytecode instructions on average. Finally, we show that the (suitably normalized) costs of Yaroslavskiy's algorithm converge to a random variable whose distribution is characterized by a fixed-point equation. From that, we compute variances of costs and show that for large n, costs are concentrated around their mean. © 2015 ACM 1549-6325/2015/01-ART16 $15.00.",Analysis of algorithms; Dual-pivot Quicksort; Limiting distributions,Costs; Java programming language; Analysis of algorithms; Average-case analysis; Fixed point equation; Limiting distributions; Quicksort; Run-time library; Sorting algorithm; Theoretical study; Algorithms
Time complexity of link reversal routing,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950237820&doi=10.1145%2f2644815&partnerID=40&md5=91bb468b24b67185d4384f5ea47588fe,"Link reversal is a versatile algorithm design paradigm, originally proposed by Gafni and Bertsekas in 1981 for routing and subsequently applied to other problems including mutual exclusion, leader election, and resource allocation. Although these algorithms are well known, until now there have been only preliminary results on time complexity, even for the simplest link reversal algorithm for routing, called Full Reversal. In Full Reversal, a sink reverses all its incident links, whereas in other link reversal algorithms (e.g., Partial Reversal), a sink reverses only some of its incident links. Charron-Bost et al. introduced a generalization, called LR, that includes Full and Partial Reversal as special cases. In this article, we present an exact expression for the time complexity of LR. The expression is stated in terms of simple properties of the initial graph. The result specializes to exact formulas for the time complexity of any node in any initial acyclic directed graph for both Full and Partial Reversal. Having the exact formulas provides insight into the behavior of Full and Partial Reversal on specific graph families. Our first technical insight is to describe the behavior of Full Reversal as a dynamical system and to observe that this system is linear in min-plus algebra. Our second technical insight is to overcome the difficulty posed by the fact that LR is not linear by transforming every execution of LR from an initial graph into an execution of Full Reversal from a different initial graph while maintaining the execution's work and time complexity. © 2015 ACM 1549-6325/2015/01-ART16 $15.00.",Linear dynamical systems; Link reversal; Min-plus algebra; Routing; Time complexity,Algebra; Directed graphs; Dynamical systems; Linear control systems; Linear dynamical systems; Link reversal; Min-plus algebra; Routing; Time complexity; Algorithms
Maximizing the minimum load for random processing times,2015,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950244478&doi=10.1145%2f2651421&partnerID=40&md5=d9048518ab8b0ceadd585fa4abb493b8,"In this article, we consider a stochastic variant of the so-called Santa Claus problem. The Santa Claus problem is equivalent to the problem of scheduling a set of n jobs on mparallel machines without preemption, so as to maximize the minimum load. We consider the identical machine version of this scheduling problem with the additional restriction that the scheduler has only a guess of the processing times; that is, the processing time of a job is a random variable. We show that there is a critical value ρ(n, m) such that if the duration of the jobs is exponentially distributed and the expected values deviate by less than a multiplicative factor of ρ(n, m) from each other, then a greedy algorithm has an expected competitive ratio arbitrarily close to one; that is, it performs in expectation almost as good as an algorithm that knows the actual values in advance. On the other hand, if the expected values deviate by more than a multiplicative factor of ρ(n, m), then the expected performance is arbitrarily bad for all algorithms. © 2015 ACM 1549-6325/2015/01-ART16 $15.00.",Average-case analysis; Expected competitive ratio; Scheduling,Algorithms; Stochastic systems; Average-case analysis; Competitive ratio; Expected values; Greedy algorithms; Identical machines; Multiplicative factors; Random processing time; Scheduling problem; Scheduling
Two-dimensional parameterized matching,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908892377&doi=10.1145%2f2650220&partnerID=40&md5=2c264fae6dfa51b5e3d3ff31da62337b,"Two equal-length strings, or two equal-sized two-dimensional texts, parameterize match (p-match) if there is a one-one mapping (relative to the alphabet) of their characters. Two-dimensional parameterized matching is the task of finding all m x m substrings of an n x n text that p-match an m x m pattern. This models searching for color images with changing of color maps, for example. We present two algorithms that solve the twodimensional parameterized matching problem. The time complexities of our algorithms are O(n2log2m) and O(n2 + m2.5 polylog(m)). Our algorithms are faster than the O(n2mlog2m log log m) time algorithm for this problem of Amir et al. [2006]. A key step in both of our algorithms is to count the number of distinct characters in every m x m substring of an n x n string. We show how to solve this problem in O(n2) time. This result may be of independent interest. © 2014 ACM.",Parameterized matching; Two-dimensional pattern matching; Witness computation,Parameterization; Pattern matching; Color images; One-one mapping; Parameterized matching; Sub-strings; Time algorithms; Time complexity; Two-dimensional pattern matching; Two-dimensional texts; Parameter estimation
Minimizing movement: Fixed-parameter tractability,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908880955&doi=10.1145%2f2650247&partnerID=40&md5=f4419988e526150f863fbf9f058c413a,"We study an extensive class of movement minimization problems that arise from many practical scenarios but so far have little theoretical study. In general, these problems involve planning the coordinated motion of a collection of agents (representing robots, people, map labels, network messages, etc.) to achieve a global property in the network while minimizing the maximum or average movement (expended energy). The only previous theoretical results about this class of problems are about approximation and are mainly negative:manymovement problems of interest have polynomial inapproximability. Given that the number of mobile agents is typically much smaller than the complexity of the environment, we turn to fixed-parameter tractability. We characterize the boundary between tractable and intractable movement problems in a very general setup: it turns out the complexity of the problem fundamentally depends on the treewidth of the minimal configurations. Thus, the complexity of a particular problem can be determined by answering a purely combinatorial question. Using our general tools, we determine the complexity of several concrete problems and fortunately show that many movement problems of interest can be solved efficiently. © 2014 ACM.",Fixed-parameter tractability; Movement problems; Pebbles; Treewidth,Complex networks; Polynomial approximation; Robot programming; Collection of agents; Coordinated motion; Fixed-parameter tractability; Minimizing movements; Movement minimization problems; Movement problems; Pebbles; Tree-width; Mobile agents
Kernelization lower bounds through colors and IDs,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908891666&doi=10.1145%2f2650261&partnerID=40&md5=2a792e72b6fadf2c9e4d1c607ec09575,"In parameterized complexity, each problem instance comes with a parameter k, and a parameterized problem is said to admit a polynomial kernel if there are polynomial time preprocessing rules that reduce the input instance to an instance with size polynomial in k. Many problems have been shown to admit polynomial kernels, but it is only recently that a framework for showing the nonexistence of polynomial kernels for specific problems has been developed by Bodlaender et al. [2009] and Fortnow and Santhanam [2008]. With few exceptions, all known kernelization lower bounds results have been obtained by directly applying this framework. In this article, we show how to combine these results with combinatorial reductions that use colors and IDs in order to prove kernelization lower bounds for a variety of basic problems. To follow we give a summary of our main results. All results are under the assumption that the polynomial hierarchy does not collapse to the third level. - We show that the STEINER TREE problem parameterized by the number of terminals and solution size k, and the CONNECTED VERTEX COVER and CAPACITATED VERTEX COVER problems do not admit a polynomial kernel. The two latter results are surprising because the closely related VERTEX COVER problem admits a kernel with at most 2k vertices. - Alon and Gutner [2008] obtain a kpoly(h) kernel for DOMINATING SET IN H -MINOR FREE GRAPHS parameterized by h = | H | and solution size k, and ask whether kernels of smaller size exist. We partially resolve this question by showing that DOMINATING SET IN H -MINOR FREE GRAPHS does not admit a kernel with size polynomial in k + h. - Harnik and Naor [2007] obtain a ""compression algorithm"" for the SPARSE SUBSET SUM problem. We show that their algorithm is essentially optimal by showing that the instances cannot be compressed further. - The HITTING SET and SET COVER problems are among the most-studied problems in algorithmics. Both problems admit a kernel of size kO(d) when parameterized by solution size k and maximum set size d. We show that neither of them, along with the UNIQUE COVERAGE and BOUNDED RANK DISJOINT SETS problems, admits a polynomial kernel. The existence of polynomial kernels for several of the problems mentioned previously was an open problem explicitly stated in the literature [Alon and Gutner 2008; Betzler 2006; Guo and Niedermeier 2007; Guo et al. 2007; Moser et al. 2007]. Many of our results also rule out the existence of compression algorithms, a notion similar to kernelization defined by Harnik and Naor [2007], for the problems in question. © 2014 ACM.",Kernelization; Lower bounds; Parameterized complexity,Graph algorithms; Polynomial approximation; Trees (mathematics); Compression algorithms; Connected vertex cover; Kernelization; Lower bounds; Parameterized complexity; Parameterized problems; Polynomial hierarchies; Polynomial-time preprocessings; Parameterization
Dynamic ray stabbing,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908891665&doi=10.1145%2f2559153&partnerID=40&md5=972930ecbc33438aa6407f397f867661,"We consider maintaining a dynamic set S of N horizontal segments in ℝ2 such that, given a vertical ray Q in ℝ2, the segments in S intersecting Q can be reported efficiently. In the external memory model, we give a structure that consumes O (N/B) space, answers a query in O(logB N + K/B) time (where K is the number of reported segments), and can be updated in O(logB N) amortized time per insertion and deletion. With B set to a constant, the structure also works in internal memory, consuming space O(N), answering a query in O (log N + K) time, and supporting an update in O(log N) amortized time. © 2014 ACM.",Computational geometry; Dynamic data structures; Pointer machines; Ray stabbing; Segment intersection,Algorithms; Mathematical techniques; Amortized time; Dynamic data structure; External memory models; Horizontal segments; Internal memory; Pointer machines; Ray stabbing; Segment intersections; Computational geometry
Quasirandom rumor spreading,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908876815&doi=10.1145%2f2650185&partnerID=40&md5=ef61dbd1aefd1fdac8b6f3e3e959480a,"We propose and analyze a quasirandom analogue of the classical push model for disseminating information in networks (""randomized rumor spreading""). In the classical model, in each round, each informed vertex chooses a neighbor at random and informs it, if it was not informed before. It is known that this simple protocol succeeds in spreading a rumor from one vertex to all others within O(log n) rounds on complete graphs, hypercubes, random regular graphs, Erdo{combining double acute accent}s-Rényi random graphs, and Ramanujan graphs with probability 1 - o(1). In the quasirandom model, we assume that each vertex has a (cyclic) list of its neighbors. Once informed, it starts at a random position on the list, but from then on informs its neighbors in the order of the list. Surprisingly, irrespective of the orders of the lists, the above-mentioned bounds still hold. In some cases, even better bounds than for the classical model can be shown. 2014 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",,Graphic methods; Information dissemination; Classical model; Complete graphs; Ramanujan graphs; Random graphs; Random position; Random regular graph; Rumor spreading; SIMPLE protocol; Graph theory
Deterministic network exploration by anonymous silent agents with local traffic reports,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908887656&doi=10.1145%2f2594581&partnerID=40&md5=3ce200064eb2a5e3559e73816a06e0d2,"A team consisting of an unknown number of mobile agents starting from different nodes of an unknown network, possibly at different times, have to explore the network: Every node must be visited by at least one agent, and all agents must eventually stop. Agents are anonymous (identical), execute the same deterministic algorithm, and move in synchronous rounds along links of the network. They are silent: They cannot send any messages to other agents or mark visited nodes in any way. In the absence of any additional information, exploration with termination of an arbitrary network in this model, devoid of any means of communication between agents, is impossible. Our aim is to solve the exploration problem by giving to agents very restricted local traffic reports. Specifically, an agent that is at a node v in a given round is provided with three bits of information answering the following questions: Am I alone at v? Did any agent enter v in this round? Did any agent exit v in this round? We show that this small amount of information permits us to solve the exploration problem in arbitrary networks. More precisely, we give a deterministic terminating exploration algorithm working in arbitrary networks for all initial configurations that are not perfectly symmetric; that is, in which there are agents with different views of the network. The algorithm works in polynomial time in the (unknown) size of the network. A deterministic terminating exploration algorithm working for all initial configurations in arbitrary networks does not exist. © 2014 ACM.",Anonymous mobile agents; Deterministic algorithm; Exploration; Graphs; Network,Natural resources exploration; Networks (circuits); Polynomial approximation; Software agents; Amount of information; Arbitrary networks; Communication between agents; Deterministic algorithms; Exploration algorithms; Graphs; Initial configuration; Unknown networks; Mobile agents
Faster parameterized algorithms using linear programming,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908881799&doi=10.1145%2f2566616&partnerID=40&md5=fc4279b6893befb41f1fde55ee7ca4e2,"We investigate the parameterized complexity of VERTEX COVER parameterized by the difference between the size of the optimal solution and the value of the linear programming (LP) relaxation of the problem. By carefully analyzing the change in the LP value in the branching steps, we argue that combining previously known preprocessing rules with the most straightforward branching algorithm yields an O∗(2.618k) algorithm for the problem. Here, k is the excess of the vertex cover size over the LP optimum, and we write O∗ (f(k)) for a time complexity of the form O(f(k)nO(1)). We proceed to show that a more sophisticated branching algorithm achieves a running time of O∗(2.3146k). Following this, using previously known as well as new reductions, we give O∗(2.3146k) algorithms for the parameterized versions of ABOVE GUARANTEE VERTEX COVER, ODD CYCLE TRANSVERSAL, SPLIT VERTEX DELETION, and ALMOST 2-SAT, and O∗(1.5214k) algorithms for KÖNIG VERTEX DELETION and VERTEX COVER parameterized by the size of the smallest odd cycle transversal and König vertex deletion set. These algorithms significantly improve the best known bounds for these problems. The most notable improvement among these is the new bound for ODD CYCLE TRANSVERSAL - this is the first algorithm that improves on the dependence on k of the seminal O∗(3k) algorithm of Reed, Smith, and Vetta. Finally, using our algorithm, we obtain a kernel for the standard parameterization of VERTEX COVER with at most 2k - c log k vertices. Our kernel is simpler than previously known kernels achieving the same size bound. © 2014 ACM.",Parameterized algorithms; Vertex cover,Linear programming; Parameterization; Branching algorithms; Linear programming relaxation; Odd cycle transversals; Optimal solutions; Parameterized algorithm; Parameterized complexity; Time complexity; Vertex cover; Parameter estimation
Approximating rooted Steiner networks,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908876255&doi=10.1145%2f2650183&partnerID=40&md5=20297cf3a81221d8b0eae9ffed622011,"The Directed Steiner Tree (DST) problem is a cornerstone problem in network design. We focus on the generalization of the problem with higher connectivity requirements. The problem with one root and two sinks is APX-hard. The problem with one root and many sinks is as hard to approximate as the directed Steiner forest problem, and the latter is well known to be as hard to approximate as the label cover problem. Utilizing previous techniques, we strengthen these results and extend them to undirected graphs. Specifically, we give an Ω(k∈) hardness bound for the rooted k-connectivity problem in undirected graphs. As a consequence, we obtain an Ω(k∈) hardness bound for the undirected subset k-connectivity problem. Additionally, we give a result on the integrality ratio of the natural linear programming relaxation of the directed rooted k-connectivity problem. © 2014 ACM.",Approximation algorithms; Graph connectivity; Hardness of approximation; Network design; Rooted connectivity,Graph algorithms; Hardness; Linear programming; Directed steiner trees; Graph connectivity; Hardness of approximation; Integrality ratio; Linear programming relaxation; Network design; Rooted connectivity; Steiner forest problem; Approximation algorithms
ACM Transactions on Algorithms: Editorial,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995584166&doi=10.1145%2f2675311&partnerID=40&md5=db7ea435085dfe5f378b34e1670298f2,[No abstract available],,
Union-find with constant time deletions,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907010271&doi=10.1145%2f2636922&partnerID=40&md5=228284132ad43a2c217a1e78e04480bf,"A union-find data structure maintains a collection of disjoint sets under the operations makeset, union, and find. Kaplan, Shafrir, and Tarjan [SODA 2002] designed data structures for an extension of the union-find problem in which items of the sets maintained may be deleted. The cost of a delete operation in their implementations is essentially the same as the cost of a find operation; namely, O(log n) worst-case and O(α[M/N](n)) amortized, where n is the number of items in the set returned by the find operation, N is the total number of makeset operations performed, M is the total number of find operations performed, and α[M/N](n) is a functional inverse of Ackermann's function. They left open the question whether delete operations can be implemented more efficiently than find operations, for example, in o(log n) worst-case time. We resolve this open problem by presenting a relatively simple modification of the classical union-find data structure that supports delete, as well as makeset and union operations, in constant worst-case time, while still supporting find operations in O(log n) worst-case time and O(α [M/N](n)) amortized time. Our analysis supplies, in particular, a very concise potential-based amortized analysis of the standard union-find data structure that yields an O(α [M/N] (n)) amortized bound on the cost of find operations. All previous potential-based analyses yielded the weaker amortized bound of O(α [M/N] (N)). Furthermore, our tighter analysis extends to one-path variants of the path compression technique such as path splitting. © 2014 ACM.",Disjoint sets; Union-find,Data structures; Inverse problems; Ackermann's functions; Amortized analysis; Disjoint sets; Functional inverse; Simple modifications; Union find; Union-find data structures; Union-find problems; Cost benefit analysis
Constraint solving via fractional edge covers,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907016415&doi=10.1145%2f2636918&partnerID=40&md5=7128e5e70e1679f5a0efc7bb34f01be4,"Many important combinatorial problems can be modeled as constraint satisfaction problems. Hence, identifying polynomial-time solvable classes of constraint satisfaction problems has received a lot of attention. In this article, we are interested in structural properties that can make the problem tractable. So far, the largest structural class that is known to be polynomial-time solvable is the class of bounded hypertree width instances introduced by Gottlob et al. [2002]. Here we identify a new class of polynomial-time solvable instances: those having bounded fractional edge cover number. Combining hypertree width and fractional edge cover number, we then introduce the notion of fractional hypertree width. We prove that constraint satisfaction problems with bounded fractional hypertree width can be solved in polynomial time (provided that the tree decomposition is given in the input). Together with a recent approximation algorithm for finding such decompositions [Marx 2010], it follows that bounded fractional hypertree width is now the most generally known structural property that guarantees polynomial-time solvability. © 2014 ACM.",Constraint satisfaction; Fractional edge covers; Hypergraphs; Hypertree width,Approximation algorithms; Polynomial approximation; Polynomials; Structural properties; Combinatorial problem; Constraint Satisfaction; Constraint Solving; Edge cover; Fractional hypertree widths; Hyper graph; Hypertree width; Tree decomposition; Constraint satisfaction problems
Better scalable algorithms for broadcast scheduling,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907014073&doi=10.1145%2f2636916&partnerID=40&md5=b65bf194b1a9171317044abeb9641368,"In the classical broadcast scheduling problem, there are n pages stored at a server, and requests for these pages arrive over time. Whenever a page is broadcast, it satisfies all outstanding requests for that page. The objective is to minimize average flow time of the requests. For any ε > 0, we give a (1 +ε)-speed O(1/ε3)- competitive online algorithm for broadcast scheduling. This improves over the recent breakthrough result of Im and Moseley [2010], where they obtained a (1 +/ε)-speed O(1//ε11)-competitive algorithm. Our algorithm and analysis are considerably simpler than Im and Moseley [2010]. More importantly, our techniques also extend to the general setting of nonuniform page sizes and dependent requests. This is the first scalable algorithm for broadcast scheduling with varying size pages and resolves the main open question from Im and Moseley [2010]. © 2014 ACM.",Broadcast scheduling; Online algorithms; Resource augmentation,Algorithms; Mathematical techniques; Average flows; Broadcast scheduling; Broadcast scheduling problems; Competitive algorithms; On-line algorithms; Page sizes; Resource augmentation; Scalable algorithms; Scheduling
Approximation algorithms for min-max generalization problems,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907016359&doi=10.1145%2f2636920&partnerID=40&md5=98fa3c8ba573647d1163d86932a937c1,"We provide improved approximation algorithms for the min-max generalization problems considered by Du, Eppstein, Goodrich, and Lueker [Du et al. 2009]. Generalization is widely used in privacy-preserving data mining and can also be viewed as a natural way of compressing a dataset. In min-max generalization problems, the input consists of data items with weights and a lower bound wlb, and the goal is to partition individual items into groups of weight at least wlb while minimizing the maximum weight of a group. The rules of legal partitioning are specific to a problem. Du et al. consider several problems in this vein: (1) partitioning a graph into connected subgraphs, (2) partitioning unstructured data into arbitrary classes, and (3) partitioning a two-dimensional array into contiguous rectangles (subarrays) that satisfy these weight requirements. We significantly improve approximation ratios for all the problems considered by Du et al. and provide additional motivation for these problems.Moreover, for the first problem, whereas Du et al. give approximation algorithms for specific graph families, namely, 3-connected and 4-connected planar graphs, no approximation algorithm that works for all graphs was known prior to this work. © 2014 ACM.",Bin covering; Generalization problems; K-anonymity; Rectangle tiling,Data handling; Data mining; Graph algorithms; Approximation ratios; Bin coverings; Connected subgraphs; Generalization problems; K-Anonymity; Privacy preserving data mining; Rectangle tiling; Two-dimensional arrays; Approximation algorithms
Gathering despite mischief,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907013078&doi=10.1145%2f2629656&partnerID=40&md5=fe26ba0340a1548c851b933ee176c768,"A team consisting of an unknown number of mobile agents, starting from different nodes of an unknown network, have to meet at the same node. Agents move in synchronous rounds. Each agent has a different label. Up to f of the agents are Byzantine. We consider two levels of Byzantine behavior. A strongly Byzantine agent can choose an arbitrary port when it moves and it can convey arbitrary information to other agents, while a weakly Byzantine agent can do the same, except changing its label. What is the minimum number of good agents that guarantees deterministic gathering of all of them, with termination? We solve exactly this Byzantine gathering problem in arbitrary networks for weakly Byzantine agents and give approximate solutions for strongly Byzantine agents, both when the size of the network is known and when it is unknown. It turns out that both the strength versus the weakness of Byzantine behavior and the knowledge of network size significantly impact the results. For weakly Byzantine agents, we show that any number of good agents permits solving the problem for networks of known size. If the size is unknown, then this minimum number is f + 2. More precisely, we show a deterministic polynomial algorithm that gathers all good agents in an arbitrary network, provided that there are at least f + 2 of them. We also provide a matching lower bound: we prove that if the number of good agents is at most f + 1, then they are not able to gather deterministically with termination in some networks. For strongly Byzantine agents, we give a lower bound of f + 1, even when the graph is known: we show that f good agents cannot gather deterministically in the presence of f Byzantine agents even in a ring of known size. On the positive side, we give deterministic gathering algorithms for at least 2 f + 1 good agents when the size of the network is known and for at least 4 f + 2 good agents when it is unknown. © 2014 ACM.",Byzantine agents; Graph; Rendezvous,Software agents; Approximate solution; Arbitrary information; Arbitrary networks; Gathering problem; Graph; Polynomial algorithm; Rendezvous; Unknown networks; Mobile agents
Annotations in data streams,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907015064&doi=10.1145%2f2636924&partnerID=40&md5=17d731c3ae876c8575b2910c5d9cd218,"The central goal of data stream algorithms is to process massive streams of data using sublinear storage space. Motivated by work in the database community on outsourcing database and data stream processing, we ask whether the space usage of such algorithms can be further reduced by enlisting a more powerful ""helper"" that can annotate the stream as it is read. We do not wish to blindly trust the helper, so we require that the algorithm be convinced of having computed a correct answer. We show upper bounds that achieve a nontrivial tradeoff between the amount of annotation used and the space required to verify it. We also prove lower bounds on such tradeoffs, often nearly matching the upper bounds, via notions related to Merlin-Arthur communication complexity. Our results cover the classic data stream problems of selection, frequency moments, and fundamental graph problems such as triangle-freeness and connectivity. Our work is also part of a growing trend-including recent studies of multipass streaming, read/write streams, and randomly ordered streams-of asking more complexity-theoretic questions about data stream processing. It is a recognition that, in addition to practical relevance, the data stream model raises many interesting theoretical questions in its own right. © 2014 ACM.",Annotations; Data streams; Interactive proof,Digital storage; Annotations; Communication complexity; Data stream algorithms; Data stream model; Data stream processing; Database community; Graph problems; Interactive proofs; Data streams
Distributed selfish load balancing on networks,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907017635&doi=10.1145%2f2629671&partnerID=40&md5=2e10682c42f43bb3d005ef7c660602d1,"We study distributed load balancing in networks with selfish agents. In the simplest model considered here, there are n identical machines represented by vertices in a network and m ⋙ n selfish agents that unilaterally decide to move from one vertex to another if this improves their experienced load. We present several protocols for concurrent migration that satisfy desirable properties such as being based only on local information and computation and the absence of global coordination or cooperation of agents. Our main contribution is to show rapid convergence of the resulting migration process to states that satisfy different stability or balance criteria. In particular, the convergence time to a Nash equilibrium is only logarithmic in mand polynomial in n, where the polynomial depends on the graph structure. In addition, we show reduced convergence times to approximate Nash equilibria. Finally, we extend our results to networks of machines with different speeds or to agents that have different weights and show similar results for convergence to approximate and exact Nash equilibria. © 2014 ACM.",Algorithmic game theory; Distributed computing; Load balancing; Randomized algorithms,Computation theory; Computer games; Distributed computer systems; Game theory; Graph structures; Resource allocation; Stability criteria; Algorithmic Game Theory; Distributed load balancing; Global coordination; Identical machines; Local information; Migration process; Randomized Algorithms; Rapid convergence; Balancing
Envy-free pricing in multi-item markets,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995646201&doi=10.1145%2f2567923&partnerID=40&md5=7ece440a8c58eb255941d48f57881986,"In this article, we study revenue maximizing envy-free pricing inmulti-item markets: There are m indivisible items with unit supply each and n potential buyers where each buyer is interested in acquiring one item. The goal is to determine allocations (a matching between buyers and items) and prices of all items to maximize total revenue given that all buyers are envy-free. We give a polynomial time algorithm to compute a revenue maximizing envy-free pricing when every buyer evaluates atmost two items at a positive valuation, by reducing it to an instance of weighted independent set in a perfect graph and applying the Strong Perfect Graph Theorem. We complement our result by showing that the problem becomes NP-hard if some buyers are interested in at least three items. © 2014 ACM.",Algorithmic pricing; Algorithms; Envy-freeness; G.2 [discrete mathematics]: combinatorial algorithms; Theory,Algorithms; Commerce; Costs; Graph theory; Polynomial approximation; Sales; Algorithmic pricing; Combinatorial algorithm; Envy-free pricing; Envy-freeness; Polynomial-time algorithms; Revenue maximizing; Theory; Weighted independent sets; Economics
Socially desirable approximations for Dodgson's voting rule,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995612379&doi=10.1145%2f2556950&partnerID=40&md5=017fd6ec3fc9890adccdf99f999fd9c0,"In 1876, Charles Lutwidge Dodgson suggested the intriguing voting rule that today bears his name. Although Dodgson's rule is one of the most well-studied voting rules, it suffers from serious deficiencies, both from the computational point of view - it is NP-hard even to approximate the Dodgson score within sublogarithmic factors - and from the social choice point of view - it fails basic social choice desiderata such as monotonicity and homogeneity. However, this does not preclude the existence of approximation algorithms for Dodgson that are monotonic or homogeneous, and indeed it is natural to ask whether such algorithms exist. In this article, we give definitive answers to these questions. We design a monotonic exponential-time algorithm that yields a 2-approximation to the Dodgson score, while matching this result with a tight lower bound. We also present a monotonic polynomial-time O(logm)-approximation algorithm (where m is the number of alternatives); this result is tight as well due to a complexity-theoretic lower bound. Furthermore, we show that a slight variation on a known voting rule yields a monotonic, homogeneous, polynomial-time O(mlogm)-approximation algorithm and establish that it is impossible to achieve a better approximation ratio even if one just asks for homogeneity. We complete the picture by studying several additional social choice properties; for these properties, we prove that algorithms with an approximation ratio that depends only on mdo not exist. © 2014 ACM.",Algorithms; Approximation algorithms; Dodgson's voting rule; Economics; F.2 [theory of computation]: analysis of algorithms and problem complexity; J.4 [computer applications]: social and behavioral sciences - economics; Social choice; Theory,Algorithms; Behavioral research; Computational complexity; Economics; Image matching; Polynomial approximation; Analysis of algorithms; Dodgson's voting rules; Social and behavioral science; Social choice; Theory; Approximation algorithms
Weighted popular matchings,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995588398&doi=10.1145%2f2556951&partnerID=40&md5=e9067807165d454b63d7f47d058e8407,"We study the problem of assigning jobs to applicants. Each applicant has a weight and provides a preference list, which may contain ties, ranking a subset of the jobs. An applicant x may prefer one matching to another (or be indifferent between them, in case of a tie) based on the jobs x gets in the two matchings and x's personal preference. A matching M is popular if there is no other matching M′ such that the weight of the applicants who prefer M′ to M exceeds the weight of those who prefer M to M′. We present algorithms to find a popular matching, or if none exists, to establish so. For instances with strict preference lists, we give an O(n + m) time algorithm. For preference lists with ties, we give a more involved algorithm that solves the problem in O(min(k√n, n)m) time, where k is the number of distinct weights the applicants are given. © 2014 ACM.",Algorithm design and analysis; Bipartite matching; G.2.2 [discrete mathematics]: graph theory; Graph algorithms; Popular matching,Trees (mathematics); Algorithm design and analysis; Bipartite matchings; G.2.2 [discrete mathematics]: graph theories; Graph algorithms; Popular matching; Graph theory
Dynamic programming for graphs on surfaces,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894174879&doi=10.1145%2f2556952&partnerID=40&md5=8cc84d8a6aea0251368345564e65bd73,"We provide a framework for the design and analysis of dynamic programming algorithms for surfaceembedded graphs on n vertices and branchwidth at most k. Our technique applies to general families of problems where standard dynamic programming runs in 2O(k⋯log k) ⋯ n steps. Our approach combines tools from topological graph theory and analytic combinatorics. In particular, we introduce a new type of branch decomposition called surface cut decomposition, generalizing sphere cut decompositions of planar graphs, which has nice combinatorial properties. Namely, the number of partial solutions that can be arranged on a surface cut decomposition can be upper-bounded by the number of noncrossing partitions on surfaces with boundary. It follows that partial solutions can be represented by a single-exponential (in the branchwidth k) number of configurations. This proves that, when applied on surface cut decompositions, dynamic programming runs in 2O(k) ⋯ n steps. That way, we considerably extend the class of problems that can be solved in running times with a single-exponential dependence on branchwidth and unify/improvemost previous results in this direction. © 2014 ACM.",Analysis of algorithms; Branchwidth; Dynamic programming; Graphs on surfaces; Noncrossing partitions; Parameterized algorithms; Polyhedral embeddings,
Geometric optimization and sums of algebraic functions,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995543480&doi=10.1145%2f2532647&partnerID=40&md5=74f2976e477136cebeb0a7f0c1e1814a,"We present a new optimization technique that yields the first FPTAS for several geometric problems. These problems reduce to optimizing a sum of nonnegative, constant description complexity algebraic functions. We first give an FPTAS for optimizing such a sum of algebraic functions, and then we apply it to several geometric optimization problems.We obtain the first FPTAS for two fundamental geometric shape-matching problems in fixed dimension: maximizing the volume of overlap of two polyhedra under rigid motions and minimizing their symmetric difference. We obtain the first FPTAS for other problems in fixed dimension, such as computing an optimal ray in a weighted subdivision, finding the largest axially symmetric subset of a polyhedron, and computing minimum-area hulls. © 2014 ACM.",Algorithms; Approximation algorithms; F.2.2 [analysis of algorithms and problem complexity]: nonnumerical algorithms and problems - geometrical problems computations; Geometric optimization; I.3.5 [computing methodologies]: computer graphics - geometric algorithms languages systems; Theory,Algebra; Algorithms; Approximation algorithms; Computational complexity; Computer graphics; Functions; Optimization; Algebraic functions; Description complexity; Geometric algorithm; Geometric optimization; Optimization techniques; Problem complexity; Symmetric difference; Theory; Geometry
The Fréchet distance revisited and extended,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995677049&doi=10.1145%2f2532646&partnerID=40&md5=1e288d59be67a9c2af1ff51c207d0461,"Given two simplicial complexes in IRd and start and end vertices in each complex, we show how to compute curves (in each complex) between these vertices, such that the weak Fréchet distance between these curves is minimized. As a polygonal curve is a complex, this generalizes the regular notion of weak Fréchet distance between curves. We also generalize the algorithm to handle an input of k simplicial complexes. Using this new algorithm, we can solve a slew of new problems, from computing a mean curve for a given collection of curves to various motion planning problems. Additionally, we show that for the mean curve problem, when the k input curves are c-packed, one can (1 + ε)-approximate the mean curve in near-linear time, for fixed k and ε. Additionally, we present an algorithm for computing the strong Fréchet distance between two curves, which is simpler than previous algorithms and avoids using parametric search. © 2014 ACM.",Algorithms; Approximation algorithms; F.2.2 [analysis of algorithms and problem complexity]: nonnumerical algorithms and problems - geometrical problems and computations; Fréchet distance; Realistic input models; Theory,Algorithms; Computational complexity; Motion planning; Input curve; Motion planning problems; Near-linear time; Polygonal curve; Problem complexity; Realistic input models; Simplicial complex; Theory; Approximation algorithms
Race to idle: New algorithms for speed scaling with a sleep state,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894150136&doi=10.1145%2f2556953&partnerID=40&md5=2e34a58d1f5a8cc8dd4e78c208690923,"We study an energy conservation problem where a variable-speed processor is equipped with a sleep state. Executing jobs at high speeds and then setting the processor asleep is an approach that can lead to further energy savings compared to standard dynamic speed scaling. We consider classical deadline-based scheduling, that is, each job is specified by a release time, a deadline and a processing volume. For general convex power functions, Irani et al. [2007] devised an offline 2-approximation algorithm. Roughly speaking, the algorithm schedules jobs at a critical speed scrit that yields the smallest energy consumption while jobs are processed. For power functions P(s) = sα + γ, where s is the processor speed, Han et al. [2010] gave an (αα + 2)-competitive online algorithm. We investigate the offline setting of speed scaling with a sleep state. First, we prove NP-hardness of the optimization problem. Additionally, we develop lower bounds, for general convex power functions: No algorithm that constructs scrit-schedules, which execute jobs at speeds of at least s crit, can achieve an approximation factor smaller than 2. Furthermore, no algorithm that minimizes the energy expended for processing jobs can attain an approximation ratio smaller than 2. We then present an algorithmic framework for designing good approximation algorithms. For general convex power functions, we derive an approximation factor of 4/3. For power functions P(s) = βsα + γ, we obtain an approximation of 137/117 < 1.171. We finally show that our framework yields the best approximation guarantees for the class of scrit-schedules. For general convex power functions, we give another 2-approximation algorithm. For functions P(s) = βsα + γ, we present tight upper and lower bounds on the best possible approximation factor. The ratio is exactly eW-1(-e-1-1/e)/(eW-1(-e-1-1/e)+1) < 1.211, where W-1 is the lower branch of the Lambert W function. © 2014 ACM.",Approximation algorithm; NP-hard; Power-down mechanism; Variable-speed processor,
Price-based protocols for fair resource allocation: Convergence time analysis and extension to Leontief utilities,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894160307&doi=10.1145%2f2556949&partnerID=40&md5=c4aee6f658a34ecd035d57fd55120a25,"We analyze several distributed, continuous time protocols for a fair allocation of bandwidths to flows in a network (or resources to agents). Our protocols converge to an allocation that is a logarithmic approximation, simultaneously, to all canonical social welfare functions (i.e., functions that are symmetric, concave, and nondecreasing). These protocols can be started in an arbitrary state. Although a similar protocol was known before, it only applied to the simple bandwidth allocation problem, and its stability and convergence time were not understood. In contrast, our protocols also apply to the more general case of Leontief utilities, where each user may place a different requirement on each resource. Furthermore, we prove that our protocols converge in polynomial time. The best convergence time we prove is O(n log ncMAX aMAX/cMINaMIN), where n is the number of agents in the network, cMAX and cMIN are the maximum and minimum capacity of the links, and amax, amin are respectively the largest and smallest Leontief coefficients. This time is achieved by a simple Multiplicative Increase, Multiplicative Decrease (MIMD) protocol that had not been studied before in this setting. We also identify combinatorial properties of these protocols that may be useful in proving stronger convergence bounds. The final allocations by our protocols are supported by usage-sensitive dual prices that are fair in the sense that they shield light users of a resource from the impact of heavy users. Thus, our protocols can also be thought of as efficient distributed schemes for computing fair prices. © 2014 ACM.",Decentralized algorithms; Fairness; Leontief utilities; Majorization; Network protocols; Shadow prices,
Alphabet-independent compressed text indexing,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906858426&doi=10.1145%2f2635816&partnerID=40&md5=3e23a203db3d33b4986f759e7cff6b77,"Self-indexes are able to represent a text asymptotically within the information-theoretic lower bound under the kth order entropy model and offer access to any text substring and indexed pattern searches. Their time complexities are not optimal, however; in particular, they are always multiplied by a factor that depends on the alphabet size. In this article, we achieve, for the first time, full alphabet independence in the time complexities of self-indexes while retaining space optimality. We also obtain some relevant byproducts. © 2014 ACM.",Compression; Succinct data structures; Suffix trees; Text indexing,Byproducts; Compaction; Information theory; Trees (mathematics); Compressed text indexing; Information-theoretic lower bounds; Pattern search; Space-optimality; Succinct data structure; Suffix-trees; Text-indexing; Time complexity; Indexing (of information)
Real-time streaming string-matching,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906873782&doi=10.1145%2f2635814&partnerID=40&md5=2abe32465245ee0e2577e21e41abf8e0,"This article presents a real-time randomized streaming string-matching algorithm that uses O(logm) space. The algorithm only makes one-sided small probability false-positive errors, possibly reporting phantom occurrences of the pattern, but never missing an actual occurrence. © 2014 ACM.",Randomized algorithms; Streaming algorithms; String matching,Mathematical techniques; Randomized Algorithms; Real time streaming; Streaming algorithm; String matching; Algorithms
Exponential time complexity of the permanent and the Tutte polynomial,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906849708&doi=10.1145%2f2635812&partnerID=40&md5=6b818071929e9db5ab5e4bafaeb3ce0e,"We show conditional lower bounds for well-studied #P-hard problems: -The number of satisfying assignments of a 2-CNF formula with n variables cannot be computed in time exp(o(n)), and the same is true for computing the number of all independent sets in an n-vertex graph. -The permanent of an n× n matrix with entries 0 and 1 cannot be computed in time exp(o(n)). -The Tutte polynomial of an n-vertex multigraph cannot be computed in time exp(o(n)) at most evaluation points (x, y) in the case of multigraphs, and it cannot be computed in time exp(o(n/ poly log n)) in the case of simple graphs. Our lower bounds are relative to (variants of) the Exponential Time Hypothesis (ETH), which says that the satisfiability of n-variable 3-CNF formulas cannot be decided in time exp(o(n)). We relax this hypothesis by introducing its counting version #ETH; namely, that the satisfying assignments cannot be counted in time exp(o(n)). In order to use #ETH for our lower bounds, we transfer the sparsification lemma for d-CNF formulas to the counting setting. © 2014 ACM.",Computational complexity; Counting problems; Exponential time hypothesis; Permanent; Tutte polynomial,Computational complexity; Directed graphs; Counting problems; Exponential time complexity; Exponential time hypothesis; Independent set; Permanent; Satisfying assignments; Sparsification lemmata; Tutte polynomial; Polynomials
Compression via matroids: A randomized polynomial kernel for odd cycle transversal,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906890693&doi=10.1145%2f2635810&partnerID=40&md5=f0e139e2a7b78a1365e8ec7946275b96,"The Odd Cycle Transversal problem (OCT) asks whether a given undirected graph can be made bipartite by deleting at most k of its vertices. In a breakthrough result, Reed, Smith, and Vetta (Operations Research Letters, 2004) gave a O(4kkmn) time algorithm for it; this also implies that instances of the problem can be reduced to a so-called problem kernel of size O(4k). Since then, the existence of a polynomial kernel for OCT (i.e., a kernelization with size bounded polynomially in k) has turned into one of the main open questions in the study of kernelization, open even for the special case of planar input graphs. This work provides the first (randomized) polynomial kernelization for OCT. We introduce a novel kernelization approach based on matroid theory, where we encode all relevant information about a problem instance into a matroid with a representation of size polynomial in k. This represents the first application of matroid theory to kernelization. © 2014 ACM.",Kernelization; Matroids; Odd cycle transversal; Parameterized complexity,Matrix algebra; Operations research; Polynomials; Kernelization; Odd cycle transversals; Parameterized complexity; Polynomial kernels; Problem instances; Problem kernel; Time algorithms; Undirected graph; Combinatorial mathematics
Principles of robust medium access and an application to leader election,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906854664&doi=10.1145%2f2635818&partnerID=40&md5=cf57332dadfab3eb96af70d5fd6229a1,"This article studies the design of medium access control (MAC) protocols for wireless networks that are provably robust against arbitrary and unpredictable disruptions (e.g., due to unintentional external interference from co-existing networks or due to jamming). We consider a wireless network consisting of a set of n honest and reliable nodes within transmission (and interference) range of each other, and we model the external disruptions with a powerful adaptive adversary. This adversary may know the protocol and its entire history and can use this knowledge to jam the wireless channel at will at any time. It is allowed to jam a (1 ? ∈)-fraction of the timesteps, for an arbitrary constant isin; > 0 unknown to the nodes. The nodes cannot distinguish between the adversarial jamming or a collision of two or more messages that are sent at the same time. We demonstrate, for the first time, that there is a local-control MAC protocol requiring only very limited knowledge about the adversary and the network that achieves a constant (asymptotically optimal) throughput for the nonjammed time periods under any of the aforementioned adversarial strategies. The derived principles are also useful to build robust applications on top of the MAC layer, and we present an exemplary study for leader election, one of the most fundamental tasks in distributed computing. © 2014 ACM.",Jamming; MAC protocols; Wireless ad-hoc networks,Bit error rate; Jamming; Wireless ad hoc networks; Wireless networks; Adaptive adversary; Arbitrary constants; Asymptotically optimal; External interference; MAC protocol; Medium access control protocols; Robust application; Wireless channel; Medium access control
Co-nondeterminism in compositions: A kernelization lower bound for a ramsey-type problem,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906489123&doi=10.1145%2f2635808&partnerID=40&md5=ad2adfbc8cb70dbacb511e1b8317c54d,"The field of kernelization offers a rigorous way of studying the ubiquitous technique of data reduction and preprocessing for combinatorially hard problems. A widely accepted definition of useful data reduction is that of a polynomial kernelization where the output instance is guaranteed to be of size polynomial in some parameter of the input. The fairly recent development of a framework for kernelization lower bounds has made this notion even more attractive as we can now classify many problems into admitting or not admitting polynomial kernelizations. The central notion of the framework is that of a polynomial-time composition algorithm due to Bodlaender et al. (ICALP 2008, JSCC 2009): given t input instances, an OR-composition algorithm returns a single-output instance with bounded parameter value that is yes if and only if one of t input instances is yes; it encodes the logical OR of the input instances. Based on a result of Fortnow and Santhanam (STOC 2008, JSCC 2011), Bodlaender et al. show that an OR-composition for an NP-hard problem rules out polynomial kernelizations for it unless NP ⊆ coNP/poly (which is known to imply a collapse of the polynomial hierarchy). It is implicit in the work of Fortnow and Santhanam that even co-nondeterministic composition algorithms suffice to rule out polynomial kernelizations. This was first observed in unpublished work of Chen and Müller, and it is an explicit conclusion of recent results by Dell and van Melkebeek (STOC 2010). However, in contrast to the numerous applications of deterministic composition, the added power of co-nondeterminism has not yet been harnessed to obtain kernelization lower bounds. In this work, we present the first example of how co-nondeterminism can help to make a composition algorithm. We study the existence of polynomial kernels for a Ramsey-type problem where, given a graph G and an integer k, the question is whether G contains an independent set or a clique of size at least k. It was asked by Rod Downey whether this problem admits a polynomial kernelization with respect to k; such a result would greatly speed up the computation of Ramsey numbers.We provide a co-nondeterministic composition based on embedding t instances into a single host graph H. The crux is that the host graph H needs to observe a bound of ℓ ∈ O(log t) on both its maximum independent set and maximum clique size, while also having a cover of its vertex set by independent sets and cliques all of size ℓ; the co-nondeterministic composition is built around the search for such graphs. Thus, we show that, unless NP ? coNP/poly, the problem does not admit a kernelization with polynomial size guarantee. © 2014 ACM.",Kernelization; Nondeterminism; Parameterized complexity; Ramsey numbers,Algorithms; Computational complexity; Data reduction; Graph theory; Bounded parameters; Kernelization; Maximum independent sets; Non-determinism; Parameterized complexity; Polynomial hierarchies; Polynomial kernels; Ramsey numbers; Polynomials
Maximum quadratic assignment problem: Reduction from maximum label cover and LP-based approximation algorithm,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906852643&doi=10.1145%2f2629672&partnerID=40&md5=55a6b45b586d5ccd13a2700e44a105f2,"We show that for every positive ε > 0, unless NP ∪ BPQP, it is impossible to approximate the maximum quadratic assignment problem within a factor better than 2log1?ε n by a reduction from the maximum label cover problem. Our result also implies that Approximate Graph Isomorphism is not robust and is, in fact, 1 ? ε versus ε hard assuming the Unique Games Conjecture. Then, we present an O(√n)- approximation algorithm for the problem based on rounding of the linear programming relaxation often used in state-of-the-art exact algorithms. © 2014 ACM.",Inapproximability; Quadratic assignment problem,Approximation algorithms; Linear programming; Cover problem; Exact algorithms; Graph isomorphism; Inapproximability; Linear programming relaxation; Problem-based; Quadratic assignment problems; Unique games conjecture; Combinatorial optimization
Testing properties of sparse images,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906855633&doi=10.1145%2f2635806&partnerID=40&md5=f3ba137413ab7fbaf95dffdd92fd80b9,"We initiate the study of testing properties of images that correspond to sparse 0/1-valued matrices of size n × n. Our study is related to but different from the study initiated by Raskhodnikova (Proceedings of RANDOM, 2003), where the images correspond to dense 0/1-valued matrices. Specifically, in the model studied by Raskhodnikova, the distance that an image has to a specific property is the number of entries that should be modified in the corresponding matrix so that the property can be obtained, divided by the total number of entries: n2. In the model we consider, the distance is the number of entries that should be modified divided by the actual number of 1's in the matrix, which may be much smaller than n2. We study several natural properties: connectivity, convexity, monotonicity, and being a line. In all cases, we give testing algorithms with sublinear complexity, and, in some of the cases, we also provide corresponding lower bounds. © 2014 ACM.",Image processing; Property testing,Image processing; Lower bounds; Monotonicity; Natural properties; Property-testing; Sparse images; Specific properties; Sublinear; Testing algorithm; Matrix algebra
Approximate privacy: Foundations and quantification,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904813858&doi=10.1145%2f2601067&partnerID=40&md5=729daa2503179afe9d42dc7a1f27d726,"The proliferation of online sensitive data about individuals and organizations makes concern about the privacy of these data a top priority. There have been many formulations of privacy and, unfortunately, many negative results about the feasibility of maintaining privacy of sensitive data in realistic networked environments. We formulate communication-complexity-based definitions, both worst case and average case, of a problem's privacy-approximation ratio.We use our definitions to investigate the extent to which approximate privacy is achievable in a number of standard problems: the 2nd-price Vickrey auction, Yao's millionaires problem, the public-good problem, and the set-theoretic disjointness and intersection problems. For both the 2nd-price Vickrey auction and the millionaires problem, we show that not only is perfect privacy impossible or infeasibly costly to achieve, but even close approximations of perfect privacy suffer from the same lower bounds. By contrast, if the inputs are drawn uniformly at random from {0, ⋯ , 2 k - 1}, then, for both problems, simple and natural communication protocols have privacy-approximation ratios that are linear in k (i.e., logarithmic in the size of the input space). We also demonstrate tradeoffs between privacy and communication in a family of auction protocols. We show that the privacy-approximation ratio provided by any protocol for the disjointness and intersection problems is necessarily exponential (in k). We also use these ratios to argue that one protocol for each of these problems is significantly fairer than the others we consider (in the sense of relative effects on the privacy of the different players). © 2014 ACM.",Approximate privacy; Bisection auction,Algorithms; Mathematical techniques; Auction protocols; Bisection auction; Close approximation; Intersection problem; Millionaires problem; Networked environments; Relative effects; Standard problems; Data privacy
Fully functional static and dynamic succinct trees,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904756682&doi=10.1145%2f2601073&partnerID=40&md5=131b7f00cec10e3729fd13695477991b,"We propose new succinct representations of ordinal trees and match various space/time lower bounds. It is known that any n-node static tree can be represented in 2n + o(n) bits so that a number of operations on the tree can be supported in constant time under the word-RAM model. However, the data structures are complicated and difficult to dynamize. We propose a simple and flexible data structure, called the range min-max tree, that reduces the large number of relevant tree operations considered in the literature to a few primitives that are carried out in constant time on polylog-sized trees. The result is extended to trees of arbitrary size, retaining constant time and reaching 2n+O(n/polylog(n)) bits of space. This space is optimal for a core subset of the operations supported and significantly lower than in any previous proposal. For the dynamic case, where insertion/deletion (indels) of nodes is allowed, the existing data structures support a very limited set of operations. Our data structure builds on the range min-max tree to achieve 2n+O(n/ log n) bits of space and O(log n) time for all operations supported in the static scenario, plus indels. We also propose an improved data structure using 2n+ O(nlog log n/ log n) bits and improving the time to the optimal O(log n/ log log n) for most operations. We extend our support to forests, where whole subtrees can be attached to or detached from others, in time O(log1+ε n) for any ε > 0. Such operations had not been considered before. Our techniques are of independent interest. An immediate derivation yields an improved solution to range minimum/maximum queries where consecutive elements differ by ±1, achieving n+ O(n/polylog(n)) bits of space. A second one stores an array of numbers supporting operations sum and search and limited updates, in optimal time O(log n/ log log n). A third one allows representing dynamic bitmaps and sequences over alphabets of size σ, supporting rank/select and indels, within zero-order entropy bounds and time O(log nlog σ/(log log n)2) for all operations. This time is the optimal O(log n/ log log n) on bitmaps and polylogsized alphabets. This improves upon the best existing bounds for entropy-bounded storage of dynamic sequences, compressed full-text self-indexes, and compressed-space construction of the Burrows-Wheeler transform. © 2014 ACM.",Compressed sequence representations; Compressed text databases; Succinct tree representations,Data; Forestry; Structures; Data structures; Random access storage; Burrows Wheeler transform; Compressed sequence representations; Dynamic sequences; Static scenarios; Succinct representation; Text database; Tree representation; Zero-order entropy; Forestry
Algebraic algorithms for linear matroid parity problems,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904797433&doi=10.1145%2f2601066&partnerID=40&md5=755970cdb0510dcc027012b700da9985,"We present fast and simple algebraic algorithms for the linear matroid parity problem and its applications. For the linearmatroid parity problem, we obtain a simple randomized algorithm with running time O(mrω-1) , where mand r are the number of columns and the number of rows, respectively, and ω ≈ 2.3727 is the matrix multiplication exponent. This improves the O(mrω)-time algorithm by Gabow and Stallmann and matches the running time of the algebraic algorithm for linear matroid intersection, answering a question of Harvey. We also present a very simple alternative algorithm with running time O(mr2), which does not need fast matrix multiplication. We further improve the algebraic algorithms for some specific graph problems of interest. For the Mader's disjoint S-path problem, we present an O(nω)-time randomized algorithm where n is the number of vertices. This improves the running time of the existing results considerably and matches the running time of the algebraic algorithms for graph matching. For the graphic matroid parity problem, we give an O(n4)-time randomized algorithm where n is the number of vertices, and an O(n3)-time randomized algorithm for a special case useful in designing approximation algorithms. These algorithms are optimal in terms of n as the input size could be Σ(n4) and Σ(n3), respectively. The techniques are based on the algebraic algorithmic framework developed by Mucha and Sankowski, Harvey, and Sankowski. While linear matroid parity and Mader's disjoint S-path are challenging generalizations for the design of combinatorial algorithms, our results show that both the algebraic algorithms for linear matroid intersection and graph matching can be extended nicely to more general settings. All algorithms are still faster than the existing algorithms even if fast matrix multiplication is not used. These provide simple algorithms that can be easily implemented in practice. © 2014 ACM.",Matroid parity,Algebra; Approximation algorithms; Combinatorial mathematics; Matrix algebra; Algorithmic framework; Alternative algorithms; Combinatorial algorithm; Fast matrix multiplication; MAtrix multiplication; Matroid intersection; Matroid parity; Randomized Algorithms; Pattern matching
Fast convergence for consensus in dynamic networks,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904790767&doi=10.1145%2f2601072&partnerID=40&md5=87c980ff6722571b2ef9ab1dccbb3336,"In this article, we study the convergence time required to achieve consensus in dynamic networks. In each timestep, a node's value is updated to some weighted average of its neighbors and its old values. We study the case when the underlying network is dynamic and investigate different averaging models. Both our analysis and experiments show that dynamic networks exhibit fast convergence behavior, even under very mild connectivity assumptions. © 2014 ACM.",Consensus problem; Convergent behavior; Dynamic networks; Natural algorithms; Weighted averaging model,Algorithms; Mathematical techniques; Consensus problems; Convergence time; Convergent behavior; Dynamic network; Fast convergence; Underlying networks; Weighted averages; Weighted averaging; Statistical methods
Node-weighted steiner tree and group steiner tree in planar graphs,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904813130&doi=10.1145%2f2601070&partnerID=40&md5=799e53ce7a6862e4b9c56a6d5cafd7a4,"We improve the approximation ratios for two optimization problems in planar graphs. For node-weighted Steiner tree, a classical network-optimization problem, the best achievable approximation ratio in general graphs is Θ(log n), and nothing better was previously known for planar graphs. We give a constant-factor approximation for planar graphs. Our algorithm generalizes to allow as input any nontrivial minor-closed graph family, and also generalizes to address other optimization problems such as Steiner forest, prizecollecting Steiner tree, and network-formation games. The second problem we address is group Steiner tree: given a graph with edge weights and a collection of groups (subsets of nodes), find a minimum-weight connected subgraph that includes at least one node from each group. The best approximation ratio known in general graphs is O(log3 n), or O(log2 n) when the host graph is a tree. We obtain an O(log npolyloglog n) approximation algorithm for the special case where the graph is planar embedded and each group is the set of nodes on a face. We obtain the same approximation ratio for the minimum-weight tour that must visit each group. © 2014 ACM.",Approximation algorithms; Network design; Node weights; Planarity,Algorithms; Forestry; Optimization; Approximation algorithms; Forestry; Graphic methods; Optimization; Approximation ratios; Best approximations; Constant factor approximation; Group Steiner tree; Network design; Node weights; Optimization problems; Planarity; Graph theory
Faster algorithms for semi-matching problems,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904787377&doi=10.1145%2f2601071&partnerID=40&md5=2eed033a25b90dc864ce18197ecea744,"We consider the problem of finding semi-matching in bipartite graphs, which is also extensively studied under various names in the scheduling literature. We give faster algorithms for both weighted and unweighted cases. For the weighted case, we give an O(nmlog n)-time algorithm, where n is the number of vertices and m is the number of edges, by exploiting the geometric structure of the problem. This improves the classical O(n3)-time algorithms by Horn [1973] and Bruno et al. [1974b]. For the unweighted case, the bound can be improved even further. We give a simple divide-and-conquer algorithm that runs in O(√nmlog n) time, improving two previous O(nm)-time algorithms by Abraham [2003] and Harvey et al. [2003, 2006]. We also extend this algorithm to solve the Balanced Edge Cover problem in O(√nmlog n) time, improving the previous O(nm)-time algorithm by Harada et al. [2008]. © 2014 ACM.",Combinatorial optimization; Design and analysis of algorithm; Scheduling; Semi-matching,Combinatorial optimization; Scheduling; Bipartite graphs; Design and analysis of algorithms; Divide-and-conquer algorithm; Edge cover; Geometric structure; Semi-matching; Time algorithms; Algorithms
"Deterministic rendezvous, treasure hunts, and strongly universal exploration sequences",2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904805520&doi=10.1145%2f2601068&partnerID=40&md5=69f0aeda45a19b6a16d26e9dcdab8eea,"We obtain several improved solutions for the deterministic rendezvous problem in general undirected graphs. Our solutions answer several problems left open by Dessmark et al.We also introduce an interesting variant of the rendezvous problem, which we call the deterministic treasure hunt problem. Both the rendezvous and the treasure hunt problems motivate the study of universal traversal sequences and universal exploration sequences with some strengthened properties. We call such sequences strongly universal traversal (exploration) sequences. We give an explicit construction of strongly universal exploration sequences. The existence of strongly universal traversal sequences, as well as the solution of the most difficult variant of the deterministic treasure hunt problem, are left as intriguing open problems. © 2014 ACM.",Mobile robots; Rendezvous; Treasure hunt; Universal exploration sequences; Universal traversal sequences,Algorithms; Mobile robots; Explicit constructions; Exploration sequences; Rendezvous; Rendezvous problems; Traversal sequences; Treasure hunt; Undirected graph; Mathematical techniques
A logarithmic approximation for unsplittable flow on line graphs,2014,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893932066&doi=10.1145%2f2532645&partnerID=40&md5=0e68aefe4b11f7cbace65bdba636c806,"We consider the unsplittable flow problem on a line. In this problem, we are given a set of n tasks, each specified by a start time si, an end time ti, a demand di > 0, and a profit pi > 0. A task, if accepted, requires di units of ""bandwidth"" from time si to ti and accrues a profit ofpi. For every time t, we are also specified the available bandwidth ct, and the goal is to find a subset of tasks with maximum profit subject to the bandwidth constraints. We present the first polynomial time O(log n) approximation algorithm for this problem. This significantly advances the state of the art, as no polynomial time o(n) approximation was known previously. Previous results for this problem were known only in more restrictive settings; in particular, either the instance satisfies the so-called ""no- bottleneck"" assumption: maxi di ≤ mint ct, or the ratio of both maximum to minimum demands and maximum to minimum capacities are polynomially (or quasi-polynomially) bounded in n. Our result, on the other hand, does not require these assumptions. Our algorithm is based on a combination of dynamic programming and rounding a natural linear programming relaxation for the problem. While there is an Ω (n) integrality gap known for this LP relaxation, our key idea is to exploit certain structural properties of the problem to show that instances that are bad for the LP can in fact be handled using dynamic programming. © 2014 ACM.",,
Approximation algorithms for a minimization variant of the order-preserving submatrices and for biclustering problems,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885653200&doi=10.1145%2f2438645.2438651&partnerID=40&md5=7b078d805717f5d290c1966c68248099,"Finding a largest Order-Preserving SubMatrix, OPSM, is an important problem arising in the discovery of patterns in gene expression. Ben-Dor et al. formulated the problem in Ben-Dor et al. [2003]. They further showed that the problem is NP-complete and provided a greedy heuristic for the problem. The complement of the OPSM problem, called MinOPSM, is to delete the least number of entries in the matrix so that the remaining submatrix is order preserving. We devise a 5-approximation algorithm for the MinOPSM based on a formulation of the problem as a quadratic, nonseparable set cover problem. An alternative formulation combined with a primal-dual algorithm improves the approximation factor to 3. The complexity of both algorithms for a matrix of size m×n is O(m2n). We further comment on the related biclustering problem. © 2013 ACM.",Approximation algorithm; Biclustering; Gene expression; Order preserving,Gene expression; Approximation factor; Bi-clustering; Greedy heuristics; Nonseparable; Order preserving; Order-preserving submatrix; Primal dual algorithms; Set cover problem; Approximation algorithms
Submodular secretary problem and extensions,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885661908&doi=10.1145%2f2500121&partnerID=40&md5=3dbc6ce7f8279383f587c90d8595fb8b,"Online auction is the essence ofmanymodernmarkets, particularly networkedmarkets, inwhich information about goods, agents, and outcomes is revealed over a period of time, and the agents must make irrevocable decisions without knowing future information. Optimal stopping theory, especially the classic secretary problem, is a powerful tool for analyzing such online scenarios which generally require optimizing an objective function over the input. The secretary problem and its generalization the multiple-choice secretary problem were under a thorough study in the literature. In this article, we consider a very general setting of the latter problem called the submodular secretary problem, in which the goal is to select k secretaries so as to maximize the expectation of a (not necessarily monotone) submodular function which defines efficiency of the selected secretarial group based on their overlapping skills. We present the first constant-competitive algorithm for this case. In a more general setting in which selected secretaries should form an independent (feasible) set in each of l given matroids as well, we obtain an O(l log2 r)-competitive algorithm generalizing several previous results, where r is the maximum rank of the matroids. Another generalization is to consider l knapsack constraints (i.e., a knapsack constraint assigns a nonnegative cost to each secretary, and requires that the total cost of all the secretaries employed be no more than a budget value) instead of the matroid constraints, for which we present an O(l)-competitive algorithm. In a sharp contrast, we show for a more general setting of subadditive secretary problem, there is no õ( √ n)-competitive algorithm and thus submodular functions are the most general functions to consider for constant- competitiveness in our setting. We complement this result by giving a matching O( √ n)-competitive algorithm for the subadditive case. At the end, we consider some special cases of our general setting as well. © 2013 ACM.",Competitive algorithms; Knapsack constraint; Matroid constraint; Online algorithms; Secretary problem; Stopping theory; Submodular,Combinatorial mathematics; Combinatorial optimization; Constraint theory; Optimization; Competitive algorithms; Knapsack constraints; Matroid constraint; On-line algorithms; Secretary problem; Stopping theory; Submodular; Algorithms
An O(log n)-approximation algorithm for the edge-disjoint paths problem in Eulerian planar graphs,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885642824&doi=10.1145%2f2438645.2438648&partnerID=40&md5=7722db7b78edaafb9dd39c08fd1fc504,"In this article, we study an approximation algorithm for the maximum edge-disjoint paths problem. In this problem, we are given a graph and a collection of pairs of vertices, and the objective is to find the maximum number of pairs that can be connected by edge-disjoint paths. We give an O(log n)-approximation algorithm for the maximum edge-disjoint paths problem when an input graph is either 4-edge-connected planar or Eulerian planar. This improves an O(log2 n)-approximation algorithm given by Kleinberg [2005] for Eulerian planar graphs. Our result also generalizes the result by Chekuri et al. [2004, 2005] who gave an O(log n)-approximation algorithm for the maximum edge-disjoint paths problem with congestion two when an input graph is planar. © 2013 ACM.",4-edgeconnected graph; Approximation algorithm; Edge-disjoint paths; Eulerian graph,Approximation algorithms; Graphic methods; 4-edgeconnected graph; Edge-disjoint paths; Eulerian; Eulerian graphs; Input graphs; Planar graph; Graph theory
Replacement paths and distance sensitivity oracles via fast matrix multiplication,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885641460&doi=10.1145%2f2438645.2438646&partnerID=40&md5=66708c546f826f113b4705701ae79632,"A distance sensitivity oracle of an n-vertex graph G = (V,E) is a data structure that can report shortest paths when edges of the graph fail. A query (u ∈ V, v ∈ V, S ⊆ E) to this oracle returns a shortest u-to-v path in the graph G′ = (V,E \ S). We present randomized (Monte Carlo) algorithms for constructing a distance sensitivity oracle of size Õ(n 3-α) for |S| = O(lg n/lg lg n) and any choice of 0 < α < 1. For real edge-lengths, the oracle is constructed in O(n4-α) time and a query to this oracle takes Õ(n2-2(1-α)/|S|) time. For integral edge-lengths in {-M,.. ,M}, using the current ω < 2.376 matrix multiplication exponent, the oracle is constructed in O(Mn 3.376-α) time with Õ(n2-(1-α)/|S|) query, or alternatively in O(M0.681n3.575-α) time with Õ(n2-2(1-α)/|S|) query. Distance sensitivity oracles generalize the replacement paths problem in which u and v are known in advance and |S| = 1. In other words, if P is a shortest path from u to v in G, then the replacement paths problem asks to compute, for every edge e on P, a shortest u-to-v path that avoids e. Our new technique for constructing distance sensitivity oracles using fast matrix multiplication also yields the first subcubictime algorithm for the replacement paths problem when the edge-lengths are small integers. In particular, it yields a randomized (Monte Carlo) Õ(Mn2.376 + M 2/3 n2.584)-time algorithm for the replacement paths problem assuming M ≤ n0.624. Finally, we mention that both our replacement paths algorithm and our distance sensitivity oracle can be made to work, in the same time and space bounds, for the case of failed vertices rather than edges, that is, when S is a set of vertices and we seek a shortest u-to-v path in the graph obtained from G by removing all vertices in S and their adjacent edges. © 2013 ACM.",Distance oracles; Fault tolerance; Replacement paths; Shortest paths,Algorithms; Fault tolerance; Manganese; Matrix algebra; Monte Carlo methods; Query processing; Distance oracles; Fast matrix multiplication; MAtrix multiplication; N-vertex graph; Replacement paths; Shortest path; Space bounds; Time algorithms; Graph theory
Delays induce an exponential memory gap for rendezvous in trees,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885652592&doi=10.1145%2f2438645.2438649&partnerID=40&md5=3bd3af37f85cd94698ee589d666c6341,"The aim of rendezvous in a graph is meeting of two mobile agents at some node of an unknown anonymous connected graph. In this article, we focus on rendezvous in trees, and, analogously to the efforts that have been made for solving the exploration problem with compact automata, we study the size of memory of mobile agents that permits to solve the rendezvous problem deterministically. We assume that the agents are identical, and move in synchronous rounds. We first show that if the delay between the starting times of the agents is arbitrary, then the lower bound on memory required for rendezvous is Ω(log n) bits, even for the line of length n. This lower bound meets a previously known upper bound of O(log n) bits for rendezvous in arbitrary graphs of size at most n. Ourmain result is a proof that the amount of memory needed for rendezvous with simultaneous start depends essentially on the number ℓ of leaves of the tree, and is exponentially less impacted by the number n of nodes. Indeed, we present two identical agents with O(log ℓ + log log n) bits of memory that solve the rendezvous problem in all trees with at most n nodes and at most ℓ leaves. Hence, for the class of trees with polylogarithmically many leaves, there is an exponential gap in minimum memory size needed for rendezvous between the scenario with arbitrary delay and the scenario with delay zero. Moreover, we show that our upper bound is optimal by proving that Ω(log ℓ + log log n) bits of memory are required for rendezvous, even in the class of trees with degrees bounded by 3. © 2013 ACM.",Compact data structure; Exploration; Rendezvous,Forestry; Natural Resources; Problem Solving; Robots; Automata theory; Forestry; Mobile agents; Natural resources exploration; Trees (mathematics); Arbitrary delays; Arbitrary graphs; Compact data structure; Connected graph; Lower bounds; Memory size; Rendezvous; Rendezvous problems; Problem solving
Finding small separators in linear time via treewidth reduction,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885659647&doi=10.1145%2f2500119&partnerID=40&md5=8199d5930162263c09eb01e6752ef22b,"We present a method for reducing the treewidth of a graph while preserving all of itsminimal s-t separators up to a certain fixed size k. This technique allows us to solve s - t CUT and MULTICUT problems with various additional restrictions (e.g., the vertices being removed from the graph form an independent set or induce a connected graph) in linear time for every fixed number k of removed vertices. Our results have applications for problems that are not directly defined by separators, but the known solution methods depend on some variant of separation. For example, we can solve similarly restricted generalizations of BIPARTIZATION (delete at most k vertices from G to make it bipartite) in almost linear time for every fixed number k of removed vertices. These results answer a number of open questions in the area of parameterized complexity. Furthermore, our technique turns out to be relevant for (H,C, K)- and (H,C,≤K)- coloring problems as well, which are cardinality constrained variants of the classical H-coloring problem. We make progress in the classification of the parameterized complexity of these problems by identifying new cases that can be solved in almost linear time for every fixed cardinality bound. © 2013 ACM.",Cut problems; Fixed-parameter tractability; Separation problems; Treewidth,Separators; Coloring problems; Connected graph; Cut problems; Fixed-parameter tractability; Parameterized complexity; Separation problems; Solution methods; Tree-width; Graph theory
Corrigendum: Improved results for data migration and open shop scheduling,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885678122&doi=10.1145%2f2500123&partnerID=40&md5=7fd81489b9d1ff47503720507a3e93f4,"In Gandhi et al. [2006], we gave an algorithm for the data migration and non-deterministic open shop scheduling problems in the minimum sum version, that was claimed to achieve a 5.06-approximation. Unfortunately, it was pointed to us by Maxim Sviridenko that the argument contained an unfounded assumption that has eluded all of its readers until now. We detail in this document how this error can be amended. A side effect is an improved approximation ratio of 4.96. © 2013 ACM.",Approximation algorithms; Data migration; Linear programming; LP rounding; Open shop; Scheduling,Approximation algorithms; Linear programming; Approximation ratios; Data migration; LP-rounding; Open shop; Open shop scheduling; Open shop scheduling problem; Side effect; Scheduling
Approximating the girth,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885662040&doi=10.1145%2f2438645.2438647&partnerID=40&md5=2afc944f0878b7655ac7862983c08e4e,"This article considers the problem of computing a minimum weight cycle in weighted undirected graphs. Given a weighted undirected graph G = (V,E,w), let C be a minimum weight cycle of G, let w(C) be the weight of C, and let wmax(C) be the weight of the maximum edge of C. We obtain three new approximation algorithms for the minimum weight cycle problem: (1) for integral weights from the range [1,M], an algorithm that reports a cycle of weight at most 4/3 w(C) in O(n2 log n(log n + logM)) time; (2) For integral weights from the range [1,M], an algorithm that reports a cycle of weight at most w(C) + w max(C) in O(n2 log n(log n+logM)) time; (3) For nonnegative real edge weights, an algorithm that for any ε > 0 reports a cycle of weight at most (4/3 + ε)w(C) in O(1/ε n2 log n(log log n)) time. In a recent breakthrough,Williams andWilliams [2010] showed that a subcubic algorithm, that computes the exact minimum weight cycle in undirected graphs with integral weights from the range [1,M], implies a subcubic algorithm for computing all-pairs shortest paths in directed graphs with integral weights from the range [-M,M]. This implies that in order to get a subcubic algorithm for computing a minimum weight cycle, we have to relax the problem and to consider an approximated solution. Lingas and Lundell [2009] were the first to consider approximation in the context of minimum weight cycle in weighted graphs. They presented a 2-approximation algorithm for integral weights with O(n2 log n(log n + logM)) running time. They also posed, as an open problem, the question whether it is possible to obtain a subcubic algorithm with a c-approximation, where c < 2. The current article answers this question in the affirmative, by presenting an algorithm with 4/3-approximation and the same running time. Surprisingly, the approximation factor of 4/3 is not accidental. We show, using the new result of Williams and Williams [2010], that a subcubic combinatorial algorithm with (4/3-ε)-approximation, where 0 < ε ≤ 1/3, implies a subcubic combinatorial algorithm for multiplying two boolean matrices. © 2013 ACM.",Approximation; Girth; Graphs; Minimum weight cycle,Graph theory; Graphic methods; All-pairs shortest paths; Approximated solutions; Approximation; Combinatorial algorithm; Girth; Graphs; Minimum weight; Weighted undirected graph; Approximation algorithms
Optimal lower bounds for projective list update algorithms,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885615209&doi=10.1145%2f2500120&partnerID=40&md5=d39848dfb3f9b4337280f05496f1edc3,"The list update problem is a classical online problem, with an optimal competitive ratio that is still open, known to be somewhere between 1.5 and 1.6. An algorithm with competitive ratio 1.6, the smallest known to date, is COMB, a randomized combination of BIT and the TIMESTAMP algorithm TS. This and almost all other list update algorithms, like MTF, are projective in the sense that they can be defined by looking only at any pair of list items at a time. Projectivity (also known as ""list factoring"") simplifies both the description of the algorithm and its analysis, and so far seems to be the only way to define a good online algorithm for lists of arbitrary length. In this article, we characterize all projective list update algorithms and show that their competitive ratio is never smaller than 1.6 in the partial cost model. Therefore, COMB is a best possible projective algorithm in this model. © 2013 ACM.",Competitive analysis; Linear lists; Online algorithms,Optimization; Competitive analysis; Competitive ratio; Linear lists; On-line algorithms; Online problems; Optimal competitive ratios; Optimal lower bound; Update algorithms; Algorithms
Speed scaling with an arbitrary power function,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885587061&doi=10.1145%2f2438645.2438650&partnerID=40&md5=66d0d18091070ad9266f5f094d48e519,"This article initiates a theoretical investigation into online scheduling problems with speed scaling where the allowable speeds may be discrete, and the power function may be arbitrary, and develops algorithmic analysis techniques for this setting. We show that a natural algorithm, which uses Shortest Remaining Processing Time for scheduling and sets the power to be one more than the number of unfinished jobs, is 3-competitive for the objective of total flow time plus energy. We also show that another natural algorithm, which uses Highest Density First for scheduling and sets the power to be the fractional weight of the unfinished jobs, is a 2-competitive algorithm for the objective of fractional weighted flow time plus energy. © 2013 ACM.",Scheduling; Speed scaling,Scheduling; Algorithmic analysis; Fractional weight; Online scheduling; Power functions; Shortest remaining processing time; Speed scaling; Theoretical investigations; Total flowtime; Algorithms
On the matrix Berlekamp-Massey algorithm,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885612111&doi=10.1145%2f2500122&partnerID=40&md5=f26eb87650ccf16f550bd08224b7bcf9,"We analyze the Matrix Berlekamp/Massey algorithm, which generalizes the Berlekamp/Massey algorithm [Massey 1969] for computing linear generators of scalar sequences. The Matrix Berlekamp/Massey algorithm computes a minimal matrix generator of a linearly generated matrix sequence and has been first introduced by Rissanen [1972a], Dickinson et al. [1974], and Coppersmith [1994]. Our version of the algorithm makes no restrictions on the rank and dimensions of the matrix sequence. We also give new proofs of correctness and complexity for the algorithm, which is based on self-contained loop invariants and includes an explicit termination criterion for a given determinantal degree bound of the minimal matrix generator. © 2013 ACM.",Linear generated sequences; Matrix polynomials; Minimal generators; Multivariable linear control; Vector Berlekamp/Massey algorithm,Algorithms; Linear control systems; Berlekamp; Linear controls; Linear generated sequences; Matrix polynomials; Minimal generators; Matrix algebra
Morphing orthogonal planar graph drawings,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885582293&doi=10.1145%2f2500118&partnerID=40&md5=b95fb567be269b9663f8fb6463e8605e,"We give an algorithm to morph between two planar orthogonal drawings of a graph, preserving planarity and orthogonality. The morph uses a quadratic number of steps, where each step is a linear morph (a linear interpolation between two drawings). This is the first algorithm to provide planarity-preserving morphs with well-behaved complexity for a significant class of graph drawings. Our method is to morph until each edge is represented by a sequence of segments, with corresponding segments parallel in the two drawings. Then, in a result of independent interest, we morph such parallel planar orthogonal drawings, preserving edge directions and planarity. © 2013 ACM.",Graph drawing; Morphing; Orthogonal drawing; Planar graphs,Algorithms; Drawing (graphics); Edge direction; Graph drawing; Linear Interpolation; Morphing; Orthogonal drawings; Orthogonality; Planar graph; Quadratic number; Graph theory
Improved deterministic algorithms for decremental reachability and strongly connected components,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880165576&doi=10.1145%2f2483699.2483707&partnerID=40&md5=069582c966fc3728344975930084a67c,"This article presents a new deterministic algorithm for decremental maintenance of the transitive closure in a directed graph. The algorithm processes any sequence of edge deletions in O(mn) time and answers queries in constant time. Previously, such time bound has only been achieved by a randomized Las Vegas algorithm. In addition to that, a few decremental algorithms for maintaining strongly connected components are shown, whose time complexity is O(n1.5) for planar graphs, O(n log n) for graphs with bounded treewidth and O(mn) for general digraphs. © 2013 ACM.",Algorithms; Decremental algorithms; Dynamic algorithms; Graph algorithms; Graphs; Strongly connected components; Transitive closure,Directed graphs; Decremental algorithms; Dynamic algorithm; Graph algorithms; Graphs; Strongly connected component; Transitive closure; Algorithms
Bin packing via discrepancy of permutations,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880162985&doi=10.1145%2f2483699.2483704&partnerID=40&md5=96eb1194f272b1f6a6a688a4e6318d42,"A well-studied special case of bin packing is the 3-partition problem, where n items of size > 1/4 have to be packed in a minimum number of bins of capacity one. The famous Karmarkar-Karp algorithm transforms a fractional solution of a suitable LP relaxation for this problem into an integral solution that requires at most O(log n) additional bins. The three-permutations-problem of Beck is the following. Given any three permutations on n symbols, color the symbols red and blue, such that in any interval of any of those permutations, the number of red and blue symbols is roughly the same. The necessary difference is called the discrepancy. We establish a surprising connection between bin packing and Beck's problem: The additive integrality gap of the 3-partition linear programming relaxation can be bounded by the discrepancy of three permutations. This connection yields an alternative method to establish an O(log n) bound on the additive integrality gap of the 3-partition. Conversely, making use of a recent example of three permutations, for which a discrepancy of Ω(log n) is necessary, we prove the following: The O(log2 n) upper bound on the additive gap for bin packing with arbitrary item sizes cannot be improved by any technique that is based on rounding up items. This lower bound holds for a large class of algorithms including the Karmarkar-Karp procedure. © 2013 ACM.",Bin packing; Discrepancy theory; Linear programming relaxations,Algorithms; Linear programming; Alternative methods; Beck's problems; Bin packing; Discrepancy theory; Fractional solutions; Integral solutions; Integrality gaps; Linear programming relaxation; Bins
Sparse euclidean spanners with tiny diameter,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880180578&doi=10.1145%2f2483699.2483708&partnerID=40&md5=fdd63b497dad6bf2bad0679c06fbbbee,"In STOC'95, Arya et al. [1995] showed that for any set of n points in ℝd, a (1 + ε)-spanner with diameter at most 2 (respectively, 3) and O(n log n) edges (respectively, O(n log log n) edges) can be built in O(n log n) time. Moreover, it was shown in Arya et al. [1995] and Narasimhan and Smid [2007] that for any k ≥ 4, one can build in O(n(log n)2kαk(n)) time a (1+ε)-spanner with diameter at most 2k and O(nkαk (n)) edges. The function αk is the inverse of a certain function at the [k/2] th level of the primitive recursive hierarchy, where α0(n) = [n/2], α1 (n) = [√n], α2 (n) = [log n], α3 (n) = [log log n], α4 (n) = log* n, α5 (n) = [1/2 log* n], ⋯, etc. It is also known [Narasimhan and Smid 2007] that if one allows quadratic time, then these bounds can be improved. Specifically, for any k ≥ 4, a (1+ε)-spanner with diameter at most k and O(nkαk(n)) edges can be constructed in O(n2) time [Narasimhan and Smid 2007]. A major open question in this area is whether one can construct within time O(n log n+nkαk(n) ) a (1+ε)- spanner with diameter at most k and O(nkαk(n)) edges. In this article, we answer this question in the affirmative. Moreover, in fact, we provide a stronger result. Specifically, we show that for any k ≥ 4, a (1 + ε)-spanner with diameter at most k and O(nαk(n)) edges can be built in optimal time O(n log n). © 2013 ACM.",Diameter; Euclidean metrics; Euclidean spanners; Graph spanners,Algorithms; Diameter; Euclidean; Euclidean metrics; Graph spanners; Optimal time; Quadratic time; Recursive hierarchy; Sparse euclidean spanners; Mathematical techniques
Foreword to the special issue on SODA'11,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880171214&doi=10.1145%2f2483699.2483700&partnerID=40&md5=2b793ccbd4ed763af7f8522f1fdf355d,[No abstract available],,
Optimal pattern matching in LZW compressed strings,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880179552&doi=10.1145%2f2483699.2483705&partnerID=40&md5=d296b61ca18c34c497e501724b7eb6fd,"We consider the following variant of the classical pattern matching problem: given an uncompressed pattern p[ 1 · ·m] and a compressed representation of a string t[ 1 · ·N], does p occur in t? When t is compressed using the LZW method, we are able to detect the occurrence in optimal linear time, thus answering a question of Amir et al. [1994]. Previous results implied solutions with complexities O(n logm + m) Amir et al. [1994], O(n + m1+ε ) [Kosaraju 1995], or (randomized) O(n log N/n + m) [Farach and Thorup 1995], where n is the size of the compressed representation of t. Our algorithm is conceptually simple and fully deterministic. © 2013 ACM.",Compression; Lempel-Ziv-Welch; Pattern matching,Compaction; Optimization; Lempel-Ziv-Welch; Linear time; Pattern matching problems; Pattern matching
Persistent predecessor search and orthogonal point location on the word RAM,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880160638&doi=10.1145%2f2483699.2483702&partnerID=40&md5=43d094f1facf3849dc89935bec58f528,"We answer a basic data structuring question (e.g., raised by Dietz and Raman [1991]): Can van Emde Boas trees be made persistent, without changing their asymptotic query/update time? We present a (partially) persistent data structure that supports predecessor search in a set of integers in {1, . . . ,U} under an arbitrary sequence of n insertions and deletions, with O(log logU) expected query time and expected amortized update time, and O(n) space. The query bound is optimal in U for linear-space structures and improves previous near-O((log logU)2) methods. The same method solves a fundamental problem from computational geometry: point location in orthogonal planar subdivisions (where edges are vertical or horizontal). We obtain the first static data structure achieving O(log logU) worst-case query time and linear space. This result is again optimal in U for linearspace structures and improves the previous O((log logU 2) method by de Berg et al. [1995]. The same result also holds for higher-dimensional subdivisions that are orthogonal binary space partitions, and for certain nonorthogonal planar subdivisions such as triangulations without small angles. Many geometric applications follow, including improved query times for orthogonal range reporting for dimensions ≥ 3 on the RAM. Our key technique is an interesting new van-Emde-Boas-style recursion that alternates between two strategies, both quite simple. © 2013 ACM.",Computational geometry; Persistent data structures; Point location; Word-RAM algorithms,Data structures; Optimization; Trees (mathematics); Binary space partition; Geometric applications; Higher-dimensional; Insertions and deletions; Linear spaces; Non-orthogonal; Planar subdivision; Point location; Computational geometry
An almost optimal unrestricted fast Johnson-Lindenstrauss transform,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880159744&doi=10.1145%2f2483699.2483701&partnerID=40&md5=3dda0f92d9d7214a783815c2eb6ef716,"The problems of random projections and sparse reconstruction have much in common and individually received much attention. Surprisingly, until now they progressed in parallel and remained mostly separate. Here, we employ new tools from probability in Banach spaces that were successfully used in the context of sparse reconstruction to advance on an open problem in random pojection. In particular, we generalize and use an intricate result by Rudelson and Veshynin [2008] for sparse reconstruction which uses Dudley's theorem for bounding Gaussian processes. Our main result states that any set of N = exp(Õ (n)) real vectors in n dimensional space can be linearly mapped to a space of dimension k = O(logN polylog(n)), while (1) preserving the pairwise distances among the vectors to within any constant distortion and (2) being able to apply the transformation in time O(n log n) on each vector. This improves on the best known bound N = exp(Õ (n1/2)) achieved by Ailon and Liberty [2009] and N = exp(Õ(n1/3)) by Ailon and Chazelle [2010]. The dependence in the distortion constant however is suboptimal, and since the publication of an early version of the work, the gap between upper and lower bounds has been considerably tightened obtained by Krahmer and Ward [2011]. For constant distortion, this settles the open question posed by these authors up to a polylog(n) factor while considerably simplifying their constructions. © 2013 ACM.",Compressed sensing; Johnson-Lindenstrauss; Restricted isometry,Banach spaces; Compressed sensing; Linear transformations; Mathematical transformations; Constant distortion; Distortion constants; Johnson-Lindenstrauss; Johnson-lindenstrauss transforms; N-dimensional space; Restricted isometries; Sparse reconstruction; Upper and lower bounds; Vector spaces
On the complexity of approximating a nash equilibrium,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880145651&doi=10.1145%2f2483699.2483703&partnerID=40&md5=59902e5ef6840e3ce501aa2cdeff2f93,"We show that computing a relatively (i.e., multiplicatively as opposed to additively) approximate Nash equilibrium in two-player games is PPAD-complete, even for constant values of the approximation. Our result is the first constant inapproximability result for Nash equilibrium, since the original results on the computational complexity of the problem [Daskalakis et al. 2006a; Chen and Deng 2006]. Moreover, it provides an apparent-assuming that PPAD is not contained in TIME(nO(log n))-dichotomy between the complexities of additive and relative approximations, as for constant values of additive approximation a quasi-polynomialtime algorithm is known [Lipton et al. 2003]. Such a dichotomy does not exist for values of the approximation that scale inverse-polynomially with the size of the game, where both relative and additive approximations are PPAD-complete [Chen et al. 2006]. As a byproduct, our proof shows that (unconditionally) the sparsesupport lemma [Lipton et al. 2003] cannot be extended to relative notions of constant approximation. © 2013 ACM.",Approximation; Complexity; Game theory; Nash equilibrium; PPAD-completeness,Game theory; Approximation; Complexity; Constant values; Inapproximability; Nash equilibria; PPAD-completeness; Two-player games; Approximation algorithms
Optimal bounds for Johnson-Lindenstrauss transforms and streaming problems with subconstant error,2013,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880148890&doi=10.1145%2f2483699.2483706&partnerID=40&md5=7f550c837b56985aa5d3b7b6d92bd1f2,"The Johnson-Lindenstrauss transform is a dimensionality reduction technique with a wide range of applications to theoretical computer science. It is specified by a distribution over projection matrices from ℝn →ℝk where k ≤ n and states that k = O(ε -2 log 1/δ) dimensions suffice to approximate the norm of any fixed vector in ℝn to within a factor of 1±ε with probability at least 1-δ. In this article, we show that this bound on k is optimal up to a constant factor, improving upon a previous Ω((ε -2 log 1/δ)/ log(1/ε)) dimension bound of Alon. Our techniques are based on lower bounding the information cost of a novel one-way communication game and yield the first space lower bounds in a data stream model that depend on the error probability δ. For many streaming problems, the most naïve way of achieving error probability δ is to first achieve constant probability, then take the median of O(log 1/δ) independent repetitions. Our techniques show that for a wide range of problems, this is in fact optimal! As an example, we show that estimating the ℓp- distance for any p ε [0, 2] requires Ω(ε-2 log n log 1/δ) space, even for vectors in {0, 1}n. This is optimal in all parameters and closes a long line of work on this problem. We also show the number of distinct elements requires Ω(ε-2 log 1/δ + log n) space, which is optimal if ε-2 = Ω(log n). We also improve previous lower bounds for entropy in the strict turnstile and general turnstile models by a multiplicative factor of Ω(log 1/δ). Finally, we give an application to one-way communication complexity under product distributions, showing that, unlike the case of constant δ, the VC-dimension does not characterize the complexity when δ = o(1). © 2013 ACM.",Communication complexity; Data streams; Distinct elements; Entropy; Frequency moments,Communication; Data communication systems; Entropy; Probability distributions; Vector spaces; Communication complexity; Data stream; Dimensionality reduction techniques; Distinct elements; Frequency moments; Johnson-lindenstrauss transforms; Multiplicative factors; Theoretical computer science; Optimization
Distributed algorithms for multicommodity flow problems via approximate steepest descent framework,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872468476&doi=10.1145%2f2390176.2390179&partnerID=40&md5=624175dac33e7b75e2cb6a6ceddfe339,"We consider solutions for distributed multicommodity flow problems, which are solved by multiple agents operating in a cooperative but uncoordinated manner. We show first distributed solutions that allow (1 + ε) approximation and whose convergence time is essentially linear in the maximal path length, and is independent of the number of commodities and the size of the graph. Our algorithms use a very natural approximate steepest descent framework, combined with a blocking flow technique to speed up the convergence in distributed and parallel environment. Previously known solutions that achieved comparable convergence time and approximation ratio required exponential computational and space overhead per agent. © 2012 ACM.",,Mathematical techniques; Approximation ratios; Blocking flow; Convergence time; Distributed solutions; Maximal path; Multicommodity flow problems; Multiple agents; Parallel environment; Space overhead; Steepest descent; Algorithms
On exact algorithms for treewidth,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872465906&doi=10.1145%2f2390176.2390188&partnerID=40&md5=8103fda415e596360ab8d62ea1ae158c,"We give experimental and theoretical results on the problem of computing the treewidth of a graph by exact exponential-time algorithms using exponential space or using only polynomial space. We first report on an implementation of a dynamic programming algorithm for computing the treewidth of a graph with running time O(2n). This algorithm is based on the old dynamic programming method introduced by Held and Karp for the TRAVELING SALESMAN problem. We use some optimizations that do not affect the worst case running time but improve on the running time on actual instances and can be seen to be practical for small instances. We also consider the problem of computing TREEWIDTH under the restriction that the space used is only polynomial and give a simple O(4n) algorithm that requires polynomial space. We also show that with a more complicated algorithm using balanced separators, TREEWIDTH can be computed in O(2.9512n) time and polynomial space. © 2012 ACM.",,Dynamic programming; Polynomials; Traveling salesman problem; Dynamic programming algorithm; Dynamic programming methods; Exact algorithms; Exponential-time algorithms; Polynomial space; Running time; Theoretical result; Tree-width; Algorithms
Cycle detection and correction,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872439754&doi=10.1145%2f2390176.2390189&partnerID=40&md5=ea140b29f24755146f4575f547d6ce36,"Assume that a natural cyclic phenomenon has been measured, but the data is corrupted by errors. The type of corruption is application-dependent and may be caused by measurements errors, or natural features of the phenomenon. We assume that an appropriate metric exists, which measures the amount of corruption experienced. This article studies the problem of recovering the correct cycle from data corrupted by various error models, formally defined as the period recovery problem. Specifically, we define a metric property which we call pseudolocality and study the period recovery problem under pseudolocal metrics. Examples of pseudolocal metrics are the Hamming distance, the swap distance, and the interchange (or Cayley) distance. We show that for pseudolocal metrics, periodicity is a powerful property allowing detecting the original cycle and correcting the data, under suitable conditions. Some surprising features of our algorithm are that we can efficiently identify the period in the corrupted data, up to a number of possibilities logarithmic in the length of the data string, even for metrics whose calculation is N P-hard. For the Hamming metric, we can reconstruct the corrupted data in near-linear time even for unbounded alphabets. This result is achieved using the property of separation in the self-convolution vector and Reed-Solomon codes. Finally, we employ our techniques beyond the scope of pseudo-local metrics and give a recovery algorithm for the non-pseudolocal Levenshtein edit metric. © 2012 ACM.",,Algorithms; Crime; Errors; Hamming distance; Reed-Solomon codes; Corrupted data; Cycle detection; Data strings; Error model; Hamming metric; Metric properties; Natural features; Near-linear time; Recovery algorithms; Recovery
Online scheduling of packets with agreeable deadlines,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872471581&doi=10.1145%2f2390176.2390181&partnerID=40&md5=cfb999f8043149a9b2c54a1624d37255,"This article concerns an online packet scheduling problem that arises as a natural model for buffer management at a network router. Packets arrive at a router at integer time steps, and are buffered upon arrival. Packets have non-negative weights and integer deadlines that are (weakly) increasing in their arrival times. In each integer time step, at most one packet can be sent. The objective is to maximize the sum of the weights of the packets that are sent by their deadlines. The main results include an optimal (φ := (1+ √ 5)/2 ≈ 1.618)- competitive deterministic online algorithm, a (4/3 ≈ 1.33)-competitive randomized online algorithm against an oblivious adversary, and a 2-speed 1-competitive deterministic online algorithm. The analysis does not use a potential function explicitly, but instead modifies the adversary's buffer and credits the adversary to account for these modifications. © 2012 ACM.",,Routers; Arrival time; Buffer management; Deterministic online algorithms; Natural models; Network routers; On-line algorithms; Online scheduling; Packet scheduling; Potential function; Time step; Algorithms
Approximating parameterized convex optimization problems,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872453870&doi=10.1145%2f2390176.2390186&partnerID=40&md5=0bb1f44f3395e2e7199ed49ae33ea46b,"We consider parameterized convex optimization problems over the unit simplex, that depend on one parameter. We provide a simple and efficient scheme for maintaining an ε-approximate solution (and a corresponding ε-coreset) along the entire parameter path. We prove correctness and optimality of the method. Practically relevant instances of the abstract parameterized optimization problem are for example regularization paths of support vector machines, multiple kernel learning, and minimum enclosing balls of moving points. © 2012 ACM.",,Convex optimization; Optimization; Support vector machines; Approximate solution; Convex optimization problems; Minimum enclosing ball; Multiple Kernel Learning; Optimality; Optimization problems; Parameterized; Regularization paths; Parameterization
On the set multicover problem in geometric settings,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872482196&doi=10.1145%2f2390176.2390185&partnerID=40&md5=b3e6b7b9597fa04c8d103b9d32848dbb,"We consider the set multicover problem in geometric settings. Given a set of points P and a collection of geometric shapes (or sets) F, we wish to find a minimum cardinality subset of F such that each point p ∈ P is covered by (contained in) at least d(p) sets. Here, d(p) is an integer demand (requirement) for p. When the demands d(p) = 1 for all p, this is the standard set cover problem. The set cover problem in geometric settings admits an approximation ratio that is better than that for the general version. In this article, we show that similar improvements can be obtained for the multicover problem as well. In particular, we obtain an O(log opt) approximation for set systems of bounded VC-dimension, and an O(1) approximation for covering points by half-spaces in three dimensions and for some other classes of shapes. © 2012 ACM.",,Algorithms; Mathematical techniques; Approximation ratios; Cardinalities; General version; Geometric shape; Half spaces; Set cover problem; Set system; Three dimensions; VC-dimension; Geometry
Prize-collecting steiner network problems,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872480216&doi=10.1145%2f2390176.2390178&partnerID=40&md5=3aeb641a32007ad0c214857c660ad183,"In the Steiner Network problem, we are given a graph G with edge-costs and connectivity requirements ruv between node pairs u, v. The goal is to find a minimum-cost subgraph H of G that contains ruv edge-disjoint paths for all u, v ∈ V. In Prize-Collecting Steiner Network problems, we do not need to satisfy all requirements, but are given a penalty function for violating the connectivity requirements, and the goal is to find a subgraph H that minimizes the cost plus the penalty. The case when ruv ∈ {0, 1} is the classic Prize-Collecting Steiner Forest problem. In this article, we present a novel linear programming relaxation for the Prize-Collecting Steiner Network problem, and by rounding it, obtain the first constant-factor approximation algorithm for submodular and monotone nondecreasing penalty functions. In particular, our setting includes all-or-nothing penalty functions, which charge the penalty even if the connectivity requirement is slightly violated; this resolves an open question posed by Nagarajan et al. [2008]. We further generalize our results for element-connectivity and node-connectivity. © 2012 ACM.",,Approximation algorithms; Graph theory; Constant-factor approximation algorithms; Edge-disjoint paths; Graph G; Linear programming relaxation; Node connectivity; Node pairs; Penalty function; Prize-collecting; Steiner forest problem; Steiner network; Subgraphs; Submodular; Costs
Algorithms and complexity for periodic real-time scheduling,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872459963&doi=10.1145%2f2390176.2390182&partnerID=40&md5=24bc81f47595437d5f2ee398c69809a8,"We investigate the preemptive scheduling of periodic tasks with hard deadlines. We show that, even in the uniprocessor case, no pseudopolynomial-time algorithm can test the feasibility of a task system within a constant speedup bound, unless P = NP. This result contrasts with recent results for sporadic task systems. For two special cases, synchronous task systems and systems with a constant number of different task types, we provide the first polynomial-time constant-speedup feasibility tests for multiprocessor platforms. Furthermore, we show that the problem of testing feasibility is coNP-hard for synchronous multiprocessor task systems. The complexity of some of these problems has been open for a long time. We also propose a weight maximization variant of the feasibility problem, where every task has a nonnegative weight, and the goal is to find a subset of tasks that can be scheduled feasibly and has maximum weight. We give the first constant-speed, constant-approximation algorithm for the case of synchronous task systems, together with related hardness results. © 2012 ACM.",,Distributed computer systems; Algorithms and complexity; Feasibility problem; Feasibility tests; Hardness result; Multi-processor platforms; Multiprocessor tasks; Periodic tasks; Polynomial-time; Pre-emptive scheduling; Real time scheduling; Sporadic task systems; Task type; Testing feasibility; Uniprocessors; Approximation algorithms
A compact routing scheme and approximate distance oracle for power-law graphs,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872455087&doi=10.1145%2f2390176.2390180&partnerID=40&md5=327f2c758dbf0a92337f8c5bb1603668,"Compact routing addresses the tradeoff between table sizes and stretch, which is the worst-case ratio between the length of the path a packet is routed through by the scheme and the length of an actual shortest path from source to destination. We adapt the compact routing scheme by Thorup and Zwick [2001] to optimize it for power-law graphs. We analyze our adapted routing scheme based on the theory of unweighted random power-law graphs with fixed expected degree sequence by Aiello et al. [2000]. Our result is the first analytical bound coupled to the parameter of the power-law graph model for a compact routing scheme. Let n denote the number of nodes in the network. We provide a labeled routing scheme that, after a stretch-5 handshaking step (similar to DNS lookup in TCP/IP), routes messages along stretch-3 paths. We prove that, instead of routing tables with Õ (n1/2) bits ( ̃O suppresses factors logarithmic in n) as in the general scheme by Thorup and Zwick, expected sizes of O(nγ log n) bits are sufficient, and that all the routing tables can be constructed at once in expected time O(n1+γlog n), with γ = τ-2 2τ-3 + ε, where τ ∈ (2, 3) is the power-law exponent and ε > 0 (which implies ε < γ < 1/3 + ε). Both bounds also hold with probability at least 1-1/n (independent of ε). The routing scheme is a labeled scheme, requiring a stretch-5 handshaking step. The scheme uses addresses and message headers with O(log nlog log n) bits, with probability at least 1-o(1).We further demonstrate the effectiveness of our scheme by simulations on real-world graphs as well as synthetic power-law graphs. With the same techniques as for the compact routing scheme, we also adapt the approximate distance oracle by Thorup and Zwick [2001, 2005] for stretch-3 and we obtain a new upper bound of expected ̃O(n 1+γ) for space and preprocessing for random power-law graphs. Our distance oracle is the first one optimized for power-law graphs. Furthermore, we provide a linear-space data structure that can answer 5-approximate distance queries in time at most ̃O(n1/4+ε) (similar to γ , the exponent actually depends on τ and lies between ε and 1/4 + ε). © 2012 ACM.",,Data structures; Graphic methods; Internet protocols; Optimization; Query processing; Routing protocols; Analytical bounds; Compact routing; Degree sequence; Expected time; Linear spaces; Lookups; Power law exponent; Power-law graph; Real-world graphs; Routing scheme; Routing table; Shortest path; Table size; Upper Bound; Graph theory
Wireless scheduling with power control,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865004967&doi=10.1145%2f2390176.2390183&partnerID=40&md5=33ceeece5b2ccdf12f5515a1b17c65bc,"We consider the scheduling of arbitrary wireless links in the physical model of interference to minimize the time for satisfying all requests. We study here the combined problem of scheduling and power control, where we seek both an assignment of power settings and a partition of the links so that each set satisfies the signal-to-interference-plus-noise (SINR) constraints. We give an algorithm that attains an approximation ratio of O(log n . log logΔ), where n is the number of links and Δ is the ratio between the longest and the shortest link length. Under the natural assumption that lengths are represented in binary, this gives the first approximation ratio that is polylogarithmic in the size of the input. The algorithm has the desirable property of using an oblivious power assignment, where the power assigned to a sender depends only on the length of the link. We give evidence that this dependence on Δ is unavoidable, showing that any reasonably behaving oblivious power assignment results in a Δ(log logΔ)-approximation. These results hold also for the (weighted) capacity problem of finding a maximum (weighted) subset of links that can be scheduled in a single time slot. In addition, we obtain improved approximation for a bidirectional variant of the scheduling problem, give partial answers to questions about the utility of graphs for modeling physical interference, and generalize the setting from the standard 2-dimensional Euclidean plane to doubling metrics. Finally, we explore the utility of graph models in capturing wireless interference. © 2012 ACM.",,Geometry; Models; Power control; Scheduling; Signal interference; Spurious signal noise; Approximation ratios; Capacity problems; Doubling metrics; Euclidean planes; Graph model; Link length; Physical model; Polylogarithmic; Power assignment; Power settings; Scheduling problem; Signal-to-interference-plus-noise; Time slots; Wireless interference; Wireless link; Wireless scheduling; Approximation algorithms
Polynomial kernels for dominating set in graphs of bounded degeneracy and beyond,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872473214&doi=10.1145%2f2390176.2390187&partnerID=40&md5=de68a13d443d532a4c96053ea4f12133,"We show that for every fixed j ≥ i ≥ 1, the k-DOMINATING SET problem restricted to graphs that do not have Ki, j(the complete bipartite graph on (i + j) vertices, where the two parts have i and j vertices, respectively) as a subgraph is fixed parameter tractable (FPT) and has a polynomial kernel.We describe a polynomial-time algorithm that, given a K i, j-free graph G and a nonnegative integer k, constructs a graph H (the ""kernel"") and an integer k′ such that (1) G has a dominating set of size at most k if and only if H has a dominating set of size at most k′, (2) H has O(( j + 1)i+1ki2 ) vertices, and (3) k′ = O(( j + 1)i+1ki2 ). Since d-degenerate graphs do not have Kd+1,d+1as a subgraph, this immediately yields a polynomial kernel on O((d+ 2)d+2k(d+1) 2) vertices for the k-DOMINATING SET problem on d-degenerate graphs, solving an open problem posed by Alon and Gutner [Alon and Gutner 2008; Gutner 2009]. The most general class of graphs for which a polynomial kernel was previously known for k-DOMINATING SET is the class of Kh-topological- minor-free graphs [Gutner 2009]. Graphs of bounded degeneracy are the most general class of graphs for which an FPT algorithm was previously known for this problem. Kh-topologicalminor- free graphs are Ki, j-free for suitable values of i, j (but not vice-versa), and so our results show that k-DOMINATING SET has both FPT algorithms and polynomial kernels in strictly more general classes of graphs. Using the same techniques, we also obtain an O( jki) vertex-kernel for the k-INDEPENDENT DOMINATING SET problem on Ki, j -free graphs. © 2012 ACM.",,Algorithms; Graphic methods; Integer programming; Polynomials; Problem solving; Complete bipartite graphs; Dominating set problems; Dominating sets; FPT algorithms; Free graphs; General class; Nonnegative integers; Polynomial kernels; Polynomial-time algorithms; Set problems; Subgraphs; Topological-minor; Graph theory
Combinatiorial algorithms for wireless information flow,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872454373&doi=10.1145%2f2390176.2390184&partnerID=40&md5=5f4234c40d2d8c3f37006d4fddd483c0,"A long-standing open question in information theory is to characterize the unicast capacity of a wireless relay network. The difficulty arises due to the complex signal interactions induced in the network, since the wireless channel inherently broadcasts the signals and there is interference among transmissions. Recently, Avestimehr et al. [2007b] proposed a linear deterministic model that takes into account the shared nature of wireless channels, focusing on the signal interactions rather than the background noise. They generalized the min-cut max-flow theorem for graphs to networks of deterministic channels and proved that the capacity can be achieved using information theoretical tools. They showed that the value of the minimum cut is in this case the minimum rank of all the adjacency matrices describing source-destination cuts. In this article, we develop a polynomial-time algorithm that discovers the relay encoding strategy to achieve the min-cut value in linear deterministic (wireless) networks, for the case of a unicast connection. Our algorithm crucially uses a notion of linear independence between channels to calculate the capacity in polynomial time. Moreover, we can achieve the capacity by using very simple one-symbol processing at the intermediate nodes, thereby constructively yielding finite-length strategies that achieve the unicast capacity of the linear deterministic (wireless) relay network. © 2012 ACM.",,Information theory; Polynomial approximation; Relay control systems; Adjacency matrices; Background noise; Complex signal; Deterministic models; Encoding strategy; Information flows; Intermediate node; Linear independence; Max-flow theorem; Min-cut; Minimum cut; Minimum rank; Polynomial-time; Polynomial-time algorithms; Relay network; Signal interactions; Unicast; Unicast connections; Wireless channel; Wireless relay networks; Algorithms
The effectiveness of stackelberg strategies and tolls for network congestion games,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870209759&doi=10.1145%2f2344422.2344426&partnerID=40&md5=f9ffdd4a1683343978a7fbac4bd2fe31,"It is well known that in a network with arbitrary (convex) latency functions that are a function of edge traffic, the worst-case ratio, over all inputs, of the system delay caused due to selfish behavior versus the system delay of the optimal centralized solution may be unbounded even if the system consists of only two parallel links. This ratio is called the price of anarchy (PoA). In this article, we investigate ways by which one can reduce the performance degradation due to selfish behavior. We investigate two primary methods (a) Stackelberg routing strategies, where a central authority, for example, network manager, controls a fixed fraction of the flow, and can route this flow in any desired way so as to influence the flow of selfish users; and (b) network tolls, where tolls are imposed on the edges to modify the latencies of the edges, and thereby influence the induced Nash equilibrium. We obtain results demonstrating the effectiveness of both Stackelberg strategies and tolls in controlling the price of anarchy. For Stackelberg strategies, we obtain the first results for nonatomic routing in graphs more general than parallel-link graphs, and strengthen existing results for parallel-link graphs. (i) In series-parallel graphs, we show that Stackelberg routing reduces the PoA to a constant (depending on the fraction of flow controlled). (ii) For general graphs, we obtain latency-class specific bounds on the PoA with Stackelberg routing, which give a continuous trade-off between the fraction of flow controlled and the price of anarchy. (iii) In parallellink graphs, we show that for any given class L of latency functions, Stackelberg routing reduces the PoA to at most α+(1-α) · ρ(L), where α is the fraction of flow controlled and ρ(L) is the PoA of class L (when α = 0). For network tolls, motivated by the known strong results for nonatomic games, we consider the more general setting of atomic splittable routing games. We show that tolls inducing an optimal flow always exist, even for general asymmetric games with heterogeneous users, and can be computed efficiently by solving a convex program. This resolves a basic open question about the effectiveness of tolls for atomic splittable games. Furthermore, we give a complete characterization of flows that can be induced via tolls. © 2012 ACM.",,Costs; Graphic methods; Optimization; Asymmetric games; Convex programs; General graph; Heterogeneous users; Latency function; Nash equilibria; Network congestion game; Network managers; Optimal flows; Parallel links; Performance degradation; Price of anarchy; Routing strategies; Selfish users; Series-parallel graph; Stackelberg; System delay; Flow graphs
The smoothed complexity of edit distance,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870204326&doi=10.1145%2f2344422.2344434&partnerID=40&md5=4b1d165e7a5a0bc944f90966f7a01539,"We initiate the study of the smoothed complexity of sequence alignment, by proposing a semi-random model of edit distance between two input strings, generated as follows: First, an adversary chooses two binary strings of length d and a longest common subsequence Aof them. Then, every character is perturbed independently with probability p, except that A is perturbed in exactly the same way inside the two strings. We design two efficient algorithms that compute the edit distance on smoothed instances up to a constant factor approximation. The first algorithm runs in near-linear time, namely d{1+ε} for any fixed ε > 0. The second one runs in time sublinear in d, assuming the edit distance is not too small. These approximation and runtime guarantees are significantly better than the bounds that were known for worst-case inputs. Our technical contribution is twofold. First, we rely on finding matches between substrings in the two strings, where two substrings are considered a match if their edit distance is relatively small, a prevailing technique in commonly used heuristics, such as PatternHunter of Ma et al. [2002]. Second, we effectively reduce the smoothed edit distance to a simpler variant of (worst-case) edit distance, namely, edit distance on permutations (a.k.a. Ulam's metric). We are thus able to build on algorithms developed for the Ulam metric, whose much better algorithmic guarantees usually do not carry over to general edit distance. © 2012 ACM.",,Mathematical techniques; Binary string; Constant factor approximation; Edit distance; Input string; Longest common subsequences; Near-linear time; Runtimes; Semi-random; Sequence alignments; Sub-strings; Sublinear; Technical contribution; Algorithms
Parallel pipelined filter ordering with precedence constraints,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870226281&doi=10.1145%2f2344422.2344431&partnerID=40&md5=78e72f61aa2099e849d21a41aa3991ce,"In the parallel pipelined filter ordering problem, we are given a set of n filters that run in parallel. The filters need to be applied to a stream of elements, to determine which elements pass all filters. Each filter has a rate limit ri on the number of elements it can process per unit time, and a selectivity pi, which is the probability that a random element will pass the filter. The goal is to maximize throughput. This problem appears naturally in a variety of settings, including parallel query optimization in databases and query processing over Web services. We present an O(n3) algorithm for this problem, given tree-structured precedence constraints on the filters. This extends work of Condon et al. [2009] and Kodialam [2001], who presented algorithms for solving the problem without precedence constraints. Our algorithm is combinatorial and produces a sparse solution. Motivated by join operators in database queries, we also give algorithms for versions of the problem in which ""filter"" selectivities may be greater than or equal to 1. We prove a strong connection between the more classical problem of minimizing total work in sequential filter ordering (A), and the parallel pipelined filter ordering problem (B). More precisely, we prove that A is solvable in polynomial time for a given class of precedence constraints if and only if B is as well. This equivalence allows us to show that B is NP-Hard in the presence of arbitrary precedence constraints (since A is known to be NP-Hard in that setting. © 2012 ACM.",,Algorithms; Computational complexity; Polynomial approximation; Trees (mathematics); Web services; Classical problems; Database queries; Join operators; Maximize throughput; NP-hard; Parallel query optimization; Per unit; Polynomial-time; Precedence constraints; Random elements; Rate limit; Sequential filter; Sparse solutions; Total work; Problem solving
All-Pairs shortest paths for unweighted undirected graphs in o(mn) time,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870216811&doi=10.1145%2f2344422.2344424&partnerID=40&md5=de31a7993db5ed7f4c83a0ef2c8ce110,We revisit the all-pairs-shortest-paths problem for an unweighted undirected graph with n vertices and m edges. We present new algorithms with the following running times: O(mn/ log n) if m> nlog nlog log log n O(mnlog log n/ log n) ifm> nlog log n O(n2 log2 log n/ log n) ifm≤ nlog log n. These represent the best time bounds known for the problem for all m≪ n1.376.We also obtain a similar type of result for the diameter problem for unweighted directed graphs. © 2012 ACM.,,Algorithms; Mathematical techniques; Diameter problem; Running time; Shortest path; Time bound; Undirected graph; Unweighted directed graphs; Graph theory
How to meet asynchronously (almost) everywhere,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870214759&doi=10.1145%2f2344422.2344427&partnerID=40&md5=a27137ef21b9de84bf359a12d228935f,"Two mobile agents (robots) with distinct labels have to meet in an arbitrary, possibly infinite, unknown connected graph or in an unknown connected terrain in the plane. Agents are modeled as points, and the route of each of them only depends on its label and on the unknown environment. The actual walk of each agent also depends on an asynchronous adversary that may arbitrarily vary the speed of the agent, stop it, or even move it back and forth, as long as the walk of the agent is continuous, does not leave its route and covers all of it. Meeting in a graph means that both agents must be at the same time in some node or in some point inside an edge of the graph, while meeting in a terrain means that both agents must be at the same time in some point of the terrain. Does there exist a deterministic algorithm that allows any two agents to meet in any unknown environment in spite of this very powerful adversary? We give deterministic rendezvous algorithms for agents starting at arbitrary nodes of any anonymous connected graph (finite or infinite) and for agents starting at any interior points with rational coordinates in any closed region of the plane with path-connected interior. In the geometric scenario agents may have different compasses and different units of length. While our algorithms work in a very general setting - agents can, indeed, meet almost everywhere - we show that none of these few limitations imposed on the environment can be removed. On the other hand, our algorithm also guarantees the following approximate rendezvous for agents starting at arbitrary interior points of a terrain as previously stated agents will eventually get to within an arbitrarily small positive distance from each other. © 2012 ACM.",,Algorithms; Graph theory; Landforms; Connected graph; Deterministic algorithms; Interior point; Unknown environments; Mobile agents
An online scalable algorithm for average flow time in broadcast scheduling,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870223998&doi=10.1145%2f2344422.2344429&partnerID=40&md5=e4f3fd833a1f9bf5a8735ba46032bfba,"In this article, the online pull-based broadcast model is considered. In this model, there are n pages of data stored at a server and requests arrive for pages online. When the server broadcasts page p, all outstanding requests for the same page p are simultaneously satisfied. We consider the problem of minimizing average (total) flow time online where all pages are unit-sized. For this problem, there has been a decade-long search for an online algorithm which is scalable, that is, (1 + ε)-speed O(1)-competitive for any fixed ε > 0. In this article, we give the first analysis of an online scalable algorithm. © 2012 ACM.",,Mathematical techniques; Broadcast scheduling; Flow-time; On-line algorithms; Scalable algorithms; Algorithms
Succinct ordinal trees based on tree covering,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870222953&doi=10.1145%2f2344422.2344432&partnerID=40&md5=3aa23c59d92940ca1e02561f209c80fb,"Various methods have been used to represent a tree on n nodes in essentially the information-theoretic minimum space while supporting various navigational operations in constant time, but different representations usually support different operations. Our main contribution is a succinct representation of ordinal trees, based on that of Geary et al. [2006], that supports all the navigational operations supported by various succinct tree representations while requiring only 2n+ o(n) bits. It also supports efficient level-order traversal, a useful ordering previously supported only with a very limited set of operations. Our second contribution expands on the notion of a single succinct representation supporting more than one traversal ordering, by showing that our method supports two other encoding schemes as abstract data types. In particular, it supports extracting a word (O(lg n) bits) of the balanced parenthesis sequence or depth first unary degree sequence in O( f (n)) time, using at most n/ f (n)+o(n) additional bits, for any f (n) in O(lg n) and Ω (1). © 2012 ACM.",,Forestry; Information Retrieval; Trees; Information theory; Navigation; Constant time; Degree sequence; Depth first; Encoding schemes; Succinct representation; Tree representation; Forestry
Kernel(s) for problems with no kernel: On out-trees with many leaves,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870184251&doi=10.1145%2f2344422.2344428&partnerID=40&md5=d8807134f6d0261cd9d723459d37e9f3,"The k-LEAF OUT-BRANCHING problem is to find an out-branching, that is a rooted oriented spanning tree, with at least k leaves in a given digraph. The problem has recently received much attention from the viewpoint of parameterized algorithms. Here, we take a kernelization based approach to the k-LEAF-OUT-BRANCHING problem. We give the first polynomial kernel for ROOTED k-LEAF-OUT-BRANCHING, a variant of k-LEAF-OUTBRANCHING where the root of the tree searched for is also a part of the input. Our kernel with O(k3) vertices is obtained using extremal combinatorics. For the k-LEAF-OUT-BRANCHING problem, we show that no polynomial-sized kernel is possible unless coNP is in NP/poly. However, our positive results for ROOTED k-LEAF-OUT-BRANCHING immediately imply that the seemingly intractable k-LEAF-OUT-BRANCHING problem admits a data reduction to n independent polynomialsized kernels. These two results, tractability and intractability side by side, are the first ones separating Karp kernelization from Turing kernelization. This answers affirmatively an open problem regarding ""cheat kernelization"" raised by Mike Fellows and Jiong Guo independently. © 2012 ACM.",,Algorithms; Mathematical techniques; Extremal combinatorics; Kernelization; Parameterized algorithm; Polynomial kernels; Spanning tree; Data reduction
Fully dynamic randomized algorithms for graph spanners,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870158634&doi=10.1145%2f2344422.2344425&partnerID=40&md5=33d26deca22134e3fe332a772663304e,"Spanner of an undirected graph G = (V, E) is a subgraph that is sparse and yet preserves all-pairs distances approximately. More formally, a spanner with stretch t ∈ N is a subgraph (V, ES), ES ⊆ E such that the distance between any two vertices in the subgraph is at most t times their distance in G. Though G is trivially a t-spanner of itself, the research as well as applications of spanners invariably deal with a t-spanner that has as small number of edges as possible. We present fully dynamic algorithms for maintaining spanners in centralized as well as synchronized distributed environments. These algorithms are designed for undirected unweighted graphs and use randomization in a crucial manner. Our algorithms significantly improve the existing fully dynamic algorithms for graph spanners. The expected size (number of edges) of a t-spanner maintained at each stage by our algorithms matches, up to a polylogarithmic factor, the worst case optimal size of a t-spanner. The expected amortized time (or messages communicated in distributed environment) to process a single insertion/deletion of an edge by our algorithms is close to optimal. © 2012 ACM.",,Algorithms; Optimization; Amortized time; Distributed environments; Dynamic algorithm; Optimal size; Poly-logarithmic factors; Randomized Algorithms; Subgraphs; Undirected graph; Unweighted graphs; Graph theory
Replacement paths and k simple shortest paths in unweighted directed graphs,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870157665&doi=10.1145%2f2344422.2344423&partnerID=40&md5=0ec4b869be1727913e103c69a4fc06fb,"Let G = (V, E) be a directed graph and let P be a shortest path from s to t in G. In the replacement paths problem, we are required to find, for every edge e on P, a shortest path from s to t in G that avoids e. The only known algorithm for solving the problem, even for unweighted directed graphs, is the trivial algorithm in which each edge on the path, in its turn, is excluded from the graph and a shortest paths tree is computed from s. The running time is O(mn+ n2 log n). The replacement paths problem is strongly motivated by two different applications: (1) The fastest algorithm to compute the k simple shortest paths between s and t in directed graphs [Yen 1971; Lawler 1972] computes the replacement paths between s and t. Its running time is Õ (mnk). (2) The replacement paths problem is used to compute the Vickrey pricing of edges in a distributed network. It was raised as an open problem by Nisan and Ronen [2001] whether it is possible to compute the Vickrey pricing faster than n computations of a shortest paths tree. In this article we present the first nontrivial algorithm for computing replacement paths in unweighted directed graphs (and in graphs with small integer weights). Our algorithm is Monte-Carlo and its running time is ̃O(m √ n). This result immediately improves the running time of the two applications mentioned above in a factor of √ n. We also show how to reduce the problem of computing k simple shortest paths between s and t to O(k) computations of a second simple shortest path from s to t each time in a different subgraph of G. The importance of this result is that computing a second simple shortest path may turn out to be an easier problem than computing the replacement paths, thus, we can focus our efforts to improve the k simple shortest paths algorithm in obtaining a faster algorithm for the second shortest path problem. © 2012 ACM.",,Forestry; Mathematics; Problem Solving; Trees; Algorithms; Directed graphs; Forestry; Problem solving; Distributed networks; Integer weights; MONTE CARLO; Non-trivial algorithms; Replacement paths; Running time; Shortest path; Shortest path problem; Shortest Paths Tree; Subgraphs; Unweighted directed graphs; Trees (mathematics)
Range searching on uncertain data,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870196532&doi=10.1145%2f2344422.2344433&partnerID=40&md5=16db8cc02848faf387b081294bef44a1,"Querying uncertain data has emerged as an important problem in data management due to the imprecise nature of many measurement data. In this article, we study answering range queries over uncertain data. Specifically, we are given a collection P of n uncertain points in ℝ, each represented by its one-dimensional probability density function (pdf). The goal is to build a data structure on P such that, given a query interval I and a probability threshold τ , we can quickly report all points of P that lie in I with probability at least τ . We present various structures with linear or near-linear space and (poly)logarithmic query time. Our structures support pdf's that are either histograms or more complex ones such as Gaussian or piecewise algebraic. © 2012 ACM.",,Information management; Probability density function; Gaussians; Measurement data; Piece-wise; Probability density function (pdf); Probability threshold; Query time; Range query; Range searching; Uncertain datas; Data structures
An FPTAS for the minimum total weighted tardiness problem with a fixed number of distinct due dates,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870197733&doi=10.1145%2f2344422.2344430&partnerID=40&md5=14b97ce5e460058658f6992754ef39ba,"Given a sequencing of jobs on a single machine, each one with a weight, processing time, and a due date, the tardiness of a job is the time needed for its completion beyond its due date. We present an FPTAS for the basic scheduling problem of minimizing the total weighted tardiness when the number of distinct due dates is fixed. Previously, an FPTAS was known only for the case where all jobs have a common due date. © 2012 ACM.",,Algorithms; Common due date; Due dates; Fixed numbers; Processing time; Scheduling problem; Total-weighted tardiness; Mathematical techniques
Improved algorithms for orienteering and related problems,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864832026&doi=10.1145%2f2229163.2229167&partnerID=40&md5=d3849d4b7c6c21ae6c81c49837d327e5,"In this article, we consider the orienteering problem in undirected and directed graphs and obtain improved approximation algorithms. The point to point-orienteering problem is the following: Given an edge-weighted graph G = (V, E) (directed or undirected), two nodes s, t ε V and a time limit B, find an s-t walk in G of total length at most B that maximizes the number of distinct nodes visited by the walk. This problem is closely related to tour problems such as TSP as well as network design problems such as k-MST. Orienteering with time-windows is the more general problem in which each node v has a specified time-window [R(v), D(υ)] and a node v is counted as visited by the walk only if v is visited during its time-window. We design new and improved algorithms for the orienteering problem and orienteering with time-windows. Our main results are the following: -A (2 + ε) approximation for orienteering in undirected graphs, improving upon the 3-approximation of Bansal et al. [2004. -An O(log 2 OPT) approximation for orienteering in directed graphs, where OPT ≤ n is the number of vertices visited by an optimal solution. Previously, only a quasipolynomial-time algorithm due to Chekuri and PáL al [2005] achieved a polylogarithmic approximation (a ratio of O(logOPT)). -Given an α approximation for orienteering, we show an O(α · max{logOPT, log (Mathematical Equation Presented) }) approximation for orienteering with time-windows, where L max and L min are the lengths of the longest and shortest timewindows respectively. © 2012 ACM.",Approximation algorithms; K-stroll; Orienteering; Orienteering with timewindows,Directed graphs; Edge-weighted graph; K-stroll; Mathematical equations; Network design problems; Optimal solutions; Orienteering; Orienteering problem; Polylogarithmic approximation; Time windows; Total length; Undirected graph; Approximation algorithms
Counting occurrences for a finite set of words: Combinatorial methods,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864839856&doi=10.1145%2f2229163.2229175&partnerID=40&md5=7ffd931588d4a7c0cd0af172665c5ca9,"In this article, we provide the multivariate generating function counting texts according to their length and to the number of occurrences of words from a finite set. The application of the inclusion-exclusion principle to word counting due to Goulden and Jackson [1979, 1983] is used to derive the result. Unlike some other techniques which suppose that the set of words is reduced (i.e., where no two words are factor of one another), the finite set can be chosen arbitrarily. Noonan and Zeilberger [1999] already provided a MAPLE package treating the nonreduced case, without giving an expression of the generating function or a detailed proof. We provide a complete proof validating the use of the inclusion-exclusion principle. Some formulæ for expected values, variance, and covariance for number of occurrences when considering two arbitrary sets of finite words are given as an application of our methodology. © 2012 ACM.",Aho-Corasick automaton; Generating functions; Inclusion-exclusion; Word statistics,Algorithms; Mathematical techniques; Aho-Corasick; Arbitrary sets; Combinatorial method; Expected values; Finite set; Finite words; Generating functions; Inclusion-exclusion; Jackson; Set theory
Polynomial-time algorithms for minimum energy scheduling,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864844385&doi=10.1145%2f2229163.2229170&partnerID=40&md5=7fa7bea0c9c860adacb197ab25e17f20,"The aim of power management policies is to reduce the amount of energy consumed by computer systems while maintaining a satisfactory level of performance. One common method for saving energy is to simply suspend the system during idle times. No energy is consumed in the suspend mode. However, the process of waking up the system itself requires a certain fixed amount of energy, and thus suspending the system is beneficial only if the idle time is long enough to compensate for this additional energy expenditure. In the specific problem studied in the article, we have a set of jobs with release times and deadlines that need to be executed on a single processor. Preemptions are allowed. The processor requires energy L to be woken up and, when it is on, it uses one unit of energy per one unit of time. It has been an open problem whether a schedule minimizing the overall energy consumption can be computed in polynomial time. We solve this problem in positive, by providing an O(n 5)-time algorithm. In addition we provide an O(n4)-time algorithm for computing the minimum energy schedule when all jobs have unit length. © 2012 ACM.",Dynamic programming; Job scheduling; Minimizing energy consumption,Algorithms; Dynamic programming; Energy utilization; Polynomial approximation; Scheduling; Energy expenditure; Idle time; Job scheduling; Minimum energy; Polynomial-time; Polynomial-time algorithms; Power managements; Release time; Saving energy; Single processors; Specific problems; Time algorithms; Problem solving
I/O-efficient shortest path algorithms for undirected graphs with random or bounded edge lengths,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864840344&doi=10.1145%2f2229163.2229166&partnerID=40&md5=f80fc0286d89c90ca05a8e2a4a3e16c6,"We present I/O-efficient single-source shortest path algorithms for undirected graphs. Our main result is an algorithm with I/O complexity O(Mathematical Equation Presented) on graphs with n vertices, m edges, and arbitrary edge lengths between 1 and L; MST(n, m) denotes the I/O complexity of computing a minimum spanning tree; B denotes the disk block size. If the edge lengths are drawn uniformly at random from (0, 1], the expected I/O complexity of the algorithm is O (Mathematical Equation Presented). A simpler algorithm has expected I/O complexity O (Mathematical Equation Presented) for uniformly random edge lengths. © 2012 ACM.",Graph algorithms; I/O-efficient algorithms; Memory hierarchies; Shortest path algorithms,Algorithms; Disk block; Edge length; Graph algorithms; I/O-complexity; Mathematical equations; Memory hierarchy; Minimum spanning trees; Random edges; Shortest path algorithms; SIMPLER algorithms; Undirected graph; Graph theory
The speed of convergence in congestion games under best-response dynamics,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864834114&doi=10.1145%2f2229163.2229169&partnerID=40&md5=1f18a61af371e61fea723b71e4991e39,"We investigate the speed of convergence of best response dynamics to approximately optimal solutions in congestion games with linear delay functions. In Ackermann et al. [2008] it has been shown that the convergence time of such dynamics to Nash equilibrium may be exponential in the number of players n. Motivated by such a negative result, we focus on the study of the states (not necessarily being equilibria) reached after a limited number of players' selfish moves, and we show that Θ(nlog log n) best responses are necessary and sufficient to achieve states that approximate the optimal solution by a constant factor, under the assumption that every O(n) steps each player performs a constant (and nonnull) number of best responses. We show that such result is tight also for the simplest case of singleton congestion games. © 2012 ACM.",Best response dynamics; Congestion games,Optimal systems; Best response; Congestion Games; Constant factors; Convergence time; Linear delay function; Nash equilibria; Optimal solutions; Speed of convergence; Dynamics
Scalably scheduling processes with arbitrary speedup curves,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864844835&doi=10.1145%2f2229163.2229172&partnerID=40&md5=462bab02cc0220f88de0fb265a5c16f8,We give a scalable ((1+ε)-speed O(1)-competitive) nonclairvoyant algorithm for scheduling jobswith sublinear nondecreasing speedup curves on multiple processors with the objective of average response time. © 2012 ACM.,Multiprocessor; Scheduling,Scheduling; Multiple processors; Multiprocessor; Non-clairvoyant algorithms; Scheduling process; Sublinear; Scheduling algorithms
Testing nilpotence of galois groups in polynomial time,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864831013&doi=10.1145%2f2229163.2229176&partnerID=40&md5=03305da2c0875044a948fdfa95fa66e5,"We give the first polynomial-time algorithm for checking whether the Galois group Gal ( f ) of an input polynomial f (X) ε f[X] is nilpotent: the running time of our algorithm is bounded by a polynomial in the size of the coefficients of f and the degree of f. Additionally, we give a deterministic polynomial-time algorithm that, when given as input a polynomial f (X) ε f[X] with nilpotent Galois group, computes for each prime factor p of #Gal (f), a polynomial gp(X) ε f[X] whose Galois group of is the p-Sylow subgroup of Gal (f). © 2012 ACM.",Galois group; Nilpotence; Polynomial time; Univariate polynomials,Algorithms; Polynomial approximation; Galois group; Input polynomials; Nilpotence; Nilpotent; Polynomial-time; Polynomial-time algorithms; Prime factors; Running time; Univariate; Algebra
Tight approximation algorithms for scheduling with fixed jobs and nonavailability,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864859146&doi=10.1145%2f2229163.2229171&partnerID=40&md5=06e446582a8da49caf1f16d7b656e2ce,"We study two closely related problems in nonpreemptive scheduling of jobs on identical parallel machines. In these two settings there are either fixed jobs or nonavailability intervals during which the machines are not available; in both cases, the objective is to minimize the makespan. Both formulations have different applications, for example, in turnaround scheduling or overlay computing. For both problems we contribute approximation algorithms with an improved ratio of 3/2. For scheduling with fixed jobs, a lower bound of 3/2 on the approximation ratio has been obtained by Scharbrodt et al. [1999]; for scheduling with nonavailability we provide the same lower bound. We use dual approximation, creation of a gap structure, and a PTAS for the multiple subset sum problem, combined with a postprocessing step to assign large jobs. © 2012 ACM.",Approximation; Optimization; Scheduling,Approximation algorithms; Business machines; Optimization; Scheduling algorithms; Approximation; Approximation ratios; Fixed jobs; Gap structures; Identical parallel machines; Lower bounds; Makespan; Nonpreemptive scheduling; Subset sum problems; Scheduling
Smoothed analysis of left-to-right maxima with applications,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864839789&doi=10.1145%2f2229163.2229174&partnerID=40&md5=9ccfc5c3c90879d0fed81ede3c74e364,"A left-to-right maximum in a sequence of n numbers s1, sn is a number that is strictly larger than all preceding numbers. In this article we present a smoothed analysis of the number of left-to-right maxima in the presence of additive random noise. We show that for every sequence of nnumbers si ε [0, 1] that are perturbed by uniform noise from the interval [-ε, ε], the expected number of left-to-right maxima is Θ (Mathematical Equation Presented) for ε > 1/n. For Gaussian noise with standard deviation σ we obtain a bound of O((log 3/2 n)/σ + log n). We apply our results to the analysis of the smoothed height of binary search trees and the smoothed number of comparisons in the quicksort algorithm and prove bounds of Θ (Mathematical Equation Presented), respectively, for uniform random noise from the interval [-ε, ε]. Our results can also be applied to bound the smoothed number of points on a convex hull of points in the two-dimensional plane and to smoothed motion complexity, a concept we describe in this article. We bound how often one needs to update a data structure storing the smallest axis-aligned box enclosing a set of points moving in d-dimensional space. © 2012 ACM.",Binary search trees; Convex hull; Motion complexity; Quicksort; Smoothed analysis,Computation; Forestry; Geometry; Mathematics; Trees; Binary trees; Computational geometry; Forestry; Gaussian noise (electronic); Binary search trees; Convex hull; Motion complexity; Quicksort; Smoothed analysis; Trees (mathematics)
Santa claus meets hypergraph matchings,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864852493&doi=10.1145%2f2229163.2229168&partnerID=40&md5=6c99a12d75768ed0607fc3accf439671,"We consider the restricted assignment version of the problem of max-min fair allocation of indivisible goods, also known as the Santa Claus problem. There are mitems and n players. Every item has some nonnegative value, and every player is interested in only some of the items. The goal is to distribute the items to the players in a way that maximizes the minimum of the sum of the values of the items given to any player. It was previously shown via a nonconstructive proof that uses the Lovász local lemma that the integrality gap of a certain configuration LP for the problem is no worse than some (unspecified) constant. This gives a polynomial-time algorithm to estimate the optimum value of the problem within a constant factor, but does not provide a polynomial-time algorithm for finding a corresponding allocation. We use a different approach to analyze the integrality gap. Our approach is based upon local search techniques for finding perfect matchings in certain classes of hypergraphs. As a result, we prove that the integrality gap of the configuration LP is no worse than (Mathematical Equation Presented). Our proof provides a local search algorithm which finds the corresponding allocation, but is nonconstructive in the sense that this algorithm is not known to converge to a local optimum in a polynomial number of steps. © 2012 ACM.",Bipartite hypergraphs; Hypergraph matchings; Santa Claus problem,Graph theory; Polynomials; Constant factors; Fair allocation; Hyper graph; Indivisible good; Integrality gaps; Local lemmata; Local optima; Local search algorithm; Local search techniques; Matchings; Mathematical equations; Max-min; Optimum value; Perfect matchings; Polynomial number; Polynomial-time algorithms; Santa Claus problem; Algorithms
Expansion properties of (secure) wireless networks,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864849762&doi=10.1145%2f2229163.2229165&partnerID=40&md5=31b56f6c47fab42cf95fd909b29dd019,"We show that some topologies arising naturally in the context of wireless networking are low-degree, expander graphs. © 2012 ACM.",Ad hoc networks; Bluetooth; Distributed algorithms; Expander graphs; Random key predistribution,Ad hoc networks; Algorithms; Bluetooth; Mathematical techniques; Parallel algorithms; Expander graphs; Expansion properties; Key pre-distribution; Wireless networking; Topology
Assignment problem in content distribution networks: Unsplittable hard-capacitated facility location,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864864465&doi=10.1145%2f2229163.2229164&partnerID=40&md5=076180d1762ecc31a892b947389a0a48,"In a Content Distribution Network (CDN), there are mservers storing the data; each of them has a specific bandwidth. All the requests from a particular client should be assigned to one server because of the routing protocol used. The goal is to minimize the total cost of these assignments-cost of each is proportional to the distance between the client and the server as well as the request size-while the load on each server is kept below its bandwidth limit. When each server also has a setup cost, this is an unsplittable hard-capacitated facility location problem. As much attention as facility location problems have received, there has been no nontrivial approximation algorithm when we have hard capacities (i.e., there can only be one copy of each facility whose capacity cannot be violated) and demands are unsplittable (i.e., all the demand from a client has to be assigned to a single facility). We observe it is NP-hard to approximate the cost to within any bounded factor in this case. Thus, for an arbitrary constantε > 0, we relax the capacities to a 1+ε factor. For the case where capacities are almost uniform, we give a bicriteria O(log n, 1+ε)-approximation algorithm for generalmetrics and a (1+ε, 1+ε)-approximation algorithm for treemetrics. A bicriteria (α, β)-approximation algorithm produces a solution of cost at most α times the optimum, while violating the capacities by no more than a β factor. We can get the same guarantees for nonuniform capacities if we allow quasipolynomial running time. In our algorithm, some clients guess the facility they are assigned to, and facilities decide the size of the clients they serve. A straightforward approach results in exponential running time. When costs do not satisfy metricity, we show that a 1.5 violation of capacities is necessary to obtain any approximation. It is worth noting that our results generalize bin packing (zero connection costs and facility costs equal to one), knapsack (single facility with all costs being zero), minimummakespan scheduling for relatedmachines (all connection costs being zero), and some facility location problems. © 2012 ACM.",Approximation algorithm; Facility location; Network; PTAS,Bandwidth; Costs; Networks (circuits); Site selection; Arbitrary constants; Assignment problems; Bandwidth limit; Bi-criteria; Bin packing; Connection-costs; Content distribution networks; Exponential running time; Facility location problem; Facility locations; NP-hard; PTAS; Quasi-poly-nomial; Running time; Setup costs; Total costs; Approximation algorithms
"Entropy, triangulation, and point location in planar subdivisions",2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864863491&doi=10.1145%2f2229163.2229173&partnerID=40&md5=f365fc2932aa4200d2cd49f91db00292,"A data structure is presented for point location in connected planar subdivisions when the distribution of queries is known in advance. The data structure has an expected query time that is within a constant factor of optimal. More specifically, an algorithm is presented that preprocesses a connected planar subdivision G of size n and a query distribution D to produce a point location data structure for G. The expected number of point-line comparisons performed by this data structure, when the queries are distributed according to D, is (Mathematical Equation Presented) + O( (Mathematical Equation Presented) 1/2 + 1) where (Mathematical Equation Presented)(G, D) is a lower bound on the expected number of point-line comparisons performed by any linear decision tree for point location in Gunder the query distribution D. The preprocessing algorithm runs in O(nlog n) time and produces a data structure of size O(n). These results are obtained by creating a Steiner triangulation of G that has near-minimum entropy. © 2012 ACM.",Computational geometry; Minimum-entropy triangulation; Point location,Algorithms; Computational geometry; Decision trees; Entropy; Mechanical engineering; Triangulation; Constant factors; Linear decision trees; Lower bounds; Mathematical equations; Planar subdivision; Point location; Pre-processing algorithms; Query distributions; Query time; Data structures
The price of anarchy in network creation games,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860273124&doi=10.1145%2f2151171.2151176&partnerID=40&md5=f8150fb2bce9eeb6ce7973363bcba450,"We study Nash equilibria in the setting of network creation games introduced recently by Fabrikant, Luthra, Maneva, Papadimitriou, and Shenker. In this game we have a set of selfish node players, each creating some incident links, and the goal is to minimize α times the cost of the created links plus sum of the distances to all other players. Fabrikant et al. proved an upper bound O(√α) on the price of anarchy: the relative cost of the lack of coordination. Albers, Eilts, Even-Dar, Mansour, and Roditty show that the price of anarchy is constant for α = O(√n) and for α ≥ 12n[lgn], and that the price of anarchy is 15(1 + (min{α/n, n 2/alpha;}) 1/3) for any α. The latter bound shows the first sublinear worst-case bound, O(n 1/3), for all α. But no better bound is known for α between ω(√n) and o(nlgn). Yet α ≈ n is perhaps the most interesting range, for it corresponds to considering the average distance (instead of the sum of distances) to other nodes to be roughly on par with link creation (effectively dividing α by n). In this article, we prove the first o(n ε) upper bound for general α, namely 2 (√lgn). We also prove a constant upper bound for α = O(n 1-ε) for any fixed ε ≤ 0, substantially reducing the range of α for which constant bounds have not been obtained. Along the way, we also improve the constant upper bound by Albers et al. (with the lead constant of 15) to 6 for α < (n/2) 1/2 and to 4 for α < (n/2) 1/3. Next we consider the bilateral network variant of Corbo and Parkes, in which links can be created only with the consent of both endpoints and the link price is shared equally by the two. Corbo and Parkes show an upper bound of O(√α) and a lower bound of Ω(lgα) for α ≤ n. In this article, we show that in fact the upper bound O(√α) is tight for α ≤ n, by proving a matching lower bound of Ω(√α). For α > n, we prove that the price of anarchy is Θ(n/√α). Finally we introduce a variant of both network creation games, in which each player desires to minimize α times the cost of its created links plus the maximum distance (instead of the sum of distances) to the other players. This variant of the problem is naturally motivated by considering the worst case instead of the average case. Interestingly, for the original (unilateral) game, we show that the price of anarchy is at most 2 for α ≥ n, O(min{4√lgn, (n/α) 1/3}) for 2√lgn ≥ α ≥ n, and O(n 2/α) for α > 2√lgn. For the bilateral game, we prove matching upper and lower bounds of Θ(n/α+1) for α ≤ n, and an upper bound of 2 for α > n. © 2012 ACM.",Nash equilibrium; Network design; Price of anarchy; Routing,Telecommunication networks; Average case; Average Distance; Lower bounds; Maximum distance; Nash equilibria; Network creation; Network design; Price of anarchy; Routing; Selfish node; Sublinear; Sum of distances; Upper and lower bounds; Upper Bound; Costs
Multidimensional online tracking,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860274212&doi=10.1145%2f2151171.2151175&partnerID=40&md5=f4f4db7326f693776b51909b4c42f880,"We propose and study a new class of online problems, which we call online tracking. Suppose an observer, say Alice, observes a multivalued function f: ℤ + → ℤ d over time in an online fashion, that is, she only sees f(t) for t ≥ t now where t now is the current time. She would like to keep a tracker, say Bob, informed of the current value of f at all times. Under this setting, Alice could send new values of f to Bob from time to time, so that the current value of f is always within a distance of A to the last value received by Bob. We give competitive online algorithms whose communication costs are compared with the optimal offline algorithm that knows the entire f in advance. We also consider variations of the problem where Alice is allowed to send predictions to Bob, to further reduce communication for well-behaved functions. These online tracking problems have a variety of application, ranging from sensor monitoring, location-based services, to publish/subscribe systems. © 2012 ACM.",Online tracking,Location based services; Communication cost; Multivalued functions; Off-line algorithm; On-line algorithms; On-line fashion; On-line tracking; Online problems; Publish/Subscribe system; Sensor monitoring; Communication
A precise analysis of Cuckoo hashing,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860267754&doi=10.1145%2f2151171.2151174&partnerID=40&md5=c0d3aadd5e498a0e7c81ab5561a1fffc,"Cuckoo hashing was introduced by Pagh and Rodler in 2001. Its main feature is that it provides constant worst-case search time. The aim of this article is to present a precise average case analysis of Cuckoo hashing. In particular, we determine the probability that Cuckoo hashing produces no conflicts and give an upper bound for the construction time, that is linear in the size of the table. The analysis rests on a generating function approach to the so called Cuckoo Graph, a random bipartite graph, and an application of a double saddle point method to obtain asymptotic expansions. Furthermore, we provide some results concerning the structure of these kinds of random graphs. Our results extend the analysis of Devroye and Morin [2003]. Additionally, we provide numerical results confirming the mathematical analysis. © 2012 ACM.",Cuckoo hashing; Generating functions; Hashing; Random bipartite graphs; Saddle point method,Asymptotic analysis; Bipartite graphs; Cuckoo hashing; Generating functions; Hashing; Saddlepoint method; Graph theory
Elimination graphs,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860287086&doi=10.1145%2f2151171.2151177&partnerID=40&md5=319e0da2bf68515f23a4ee33be2f4701,"In this article we study graphs with inductive neighborhood properties. Let P bea graph property, a graph G = (V, E) with n vertices is said to have an inductive neighborhood property with respect to P if there is an ordering of vertices ν 1,⋯, ν n such that the property P holds on the induced subgraph G[N(ν i) ∩ Vi], where N(ν i) is the neighborhood of ν i and Vi = {ν i,⋯, ν n}. It turns out that if we take Pasa graph with maximum independent set size no greater than k, then this definition gives a natural generalization of both chordal graphs and (k + 1)-claw-free graphs. We refer to such graphs as inductive k-independent graphs. We study properties of such families of graphs, and we show that several natural classes of graphs are inductive k-independent for small k. In particular, any intersection graph of translates of a convex object in a two dimensional plane is an inductive 3-independent graph; furthermore, any planar graph is an inductive 3-independent graph. For any fixed constant k, we develop simple, polynomial time approximation algorithms for inductive k-independent graphs with respect to several well-studied NP-complete problems. Our generalized formulation unifies and extends several previously known results. © 2012 ACM.",Approximation algorithm; Graph class; Greedy algorithm; Independent set; Local ratio; Structure property; Time complexity,Approximation algorithms; Computational complexity; Graphic methods; Graph class; Greedy algorithms; Independent set; Local ratio; Structure property; Time complexity; Graph theory
Succinct geometric indexes supporting point location queries,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860313483&doi=10.1145%2f2151171.2151173&partnerID=40&md5=5e4e81c72b1d9f58c6c443132ec203f8,"We propose designing data structures called succinct geometric indexes of negligible space (more precisely, o(n) bits) that support geometric queries in optimal time, by taking advantage of the n points in the dataset permuted and stored elsewhere as a sequence. Our first and main result is a succinct geometric index that can answer point location queries, a fundamental problem in computational geometry, on planar triangulations in O (lg n) time. We also design three variants of this index. The first supports point location using lg n+2 √lgn+ O (lg 1/4 n) point-line comparisons. The second supports point location in o (lgn) time when the coordinates are integers bounded by U. The last variant can answer point location queries in O(H + 1) expected time, where H is the entropy of the query distribution. These results match the query efficiency of previous point location structures that occupy O(n) words or O(nlgn) bits, while saving drastic amounts of space. We generalize our succinct geometric index to planar subdivisions, and design indexes for other types of queries. Finally, we apply our techniques to design the first implicit data structures that support point location in O (lg 2 n) time. © 2012 ACM.",Geometric data structures; Geometric queries; Implicit data structures; Membership; Planar subdivisions; Planar triangulations; Point location; Succinct data structures; Succinct geometric indexes; Vertical ray shooting,Computational geometry; Data structures; Design; Triangulation; Geometric data structures; Geometric queries; Membership; Planar subdivision; Planar triangulation; Point location; Ray shooting; Succinct data structure; Succinct geometric indexes; Query processing
On approximating multicriteria TSP,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860304240&doi=10.1145%2f2151171.2151180&partnerID=40&md5=978a89feec8850badddd6bb8b1d2ee29,"We present approximation algorithms for almost all variants of the multicriteria traveling salesman problem (TSP). First, we devise randomized approximation algorithms for multicriteria maximum traveling salesman problems (Max-TSP). For multicriteria Max-STSP where the edge weights have to be symmetric, we devise an algorithm with an approximation ratio of 2/3 - ε. For multicriteria Max-ATSP where the edge weights may be asymmetric, we present an algorithm with a ratio of 1/2 - ε. Our algorithms work for any fixed number k of objectives. Furthermore, we present a deterministic algorithm for bicriteria Max-STSP that achieves an approximation ratio of 7/27. Finally, we present a randomized approximation algorithm for the asymmetric multicriteria minimum TSP with triangle inequality (Min-ATSP). This algorithm achieves a ratio of logn+ ε. © 2012 ACM.",Approximation algorithms; Multicriteria optimization; Multiobjective optimization; Traveling salesman problem,Approximation algorithms; Multiobjective optimization; Approximation ratios; Bi-criteria; Deterministic algorithms; Edge weights; Fixed numbers; Maximum traveling salesman problem; Multi-criteria; Multicriteria optimization; Randomized approximation; Traveling salesman problems (TSP); Triangle inequality; Traveling salesman problem
How to trim a MST: A 2-approximation algorithm for minimum cost-tree cover,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860288388&doi=10.1145%2f2151171.2151179&partnerID=40&md5=4d3351584dfe4b2a340654cfd06183a5,"The minimum cost-tree cover problem is to compute a minimum cost-tree T in a given connected graph G with costs on the edges, such that the vertices spanned by T form a vertex cover for G. The problem is supposed to occur in applications of vertex cover and in edge-dominating sets when additional connectivity is required for solutions. Whereas a linear-time 2-approximation algorithm for the unweighted case has been known for quite a while, the best approximation ratio known for the weighted case is 3. Moreover, the 3-approximation algorithms for such cases are far from practical due to their inefficiency. In this article we present a fast, purely combinatorial 2-approximation algorithm for the minimum costtree cover problem. It constructs a good approximate solution by trimming some leaves within a minimum spanning tree (MST); and, to determine which leaves to trim, it uses both the primal-dual schema and an instance layering technique adapted from the local ratio method. © 2012 ACM.",Approximation algorithms; Local ratio technique; Primal-dual method; Tree cover,Approximation algorithms; Trees (mathematics); Approximate solution; Best approximations; Connected graph; Cover problem; Local ratio; Local ratio technique; Minimum spanning trees; Primal-dual methods; Primal-dual schemata; Tree cover; Vertex cover; Costs
The traveling salesman problem in bounded degree graphs,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860270503&doi=10.1145%2f2151171.2151181&partnerID=40&md5=034ecad1357edb978614b6d43651a109,"We show that the traveling salesman problem in bounded-degree graphs can be solved in time O((2 - ∈) n), where ∈ >; 0 depends only on the degree bound but not on the number of cities, n. The algorithm is a variant of the classical dynamic programming solution due to Bellman, and, independently, Held and Karp. In the case of bounded integer weights on the edges, we also give a polynomial-space algorithm with running time O((2 - ∈) n) on bounded-degree graphs. In addition, we present an analogous analysis of Ryser's algorithm for the permanent of matrices with a bounded number of nonzero entries in each column. © 2012 ACM.",Counting; Dynamic programming; Inclusion-exclusion; Permanent; Shearer's entropy lemma; Traveling salesman problem; Trimming,Algorithms; Dynamic programming; Trimming; Bounded degree graphs; Classical dynamics; Counting; Degree bounds; Inclusion-exclusion; Integer weights; Nonzero entries; Permanent; Programming solutions; Running time; Shearer's entropy lemma; Traveling salesman problem
Dotted interval graphs,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860277505&doi=10.1145%2f2151171.2151172&partnerID=40&md5=aabd9618a1020f03f647d4f562863c5a,"We introduce a generalization of interval graphs, which we call Dotted Interval Graphs (DIG). A dotted interval graph is an intersection graph of arithmetic progressions (dotted intervals). Coloring of dotted interval graphs naturally arises in the context of high throughput genotyping. We study the properties of dotted interval graphs, with a focus on coloring. We show that any graph is a DIG, but that DIGd graphs, that is, DIGs in which the arithmetic progressions have a jump of at most d, form a strict hierarchy. We show that coloring DIG d graphs is NP-complete even for d = 2. For any fixed d, we provide a 5/6d + o(d) approximation for the coloring of DIG d graphs. Finally, we show that finding the maximal clique in DIG d graphs is fixed parameter tractable in d. © 2012 ACM.",Approximation algorithms; Circular arc graph; Graph coloring; Graph theory; Intersection graph; Interval graph,Approximation algorithms; Graph theory; Arithmetic progressions; Circular-arc graph; Dotted interval graphs; Fixed parameters; Graph colorings; High throughput genotyping; Intersection graph; Interval graph; Maximal clique; NP Complete; Graphic methods
On the hardness of losing weight,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860304286&doi=10.1145%2f2151171.2151182&partnerID=40&md5=4f07c36e93257a719280b84503156d69,"We study the complexity of local search for the Boolean constraint satisfaction problem (CSP), in the following form: given a CSP instance, that is, a collection of constraints, and a solution to it, the question is whether there is a better (lighter, i.e., having strictly less Hamming weight) solution within a given distance from the initial solution. We classify the complexity, both classical and parameterized, of such problems by a Schaefer-style dichotomy result, that is, with a restricted set of allowed types of constraints. Our results show that there is a considerable amount of such problems that are NP-hard, but fixed-parameter tractable when parameterized by the distance. © 2012 ACM.",Complexity; Constraint satisfaction problem; Fixedparameter tractability; Local search,Algorithms; Boolean constraint; Complexity; Constraint satisfaction problems; Fixed-parameter tractability; Hamming weights; Initial solution; Local search; NP-hard; Parameterized; Mathematical techniques
On the query complexity of testing orientations for being Eulerian,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860266052&doi=10.1145%2f2151171.2151178&partnerID=40&md5=b8bd83fbe31cb8b5cb21c14451bc3969,"We consider testing directed graphs Eulerianity in the orientation model introduced in Halevy et al. [2005]. Despite the local nature of the Eulerian property, it turns out to be significantly harder to test than other properties studied in the orientation model. We show a nonconstant lower bound on the query complexity of 2-sided tests and a linear lower bound on the query complexity of 1-sided tests for this property. On the positive side, we give several 1-sided and 2-sided tests, including a sublinear query complexity 2-sided test, for general graphs. For special classes of graphs, including bounded-degree graphs and expander graphs, we provide improved results. In particular, we give a 2-sided test with constant query complexity for dense graphs, as well as for expander graphs with a constant expansion parameter. © 2012 ACM.",Eulerian orientations; Graph algorithms; Massively parameterized; Property testing,Algorithms; Mathematical techniques; Dense graphs; Eulerian; Eulerianity; Expander graphs; General graph; Graph algorithms; Lower bounds; Parameterized; Property-testing; Query complexity; Special class; Sub-linear queries; Graphic methods
Approximating minimum-cost connectivity problems via uncrossable bifamilies,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872462374&doi=10.1145%2f2390176.2390177&partnerID=40&md5=083b5e18082e2cdc2b6edc23a66ab76e,"We give approximation algorithms for the Survivable Network problem. The input consists of a graph G = (V, E) with edge/node-costs, a node subset S ⊆ V, and connectivity requirements {r(s, t) : s, t ∈ T ⊆ V}. The goal is to find a minimum cost subgraph H of G that for all s, t ∈ T contains r(s, t) pairwise edge-disjoint st-paths such that no two of them have a node in S \ {s, t} in common. Three extensively studied particular cases are: Edge-Connectivity Survivable Network (S = ∅), Node-Connectivity Survivable Network (S = V), and Element-Connectivity Survivable Network (r(s, t) = 0 whenever s ∈ S or t ∈ S). Let k = maxs,t∈T r(s, t). In Rooted Survivable Network, there is s ∈ T such that r(u, t) = 0 for all u ε= s, and in the Subset k-Connected Subgraph problem r(s, t) = k for all s, t ∈ T. © 2012 ACM.",,Approximation algorithms; Costs; Connectivity problems; Edge connectivity; Element connectivity; K-connected; Minimum cost; Node connectivity; Subgraphs; Survivable networks; Graph theory
Online optimization with uncertain information,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857186081&doi=10.1145%2f2071379.2071381&partnerID=40&md5=41c690ff5107d433eda4b0ee7d884339,"We introduce a new framework for designing online algorithms that can incorporate additional information about the input sequence, while maintaining a reasonable competitive ratio if the additional information is incorrect. Within this framework, we present online algorithms for several problems including allocation of online advertisement space, load balancing, and facility location. © 2012 ACM.",Competitive analysis; Online advertising; Online algorithms,Mathematical techniques; Competitive analysis; Competitive ratio; Facility locations; Input sequence; On-line algorithms; Online advertisements; Online advertising; Online optimization; Uncertain informations; Algorithms
Iterative expansion and color coding: An improved algorithm for 3D-matching,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863176149&doi=10.1145%2f2071379.2071385&partnerID=40&md5=23f1ac3e44483a0a29aca716873f70da,"The research in the parameterized 3D-MATCHING problem has yielded a number of new algorithmic techniques and an impressive list of improved algorithms. In this article, a new deterministic algorithm for the problem is developed that integrates and improves a number of known techniques, including greedy localization, dynamic programming, and color coding. The new algorithm, which either constructs a matching of k triples in a given triple set or correctly reports that no such a matching exists, runs in time O*(2.80 3k), improving a long list of previous algorithms for the problem. © 2012 ACM.",Color coding; Dynamic programming matching; Parameterized algorithms,Algorithms; Codes (symbols); Color; Dynamic programming; Three dimensional; Algorithmic techniques; Color coding; Deterministic algorithms; Dynamic programming matching; Improved algorithm; Matching problems; Parameterized; Parameterized algorithm; Color matching
Cache-oblivious algorithms,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857163513&doi=10.1145%2f2071379.2071383&partnerID=40&md5=dc49d22622ecf37b11ce709d5f5d2a94,"This article presents asymptotically optimal algorithms for rectangularmatrix transpose, fast Fourier transform (FFT), and sorting on computers with multiple levels of caching. Unlike previous optimal algorithms, these algorithms are cache oblivious: no variables dependent on hardware parameters, such as cache size and cache-line length, need to be tuned to achieve optimality. Nevertheless, these algorithms use an optimal amount of work and move data optimally among multiple levels of cache. For a cache with size M and cache-line length B where M = Ω(B 2), the number of cache misses for an m × n matrix transpose is θ(1 + mn/B). The number of cache misses for either an n-point FFT or the sorting of n numbers is θ(1 + (n/B)(1 + log Mn)). We also give a θ(mnp)-work algorithm to multiply an m× n matrix by an n × p matrix that incurs θ(1 + (mn+ np + mp)/B + mnp/B √ M) cache faults. We introduce an ""ideal-cache"" model to analyze our algorithms. We prove that an optimal cache-oblivious algorithm designed for two levels of memory is also optimal for multiple levels and that the assumption of optimal replacement in the ideal-cache model can be simulated efficiently by LRU replacement. We offer empirical evidence that cache-oblivious algorithms perform well in practice. © 2012 ACM.",Algorithm; Cache-oblivious; Caching; Fast Fourier transform; I/O complexity; Matrix multiplication; Matrix transpose; Sorting,Cache memory; Computer simulation; Fast Fourier transforms; Matrix algebra; Optimization; Sorting; Cache-oblivious; Caching; I/O complexity; MAtrix multiplication; Matrix transpose; Algorithms
Even faster exact bandwidth,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857178902&doi=10.1145%2f2071379.2071387&partnerID=40&md5=df6fa8427cb4da7e0d388cddb94ee751,"We deal with exact algorithms for BANDWIDTH, a long studied NP-hard problem. For a long time nothing better than the trivial O*(n!) 1 exhaustive search was known. In 2000, Feige and Kilian [Feige 2000] came up with a O*(10 n)-time and polynomial space algorithm. In this article we present a new algorithm that solves BANDWIDTH in O*(5 n) time and O*(2 n) space. Then, we take a closer look and introduce a major modification that makes it run in O(4.83 n) time with a cost of a O*(4 n) space complexity. This modification allowed us to perform the Measure & Conquer analysis for the time complexity which was not used for graph layout problems before. © 2012 ACM.",Bandwidth; Exact algorithms; Measure & Conquer,Algorithms; Computational complexity; Exact algorithms; Exhaustive search; Graph layout; NP-HARD problem; Polynomial space; Space complexity; Time complexity; Bandwidth
Adversarial queuing on the multiple access channel,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857152811&doi=10.1145%2f2071379.2071384&partnerID=40&md5=d15688d93b1ba814779ffe8883a41c9d,"We study deterministic broadcasting on multiple access channels when packets are injected continuously. The quality of service is considered in the framework of adversarial queuing. An adversary is determined by injection rate and burstiness, the latter denoting the number of packets that can be injected simultaneously in a round. We consider only injection rates that are less than 1. A protocol is stable when the numbers of packets in queues stay bounded at all rounds, and it is of fair latency when waiting times of packets in queues are O(burstiness/rate). For channels with collision detection, we give a full-sensing protocol of fair latency for injection rates that are at most 1/2(⌈lg n⌈+1), where n is the number of stations, and show that fair latency is impossible to achieve for injection rates that are ω(1/log n). For channels without collision detection, we present a full-sensing protocol of fair latency for injection rates that are at most 1/clg 2n, for some c > 0. We show that there exists an acknowledgment-based protocol that has fair latency for injection rates that are at most 1/cnlg 2 n , for some c > 0, and develop an explicit acknowledgment-based protocol of fair latency for injection rates that are at most 1/27n 2 ln n. Regarding impossibility to achieve just stability by restricted protocols, we prove that no acknowledgment-based protocol can be stable for injection rates larger than 3/1+lg n. © 2012 ACM.",Adversarial queuing; Deterministic protocol; Distributed broadcasting; Multiple access channel; Packet latency; Stability,Communication channels (information theory); Convergence of numerical methods; Information dissemination; Quality of service; Adversarial queuing; Burstiness; Collision detection; Deterministic broadcasting; Deterministic protocols; Injection rates; Multiple access channels; Packet latencies; Waiting time; Queueing theory
Improved fixed-parameter algorithms for minimum-flip consensus trees,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857182465&doi=10.1145%2f2071379.2071386&partnerID=40&md5=6548678620072afb696286f0b5bb7beb,"In computational phylogenetics, the problem of constructing a consensus tree for a given set of rooted input trees has frequently been addressed. In this article we study the MINIMUM-FLIP PROBLEM: the input trees are transformed into a binary matrix, and we want to find a perfect phylogeny for this matrix using a minimum number of flips, that is, corrections of single entries in the matrix. The graph-theoretical formulation of the problem is as follows: Given a bipartite graph G = (V t ∪ V c , E), the task is to find a minimum set of edge modifications such that the resulting graph has no induced path with four edges that starts and ends in V t, where V t corresponds to the taxa set and V c corresponds to the character set. We present two fixed-parameter algorithms for the MINIMUM-FLIP PROBLEM, one with running time O(4.83 k + poly(m, n)) and another one with running time O(4.42 k + poly(m, n)) for n taxa, m characters, k flips, and poly(m, n) denotes a polynomial function in m and n. Additionally, we discuss several heuristic improvements. We also report computational results on phylogenetic data. © 2012 ACM.",Consensus tree; Fixed-parameter algorithm; Phylogenetics,Algorithms; Biology; Forestry; Problem Solving; Binary trees; Biology; Character sets; Forestry; Graph theory; Heuristic algorithms; Matrix algebra; Binary matrix; Bipartite graphs; Computational results; Consensus tree; Edge modification; Fixed-parameter algorithms; Induced paths; matrix; Perfect phylogeny; Phylogenetics; Polynomial functions; Running time; Single entry; Parameter estimation
"Incremental cycle detection, topological ordering, and strong component maintenance",2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857148539&doi=10.1145%2f2071379.2071382&partnerID=40&md5=bcc2a626bcdc356b77ea390cf6596f7d,"We present two online algorithms for maintaining a topological order of a directed n-vertex acyclic graph as arcs are added, and detecting a cycle when one is created. Our first algorithm handles m arc additions in O(m 3/2) time. For sparse graphs (m/n = O(1)), this bound improves the best previous bound by a logarithmic factor, and is tight to within a constant factor among algorithms satisfying a natural locality property. Our second algorithm handles an arbitrary sequence of arc additions in O(n 5/2) time. For sufficiently dense graphs, this bound improves the best previous bound by a polynomial factor. Our bound may be far from tight: we show that the algorithm can take Ω(n 22 √ 2 lgn) time by relating its performance to a generalization of the k-levels problem of combinatorial geometry. A completely different algorithm running in θ(n2 log n) time was given recently by Bender, Fineman, and Gilbert. We extend both of our algorithms to the maintenance of strong components, without affecting the asymptotic time bounds. © 2012 ACM.",Arrangement; Cycle detection; Directed graphs; Dynamic algorithms; Halving intersection; Strong components; Topological order,Graph theory; Maintenance; Arrangement; Cycle detection; Directed graphs; Dynamic algorithms; Strong components; Topological order; Algorithms
Adaptive uncertainty resolution in bayesian combinatorial optimization problems,2012,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857149865&doi=10.1145%2f2071379.2071380&partnerID=40&md5=df7df9de89e2fded747b401355d31feb,"In several applications such as databases, planning, and sensor networks, parameters such as selectivity, load, or sensed values are known only with some associated uncertainty. The performance of such a system (as captured by some objective function over the parameters) is significantly improved if some of these parameters can be probed or observed. In a resource constrained situation, deciding which parameters to observe in order to optimize system performance, itself becomes an interesting and important optimization problem. This general problem is the focus of this article. One of the most important considerations in this framework is whether adaptivity is required for the observations. Adaptive observations introduce blocking or sequential operations in the system whereas nonadaptive observations can be performed in parallel. One of the important questions in this regard is to characterize the benefit of adaptivity for probes and observation. We present general techniques for designing constant factor approximations to the optimal observation schemes for several widely used scheduling and metric objective functions. We show a unifying technique that relates this optimization problem to the outlier version of the corresponding deterministic optimization. By making this connection, our technique shows constant factor upper bounds for the benefit of adaptivity of the observation schemes. We show that while probing yields significant improvement in the objective function, being adaptive about the probing is not beneficial beyond constant factors. © 2012 ACM.",Approximation algorithms; Bayesian optimization; Stochastic optimization,Approximation algorithms; Combinatorial optimization; Sensor networks; Adaptivity; Bayesian optimization; Combinatorial optimization problems; Constant factor approximation; Constant factors; Deterministic optimization; Objective functions; Optimal observation; Optimization problems; Resource-constrained; Sequential operations; Stochastic optimizations; Upper Bound; Constrained optimization
"Philippe flajolet, the father of analytic combinatorics",2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053509309&doi=10.1145%2f2000807.2000808&partnerID=40&md5=3541fd438f9ef7bee57b5ce1fd0ca130,[No abstract available],,
"Succinct indexes for strings, binary relations and multilabeled trees",2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053478956&doi=10.1145%2f2000807.2000820&partnerID=40&md5=b6f226a4ef25521e1d2f42cb0b8150c0,"We define and design succinct indexes for several abstract data types (ADTs). The concept is to design auxiliary data structures that ideally occupy asymptotically less space than the information-theoretic lower bound on the space required to encode the given data, and support an extended set of operations using the basic operators defined in the ADT. The main advantage of succinct indexes as opposed to succinct (integrated data/index) encodings is that we make assumptions only on the ADT through which the main data is accessed, rather than the way in which the data is encoded. This allows more freedom in the encoding of the main data. In this article, we present succinct indexes for various data types, namely strings, binary relations and multilabeled trees. Given the support for the interface of the ADTs of these data types, we can support various useful operations efficiently by constructing succinct indexes for them. When the operators in the ADTs are supported in constant time, our results are comparable to previous results, while allowing more flexibility in the encoding of the given data. Using our techniques, we design a succinct encoding that represents a string of length nover an alphabet of size σ usingn Hk(S)+lg σ•o(n)+O(nlg σ/lg lg lg σ) bits to support access/rank/select operations in o((lg lg σ)1+ε) time, for any fixed constant ε > 0. We also design a succinct text index using nH0(S) + O(nlg σ/lg lg σ) bits that supports finding all the occ occurrences of a given pattern of length m in O(mlg lg σ + occ lg n/lgε σ) time, for any fixed constant 0 > ε > 1. Previous results on these two problems either have a lg σ factor instead of lg lg σ in the running time, or are not compressed. Finally, we present succinct encodings of binary relations and multi-labeled trees that are more compact than previous structures. © 2011 ACM.",Binary relations; Compressed text indexes; Labeled trees; Multilabeled trees; Strings; Succinct data structures; Succinct indexes; Text indexing; Trees,Binary trees; Design; Encoding (symbols); Information theory; Plant extracts; Binary relation; Compressed text indexes; Labeled trees; Multilabeled trees; Strings; Succinct data structure; Succinct indexes; Text-indexing; Trees; Trees (mathematics)
A new upper bound 2.5545 on 2D online bin packing,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053540909&doi=10.1145%2f2000807.2000818&partnerID=40&md5=45d4f5e3958696ad0490cd6d326952fc,"The 2D Online Bin Packing is a fundamental problem in Computer Science and the determination of its asymptotic competitive ratio has research attention. In a long series of papers, the lower bound of this ratio has been improved from 1.808, 1.856 to 1.907 and its upper bound reduced from 3.25, 3.0625, 2.8596, 2.7834 to 2.66013. In this article, we rewrite the upper bound record to 2.5545. Our idea for the improvement is as follows. In 2002, Seiden and van Stee [Seiden and van Stee 2003] proposed an elegant algorithm called H. C, comprised of the Harmonic algorithm H and the Improved Harmonic algorithm C, for the two-dimensional online bin packing problem and proved that the algorithm has an asymptotic competitive ratio of at most 2.66013. Since the best known online algorithm for one-dimensional bin packing is the Super Harmonic algorithm [Seiden 2002], a natural question to ask is: could a better upper bound be achieved by using the Super Harmonic algorithm instead of the Improved Harmonic algorithm? However, as mentioned in Seiden and van Stee [2003], the previous analysis framework does not work. In this article, we give a positive answer for this question. A new upper bound of 2.5545 is obtained for 2-dimensional online bin packing. The main idea is to develop new weighting functions for the Super Harmonic algorithm and propose new techniques to bound the total weight in a rectangular bin. © 2011 ACM.",Bin packing problems; Competitive ratio; Online algorithms,Bins; Harmonic analysis; Harmonic functions; Asymptotic competitive ratio; Bin packing; Bin packing problem; Competitive ratio; Fundamental problem; Lower bounds; On-line algorithms; Online bin packing; Super-harmonic; Upper Bound; Weighting functions; Algorithms
Tight bounds and a fast FPT algorithm for directed max-leaf spanning tree,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053480516&doi=10.1145%2f2000807.2000812&partnerID=40&md5=25de648f246c94fd9fe71db5bb6bde0e,"An out-tree T of a directed graph D is a rooted tree subgraph with all arcs directed outwards from the root. An out-branching is a spanning out-tree. By l(D) and lS(D), we denote the maximum number of leaves over all out-trees and out-branchings of D, respectively. We give fixed parameter tractable algorithms for deciding whether ls(D) ≥ k and whether l(D) ≥ k for a digraph D on n vertices, both with time complexity 2 O(klog k) • nO(1). This answers an open question whether the problem for out-branchings is in FPT, and improves on the previous complexity of 2O(klog2 k) • nO(1) in the case of out-trees. To obtain the complexity bound in the case of out-branchings, we prove that when all arcs of D are part of at least one out-branching, l s(D) ≥ l(D)/3. The second bound we prove in this article states that for strongly connected digraphs D with minimum in-degree 3, l s(D) = (□n), where previously ls(D) = (3'n) was the best known bound. This bound is tight, and also holds for the larger class of digraphs with minimum in-degree 3 in which every arc is part of at least one out-branching. © 2011 ACM.",Directed graphs; Fixed parameter tractable; Maximum leaf,Algorithms; Plant extracts; Complexity bounds; Directed graphs; Fixed parameters; FPT algorithms; In-Degree; Maximum leaf; Rooted trees; Spanning tree; Strongly connected; Subgraphs; Tight bound; Time complexity; Tractable algorithms; Trees (mathematics)
Partial convex recolorings of trees and galled networks: Tight upper and lower bounds,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053475263&doi=10.1145%2f2000807.2000810&partnerID=40&md5=46a43038565f87d9712fffd0705a55d3,"A coloring of a graph is convex if the vertices that pertain to any color induce a connected subgraph; a partial coloring (which assigns colors to a subset of the vertices) is convex if it can be completed to a convex (total) coloring. Convex coloring has applications in fields such as phylogenetics, communication or transportation networks, etc. When a coloring of a graph is not convex, a natural question is how far it is from a convex one. This problem is denoted as convex recoloring (CR).While the initial works on CR defined and studied the problem on trees, recent efforts aim at either generalizing the underlying graphs or specializing the input colorings. In this work, we extend the underlying graph and the input coloring to partially colored galled networks. We show that although determining whether a coloring is convex on an arbitrary network is hard, it can be found efficiently on galled networks. We present a fixed parameter tractable algorithm that finds the recoloring distance of such a network whose running time is quadratic in the network size and exponential in that distance. This complexity is achieved by amortized analysis that uses a novel technique for contracting colored graphs that seems to be of independent interest. © 2011 ACM.",Convex recoloring; NP-hardness; Partially colored galled networks; Partially colored trees,Coloring; Plant extracts; Amortized analysis; Arbitrary networks; Convex recolorings; Fixed parameters; In-field; Network size; Novel techniques; NP-hardness; Partial coloring; Partially colored trees; Phylogenetics; Recoloring; Running time; Subgraphs; Tractable algorithms; Transportation network; Underlying graphs; Upper and lower bounds; Trees (mathematics)
Fully compressed suffix trees,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053483441&doi=10.1145%2f2000807.2000821&partnerID=40&md5=31e8a8f9358dc17cd80d952539c6463f,"Suffix trees are by far themost important data structure in stringology, with a myriad of applications in fields like bioinformatics and information retrieval. Classical representations of suffix trees require (nlog n) bits of space, for a string of size n. This is considerably more than the nlog 2 σ bits needed for the string itself, where σ is the alphabet size. The size of suffix trees has been a barrier to their wider adoption in practice. Recent compressed suffix tree representations require just the space of the compressed string plus (n) extra bits. This is already spectacular, but the linear extra bits are still unsatisfactory when σ is small as in DNA sequences. In this article, we introduce the first compressed suffix tree representation that breaks this (n)-bit space barrier. The Fully Compressed Suffix Tree (FCST) representation requires only sublinear space on top of the compressed text size, and supports a wide set of navigational operations in almost logarithmic time. This includes extracting arbitrary text substrings, so the FCST replaces the text using almost the same space as the compressed text. An essential ingredient of FCSTs is the lowest common ancestor (LCA) operation. We reveal important connections between LCAs and suffix tree navigation. We also describe how to make FCSTs dynamic, that is, support updates to the text. The dynamic FCST also supports several operations. In particular, it can build the static FCST within optimal space and polylogarithmic time per symbol. Our theoretical results are also validated experimentally, showing that FCSTs are very effective in practice as well. © 2011 ACM.",Compressed index; Data compression; Pattern matching; String algorithms; Suffix tree; Text processing,Bioinformatics; Data compression; Data structures; DNA sequences; Information retrieval; Pattern matching; Plant extracts; Text processing; Alphabet size; Classical representation; Compressed index; In-field; Logarithmic time; Lowest common ancestors; Optimal space; Per-symbol; Polylogarithmic time; String algorithms; Stringology; Sub-strings; Sublinear; Suffix-trees; Theoretical result; Trees (mathematics)
Fast computation of small cuts via cycle space sampling,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053482076&doi=10.1145%2f2000807.2000814&partnerID=40&md5=65361216cf3fbcc062d73ed87a3a75ed,"We describe a new sampling-based method to determine cuts in an undirected graph. For a graph (V, E), its cycle space is the family of all subsets of E that have even degree at each vertex. We prove that with high probability, sampling the cycle space identifies the cuts of a graph. This leads to simple new linear-time sequential algorithms for finding all cut edges and cut pairs (a set of 2 edges that form a cut) of a graph. In the model of distributed computing in a graph G = (V, E) with O(log |V|)-bit messages, our approach yields faster algorithms for several problems. The diameter of G is denoted by D, and the maximum degree by. We obtain simple O(D)-time distributed algorithms to find all cut edges, 2-edge-connected components, and cut pairs, matching or improving upon previous time bounds. Under natural conditions these new algorithms are universally optimal-that is, a (D)-time lower bound holds on every graph. We obtain a O(D + Δ/log |V|)-time distributed algorithm for finding cut vertices; this is faster than the best previous algorithm when Δ,D = O(√|V|). A simple extension of our work yields the first distributed algorithm with sub-linear time for 3-edge-connected components. The basic distributed algorithms are Monte Carlo, but they can be made Las Vegas without increasing the asymptotic complexity. In the model of parallel computing on the EREW PRAM, our approach yields a simple algorithm with optimal time complexity O(log V) for finding cut pairs and 3-edge-connected components. © 2011 ACM.",Distributed algorithms; Graph connectivity; Linear algebra; Parallel algorithms; Randomized algorithms; Universal optimality,Algorithms; Linear algebra; Optimization; Parallel algorithms; Parallel architectures; Asymptotic complexity; Cut edge; Cut vertex; Cycle space; Distributed algorithm; EREW PRAM; Fast computation; Graph connectivity; Graph G; High probability; Las Vegas; Lower bounds; Maximum degree; MONTE CARLO; Natural conditions; Optimal time complexity; Randomized Algorithms; Sampling-based method; Sequential algorithm; SIMPLE algorithm; Time bound; Undirected graph; Universal optimality; Graph theory
An improved approximation algorithm for RESOURCE ALLOCATION,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053488211&doi=10.1145%2f2000807.2000816&partnerID=40&md5=31972848bbda666b4be995d2aabac5fb,"We study the problem of finding a most profitable subset of n given tasks, each with a given start and finish time as well as profit and resource requirement, that at no time exceeds the quantity Bof available resource. We show that this NP-hard RESOURCE ALLOCATION problem can be (1/2. ε)-approximated in randomized polynomial time, which improves upon earlier approximation results. © 2011 ACM.",Approximation algorithm; Call admission control; Resource allocation,Congestion control (communication); Polynomial approximation; Profitability; Resource allocation; Approximation results; Finish time; NP-hard; Polynomial-time; Resource allocation problem; Resource requirements; Approximation algorithms
Geometric clustering: Fixed-parameter tractability and lower bounds with respect to the dimension,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053468956&doi=10.1145%2f2000807.2000811&partnerID=40&md5=3a19e5dafe2b60bf125503285abe8b64,"We study the parameterized complexity of the k-center problem on a given n-point set P in Rd, with the dimension d as the parameter. We show that the rectilinear 3-center problem is fixed-parameter tractable, by giving an algorithm that runs in O(nlog n) time for any fixed dimension d. On the other hand, we show that this is unlikely to be the case with both the Euclidean and rectilinear k-center problems for any k ≥ 2 and k ≥ 4 respectively. In particular, we prove that deciding whether P can be covered by the union of 2 balls of given radius or by the union of 4 cubes of given side length is W[1]-hard with respect to d, and thus not fixed-parameter tractable unless FPT=W[1]. For the Euclidean case, we also show that even an no(d)-time algorithm does not exist, unless there is a 2o(n)-time algorithm for n-variable 3SAT, that is, the Exponential Time Hypothesis fails. © 2011 ACM.",Clustering; Complexity; Dimension; Fixed-parameter tractability; Lower bound,Clustering; Complexity; Dimension; Fixed-parameter tractability; Lower bounds; Algorithms
Cake cutting really is not a piece of cake,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053476295&doi=10.1145%2f2000807.2000819&partnerID=40&md5=cc9a58e21d30eeb51148f904222e8c02,"We consider the well-known cake cutting problem in which a protocol wants to divide a cake among n ≥ 2 players in such a way that each player believes that they got a fair share. The standard Robertson-Webb model allows the protocol to make two types of queries, Evaluation and Cut, to the players. A deterministic divide-and-conquer protocol with complexity O(nlog n) is known.We provide the first a ω(nlog n) lower bound on the complexity of any deterministic protocol in the standard model. This improves previous lower bounds, in that the protocol is allowed to assign to a player a piece that is a union of intervals and only guarantee approximate fairness. We accomplish this by lower bounding the complexity to find, for a single player, a piece of cake that is both rich in value, and thin in width. We then introduce a version of cake cutting in which the players are able to cut with only finite precision. In this case, we can extend the ω(nlog n) lower bound to include randomized protocols. © 2011 ACM.",Cake cutting; Fair division,Cutting problems; Deterministic protocols; Divide and conquer; Fair division; Fair share; Finite precision; Lower bounds; Randomized protocols; The standard model
All-pairs shortest paths with a sublinear additive error,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053519825&doi=10.1145%2f2000807.2000813&partnerID=40&md5=ad74af947667e41411e0200b5ffab4c9,"We show that, for every 0 ≤ p ≤ 1, there is an O(n 2.575-p/(7.4-2.3p))-time algorithm that given a directed graph with small positive integer weights, estimates the length of the shortest path between every pair of vertices u, v in the graph to within an additive error δp(u, v), where δ(u, v) is the exact length of the shortest path between u and v. This algorithm runs faster than the fastest algorithm for computing exact shortest paths for any 0 < p ≤ 1. Previously the only way to ""beat"" the running time of the exact shortest path algorithms was by applying an algorithm of Zwick [2002] that approximates the shortest path distances within a multiplicative error of (1 + ε). Our algorithm thus gives a smooth qualitative and quantitative transition between the fastest exact shortest paths algorithm, and the fastest approximation algorithm with a linear additive error. In fact, the main ingredient we need in order to obtain the above result, which is also interesting in its own right, is an algorithm for computing (1 + ε) multiplicative approximations for the shortest paths, whose running time is faster than the running time of Zwick's approximation algorithm when ε 1 and the graph has small integer weights. © 2011 ACM.",Graph algorithms; Matrix multiplication; Shortest paths,Approximation algorithms; Wireless sensor networks; Additive errors; Directed graphs; Graph algorithms; Integer weights; MAtrix multiplication; Multiplicative errors; Positive integers; Running time; Shortest path; Shortest path algorithms; Sublinear; Time algorithms; Graph theory
Memoryless facility location in one pass,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053513160&doi=10.1145%2f2000807.2000817&partnerID=40&md5=1811c45a842c97136ef152244f98ea56,"We present the first one-pass memoryless algorithm for metric Facility Location that maintains a set of facilities approximating the optimal facility configuration within a constant factor. The algorithm is randomized and very simple to state and implement. It processes the demand points one-by-one as they arrive, and keeps in memory only the facility locations currently open. We prove that its competitive ratio is less than 14 in the special case of uniform facility costs, and less than 49 in the general case of nonuniform facility costs. © 2011 ACM.",Facility location; Incremental algorithms; One-pass algorithms,Algorithms; Competitive ratio; Constant factors; Facility locations; Incremental algorithm; Memoryless; One-pass; Location
Broadcast scheduling: Algorithms and complexity,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053470526&doi=10.1145%2f2000807.2000815&partnerID=40&md5=33515323e79e86e87239678a0238ed22,"Broadcast Scheduling is a popular method for disseminating information in response to client requests. There are n pages of information, and clients request pages at different times. However, multiple clients can have their requests satisfied by a single broadcast of the requested page. In this article, we consider several related broadcast scheduling problems. One central problem we study simply asks to minimize the maximum response time (over all requests). Another related problem we consider is the version in which every request has a release time and a deadline, and the goal is to maximize the number of requests that meet their deadlines. While approximation algorithms for both these problems were proposed several years back, it was not known if they were NP-complete. One of our main results is that both these problems are NP-complete. In addition, we use the same unified approach to give a simple NP-completeness proof for minimizing the sum of response times. A very complicated proof was known for this version. Furthermore, we give a proof that FIFO is a 2-competitive online algorithm for minimizing the maximum response time (this result had been claimed earlier with no proof) and that there is no better deterministic online algorithm (this result was claimed earlier as well, but with an incorrect proof). © 2011 ACM.",Approximation algorithms; NP-completeness; Online algorithms,Computational complexity; Information dissemination; Scheduling algorithms; Algorithms and complexity; Broadcast scheduling; Broadcast scheduling problems; Central problems; Client request; Deterministic online algorithms; Maximum response time; Multiple clients; NP Complete; Np-completeness; On-line algorithms; Release time; Unified approach; Approximation algorithms
Carry propagation in multiplication by constants,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053517156&doi=10.1145%2f2000807.2000822&partnerID=40&md5=608ddc0c53e33d633956fcc24057f3a4,"Suppose that a random n-bit number V is multiplied by an odd constant M ≥ 3, by adding shifted versions of the number V corresponding to the 1s in the binary representation of the constant M. Suppose further that the additions are performed by carry-save adders until the number of summands is reduced to two, at which time the final addition is performed by a carry-propagate adder. We show that in this situation the distribution of the length of the longest carry-propagation chain in the final addition is the same (up to terms tending to 0 as n → ∞) as when two independent n-bit numbers are added, and in particular the mean and variance are the same (again up to terms tending to 0). This result applies to all possible orders of performing the carry-save additions. © 2011 ACM.",Addition; Carry propagation; Multiplication,Addition; Binary representations; Carry propagation; Carry save adder; Carry save addition; Multiplication; Summands; Adders
Three-coloring triangle-free planar graphs in linear time,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053544050&doi=10.1145%2f2000807.2000809&partnerID=40&md5=9b6b4b4d35ec13d1a9c59aed85431e4d,"Grötzsch's theorem states that every triangle-free planar graph is 3-colorable, and several relatively simple proofs of this fact were provided by Thomassen and other authors. It is easy to convert these proofs into quadratic-time algorithms to find a 3-coloring, but it is not clear how to find such a coloring in linear time (Kowalik used a nontrivial data structure to construct an O(nlog n) algorithm). We design a linear-time algorithm to find a 3-coloring of a given triangle-free planar graph. The algorithm avoids using any complex data structures, which makes it easy to implement. As a by-product, we give a yet simpler proof of Gr̈otzsch's theorem. © 2011 ACM.",Graph coloring; planar graphs; Triangle-free graphs,Clustering algorithms; Coloring; Data structures; Graphic methods; 3-coloring; Complex data structures; Graph colorings; Linear time; Linear-time algorithms; Planar graph; Triangle-free; Triangle-free graphs; Graph theory
Rate vs. Buffer size-greedy information gathering on the line,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051901393&doi=10.1145%2f1978782.1978787&partnerID=40&md5=8e40566b29182c434869f75cda323c1e,"We consider packet networks with limited buffer space at the nodes, and are interested in the question of maximizing the number of packets that arrive to destination rather than being dropped due to full buffers. We initiate a more refined analysis of the throughput competitive ratio of admission and scheduling policies in the Competitive Network Throughput model [Aiello et al. 2005], taking into account not only the network size but also the buffer size and the injection rate of the traffic. We specifically consider the problem of information gathering on the line, with limited buffer space, under adversarial traffic.We examine how the buffer size and the injection rate of the traffic affect the performance of the greedy protocol for this problem. We establish upper bounds on the competitive ratio of the greedy protocol in terms of the network size, the buffer size, and the adversary's rate, and present lower bounds which are tight up to constant factors. These results show, for example, that provisioning the network with sufficiently large buffers may substantially improve the performance of the greedy protocol in some cases, whereas for some high-rate adversaries, using larger buffers does not have any effect on the competitive ratio of the protocol. © 2011 ACM.",Buffer management; Competitive analysis; Competitive network throughput; Information gathering; Online algorithms,Buffer management; Competitive analysis; Competitive network; Information gathering; On-line algorithms; Throughput
Improved approximations for the hotlink assignment problem,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051891536&doi=10.1145%2f1978782.1978794&partnerID=40&md5=df2825e0ff6addf07cd734cf7244b4cc,"Let G = (V, E) be a graph representing a Web site, where nodes correspond to pages and arcs to hyperlinks. In this context, hotlinks are defined as shortcuts (new arcs) added to Web pages of G in order to reduce the time spent by users to reach their desired information. In this article, we consider the problem where G is a rooted directed tree and the goal is minimizing the expected time spent by users by assigning at most k hotlinks to each node. For the most studied version of this problem where atmost one hotlink can be added to each node, we prove the existence of two FPTAS's which optimize different objectives considered in the literature: one minimizes the expected user path length and the other maximizes the expected reduction in user path lengths. These results improve over a constant factor approximation for the expected length and over a PTAS for the expected reduction, both obtained recently in Jacobs [2007]. Indeed, these FPTAS's are essentially the best possible results one can achieve under the assumption that P ≠= NP. Another contribution we give here is a 16-approximation algorithm for the most general version of the problem where up to k hotlinks can be assigned from each node. This algorithm runs in O(|V| log |V|) time and it turns to be the first algorithm with constant approximation for this problem. © 2011 ACM.",Approximation algorithms; Hotlink assignment,Decision trees; Hypertext systems; User interfaces; Websites; Assignment problems; Constant factor approximation; Directed trees; Expected length; Expected time; General version; Hotlink assignment; Hotlinks; Hyperlinks; Time spent; User path; Approximation algorithms
The tree inclusion problem: In linear space and faster,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051890325&doi=10.1145%2f1978782.1978793&partnerID=40&md5=8a93414f86241c30c655416d30feb203,"Given two rooted, ordered, and labeled trees P and T the tree inclusion problem is to determine if P can be obtained from T by deleting nodes in T. This problem has recently been recognized as an important query primitive in XML databases. Kilpel äinen and Mannila [1995] presented the first polynomial-time algorithm using quadratic time and space. Since then several improved results have been obtained for special cases when P and T have a small number of leaves or small depth. However, in the worst case these algorithms still use quadratic time and space. Let nS, lS, and dS denote the number of nodes, the number of leaves, and the depth of a tree S ε {P, T}. In this article we show that the tree inclusion problem can be solved in space O(nT ) and time: Eqation Presented ∑ This improves or matches the best known time complexities while using only linear space instead of quadratic. This is particularly important in practical applications, such as XML databases, where the space is likely to be a bottleneck. © 2011 ACM.",Pattern matching; Tree inclusion,Algorithms; Pattern matching; XML; Labeled trees; Linear spaces; Polynomial-time algorithms; Quadratic time; Time complexity; Tree inclusion; Worst case; XML database; Plant extracts
To fill or not to fill: The gas station problem,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051867127&doi=10.1145%2f1978782.1978791&partnerID=40&md5=ae072f2ce42a4aaf4d7905275f029cd0,"In this article we study several routing problems that generalize shortest paths and the traveling salesman problem. We consider a more general model that incorporates the actual cost in terms of gas prices. We have a vehicle with a given tank capacity. We assume that at each vertex gas may be purchased at a certain price. The objective is to find the cheapest route to go froms to t, or the cheapest tour visiting a given set of locations. We show that the problem of finding a cheapest plan to go from s to t can be solved in polynomial time. For most other versions, however, the problem is NP-complete and we develop polynomial-time approximation algorithms for these versions. © 2011 ACM.",Approximation algorithms; Graph theory; Shortest paths; Vehicle routing,Graph theory; Polynomial approximation; Traveling salesman problem; Vehicle routing; Actual cost; Gas price; Gas stations; General model; NP Complete; Polynomial-time; Polynomial-time approximation algorithms; Routing problems; Shortest path; Tank capacity; Approximation algorithms
S-T connectivity on digraphs with a known stationary distribution,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051880215&doi=10.1145%2f1978782.1978785&partnerID=40&md5=9dc060e8eaba0aa9051196dbb1a44e42,"We present a deterministic logspace algorithm for solving S-T Connectivity on directed graphs if: (i) we are given a stationary distribution of the random walk on the graph in which both of the input vertices s and t have nonnegligible probability mass and (ii) the random walk which starts at the source vertex s has polynomial mixing time. This result generalizes the recent deterministic logspace algorithm for S-T Connectivity on undirected graphs [Reingold, 2008]. It identifies knowledge of the stationary distribution as the gap between the S-T Connectivity problems we know how to solve in logspace (L) and those that capture all of randomized logspace (RL). © 2011 ACM.",Graph reachability; Logspace computation; Random walk and stationary distribution; RL vs. L,Algorithms; Probability distributions; Random processes; Connectivity problems; Directed graphs; Know-how; Log-space algorithms; Logspace; Mixing time; Random Walk; Reachability; RL vs. L; Stationary distribution; Undirected graph; Graph theory
Randomized rendezvous with limited memory,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051888581&doi=10.1145%2f1978782.1978789&partnerID=40&md5=886b1640543a6cdee9efdbbbab74a892,"We present a trade-off between the expected time for two identical agents to rendezvous on a synchronous, anonymous, oriented ring and the memory requirements of the agents. In particular, we show there exists a 2tstate agent which can achieve rendezvous on an n-node ring in expected time O(n2/2t+ 2t) and that any t/2 state agent requires expected time Ω(n2/2t). As a corollary we observe that Θ (log log n) bits of memory are necessary and sufficient to achieve rendezvous in linear time. © 2011 ACM.",Distributed computing; Rendezvous,Distributed computer systems; Expected time; Limited memory; Linear time; Memory requirements; Node ring; Rendezvous; Trade off; Economic and social effects
How well can primal-dual and local-ratio algorithms perform?,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051894334&doi=10.1145%2f1978782.1978784&partnerID=40&md5=2366c77c40424c26e06ea827e965eb1f,"We define an algorithmic paradigm, the stack model, that captures many primal-dual and local-ratio algorithms for approximating covering and packing problems. The stack model is defined syntactically and without any complexity limitations and hence our approximation bounds are independent of the P versus NP question. Using the stack model, we bound the performance of a broad class of primal-dual and localratio algorithms and supply a (log n+1)/2 inapproximability result for set cover, a 4/3 inapproximability for min Steiner tree, and a 0.913 inapproximability for interval scheduling on two machines. © 2011 ACM.",Algorithmic limitations; Approximation algorithms; Bandwidth allocation; Greedy algorithms; Interval scheduling; Local ratio; Primal dual; Set cover; Steiner tree,Approximation algorithms; Plant extracts; Algorithmic limitations; Bandwidth allocations; Greedy algorithms; Interval scheduling; Local ratio; Primal-dual; Set cover; Steiner trees; Scheduling algorithms
Minimizing movement in mobile facility location problems,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051898486&doi=10.1145%2f1978782.1978783&partnerID=40&md5=dae0f500bc93555114b27c498ca2e77a,"In the mobile facility location problem, which is a variant of the classical facility location, each facility and client is assigned to a start location in a metric graph and our goal is to find a destination node for each client and facility such that every client is sent to a node which is the destination of some facility. The quality of a solution can be measured either by the total distance clients and facilities travel or by the maximum distance traveled by any client or facility. As we show in this article (by an approximationpreserving reduction), the problem of minimizing the total movement of facilities and clients generalizes the classical k-median problem. The class of movement problems was introduced by Demaine et al. [2007] where a simple 2-approximation was proposed for the minimummaximum movement mobile facility location problem while an approximation for the minimum total movement variant and hardness results for both were left as open problems. Our main result here is an 8-approximation algorithm for the minimum total movement mobile facility location problem. Our algorithm is obtained by rounding an LP relaxation in five phases. For the minimum maximum movement mobile facility location problem, we show that we cannot have a better than a 2-approximation for the problem, unless P = NP; so the simple algorithm proposed by Demaine et al. [2007] is essentially best possible. © 2011 ACM.",Approximation algorithms; Facility location; Linear programming,Approximation algorithms; Site selection; Destination nodes; Facility locations; Hardness result; K-median problem; LP relaxation; Maximum distance; Minimizing movements; Mobile facility; Open problems; SIMPLE algorithm; Location
The optimality of the online greedy algorithm in carpool and chairman assignment problems,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051877153&doi=10.1145%2f1978782.1978792&partnerID=40&md5=ae65da2859277aa0edba9c0b77d6d800,"We study several classes of related scheduling problems including the carpool problem, its generalization to arbitrary inputs and the chairman assignment problem. We derive both lower and upper bounds for online algorithms solving these problems. We show that the greedy algorithm is optimal among online algorithms for the chairman assignment problem and the generalized carpool problem. We also consider geometric versions of these problems and show how the bounds adapt to these cases. © 2011 ACM.",Carpool problems; Chairman problems; Greedy algorithm; Optimality,Algorithms; Assignment problems; Carpool problem; Chairman problems; Greedy algorithms; Lower and upper bounds; On-line algorithms; Optimality; Scheduling problem; Problem solving
Minimizing flow time in the wireless gathering problem,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051877154&doi=10.1145%2f1978782.1978788&partnerID=40&md5=8ea10cd74d0ffa39d409080563a1700f,"We address the problem of efficient data gathering in a wireless network through multihop communication. We focus on two objectives related to flow times, that is, the times spent by data packets in the system: minimization of the maximum flow time and minimization of the average flow time of the packets. For both problems we prove that, unless P = NP, no polynomial-time algorithm can approximate the optimal solution within a factor less than ω(m 1?ε;) for any 0 <ε < 1, where m is the number of packets. We then assess the performance of two natural algorithms by proving that their cost remains within the optimal cost of the respective problem if we allow the algorithms to transmit data at a speed 5 times higher than that of the optimal solutions to which we compare them. © 2011 ACM.",Approximation algorithms; Data gathering; Local algorithms; Wireless networks,Approximation algorithms; Optimal systems; Optimization; Polynomials; Data gathering; Data packet; Flow-time; Gathering problem; Local algorithm; Maximum flows; Multi hop communication; Optimal costs; Optimal solutions; Polynomial-time algorithms; Transmit data; Wireless networks
Routing (Un-) splittable flow in games with player-specific affine latency functions,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051904488&doi=10.1145%2f1978782.1978786&partnerID=40&md5=d1b2edf54d1af85b789474199f9306f7,"In this work we studyweighted network congestion gameswith player-specific latency functionswhere selfish players wish to route their traffic through a shared network. We consider both the case of splittable and unsplittable traffic. Our main findings are as follows. For routing games on parallel links with linear latency functions, we introduce two new potential functions for unsplittable and for splittable traffic, respectively. We use these functions to derive results on the convergence to pure Nash equilibria and the computation of equilibria. For several generalizations of these routing games, we show that such potential functions do not exist. We prove tight upper and lower bounds on the price of anarchy for games with polynomial latency functions. All our results on the price of anarchy translate to general congestion games. © 2011 ACM.",Congestion games; Convergence to pure Nash equilibria; Price of anarchy,Telecommunication networks; Congestion Games; Latency function; Network congestions; Parallel links; Potential function; Price of anarchy; Pure Nash equilibrium; Shared network; Splittable flows; Upper and lower bounds; Traffic congestion
Max-coloring and online coloring with bandwidths on interval graphs,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051884180&doi=10.1145%2f1978782.1978790&partnerID=40&md5=f238eb7a8904e2ed96bc3d55b1a9d3cf,"Given a graph G = (V, E) and positive integral vertex weights w : V→ N, the max-coloring problem seeks to find a proper vertex coloring of G whose color classes C1,C2,⋯,Ck, minimize Σki=1 maxvεCiw(v). This problem, restricted to interval graphs, arises whenever there is a need to design dedicatedmemorymanagers that provide better performance than the general-purpose memory management of the operating system. Though this problem seems similar to the dynamic storage allocation problem, there are fundamental differences. We make a connection between max-coloring and online graph coloring and use this to devise a simple 2-approximation algorithm for max-coloring on interval graphs. We also show that a simple first-fit strategy, that is a natural choice for this problem, yields an 8-approximation algorithm. We show this result by proving that the first-fit algorithm for online coloring an interval graph G uses no more than 8 ? χ(G) colors, significantly improving the bound of 26 χ(G) by Kierstead and Qin [1995]. We also show that the max-coloring problem is NP-hard. The problem of online coloring of intervals with bandwidths is a simultaneous generalization of online interval coloring and online bin packing. The input is a set I of intervals, each interval i ε I having an associated bandwidth b(i) ε (0, 1].We seek an online algorithm that produces a coloring of the intervals such that for any color c and any real r, the sum of the bandwidths of intervals containing r and colored c is at most 1. Motivated by resource allocation problems, Adamy and Erlebach [2003] consider this problem and present an algorithm that uses at most 195 times the number of colors used by an optimal offline algorithm. Using the new analysis of first-fit coloring of interval graphs, we show that the Adamy-Erlebach algorithm is 35-competitive. Finally, we generalize the Adamy-Erlebach algorithm to a class of algorithms and show that a different instance from this class is 30-competitive. © 2011 ACM.",Approximation algorithms; Buffer minimization; Coloring; Online algorithms; Scheduling,Approximation algorithms; Bandwidth; Color; Graph theory; Graphic methods; Scheduling algorithms; Buffer minimization; Dynamic storage allocation; Graph colorings; Graph G; Interval coloring; Interval graph; Max-coloring; Memory management; NP-hard; Off-line algorithm; On-line algorithms; On-line coloring; Online bin packing; Resource allocation problem; Vertex coloring; Vertex- weights; Coloring
Competitive analysis of flash memory algorithms,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953242855&doi=10.1145%2f1921659.1921669&partnerID=40&md5=5802c363a0baee971559fa21ac29d8fc,"Flash memories are widely used in computer systems ranging from embedded systems to workstations and servers to digital cameras and mobile phones. The memory cells of flash devices can only endure a limited number of write cycles, usually between 10,000 and 1,000,000. Furthermore, cells containing data must be erased before they can store new data, and erasure operations erase large blocks of memory, not individual cells. To maximize the endurance of the device (the amount of useful data that can be written to it before one of its cells wears out), flash-based systems move data around in an attempt to reduce the total number of erasures and to level the wear of the different erase blocks. This data movement introduces an interesting online problem called the wear-leveling problem.Wear-leveling algorithms have been used at least since 1993, but they have never been mathematically analyzed. In this article we analyze the two main wear-leveling problems. We show that a simple randomized algorithm for one of them is essentially optimal both in the competitive sense and in the absolute sense (our competitive result relies on an analysis of a nearly-optimal offline algorithm). We show that deterministic algorithms cannot achieve comparable endurance. We also analyze a more difficult problem and show that offline algorithms for it can improve upon naive approaches, but that online algorithms essentially cannot. © 2011 ACM.",Competitive analysis; Flash memories; Online algorithms; Wear leveling,Algorithms; Cameras; Cellular telephone systems; Computer workstations; Durability; Embedded systems; Optimization; Telecommunication equipment; Competitive analysis; Data movements; Deterministic algorithms; Flash devices; Individual cells; Memory cell; Off-line algorithm; Online algorithms; Online problems; Randomized Algorithms; Wear leveling; Flash memory
Sum edge coloring of multigraphs via configuration LP,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953248649&doi=10.1145%2f1921659.1921668&partnerID=40&md5=b8c4ad8a2945f095eb74ced34a27f08c,"We consider the scheduling of biprocessor jobs under sum objective (BPSMS). Given a collection of unitlength jobs where each job requires the use of two processors, find a schedule such that no two jobs involving the same processor run concurrently. The objective is to minimize the sum of the completion times of the jobs. Equivalently, we would like to find a sum edge coloring of a given multigraph, that is, a partition of its edge set into matchings M1, ⋯ ,Mt minimizing ∑t i=1 i|M i|. This problem is APX-hard, even in the case of bipartite graphs [Marx 2009]. This special case is closely related to the classic open shop scheduling problem. We give a 1.8298-approximation algorithm for BPSMS improving the previously best ratio known of 2 [Bar-Noy et al. 1998]. The algorithm combines a configuration LP with greedy methods, using nonstandard randomized rounding on the LP fractions. We also give an efficient combinatorial 1.8886-approximation algorithm for the case of simple graphs, which gives an improved 1.79568 + O(log d̄/d̄)-approximation in graphs of large average degree d̄. © 2011 ACM.",Approximation algorithms; Configuration LP; Edge scheduling,Scheduling algorithms; Average degree; Bipartite graphs; Completion time; Configuration LP; Edge coloring; Edge scheduling; Edge-sets; Greedy method; Matchings; Multigraphs; Open shop scheduling problem; Randomized rounding; Two processors; Approximation algorithms
Decision trees for entity identification: Approximation algorithms and hardness results,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953246501&doi=10.1145%2f1921659.1921661&partnerID=40&md5=3cb2b3acd2cf1cfd5090ca946375be3e,"We consider the problem of constructing decision trees for entity identification from a given relational table. The input is a table containing information about a set of entities over a fixed set of attributes and a probability distribution over the set of entities that specifies the likelihood of the occurrence of each entity. The goal is to construct a decision tree that identifies each entity unambiguously by testing the attribute values such that the average number of tests is minimized. This classical problem finds such diverse applications as efficient fault detection, species identification in biology, and efficient diagnosis in the field of medicine. Prior work mainly deals with the special case where the input table is binary and the probability distribution over the set of entities is uniform. We study the general problem involving arbitrary input tables and arbitrary probability distributions over the set of entities. We consider a natural greedy algorithm and prove an approximation guarantee of O(rK . log N), where N is the number of entities and K is the maximum number of distinct values of an attribute. The value rK is a suitably defined Ramsey number, which is at most log K. We show that it is NP-hard to approximate the problem within a factor of Ω(log N), even for binary tables (i.e., K = 2). Thus, for the case of binary tables, our approximation algorithm is optimal up to constant factors (since r2 = 2). In addition, our analysis indicates a possible way of resolving a Ramsey-theoretic conjecture by Erdös. © 2011 ACM.",Approximation algorithms; Decision tree; Ramsey numbers,Decision tables; Decision trees; Fault detection; Probability distributions; Arbitrary probability distribution; Attribute values; Average numbers; Classical problems; Constant factors; Diverse applications; Entity identification; Greedy algorithms; Hardness result; NP-hard; Ramsey numbers; Relational tables; Species identification; Approximation algorithms
Finding witnesses by peeling,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953244288&doi=10.1145%2f1921659.1921670&partnerID=40&md5=7aec15454366d85758deedea5c71288e,"In the k-matches problem, we are given a pattern and a text, and for each text location, the desired output consists of all aligned matching characters if there are k or fewer of them, and any k aligned matching characters if there are more than k of them. This problem is one of several string matching problems that seek not only to find where the pattern matches the text under different ""match"" definitions, but also to provide witnesses to the match. Other such problems include k-aligned ones, k-witnesses, and k-mismatches. In addition, the solutions to several other string matching problems rely on the efficient solutions of the witness finding problems. In this article we provide a general method for solving such witness finding problems efficiently. We do so by casting the problem as a generalization of group testing, which we then solve by a process we call peeling. Using this general framework we obtain improved results for all of the problems mentioned. We also show that our method also solves a couple of problems outside the pattern matching domain. © 2011 ACM.",Shortest paths; Witness computation,Graph theory; General method; Group testing; If there are; K-mismatches; Pattern match; Shortest path; String matching; Text location; Witness computation; Pattern matching
Shortest vertex-disjoint two-face paths in planar graphs,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953253956&doi=10.1145%2f1921659.1921665&partnerID=40&md5=cb44540f3cbfade97e7dfedf5427c47c,"Let G be a directed planar graph of complexity n, each arc having a nonnegative length. Let s and t be two distinct faces of G; let s1, ⋯ , sk be vertices incident with s; let t1, ⋯ , tk be vertices incident with t. We give an algorithm to compute k pairwise vertex-disjoint paths connecting the pairs (si , ti) in G, with minimal total length, in O(knlog n) time. © 2011 ACM.",Algorithm; Disjoint paths; Planar graph; Shortest path,Algorithms; Graphic methods; Disjoint paths; Planar graph; Shortest path; Total length; Vertex disjoint paths; Graph theory
Discovering almost any hidden motif from multiple sequences,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953237098&doi=10.1145%2f1921659.1921672&partnerID=40&md5=bd258062ed19f009b75aa9bbd7db855e,"We study a natural probabilistic model for motif discovery. In this model, there are k background sequences, and each character in a background sequence is a random character from an alphabet ∑. A motif G = g1g 2 ⋯ gm is a string of m characters. Each background sequence is implanted with a probabilistically generated approximate copy of G. For a probabilistically generated approximate copy b1b2 ⋯ bm of G, every character is probabilistically generated such that the probability for bi ≠ gi is at most α. In this article, we develop an efficient algorithm that can discover a hidden motif from a set of sequences for any alphabet ∑ with |∑| ≥ 2 and is applicable to DNA motif discovery. We prove that for α < 1/8 (1 - 1/|∑| ), there exist positive constants c0, ε, and δ2 such that if there are at least c0 logn input sequences, then in O( n2/h (log n)O(1)) time this algorithm finds the motif with probability at least 3/4 for every G ε ∑p - ψp,h,ε (∑), where n the length of longest sequences, p is the length of the motif, h is a parameter with p ≥ 4h ≥ δ2 log n, and εp,h,ε (∑) is a small subset of at most 2-Θ(ε2h) fraction of the sequences in ∑p . © 2011 ACM.",Complexity; Motif detection; Probabilistic analysis; Probability,Algorithms; Probability; Complexity; DNA motif; Efficient algorithm; If there are; Input sequence; Motif detection; Motif discovery; Multiple sequences; Positive constant; Probabilistic analysis; Probabilistic models; Random characters; DNA sequences
Set connectivity problems in undirected graphs and the directed steiner network problem,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953242006&doi=10.1145%2f1921659.1921664&partnerID=40&md5=4a345d1126fde16fcda8b6dfc29eeb08,"In the generalized connectivity problem, we are given an edge-weighted graph G = (V, E) and a collection = {(S1, T1), ⋯ , (Sk, Tk)} of distinct demands; each demand (Si , Ti ) is a pair of disjoint vertex subsets. We say that a subgraph F of Gconnects a demand (Si , Ti ) when it contains a path with one endpoint in Si and the other in Ti . The goal is to identify a minimum weight subgraph that connects all demands in D. Alon et al. (SODA '04) introduced this problem to study online network formation settings and showed that it captures some well-studied problems such as Steiner forest, facility location with nonmetric costs, tree multicast, and group Steiner tree. Obtaining a nontrivial approximation ratio for generalized connectivity was left as an open problem. We describe the first poly-logarithmic approximation algorithm for generalized connectivity that has a performance guarantee of O(log2 nlog2 k). Here, n is the number of vertices in G and k is the number of demands. We also prove that the cut-covering relaxation of this problem has an O(log3 nlog 2 k) integrality gap. Building upon the results for generalized connectivity, we obtain improved approximation algorithms for two problems that contain generalized connectivity as a special case. For the directed Steiner network problem, we obtain an O(k1/2+ε ) approximation which improves on the currently best performance guarantee of Õ(k2/3) due to Charikar et al. (SODA '98). For the set connector problem, recently introduced by Fukunaga and Nagamochi (IPCO '07), we present a poly-logarithmic approximation; this result improves on the previously known ratio which can be Ω(n) in the worst case. © 2011 ACM.",Approximation algorithms; Directed Steiner network; Generalized connectivity,Approximation ratios; Charikar; Connectivity problems; Directed Steiner network; Edge-weighted graph; Facility locations; Generalized connectivity; Group Steiner tree; Integrality gaps; Logarithmic approximation; Minimum weight; Multicasts; On-line network; Open problems; Performance guarantees; Steiner forests; Steiner network; Subgraphs; Undirected graph; Worst case; Approximation algorithms
Tree exploration with logarithmic memory,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953254622&doi=10.1145%2f1921659.1921663&partnerID=40&md5=e2d50bf47140cb89651782d181db5eb1,"We consider the task of network exploration by a mobile agent (robot) with small memory. The agent has to traverse all nodes and edges of a network (represented as an undirected connected graph), and return to the starting node. Nodes of the network are unlabeled and edge ports are locally labeled at each node. The agent has no a priori knowledge of the topology of the network or of its size, and cannot mark nodes in any way. Under such weak assumptions, cycles in the network may prevent feasibility of exploration, hence we restrict attention to trees. We present an algorithm to accomplish tree exploration (with return) using O(log n)-bit memory for all n-node trees. This strengthens the result from Diks et al. [2004], where O(log2 n)-bit memory was used for tree exploration, and matches the lower bound on memory size proved there. We also extend our O(log n)-bit memory traversal mechanism to a weaker model in which ports at each node are ordered in circular manner, however, the explicit values of port numbers are not available. © 2011 ACM.",Distributed algorithms; Graph exploration; Small memory,Algorithms; Mobile agents; Connected graph; Distributed algorithm; Edge ports; Graph exploration; Lower bounds; Memory size; Port numbers; Priori knowledge; Small memory; Trees (mathematics)
Constant factor approximations for the hotlink assignment problem,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953235169&doi=10.1145%2f1921659.1921662&partnerID=40&md5=d38d4529d6d5490ced0ce1b5c5daa2d0,"The concept of hotlink assignment aims at reducing the navigation effort for the users of a Web directory or similar structure by inserting a limited number of additional hyperlinks called hotlinks. The k-hotlink assignment problem denotes the task of adding at most k outgoing hotlinks to each page of a tree-like site, minimizing the path length, that is, the expected number of ""clicks"" necessary for the user to reach her destination page. Another common formulation of this problem is to maximize the gain, that is, the path length reduction achieved by the assignment. In this work we analyze the natural greedy strategy, proving that it reaches the optimal gain up to the constant factor of 2. Considering the gain, we also prove the existence of a PTAS. Finally, we give a polynomial-time 2-approximation for the 1-hotlink assignment problem, which constitutes the first constant factor approximation in terms of the path length. The algorithms' performance analyses are made possible by a set of three new basic operations for the transformation of hotlink assignments. © 2011 ACM.",Approximation algorithms; Search trees,Hypertext systems; Optimization; Polynomial approximation; Trees (mathematics); Assignment problems; Basic operation; Constant factor approximation; Constant factors; Greedy strategies; Hotlinks; Hyperlinks; Optimal gain; Path length; Performance analysis; Polynomial-time; Search trees; Web directories; Approximation algorithms
Computing the inverse sort transform in linear time,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953238799&doi=10.1145%2f1921659.1921673&partnerID=40&md5=79cfce7cce5bf267b76a0ecbd3a2f434,"The Sort Transform (ST) can significantly speed up the block sorting phase of the Burrows-Wheeler Transform (BWT) by sorting the limited order contexts. However, the best result obtained so far for the inverse ST has a time complexity O(Nlog k) and a space complexity O(N), where N and k are the text size and the context order of the transform, respectively. In this article, we present a novel algorithm that can compute the inverse ST for any k-order contexts in an O(N) time and space complexity, a linear result independent of k. The main idea behind the design of this linear algorithm is a set of cycle properties of k-order contexts that we explore for this work. These newly discovered cycle properties allow us to quickly compute the Longest Common Prefix (LCP) between any pair of adjacent k-order contexts that may belong to two different cycles, which eventually leads to the proposed linear-time solution. © 2011 ACM.",Burrows-Wheeler transform; Inverse transform; Limited order context; Linear time,Algorithms; Linear transformations; Belong to; Block-sorting; Burrows Wheeler transform; Cycle property; Inverse transform; Limited order context; Linear algorithms; Linear time; Linear-time solutions; Novel algorithm; Space complexity; Speed-ups; Time and space; Time complexity; Mathematical transformations
Streaming and fully dynamic centralized algorithms for constructing and maintaining sparse spanners,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953239982&doi=10.1145%2f1921659.1921666&partnerID=40&md5=d19d96bb9e81b8fbdb04eb613aa5f660,"We present a streaming algorithm for constructing sparse spanners and show that our algorithm significantly outperforms the state-of-the-art algorithm for this task (due to Feigenbaum et al.). Specifically, the processing time per edge of our algorithm is O(1), and it is drastically smaller than that of the algorithm of Feigenbaum et al., and all other efficiency parameters of our algorithm are no greater (and some of them are strictly smaller) than the respective parameters of the state-of-the-art algorithm. We also devise a fully dynamic centralized algorithm maintaining sparse spanners. This algorithm has incremental update time of O(1), and a nontrivial decremental update time. To our knowledge, this is the first fully dynamic centralized algorithm for maintaining sparse spanners that provides nontrivial bounds on both incremental and decremental update time for a wide range of stretch parameter t. © 2011 ACM.",Graph spanners; Streaming algorithms,Algorithms; Centralized algorithms; Efficiency parameters; Graph spanners; Incremental updates; Processing Time; State-of-the-art algorithms; Streaming algorithm; Streaming algorithms; Parameter estimation
Data structures for mergeable trees,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953250881&doi=10.1145%2f1921659.1921660&partnerID=40&md5=b5d3911e3ac8ebede5a48ce6247a6411,"Motivated by an application in computational geometry, we consider a novel variant of the problem of efficiently maintaining a forest of dynamic rooted trees. This variant includes an operation that merges two tree paths. In contrast to the standard problem, in which a single operation can only add or delete one arc, one merge can add and delete up to a linear number of arcs. In spite of this, we develop three different methods that need only polylogarithmic time per operation. The firstmethod extends a solution of Farach and Thorup [1998] for the special case of paths. Each merge takes O(log2 n) amortized time on an n-node forest and each standard dynamic tree operation takes O(log n) time; the latter bound is amortized, worst case, or randomized depending on the underlying data structure. For the special case that occurs in the motivating application, in which arbitrary arc deletions (cuts) do not occur, we give amethod that takes O(log n) time per operation, including merging. This is best possible in a model of computation with an Ω(nlog n) lower bound for sorting n numbers, since such sorting can be done in O(n) tree operations. For the even-more-special case in which there are no cuts and no parent queries, we give a method that uses standard dynamic trees as a black box: each mergeable tree operation becomes a constant number of standard dynamic tree operations. This third method can also be used in the motivating application, but only by changing the algorithm in the application. Each of our three methods needs different analytical tools and reveals different properties of dynamic trees. © 2011 ACM.",Amortized efficiency; Computational topology; Critical pairs; Dynamic trees; Extreme points; Link-cut trees; Manifolds; Merging,Computational geometry; Data structures; Merging; Motivation; Sorting; Standards; Topology; Amortized efficiency; Computational topology; Critical pairs; Cut-tree; Dynamic trees; Extreme points; Manifolds; Computational efficiency
Algorithms for distributed functional monitoring,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953241666&doi=10.1145%2f1921659.1921667&partnerID=40&md5=c9b468c7d45d252f67b6caca3c071fae,"Consider the following problem: We have k players each receiving a stream of items, and communicating with a central coordinator. Let the multiset of items received by player i up until time t be Ai (t). The coordinator's task is to monitor a given function f computed over the union of the inputs Ui Ai (t), continuously at all times t. The goal is to minimize the number of bits communicated between the players and the coordinator. Of interest is the approximate version where the coordinator outputs 1 if f ≥ τ and 0 if f ≤ (1-ε)τ . This defines the (k, f, τ, ε) distributed functional monitoring problem. Functional monitoring problems are fundamental in distributed systems, in particular sensor networks, where we must minimize communication; they also connect to the well-studied streaming model and communication complexity. Yet few formal bounds are known for functional monitoring. We give upper and lower bounds for the (k, f, τ, ε) problem for some of the basic f's. In particular, we study the frequency moments Fp for p = 0, 1, 2. For F0 and F1, we obtain monitoring algorithms with cost almost the same as algorithms that compute the function for a single instance of time. However, for F2 the monitoring problem seems to be much harder than computing the function for a single time instance. We give a carefully constructed multiround algorithm that uses ""sketch summaries"" at multiple levels of details and solves the (k, F2, τ, ε) problem with communication Õ (k2/ε + k3/2/ε3). Our algorithmic techniques are likely to be useful for other functional monitoring problems as well. © 2011 ACM.",Distributed computing; Functional monitoring,Algorithms; Monitoring; Sensor networks; Algorithmic techniques; Communication complexity; Distributed Computing; Distributed systems; Following problem; Functional monitoring; Monitoring algorithms; Multiple levels of detail; Multiround algorithms; Multiset; Streaming model; Time instances; Upper and lower bounds; Communication
Constrained pattern matching,2011,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953246397&doi=10.1145%2f1921659.1921671&partnerID=40&md5=5d84b4d9a68cf73258df3fa978800b86,"Constrained sequences are strings satisfying certain additional structural restrictions (e.g., some patterns are forbidden). They find applications in communication, digital recording, and biology. In this article, we restrict our attention to the so-called (d, k) constrained binary sequences in which any run of zeros must be of length at least d and at most k, where 0 ≤ d < k. In many applications, one needs to know the number of occurrences of a given pattern w in such sequences, for which we coin the term constrained pattern matching. For a given word w, we first estimate the mean and the variance of the number of occurrences of w in a (d, k) sequence generated by a memoryless source. Then we present the central limit theorem and large deviations results. As a by-product, we enumerate asymptotically the number of (d, k) sequences with exactly r occurrences of w, and compute Shannon entropy of (d, k) sequences with a given number of occurrences of w. We also apply our results to detect under- and overrepresented patterns in neuronal data (spike trains), which satisfy structural constraints that match the framework of (d, k) binary sequences. Throughout this article we use techniques of analytic combinatorics such as combinatorial calculus, generating functions, and complex asymptotics. © 2011 ACM.",Autocorrelation polynomials; Complex asymptotics; Constrained sequences; Languages; Neuronal spike trains; Pattern matching,Autocorrelation; Function evaluation; Pattern matching; Autocorrelation polynomial; Complex asymptotics; Constrained sequences; Languages; Neuronal spike trains; Binary sequences
A faster algorithm for computing the girth of planar and bounded genus graphs,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650638952&doi=10.1145%2f1868237.1868240&partnerID=40&md5=67402050d716310af452edfc95e59bef,"The girth of a graph G is the length of a shortest cycle of G. In this article we design an O(n5/4 log n) algorithm for finding the girth of an undirected n-vertex planar graph, the first o(n2) algorithm for this problem. We also extend our results for the class of graphs embedded into an orientable surface of small genus. Our approach uses several techniques such as graph partitioning, hammock decomposition, graph covering, and dynamic shortest-path computation. We discuss extensions and generalizations of our result. © 2010 ACM.",Cycles; Dynamic algorithms; Girth; Graph algorithms; Graph genus; Graph separators; Graphs; Hammock decomposition; Path and circuit problems; Planar graphs; Shortest paths,Algorithms; Graphic methods; Separators; Cycles; Dynamic algorithm; Girth; Graph algorithms; Graph genus; Graph separators; Graphs; Hammock decomposition; Path and circuit problems; Planar graph; Shortest path; Graph theory
How to probe for an extreme value,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650638951&doi=10.1145%2f1868237.1868250&partnerID=40&md5=4c276b7755293c124b927a9ade64a5b2,"In several systems applications, parameters such as load are known only with some associated uncertainty, which is specified, or modeled, as a distribution over values. The performance of the system optimization and monitoring schemes can be improved by spending resources such as time or bandwidth in observing or resolving the values of these parameters. In a resource-constrained situation, deciding which parameters to observe in order to best optimize the expected system performance (or in general, optimize the expected value of a certain objective function) itself becomes an interesting optimization problem. In this article, we initiate the study of such problems that we term ""model-driven optimization"". In particular, we study the problem of optimizing the minimum value in the presence of observable distributions. We show that this problem is NP-HARD, and present greedy algorithms with good performance bounds. The proof of the performance bounds are via novel sub-modularity arguments and connections to covering integer programs. © 2010 ACM.",Approximation algorithms; Minimum value; Observations; Stochastic optimization,Constrained optimization; Integer programming; Stochastic systems; Covering integer program; Expected values; Extreme value; Greedy algorithms; Minimum value; Model-driven optimization; NP-hard; Objective functions; Observations; Optimization problems; Performance bounds; Resource-constrained; Stochastic optimizations; System optimizations; Systems applications; Approximation algorithms
"Additive spanners and (α, β)-spanners",2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650661184&doi=10.1145%2f1868237.1868242&partnerID=40&md5=32126c176b3980b79c909f2b1562cf40,"An (α, β)-spanner of an unweighted graph G is a subgraph H that distorts distances in G up to a multiplicative factor of α and an additive term β. It is well known that any graph contains a (multiplicative) (2k-1, 0)-spanner of size O(n1+1/k) and an (additive) (1, 2)-spanner of size O(n3/2). However no other additive spanners are known to exist. In this article we develop a couple of new techniques for constructing (α, β)-spanners. Our first result is an additive (1, 6)-spanner of size O(n4/3). The construction algorithm can be understood as an economical agent that assigns costs and values to paths in the graph, purchasing affordable paths and ignoring expensive ones, which are intuitively well approximated by paths already purchased. We show that this path buying algorithm can be parameterized in different ways to yield other sparseness-distortion tradeoffs. Our second result addresses the problem of which (α, β)-spanners can be computed efficiently, ideally in linear time.We show that, for any k, a (k, k -1)-spanner with size O(kn1+1/k ) can be found in linear time, and, further, that in a distributed network the algorithm terminates in a constant number of rounds. Previous spanner constructions with similar performance had roughly twice the multiplicative distortion. © 2010 ACM.",Metric embedding; Spanner,Additive spanners; Construction algorithms; Distributed networks; Linear time; Metric embeddings; Multiplicative factors; Parameterized; Spanner; Subgraphs; Unweighted graphs; Algorithms
On the online unit clustering problem,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650653757&doi=10.1145%2f1868237.1868245&partnerID=40&md5=9c7c05a01fbde284057244ffe37319bb,"We continue the study of the online unit clustering problem, introduced by Chan and Zarrabi-Zadeh (Workshop on Approximation and Online Algorithms 2006, LNCS 4368, p. 121-131. Springer, 2006). We design a deterministic algorithm with a competitive ratio of 7/4 for the one-dimensional case. This is the first deterministic algorithm that beats the bound of 2. It also has a better competitive ratio than the previous randomized algorithms. Moreover, we provide the first non-trivial deterministic lower bound, improve the randomized lower bound, and prove the first lower bounds for higher dimensions. © 2010 ACM.",Clustering; Online algorithms,Approximation algorithms; Clustering; Clustering problems; Competitive ratio; Deterministic algorithms; Higher dimensions; Lower bounds; Non-trivial; Online algorithms; Randomized Algorithms; Clustering algorithms
Geodesic Fréchet distance inside a simple polygon,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650664983&doi=10.1145%2f1868237.1868247&partnerID=40&md5=3ae92625094ab083d874b80911ab688f,"We present an alternative to parametric search that applies to both the nongeodesic and geodesic Fréchet optimization problems. This randomized approach is based on a variant of redblue intersections and is appealing due to its elegance and practical efficiency when compared to parametric search. We introduce the first algorithm to compute the geodesic Fréchet distance between two polygonal curves A and B inside a simple bounding polygon P. The geodesic Fréchet decision problem is solved almost as fast as its nongeodesic sibling in O(N2 log k) time and O(k + N) space after O(k) preprocessing, where N is the larger of the complexities of A and B and k is the complexity of P. The geodesic Fréchet optimization problem is solved by a randomized approach in O(k+N2 logkN log N) expected time and O(k+ N2) space. This runtime is only a logarithmic factor larger than the standard nongeodesic Fréchet algorithm [Alt and Godau 1995]. Results are also presented for the geodesic Fréchet distance in a polygonal domain with obstacles and the geodesic Hausdorff distance for sets of points or sets of line segments inside a simple polygon P. © 2010 ACM.",Fréchet distance; Geodesic; Shortest path; Simple polygon,Algorithms; Graph theory; Optimization; Decision problems; Expected time; Geodesic; Hausdorff distance; Line segment; Non-geodesic; Optimization problems; Polygonal curve; Polygonal domain; Randomized approach; Runtimes; Shortest path; Simple polygon; Geodesy
Approximation algorithms for the sex-equal stable marriage problem,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650655897&doi=10.1145%2f1868237.1868239&partnerID=40&md5=b5c3895d9ba2e47789c40e41ca4496e7,"The stable marriage problem is a classical matching problem introduced by Gale and Shapley. It is known that for any instance, there exists a solution, and there is a polynomial time algorithm to find one. However, the matching obtained by this algorithm is man-optimal, that is, the matching is favorable for men but unfavorable for women, (or, if we exchange the roles of men and women, the resulting matching is woman-optimal). The sex-equal stable marriage problem, posed by Gusfield and Irving, seeks a stable matching ""fair"" for both genders. Specifically it seeks a stable matching with the property that the sum of the men's scores is as close as possible to that of the women's. This problem is known to be strongly NP-hard. In this paper, we give a polynomial time algorithm for finding a near optimal solution for the sex-equal stablemarriage problem. Furthermore, we consider the problem of optimizing an additional criterion: among stable matchings that are near optimal in terms of the sex-equality, find a minimum egalitarian stable matching. We show that this problem is strongly NP-hard, and give a polynomial time algorithm whose approximation ratio is less than two. © 2010 ACM.",Approximation Algorithms; The sex-equal stable marriage problem; The stable marriage problem,Computational complexity; Optimization; Polynomial approximation; Approximation ratios; Matching problems; Matchings; Near-optimal solutions; Polynomial-time algorithms; Stable marriage problem; Stable matching; Strongly NP-hard; The sex-equal stable marriage problem; Approximation algorithms
Taxes for linear atomic congestion games,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650673348&doi=10.1145%2f1868237.1868251&partnerID=40&md5=e18efcdbc315857b321530c8d7bc738e,"We study congestion games where players aim to access a set of resources. Each player has a set of possible strategies and each resource has a function associating the latency it incurs to the players using it. Players are non-cooperative and each wishes to follow a strategy that minimizes her own latency with no regard to the global optimum. Previous work has studied the impact of this selfish behavior on system performance. In this article, we study the question of how much the performance can be improved if players are forced to pay taxes for using resources. Our objective is to extend the original game so that selfish behavior does not deteriorate performance. We consider atomic congestion games with linear latency functions and present both negative and positive results. Our negative results show that optimal system performance cannot be achieved even in very simple games. On the positive side, we show that there are ways to assign taxes that can improve the performance of linear congestion games by forcing players to follow strategies where the total latency suffered is within a factor of 2 of the minimum possible; this result is shown to be tight. Furthermore, even in cases where in the absence of taxes the system behavior may be very poor, we show that the total disutility of players (latency plus taxes) is not much larger than the optimal total latency. Besides existential results, we showhowto compute taxes in time polynomial in the size of the game by solving convex quadratic programs. Similar questions have been extensively studied in the model of non-atomic congestion games. To the best of our knowledge, this is the first study of the efficiency of taxes in atomic congestion games. © 2010 ACM.",Congestion games; Nash equilibria; Taxes,Atoms; Optimization; Congestion Games; Global optimum; Latency function; Linear congestion games; Nash equilibria; Non-cooperative; Optimal system performance; Quadratic programs; Simple games; System behaviors; Taxes; Time polynomials; Taxation
Computing large matchings fast,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650640672&doi=10.1145%2f1868237.1868238&partnerID=40&md5=c4c97717224b8af5bf9f7f7d6b063ad4,"In this article we present algorithms for computing large matchings in 3-regular graphs, graphs with maximum degree 3, and 3-connected planar graphs. The algorithms give a guarantee on the size of the computed matching and take linear or slightly superlinear time. Thus they are faster than the best-known algorithm for computing maximum matchings in general graphs, which runs in O(√nm) time, where n denotes the number of vertices and m the number of edges of the given graph. For the classes of 3-regular graphs and graphs with maximum degree 3, the bounds we achieve are known to be best possible. We also investigate graphs with block trees of bounded degree, where the d-block tree is the adjacency graph of the d-connected components of the given graph. In 3-regular graphs and 3- connected planar graphs with bounded-degree 2- and 4-block trees, respectively, we show how to compute maximum matchings in slightly superlinear time. © 2010 ACM.",3-connected planar graphs; 3-regular graphs; Bounded-degree block trees; Large matchings; Maxdeg-3 graphs; Maximum matchings,Algorithms; Graphic methods; Trees (mathematics); 3-connected planar graphs; 3-regular graphs; Block trees; Matchings; Maxdeg-3 graphs; Maximum matchings; Graph theory
Length-bounded cuts and flows,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650653050&doi=10.1145%2f1868237.1868241&partnerID=40&md5=0e334f6dbb8c6e71af9e5e87ebe49759,"For a given number L, an L-length-bounded edge-cut (node-cut, respectively) in a graph G with source s and sink t is a set C of edges (nodes, respectively) such that no s-t-path of length at most L remains in the graph after removing the edges (nodes, respectively) in C. An L-length-bounded flow is a flowthat can be decomposed into flowpaths of length at most L. In contrast to classical flowtheory, we describe instances for which the minimum L-length-bounded edge-cut (node-cut, respectively) is ⊖(n2/3)-times (⊖(√n)-times, respectively) larger than themaximum L-length-bounded flow, where n denotes the number of nodes; this is the worst case. We show that the minimum length-bounded cut problem is NP-hard to approximate within a factor of 1.1377 for L ≥ 5 in the case of node-cuts and for L ≥ 4 in the case of edge-cuts. We also describe algorithms with approximation ratio O(min{L, n/L}) ⊆ O(√n) in the node case and O(min{L, n2/L 2, √m}) ⊆ O(n2/3) in the edge case, where m denotes the number of edges. Concerning L-length-bounded flows, we show that in graphs with unit-capacities and general edge lengths it is NP-complete to decide whether there is a fractional length-bounded flow of a given value. We analyze the structure of optimal solutions and present further complexity results. © 2010 ACM.",Approximation algorithms; Flows and cuts in graphs; Graphs,Computational complexity; Graphic methods; Structural optimization; Approximation ratios; Bounded flow; Complexity results; Edge cuts; Edge length; Flow path; Flows and cuts in graphs; Graph G; Graphs; NP Complete; NP-hard; Optimal solutions; Worst case; Approximation algorithms
On the bicriteria k-server problem,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650666951&doi=10.1145%2f1868237.1868244&partnerID=40&md5=32e860671cab318471c3a40a7587c71b,"In this article we consider multicriteria formulations of classical online problems in which an algorithm must simultaneously perform well with respect to two different cost measures. Every strategy for serving a sequence of requests is characterized by a pair of costs and therefore there can be many different minimal or optimal incomparable solutions. The adversary is assumed to choose from one of these minimal strategies and the performance of the algorithm is measured with respect to the costs the adversary pays servicing the sequence according to its determined choice of strategy. We consider a parametric family of functions which includes all the possible selections for such strategies. Then, starting from a simple general method that combines any multicriteria instance into a single-criterion one, we provide a universal multicriteria algorithm that can be applied to different online problems. In the multicriteria k-server formulationwith two different edge weightings, for each function class, such a universal algorithm achieves competitive ratios that are only an O(logW) multiplicative factor away from the corresponding determined lower bounds, where W is the maximum ratio between the two weights associated to each edge. We then extend our results to two specific functions, for which nearly optimal competitive algorithms are obtained by exploiting more knowledge of the selection properties. Finally, we show how to apply our framework to other multicriteria online problems sharing similar properties. © 2010 ACM.",κ-server problem; Bicriteria optimization; Online algorithms,Algorithms; Optimization; Bi-criteria; Bicriteria optimization; Competitive algorithms; Competitive ratio; Function class; General method; K-server; K-server problem; Lower bounds; Maximum ratio; Multi-criteria; Multiplicative factors; On-line algorithms; Online problems; Parametric family; Server problem; Universal algorithm; Servers
The Compressed Permuterm Index,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650653284&doi=10.1145%2f1868237.1868248&partnerID=40&md5=d713a79c59ab8cad0bd0d15bcfd20314,"The Permuterm index [Garfield 1976] is a time-efficient and elegant solution to the string dictionary problem in which pattern queries may possibly include one wild-card symbol (called Tolerant Retrieval problem). Unfortunately the Permuterm index is space inefficient because it quadruples the dictionary size. In this article we propose the Compressed Permuterm Index which solves the Tolerant Retrieval problem in time proportional to the length of the searched pattern, and space close to the kth order empirical entropy of the indexed dictionary.We also design a dynamic version of this index that allows to efficiently manage insertion in, and deletion from, the dictionary of individual strings. The result is based on a simple variant of the Burrows-Wheeler Transform, defined on a dictionary of strings of variable length, that allows to efficiently solve the Tolerant Retrieval problem via known (dynamic) compressed indexes [Navarro and Mäkinen 2007]. We will complement our theoretical study with a significant set of experiments that show that the Compressed Permuterm Index supports fast queries within a space occupancy that is close to the one achievable by compressing the string dictionary via gzip or bzip2. This improves known approaches based on Front-Coding [Witten et al. 1999] by more than 50% in absolute space occupancy, still guaranteeing comparable query time. © 2010 ACM.",Burrows-Wheeler transform; Compressed index; Indexing data structure; Permuterm; String dictionary,Data structures; Indexing (of information); Burrows Wheeler transform; Compressed index; Indexing data structure; Permuterm; String dictionary; Metadata
I/O-efficient batched union-find and its applications to terrain analysis,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650633271&doi=10.1145%2f1868237.1868249&partnerID=40&md5=2ee960646b92c57735dc709114941897,"In this article we present an I/O-efficient algorithm for the batched (off-line) version of the union-find problem. Given any sequence of N union and find operations, where each union operation joins two distinct sets, our algorithm uses O(SORT(N)) = O(N/B logM/B N/B) I/Os, where M is the memory size and B is the disk block size. This bound is asymptotically optimal in the worst case. If there are union operations that join a set with itself, our algorithm uses O(SORT(N) + MST(N)) I/Os, where MST(N) is the number of I/Os needed to compute the minimum spanning tree of a graph with N edges. We also describe a simple and practical O(SORT(N) log(N/M ))-I/O algorithm for this problem, which we have implemented. We are interested in the union-find problem because of its applications in terrain analysis. A terrain can be abstracted as a height function defined over ℝ2, and many problems that deal with such functions require a union-find data structure. With the emergence of modern mapping technologies, huge amount of elevation data is being generated that is too large to fit in memory, thus I/O-efficient algorithms are needed to process this data efficiently. In this article, we study two terrain-analysis problems that benefit from a union-find data structure: (i) computing topological persistence and (ii) constructing the contour tree.We give the first O(SORT(N))-I/O algorithms for these two problems, assuming that the input terrain is represented as a triangular mesh with N vertices. © 2010 ACM.",I/O-efficient algorithms; Union-find,Data structures; Landforms; Topology; Trees (mathematics); Analysis problems; Asymptotically optimal; Disk block; Elevation data; Height functions; I/O-efficient algorithms; If there are; Memory size; Minimum spanning trees; Terrain analysis; Topological persistences; Triangular meshes; Union-find; Union-find data structures; Union-find problems; Worst case; Algorithms
Clustering lines in high-dimensional space: Classification of incomplete data,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650647964&doi=10.1145%2f1868237.1868246&partnerID=40&md5=40fce9b3b6db42ec50f96523d01fbc2f,"A set of κ balls B1,. . ., Bκ in a Euclidean space is said to cover a collection of lines if every line intersects some ball.We consider the κ-center problem for lines in high-dimensional space: Given a set of n lines l = {l1,. . ., ln} in ℝd, find κ balls of minimum radius which cover l. We present a 2-approximation algorithm for the cases κ = 2, 3 of this problem, having running time quasi-linear in the number of lines and the dimension of the ambient space. Our result for 3-clustering is strongly based on a new result in discrete geometry that may be of independent interest: a Helly-type theorem for collections of axis-parallel ""crosses"" in the plane. The family of crosses does not have finite Helly number in the usual sense. Our Helly theorem is of a new type: it depends on ε-contracting the sets. In statistical practice, data is often incompletely specified; we consider lines as the most elementary case of incompletely specified data points. Clustering of data is a key primitive in nonparametric statistics. Our results provide a way of performing this primitive on incomplete data, as well as imputing the missing values. © 2010 ACM.",κ-center; Clustering; Helly theorem; High dimension; Lines,Geometry; Clustering; Data points; Discrete geometry; Euclidean spaces; Helly number; Helly theorem; Helly-type theorems; High dimensional spaces; High dimensions; Incomplete data; Lines; Missing values; New results; Non-parametric statistics; Quasi-linear; Running time; Approximation algorithms
Nondecreasing paths in a weighted graph or: How to optimally read a train schedule,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956528814&doi=10.1145%2f1824777.1824790&partnerID=40&md5=56eb3fd7d9d60eb069602af5d3253b0a,"A travel booking office has timetables giving arrival and departure times for all scheduled trains, including their origins and destinations. A customer presents a starting station and demands a route with perhaps several train connections taking him to his destination as early as possible. The booking office must find the best route for its customers. This problem was first considered in the theory of algorithms by Minty [1958], who reduced it to a problem on directed edge-weighted graphs: find a path from a given source to a given target such that the consecutive weights on the path are nondecreasing and the last weight on the path is minimized. Minty gave the first algorithm for the single-source version of the problem, in which one finds minimum last weight nondecreasing paths from the source to every other vertex. In this article we give the first linear-time algorithm for this problem in the word-RAM model of computation. We also define an all-pairs version for the problem and give a strongly polynomial truly subcubic algorithm for it. Finally, we discuss an extension of the problem in which one also has prices on trip segments and one wishes to find a cheapest valid itinerary. © 2010 ACM 1549-6325/2010/08- ART70 $10.00.",All pairs; Earliest arrivals; Matrix products; Nondecreasing paths; Shortest paths; Single source,Clustering algorithms; Graphic methods; Locomotives; Railroad cars; All pairs; Earliest arrival; Matrix products; Nondecreasing paths; Shortest path; Single source; Graph theory
Foreword to special issue on SODA 2008,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956510399&doi=10.1145%2f1824777.1824793&partnerID=40&md5=4b1d9f03908338702f0060fcc610a892,[No abstract available],,
On distributing symmetric streaming computations,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956508106&doi=10.1145%2f1824777.1824786&partnerID=40&md5=78b4e376fe7bb8d8da95c65bda54f9a3,"A common approach for dealing with large datasets is to stream over the input in one pass, and perform computations using sublinear resources. For truly massive datasets, however, even making a single pass over the data is prohibitive. Therefore, streaming computations must be distributed over many machines. In practice, obtaining significant speedups using distributed computation has numerous challenges including synchronization, load balancing, overcoming processor failures, and data distribution. Successful systems in practice such as Google's MapReduce and Apache's Hadoop address these problems by only allowing a certain class of highly distributable tasks defined by local computations that can be applied in any order to the input. The fundamental question that arises is: How does the class of computational tasks supported by these systems differ from the class for which streaming solutions exist? We introduce a simple algorithmic model for massive, unordered, distributed (mud) computation, as implemented by these systems. We show that in principle, mud algorithms are equivalent in power to symmetric streaming algorithms. More precisely, we show that any symmetric (order- invariant) function that can be computed by a streaming algorithm can also be computed by a mud algorithm, with comparable space and communication complexity. Our simulation uses Savitch's theorem and therefore has superpolynomial time complexity. We extend our simulation result to some natural classes of approximate and randomized streaming algorithms. We also give negative © 2010 ACM 1549-6325/2010/08- ART66 $10.00.",Distributed; Distributed computations; Mapreduce; Streaming; Symmetric,Distributed computer systems; Distributed; Distributed computations; Map-reduce; Streaming; Symmetric; Algorithms
"Coresets, sparse greedy approximation, and the frank-wolfe algorithm",2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956546378&doi=10.1145%2f1824777.1824783&partnerID=40&md5=d8e66d081ca4b3acdbd750ed0a29e7d0,"The problem of maximizing a concave function f (x) in the unit simplex Δ can be solved approximately by a simple greedy algorithm. For given κ, the algorithm can find a point x(κ) on a κ-dimensional face of Δ, such that f (x(κ)) ≥ f (x*) - O(1/κ). Here f (x*) is the maximum value of f in A, and the constant factor depends on f. This algorithm and analysis were known before, and related to problems of statistics and machine learning, such as boosting, regression, and density mixture estimation. In other work, coming from computational geometry, the existence of e-coresets was shown for the minimum enclosing ball problem by means of a simple greedy algorithm. Similar greedy algorithms, which are special cases of the Frank-Wolfe algorithm, were described for other enclosure problems. Here these results are tied together, stronger convergence results are reviewed, and several coreset bounds are generalized or strengthened. ©2010 ACM 1549-6325/2010/08-ART63 $10.00.",Approximation; Boosting; Coresets; Minimum enclosing ball; Regression; Sparsity,Adaptive boosting; Computational geometry; Regression analysis; Approximation; Boosting; Core set; Minimum enclosing ball; Regression; Sparsity; Approximation algorithms
Finding equitable convex partitions of points in a polygon efficiently,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956552949&doi=10.1145%2f1824777.1824792&partnerID=40&md5=ae08723638a943386987b397eb2de6d8,"Previous work has developed algorithms for finding an equitable convex partition that partitions the plane into n convex pieces each containing an equal number of red and blue points. Motivated by a vehicle routing heuristic, we look at a related problem where each piece must contain one point and an equal fraction of the area of some convex polygon. We first show how algorithms for solving the older problem lead to approximate solutions for this new equitable convex partition problem. Then we demonstrate a new algorithm that finds an exact solution to our problem in O (Nn log N) time or operations, where n is the number of points, m the number of vertices or edges of the polygon, and N := n + m the sum. © 2010 ACM 1549-6325/2010/08-ART72 $10.00.",Algorithms; Performance; Theory,Approximate solution; Blue points; Convex polygon; Exact solution; Partition problem; Performance; Theory; Algorithms
On the dichromatic κ-set problem,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956506293&doi=10.1145%2f1824777.1824782&partnerID=40&md5=dfc1f25ac73c94c32491a985ab352b45,"We study a bichromatic version of the well-known κ-set problem: given two sets R and B of points of total size n andaninteger κ, how many subsets of the form (R ∩ h) ∪ (B \ h) can have size exactly κ over all halfspaces h? In the dual, the problem is asymptotically equivalent to determining the worst-case combinatorial complexity of the κ-level in an arrangement of n halfspaces. Disproving an earlier conjecture by Linhart [1993], we present the first nontrivial upper bound foi all κ < n in two dimensions: O(nk1/3 + n5/6εκ2/3+2ε + κ2)for anyfixed ε 0. In three dimensions, we obtain the bound O(nκ3/2 + n0.5034κ2'4932 + κ3). Incidentally, this also implies a new upper bound for the original κ-set problem in four dimensions: O(n27kappa;3/2 + nL5034 7kappa;2'4932 + nκ3), which improves the best previous result for all κ < n0923. Extensions to other cases, such as arrangements of disks. are also discussed. © 2010 ACM 1549-6325/2010/08-ART62 $ 10.00.",κ-levels; κ-sets; Arrangements; Combinatorial geometry,Arrangements; Combinatorial complexity; Combinatorial geometry; Half spaces; Set problems; Three dimensions; Two-dimension; Upper Bound
Two-phase greedy algorithms for some classes of combinatorial linear programs,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956557054&doi=10.1145%2f1824777.1824785&partnerID=40&md5=18b4af89e6efa2f79151e9d49d2c67d3,"We present greedy algorithms for some classes of combinatorial packing and cover problems within the general formal framework of Hoffman and Schwartz' lattice polyhedra. Our algorithms compute in a first phase Monge solutions for the associated dual cover and packing problems and then proceed to construct greedy solutions for the primal problems in a second phase. We show optimality of the algorithms under certain sub- and supermodular assumptions and monotone constraints. For supermodular lattice polyhedra with submodular constraints, our algorithms offer the farthest reaching generalization of Edmonds' polymatroid greedy algorithm currently known. © 2010 ACM 1549-6325/2010/08-ART65 $10.00.",Lattices; Submodularity,Algorithms; Geometry; Cover problem; Formal framework; Greedy algorithms; Lattices; Linear programs; Optimality; Packing problems; Polymatroids; Primal problem; Second phase; Submodular constraints; Submodularity; Supermodular; Combinatorial mathematics
ACM Transactions on Algorithms: Editorial note,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956540298&doi=10.1145%2f1824777.1824778&partnerID=40&md5=4ebb3fbb06cc19ad29a574cdb0c56a27,[No abstract available],,
Hausdorff distance under translation for points and balls,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956556230&doi=10.1145%2f1824777.1824791&partnerID=40&md5=0af7a28c38f5374d3b1f4d1f8eabb321,"We study the shape matching problem under the Hausdorff distance and its variants. In the first part of the article, we consider two sets A, B of balls in Rd, d = 2, 3, andwishtofind a translation t that minimizes the Hausdorff distance between A + t, the set of all balls in A shifted by t , and B. We consider several variants of this problem. First, we extend the notion of Hausdorff distance from sets of points to sets of balls, so that each ball has to be matched with the nearest ball in the other set. We also consider the problem in the standard setting, by computing the Hausdorff distance between the unions of the two sets (as point sets). Second, we consider either all possible translations t (as is the standard approach), or consider only translations that keep the balls of Α + t disjoint from those of Β. We propose several exact and approximation algorithms for these problems. In the second part of the article, we note that the Hausdorff distance is sensitive to outliers, and thus consider two variants that are more robust: the root-mean-square (rms) and the summed Hausdorff distance. We propose ef?cient approximation algorithms for computing the minimum rms and the minimum summed Hausdorff distances under translation, between two point sets in ℝd .Inorder to obtain a fast algorithm for the summed Hausdorff distance, we propose a deterministic ef?cient dynamic data structure for maintaining an ε-approximation of the 1-median of a set of points in ℝd, under insertions and deletions. © 2010 ACM 1549-6325/2010/08-ART71 $10.00.",Algorithms; Theory,Data structures; Geometry; Spheres; 1-Median; Dynamic data structure; Fast algorithms; Hausdorff distance; Insertions and deletions; Point set; Root mean squares; Shape matching; Standard setting; Theory; Two-point; Approximation algorithms
A near-linear-time algorithm for computing replacement paths in planar directed graphs,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956555566&doi=10.1145%2f1824777.1824784&partnerID=40&md5=00c51bae13dfc0cc448b392cd79a461e,"Let G = (V(G), E(G)) be a directed graph with nonnegative edge lengths and let P be a shortest path from s to t in G .Inthe replacement paths problem we are required to compute for every edge e in P, the length of a shortest path from s to t that avoids e. The fastest known algorithm for solving the problem in weighted directed graphs is the trivial one: each edge in P is removed from the graph in its turn and the distance from s to t in the modified graph is computed. The running time of this algorithm is O (mn + n2 log n), where n = | V(G)| and m = | E(G)|. The replacement paths problemis strongly motivated by two different applications. First, the fastest algorithm to compute the k simple shortest paths from s to t in directed graphs [Yen 1971; Lawler 1972] repeatedly computes the replacement paths from s to t . Its running time is O(kn(m +n log n)). Second, the computation of Vickrey pricing of edges in distributed networks can be reduced to the replacement paths problem. An open question raised by Nisan and Ronen [2001] asks whether it is possible to compute the Vickrey pricing faster than the trivial algorithm described in the previous paragraph. In this article we present a near-linear time algorithm for computing replacement paths in weighted planar directed graphs. In particular, the algorithm computes the lengths of the re-placement paths in O(n log3 n) time (recall that in planar graphs m = O(n)). This result immediately improves the running time of the two applications mentioned before by almost a linear factor. Our algorithm is obtained by combining several new ideas with a data structure of Klein [2005] that supports multisource shortest paths queries in planar directed graphs in logarithmic time. Our algorithm can be adapted to address the variant of the problem in which one is interested in the replacement path itself (rather than the length of the path). In that case the algorithm is executed in a preprocessing stage constructing a data structure that supports replacement path queries in time Õ(h), where h is the number of hops in the replacement path. In addition, we can handle the variant in which vertices should be avoided instead of edges. © 2010 ACM 1549-6325/2010/08-ART64 $10.00.",Planar graphs; Replacement paths,Clustering algorithms; Data structures; Graphic methods; Problem solving; Query processing; Wireless sensor networks; Directed graphs; Distributed networks; Linear-time algorithms; Logarithmic time; Multisources; Near-linear time; Nonnegative edge lengths; Number of hops; Planar graph; Replacement paths; Running time; Shortest path; Weighted directed graph; Graph theory
Lower-bounded facility location,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956546073&doi=10.1145%2f1824777.1824789&partnerID=40&md5=73730f6fd7c9a412c4eb2fb3ee9840e2,"We study the lower-bounded facility location problem which generalizes the classical uncapacitated facility location problem in that it comes with lower bound constraints for the number of clients assigned to a facility in the case that this facility is opened. This problem was introduced independently in the papers by Karger and Minkoff [2000] and by Guha et al. [2000], both of which give bicriteria approximation algorithms for it. These bicriteria algorithms come within a constant factor of the optimal solution cost, but they also violate the lower bound constraints by a constant factor. Our result in this article is the first true approximation algorithm for the lower-bounded facility location problem which respects the lower bound constraints and achieves a constant approximation ratio for the objective function. The main technical idea for the design of the algorithm is a reduction to the capacitated facility location problem, which has known constant-factor approximation algorithms. © 2010 ACM 1549-6325/2010/08-ART69 $10.00.",Approximation algorithm; Facility location,Approximation ratios; Bicriteria algorithm; Bicriteria approximation; Capacitated facility location problems; Constant factors; Constant-factor approximation algorithms; Facility location; Facility location problem; Facility locations; Lower bounds; Objective functions; Optimal solutions; Technical ideas; Approximation algorithms
A local algorithm for finding dense subgraphs,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956529455&doi=10.1145%2f1824777.1824780&partnerID=40&md5=8304665ea1399c8d6f6e25c759f28267,"We describe a local algorithm for finding subgraphs with high density, according to a measure of density introduced by Kannan and Vinay [1999]. The algorithm takes as input a bipartite graph G, a starting vertex v, and a parameter κ and outputs an induced subgraph of G. It is local in the sense that it does not examine the entire input graph; instead, it adaptively explores a region of the graph near the starting vertex. The running time of the algorithm is bounded by O (δκ2), which depends on the maximum degree δ, but is otherwise independent of the graph. We prove the following approximation guarantee: for any subgraph S with κ vertices and density θ, there exists aset S'⊆ S for which the algorithm outputs a subgraph with density ω(θ/ log δ) whenever v ∈ S' and κ ≥ κ'. We prove that S' contains at least half of the edges in S. © 2010 ACM 1549-6325/2010/08-ART60 $10.0.",Dense subgraphs; Local algorithms; Spectral graph theory,Approximation algorithms; Bipartite graphs; High density; Induced subgraphs; Input graphs; Local algorithm; Maximum degree; Running time; Spectral graph theory; Subgraphs; Graph theory
Fast asynchronous byzantine agreement and leader election with full information,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956543874&doi=10.1145%2f1824777.1824788&partnerID=40&md5=77acc8539001e6903d0f749e515cbeda,"We resolve two long-standing open problems in distributed computation by describing polylogarithmic protocols for Byzantine agreement and leader election in the asynchronous full information model with a nonadaptive malicious adversary. All past protocols for asynchronous Byzantine agreement had been exponential, and no protocol for asynchronous leader election had been known. Our protocols tolerate up to (1/3 - ∈) n faulty processors, for any positive constant ∈. They are Monte Carlo, succeeding with probability 1 - o(1) for Byzantine agreement, and constant probability for leader election. A key technical contribution of our article is a new approach for emulating Feige's lightest bin protocol, even with adversarial message scheduling. © 2010 ACM 1549-6325/2010/08-ART68 $10.00.",Asynchronous communication; Byzantine agreement; Distributed algorithms; Monte Carlo algorithms; Probabilistic method,Algorithms; Distributed computer systems; Asynchronous communication; Byzantine Agreement; Distributed algorithm; Monte Carlo algorithms; Probabilistic methods; Monte Carlo methods
Clustering for metric and nonmetric distance measures,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956515628&doi=10.1145%2f1824777.1824779&partnerID=40&md5=9acbd64a0acf6a1d6b32dd38f2d99129,"We study a generalization of the κ-median problem with respect to an arbitrary dissimilarity measure D. Given a finite set P of size n, our goal is to find a set C of size κ such that the sum of errors D(P, C) =∑p∈P minc∈C{D(p, c)} is minimized. The main result in this article can be stated as follows: There exists a (1 + ∈ )-approximation algorithm for the κ-median problem with respect to D, if the 1-median problem can be approximated within a factor of (1 + ∈) by taking a random sample of constant size and solving the 1-median problem on the sample exactly. This algorithm requires time n2(imklog(mk/∈)), where m is a constant that depends only on ∈ and D. Using this characterization, we obtain the first linear time (1 + ∈)-approximation algorithms for the κ-median problem in an arbitrary metric space with bounded doubling dimension, for the Kullback-Leibler divergence (relative entropy), for the Itakura-Saito divergence, for Mahalanobis distances, and for some special cases of Bregman divergences. Moreover, we obtain previously known results for the Euclidean κ-median problem and the Euclidean κ-means problem in a simplified manner. Our results are based on a new analysis of an algorithm of Kumar et al. [2004]. ©2010 ACM 1549-6325/2010/08-ART59 $10.00.",κ-means clustering; κ-median clustering; Approximation algorithm; Bregman divergences; Itakura-Saito divergence; Kullback-Leibler divergence; Mahalanobis distance; Random sampling,Approximation algorithms; Set theory; Topology; Bregman divergences; Itakura-Saito divergence; Kullback Leibler divergence; Mahalanobis distances; Random sampling; Clustering algorithms
Competitive weighted throughput analysis of greedy protocols on DAGs,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954410370&doi=10.1145%2f1798596.1798603&partnerID=40&md5=298bfd8387b78a947800d7a1164d1991,"The combination of the buffer sizes of routers deployed in the Internet, and the Internet traffic itself, leads routinely to the dropping of packets. Motivated by this, we are interested in the problem of maximizing the throughput of protocols that control packet networks. Moreover, we are interested in a setting where different packets have different priorities (or weights), thus taking into account Quality-of-Service considerations. We first extend the Competitive Network Throughput (CNT) model introduced by Aiello et al. [2003] to the weighted packets case. We analyze the performance of online, local-control protocols by their competitive ratio, in the face of arbitrary traffic, using as a measure the total weight of the packets that arrive to their destinations, rather than being dropped en-route. We prove that on Directed Acyclic Graphs (DAGs), any greedy protocol is competitive, with competitive ratio independent of the weights of the packets. Here we mean by a ""greedy protocol"" a protocol that not only does not leave a resource idle unnecessarily, but also prefers packets with higher weight over those with lower weight. We give two independent upper bounds on the competitive ratio of general greedy protocols on DAGs. We further give lower bounds that show that our upper bounds cannot be improved (other than constant factors) in the general case. Both our upper and lower bounds apply also to the unweighted case, and they improve the results given in Aiello et al. [2003] for that case. We thus give tight (up to constant factors) upper and lower bounds for both the unweighted and weighted cases. In the course of proving our upper bounds we prove a lemma that gives upper bounds on the delivery times of packets by any greedy protocol on general DAGs (without buffer size considerations). We believe that this lemma may be of independent interest and may find additional applications. © 2010 ACM.",,Internet; Throughput; Buffer sizes; Competitive network; Competitive ratio; Constant factors; Control packets; Control protocols; Delivery time; Directed acyclic graphs; En-route; Greedy protocols; Higher weight; Internet traffic; Lower bounds; Throughput analysis; Upper and lower bounds; Upper Bound; Internet protocols
Adaptive sampling strategies for quickselect,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954398290&doi=10.1145%2f1798596.1798606&partnerID=40&md5=a02d20b6b6ceeaa5facab381c57a2249,"Quickselect with median-of-3 is largely used in practice and its behavior is fairly well understood. However, the following natural adaptive variant, which we call proportion-from-3, had not been previously analyzed: ""choose as pivot the smallest of the sample if the relative rank of the sought element is below 1/3, the largest if the relative rank is above 2/3, and the median if the relative rank is between 1/3 and 2/3."" We first analyze the average number of comparisons made when using proportion-from-2 and then for proportion-from-3. We also analyze v-find, a generalization of proportion-from-3 with interval breakpoints at v and 1 - v. We show that there exists an optimal value of v and we also provide the range of values of v where v-find outperforms median-of-3. Then, we consider the average total cost of these strategies, which takes into account the cost of both comparisons and exchanges. Our results strongly suggest that a suitable implementation of v-find could be the method of choice in a practical setting. We also study the behavior of proportion-from-s with s > 3 and in particular we show that proportion-from-s-like strategies are optimal when s → ∞. © 2010 ACM.",Analysis; Analytic combinatorics; Divide-and-conquer; Find; Quickselect; Sampling; Selection,Adaptive sampling; Analytic combinatorics; Average numbers; Break-points; Divide and conquer; Optimal values; Total costs; Optimization
An explicit universal cycle for the (n - 1)-permutations of an n-set,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954420399&doi=10.1145%2f1798596.1798598&partnerID=40&md5=a938c8cb825c981ca3e64dfca2a794f7,"We show how to construct an explicit Hamilton cycle in the directed Cayley graph Cay({σn, σrn-1} : Sn), where σk is the rotation (12 ? ? ? k). The existence of such cycles was shown by Jackson [1996] but the proof only shows that a certain directed graph is Eulerian, and Knuth [2005] asks for an explicit construction. We show that a simple recursion describes our Hamilton cycle and that the cycle can be generated by an iterative algorithm that uses O(n) space. Moreover, the algorithm produces each successive edge of the cycle in constant time; such algorithms are said to be loopless. Finally, our Hamilton cycle can be used to construct an explicit universal cycle for the (n - 1)-permutations of a n-set, or as the basis of an efficient algorithm for generating every n-permutation of an n-set within a circular array or linked list. © 2010 ACM.",Loopless algorithm; Universal cycle,Data structures; Graph theory; Cayley graphs; Circular arrays; Constant time; Directed graphs; Efficient algorithm; Eulerian; Explicit constructions; Hamilton cycle; Iterative algorithm; Jackson; Loopless; Loopless algorithm; Recursions; Algorithms
"Finding heaviest H-subgraphs in real weighted graphs, with applications",2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954397924&doi=10.1145%2f1798596.1798597&partnerID=40&md5=313324e3cfa4c26a8e88a8eee273495e,"For a graph G with real weights assigned to the vertices (edges), the MAX H-SUBGRAPH problem is to find an H-subgraph of G with maximum total weight, if one exists. Our main results are new strongly polynomial algorithms for the MAX H-SUBGRAPH problem. Some of our algorithms are based, in part, on fast matrix multiplication. For vertex-weighted graphs with n vertices we solve a more general problem: The all pairs MAX H-SUBGRAPH problem, where the task is to find for every pair of vertices u, v, a maximum H-subgraph containing both u and v, if one exists. We obtain an 0(nt(ω,h))-time algorithm for the all pairs MAX H-SUBGRAPH problem in the case where H is a fixed graph with h vertices and ω < 2.376 is the exponent of matrix multiplication. The value of t(ω, h) is determined by solving a small integer program. In particular, heaviest triangles for all pairs can be found in 0(n2+1/(4-ω)) ≤ o(n2.616)-time. For h = 4, 5, 8 the running time of our algorithm essentially matches that of the (unweighted) H-subgraph detection problem. Using rectangular matrix multiplication, the value of t(ω, h) can be improved; for example, the runtime for triangles becomes O(n2.575). We also present improved algorithms for the MAX H-SUBGRAPH problem in the edge-weighted case. In particular, we obtain an 0(m2-1/k log n)-time algorithm for the heaviest cycle of length 2k or 2k - 1 in a graph with m edges and an 0(n3 /log n)-time randomized algorithm for finding the heaviest cycle of any fixed length. Our methods also yield efficient algorithms for several related problems that are faster than any previously existing algorithms. For example, we show how to find chromatic H-subgraphs in edge-colored graphs, and how to compute the most significant bits of the distance product of two real matrices, in truly subcubic time. © 2010 ACM.",,Algorithms; Graphic methods; Integer programming; Detection problems; Edge colored graphs; Efficient algorithm; Fast matrix multiplication; Fixed graphs; Graph G; Improved algorithm; Integer program; MAtrix multiplication; Most significant bit; Randomized Algorithms; Real matrices; Rectangular matrix; Running time; Runtimes; Strongly polynomial algorithm; Subgraph problems; Subgraphs; Time algorithms; Weighted graph; Graph theory
The directed circular arrangement problem,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954414402&doi=10.1145%2f1798596.1798600&partnerID=40&md5=109e7e72a32b54d98482267a31956f7a,"We consider the problem of embedding a directed graph onto evenly spaced points on a circle while minimizing the total weighted edge length. We present the first poly-logarithmic approximation factor algorithm for this problem which yields an approximation factor of O(log n log log n), thus improving the previous O(√n) approximation factor. In order to achieve this, we introduce a new problem which we call the directed penalized linear arrangement. This problem generalizes both the directed feedback edge set problem and the directed linear arrangement problem. We present an O(log n log log n)-approximation factor algorithm for this newly defined problem. Our solution uses two distinct directed metrics (""right"" and ""left"" ) which together yield a lower bound on the value of an optimal solution. In addition, we define a sequence of new directed spreading metrics that are used for applying the algorithm recursively on smaller subgraphs. The new spreading metrics allow us to define an asymmetric region growing procedure that accounts simultaneously for both incoming and outgoing edges. To the best of our knowledge, this is the first time that a region growing procedure is defined in directed graphs that allows for such an accounting. © 2010 ACM.",Region growing; Scheduling; Spreading metrics,Graph theory; Optimization; Approximation factor; Directed graphs; Edge length; Edge-sets; Linear arrangements; Logarithmic approximation; Lower bounds; Optimal solutions; Region growing; Subgraphs; Approximation algorithms
Achieving anonymity via clustering,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954396736&doi=10.1145%2f1798596.1798602&partnerID=40&md5=103b4b47a7330683eb1ae333e929bf99,"Publishing data for analysis from a table containing personal records, while maintaining individual privacy, is a problem of increasing importance today. The traditional approach of deidentifying records is to remove identifying fields such as social security number, name, etc. However, recent research has shown that a large fraction of the U.S. population can be identified using nonkey attributes (called quasi-identifiers) such as date of birth, gender, and zip code. The κ-anonymity model protects privacy via requiring that nonkey attributes that leak information are suppressed or © 2010 ACM.",Anonymity; Approximation algorithms; Clustering; Privacy,Cluster analysis; Clustering algorithms; Data privacy; Population statistics; Anonymity; Individual privacy; Social security numbers; Zip code; Approximation algorithms
Approximating the distance to monotonicity in high dimensions,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954393623&doi=10.1145%2f1798596.1798605&partnerID=40&md5=51145477efa48ba95870ae10ee7778ca,"In this article we study the problem of approximating the distance of a function Ÿ: [n]d → R to monotonicity where [n] = {1, ⋯, n} and R is some fully ordered range. Namely, we are interested in randomized sublinear algorithms that approximate the Hamming distance between a given function and the closest monotone function. We allow both an additive error, parameterized by δ, and a multiplicative error. Previous work on distance approximation to monotonicity focused on the one-dimensional case and the only explicit extension to higher dimensions was with a multiplicative approximation factor exponential in the dimension d. Building on Goldreich et al. [2000] and Dodis et al. [1999], in which there are better implicit results for the case n = 2, we describe a reduction from the case of functions over the d-dimensional hypercube [n]d to the case of functions over the κ-dimensional hypercube [n]κ, where 1 ≤ κ ≤ d. The quality of estimation that this reduction provides is linear in [d/κ] and logarithmic in the size of the range [R] (if the range is infinite or just very large, then log [R] can be replaced by d log n). Using this reduction and a known distance approximation algorithm for the one-dimensional case, we obtain a distance approximation algorithm for functions over the d-dimensional hypercube, with any range R, which has a multiplicative approximation factor of 0(d log [R]). For the case of a binary range, we present algorithms for distance approximation to monotonicity of functions over one dimension, two dimensions, and the κ-dimensional hypercube (for any κ ≥ 1). Applying these algorithms and the reduction described before, we obtain a variety of distance approximation algorithms for Boolean functions over the d-dimensional hypercube which suggest a trade-off between quality of estimation and efficiency of computation. In particular, the multiplicative error ranges between O(d) and 0(1). © 2010 ACM.",Distance approximation; Monotonicity; Property testing; Sublinear approximation algorithms,Boolean functions; Computational efficiency; Distance measurement; Geometry; Hamming distance; Additive errors; Approximation factor; High dimensions; Higher dimensions; Hypercube; Monotone functions; Monotonicity; Monotonicity property; Multiplicative errors; One dimension; Parameterized; Sublinear; Sublinear algorithm; Two-dimension; Approximation algorithms
Ordering by weighted number of wins gives a good ranking for weighted tournaments,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954412721&doi=10.1145%2f1798596.1798608&partnerID=40&md5=178813ab5b04239d1b0bd22b8a4daf59,"We consider the following simple algorithm for feedback arc set problem in weighted tournaments: order the vertices by their weighted indegrees. We show that this algorithm has an approximation guarantee of 5 if the weights satisfy probability constraints (for any pair of vertices u and v, wuv + wuv = 1). Special cases of the feedback arc set problem in such weighted tournaments include the feedback arc set problem in un weighted tournaments and rank aggregation. To complement the upper bound, for any constant ε > 0, we exhibit an infinite family of (un weighted) tournaments for which the aforesaid algorithm (irrespective of how ties are broken) has an approximation ratio of 5 - ε. © 2010 ACM.",Approximation algorithms; Borda-s method; Feedback are set problem; Rank aggregation; Tournaments,Borda-s method; Rank aggregation; S-method; Set problems; Tournaments; Approximation algorithms
Approximating corridors and tours via restriction and relaxation techniques,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954391259&doi=10.1145%2f1798596.1798609&partnerID=40&md5=b7ad413f3a0863c00bd47410b570eaeb,"Given a rectangular boundary partitioned into rectangles, the Minimum-Length Corridor (MLC-R) problem consists of finding a corridor of least total length. A corridor is a set of connected line segments, each of which must lie along the line segments that form the rectangular boundary and/or the boundary of the rectangles, and must include at least one point from the boundary of every rectangle and from the rectangular boundary. The MLC-R problem is known to be NP-hard. We present the first polynomial-time constant ratio approximation algorithm for the MLC-R and MLCκ problems. The MLCκ problem is a generalization of the MLC-R problem where the rectangles are rectilinear c-gons, for c ≤ κ and κ is a constant. We also present the first polynomial-time constant ratio approximation algorithm for the Group Traveling Salesperson Problem (GTSP) for a rectangular boundary partitioned into rectilinear c-gons as in the MLCκ problem. Our algorithms are based on the restriction and relaxation approximation techniques. © 2010 ACM.",Approximation algorithms; Complexity; Computational geometry; Corridors; Restriction and relaxation techniques,Computational complexity; Computational geometry; Polynomial approximation; Approximation techniques; Connected lines; Line segment; NP-hard; Polynomial-time; Relaxation techniques; Total length; Traveling salesperson problem; Approximation algorithms
A near-optimal algorithm for estimating the entropy of a stream,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954392247&doi=10.1145%2f1798596.1798604&partnerID=40&md5=621df52f8eed80004d8dde960ce8288f,"We describe a simple algorithm for approximating the empirical entropy of a stream of m values up to a multiplicative factor of (1 + ε) using a single pass, O(ε-2 log(δ-1)log m) words of space, and O(log ε-1 + log log δ-1 + log log m) processing time per item in the stream. Our algorithm is based upon a novel extension of a method introduced by Alon et al. [1999]. This improves over previous work on this problem. We show a space lower bound of ω(ε-2/ log2(ε-1)), demonstrating that our algorithm is near-optimal in terms of its dependency on ε. We show that generalizing to multiplicative-approximation of the κth-order entropy requires close to linear space for κ ≥ 1. In contrast we show that additive-approximation is possible in a single pass using only poly-logarithmic space. Lastly, we show how to compute a multiplicative approximation to the entropy of a random walk on an undirected graph. © 2010 ACM.",Approximation algorithms; Data streams; Entropy,Data communication systems; Entropy; Hydraulics; Optimization; Data stream; Data streams; Linear spaces; Lower bounds; Multiplicative factors; Near-optimal algorithms; Processing Time; Random Walk; SIMPLE algorithm; Single pass; Undirected graph; Approximation algorithms
An approximation algorithm for the maximum leaf spanning arborescence problem,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954388894&doi=10.1145%2f1798596.1798599&partnerID=40&md5=179468cba31214c06a9acdc43ef31c50,"We present an O(√OPT)-approximation algorithm for the maximum leaf spanning arborescence problem, where OPT is the number of leaves in an optimal spanning arborescence. The result is based upon an O(1)-approximation algorithm for a special class of directed graphs called willows. Incorporating the method for willow graphs as a subroutine in a local improvement algorithm gives the bound for general directed graphs. © 2010 ACM.",Approximation algorithms; Arborescence; Directed graphs; Maximum leaf,Graph theory; Graphic methods; Arborescence; Directed graphs; Special class; Approximation algorithms
Balanced families of perfect hash functions and their applications,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954392637&doi=10.1145%2f1798596.1798607&partnerID=40&md5=6bb9d5de86368b02267073b828aea5e8,"The construction of perfect hash functions is a well-studied topic. In this article, this concept is generalized with the following definition. We say that a family of functions from [n] to [κ] is a δ-balanced (n, κ)-family of perfect hash functions if for every S ⊆ [n], |S| = κ, the number of functions that are 1-1 on S is between T/δ and δT for some constant T > 0. The standard definition of a family of perfect hash functions requires that there will be at least one function that is 1-1 on S, for each S of size k. In the new notion of balanced families, we require the number of 1-1 functions to be almost the same (taking δ to be close to 1) for every such S. Our main result is that for any constant δ > 1, a δ-balanced (n, κ)-family of perfect hash functions of size 2O(κ log log κ) log n can be constructed in time 2O(κ log log κ)n log n. Using the technique of color-coding we can apply our explicit constructions to devise approximation algorithms for various counting problems in graphs. In particular, we exhibit a deterministic polynomial-time algorithm for approximating both the number of simple paths of length k and the number of simple cycles of size κ for any κ ≤ 0(log n/log log log n) in a graph with n vertices. The approximation is up to any fixed desirable relative error. © 2010 ACM.",Approximate counting of subgraphs; Color-coding; Perfect hashing,Approximation algorithms; Color; Graph theory; Hash functions; Approximate counting; Counting problems; Explicit constructions; Perfect hashing; Polynomial-time algorithms; Relative errors; Standard definitions; Subgraphs; Number theory
Distributed error confinement,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954391600&doi=10.1145%2f1798596.1798601&partnerID=40&md5=4a67e7be4ce5818596686eeedd0a001f,"We study error confinement in distributed applications, which can be viewed as an extreme case of various fault locality notions studied in the past. Error confinement means that to the external observer, only nodes that were directly hit by a fault may deviate from their specified correct behavior, and only temporarily. The externally observable behavior of all other nodes must remain impeccable, even though their internal state may be affected. Error confinement is impossible if an adversary is allowed to inflict arbitrary transient faults on the system, since the faults might completely wipe out input values. We introduce a new fault-tolerance measure we call agility, which quantifies the fault tolerance of an algorithm that disseminates information against state corrupting faults. We then propose broadcast algorithms that guarantee error confinement with optimal agility to within a constant factor in synchronous networks. These algorithms can serve as building blocks in more general reactive systems. Previous results in exploring locality in reactive systems were not error confined, or allowed a wide range of behaviors to be considered correct. Our results also include a new technique that can be used to analyze the ""cow path"" problem. © 2010 ACM.",Distributed algorithms; Persistence; Self-stabilization; Voting,Algorithms; Fault tolerant computer systems; Quality assurance; Stabilization; Broadcast algorithm; Building blockes; Constant factors; Distributed algorithm; Distributed applications; Error confinement; External observer; Extreme case; Input values; Internal state; Observable behavior; Reactive system; Self-stabilization; Synchronous networks; Transient faults; Fault tolerance
Dial a ride from k-forest,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950856321&doi=10.1145%2f1721837.1721857&partnerID=40&md5=914e27ce56398267f8612b89c7aa32e0,"The k-forest problem is a common generalization of both the k-MST and the dense-k-subgraph problems. Formally, given a metric space on n vertices V, with m demand pairs ⊆ V × V and a target k ≤ m, the goal is to find a minimum cost subgraph that connects at least k pairs. In this paper, we give an O(min{nlog k,k})-approximation algorithm for k-forest, improving on the previous best ratio of O(min {n2/3,m}log n) by Segev and Segev. We then apply our algsorithm for k-forest to obtain approximation algorithms for several Dial-a-Ride problems. The basic Dial-a-Ride problem is the following: given an n point metric space with m objects each with its own source and destination, and a vehicle capable of carrying at most k objects at any time, find the minimum length tour that uses this vehicle to move each object from its source to destination. We want that the tour be non-preemptive: that is, each object, once picked up at its source, is dropped only at its destination. We prove that an α-approximation algorithm for the k-forest problem implies an O(αlog2n)-approximation algorithm for Dial-a-Ride. Using our results for k-forest, we get an O(min{n,k}log2 n)-approximation algorithm for Dial-a-Ride. The only previous result known for Dial-a-Ride was an O(klog n)-approximation by Charikar and Raghavachari; our results give a different proof of a similar approximation guaranteein fact, when the vehicle capacity k is large, we give a slight improvement on their results. The reduction from Dial-a-Ride to the k-forest problem is fairly robust, and allows us to obtain approximation algorithms (with the same guarantee) for some interesting generalizations of Dial-a-Ride. © 2010 ACM.",Approximation algorithms; Network design; Vehicle routing,Set theory; Topology; Vehicles; Charikar; Dial-a-Ride; Dial-a-ride problem; Metric spaces; Minimum cost; Network design; Non-preemptive; Subgraph problems; Subgraphs; Vehicle capacity; Approximation algorithms
ACM Transactions on Algorithms: Editorial note,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950852322&doi=10.1145%2f1721837.1721838&partnerID=40&md5=355d7db38249f6c7b8031622b829319c,[No abstract available],,
Truthful unsplittable flow for large capacity networks,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950854107&doi=10.1145%2f1721837.1721852&partnerID=40&md5=85ea7cfeee32f1e81d1fedb997eb0537,"The unsplittable flow problem is one of the most extensively studied optimization problems in the field of networking. An instance of it consists of an edge capacitated graph and a set of connection requests, each of which is associated with source and target vertices, a demand, and a value. The objective is to route a maximum value subset of requests subject to the edge capacities. It is a well known fact that as the capacities of the edges are larger with respect to the maximal demand among the requests, the problem can be approximated better. In particular, it is known that for sufficiently large capacities, the integrality gap of the corresponding integer linear program becomes 1 + ε, which can be matched by an algorithm that utilizes the randomized rounding technique. In this article, we focus our attention on the large capacities unsplittable flow problem in a game theoretic setting. In this setting, there are selfish agents, which control some of the requests characteristics, and may be dishonest about them. It is worth noting that in game theoretic settings many standard techniques, such as randomized rounding, violate certain monotonicity properties, which are imperative for truthfulness, and therefore cannot be employed. In light of this state of affairs, we design a monotone deterministic algorithm, which is based on a primal-dual machinery, which attains an approximation ratio of e/e-1, up to a disparity of away. This implies an improvement on the current best truthful mechanism, as well as an improvement on the current best combinatorial algorithm for the problem under consideration. Surprisingly, we demonstrate that any algorithm in the family of reasonable iterative path minimizing algorithms, cannot yield a better approximation ratio. Consequently, it follows that in order to achieve a monotone PTAS, if that exists, one would have to exert different techniques. We also consider the large capacities single-minded multi-unit combinatorial auction problem. This problem is closely related to the unsplittable flow problem since one can formulate it as a special case of the integer linear program of the unsplittable flow problem. Accordingly, we obtain a comparable performance guarantee by refining the algorithm suggested for the unsplittable flow problem. © 2010 ACM.",Approximation algorithms; Combinatorial and multi-unit auctions; Mechanism design; Primal-dual method,Combinatorial mathematics; Commerce; Game theory; Integer programming; Machine design; Machinery; A-monotone; Approximation ratios; Combinatorial algorithm; Deterministic algorithms; Edge capacity; Integer linear programs; Integrality gaps; Maximum values; Mechanism design; Monotonicity property; Multi-unit auction; Multi-unit combinatorial auctions; Optimization problems; Performance guarantees; Primal-dual; Primal-dual method; Primal-dual methods; Randomized rounding; Selfish Agents; State of affairs; Truthful mechanisms; Unsplittable flow; Approximation algorithms
Comparison-based time-space lower bounds for selection,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950795839&doi=10.1145%2f1721837.1721842&partnerID=40&md5=18121368503225c91cebe514c771cb83,"We establish the first nontrivial lower bounds on time-space trade-offs for the selection problem. We prove that any comparison-based randomized algorithm for finding the median requires Ω(nlog logS n) expected time in the RAM model (or more generally in the comparison branching program model), if we have S bits of extra space besides the read-only input array. This bound is tight for all S log n, and remains true even if the array is given in a random order. Our result thus answers a 16-year-old question of Munro and Raman [1996], and also complements recent lower bounds that are restricted to sequential access, as in the multipass streaming model [Chakrabarti et al. 2008b]. We also prove that any comparison-based, deterministic, multipass streaming algorithm for finding the median requires Ω(n log*(n/s)+ nlog s n) worst-case time (in scanning plus comparisons), if we have s cells of space. This bound is also tight for all s log2 n. We get deterministic lower bounds for I/O-efficient algorithms as well. The proofs in this article are self-contained and do not rely on communication complexity techniques. © 2010 ACM.",Adversary arguments; Lower bounds; Median finding; RAM; Randomized algorithms; Streaming algorithms; Time-space trade-offs,Commerce; Adversary argument; Lower bounds; Randomized Algorithms; Streaming algorithm; Time-space; Algorithms
Finding shortest contractible and shortest separating cycles in embedded graphs,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950821253&doi=10.1145%2f1721837.1721840&partnerID=40&md5=7425921e4a91b85ec04c806350af0f9f,"We give a polynomial-time algorithm to find a shortest contractible cycle (i.e., a closed walk without repeated vertices) in a graph embedded in a surface. This answers a question posed by Hutchinson. In contrast, we show that finding a shortest contractible cycle through a given vertex is NP-hard. We also show that finding a shortest separating cycle in an embedded graph is NP-hard. This answers a question posed by Mohar and Thomassen. © 2010 ACM.",3-path condition; Forbidden pairs; Graphs on surfaces; Topological graph theory,Computational complexity; 3-path condition; Embedded graphs; NP-hard; Path condition; Polynomial-time algorithms; Topological graphs; Graph theory
Approximating fractional hypertree width,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950818751&doi=10.1145%2f1721837.1721845&partnerID=40&md5=50b4e031bd67e960885be51537868ef9,"Fractional hypertree width is a hypergraph measure similar to tree width and hypertree width. Its algorithmic importance comes from the fact that, as shown in previous work, Constraint Satisfaction Problems (CSP) and various problems in database theory are polynomial-time solvable if the input contains a bounded-width fractional hypertree decomposition of the hypergraph of the constraints. In this article, we show that for every fixed w ≥ 1, there is a polynomial-time algorithm that, given a hypergraph H with fractional hypertree width at most w, computes a fractional hypertree decomposition of width O(w 3) for H. This means that polynomial-time algorithms relying on bounded-width fractional hypertree decompositions no longer need to be given a decomposition explicitly in the input, since an appropriate decomposition can be computed in polynomial time. Therefore, if H is a class of hypergraphs with bounded fractional hypertree width, then a CSP restricted to instances whose structure is in H is polynomial-time solvable. This makes bounded fractional hypertree width the most general known hypergraph property that makes CSP, Boolean conjunctive queries, and conjunctive query containment polynomial-time solvable. © 2010 ACM.",Constraint satisfaction; Fractional hypertree width; Treewidth,Computer operating procedures; Graph theory; Polynomial approximation; Query languages; Conjunctive queries; Conjunctive-query containment; Constraint Satisfaction; Constraint Satisfaction Problems; Data base theory; Hypergraph; Hypertree decomposition; Polynomial-time; Polynomial-time algorithms; Tree-width; Constraint theory
Labeling schemes for vertex connectivity,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950833134&doi=10.1145%2f1721837.1721855&partnerID=40&md5=be6f2efcc9c9bb03c5c3cad15ac26509,"This article studies labeling schemes for the vertex connectivity function on general graphs. We consider the problem of assigning short labels to the nodes of any n-node graph is such a way that given the labels of any two nodes u and v, one can decide whether u and v are k-vertex connected in G, that is, whether there exist k vertex disjoint paths connecting u and v. This article establishes an upper bound of k2log n on the number of bits used in a label. The best previous upper bound for the label size of such a labeling scheme is 2klog n. © 2010 ACM.",Graph algorithms; Labeling schemes; Vertex-connectivity,Algorithms; Mathematical techniques; General graph; Graph algorithms; Labeling scheme; Node graph; Upper Bound; Vertex connectivity; Vertex disjoint paths; Graph theory
Facility location with hierarchical facility costs,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950787466&doi=10.1145%2f1721837.1721853&partnerID=40&md5=39d61952e94cf6df45fe086a51c9c577,"We introduce a facility location problem with submodular facility cost functions, and give an O(log n) approximation algorithm for it. Then we focus on a special case of submodular costs, called hierarchical facility costs, and give a (4.237 + ε)-approximation algorithm using local search. The hierarchical facility costs model multilevel service installation. Shmoys et al. [2004] gave a constant factor approximation algorithm for a two-level version of the problem. Here we consider a multilevel problem, and give a constant factor approximation algorithm, independent of the number of levels, for the case of identical costs on all facilities. © 2010 ACM.",Approximation algorithm; Facility location; Local search; Submodular function,Cost functions; Costs; Constant-factor approximation algorithms; Facility location problem; Facility locations; Local search; Service installation; Submodular; Submodular function; Submodular functions; Approximation algorithms
A fast algorithm to generate open meandric systems and meanders,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950806043&doi=10.1145%2f1721837.1721858&partnerID=40&md5=44f91cc497a464e0dcd1909bdc755a6d,"An open meandric system is a planar configuration of acyclic curves crossing an infinite horizontal line in the plane such that the curves may extend in both horizontal directions. We present a fast, recursive algorithm to exhaustively generate open meandric systems with n crossings. We then illustrate how to modify the algorithm to generate unidirectional open meandric systems (the curves extend only to the right) and nonisomorphic open meandric systems where equivalence is taken under horizontal reflection. Each algorithm can be modified to generate systems with exactly k curves. In the unidirectional case when k = 1, we can apply a minor modification along with some additional optimization steps to yield the first fast and simple algorithm to generate open meanders. © 2010 ACM.",CAT algorithm; Meander; Open meandric system,Algorithms; CAT algorithm; Fast algorithms; Planar configurations; Recursive algorithms; SIMPLE algorithm; Curve fitting
Optimization problems in multiple-interval graphs,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950842860&doi=10.1145%2f1721837.1721856&partnerID=40&md5=f0334441b79b1c6b9cda5e3d449f0582,"Multiple-interval graphs are a natural generalization of interval graphs where each vertex may have more then one interval associated with it. We initiate the study of optimization problems in multiple-interval graphs by considering three classical problems: Minimum Vertex Cover, Minimum Dominating Set, and Maximum Clique. We describe applications for each one of these problems, and then proceed to discuss approximation algorithms for them. Our results can be summarized as follows: Let t be the number of intervals associated with each vertex in a given multiple-interval graph. For Minimum Vertex Cover, we give a (2-1/t)-approximation algorithm which also works when a t-interval representation of our given graph is absent. Following this, we give a t2-approximation algorithm for Minimum Dominating Set which adapts well to more general variants of the problem. We then proceed to prove that Maximum Clique is NP-hard already for 3-interval graphs, and provide a (t 2-t+1)/2-approximation algorithm for general values of t ≥ 2, using bounds proven for the so-called transversal number of t-interval families. © 2010 ACM.",Approximation algorithms; Maximum clique; Minimum dominating set; Minimum vertex cover; Multiple-interval graphs; T-interval graphs,Graphic methods; Interval graph; Maximum clique; Minimum dominating set; Minimum vertex cover; T-interval graphs; Approximation algorithms
Approximate shared-memory counting despite a strong adversary,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950795090&doi=10.1145%2f1721837.1721841&partnerID=40&md5=1698a4122634d43719e164a13d0a790e,"A new randomized asynchronous shared-memory data structure is given for implementing an approximate counter that can be incremented once by each of n processes in a model that allows up to n - 1 crash failures. For any fixed ε, the counter achieves a relative error of δ with high probability, at the cost of O(((1/δ) log n)O(1/ε)) register operations per increment and O(n4/5+ε((1/δ) log n) O(1/ε)) register operations per read. The counter combines randomized sampling for estimating large values with an expander for estimating small values. This is the first counter implementation that is sublinear the number of processes and works despite a strong adversary scheduler that can observe internal states of processes. An application of the improved counter is an improved protocol for solving randomized sharedmemory consensus, which reduces the best previously known individual work complexity from O(n log n) to an optimal O(n), resolving one of the last remaining open problems concerning consensus in this model. © 2010 ACM.",Approximate counting; Consensus; Distributed computing; Expanders; Martingales,Approximate counting; Crash failures; Distributed Computing; High probability; Internal state; Open problems; Randomized sampling; Relative errors; Shared memories; Sublinear; Work complexity; Data structures
Discounted deterministic Markov decision processes and discounted all-pairs shortest paths,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950836623&doi=10.1145%2f1721837.1721849&partnerID=40&md5=7c19173a2121bed5affe5ef8bc239488,"We present algorithms for finding optimal strategies for discounted, infinite-horizon, Determinsitc Markov Decision Processes (DMDPs). Our fastest algorithm has a worst-case running time of O(mn), improving the recent bound of O(mn2) obtained by Andersson and Vorbyov [2006]. We also present a randomized O(m1/2n2)-time algorithm for finding Discounted All-Pairs Shortest Paths (DAPSP), improving an O(mn2)-time algorithm that can be obtained using ideas of Papadimitriou and Tsitsiklis [1987]. © 2010 ACM.",Markov decision processes; Minimum mean weight cycles; Shortest paths,Algorithms; Markov processes; Finding optimal strategies; Infinite horizons; Markov Decision Processes; Mean weight; Running time; Shortest path; Time algorithms; Graph theory
Periodicity testing with sublinear samples and space,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950813642&doi=10.1145%2f1721837.1721859&partnerID=40&md5=328e0d0d95de4f87fb0403820a04f0ef,"In this work, we are interested in periodic trends in long data streams in the presence of computational constraints. To this end; we present algorithms for discovering periodic trends in the combinatorial property testing model in a data stream S of length n using o(n) samples and space. In accordance with the property testing model, we first explore the notion of being close to periodic by defining three different notions of self-distance through relaxing different notions of exact periodicity. An input S is then called approximately periodic if it exhibits a small self-distance (with respect to any one self-distance defined). We show that even though the different definitions of exact periodicity are equivalent, the resulting definitions of self-distance and approximate periodicity are not; we also show that these self-distances are constant approximations of each other. Afterwards, we present algorithms which distinguish between the two cases where S is exactly periodic and S is far from periodic with only a constant probability of error. Our algorithms sample only O(nlog2 n) (or O(nlog4 n), depending on the self-distance) positions and use as much space. They can also find, using o(n) samples and space, the largest/smallest period, and/or all of the approximate periods of S. These algorithms can also be viewed as working on streaming inputs where each data item is seen once and in order, storing only a sublinear (O(nlog 2 n) or O(nlog4 n)) size sample from which periodicities are identified. © 2010 ACM.",Combinatorial property testing; Periodicity,Algorithms; Data communication systems; Approximate period; Combinatorial properties; Computational constraints; Data items; Data stream; Periodicity testing; Probability of errors; Property-testing; Self-distance; Sublinear; Combinatorial mathematics
Dynamic pricing for impatient bidders,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950847842&doi=10.1145%2f1721837.1721851&partnerID=40&md5=dde51b66f77c55d6fffa4b29a5b4e2d4,"We study the following problem related to pricing over time. Assume there is a collection of bidders, each of whom is interested in buying a copy of an item of which there is an unlimited supply. Every bidder is associated with a time interval over which the bidder will consider buying a copy of the item, and a maximum value the bidder is willing to pay for the item. On every time unit, the seller sets a price for the item. The seller's goal is to set the prices so as to maximize revenue from the sale of copies of items over the time period. In the first model considered, we assume that all bidders are impatient, that is, bidders buy the item at the first time unit within their bid interval that they can afford the price. To the best of our knowledge, this is the first work that considers this model. In the offline setting, we assume that the seller knows the bids of all the bidders in advance. In the online setting we assume that at each time unit the seller only knows the values of the bids that have arrived before or at that time unit. We give a polynomial time offline algorithm and prove upper and lower bounds on the competitiveness of deterministic and randomized online algorithms, compared with the optimal offline solution. The gap between the upper and lower bounds is quadratic. We also consider the envy-free model in which bidders are sold the item at the minimum price during their bid interval, as long as it is not over their limit value. We prove tight bounds on the competitiveness of deterministic online algorithms for this model, and upper and lower bounds on the competitiveness of randomized algorithms with quadratic gap. The lower bounds for the randomized case in both models use a novel general technique. © 2010 ACM.",Digital goods; Online algorithms; Pricing,Algorithms; Competition; Economics; Polynomial approximation; Deterministic online algorithms; Digital goods; Dynamic pricing; Following problem; Free model; Limit values; Lower bounds; Maximum values; Off-line algorithm; Offline; On-line algorithms; On-line setting; Polynomial-time; Randomized Algorithms; Tight bound; Time interval; Time periods; Time units; Upper and lower bounds; Costs
A 4k2 kernel for feedback vertex set,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950820874&doi=10.1145%2f1721837.1721848&partnerID=40&md5=ee6cc738e56edc1cfb64928358ef631e,"We prove that given an undirected graph G on n vertices and an integer k, one can compute, in polynomial time in n, a graph G′ with at most 4k 2 vertices and an integer k′ such that G has a feedback vertex set of size at most k iff G′ has a feedback vertex set of size at most k′. This result improves a previous O(k11) kernel of Burrage et al., and a more recent cubic kernel of Bodlaender. This problem was communicated by Fellows. © 2010 ACM.",Feedback vertex set; Fixed parameter tractability; Kernelization; Matching,Feedback vertex set; Fixed-parameter tractability; Graph G; Kernelization; Polynomial-time; Undirected graph; Polynomial approximation
Shortest paths in directed planar graphs with negative lengths: A linear-space O(n log2 n)-time algorithm,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950798339&doi=10.1145%2f1721837.1721846&partnerID=40&md5=2e2e247efd1a985779b1ec880fcf4a90,"We give an O(n log2 n)-time, linear-space algorithm that, given a directed planar graph with positive and negative arc-lengths, and given a node s, finds the distances from s to all nodes. © 2010 ACM.",Monge; Planar graphs; Replacement paths; Shortest paths,Graph theory; Linear spaces; Planar graph; Replacement paths; Shortest path; Time algorithms; Graphic methods
Maximal biconnected subgraphs of random planar graphs,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950810272&doi=10.1145%2f1721837.1721847&partnerID=40&md5=bb042815824d4e7042a9b735401eaf87,"Let C be a class of labeled connected graphs, and let Cn be a graph drawn uniformly at random from graphs in C that contain exactly n vertices. Denote by b(ℓ; Cn) the number of blocks (i.e., maximal biconnected subgraphs) of Cn that contain exactly ℓ vertices, and let lb(Cn) be the number of vertices in a largest block of C n. We show that under certain general assumptions on C, Cn belongs with high probability to one of the following categories: (1) lb(C n) ∼ cn, for some explicitly given c = c(C), and the second largest block is of order nα, where 1 > α = α(C), or (2) lb(Cn) = O(log n), that is, all blocks contain at most logarithmically many vertices. Moreover, in both cases we show that the quantity b(ℓ; Cn) is concentrated for all ℓ and we determine its expected value. As a corollary we obtain that the class of planar graphs belongs to category (1). In contrast to that, outerplanar and series-parallel graphs belong to category (2). © 2010 ACM.",Graphs with constraints; Planar graphs; Random structures,Connected graph; Expected values; High probability; Number of blocks; Planar graph; Random structures; Series-parallel graph; Subgraphs; Graphic methods
Reasoning about online algorithms with weighted automata,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950802434&doi=10.1145%2f1721837.1721844&partnerID=40&md5=e3209fdea8f412db1bad20c8adff8d9e,"We describe an automata-theoretic approach for the competitive analysis of online algorithms. Our approach is based on weighted automata, which assign to each input word a cost in R≥0. By relating the unbounded look ahead of optimal offline algorithms with nondeterminism, and relating the no look ahead of online algorithms with determinism, we are able to solve problems about the competitive ratio of online algorithms, and the memory they require, by reducing them to questions about determinization and approximated determinization of weighted automata. © 2010 ACM.",Formal verification; Online algorithms; Weighted automata,Algorithms; Robots; Translation (languages); Automata-theoretic approach; Competitive analysis; Competitive ratio; Determinization; Formal verifications; Look-ahead; Non-determinism; Off-line algorithm; On-line algorithms; Weighted automata; Automata theory
Foreword to special issue SODA 2009,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950809501&doi=10.1145%2f1721837.1721839&partnerID=40&md5=3fc10b5db28005b9b6b1f64f1bec11db,[No abstract available],,
Efficient algorithms for the 2-gathering problem,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950817533&doi=10.1145%2f1721837.1721850&partnerID=40&md5=22d44cd0c59070e82511b4cbd9b562c1,"Pebbles are placed on some vertices of a directed graph. Is it possible to move each pebble along at most one edge of the graph so that in the final configuration no pebble is left on its own? We give an O(mn)-time algorithm for solving this problem, which we call the 2-gathering problem, where n is the number of vertices and m is the number of edges of the graph. If such a 2-gathering is not possible, the algorithm finds a solution that minimizes the number of solitary pebbles. The 2-gathering problem forms a nontrivial generalization of the nonbipartite matching problem and it is solved by extending the augmenting paths technique used to solve matching problems. © 2010 ACM.",2-gatherings; Augmenting paths; Nonbipartite matchings,Algorithms; Augmenting path; Directed graphs; Efficient algorithm; Gathering problem; Matching problems; Matchings; Time algorithms; Problem solving
Mechanism design for fractional scheduling on unrelated machines,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950794360&doi=10.1145%2f1721837.1721854&partnerID=40&md5=4c4e783997c029e8ae3a4164e73596aa,"Scheduling on unrelated machines is one of the most general and classical variants of the task scheduling problem. Fractional scheduling is the LP-relaxation of the problem, which is polynomially solvable in the nonstrategic setting, and is a useful tool to design deterministic and randomized approximation algorithms. The mechanism design version of the scheduling problem was introduced by Nisan and Ronen. In this article, we consider the mechanism design version of the fractional variant of this problem. We give lower bounds for any fractional truthful mechanism. Our lower bounds also hold for any (randomized) mechanism for the integral case. In the positive direction, we propose a truthful mechanism that achieves approximation 3/2 for 2 machines, matching the lower bound. This is the first new tight bound on the approximation ratio of this problem, after the tight bound of 2, for 2 machines, obtained by Nisan and Ronen. For n machines, our mechanism achieves an approximation ratio of n+1/2. Motivated by the fact that all the known deterministic and randomized mechanisms for the problem assign each task independently from the others, we focus on an interesting subclass of allocation algorithms, the task-independent algorithms. We give a lower bound of n+1/2, that holds for every (not only monotone) allocation algorithm that takes independent decisions. Under this consideration, our truthful independent mechanism is the best that we can hope from this family of algorithms. © 2010 ACM.",Scheduling; Truthful mechanisms; Unrelated machines,Approximation algorithms; Allocation algorithm; Approximation ratios; Lower bounds; LP-relaxation; Mechanism design; Randomized approximation; Randomized mechanism; Scheduling problem; Task scheduling problem; Tight bound; Truthful mechanisms; Unrelated machines; Machine design
Perfect matchings via uniform sampling in regular bipartite graphs,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950832187&doi=10.1145%2f1721837.1721843&partnerID=40&md5=942049e8654f18a6e7af6b2cd05bd3f8,"In this article we further investigate the well-studied problem of finding a perfect matching in a regular bipartite graph. The first nontrivial algorithm, with running time O(mn), dates back to König's work in 1916 (here m=nd is the number of edges in the graph, 2n is the number of vertices, and d is the degree of each node). The currently most efficient algorithm takes time O(m), and is due to Cole et al. [2001]. We improve this running time to O(min{m, n2.5ln n/d}); this minimum can never be larger than O(n 1.75ln n). We obtain this improvement by proving a uniform sampling theorem: if we sample each edge in a d-regular bipartite graph independently with a probability p = O(n ln n/d2) then the resulting graph has a perfect matching with high probability. The proof involves a decomposition of the graph into pieces which are guaranteed to have many perfect matchings but do not have any small cuts. We then establish a correspondence between potential witnesses to nonexistence of a matching (after sampling) in any piece and cuts of comparable size in that same piece. Karger's sampling theorem [1994a, 1994b] for preserving cuts in a graph can now be adapted to prove our uniform sampling theorem for preserving perfect matchings. Using the O(mn) algorithm (due to Hopcroft and Karp [1973]) for finding maximum matchings in bipartite graphs on the sampled graph then yields the stated running time. We also provide an infinite family of instances to show that our uniform sampling result is tight up to polylogarithmic factors (in fact, up to ln2 n). © 2010 ACM.",Perfect matching; Regular bipartite graphs,Algorithms; Graphic methods; Theorem proving; Bipartite graphs; Efficient algorithm; High probability; Maximum matchings; Non-trivial algorithms; Perfect matchings; Poly-logarithmic factors; Running time; Sampling theorems; Uniform sampling; Graph theory
Geodesic delaunay triangulations in bounded planar domains,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956546377&doi=10.1145%2f1824777.1824787&partnerID=40&md5=283fd6bdab241452ee34c7f6f439e4c0,"We introduce a new feature size for bounded domains in the plane endowed with an intrinsic metric. Given a point x in a domain X , the systolic feature size of X at x measures half the length of the shortest loop through x that is not null-homotopic in X. The resort to an intrinsic metric makes the systolic feature size rather insensitive to the local geometry of the domain, in contrast with its predecessors (local feature size, weak feature size, homology feature size). This reduces the number of samples required to capture the topology of X, provided that a reliable approximation to the intrinsic metric of X is available. Under sufficient sampling conditions involving the systolic feature size, we show that the geodesic Delaunay triangulation DX (L) of a finite sampling L is homotopy equivalent to X. Under similar conditions, D X (L) is sandwiched between the geodesic witness complex C xw(L) and a relaxed version Cx, vw(L). In the conference version of the article, we took advantage of this fact and proved that the homology of DX (L) (and hence the one of X) can be retrieved by computing the persistent homology between CX[(L)and Cx, vw (L). Here, we investigate further and show that the homology of X can also be recovered from the persistent homology associated with inclusions of type C x, vw(L) → Cx, vw (L), under some conditions on the parameters v ≤ v'. Similar results are obtained for Vietoris-Rips complexes in the intrinsic metric. The proofs draw some connections with recent advances on the front of homology inference from point cloud data, but also with several well-known © 2010 ACM 1549-6325/2010/08-ART67 $10.00.",Alexandrov space; Delaunay triangulation; Feature size; Intrinsic metric; Systole; Witness complex,Triangulation; Alexandrov space; Delau-nay triangulations; Feature sizes; Intrinsic metric; Systole; Witness complex; Geodesy
Finding one tight cycle,2010,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954917057&doi=10.1145%2f1824777.1824781&partnerID=40&md5=5405ef4ac4ce7eaf90f81ae69b485620,"A cycle on a combinatorial surface is tight if it as short as possible in its (free) homotopy class. We describe an algorithm to compute a single tight, noncontractible, essentially simple cycle on a given orientable combinatorial surface in O(n log n) time. The only method previously known for this problem was to compute the globally shortest noncontractible or nonseparating cycle in O(min{g3, n} n log n) time, where g is the genus of the surface. As a consequence, we can compute the shortest cycle freely homotopic to a chosen boundary cycle in O (n log n) time, a tight octagonal decomposition in O(gn log n) time, and a shortest contractible cycle enclosing a nonempty set of faces in O(n log2 n) time. ©2010 ACM 1549-6325/2010/08-ART61 $10.00.",Combinatorial surface; Noncontractible cycle; Nonseparating cycle; Topological graph theory,Algorithms; Mathematical techniques; Homotopic; Homotopy class; Non-separating cycles; Noncontractible cycle; Orientable; Shortest cycle; Topological graph theories; Graph theory
Low distortion spanners,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049159499&doi=10.1145%2f1644015.1644022&partnerID=40&md5=879bdf7c00e537db1e3e5d4de5d6c7a7,"A spanner of an undirected unweighted graph is a subgraph that approximates the distance metric of the original graph with some specified accuracy. Specifically,we say H ⊆ G is an f 'spanner of G if any two vertices u, v at distance d in G are at distance at most f (d) in H. There is clearly some trade-off between the sparsity of H and the distortion function f , though the nature of the optimal trade-off is still poorly understood. In this article we present a simple, modular framework for constructing sparse spanners that is based on interchangable components called connection schemes. By assembling connection schemes in different ways we can recreate the additive 2- and 6-spanners of Aingworth et al. [1999] and Baswana et al. [2009], and give spanners whose multiplicative distortion quickly tends toward 1. Our results rival the simplicity of all previous algorithms and provide substantial improvements (up to a doubly exponential reduction in edge density) over the comparable spanners of Elkin and Peleg [2004] and Thorup and Zwick [2006]. © 2009 ACM.",Metric embedding; Spanner,Connection schemes; Distance metrics; Distortion functions; Edge densities; Exponential reduction; Interchangable components; Low distortion; Metric embeddings; Modular framework; Subgraphs; Unweighted graphs
Jitter regulation for multiple streams,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049106162&doi=10.1145%2f1644015.1644027&partnerID=40&md5=318ce6ff20052c0bb9cb3790de432cf5,"For widely used interactive communication, it is essential that traffic is kept as smooth as possible; the smoothness of the traffic is typically captured by its delay jitter, that is, the difference between the maximal and minimal end-to-end delays. The task of minimizing the jitter is done by jitter regulators that use a limited-size buffer in order to shape the traffic. In many real-life situations regulators must handle multiple streams simultaneously and provide low jitter on each of them separately. Moreover, communication links have limited capacity, and these may pose further restrictions on the choices made by the regulator. This article investigates the problem of minimizing jitter in such an environment, using a fixed-size buffer. We showthat the offline version of the problem can be solved in polynomial time, by introducing an efficient offline algorithm that finds a release schedule with optimal jitter. When regulating M streams in the online setting, we take a competitive analysis point of view and note that, in the upcapacitated case, previous results in Mansour and Patt-Shamir [2001] can be extended to an online algorithm that uses a buffer of size 2·M·B and obtains the optimal jitter possible with a buffer of size B (and an offline algorithm). The question arises whether such a resource augmentation is essential. We answer this question in the affirmative, by proving a lower bound that is tight up to a factor of 2, thus showing that jitter regulation does not scale well as the number of streams increases unless the buffer is sized-up proportionally. © 2009 ACM.",Buffer management; Competitive analysis; Jitter regulation; Online algorithms; Quality of service,Algorithms; Optimization; Polynomial approximation; Quality control; Quality of service; Buffer management; Communication links; Competitive analysis; Delay jitters; End to end delay; Interactive communications; Jitter regulation; Limited capacity; Low jitters; Lower bounds; Multiple streams; Off-line algorithm; Offline; On-line algorithms; On-line setting; Polynomial-time; Resource augmentation; Jitter
Trading off space for passes in graph streaming problems,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049151098&doi=10.1145%2f1644015.1644021&partnerID=40&md5=5761a680f9408dbf25a239bf10d17870,"Data stream processing has recently received increasing attention as a computational paradigm for dealing with massive data sets. Surprisingly, no algorithm with both sublinear space and passes is known for natural graph problems in classical read-only streaming. Motivated by technological factors of modern storage systems, some authors have recently started to investigate the computational power of less restrictive models where writing streams is allowed. In this article, we show that the use of intermediate temporary streams is powerful enough to provide effective space-passes tradeoffs for natural graph problems. In particular, for any space restriction of s bits, we show that single-source shortest paths in directed graphs with small positive integer edge weights can be solved in O((n log3/2 n)/√s) passes. The result can be generalized to deal with multiple sources within the same bounds. This is the first known streaming algorithm for shortest paths in directed graphs. For undirected connectivity, we devise an O((n log n)/s) passes algorithm. Both problems require ω(n/s) passes under the restrictions we consider. We also show that the model where intermediate temporary streams are allowed can be strictly more powerful than classical streaming for some problems, while maintaining all of its hardness for others. © 2009 ACM.",Data streaming; Graph connectivity; Shortest paths,Computer crime; Data reduction; Graph theory; Computational paradigm; Computational power; Data stream processing; Data streaming; Directed graphs; Edge weights; Effective space; Graph connectivity; Graph problems; Massive data sets; Multiple source; Positive integers; Shortest path; Storage systems; Streaming algorithm; Sublinear; Technological factors; Data processing
Approximating connectivity augmentation problems,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049127921&doi=10.1145%2f1644015.1644020&partnerID=40&md5=c3d77182eb0052a5ac328025af98b0ac,"Let G = (V,E) be an undirected graph and let S V. The S-connectivity ΛSG(u,v) of a node pair (u,v) in G is the maximum number of uv-paths that no two of them have an edge or a node in S - {u,v} in common. The corresponding Connectivity Augmentation (CA) problem is: given a graph G = (V,E), a node subset S V, and a nonnegative integer requirement function r(u,v) on V × V, add a minimum size set F of new edges to G so that ΛSG+F(u,v) ≥ r(u,v) for all (u,v) ε V × V. Three extensively studied particular cases are: the Edge-CA (S = Ø), the Node-CA (S = V), and the Element-CA (r(u,v)= 0 whenever u S or v S). A polynomial-time algorithm for Edge-CA was developed by Frank. In this article we consider the Element-CA and the Node-CA, that are NP-hard even for r(u,v) ε {0,2}. The best known ratios for these problems were: 2 for Element-CA and O(rmax · ln n) for Node-CA, where r max = maxu,vεV r(u,v) and n = V. Our main result is a 7/4-approximation algorithm for the Element-CA, improving the previously best known 2-approximation. For Element-CA with r(u,v) ε {0,1,2} we give a 3/2-approximation algorithm. These approximation ratios are based on a new splitting-off theorem, which implies an improved lower bound on the number of edges needed to cover a skew-supermodular set function. For Node-CA we establish the following approximation threshold: Node-CA with r(u,v) ε {0,k} cannot be approximated within O(2log1-εn) for any fixed ε > 0, unless NP ⊆ DTIME(npolylog(n)).© 2009 ACM.",Approximation algorithms; Connectivity augmentation; Element-connectivity; Hardness of approximation; Node-connectivity,Computational complexity; Hardness; Set theory; Approximation ratios; Connectivity augmentation; Connectivity augmentation problems; Graph G; Hardness of approximation; Lower bounds; Node connectivity; Node pairs; Nonnegative integers; NP-hard; Polynomial-time algorithms; Supermodular set function; Undirected graph; Approximation algorithms
Updating relaxed K-d trees,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049123166&doi=10.1145%2f1644015.1644019&partnerID=40&md5=cdb83ad153e8a86afab72c66a73f0eb7,"In this work we present an in-depth study of randomized relaxed K-d trees. It covers two fundamental aspects: the randomized algorithms that allow to preserve the random properties of relaxed K-d trees and the mathematical analysis of the expected performance of these algorithms. In particular, we describe randomized update algorithms for K-d trees based on the split and join algorithms of Duch et al. [1998].We carry out an analysis of the expected cost of all these algorithms, using analytic combinatorics techniques.We show that the average cost of split and join is of the form ζ(K) · n φ (K) +o(nφ (K)), with 1 ≤ φ(K) < 1.561552813, and we give explicit formul' for both ζ(K) and φ(K). These results on the average performance of split and join imply that the expected cost of an insertion or a deletion is Θ(nφ(K)1) when K > 2 and Θ(log n) for K = 2. © 2009 ACM.",Analytic combinatorics; Continuous Master Theorem; K-dimensional trees; Multidimensional data structures; Randomized algorithms; Relaxed K-d trees,Algorithms; Data structures; Analytic combinatorics; K-d tree; K-dimensional tree; Multidimensional data structure; Randomized Algorithms; Cost benefit analysis
Quantum algorithms for Simon's problem over nonabelian groups,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049084945&doi=10.1145%2f1644015.1644034&partnerID=40&md5=0369754bb2055eeee35a724a66141db0,"Daniel Simon's 1994 discovery of an efficient quantum algorithm for finding hidden shifts of ℤ2n provided the first algebraic problem for which quantum computers are exponentially faster than their classical counterparts. In this article, we study the generalization of Simon's problem to arbitrary groups. Fixing a finite group G, this is the problem of recovering an involution →m = (m1,⋯,m n)εGn from an oracle f with the property that f( →x →y) = f(→x) ⇔→y ε{→1, →m}. In the current parlance, this is the hidden subgroup problem (HSP) over groups of the form Gn, where G is a nonabelian group of constant size, and where the hidden subgroup is either trivial or has order two. Although groups of the form Gn have a simple product structure, they share important representation - theoretic properties with the symmetric groups Sn, where a solution to the HSP would yield a quantum algorithm for Graph Isomorphism. In particular, solving their HSP with the so-called standard method requires highly entangled measurements on the tensor product of many coset states. In this article, we provide quantum algorithms with time complexity 2O(√n) that recover hidden involutions →m = (m1,⋯,mn) ε Gn where, as in Simon's problem, each mi is either the identity or the conjugate of a known element m which satisfies κ(m) = -κ(1) for some κ ε Ĝ. Our approach combines the general idea behind Kuperberg's sieve for dihedral groups with the missing harmonic approach of Moore and Russell. These are the first nontrivial HSP algorithms for group families that require highly entangled multiregister Fourier sampling. © 2009 ACM.",Hidden subgroup problem; Quantum algorithms,Algorithms; Combinatorial circuits; Quantum computers; Quantum optics; Quantum theory; Set theory; Tensors; Classical counterpart; Coset state; Dihedral groups; Finite groups; Fourier sampling; Graph isomorphism; Harmonic approach; Hidden subgroup problem; Non-abelian groups; Product structure; Quantum algorithms; Simon's problem; Standard method; Symmetric groups; Tensor products; Time complexity; Group technology
Simultaneous source location,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049084143&doi=10.1145%2f1644015.1644031&partnerID=40&md5=c7009adb5812ee7fc2ee4d571088cce6,"We consider the problem of simultaneous source location: selecting locations for sources in a capacitated graph such that a given set of demands can be satisfied simultaneously, with the goal of minimizing the number of locations chosen. For general directed and undirected graphs we give an O(log D)-approximation algorithm, where D is the sum of demands, and prove matching ω(log D) hardness results assuming P ≠ NP. For undirected trees, we give an exact algorithm and show how this can be combined with a result of Räcke to give a solution that exceeds edge capacities by at most O(log 2 n log log n), where n is the number of nodes. For undirected graphs of bounded treewidth we show that the problem is still NP-hard, but we are able to give a PTAS with at most (1 + ε) violation of the capacities for arbitrarily small ε, or a (k+1) approximation with exact capacities, where k is the treewidth. Categories and Subject Descriptors: F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems'Routing and layout; G.2.1 [Discrete Mathematics]: Combinatorics' Combinatorial algorithms; G.2.2 [Discrete Mathematics]: Graph Theory'Network problems graph algorithms, trees. © 2009 ACM.",Approximation algorithms; Submodular cover,Computational complexity; Graph theory; Location; Mathematical techniques; Analysis of algorithms; Bounded treewidth; Combinatorial algorithm; Descriptors; Discrete mathematics; Edge capacity; Exact algorithms; Graph algorithms; Hardness result; Network problems; Nonnumerical algorithms and problems; NP-hard; Problem complexity; Source location; Submodular; Submodular cover; Tree-width; Undirected graph; Approximation algorithms
Computing rank-convolutions with a mask,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049141957&doi=10.1145%2f1644015.1644035&partnerID=40&md5=7349be959279df44b2e2a0f09d7dd53b,"Rank-convolutions have important applications in a variety of areas such as signal processing and computer vision. We define a mask as a function taking only values zero and infinity. Rank-convolutions with masks are of special interest to image processing. We show how to compute the rank-k convolution of a function over an interval of length n with an arbitrary mask of length m in O(n√mlog m) time. The result generalizes to the d-dimensional case. Previously no algorithm performing significantly better than the brute-force O(nm) bound was known. Our algorithm seems to perform well in practice. We describe an implementation, illustrating its application to a problem in image processing. Already on relatively small images, our experiments show a signficant speedup compared to brute force. © 2009 ACM.",Image processing; Min-convolution; Signal processing,Computer vision; Imaging systems; Signal processing; Brute force; Convolution signals; Min-convolution; Convolution
Time-dependent multi-scheduling of multicast,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049140746&doi=10.1145%2f1644015.1644029&partnerID=40&md5=afd2c8ea48c11c211f069d72eb523416,"Many network applications that need to distribute content and data to a large number of clients use a hybrid scheme in which one (or more) multicast channel is used in parallel to a unicast dissemination. This way the application can distribute data using one of its available multicast channels or by sending one or more unicast transmissions. In such a model the utilization of the multicast channels is critical for the overall performance of the system. We study the scheduling algorithm of the sender in such a model. We describe this scheduling problem as an optimization problem where the objective is to maximize the utilization of the multicast channel. Our model captures the fact that it may be beneficial to multicast an object more than once (e.g., page update). Thus, the benefit depends, among other things, on the last time the object was sent, which makes the problem much more complex than previous related scheduling problems. We show that our problem is NP-hard. Then, using the local ratio technique we obtain a 4-approximation algorithm for the case where the objects are of fixed size and a 10-approximation algorithm for the general case. We also consider a special case which may be of practical interest, and prove that a simple greedy algorithm is a 3-approximation algorithm in this case. © 2009 ACM.",Approximation algorithms; Local ratio; Multicast; Scheduling,Computational complexity; Multicasting; Scheduling algorithms; Fixed size; Greedy algorithms; Hybrid scheme; Local ratio; Local ratio technique; Multicast; Multicast channels; Multicast scheduling; Multicasts; Network applications; NP-hard; Optimization problems; Scheduling problem; Time-dependent; Unicast; Unicast transmissions; Approximation algorithms
Exponential time algorithms for the minimum dominating set problem on some graph classes,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049140341&doi=10.1145%2f1644015.1644024&partnerID=40&md5=f757c07cb643045abfd76b10ff1f04dc,"The minimum dominating set problem remains NP-hard when restricted to any of the following graph classes: c-dense graphs, chordal graphs, 4-chordal graphs, weakly chordal graphs, and circle graphs. Developing and using a general approach, for each of these graph classes we present an exponential time algorithm solving the minimum dominating set problem faster than the best known algorithm for general graphs. Our algorithms have the following running time: O(1.4124n) for chordal graphs, O(1.4776n) for weakly chordal graphs, O(1.4845n) for 4-chordal graphs, O(1.4887 n) for circle graphs, and O(1.2273(1+√1-2c)n) for c-dense graphs. © 2009 ACM.",Dominating set problem; Graph classes; Moderately exponential time algorithms,Computational complexity; Best-known algorithms; Chordal graphs; Dense graphs; Dominating set problems; Exponential time algorithm; General approach; General graph; Graph class; Minimum dominating set; NP-hard; Running time; Weakly chordal graph; Algorithms
Optimizing throughput and energy in online deadline scheduling,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049129322&doi=10.1145%2f1644015.1644025&partnerID=40&md5=a6697db8e9f390295f2ab338c8017d53,"This article extends the study of online algorithms for energy-efficient deadline scheduling to the overloaded setting. Specifically, we consider a processor that can vary its speed between 0 and a maximum speed T to minimize its energy usage (the rate is believed to be a cubic function of the speed). As the speed is upper bounded, the processor may be overloaded with jobs and no scheduling algorithms can guarantee to meet the deadlines of all jobs.Anoptimal schedule is expected to maximize the throughput, and furthermore, its energy usage should be the smallest among all schedules that achieve the maximum throughput. In designing a scheduling algorithm, one has to face the dilemma of selecting more jobs and being conservative in energy usage. If we ignore energy usage, the best possible online algorithm is 4-competitive on throughput [Koren and Shasha 1995]. On the other hand, existing work on energy-efficient scheduling focuses on a setting where the processor speed is unbounded and the concern is on minimizing the energy to complete all jobs; O(1)-competitive online algorithms with respect to energy usage have been known [Yao et al. 1995; Bansal et al. 2007a; Li et al. 2006]. This article presents the first online algorithm for the more realistic setting where processor speed is bounded and the system may be overloaded; the algorithm is O(1)-competitive on both throughput and energy usage. If the maximum speed of the online scheduler is relaxed slightly to (1 + ε)T for some ε > 0, we can improve the competitive ratio on throughput to arbitrarily close to one, while maintaining O(1)-competitiveness on energy usage. © 2009 ACM.",Competitive analysis; Deadline scheduling; Online algorithms; Power saving; Speed scaling,Competition; Speed; Throughput; Wireless telecommunication systems; Competitive analysis; Deadline scheduling; On-line algorithms; Power savings; Speed scaling; Scheduling algorithms
An optimal decomposition algorithm for tree edit distance,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049164767&doi=10.1145%2f1644015.1644017&partnerID=40&md5=dfda928ea0c90379ad9741919eda2400,"The edit distance between two ordered rooted trees with vertex labels is the minimum cost of transforming one tree into the other by a sequence of elementary operations consisting of deleting and relabeling existing nodes, as well as inserting new nodes. In this article, we present a worst-case O(n 3)-time algorithm for the problem when the two trees have size n, improving the previous best O(n3 log n)-time algorithm. Our result requires a novel adaptive strategy for deciding how a dynamic program divides into subproblems, together with a deeper understanding of the previous algorithms for the problem. We prove the optimality of our algorithm among the family of decomposition strategy algorithmswhich also includes the previous fastest algorithmsby tightening the known lower bound of ω(n2 log2 n) to ω(n3), matching our algorithm's running time. Furthermore, we obtain matching upper and lower bounds for decomposition strategy algorithms of Θ(nm2 (1 + log n/m)) when the two trees have sizes m and n and m < n. © 2009 ACM.",Decomposition strategy; Dynamic programming; Edit distance; Ordered trees; Tree edit distance,Dynamic programming; Adaptive strategy; Decomposition strategy; Dynamic programs; Edit distance; Elementary operations; Lower bounds; Minimum cost; Optimal decomposition; Optimality; Ordered tree; Relabeling; Rooted trees; Running time; Sub-problems; Time algorithms; Tree edit distance; Upper and lower bounds; Algorithms
Inverse auctions: Injecting unique minima into random sets,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049144080&doi=10.1145%2f1644015.1644036&partnerID=40&md5=f7f1fd0e5943d276ccd17ff75e7bd54f,"We consider auctions in which the winning bid is the smallest bid that is unique. Only the upper-price limit is given. Neither the number of participants nor the distribution of the offers are known, so that the problem of placing a bid to win with maximum probability looks, a priori, ill-posed. Indeed, the essence of the problem is to inject a (final) minimum into a random subset (of unique offers) of a larger random set. We will see, however, that here no more than two external (and almost compelling) arguments make the problem meaningful. By appropriately modeling the relationship between the number of participants and the distribution of the bids, we can then maximize our chances of winning the auction and propose a computable algorithm for placing our bid. © 2009 ACM.",Asymptotic expansions; Asymptotic independence; Dominant terms; Game theory; Internet auctions; Memoryless property; Poisson approximation; Reverse auctions; Sealed unique-bid auctions; Sequential problems; Unique minimum; Urn model,Asymptotic analysis; Electronic commerce; Internet; Poisson distribution; Poisson equation; Set theory; World Wide Web; Asymptotic expansion; Asymptotic independence; Bid auction; Internet auctions; Poisson approximations; Reverse auction; Sequential problems; Urn models; Game theory
Improved online algorithms for the sorting buffer problem on line metrics,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049103008&doi=10.1145%2f1644015.1644030&partnerID=40&md5=e31f333fc2f426276a237cc374a2f7d2,"An instance of the sorting buffer problem consists of a metric space and a server, equipped with a finite-capacity buffer capable of holding a limited number of requests. An additional ingredient of the input is an online sequence of requests, each of which is characterized by a destination in the given metric space; whenever a request arrives, it must be stored in the sorting buffer. At any point in time, a currently pending request can be served by drawing it out of the buffer and moving the server to its corresponding destination. The objective is to serve all input requests in a way that minimizes the total distance traveled by the server. In this article, we focus our attention on instances of the problem in which the underlying metric is either an evenly-spaced line metric or a continuous line metric. Our main findings can be briefly summarized as follows. (1) We present a deterministic O(log n)-competitive algorithm for n-point evenly-spaced line metrics. This result improves on a randomized O(log2 n)-competitive algorithm due to Khandekar and Pandit [2006b]. It also refutes their conjecture, stating that a deterministic strategy is unlikely to obtain a nontrivial competitive ratio. (2) We devise a deterministic O(log N log log N)-competitive algorithm for continuous line metrics, where N denotes the length of the input sequence. In this context, we introduce a novel discretization technique of independent interest. (3) We establish the first nontrivial lower bound for the evenly-spaced case, by proving that the competitive ratio of any deterministic algorithm is at least 2+ √3/√3 ≈ 2.154. This result settles, to some extent, an open question due to Khandekar and Pandit [2006b], who posed the task of attaining lower bounds on the achievable competitive ratio as a foundational objective for future research. © 2009 ACM.",Disk scheduling; Line metrics; Online algorithms; Sorting buffer,Disks (structural components); Set theory; Topology; Capacity buffer; Competitive algorithms; Competitive ratio; Deterministic algorithms; Discretizations; Disk scheduling; Input sequence; Line metrics; Lower bounds; Metric spaces; On-line algorithms; Algorithms
Minimum cycle bases: Faster and simpler,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049093997&doi=10.1145%2f1644015.1644023&partnerID=40&md5=5c21225ffc0d9002b014f8f63d01310b,"We consider the problem of computing exact or approximate minimum cycle bases of an undirected (or directed) graph G with m edges, n vertices and nonnegative edge weights. In this problem, a {0, 1} ({?1, 0, 1}) incidence vector is associated with each cycle and the vector space over F2 (ℚ) generated by these vectors is the cycle space of G. A set of cycles is called a cycle basis of G if it forms a basis for its cycle space. A cycle basis where the sum of the weights of the cycles is minimum is called a minimum cycle basis of G. Cycle bases of low weight are useful in a number of contexts, for example, the analysis of electrical networks, structural engineering, chemistry, and surface reconstruction. There exists a set of (mn) cycles which is guaranteed to contain a minimum cycle basis. A minimum basis can be extracted by Gaussian elimination. The resulting algorithm [Horton 1987] was the first polynomial-time algorithm. Faster and more complicated algorithms have been found since then. We present a very simple method for extracting a minimum cycle basis from the candidate set with running time O(m2n), which improves the running time for sparse graphs. Furthermore, in the undirected case by using bit-packing we improve the running time also in the case of dense graphs. For undirected graphs we derive an O(m2n/ log n + n 2m) algorithm. For directed graphs we get an O(m3n) deterministic and an O(m2n) randomized algorithm. Our results improve the running times of both exact and approximate algorithms. Finally,we derive a smaller candidate set with size in ω(m) ∩ O(mn). © 2009 ACM.",Cycle space; Minimum cycle basis; Sparse basis,Algorithms; Electric network analysis; Approximate algorithms; Cycle basis; Cycle space; Dense graphs; Directed graphs; Edge weights; Electrical networks; Gaussian elimination; Graph G; Low weight; Minimum cycle basis; Polynomial-time algorithms; Randomized Algorithms; Running time; SIMPLE method; Sparse basis; Sparse graphs; Structural engineering; Undirected graph; Vector spaces
Improved approximate string matching and regular expression matching on Ziv-Lempel compressed texts,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049088174&doi=10.1145%2f1644015.1644018&partnerID=40&md5=cb060621a7b0f8c89707030522b80593,"We study the approximate string matching and regular expression matching problem for the case when the text to be searched is compressed with the Ziv-Lempel adaptive dictionary compression schemes. We present a time-space trade-off that leads to algorithms improving the previously known complexities for both problems. In particular, we significantly improve the space bounds, which in practical applications are likely to be a bottleneck. © 2009 ACM.",Approximate string matching; Compressed pattern matching; Regular expression matching; Ziv-Lempel,Pattern matching; Approximate string matching; Compressed pattern matching; Dictionary compression; Regular-expression matching; Space bounds; Time-space; Data compression
Admission control to minimize rejections and online set cover with repetitions,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049111454&doi=10.1145%2f1644015.1644026&partnerID=40&md5=9c377c707b7664ce72cbb5cec5d37e42,"We study the admission control problem in general networks. Communication requests arrive over time, and the online algorithm accepts or rejects each request while maintaining the capacity limitations of the network. The admission control problem has been usually analyzed as a benefit problem, where the goal is to devise an online algorithm that accepts the maximum number of requests possible. The problem with this objective function is that even algorithms with optimal competitive ratios may reject almost all of the requests, when it would have been possible to reject only a few. This could be inappropriate for settings in which rejections are intended to be rare events. In this article, we consider preemptive online algorithms whose goal is to minimize the number of rejected requests. Each request arrives together with the path it should be routed on. We show an O(log2(mc))-competitive randomized algorithm for the weighted case, wherem is the number of edges in the graph and c is the maximum edge capacity. For the unweighted case, we give an O(logm log c)- competitive randomized algorithm. This settles an open question of Blum et al. [2001]. We note that allowing preemption and handling requests with given paths are essential for avoiding trivial lower bounds. The admission control problem is a generalization of the online set cover with repetitions problem, whose input is a family of m subsets of a ground set of n elements. Elements of the ground set are given to the online algorithm one by one, possibly requesting each element a multiple number of times. (If each element arrives at most once, this corresponds to the online set cover problem.) The algorithm must cover each element by different subsets, according to the number of times it has been requested. We give an O(logm log n)-competitive randomized algorithm for the online set cover with repetitions problem. This matches a recent lower bound of ω(logm log n) given by Korman [2005] (based on Feige [1998]) for the competitive ratio of any randomized polynomial time algorithm, under the BPP ≠NP assumption. Given any constant ε > 0, an O(logm log n)-competitive deterministic bicriteria algorithm is shown that covers each element by at least (1 ?ε)k sets, where k is the number of times the element is covered by the optimal solution. © 2009 ACM.",Admission control; Competitive; Online; Set cover,Polynomial approximation; Rough set theory; Admission Control; Bicriteria algorithm; Capacity limitation; Competitive ratio; Edge capacity; General networks; Lower bounds; Objective functions; On-line algorithms; On-line set; Optimal competitive ratios; Optimal solutions; Polynomial-time algorithms; Randomized Algorithms; Rare event; Algorithms
Latency-constrained aggregation in sensor networks,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049128688&doi=10.1145%2f1644015.1644028&partnerID=40&md5=3b955e6212ae6a26b6467e6de16eb665,"A sensor network consists of sensing devices which may exchange data through wireless communication; sensor networks are highly energy constrained since they are usually battery operated. Data aggregation is a possible way to save energy consumption: nodes may delay data in order to aggregate them into a single packet before forwarding them towards some central node (sink). However, many applications impose constraints on the maximum delay of data; this translates into latency constraints for data arriving at the sink. We study the problem of data aggregation to minimize maximum energy consumption under latency constraints on sensed data delivery, and we assume unique communication paths that form an intree rooted at the sink. We prove that the offline problem is strongly NP-hard and we design a 2-approximation algorithm. The latter uses a novel rounding technique. Almost all real-life sensor networks are managed online by simple distributed algorithms in the nodes. In this context we consider both the case in which sensor nodes are synchronized or not. We assess the performance of the algorithm by competitive analysis. We also provide lower bounds for the models we consider, in some cases showing optimality of the algorithms we propose. Most of our results also hold when minimizing the total energy consumption of all nodes. © 2009 ACM.",Competitive analysis; Data aggregation; Distributed algorithms; Wireless sensor networks,Approximation algorithms; Computational complexity; Data handling; Electric load forecasting; Sensor networks; Sensor nodes; Telecommunication equipment; Wireless telecommunication systems; Communication path; Competitive analysis; Data aggregation; Data delivery; Distributed algorithm; Distributed algorithms; Energy consumption; Energy-constrained; Latency constraints; Lower bounds; Maximum delay; Off-line problems; Optimality; Save energy; Sensing devices; Strongly NP-hard; Total energy consumption; Wireless communications; Wireless sensor networks
The Knuth-Yao quadrangle-inequality speedup is a consequence of total monotonicity,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049126542&doi=10.1145%2f1644015.1644032&partnerID=40&md5=cb4733067c0f6b5a9e586c03c413af8b,"There exist several general techniques in the literature for speeding up naive implementations of dynamic programming. Two of the best known are the Knuth-Yao quadrangle inequality speedup and the SMAWK algorithm for finding the row-minima of totally monotone matrices. Although both of these techniques use a quadrangle inequality and seem similar, they are actually quite different and have been used differently in the literature. In this article we show that the Knuth-Yao technique is actually a direct consequence of total monotonicity. As well as providing new derivations of the Knuth-Yao result, this also permits to solve the Knuth-Yao problem directly using the SMAWK algorithm. Another consequence of this approach is a method for solving online versions of problems with the Knuth-Yao property. The online algorithms given here are asymptotically as fast as the best previously known static ones. For example, the Knuth-Yao technique speeds up the standard dynamic program for finding the optimal binary search tree of n elements from Θ(n3) down to O(n2), and the results in this article allow construction of an optimal binary search tree in an online fashion (adding a node to the left or the right of the current nodes at each step) in O(n) time per step. © 2009 ACM.",Binary search tree; Dynamic programming; Online computation; Totally monotone,Binary search tree; Binary search trees; Dynamic programs; Monotonicity; On-line algorithms; On-line fashion; Online computations; Online versions; Dynamic programming
Resilient dictionaries,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049139041&doi=10.1145%2f1644015.1644016&partnerID=40&md5=b9d7dfb622804e13a5c6d0c1b097c7e5,"We address the problem of designing data structures in the presence of faults that may arbitrarily corrupt memory locations. More precisely, we assume that an adaptive adversary can arbitrarily overwrite the content of up to Δ memory locations, that corrupted locations cannot be detected, and that only O(1) memory locations are safe. In this framework, we call a data structure resilient if it is able to operate correctly (at least) on the set of uncorrupted values. We present a resilient dictionary, implementing search, insert, and delete operations. Our dictionary has O(log n + Δ) expected amortized time per operation, and O(n) space complexity, where n denotes the current number of keys in the dictionary. We also describe a deterministic resilient dictionary, with the same amortized cost per operation over a sequence of at least Δepsi; operations, where ε > 0 is an arbitrary constant. Finally, we show that any resilient comparison-based dictionary must take ω(log n + Δ) expected time per search. Our results are achieved by means of simple, new techniques which might be of independent interest for the design of other resilient algorithms. © 2009 ACM.",Memory faults; Memory models; Search trees; Software-based fault tolerance; Unreliable information,Computer software; Data structures; Fault tolerance; Memory faults; Memory models; Search trees; Software-based; Software-based fault tolerance; Quality assurance
Approximating the minimum quadratic assignment problems,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049143133&doi=10.1145%2f1644015.1644033&partnerID=40&md5=40388e1f2750cb184e2b7dddf4c0365b,"We consider the well-known minimum quadratic assignment problem. In this problem we are given two n × n nonnegative symmetric matrices A = (a ij) and B = (bij). The objective is to compute a permutation π of V = {1,⋯,n} so that ∑ i,jεVi≠j aπ(i),π(j)bi,j is minimized. We assume that A is a 0/1 incidence matrix of a graph, and that B satisfies the triangle inequality. We analyze the approximability of this class of problems by providing polynomial bounded approximations for some special cases, and inapproximability results for other cases. © 2009 ACM.",Approximation algorithms; Quadratic assignment problem,Traveling salesman problem; Approximability; Inapproximability; Incidence matrices; Quadratic assignment problem; Quadratic assignment problems; Symmetric matrices; Triangle inequality; Approximation algorithms
"Polynomial constraint satisfaction problems, graph bisection, and the Ising partition function",2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649088361&doi=10.1145%2f1597036.1597049&partnerID=40&md5=858b7f5229edf6d7c46c16a1f2dc0c1b,"We introduce a problem classwecall Polynomial Constraint Satisfaction Problems, orPCSP. Where the usual CSPs from computer science and optimization have real-valued score functions, and partition functions from physics have monomials, PCSP has scores that are arbitrary multivariate formal polynomials, or indeed take values in an arbitrary ring. Although PCSP is much more general than CSP, remarkably, all (exact, exponential-time) algorithms we know of for 2-CSP (where each score depends on at most 2 variables) extend to 2-PCSP, at the expense of just a polynomial factor in running time. Specifically, we extend the reduction-based algorithm of Scott and Sorkin [2007]; the specialization of that approach to sparse random instances, where the algorithm runs in polynomial expected time; dynamic-programming algorithms based on tree decompositions; and the split-and-list matrix-multiplication algorithm of Williams [2004]. This gives the first polynomial-space exact algorithm more efficient than exhaustive enumeration for the well-studied problems of finding a maximum bisection of a graph, and calculating the partition function of an Ising model. It also yields the most efficient algorithm known for certain instances of counting and/or weighted Maximum Independent Set. Furthermore, PCSP solves both optimization and counting versions of a wide range of problems, including all CSPs, and thus enables samplers including uniform sampling of optimal solutions and Gibbs sampling of all solutions. © 2009 ACM.",Constraint satisfaction; Discrete optimization; Exact algorithm; Exponential-time algorithm; Generating function; Partition function,Computer operating procedures; Exponential functions; Function evaluation; Ising model; Optimization; Polynomials; Set theory; Constraint Satisfaction; Discrete optimization; Exact algorithms; Exponential-time algorithms; Generating functions; Partition functions; Algorithms
Bootstrapping a hop-optimal network in the weak sensor model,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649112437&doi=10.1145%2f1597036.1597040&partnerID=40&md5=1249fe8348d930f063a5f00b8dd2834d,"Sensor nodes are very weak computers that get distributed at random on a surface. Once deployed, they must wake up and form a radio network. Sensor network bootstrapping research thus has three parts: One must model the restrictions on sensor nodes; one must prove that the connectivity graph of the sensors has a subgraph that would make a good network; and one must give a distributed protocol for finding such a network subgraph that can be implemented on sensor nodes. Although many particular restrictions on sensor nodes are implicit or explicit in many papers, there remain many inconsistencies and ambiguities from paper to paper. The lack of a clear model means that solutions to the network bootstrapping problem in both the theory and systems literature all violate constraints on sensor nodes. For example, random geometric graph results on sensor networks predict the existence of subgraphs on the connectivity graph with good route-stretch, but these results do not address the degree of such a graph, and sensor networks must have constant degree. Furthermore, proposed protocols for actually finding such graphs require that nodes have too much memory, whereas others assume the existence of a contention-resolution mechanism. We present a formal Weak Sensor model that summarizes the literature on sensor node restrictions, taking the most restrictive choices when possible. We show that sensor connectivity graphs have low-degree subgraphs with good hop-stretch, as required by the Weak Sensor model. Finally, we give a Weak Sensor model-compatible protocol for finding such graphs. Ours is the first network initialization algorithm that is implementable on sensor nodes. © 2009 ACM.",Ad hoc network; Contention resolution; Maximal independent set; Radio network; Random geometric graphs; Sensor network; Weak sensor model,Ad hoc networks; Intelligent robots; Radio; Sensor networks; Sensor nodes; Telecommunication equipment; Wireless sensor networks; Maximal independent set; Radio network; Radio networks; Random geometric graphs; Sensor model; Network protocols
All maximal independent sets and dynamic dominance for sparse graphs,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649130643&doi=10.1145%2f1597036.1597042&partnerID=40&md5=0900f8e71ba836eb16465695c51b727a,"We describe algorithms, based on Avis and Fukuda's reverse search paradigm, for listing all maximal independent sets in a sparse graph in polynomial time and delay per output. For bounded degree graphs, our algorithms take constant time per set generated; for minor-closed graph families, the time is O(n) per set, and for more general sparse graph families we achieve subquadratic time per set. We also describe new data structures for maintaining a dynamic vertex set S in a sparse or minor-closed graph family, and querying the number of vertices not dominated by S; for minor-closed graph families the time per update is constant, while it is sublinear for any sparse graph family. We can also maintain a dynamic vertex set in an arbitrary m-edge graph and test the independence of the maintained set in time O(√m) per update. We use the domination data structures as part of our enumeration algorithms. © 2009 ACM.",Bounded degree graphs; Combinatorial enumeration; Dominating sets; Dynamic graph algorithms; Maximal independent sets; Minor-closed graph families; Reverse search; Sparse graphs,Algorithms; Data structures; Polynomial approximation; Bounded degree graphs; Combinatorial enumeration; Dominating sets; Dynamic graph algorithms; Maximal independent set; Reverse search; Sparse graphs; Combinatorial mathematics
Low-dimensional lattice basis reduction revisited,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649112778&doi=10.1145%2f1597036.1597050&partnerID=40&md5=42b84f5b8e57c7d4db173d273a513af0,"Lattice reduction is a geometric generalization of the problem of computing greatest common divisors. Most of the interesting algorithmic problems related to lattice reduction are NP-hard as the lattice dimension increases. This article deals with the low-dimensional case. We study a greedy lattice basis reduction algorithm for the Euclidean norm, which is arguably the most natural lattice basis reduction algorithm because it is a straightforward generalization of an old two-dimensional algorithm of Lagrange, usually known as Gauss' algorithm, and which is very similar to Euclid's gcd algorithm. Our results are twofold. From a mathematical point of view, we show that up to dimension four, the output of the greedy algorithm is optimal: The output basis reaches all the successive minima of the lattice. However, as soon as the lattice dimension is strictly higher than four, the output basis may be arbitrarily bad as it may not even reach the first minimum. More importantly, from a computational point of view, we show that up to dimension four, the bit-complexity of the greedy algorithm is quadratic without fast integer arithmetic, just like Euclid's gcd algorithm. This was already proved by Semaev up to dimension three using rather technical means, but it was previously unknown whether or not the algorithm was still polynomial in dimension four. We propose two different analyzes: a global approach based on the geometry of the current basis when the length decrease stalls, and a local approach showing directly that a significant length decrease must occur every O(1) consecutive steps. Our analyzes simplify Semaev's analysis in dimensions two and three, and unify the cases of dimensions two to four. Although the global approach is much simpler, we also present the local approach because it gives further information on the behavior of the algorithm. © 2009 ACM.",Gauss' algorithm; Lattice reduction,Communication channels (information theory); Computational complexity; Algorithmic problems; Dimension four; Euclidean norm; GCD algorithm; Greatest common divisors; Greedy algorithms; Integer arithmetic; Lagrange; Lattice basis reduction; Lattice dimensions; Lattice reduction; Local approaches; NP-hard; Successive minima; Two-dimensional algorithms; Algorithms
Parametric analysis for ungapped Markov models of evolution,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649112779&doi=10.1145%2f1597036.1597048&partnerID=40&md5=cfa99507430c5ca19b31512ab98f74ef,"Efficient sensitivity analysis algorithms are presented for two problems arising in the study of Markov models of sequence evolution: ancestral reconstruction in evolutionary trees and local ungapped alignment under log-odds scoring. The algorithms generate complete descriptions of the optimum solutions for all possible values of the evolutionary distance. The running time for the parametric ancestral reconstruction problem under the Kimura 2-parameter model is O(kn + kn2/3 log k), where n is the number of sequences and k is their length, assuming all edges have the same length. For the parametric gapless alignment problem under the Jukes-Cantor model, the running time is O(mn + mn2/3 log m), where m and n are the sequence lengths and n ≤ m. © 2009 ACM.",Computational biology; Evolutionary tress; Markov models; Parametric analysis; Phylogenetic trees; Sensitivity analysis; Sequence alignmnent,Alignment; Bioinformatics; Biology; Computational efficiency; Markov processes; Computational biology; Evolutionary distance; Evolutionary tree; Markov model; Optimum solution; Parameter model; Parametric analysis; Parametric gapless alignment problem; Phylogenetic trees; Reconstruction problems; Running time; Sequence lengths; Sensitivity analysis
A better approximation ratio for the vertex cover problem,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649088030&doi=10.1145%2f1597036.1597045&partnerID=40&md5=7a988060ace70c4c52a0ab4464196fd8,"We reduce the approximation factor for the vertex cover to 2 - Θ( 1/√log n ) (instead of the previous 2 - Θin in n/2in n obtained by Bar-Yehuda and Even [1985] and Monien and Speckenmeyer [1985]). The improvement of the vanishing factor comes as an application of the recent results of Arora et al. [2004] that improved the approximation factor of the sparsest cut and balanced cut problems. In particular, we use the existence of two big and well-separated sets of nodes in the solution of the semidefinite relaxation for balanced cut, proven by Arora et al. [2004]. We observe that a solution of the semidefinite relaxation for vertex cover, when strengthened with the triangle inequalities, can be transformed into a solution of a balanced cut problem, and therefore the existence of big well-separated sets in the sense of Arora et al. [2004] translates into the existence of a big independent set. © 2009 ACM.",Approximation algorithm; Semidefinite programming; Vertex cover,Mathematical programming; Approximation factor; Approximation ratios; Independent set; Semi-definite programming; Semidefinite relaxation; Separated sets; Sparsest cut; Triangle inequality; Vertex cover; Vertex Cover problems; Approximation algorithms
Randomized fast design of short DNA words,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649131608&doi=10.1145%2f1597036.1597047&partnerID=40&md5=2473871fbe86ae15d3e5753f8b003db2,"We consider the problem of efficiently designing sets (codes) of equal-length DNA strings (words) that satisfy certain combinatorial constraints. This problem has numerous motivations including DNA self-assembly and DNA computing. Previous work has extended results from coding theory to obtain bounds on code size for new biologically motivated constraints and has applied heuristic local search and genetic algorithm techniques for code design. This article proposes a natural optimization formulation of the DNA code design problem in which the goal is to design n strings that satisfy a given set of constraints while minimizing the length of the strings. For multiple sets of constraints, we provide simple randomized algorithms that run in time polynomial in n and any given constraint parameters, and output strings of length within a constant factor of the optimal with high probability. To the best of our knowledge, this work is the first to consider this type of optimization problem in the context of DNA code design. © 2009 ACM.",DNA code design; Randomized algorithms,Algorithms; Design; Genes; Genetic programming; Information theory; Optimization; Biologically-motivated constraints; Code designs; Code size; Coding Theory; Constant factors; DNA codes; DNA self-assembly; DNA strings; DNA-computing; Heuristic local search; High probability; Multiple set; Optimization formulations; Optimization problems; Randomized Algorithms; Time polynomials; DNA
A linear-time algorithm to find a separator in a graph excluding a minor,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649108258&doi=10.1145%2f1597036.1597043&partnerID=40&md5=0a1a802f1d829bbd2d060227a2654057,"Let G be an n-vertex m-edge graph with weighted vertices. A pair of vertex sets A, B ⊆ V(G) is a 2/3 -separation of order |A ∩ B| if A ∪ B = V(G), there is no edge between A - B and B - A, and both A - B and B - A have weight at most 2/3 the total weight of G. Let ℓ ∈ ℤ Z+ be fixed. Alon et al. [1990] presented an algorithm that in O(n1/2m) time, outputs either a Kℓ-minor of G, or a separation of G of order O(n1/2). Whether there is a O(n + m)-time algorithm for this theorem was left as an open problem. In this article, we obtain a O(n + m)-time algorithm at the expense of a O(n2/3) separator. Moreover, our algorithm exhibits a trade-off between time complexity and the order of the separator. In particular, for any given ε ∈ [0, 1/2 ], our algorithm outputs either a Kℓ-minor of G, or a separation of G with order O(n (2-ε)/3) in O(n1+ε+m) time. As an application we give a fast approximation algorithm for finding an independent set in a graph with no Kℓ-mininor. © 2009 ACM.",Graph; Minor; Separation; Separator,Approximation algorithms; Clustering algorithms; Separators; Fast approximation; Graph; Graph minors; Independent set; Linear-time algorithms; Minor; Open problems; Time algorithms; Time complexity; Vertex set; Separation
A linear algorithm for computing convex hulls for random lines,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649124295&doi=10.1145%2f1597036.1597046&partnerID=40&md5=4074460d5ea5114783ee9d7fcf98cbf7,"Finding the convex hull of n points in the plane requires O(n log n) time in general. In Devroye and Toussaint [1993] and Golin et al. [2002] the problem of computing the convex hull of the intersection points of n lineswas considered, where the lines are chosen randomly according to two various models. In both models, linear-time algorithms were developed. Here we improve the results of Devroye and Toussaint [1993] by giving a universal algorithm for a wider range of distributions. © 2009 ACM.",Computational complexity; Computational geometry; Convex hull; Random lines; Randomized algorithms,Clustering algorithms; Computational complexity; Convex hull; Intersection points; Linear algorithms; Linear-time algorithms; Random lines; Randomized Algorithms; Universal algorithm; Computational geometry
Enumeration of isolated cliques and pseudo-cliques,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649097686&doi=10.1145%2f1597036.1597044&partnerID=40&md5=d2859761936ace794953059c6d825ff7,"In this article, we consider isolated cliques and isolated dense subgraphs. For a given graph G, a vertex subset S of size k (and also its induced subgraph G(S)) is said to be c-isolated if G(S) is connected to its outside via less than ck edges. The number c is sometimes called the isolation factor. The subgraph appears more isolated if the isolation factor is smaller. The main result in this work shows that for a fixed constant c, we can enumerate all c-isolated maximal cliques (including a maximum one, if any) in linear time. In more detail, we show that, for a given graph G of n vertices and m edges, and a positive real number c, all c-isolated maximal cliques can be enumerated in time O( c4 22cm). From this, we can see that: (1) if c is a constant, all c-isolated maximal cliques can be enumerated in linear time, and (2) if c O(log n), all c-isolated maximal cliques can be enumerated in polynomial time. Moreover, we show that these bounds are tight. That is, if f(n) is an increasing function not bounded by any constant, then there is a graph of n vertices and m edges for which the number of f(n)-isolated maximal cliques is superlinear in n + m. Furthermore, if f(n) = ω (log n), there is a graph of n vertices and m edges for which the number of f(n)-isolated maximal cliques is superpolynomial in n + m. We next introduce the idea of pseudo-cliques. A pseudo-clique having an average degree α and a minimum degree Β, denoted by PC(α,Β), is a set V′ ⊆ V such that the subgraph induced by V′ has an average degree of at least α and a minimum degree of at least Β. This article investigates these, and obtains some cases that can be solved in polynomial time and some other cases that have a superpolynomial number of solutions. Especially, we show the following results, where k is the number of vertices of the isolated pseudo-cliques: (1) For any ε > 0 there is a graph of n vertices for which the number of 1-isolated PC(k - (log k)1 + ε, k/(log k)1 + ε),k/(logk) 1+ε is superpolynomial, and (2) there is a polynomial-time algorithm which enumerates all c-isolated PC(k - log k, k/logk), for any constant c. © 2009 ACM.",Clique; Enumeration; Isolation,Number theory; Average degree; Graph G; Increasing functions; Induced subgraphs; Isolated cliques; Linear time; Maximal clique; Minimum degree; Polynomial-time; Polynomial-time algorithms; Positive real; Subgraphs; Superlinear; Superpolynomial numbers; Polynomial approximation
Sublinear estimation of entropy and information distances,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649097045&doi=10.1145%2f1597036.1597038&partnerID=40&md5=14cc00bea7b0f4e9d7a9905782e863df,"In many data mining and machine learning problems, the data items that need to be clustered or classified are not arbitrary points in a high-dimensional space, but are distributions, that is, points on a high-dimensional simplex. For distributions, natural measures are not ℓp distances, but information-theoretic measures such as the Kullback-Leibler and Hellinger divergences. Similarly, quantities such as the entropy of a distribution are more natural than frequency moments. Efficient estimation of these quantities is a key component in algorithms for manipulating distributions. Since the datasets involved are typically massive, these algorithms need to have only sublinear complexity in order to be feasible in practice. We present a range of sublinear-time algorithms in various oracle models in which the algorithm accesses the data via an oracle that supports various queries. In particular, we answer a question posed by Batu et al. on testing whether two distributions are close in an information-theoretic sense given independent samples. We then present optimal algorithms for estimating various information-divergences and entropy with a more powerful oracle called the combined oracle that was also considered by Batu et al. Finally, we consider sublinear-space algorithms for these quantities in the data-stream model. In the course of doing so, we explore the relationship between the aforementioned oracle models and the data-stream model. This continues work initiated by Feigenbaum et al. An important additional component to the study is considering data streams that are ordered randomly rather than just those which are ordered adversarially. © 2009 ACM.",Data streams; Entropy; Information divergences; Property testing,Algorithms; Data communication systems; Data mining; Entropy; Estimation; Hydraulics; Information theory; Arbitrary points; Close-in; Data items; Data sets; Data stream; Data streams; Efficient estimation; Hellinger divergence; High dimensional spaces; High-dimensional; Information distance; Information divergence; Key component; Kullback-Leibler; Machine learning problem; Natural measure; Optimal algorithm; Oracle model; Property-testing; Space algorithms; Stream models; Sublinear; Time algorithms; Mathematical models
Approximation algorithms for data placement on parallel disks,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649119140&doi=10.1145%2f1597036.1597037&partnerID=40&md5=dcee4611c173c6f345046704786990a1,"We study an optimization problem that arises in the context of data placement in a multimedia storage system.We are given a collection of M multimedia objects (data objects) that need to be assigned to a storage system consisting of N disks d 1, d 2..., d N .We are also given sets U 1,U 2,...,U M such thatUi is the set of clients seeking the ith data object. Each disk dj is characterized by two parameters, namely, its storage capacity C J which indicates the maximum number of data objects that may be assigned to it, and a load capacity L J which indicates the maximum number of clients that it can serve. The goal is to find a placement of data objects to disks and an assignment of clients to disks so as to maximize the total number of clients served, subject to the capacity constraints of the storage system. We study this data placement problem for two natural classes of storage systems, namely, homogeneous and uniform ratio. We show that an algorithm developed by Shachnai and Tamir [2000a] for A preliminary version of this article was presented at the 2000 ACM-SIAM Symposium on Discrete Algorithms. © 2009 ACM.",Approximation algorithms; Data placement; Storage systems,Disks (machine components); Disks (structural components); Interconnection networks; Capacity constraints; Data objects; Data placement; Discrete algorithms; Load capacity; Multimedia object; Multimedia storage; Number of datum; Optimization problems; Parallel disks; Storage capacity; Storage systems; Two parameter; Approximation algorithms
A generalized minimum cost k-clustering,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649121569&doi=10.1145%2f1597036.1597039&partnerID=40&md5=d567d8be7a82aab1e2341665fd7402cd,"We consider the problems of set partitioning into k clusters with minimum total cost and minimum of the maximum cost of a cluster. The cost function is given by an oracle, and we assume that it satisfies some natural structural constraints. That is, we assume that the cost function is monotone, the cost of a singleton is zero, and we assume that for all S ∩ S′ ≠ ø the following holds c(S) + c(S′) c(S S′) ≥. For the problem of minimizing the maximum cost of a cluster we present a (2k - 1)-approximation algorithm for k ≥ 3, a 2-approximation algorithm for k = 2, and we also show a lower bound of k on the performance guarantee of any polynomial-time algorithm. For the problem of minimizing the total cost of all the clusters, we present a 2-approximation algorithm for the case where k is a fixed constant, a (4k - 3)-approximation where k is unbounded, and we show a lower bound of 2 on the approximation ratio of any polynomial-time algorithm. Our lower bounds do not depend on the common assumption that P ≠ NP. © 2009 ACM.",Approximation algorithms; Clustering; Oracle cost functions,Clustering algorithms; Cost functions; Costs; Polynomial approximation; Approximation ratios; Clustering; K cluster; Lower bounds; Minimum cost; Performance guarantees; Polynomial-time algorithms; Set partitioning; Structural constraints; Total costs; Approximation algorithms
Squarepants in a tree: Sum of subtree clustering and hyperbolic pants decomposition,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68649088644&doi=10.1145%2f1541885.1541890&partnerID=40&md5=3ed95e1d6aae501951e0fa455884ef7b,"We provide efficient constant-factor approximation algorithms for the problems of finding a hierarchical clustering of a point set in any metric space, minimizing the sum of minimimum spanning tree lengths within each cluster, and in the hyperbolic or Euclidean planes, minimizing the sum of cluster perimeters. Our algorithms for the hyperbolic and Euclidean planes can also be used to provide a pants decomposition, that is, a set of disjoint simple closed curves partitioning the plane minus the input points into subsets with exactly three boundary components, with approximately minimum total length. In the Euclidean case, these curves are squares; in the hyperbolic case, they combine our Euclidean square pants decomposition with our tree clustering method for general metric spaces. © 2009 ACM.",Approximation algorithm; Hierarchical clustering; Hyperbolic geometry; Minimum spanning tree; Pants decomposition; Sum of cluster sizes,Approximation algorithms; Graph theory; Query processing; Set theory; Topology; Hierarchical clustering; Hyperbolic geometry; Minimum spanning tree; Pants decomposition; Sum of cluster sizes; Clustering algorithms
Near-optimal algorithms for maximum constraint satisfaction problems,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549140043&doi=10.1145%2f1541885.1541893&partnerID=40&md5=ef2fa35d36c18613843032f7137e4bc9,"In this article, we present two approximation algorithms for the maximum constraint satisfaction problem with k variables in each constraint (MAX k-CSP). Given a (1 - ε) satisfiable 2CSP our first algorithm finds an assignment of variables satisfying a 1 - O(ε) fraction of all constraints. The best previously known result, due to Zwick, was 1 - O(ε1/3). The second algorithm finds a ck/2k approximation for the MAX k-CSP problem (where c > 0.44 is an absolute constant). This result improves the previously best known algorithm by Hast, which had an approximation guarantee of (k/(2k log k)). Both results are optimal assuming the unique games conjecture and are based on rounding natural semidefinite programming relaxations. We also believe that our algorithms and their analysis are simpler than those previously known. © 2009 ACM.",MAX 2CSP; MAX k-CSP; SDP,Boolean functions; Computer operating procedures; Steelmaking; Best-known algorithms; MAX 2CSP; MAX k-CSP; Maximum constraint satisfaction problems; Near-optimal algorithms; SDP; Semidefinite programming relaxations; Unique games conjecture; Approximation algorithms
An O(n log n) approximation scheme for Steiner tree in planar graphs,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549136621&doi=10.1145%2f1541885.1541892&partnerID=40&md5=fbb6afe1f475556c3feceec495b0dcbb,We give a Polynomial-Time Approximation Scheme (PTAS) for the Steiner tree problem in planar graphs. The running time is O(n log n). © 2009 ACM.,Approximation scheme; Planar graphs; Steiner tree,Polynomial approximation; Approximation scheme; Planar graph; Planar graphs; Polynomial time approximation schemes; Running time; Steiner tree; Steiner tree problem; Steiner trees; Graph theory
Instability of FIFO in the permanent sessions model at arbitrarily small network loads,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549132412&doi=10.1145%2f1541885.1541894&partnerID=40&md5=32f62729e52a086d02b83ce9f9d5fcaa,"We show that for any r > 0, there is a network of First-In-First-Out servers and a fixed set of sessions such that: The network load is r with respect to the permanent sessions model with bounded arrivals. The network can be made unstable. © 2009 ACM.",FIFO scheduling; Network stability,FIFO scheduling; First-in-first-out; Network load; Network stability; Small networks; Scheduling
ACM Transactions on Algorithms: Foreword to special issue on SODA 2007,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549088851&doi=10.1145%2f1541885.1541886&partnerID=40&md5=eae12851721fdcaffaddf61292f95302,[No abstract available],,
Optimal dynamic vertical ray shooting in rectilinear planar subdivisions,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549085218&doi=10.1145%2f1541885.1541889&partnerID=40&md5=8b91e9d465102ee84024a1dc23483480,"We consider the dynamic vertical ray shooting problem against horizontal disjoint segments, that is, the task of maintaining a dynamic set S of n nonintersecting horizontal line segments in the plane under a query that reports the first segment in S intersecting a vertical ray from a query point. We develop a linear-size structure that supports queries, insertions, and deletion in O(log n) worst-case time. Our structure works in the comparison model on a random access machine. © 2009 ACM.",Planar point location; Ray shooting,Comparison models; Disjoint segments; Line segment; Optimal dynamics; Planar point location; Planar subdivision; Query points; Random access machines; Ray shooting; Size structure
Making deterministic signatures quickly,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549104173&doi=10.1145%2f1541885.1541887&partnerID=40&md5=be9c6408d427035b36d09c3e8d63ccae,"We present a new technique of universe reduction. Primary applications are the dictionary problem and the predecessor problem. We give several new results on static dictionaries in different computational models: the word RAM, the practical RAM, and the cache-oblivious model. All algorithms and data structures are deterministic and use linear space. Representative results are: a dictionary with a lookup time of O(log log n) and construction time of O(n) on sorted input on a word RAM, and a static predecessor structure for variable- and unbounded length binary strings that in the cache-oblivious model has a query performance of O(s/B + log s) I/Os, for query argument s. © 2009 ACM.",Deterministic algorithms; Perfect hashing,Cache memory; Data structures; Algorithms and data structures; Binary string; Cache-oblivious model; Computational model; Construction time; Deterministic algorithms; Linear spaces; Lookup time; New results; Perfect hashing; Predecessor problems; Query performance; Random access storage
Compacting cuts: A new linear formulation for minimum cut,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549083527&doi=10.1145%2f1541885.1541888&partnerID=40&md5=3b8f575f83f4e3866ee0a4c16a949c38,"For a graph (V,E), existing compact linear formulations for the minimum cut problem require Θ(VE) variables and constraints and can be interpreted as a composition of V - 1 polyhedra for minimum s-t cuts in much the same way as early approaches to finding globally minimum cuts relied on V - 1 calls to a minimum s-t cut algorithm. We present the first formulation to beat this bound, one that uses O(V2) variables and O(V3) constraints. An immediate consequence of our result is a compact linear relaxation with O(V 2) constraints and O(V3) variables for enforcing global connectivity constraints. This relaxation is as strong as standard cut-based relaxations and has applications in solving traveling salesman problems by integer programming as well as finding approximate solutions for survivable network design problems using Jain's iterative rounding method. Another application is a polynomial-time verifiable certificate of size n for for the NP-complete problem of l1-embeddability of a rational metric on an n-set (as opposed to a certificate of size n2 known previously). © 2009 ACM.",Linear programming formulation complexity; Minimum cut problem,Dynamic programming; Integer programming; Linearization; Traveling salesman problem; Approximate solution; Global connectivity; Iterative rounding; Linear formulation; Linear programming formulation complexity; Linear relaxations; Minimum cut; Minimum cut problem; Minimum s-t cuts; NP complete problems; Polynomial-time; Survivable network design; Computer systems programming
Minimizing movement,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549111016&doi=10.1145%2f1541885.1541891&partnerID=40&md5=4c6083e98eb6d04608c1d302b0b15df4,"We give approximation algorithms and inapproximability results for a class of movement problems. In general, these problems involve planning the coordinated motion of a large collection of objects (representing anything from a robot swarm or firefighter team to map labels or network messages) to achieve a global property of the network while minimizing the maximum or average movement. In particular, we consider the goals of achieving connectivity (undirected and directed), achieving connectivity between a given pair of vertices, achieving independence (a dispersion problem), and achieving a perfect matching (with applications to multicasting). This general family of movement problems encompasses an intriguing range of graph and geometric algorithms, with several real-world applications and a surprising range of approximability. In some cases, we obtain tight approximation and inapproximability results using direct techniques (without use of PCP), assuming just that P= NP. © 2009 ACM.",Euclidean plane; Graphs; Motion planning; Pebble placement,Approximation algorithms; Labels; Motion planning; Multicasting; Navigation; Approximability; Coordinated motion; Euclidean plane; Geometric algorithm; Global properties; Graphs; Inapproximability; Minimizing movements; OR-networks; Pebble placement; Perfect matchings; Real-world application; Robot swarms; Robot programming
Linear time 3-approximation for the MAST problem,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149121801&doi=10.1145%2f1497290.1497299&partnerID=40&md5=27cea1718b5255e89a1f8d9014ae53fa,"Given a set of leaf-labeled trees with identical leaf sets, the well-known Maximum Agreement SubTree (MAST) problem consists in finding a subtree homeomorphically included in all input trees and with the largest number of leaves. MAST and its variant called Maximum Compatible Tree (MCT) are of particular interest in computational biology. This article presents a linear-time approximation algorithm to solve the complement version of MAST, namely identifying the smallest set of leaves to remove from input trees to obtain isomorphic trees. We also present an O(n 2 + kn) algorithm to solve the complement version of MCT. For both problems, we thus achieve significantly lower running times than previously known algorithms. Fast running times are especially important in phylogenetics where large collections of trees are routinely produced by resampling procedures, such as the nonparametric bootstrap or Bayesian MCMC methods. © 2009 ACM.",Approximation algorithm; Maximum agreement subtree; Maximum compatible subtree; Phylogenetic tree,Bioinformatics; Industrial laboratories; Set theory; Bayesian; Computational biology; Leaf-labeled tree; Linear time; Linear-time approximation; Maximum agreement subtree; Maximum compatible subtree; MCMC method; Non-parametric; Phylogenetic tree; Phylogenetics; Resampling; Running time; Subtree; Approximation algorithms
Average-case analysis of some plurality algorithms,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149133083&doi=10.1145%2f1497290.1497293&partnerID=40&md5=6ff6c3ffef20795444cc0d3f24c7d234,"Given a set of n elements, each of which is colored one of c colors, we must determine an element of the plurality (most frequently occurring) color by pairwise equal/unequal color comparisons of elements. We focus on the expected number of color comparisons when the c n colorings are equally probable. We analyze an obvious algorithm, showing that its expected performance is c 2 + c - 2/2c n - O(c 2), with variance Θ(c 2n). We present and analyze an algorithm for the case c = 3 colors whose average complexity on the 3 n equally probable inputs is 7083/5425n + O(√n) = 1.3056n + O(√ n), substantially better than the expected complexity 5/3n + O(1) = 1.6666n + O(1) of the obvious algorithm. We describe a similar algorithm for c =4 colors whose average complexity on the 4 n equally probable inputs is 761311/402850n + O(log n) = 1.8898n + O(log n), substantially better than the expected complexity 9/4n + O(1) = 2.25n + O(1) of the obvious algorithm. © 2009 ACM.",Algorithm analysis; Majority problem; Plurality problem,Color; Algorithm analysis; Average complexity; Average-case analysis; Expected complexity; Majority problem; Plurality problem; Algorithms
Throughput maximization of real-time scheduling with batching,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149099128&doi=10.1145%2f1497290.1497294&partnerID=40&md5=275ec579d06342fbdb92afb1fc9450b1,"We consider the following scheduling with batching problem that has many applications, for example, in multimedia-on-demand and manufacturing of integrated circuits. The input to the problem consists of n jobs and k parallel machines. Each job is associated with a set of time intervals in which it can be scheduled (given either explicitly or nonexplicitly), a weight, and a family. Each family is associated with a processing time. Jobs that belong to the same family can be batched and executed together on the same machine. The processing time of each batch is the processing time of the family of jobs it contains. The goal is to find a nonpreemptive schedule with batching that maximizes the weight of the scheduled jobs. We give constant factor (4 or 4 + ε) approximation algorithms for two variants of the problem, depending on the precise representation of the input. When the batch size is unbounded and each job is associated with a time window in which it can be processed, these approximation ratios reduce to 2 and 2 + ε, respectively. We also give approximation algorithms for two special cases when all release times are the same. © 2009 ACM.",Batching; Local ratio technique; Scheduling,Integrated circuits; Approximation ratios; Batch sizes; Batching; Batching problem; Constant factors; Local ratio technique; Multimedia on demand; Nonpreemptive schedules; Parallel machine; Processing Time; Real time scheduling; Release time; Throughput maximization; Time interval; Time windows; Approximation algorithms
Testing bipartiteness of geometric intersection graphs,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149146183&doi=10.1145%2f1497290.1497291&partnerID=40&md5=dbe622f44308d2570d8a1e2c9bf06bfb,"We show how to test the bipartiteness of an intersection graph of n line segments or simple polygons in the plane, or of an intersection graph of balls in d-dimensional Euclidean space, in time O(n log n). More generally, we find subquadratic algorithms for connectivity and bipartiteness testing of intersection graphs of a broad class of geometric objects. Our algorithms for these problems return either a bipartition of the input or an odd cycle in its intersection graph. We also consider lower bounds for connectivity and k-colorability problems of geometric intersection graphs. For unit balls in d dimensions, connectivity testing has equivalent randomized complexity to construction of Euclidean minimum spanning trees, and for line segments in the plane connectivity testing has the same lower bounds as Hopcroft's point-line incidence testing problem; therefore, for these problems, connectivity is unlikely to be solved as efficiently as bipartiteness. For line segments or planar disks, testing k-colorability of intersection graphs for k > 2 is NP-complete. © 2009 ACM.",Bipartite graph; Coin graph; Disks; Geometric thickness; Graph coloring; Hopcroft's problem; Intersection graph; Line segments; Minimum spanning tree,Algorithms; Coloring; Control theory; Disks (machine components); Disks (structural components); Intersections; Bipartite graph; Coin graph; Disks; Geometric thickness; Graph coloring; Hopcroft's problem; Intersection graph; Line segments; Minimum spanning tree; Graph theory
Bicriteria approximation tradeoff for the node-cost budget problem,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149144175&doi=10.1145%2f1497290.1497295&partnerID=40&md5=f47f1e4c980845417b599496418ad6a3,"We consider an optimization problem consisting of an undirected graph, with cost and profit functions defined on all vertices. The goal is to find a connected subset of vertices with maximum total profit, whose total cost does not exceed a given budget. The best result known prior to this work guaranteed a (2, O(log n)) bicriteria approximation that is, the solution's profit is at least a fraction of 1 O(log n) of an optimum solution respecting the budget, while its cost is at most twice the given budget. We improve these results and present a bicriteria tradeoff that, given any e ? (0, 1], guarantees a (1 + e, O( 1 e log n))-approximation. © 2009 ACM.",Approximation algorithms; Bicriteria approximation,Budget control; Communication channels (information theory); Costs; Bi-criteria; Bicriteria approximation; Budget problems; Connected subset; Optimization problems; Optimum solution; Profit function; Total costs; Undirected graph; Approximation algorithms
Approximating the distance to properties in bounded-degree and general sparse graphs,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149116491&doi=10.1145%2f1497290.1497298&partnerID=40&md5=33de265bd5e524c1d92b98906f1c3d5e,"We address the problem of approximating the distance of bounded-degree and general sparse graphs from having some predetermined graph property P. That is, we are interested in sublinear algorithms for estimating the fraction of edge modifications (additions or deletions) that must be performed on a graph so that it obtains P. This fraction is taken with respect to a given upper bound m on the number of edges. In particular, for graphs with degree bound d over n vertices, m = dn. To perform such an approximation the algorithm may ask for the degree of any vertex of its choice, and may ask for the neighbors of any vertex. The problem of estimating the distance to having a propertywas first explicitly addressed by Parnas et al. [2006]. In the context of graphs this problem was studied by Fischer and Newman [2007] in the dense graphs model. In this model the fraction of edge modifications is taken with respect to n2, and the algorithm may ask for the existence of an edge between any pair of vertices of its choice. Fischer and Newman showed that every graph property that has a testing algorithm in this model, with query complexity independent of the size of the graph, also has a distance approximation algorithm with query complexity that is independent of the size of graph. In this work we focus on bounded-degree and general sparse graphs, and give algorithms for all properties shown to have efficient testing algorithms by Goldreich and Ron [2002]. Specifically, these properties are k-edge connectivity, subgraph freeness (for constant-size subgraphs), being an Eulerian graph, and cycle freeness. A variant of our subgraph-freeness algorithm approximates the size of a minimum vertex cover of a graph in sublinear time. This approximation improves on a recent result of Parnas and Ron [2007]. © 2009 ACM.",Distance approximation; Graph properties; Property testing; Sublinear approximation algorithms,Approximation algorithms; Distance measurement; Degree bounds; Dense graphs; Distance approximation; Edge connectivity; Edge modification; Eulerian graphs; Graph properties; Minimum vertex cover; Property testing; Query complexity; Sparse graphs; Subgraph; Subgraphs; Sublinear algorithm; Sublinear approximation algorithms; Sublinear time; Testing algorithm; Upper Bound; Graph theory
Algorithms for distributional and adversarial pipelined filter ordering problems,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149136164&doi=10.1145%2f1497290.1497300&partnerID=40&md5=d06fb23800156a2632b46104f3bac102,"Pipelined filter ordering is a central problem in database query optimization. The problem is to determine the optimal order in which to apply a given set of commutative filters (predicates) to a set of elements (the tuples of a relation), so as to find, as efficiently as possible, the tuples that satisfy all of the filters. Optimization of pipelined filter ordering has recently received renewed attention in the context of environments such as the Web, continuous high-speed data streams, and sensor networks. Pipelined filter ordering problems are also studied in areas such as fault detection and machine learning under names such as learning with attribute costs, minimum-sum set cover, and satisficing search. We present algorithms for two natural extensions of the classical pipelined filter ordering problem: (1) a distributional-type problem where the filters run in parallel and the goal is to maximize throughput, and (2) an adversarial-type problem where the goal is to minimize the expected value of multiplicative regret. We present two related algorithms for solving (1), both running in time O(n 2), which improve on the O(n3 log n) algorithm of Kodialam. We use techniques from our algorithms for (1) to obtain an algorithm for (2). © 2009 ACM.",Flow algorithms; Pipelined filter ordering; Query optimization; Selection ordering,Education; Fault detection; Optimization; Parallel algorithms; Sensor networks; Central problems; Database queries; Expected values; Flow algorithms; High-speed data; Machine-learning; Maximize throughput; Natural extension; Pipelined filter ordering; Query optimization; Running-in; Satisficing search; Selection ordering; Set cover; Set theory
A 1.8 approximation algorithm for augmenting edge-connectivity of a graph from 1 to 2,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149143139&doi=10.1145%2f1497290.1497297&partnerID=40&md5=8fff3de4607bf8be6eab85fd1ad198d1,"We present a 1.8-approximation algorithm for the following NP-hard problem: Given a connected graph G = (V, E) and an edge set E on V disjoint to E, find a minimum-size subset of edges F ⊆ E such that (V, E ∪ F) is 2-edge-connected. Our result improves and significantly simplifies the approximation algorithm with ratio 1.875 + ε of Nagamochi. © 2009 ACM.",Approximation algorithms; Connectivity; Graphs,Computational complexity; Graph theory; Connected graph; Connectivity; Edge connectivity; Edge-sets; Graphs; NP-HARD problem; Approximation algorithms
"Online conflict-free coloring for halfplanes, congruent disks, and axis-parallel rectangles",2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149090592&doi=10.1145%2f1497290.1497292&partnerID=40&md5=db1e6da6b5932e497138b77a829f4055,"We present randomized algorithms for online conflict-free coloring (CF in short) of points in the plane, with respect to halfplanes, congruent disks, and nearly-equal axis-parallel rectangles. In all three cases, the coloring algorithms use O(log n) colors, with high probability. We also present a deterministic algorithm for online CF coloring of points in the plane with respect to nearly-equal axis-parallel rectangles, using O(log 3n) colors. This is the first efficient (i.e, using polylog(n) colors) deterministic online CF coloring algorithm for this problem. © 2009 ACM.",Coloring; Conflict free coloring; Online algorithms,Color; Disks (machine components); Disks (structural components); Geometry; Parallel algorithms; Axis parallel rectangles; Coloring algorithms; Conflict free coloring; Deterministic algorithms; Half-planes; High probability; Online algorithms; Online conflict-free coloring; Randomized Algorithms; Coloring
A polynomial-time approximation scheme for embedding hypergraph in a cycle,2009,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149129936&doi=10.1145%2f1497290.1497296&partnerID=40&md5=a6f65975e27a6869d310a3e2a6c6b303,"We consider the problem of embedding hyperedges of a hypergraph as paths in a cycle such that the maximum congestion, namely the maximum number of paths that use any single edge in a cycle, is minimized. The minimum congestion hypergraph embedding in a cycle problem is known to be NP-hard and its graph version, the minimum congestion graph embedding in a cycle, is solvable in polynomial-time. Furthermore, for the graph problem, a polynomial-time approximation scheme for the weighted version is known. For the hypergraph model, several approximation algorithms with a ratio of two have been previously published. A recent paper reduced the approximation ratio to 1.5. We present a polynomial-time approximation scheme in this article, settling the debate regarding whether the problem is polynomial-time approximable. © 2009 ACM.",Hypergraph embedding; Minimum congestion; NP-hard; Polynomial-time approximation scheme,Computational complexity; Polynomial approximation; Approximation ratios; Graph embedding; Graph problems; Hyperedges; Hypergraph; Hypergraph embedding; Hypergraph model; Minimum congestion; NP-hard; Polynomial time approximation schemes; Polynomial-time; Polynomial-time approximation scheme; Approximation algorithms
On hard instances of approximate vertex cover,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849119303&doi=10.1145%2f1435375.1435382&partnerID=40&md5=3c9a4a92f1e7bfc1fb535e9a11fcc343,"We show that if there is a 2 - approximation algorithm for vertex cover on graphs with vector chromatic number at most 2 + Δ, then there is a 2 - f(, Δ) approximation algorithm for vertex cover for all graphs. © 2008 ACM.",Approximation algorithms; Vertex cover,Polynomial approximation; Chromatic numbers; Hard instances; Vertex cover; Approximation algorithms
Structure and linear-time recognition of 4-leaf powers,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849146134&doi=10.1145%2f1435375.1435386&partnerID=40&md5=b916da3f3dc9353bf3cc7c8353777b4a,"A graph G is the k-leaf power of a tree T if its vertices are leaves of T such that two vertices are adjacent in G if and only if their distance in T is at most k. Then T is a k-leaf root of G. This notion was introduced and studied by Nishimura, Ragde, and Thilikos [2002], motivated by the search for underlying phylogenetic trees. Their results imply an O(n3)-time recognition algorithm for 4-leaf powers. Recently, Rautenbach [2006] as well as Dom et al. [2005] characterized 4-leaf powers without true twins in terms of forbidden subgraphs. We give new characterizations for 4-leaf powers and squares of trees by a complete structural analysis. As a consequence, we obtain a conceptually simple linear-time recognition of 4-leaf powers. © 2008 ACM.",Graph powers; Leaf powers; Phylogenetic trees; Squares of trees; Trees,Graph theory; Structural analysis; Graph powers; Leaf powers; Phylogenetic trees; Squares of trees; Trees; Trees (mathematics)
Algorithms for center and Tverberg points,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849162448&doi=10.1145%2f1435375.1435380&partnerID=40&md5=9317cbdad061632729cb8e23c6692f04,"Given a set S of n points in R3, a point x in R3 is called center point of S if every closed halfspace whose bounding hyperplane passes through x contains at least ⌈n/4⌉ points from S. We present a near-quadratic algorithm for computing the center region, that is the set of all center points, of a set of n points in R3. This is nearly tight in the worst case since the center region can have (n2) complexity. We then consider sets S of 3n points in the plane which are the union of three disjoint sets consisting respectively of n red, n blue, and n green points. A point x in R2 is called a colored Tverberg point of S if there is a partition of S into n triples with one point of each color, so that x lies in all triangles spanned by these triples. We present a first polynomial-time algorithm for recognizing whether a given point is a colored Tverberg point of such a 3-colored set S. © 2008 ACM.",Arrangements; Center point; Tverberg point,Arrangements; Center point; Disjoint sets; Quadratic algorithms; Time algorithms; Tverberg point; Worst cases
Online nonpreemptive scheduling of equal-length jobs on two identical machines,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849159858&doi=10.1145%2f1435375.1435377&partnerID=40&md5=29c0818d9a3dd66e978246e37d49d98e,"We consider the nonpreemptive scheduling of two identical machines for jobs with equal processing times yet arbitrary release dates and deadlines. Our objective is to maximize the number of jobs completed by their deadlines. Using standard nomenclature, this problem is denoted as P2 pj = p,r j ∑ j. The problem is known to be polynomially solvable in an offline setting. In an online variant of the problem, a job's existence and parameters are revealed to the scheduler only upon that job's release date. We present an online deterministic algorithm for the problem and prove that it is 3/2-competitive. A simple lower bound shows that this is the optimal deterministic competitiveness. © 2008 ACM.",Admission control; Competitive analysis; Scheduling,Competition; Control theory; Admission control; Competitive analysis; Deterministic algorithms; Identical machines; Lower bounds; Nonpreemptive scheduling; Offline; Processing times; Release dates; Scheduling
Combinatorial dominance guarantees for problems with infeasible solutions,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849165868&doi=10.1145%2f1435375.1435383&partnerID=40&md5=dc08619e63ce19192c21983d08797b0a,"The design and analysis of approximation algorithms for NP-hard problems is perhaps the most active research area in the theory of combinatorial algorithms. In this article, we study the notion of a combinatorial dominance guarantee as a way for assessing the performance of a given approximation algorithm. An f(n) dominance bound is a guarantee that the heuristic always returns a solution not worse than at least f(n) solutions. We give tight analysis of many heuristics, and establish novel and interesting dominance guarantees even for certain inapproximable problems and heuristic search algorithms. For example, we show that the maximal matching heuristic of VERTEX COVER offers a combinatorial dominance guarantee of 2n - (1.839 + o(1))n. We also give inapproximability results for most of the problems we discuss. © 2008 ACM.",Algorithms analysis; Approximation algorithms; Computation complexity; Dominance analysis,Combinatorial mathematics; Computational methods; Heuristic algorithms; Heuristic methods; Learning algorithms; Polynomial approximation; Combinatorial algorithms; Computation complexity; Design and analyses; Dominance analysis; Heuristic search algorithms; Inapproximability; Maximal matching heuristic; NP-hard problems; Research areas; Approximation algorithms
Kinetic and dynamic data structures for closest pair and all nearest neighbors,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849094580&doi=10.1145%2f1435375.1435379&partnerID=40&md5=bac8436770a7c7870a9fa8066a52069e,"We present simple, fully dynamic and kinetic data structures, which are variants of a dynamic two-dimensional range tree, for maintaining the closest pair and all nearest neighbors for a set of n moving points in the plane; insertions and deletions of points are also allowed. If no insertions or deletions take place, the structure for the closest pair uses O(n log n) space, and processes O(n2Βs+2(n)log n) critical events, each in O(log2n) time. Here s is the maximum number of times where the distances between any two specific pairs of points can become equal, Βs(q) = s(q)/q, and s(q) is the maximum length of Davenport-Schinzel sequences of order s on q symbols. The dynamic version of the problem incurs a slight degradation in performance: If m n insertions and deletions are performed, the structure still uses O(n log n) space, and processes O(mnΒs+2(n)log3 n) events, each in O(log3n) time. Our kinetic data structure for all nearest neighbors uses O(n log2 n) space, and processes O(n 2Β2s+2(n)log3 n) critical events. The expected time to process all events is O(n2Β s+22(n) log4n), though processing a single event may take Θ(n) expected time in the worst case. If m n insertions and deletions are performed, then the expected number of events is O(mnΒ2s+2(n) log3n) and processing them all takes O(mnΒ2s+2(n) log4n). An insertion or deletion takes O(n) expected time. © 2008 ACM.",Closest pair; Computational geometry; Kinetic data structures; Nearest neighbors,Computational geometry; Convolutional codes; File organization; Manganese compounds; Trees (mathematics); All nearest neighbors; Closest pair; Critical events; Dynamic data structures; Insertions and deletions; Kinetic data structures; Nearest neighbors; Single events; Worst cases; Data structures
Multipartite priority queues,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149087466&doi=10.1145%2f1435375.1435389&partnerID=40&md5=bfefc78084f7bdf8dbc4ec6ade9680f4,"We introduce a framework for reducing the number of element comparisons performed in priority-queue operations. In particular, we give a priority queue which guarantees the worst-case cost of O(1) per minimum finding and insertion, and the worst-case cost of O(log n) with at most log n + O(1) element comparisons per deletion, improving the bound of 2 log n + O(1) known for binomial queues. Here, n denotes the number of elements stored in the data structure prior to the operation in question, and log n equals log 2(max {2, n}). As an immediate application of the priority queue developed, we obtain a sorting algorithm that is optimally adaptive with respect to the inversion measure of disorder, and that sorts a sequence having n elements and I inversions with at most n log (I/n) + O(n) element comparisons. © 2008 ACM.",Constant factors; Heaps; Meticulous analysis; Priority queues,Adaptive algorithms; Data structures; File organization; Packet switching; Queueing networks; Case costs; Constant factors; Element comparisons; Heaps; Meticulous analysis; Priority queues; Sorting algorithms; Queueing theory
On the minimum common integer partition problem,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849109962&doi=10.1145%2f1435375.1435387&partnerID=40&md5=646802b23b0f9467e95aa8a91d3a9600,"We introduce a new combinatorial optimization problem in this article, called the minimum common integer partition (MCIP) problem, which was inspired by computational biology applications including ortholog assignment and DNA fingerprint assembly. A partition of a positive integer n is a multiset of positive integers that add up to exactly n, and an integer partition of a multiset S of integers is defined as the multiset union of partitions of integers in S. Given a sequence of multisets S1, S2, , Sk of integers, where k 2, we say that a multiset is a common integer partition if it is an integer partition of every multiset Si, 1 i k. The MCIP problem is thus defined as to find a common integer partition of S1, S2, , Sk with the minimum cardinality, denoted as MCIP(S1, S2, , Sk). It is easy to see that the MCIP problem is NP-hard, since it generalizes the well-known subset sum problem. We can in fact show that it is APX-hard. We will also present a 5/4-approximation algorithm for the MCIP problem when k = 2, and a 3k(k-1)/3k-2-approximation algorithm for k 3. © 2008 ACM.",Approximation algorithm; Combinatorial optimization; Computational biology; Integer partition; NP-hard; Subset sum,Bioinformatics; Biology; Combinatorial mathematics; Combinatorial optimization; Nucleic acids; Optimization; Organic acids; Partitions (building); Polynomial approximation; Turbulent flow; Cardinality; Combinatorial optimization problems; Computational biology; DNA fingerprints; Integer partition; Multi sets; NP-hard; Ortholog; Positive integers; Subset sum; Subset sum problems; Approximation algorithms
Competitive buffer management for shared-memory switches,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849107230&doi=10.1145%2f1435375.1435378&partnerID=40&md5=dfa1f1a26ca8344a8cd0aa15f540eba4,"We consider buffer management policies for shared memory switches. We study the case of overloads resulting in packet loss, where the constraint is the limited shared memory capacity. The goal of the buffer management policy is that of maximizing the number of packets transmitted. The problem is online in nature, and thus we use competitive analysis to measure the performance of the buffer management policies. Our main result is to show that the well-known preemptive Longest Queue Drop (LQD) policy is at most 2-competitive and at least 2-competitive. We also demonstrate a general lower bound of 4/3 on the performance of any deterministic online policy. Finally, we consider some other popular non-preemptive policies including Complete Partition, Complete Sharing, Static Threshold and Dynamic Threshold and derive almost tight bounds on their performance. © 2008 ACM.",Buffer management; Competitive analysis; Shared memory,Control theory; Buffer management; Competitive analysis; Lower bounds; Memory switches; Shared memory; Shared memory switches; Tight bounds; Management
On an infinite family of solvable Hanoi graphs,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849100164&doi=10.1145%2f1435375.1435388&partnerID=40&md5=e202016d96c6c23137a390de539241a1,"The Tower of Hanoi problem is generalized by placing pegs on the vertices of a given directed graph G with two distinguished vertices, S and D, and allowing moves only along arcs of this graph. An optimal solution for such a graph G is an algorithm that completes the task of moving a tower of any given number of disks from S to D in a minimal number of disk moves. In this article we present an algorithm which solves the problem for two infinite families of graphs, and prove its optimality. To the best of our knowledge, this is the first optimality proof for an infinite family of graphs. Furthermore, we present a unified algorithm that solves the problem for a wider family of graphs and conjecture its optimality. © 2008 ACM.",Optimality proofs; Tower of Hanoi,Buildings; Disks (structural components); Graph theory; Directed graphs; Of graphs; Optimal solutions; Optimality proofs; Tower of Hanoi; Unified algorithms; Towers
Combinatorial bounds via measure and conquer: Bounding minimal dominating sets and applications,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849165513&doi=10.1145%2f1435375.1435384&partnerID=40&md5=4d237a8bb789910ac5162ca9a731a15c,"We provide an algorithm listing all minimal dominating sets of a graph on n vertices in time O(1.7159n). This result can be seen as an algorithmic proof of the fact that the number of minimal dominating sets in a graph on n vertices is at most 1.7159n, thus improving on the trivial O(2n/n) bound. Our result makes use of the measure-and-conquer technique which was recently developed in the area of exact algorithms. Based on this result, we derive an O(2.8718n) algorithm for the domatic number problem. © 2008 ACM.",Domatic number; Exact exponential algorithms; Listing algorithms; Measure and conquer; Minimum dominating set; Minimum set cover,Domatic number; Exact exponential algorithms; Listing algorithms; Measure and conquer; Minimum dominating set; Minimum set cover; Combinatorial mathematics
SRPT optimally utilizes faster machines to minimize flow time,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849120326&doi=10.1145%2f1435375.1435376&partnerID=40&md5=adc563b836f5b9634d9c7f958fc8d02f,"We analyze the shortest remaining processing time (SRPT) algorithm with respect to the problem of scheduling n jobs with release times on m identical machines to minimize total flow time. It is known that SRPT is optimal if m = 1 but that SRPT has a worst-case approximation ratio of Θ(min(log n/m, log Δ)) for this problem, where Δ is the ratio of the length of the longest job divided by the length of the shortest job. It has previously been shown that SRPT is able to use faster machines to produce a schedule as good as an optimal algorithm using slower machines. We now show that SRPT optimally uses these faster machines with respect to the worst-case approximation ratio. That is, if SRPT is given machines that are s 2 - 1/m times as fast as those used by an optimal algorithm, SRPT's flow time is at least s times smaller than the flow time incurred by the optimal algorithm. Clearly, no algorithm can offer a better worst-case guarantee, and we show that existing algorithms with similar performance guarantees to SRPT without resource augmentation do not optimally use extra resources. © 2008 ACM.",Eesource augmentation; Flow time; Parallel machines; Scheduling; SRPT,Polynomial approximation; Scheduling; Approximation ratios; Eesource augmentation; Flow time; Identical machines; Optimal algorithms; Parallel machines; Performance guarantees; Release times; Resource augmentations; Shortest remaining processing times; SRPT; Total flow times; Scheduling algorithms
Approximating rank-width and clique-width quickly,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849167967&doi=10.1145%2f1435375.1435385&partnerID=40&md5=102c62027a7876a3ba5acfe798831c35,"Rank-width was defined by Oum and Seymour [2006] to investigate clique-width. They constructed an algorithm that either outputs a rank-decomposition of width at most f (k) for some function f or confirms that rank-width is larger than k in time O(|V|9 log |V|) for an input graph G = (V, E) and a fixed k.We develop three separate algorithms of this kind with faster running time. We construct an O(|V|4)-time algorithm with f (k) = 3k + 1 by constructing a subroutine for the previous algorithm; we avoid eneric algorithms minimizing submodular functions used by Oum and Seymour. Another one is an O(|V|3)-time algorithm with f (k) = 24k, achieved by giving a reduction from graphs to binary matroids; then we use an approximation algorithm for matroid branch-width by Hliněny̌ [2005]. Finally we construct an O(|V|3)-time algorithm with f (k) = 3k - 1 by combining the ideas of the two previously cited papers. © 2008 ACM.",Approximation algorithms; Branch-width; Clique-width; Matroids; Rank-width,Graph theory; Matrix algebra; Polynomial approximation; Binary matroids; Branch-width; Cited papers; Clique-width; Input graphs; Matroid; Matroids; Rank-width; Running times; Separate algorithms; Submodular functions; Time algorithms; Approximation algorithms
Distributed weighted vertex cover via maximal matchings,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849089196&doi=10.1145%2f1435375.1435381&partnerID=40&md5=b2d5f001c014170e5f6d243b4cc939d6,"In this article, we consider the problem of computing a minimum-weight vertex-cover in an n-node, weighted, undirected graph G = (V,E). We present a fully distributed algorithm for computing vertex covers of weight at most twice the optimum, in the case of integer weights. Our algorithm runs in an expected number of O(log n + log Ŵ) communication rounds, where Ŵ is the average vertex-weight. The previous best algorithm for this problem requires O(log n(log n + logŴ)) rounds and it is not fully distributed. For a maximal matching M in G, it is a well-known fact that any vertex-cover in G needs to have at least M vertices. Our algorithm is based on a generalization of this combinatorial lower-bound to the weighted setting. © 2008 ACM.",Approximation algorithms; Distributed algorithms; Maximal matching; Vertex cover,Combinatorial mathematics; Packet networks; Parallel algorithms; Polynomial approximation; Trees (mathematics); Distributed algorithms; Integer weights; Maximal matching; Maximal matchings; Undirected graphs; Vertex cover; Vertex- weights; Weighted settings; Approximation algorithms
The relative worst order ratio applied to seat reservation,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849119094&doi=10.1145%2f1383369.1383379&partnerID=40&md5=49420e0c37dbc9b5e9785bae2e373913,"The seat reservation problem is the problem of assigning passengers to seats on a train with n seats and k stations enroute in an online manner. The performance of algorithms for this problem is studied using the relative worst order ratio, a fairly new measure for the quality of online algorithms, which allows for direct comparisons between algorithms. This study has yielded new separations between algorithms. For example, for both variants of the problem considered, using the relative worst order ratio, First-Fit and Best-Fit are shown to be better than Worst-Fit. © 2008 ACM.",Online; Quality measure; Relative worst order ratio; Seat reservation,Online; Quality measure; Relative worst order ratio; Seat reservation; Worst order ratio; Separation
Improved algorithms for optimal embeddings,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849138162&doi=10.1145%2f1383369.1383376&partnerID=40&md5=8740cf706f9418eb4e3425f39050f9c3,"In the last decade, the notion of metric embeddings with small distortion has received wide attention in the literature, with applications in combinatorial optimization, discrete mathematics, and bio-informatics. The notion of embedding is, given two metric spaces on the same number of points, to find a bijection that minimizes maximum Lipschitz and bi-Lipschitz constants. One reason for the popularity of the notion is that algorithms designed for one metric space can be applied to a different one, given an embedding with small distortion. The better distortion, the better the effectiveness of the original algorithm applied to a new metric space. The goal recently studied by Kenyon et al. [2004] is to consider all possible embeddings between two finite metric spaces and to find the best possible one; that is, consider a single objective function over the space of all possible embeddings that minimizes the distortion. In this article we continue this important direction. In particular, using a theorem of Albert and Atkinson [2005], we are able to provide an algorithm to find the optimal bijection between two line metrics, provided that the optimal distortion is smaller than 13.602. This improves the previous bound of 3 + 2√2, solving an open question posed by Kenyon et al. [2004]. Further, we show an inherent limitation of algorithms using the ""forbidden pattern"" based dynamic programming approach, in that they cannot find optimal mapping if the optimal distortion is more than 7 + 4√3(≃ 13.928). Thus, our results are almost optimal for this method. We also show that previous techniques for general embeddings apply to a (slightly) more general class of metrics. © 2008 ACM.",Dynamic programming; Forbidden patterns; Line embeddings; Metric spaces; Optimal metric embeddings; Shape matching,Algorithms; Boolean functions; Combinatorial optimization; Communication channels (information theory); Mathematical programming; Set theory; Systems engineering; Topology; Trees (mathematics); Dynamic programming; Forbidden patterns; Line embeddings; Metric spaces; Optimal metric embeddings; Shape matching; Combinatorial mathematics
"Ordinal embeddings of minimum relaxation: General properties, trees, and ultrametrics",2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849083491&doi=10.1145%2f1383369.1383377&partnerID=40&md5=663a4b2b5d5de3508facd65c7bd7ebda,"We introduce a new notion of embedding, called minimum-relaxation ordinal embedding, parallel to the standard notion of minimum-distortion (metric) embedding. In an ordinal embedding, it is the relative order between pairs of distances, and not the distances themselves, that must be preserved as much as possible. The (multiplicative) relaxation of an ordinal embedding is the maximum ratio between two distances whose relative order is inverted by the embedding. We develop several worst-case bounds and approximation algorithms on ordinal embedding. In particular, we establish that ordinal embedding has many qualitative differences from metric embedding, and we capture the ordinal behavior of ultrametrics and shortest-path metrics of unweighted trees. © 2008 ACM.",Distortion; Metrics; Ordinal embedding; Relaxation,Distortion; Metrics; Ordinal embedding; Relaxation; Ultrametrics; Approximation algorithms
Dynamic routing schemes for graphs with low local density,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849088312&doi=10.1145%2f1383369.1383372&partnerID=40&md5=f56866a3efd19d44b952bbe6f7097339,"This article studies approximate distributed routing schemes on dynamic communication networks. The work focuses on dynamic weighted general graphs where the vertices of the graph are fixed, but the weights of the edges may change. Our main contribution concerns bounding the cost of adapting to dynamic changes. The update efficiency of a routing scheme is measured by the time needed in order to update the routing scheme following a weight change. A naive dynamic routing scheme, which updates all vertices following a weight change, requires (Diam) time in order to perform the updates after every weight change, where Diam is the diameter of the underlying graph. In contrast, this article presents approximate dynamic routing schemes with average time complexity Θ̃(D) per topological change, where D is the local density parameter of the underlying graph. Following a weight change, our scheme never incurs more than Diam time; thus, our scheme is particularly efficient on graphs which have low local density and large diameter. The article also establishes upper and lower bounds on the size of the databases required by the scheme at each site. © 2008 ACM.",Distributed algorithms; Dynamic networks; Routing schemes,Chlorine compounds; Computer programming languages; Control theory; Graph theory; Network routing; Routing protocols; Sensor networks; Distributed algorithms; Dynamic networks; Routing schemes; Weight changes; Computer networks
Dense subgraph problems with output-density conditions,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849144347&doi=10.1145%2f1383369.1383374&partnerID=40&md5=5305bf82c92309388cd4e38843b393a2,"We consider the dense subgraph problem that extracts a subgraph, with a prescribed number of vertices, having the maximum number of edges (or total edge weight, in the weighted case) in a given graph. We give approximation algorithms with improved theoretical approximation ratios assuming that the density of the optimal output subgraph is high, where density is the ratio of number of edges (or sum of edge weights) to the number of edges in the clique on the same number of vertices. Moreover, we investigate the case where the input graph is bipartite and design a randomized pseudopolynomial time approximation scheme that can become a randomized PTAS, even if the size of the optimal output graph is comparatively small. This is a significant improvement in a theoretical sense, since no constant-ratio approximation algorithm was known previously if the output graph has o(n) vertices. © 2008 ACM.",Approximation algorithms; Combinatorial optimization; Dense subgraph; Randomized algorithms,Polynomial approximation; Combinatorial optimization; Dense subgraph; Randomized algorithms; Sub graphs; Approximation algorithms
Label-guided graph exploration by a finite automaton,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849145170&doi=10.1145%2f1383369.1383373&partnerID=40&md5=0a3da7ff32fb0e86a969aabd03336f27,"A finite automaton, simply referred to as a robot, has to explore a graph, that is, visit all the nodes of the graph. The robot has no a priori knowledge of the topology of the graph, nor of its size. It is known that for any k-state robot, there exists a graph of maximum degree 3 that the robot cannot explore. This article considers the effects of allowing the system designer to add short labels to the graph nodes in a preprocessing stage, for helping the exploration by the robot. We describe an exploration algorithm that, given appropriate 2-bit labels (in fact, only 3-valued labels), allows a robot to explore all graphs. Furthermore, we describe a suitable labeling algorithm for generating the required labels in linear time. We also show how to modify our labeling scheme so that a robot can explore all graphs of bounded degree, given appropriate 1-bit labels. In other words, although there is no robot able to explore all graphs of maximum degree 3, there is a robot R, and a way to color in black or white the nodes of any bounded-degree graph G, so that R can explore the colored graph G. Finally, we give impossibility results regarding graph exploration by a robot with no internal memory (i.e., a single-state automaton). © 2008 ACM.",Distributed algorithms; Graph exploration; Labeling schemes,Graph theory; Parallel algorithms; Bounded degree graphs; Exploration algorithms; Graph exploration; Impossibility results; Label-guided graph exploration; Labeling algorithms; Labeling scheme; Pre-processing stages; Robots
Fully dynamic algorithms for chordal graphs and split graphs,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849091588&doi=10.1145%2f1383369.1383371&partnerID=40&md5=51890decbbc38e409d9fbe6f8ddeaef2,"We present the first dynamic algorithm that maintains a clique tree representation of a chordal graph and supports the following operations: (1) query whether deleting or inserting an arbitrary edge preserves chordality; and (2) delete or insert an arbitrary edge, provided it preserves chordality. We give two implementations. In the first, each operation runs in O(n) time, where n is the number of vertices. In the second, an insertion query runs in O(log2 n) time, an insertion in O(n) time, a deletion query in O(n) time, and a deletion in O(n log n) time. We also present a data structure that allows a deletion query to run in O(m) time in either implementation, where m is the current number of edges. Updating this data structure after a deletion or insertion requires O(m) time. We also present a very simple dynamic algorithm that supports each of the following operations in O(1) time on a general graph: (1) query whether the graph is split, and (2) delete or insert an arbitrary edge. © 2008 ACM.",Chordal graphs; Clique trees; Dynamic graph algorithms; Split graphs,Chordal graphs; Clique trees; Dynamic graph algorithms; Split graphs; Trees (mathematics)
Atomic congestion games among coalitions,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849112726&doi=10.1145%2f1383369.1383383&partnerID=40&md5=2feea802d55e1175bc353e643883e6fc,"We consider algorithmic questions concerning the existence, tractability, and quality of Nash equilibria, in atomic congestion games among users participating in selfish coalitions. We introduce a coalitional congestion model among atomic players and demonstrate many interesting similarities with the noncooperative case. For example, there exists a potential function proving the existence of pure Nash equilibria (PNE) in the unrelated parallel links setting; in the network setting, the finite improvement property collapses as soon as we depart from linear delays, but there is an exact potential (and thus PNE) for linear delays. The price of anarchy on identical parallel links demonstrates a quite surprising threshold behavior: It persists on being asymptotically equal to that in the case of the noncooperative KP-model, unless the number of coalitions is sublogarithmic. We also show crucial differences, mainly concerning the hardness of algorithmic problems that are solved efficiently in the noncooperative case. Although we demonstrate convergence to robust PNE, we also prove the hardness of computing them. On the other hand, we propose a generalized fully mixed Nash equilibrium that can be efficiently constructed in most cases. Finally, we propose a natural improvement policy and prove its convergence in pseudopolynomial time to PNE which are robust against (even dynamically forming) coalitions of small size. © 2008 ACM.",Algorithmic game theory; Congestion games; Convergence to equilibria; Price of anarchy,Atoms; Computer networks; Game theory; Hardness; Algorithmic game theory; Congestion games; Convergence to equilibria; Parallel links; Price of anarchy; Telecommunication networks
Approximation schemes for wireless networks,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849100716&doi=10.1145%2f1383369.1383380&partnerID=40&md5=9d6e526fa1c6a6e8c7e9811ec8db9aba,"Wireless networks are created by the communication links between a collection of radio transceivers. The nature of wireless transmissions does not lead to arbitrary undirected graphs but to structured graphs which we characterize by the polynomially bounded growth property. In contrast to many existing graph models for wireless networks, the property of polynomially bounded growth is defined independently of geometric data such as positional information. On such wireless networks, we present an approach that can be used to create polynomial-time approximation schemes for several optimization problems called the local neighborhood-based scheme. We apply this approach to the problems of seeking maximum (weight) independent sets and minimum dominating sets. These are two important problems in the area of wireless communication networks and are also used in many applications ranging from clustering to routing strategies. However, the approach is presented in a general fashion since it can be applied to other problems as well. The approach for the approximation schemes is robust in the sense that it accepts any undirected graph as input and either outputs a solution of desired quality or correctly asserts that the graph presented as input does not satisfy the structural assumption of a wireless network (an NP-hard problem). © 2008 ACM.",Bounded growth; Maximum independent set; Minimum dominating set; PTAS; Wireless ad-hoc networks,Approximation theory; Communication; Computer networks; Diffractive optical elements; Graph theory; Nuclear propulsion; Optical communication; Polynomial approximation; Radio communication; Radio links; Set theory; Bounded growth; Maximum independent set; Minimum dominating set; PTAS; Wireless ad-hoc networks; Wireless networks
Deterministic conflict-free coloring for intervals: From offline to online,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849116105&doi=10.1145%2f1383369.1383375&partnerID=40&md5=951905545e4b4831b777ef88a9cc63f2,"We investigate deterministic algorithms for a frequency assignment problem in cellular networks. The problem can be modeled as a special vertex coloring problem for hypergraphs: In every hyperedge there must exist a vertex with a color that occurs exactly once in the hyperedge (the conflict-free property). We concentrate on a special case of the problem, called conflict-free coloring for intervals. We introduce a hierarchy of four models for the aforesaid problem: (i) static, (ii) dynamic offline, (iii) dynamic online with absolute positions, and (iv) dynamic online with relative positions. In the dynamic offline model, we give a deterministic algorithm that uses at most log3/2 n + 1 &approx; 1.71 log2 n colors and show inputs that force any algorithm to use at least 3 log5 n + 1 1.29 log2 n colors. For the online absolute-positions model, we give a deterministic algorithm that uses at most 3⌈log3 n⌉ 1.89 log2 n colors. To the best of our knowledge, this is the first deterministic online algorithm using O(log n) colors in a nontrivial online model. In the online relative-positions model, we resolve an open problem by showing a tight analysis on the number of colors used by the first-fit greedy online algorithm. We also consider conflict-free coloring only with respect to intervals that contain at least one of the two extreme points. © 2008 ACM.",Cellular networks; Coloring; Conflict free; Frequency assignment; Online algorithms,Cellular neural networks; Computer networks; Graph theory; Cellular networks; Conflict free; Frequency assignment; Online algorithms; Coloring
Approximation algorithms for a facility location problem with service capacities,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849111738&doi=10.1145%2f1383369.1383381&partnerID=40&md5=b95f69428c02cbb3f51b01ab8620946a,"We present the first constant-factor approximation algorithms for the following problem. Given a metric space (V, c), a finite set D V of terminals/customers with demands d : D → R+, a facility opening cost f R+ and a capacity u R+, find a partition D = D 1Dk and Steiner trees Ti for Di (i = 1, ,k) with c(E(Ti)) + d(Di) u for i = 1,k such that ∑i == 1k c(E(Ti)) + kf is minimum. This problem arises in VLSI design. It generalizes the bin-packing problem and the Steiner tree problem. In contrast to other network design and facility location problems, it has the additional feature of upper bounds on the service cost that each facility can handle. Among other results, we obtain a 4.1-approximation in polynomial time, a 4.5-approximation in cubic time, and a 5-approximation as fast as computing a minimum spanning tree on (D, c). © 2008 ACM.",Approximation algorithm; Facility location; Network design; VLSI design,Facilities; Set theory; Topology; Approximation algorithm; Facility location; Facility Location problems; Network design; VLSI design; Approximation algorithms
A new approximation algorithm for the asymmetric TSP with triangle inequality,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849137825&doi=10.1145%2f1383369.1383378&partnerID=40&md5=1fc4e57b286ae815e9a085880a96de1d,We present a polynomial time factor 0.999 log n approximation algorithm for the asymmetric traveling salesperson problem with triangle inequality. © 2008 ACM.,Approximation algorithm; Cycle cover; Traveling salesman problem; TSP,Boolean functions; Traveling salesman problem; Triangulation; Approximation algorithm; Cycle cover; Triangle inequalities; TSP; Polynomial approximation
Fault-tolerant facility location,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849136176&doi=10.1145%2f1383369.1383382&partnerID=40&md5=17d797873600678536c9bcf991a644f4,"We consider a fault-tolerant generalization of the classical uncapacitated facility location problem, where each client j has a requirement that r j distinct facilities serve it, instead of just one. We give a 2.076-approximation algorithm for this problem using LP rounding, which is currently the best-known performance guarantee. Our algorithm exploits primal and dual complementary slackness conditions and is based on clustered randomized rounding. A technical difficulty that we overcome is the presence of terms with negative coefficients in the dual objective function, which makes it difficult to bound the cost in terms of dual variables. For the case where all requirements are the same, we give a primal-dual 1.52-approximation algorithm. We also consider a fault-tolerant version of the k-median problem. In the metric k-median problem, we are given n points in a metric space. We must select k of these to be centers, and then assign each input point j to the selected center that is closest to it. In the fault-tolerant version we want j to be assigned to rj distinct centers. The goal is to select the k centers so as to minimize the sum of assignment costs. The primal-dual algorithm for fault-tolerant facility location with uniform requirements also yields a 4-approximation algorithm for the fault-tolerant k-median problem for this case. This the first constant-factor approximation algorithm for the uniform requirements case. © 2008 ACM.",Approximation algorithms; Facility location; K-median problem,Approximation algorithms; Facility location; Fault-tolerant; K-median problem; Facilities
An O(n2.75) algorithm for incremental topological ordering,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849130571&doi=10.1145%2f1383369.1383370&partnerID=40&md5=79ea819663f9338d82bbc8a431d02716,"We present a simple algorithm which maintains the topological order of a directed acyclic graph (DAG) with n nodes, under an online edge insertion sequence, in O(n2.75) time, independent of the number m of edges inserted. For dense DAGs, this is an improvement over the previous best result of O(min{m 32 log n,m 32 + n2 log n}) by Katriel and Bodlaender [2006].We also provide an empirical comparison of our algorithm with other algorithms for incremental topological sorting. © 2008 ACM.",Dynamic algorithms; Graphs; Online algorithms; Topological order,Dynamic algorithms; Graphs; Online algorithms; Topological order; Boolean functions
Writing-all deterministically and optimally using a nontrivial number of asynchronous processors,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249162476&doi=10.1145%2f1367064.1367073&partnerID=40&md5=1859589ceebcc82060b49c7de7fffbb8,"The problem of performing n tasks on p asynchronous or undependable processors is a basic problem in distributed computing. This article considers an abstraction of this problem called Write-All: using p processors write 1's into all locations of an array of size n. In this problem writing 1 abstracts the notion of performing a simple task. Despite substantial research, there is a dearth of efficient deterministic asynchronous algorithms for Write-All/. Efficiency of algorithms is measured in terms of work that accounts for all local steps performed by the processors in solving the problem. Thus, an optimal algorithm would have work Θ(n), however it is known that optimality cannot be achieved when p = (n). The quest then is to obtain work-optimal solutions for this problem using a nontrivial, compared to n, number of processors p. The algorithm presented in this article has work complexity of O(n + p2 + ε), and it achieves work optimality for p = O(n 1/(2 + ε)) for any ε > 0, while the previous best result achieved optimality for p4n/log n. Additionally, the new result uses only the atomic read/write memory, without resorting to using the test-and-set primitive that was necessary in the previous solution. © 2008 ACM.",Asynchrony; Distributed algorithms; Shared memory; Work; Write-All,Boolean functions; Asynchronous algorithms; Asynchronous processors; Distributed computing; Optimal algorithms; Optimal Solutions; Optimality; Work complexity; Abstracting
Getting the best response for your erg,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249144371&doi=10.1145%2f1367064.1367078&partnerID=40&md5=6746dddaec084023ba19af0739032962,"We consider the speed scaling problem of minimizing the average response time of a collection of dynamically released jobs subject to a constraint A on energy used. We propose an algorithmic approach in which an energy optimal schedule is computed for a huge A, and then the energy optimal schedule is maintained as A decreases. We show that this approach yields an efficient algorithm for equi-work jobs. We note that the energy optimal schedule has the surprising feature that the job speeds are not monotone functions of the available energy. We then explain why this algorithmic approach is problematic for arbitrary work jobs. Finally, we explain how to use the algorithm for equi-work jobs to obtain an algorithm for arbitrary work jobs that is O(1)-approximate with respect to average response time, given an additional factor of (1 + ε) energy. © 2008 ACM.",Frequency scaling; Power management; Scheduling; Speed scaling; Voltage scaling,Speed; Algorithmic approach; Best response; Efficient algorithms; Monotone functions; Optimal scheduling; Response time; Speed scaling; Boolean functions
Optimal branch-decomposition of planar graphs in O(n3) time,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249100718&doi=10.1145%2f1367064.1367070&partnerID=40&md5=11de635789f74c1751d142fd6de449ee,"We give an O(n3) time algorithm for constructing a minimum-width branch-decomposition of a given planar graph with n vertices. This is achieved through a refinement to the previously best known algorithm of Seymour and Thomas, which runs in O(n4) time. © 2008 ACM.",Branch-decompositions; Planar graphs,Optimal branch-decomposition; Planar graphs
Compact name-independent routing with minimum stretch,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249105171&doi=10.1145%2f1367064.1367077&partnerID=40&md5=eb48bd4227e21b06a0032056e2ac44c7,"Given a weighted undirected network with arbitrary node names, we present a compact routing scheme, using a(n,) space routing table at each node, and routing along paths of stretch 3, that is, at most thrice as long as the minimum cost paths. This is optimal in a very strong sense. It is known that no compact routing using o(n) space per node can route with stretch below 3. Also, it is known that any stretch below 5 requires (n,)space per node. © 2008 ACM.",Compact routing,Routing algorithms; Telecommunication networks; Compact routing; Minimum costs; Routing table (RT); Undirected network; Powders
Balanced parentheses strike back,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249092842&doi=10.1145%2f1367064.1367068&partnerID=40&md5=d90992a030f73b20bfb4a3165443489f,"An ordinal tree is an arbitrary rooted tree where the children of each node are ordered. Succinct representations for ordinal trees with efficient query support have been extensively studied. The best previously known result is due to Geary et al. [2004b, pages 1-10]. The number of bits required by their representation for an n-node ordinal tree T is 2n + o(n), whose first-order term is information-theoretically optimal. Their representation supports a large set of O(1)-time queries on T . Based upon a balanced string of 2n parentheses, we give an improved 2n+o(n)-bit representation for T . Our improvement is two-fold: First, the set of O(1)-time queries supported by our representation is a proper superset of that supported by the representation of Geary, Raman, and Raman. Second, it is also much easier for our representation to support new queries by simply adding new auxiliary strings. © 2008 ACM.",Succinct data structures; XML document representation,First orders; Rooted trees
Determining plurality,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249106467&doi=10.1145%2f1367064.1367066&partnerID=40&md5=6b8969c01664d2df9326511e9ba141ca,"Given a set of n elements, each of which is colored one of c colors, we must determine an element of the plurality (most frequently occurring) color by pairwise equal/unequal color comparisons of elements. We prove that (c - 1)(n - c)/2 color comparisons are necessary in the worst case to determine the plurality color and give an algorithm requiring (0.775c + 5.9)n + O(c 2) color comparisons for c 9. © 2008 ACM.",Algorithm analysis; Majority problem; Plurality problem,Color; 2-color; Worst case; Optical properties
"Clustering, community partition and disjoint spanning trees",2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249093276&doi=10.1145%2f1367064.1367075&partnerID=40&md5=e0b098a85266e2b4b355287d2fb16df4,"Clustering method is one of the most important tools in statistics. In a graph theory model, clustering is the process of finding all dense subgraphs. A mathematically well-defined measure for graph density is introduced in this article as follows. Let G = (V, E) be a graph (or multi-graph) and H be a subgraph of G. The dynamic density of H is the greatest integer k such that min∀P {|E(H/P)|/|V(H/P)| - 1} > k where the minimum is taken over all possible partitions P of the vertex set of H, and H/P is the graph obtained from H by contracting each part of P into a single vertex. A subgraph H of G is a level-k community if H is a maximal subgraph of G with dynamic density at least k. An algorithm is designed in this paper to detect all level-h communities of an input multi-graph G. The worst-case complexity of this algorithm is upper bounded by O(V(G)2h2). This new method is one of few available clustering methods that are mathematically well-defined, supported by rigorous mathematical proof and able to achieve the optimization goal with polynomial complexity. As a byproduct, this algorithm also can be applied for finding edge-disjoint spanning trees of a multi-graph. The worst-case complexity is lower than all known algorithms for multi-graphs. © 2008 ACM.",Clustering; Community; Dense subgraph; Dynamic density; Hierarchical clustering; Polynomial algorithm; Spanning trees,Flow of solids; Topology; Clustering methods; Dynamic density; Spanning trees; Sub graphs; Sub-graphs; Graph theory
Dynamic entropy-compressed sequences and full-text indexes,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249098580&doi=10.1145%2f1367064.1367072&partnerID=40&md5=5e8d02af5cae1c4743befda09959df4c,"We give new solutions to the SEARCHABLE PARTIAL SUMS WITH INDELS problem. Given a sequence of n k-bit numbers, we present a structure taking kn+o(kn) bits of space, able of performing operations sum, search, insert, and delete, all in O(log n) worst-case time, for any k = O(log n). This extends previous results by Hon et al. [2003c] achieving the same space and O(log n/ log log n) time complexities for the queries, yet offering complexities for insert and delete that are amortized and worse than ours, and supported only for k = O(1). Our result matches an existing lower bound for large values of k. We also give new solutions to the DYNAMIC SEQUENCE problem. Given a sequence of n symbols in the range [1, σ] with binary zero-order entropy H0, we present a dynamic data structure that requires nH0 + o(n log σ) bits of space, which is able of performing rank and select, as well as inserting and deleting symbols at arbitrary positions, in O(log n log σ) time. Our result is the first entropy-bound dynamic data structure for rank and select over general sequences. In the case σ = 2, where both previous problems coincide, we improve the dynamic solution of Hon et al. [2003c] in that we compress the sequence. The only previous result with entropy-bound space for dynamic binary sequences is by Blandford and Blelloch [2004], which has the same complexities as our structure, but does not achieve constant 1 multiplying the entropy term in the space complexity. Finally, we present a new dynamic compressed full-text self-index, for a collection of texts over an alphabet of size σ, of overall length n and hth order empirical entropy Hh. The index requires nHh + o(n log σ) bits of space, for any h ≤ α logσ n and constant 0 < α < 1. It can count the number of occurrences of a pattern of length m in time O(m log n log σ). Each such occurrence can be reported in O(log2 n log log n) time, and displaying a context of length ℓ from a text takes time O(log n(ℓ log σ +log n log log n)). Insertion/deletion of a text to/from the collection takes O(log n log σ) time per symbol. This significantly improves the space of a previous result by Chan et al. [2004] in exchange for a slight time complexity penalty. We achieve at the same time the first dynamic index requiring essentially nHh bits of space, and the first construction of a compressed full-text self-index within that working space. Previous results achieve at best O(nHh) space with constants larger than 1 [Ferragina and Manzini 2000; Arroyuelo and Navarro 2005] and higher time complexities. An important result we prove in this paper is that thewavelet tree of the Burrows-Wheeler transform of a text, if compressed with a technique that achieves zero-order compression locally (e.g., Raman et al. [2002]), automatically achieves hth order entropy space for any h. This unforeseen relation is essential for the results of the previous paragraph, but it also derives into significant simplifications on many existing static compressed full-text self-indexes that build on wavelet trees. © 2008 ACM.",Compressed dynamic data structures; Compressed text databases; Entropy; Partial sums; Sequences,Control theory; Military data processing; Bit numbers; Lower bounds; New solutions; Partial sums; Time complexities; Zero-order entropy; Vibrations (mechanical)
Improved algorithms for the minmax-regret 1-center and 1-median problems,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249107384&doi=10.1145%2f1367064.1367076&partnerID=40&md5=e64d36bc2b2af7b1853f256e22877a41,"In this article, efficient algorithms are presented for the minmax-regret 1-center and 1-median problems on a general graph and a tree with uncertain vertex weights. For the minmax-regret 1-center problem on a general graph, we improve the previous upper bound from O(mn2 log n) to O(mn log n). For the problem on a tree, we improve the upper bound from O(n2) to O(n log2 n). For the minmax-regret 1-median problem on a general graph, we improve the upper bound from O(mn2 log n) to O(mn 2 + n3 log n). For the problem on a tree, we improve the upper bound from O(n log2 n) to O(n log n). © 2008 ACM.",Centers; General graphs; Location theory; Medians; Minmax-regret optimization; Trees,Graph theory; Manganese compounds; 1 Median; Efficient algorithms; Upper bounds; Vertex- weights; Trees (mathematics)
Average-case lower bounds for the plurality problem,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249089639&doi=10.1145%2f1367064.1367067&partnerID=40&md5=1d065aeeef8d9080e02821ccbe2740af,"Given a set of n elements, each of which is colored one of c 2 colors, we have to determine an element of the plurality (most frequently occurring) color by pairwise equal/unequal color comparisons of elements. We derive lower bounds for the expected number of color comparisons when the cn colorings are equally probable. We prove a general lower bound of c/3n - O(n) for c 2; we prove the stronger particular bounds of 7/6 n - O(n) for c = 3, 54/35n - O(n) for c = 4, 607/315n - O(n) for c = 5, 1592/693n - O(n) for c = 6, 7985/3003n - O(n) for c = 7, and 19402/6435n - O(n) for c = 8. © 2008 ACM.",Algorithm analysis; Majority problem; Plurality problem,Color; Control theory; Military data processing; Average-case; Lower bounds; Plurality problems; Optical properties
Optimality of an algorithm solving the Bottleneck Tower of Hanoi problem,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249111744&doi=10.1145%2f1367064.1367065&partnerID=40&md5=023e04ce61b2932a9fbb74c083bce0f6,"We study the Bottleneck Tower of Hanoi puzzle posed by D. Wood in 1981. There, a relaxed placement rule allows a larger disk to be placed higher than a smaller one if their size difference is less than a pregiven value k. A shortest sequence of moves (optimal algorithm) transferring all the disks placed on some peg in decreasing order of size, to another peg in the same order is in question. In 1992, D. Poole suggested a natural disk-moving strategy for this problem, and computed the length of the shortest move sequence under its framework. However, other strategies were overlooked, so the lower bound/optimality question remained open. In 1998, Benditkis, Berend, and Safro proved the optimality of Poole's algorithm for the first nontrivial case k = 2. We prove Poole's algorithm to be optimal in the general case. © 2008 ACM.",Optimality proofs; Tower of Hanoi,Buildings; Disks (structural components); Polyethylene glycols; Towers; Moving strategy; Optimal algorithms; Optimality; Tower of Hanoi; Boolean functions
Roundtrip spanners and roundtrip routing in directed graphs,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249126271&doi=10.1145%2f1367064.1367069&partnerID=40&md5=6252175b45d89ef0edbe90403c3db248,"We introduce the notion of roundtrip-spanners of weighted directed graphs and describe efficient algorithms for their construction. We show that for every integer k ≥ 1 and any ε > 0, any directed graph on n vertices with edge weights in the range [1,W] has a (2k + ε)-roundtrip-spanner with O(min{(k2/ε) n1+1/k log (nW) , (k/ε)2 n1+1/k ( log n)2.1/k }) edges. We then extend these constructions and obtain compact roundtrip routing schemes. For every integer k ≥ 1 and every ε > 0, we describe a roundtrip routing scheme that has stretch 4k + ε, and uses at each vertex a routing table of size Õ ((k2/ε)n1/k log (nW)). We also show that any weighted directed graph with arbitrary positive edge weights has a 3-roundtrip-spanner with O(n3/2) edges. This result is optimal. Finally, we present a stretch 3 roundtrip routing scheme that uses local routing tables of size Õ (n1/2). This routing scheme is essentially optimal. The roundtrip-spanner constructions and the roundtrip routing schemes for directed graphs that we describe are only slightly worse than the best available spanners and routing schemes for undirected graphs. Our roundtrip routing schemes substantially improve previous results of Cowen andWagner. Our results are obtained by combining ideas of Cohen, Cowen and Wagner, Thorup and Zwick, with some new ideas. © 2008 ACM.",Distances; Roundtrip; Routing; Shortest paths; Spanners,Directed graphs; Edge weights; Efficient algorithms; Round trips; Graph theory
Testing Euclidean minimum spanning trees in the plane,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249102595&doi=10.1145%2f1367064.1367071&partnerID=40&md5=4b8cfdba6fd2ef8151e256872b8e5273,"Given a Euclidean graph G over a set P of n points in the plane, we are interested in verifying whether G is a Euclidean minimum spanning tree (EMST) of P or G differs from it in more than ε n edges. We assume that G is given in adjacency list representation and the point/vertex set P is given in an array. We present a property testing algorithm that accepts graph G if it is an EMST of P and that rejects with probability at least 2/3 if G differs from every EMST of P in more than ε, n edges. Our algorithm runs in O(n/ε log2 (n/ε)) time and has a query complexity of O(n/ε log (n/ε)). © 2008 ACM.",Euclidean minimum spanning tree; Property testing; Randomized algorithms,Clustering algorithms; Adjacency lists; Euclidean graph; Euclidean minimum spanning trees; Graph G; Property-testing; Query complexity; Randomized Algorithms; Trees (mathematics)
Algorithms for capacitated rectangle stabbing and lot sizing with joint set-up costs,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249132941&doi=10.1145%2f1367064.1367074&partnerID=40&md5=a803c710da1259d8b8ece43a72f6066d,"In the rectangle stabbing problem, we are given a set of axis parallel rectangles and a set of horizontal and vertical lines, and our goal is to find a minimum size subset of lines that intersect all the rectangles. In this article, we study the capacitated version of this problem in which the input includes an integral capacity for each line. The capacity of a line bounds the number of rectangles that the line can cover. We consider two versions of this problem. In the first, one is allowed to use only a single copy of each line (hard capacities), and in the second, one is allowed to use multiple copies of every line, but the multiplicities are counted in the size (or weight) of the solution (soft capacities). We present an exact polynomial-time algorithm for the weighted one dimensional case with hard capacities that can be extended to the one dimensional weighted case with soft capacities. This algorithm is also extended to solve a certain capacitated multi-item lot-sizing inventory problem with joint set-up costs. For the case of d-dimensional rectangle stabbing with soft capacities, we present a 3d-approximation algorithm for the unweighted case. For d-dimensional rectangle stabbing problem with hard capacities, we present a bi-criteria algorithm that computes 4d-approximate solutions that use at most two copies of every line. Finally, we present hardness results for rectangle stabbing when the dimension is part of the input and for a two-dimensional weighted version with hard capacities. © 2008 ACM.",Approximation algorithms; Capacitated covering; Lot sizing; Rectangle stabbing,Algorithms; Approximation algorithms; Boolean functions; Polynomial approximation; Solutions; Two dimensional; Approximate solutions; Axis-parallel rectangles; Bi-criteria; Inventory problems; Joint sets; Lot sizing; Minimum size; Multi item; Polynomial-time algorithms; Rectangle stabbing; Single copy; Two-dimensional (2D); Vertical lines; Set theory
"Dissections, orientations, and trees with applications to optimal mesh encoding and random sampling",2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849124764&doi=10.1145%2f1361192.1361196&partnerID=40&md5=ce675de284e3adc60ad58911eee06126,"We present a bijection between some quadrangular dissections of an hexagon and unrooted binary trees with interesting consequences for enumeration, mesh compression, and graph sampling. Our bijection yields an efficient uniform random sampler for 3-connected planar graphs, which turns out to be determinant for the quadratic complexity of the current best-known uniform random sampler for labelled planar graphs. It also provides an encoding for the set P(n) of n-edge 3-connected planar graphs that matches the entropy bound 1/n log 2 P(n) = 2 + o(1) bits per edge (bpe). This solves a theoretical problem recently raised in mesh compression as these graphs abstract the combinatorial part of meshes with spherical topology. We also achieve the optimal parametric rate 1/n log2 P(n, i, j) bpe for graphs of P(n) with i vertices and j faces, matching in particular the optimal rate for triangulations. Our encoding relies on a linear time algorithm to compute an orientation associated with the minimal Schnyder wood of a 3-connected planar map. This algorithm is of independent interest, and it is, for instance, a key ingredient in a recent straight line drawing algorithm for 3-connected planar graphs. © 2008 ACM.",Bijection; Coding; Counting; Random generation,Binary trees; Clustering algorithms; Dissection; Encoding (symbols); Graphic methods; Mesh generation; Signal encoding; 3-connected planar graphs; Bijections; Coding; Counting; Linear-time algorithms; Optimal mesh encoding; Random generation; Straight-line drawings; Graph theory
An asymptotic approximation scheme for multigraph edge coloring,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849132303&doi=10.1145%2f1361192.1361198&partnerID=40&md5=26e545ad863d1b1ad19cd46ac39cb69f,"The edge coloring problem considers the assignment of colors from a minimum number of colors to edges of a graph such that no two edges with the same color are incident to the same node. We give polynomial time algorithms for approximate edge coloring of multigraphs, that is, parallel edges are allowed. The best previous algorithms achieve a fixed constant approximation factor plus a small additive offset. One of our algorithms achieves solution quality opt + 9opt/2 and has execution time polynomial in the number of nodes and the logarithm of the maximum edge multiplicity. © 2008 ACM.",Chromatic index; Data migration; Edge coloring; Multigraphs,Algebra; Algorithms; Approximation theory; Color; Coloring; Evolutionary algorithms; Food additives; Numerical methods; Optical properties; Parallel algorithms; Polynomial approximation; Polynomials; approximation factors; Asymptotic approximations; Edge coloring problems; Edge-coloring; execution time; Multigraph edge coloring; Multigraphs; Number of nodes; Polynomial-time algorithms; Solution quality; Approximation algorithms
ACM Transactions on Alogrithms: Guest editorial,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849138762&doi=10.1145%2f1361192.1361193&partnerID=40&md5=19dc3f05d732ba87c1f3d656d3932644,[No abstract available],,
Provably good moving least squares,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849102694&doi=10.1145%2f1361192.1361195&partnerID=40&md5=e1fe62bd10ca0e85ecee1a7c74e7efe3,"We analyze a moving least squares (MLS) interpolation scheme for reconstructing a surface from point cloud data. The input is a sufficiently dense set of sample points that lie near a closed surface F with approximate surface normals. The output is a reconstructed surface passing near the sample points. For each sample point s in the input, we define a linear point function that represents the local shape of the surface near s. These point functions are combined by a weighted average, yielding a three-dimensional function I. The reconstructed surface is implicitly defined as the zero set of I. We prove that the function I is a good approximation to the signed distance function of the sampled surface F and that the reconstructed surface is geometrically close to and isotopic to F. Our sampling requirements are derived from the local feature size function used in Delaunay-based surface reconstruction algorithms. Our analysis can handle noisy data provided the amount of noise in the input dataset is small compared to the feature size of F. © 2008 ACM.",Implicit surfaces; Interpolation; Reconstruction,Approximation algorithms; Approximation theory; Curve fitting; Function evaluation; Functions; Geodesy; Isotopes; Least squares approximations; Numerical analysis; Surface properties; Surfaces; Three dimensional; (1 1 0) surface; Closed surfaces; Data sets; Delaunay; Feature sizes; Local feature sizes; Local shape; Moving least square (MLS); Moving least-squares (MLS); Noisy data; Point cloud data; point functions; Reconstructed surfaces; sample points; Signed distance functions; Surface normals; surface reconstruction algorithms; Three dimensional (3D); Weighted averaging; Set theory
On the approximability of some network design problems,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849085750&doi=10.1145%2f1361192.1361200&partnerID=40&md5=0d6f85e3378c6e2cc07ab8446a73cbeb,"Consider the following classical network design problem: a set of terminals T = {ti} wishes to send traffic to a root r in an n-node graph G = (V, E). Each terminal ti sends di units of traffic and enough bandwidth has to be allocated on the edges to permit this. However, bandwidth on an edge e can only be allocated in integral multiples of some base capacity ue and hence provisioning k × ue bandwidth on edge e incurs a cost of ⌈k⌉ times the cost of that edge. The objective is a minimum-cost feasible solution. This is one of many network design problems widely studied where the bandwidth allocation is governed by side constraints: edges can only allow a subset of cables to be purchased on them or certain quality-of-service requirements may have to be met. In this work, we show that this problem and, in fact, several basic problems in this general network design framework cannot be approximated better than (log log n) unless NP DTIME (nO(log log log n)), where V = n. In particular, we show that this inapproximability threshold holds for (i) the Priority-Steiner Tree problem, (ii) the (single-sink) Cost-Distance problem, and (iii) the single-sink version of an even more fundamental problem, Fixed Charge Network Flow. Our results provide a further breakthrough in the understanding of the level of complexity of network design problems. These are the first nonconstant hardness results known for all these problems. © 2008 ACM.",Cost-distance; Fixed charge network flow; Hardness of approximation; Network design; Priority Steiner tree,Architectural design; Computer networks; Mass transportation; Network protocols; Approximability; Network design problem (NPD); Terminals (airport); Metropolitan area networks
Primal-dual approach for directed vertex connectivity augmentation and generalizations,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849096776&doi=10.1145%2f1361192.1361197&partnerID=40&md5=c2e3eec9d5af637b67269f7dbb8bdf83,"In their seminal paper, Frank and Jordán [1995] show that a large class of optimization problems, including certain directed graph augmentation, fall into the class of covering supermodular functions over pairs of sets. They also give an algorithm for such problems, however, it relies on the ellipsoid method. Prior to our result, combinatorial algorithms existed only for the 0 - 1 valued problem. Our key result is a combinatorial algorithm for the general problem that includes directed vertex or S-T connectivity augmentation. The algorithm is based on Benczr's previous algorithm for the 0 - 1 valued case [Benczr 2003]. Our algorithm uses a primal-dual scheme for finding covers of partially ordered sets that satisfy natural abstract properties as in Frank and Jordán. For an initial (possibly greedy) cover, the algorithm searches for witnesses for the necessity of each element in the cover. If no two (weighted) witnesses have a common cover, the solution is optimal. As long as this is not the case, the witnesses are gradually exchanged for smaller ones. Each witness change defines an appropriate change in the solution; these changes are finally unwound in a shortest-path manner to obtain a solution of size one less. © 2008 ACM.",Combinatorial algorithm; Vertex connectivity augmentation,Aerospace applications; Algorithms; Boolean functions; Combinatorial mathematics; Evolutionary algorithms; Function evaluation; Graph theory; Integer programming; Ketones; Microfluidics; Scheduling algorithms; Transients; (PL) properties; Combinatorial algorithms; Connectivity augmentation; Directed graphs; Ellipsoid method; General (CO); optimiz ation problems; Partially ordered sets; Primal-dual; Primal-dual approach; Shortest Path (SP); Supermodular functions; vertex connectivity; Set theory
Compact dictionaries for variable-length keys and data with applications,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849115971&doi=10.1145%2f1361192.1361194&partnerID=40&md5=34eec88a8ace7f73f9ca4df80ae49ec2,"We consider the problem of maintaining a dynamic dictionary T of keys and associated data for which both the keys and data are bit strings that can vary in length from zero up to the length w of a machine word. We present a data structure for this variable-bit-length dictionary problem that supports constant time lookup and expected amortized constant-time insertion and deletion. It uses O(m + 3n - nlog2n) bits, where n is the number of elements in T, and m is the total number of bits across all strings in T (keys and data). Our dictionary uses an array A[1 n] in which locations store variable-bit-length strings. We present a data structure for this variable-bit-length array problem that supports worst-case constant-time lookups and updates and uses O(m + n) bits, where m is the total number of bits across all strings stored in A. The motivation for these structures is to support applications for which it is helpful to efficiently store short varying-length bit strings. We present several applications, including representations for semidynamic graphs, order queries on integers sets, cardinal trees with varying cardinality, and simplicial meshes of d dimensions. These results either generalize or simplify previous results. © 2008 ACM.",Compression,Chemical modification; Data structures; File organization; Variational techniques; Bit lengths; bit strings; Constant time (CONTI); Keys (for locks)
Limitations of cross-monotonic cost-sharing schemes,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849087693&doi=10.1145%2f1361192.1361201&partnerID=40&md5=3e1a6e05edeb3c46de49e379788eb935,"A cost-sharing scheme is a set of rules defining how to share the cost of a service (often computed by solving a combinatorial optimization problem) amongs serviced customers. A cost-sharing scheme is cross-monotonic if it satisfies the property that everyone is better off when the set of people who receive the service expands. In this article, we develop a novel technique for proving upper bounds on the budget-balance factor of cross-monotonic cost-sharing schemes or the worst-case ratio of recovered cost to total cost. We apply this technique to games defined, based on several combinatorial optimization problems, including the problems of edge cover, vertex cover, set cover, and metric facility location and, in each case, derive tight or nearly-tight bounds. In particular, we show that for the facility location game, there is no cross-monotonic cost-sharing scheme that recovers more than a third of the total cost. This result, together with a recent 1/3-budget-balanced cross-monotonic cost-sharing scheme of Pál and Tardos [2003] closes the gap for the facility location game. For the vertex cover and set cover games, we show that no cross-monotonic cost-sharing scheme can recover more than a O(n-1/3) and O(1/n) fraction of the total cost, respectively. Finally, we study the implications of our results on the existence of group-strategyproof mechanisms. We show that every group-strategyproof mechanism corresponds to a cost-sharing scheme that satisfies a condition weaker than cross-monotonicity. Using this, we prove that group-strategyproof mechanisms satisfying additional properties give rise to cross-monotonic cost-sharing schemes and therefore our upper bounds hold. © 2008 ACM.",Cross-monotonic cost-sharing schemes; Group-strategyproof mechanism design; Probabilistic method,Aerospace applications; Budget control; Combinatorial mathematics; Combinatorial optimization; Facilities; Game theory; Optimization; Recovery; Balance (weighting); Combinatorial optimization problems; Combinatorial optimization problems (COPs); Cost sharing schemes; Cost-sharing; Edge cover; facility locations; Property (S); set cover; Set cover games; Set of rules; Tight bounds; Total costs; Upper bounds; Vertex cover; Set theory
Embeddings of negative-type metrics and an improved approximation to generalized sparsest cut,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849139807&doi=10.1145%2f1361192.1361199&partnerID=40&md5=0d74e8d09369f3c8b3674c476d61f844,"In this article, we study metrics of negative type, which are metrics (V, d) such that d is an Euclidean metric; these metrics are thus also known as ℓ2-squared metrics. We show how to embed n-point negative-type metrics into Euclidean space ℓ2 with distortion D = O(log 3/4n). This embedding result, in turn, implies an O(log 3/4k)-approximation algorithm for the Sparsest Cut problem with nonuniform demands. Another corollary we obtain is that n-point subsets of ℓ1 embed into ℓ2 with distortion O(log 3/4 n). © 2008 ACM.",Approximation algorithm; Embedding; Metrics; Negative-type metric; Sparsest cut,Embeddings; Euclidean metric; Metrics (CO)
The priority R-tree: A practically efficient and worst-case optimal R-tree,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149192617&doi=10.1145%2f1328911.1328920&partnerID=40&md5=46922977da87796b86d081c59372aed6,"We present the priority R-tree, or PR-tree, which is the first R-tree variant that always answers a window query using O((N/B)1-1/d+T/B) I/Os, where N is the number of d-dimensional (hyper-) rectangles stored in the R-tree, B is the disk block size, and T is the output size. This is provably asymptotically optimal and significantly better than other R-tree variants, where a query may visit all N/B leaves in the tree even when T = 0. We also present an extensive experimental study of the practical performance of the PR-tree using both real-life and synthetic data. This study shows that the PR-tree performs similarly to the best-known R-tree variants on real-life and relatively nicely distributed data, but outperforms them significantly on more extreme data. © 2008 ACM.",R-trees,Data reduction; Distributed computer systems; Theorem proving; Distributed data; R-trees; Trees (mathematics)
A faster and simpler fully dynamic transitive closure,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149173262&doi=10.1145%2f1328911.1328917&partnerID=40&md5=78e183ab940af6b2d7666e5e150497cf,"We obtain a new fully dynamic algorithm for maintaining the transitive closure of a directed graph. Our algorithm maintains the transitive closure matrix in a total running time of O(mn+(ins+ del) · n2), where ins (del) is the number of insert (delete) operations performed. Here n is the number of vertices in the graph and m is the initial number of edges in the graph. Obviously, reachability queries can be answered in constant time. The algorithm uses only O(n2) time which is essentially optimal for maintaining the transitive closure matrix. Our algorithm can also support path queries. If v is reachable from u, the algorithm can produce a path from u to v in time proportional to the length of the path. The best previously known algorithm for the problem is due to Demetrescu and Italiano [2000]. Their algorithm has a total running time of O(n3 + (ins + del) · n2). The query time is also constant. In addition, we also present a simple algorithm for directed acyclic graphs (DAGs) with a total running time of O(mn + ins · n2 + del). Our algorithms are obtained by combining some new ideas with techniques of Italiano [1986, 1988], King [1999], King and Thorup [2001] and Frigioni et al. [2001]. We also note that our algorithms are extremely simple and can be easily implemented. © 2008 ACM.",Directed graph; Dynamic graph algorithms; Reachability,Algorithms; Matrix algebra; Numerical methods; Query languages; Directed graph; Dynamic graph algorithms; Reachability; Graph theory
"Thin heaps, thick heaps",2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149150504&doi=10.1145%2f1328911.1328914&partnerID=40&md5=26bb3110e86aba9d5c1ccb03b034fe8d,"The Fibonacci heap was devised to provide an especially efficient implementation of Dijkstra's shortest path algorithm. Although asyptotically efficient, it is not as fast in practice as other heap implementations. Expanding on ideas of Høyer [1995], we describe three heap implementations (two versions of thin heaps and one of thick heaps) that have the same amortized efficiency as Fibonacci heaps, but need less space and promise better practical performance. As part of our development, we fill in a gap in Høyer's analysis. © 2008 ACM.",Binomial queue; Data structure; Decrease key operation; Fibonacci heap; Heap; Melding; Priority queue; Thick heap; Thin heap,Algorithms; Numerical methods; Scheduling; Binomial queue; Decrease key operation; Fibonacci heap; Priority queue; Data structures
Randomized minimum spanning tree algorithms using exponentially fewer random bits,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149192061&doi=10.1145%2f1328911.1328916&partnerID=40&md5=e22cf79a773b6562cd272388a075697f,"For many fundamental problems there exist randomized algorithms that are asymptotically optimal and are superior to the best-known deterministic algorithm. Among these are the minimum spanning tree (MST) problem, the MST sensitivity analysis problem, the parallel connected components and parallel minimum spanning tree problems, and the local sorting and set maxima problems. (For the first two problems there are provably optimal deterministic algorithms with unknown, and possibly superlinear, running times.) One downside of the randomized methods for solving these problems is that they use a number of random bits linear in the size of input. In this article we develop some general methods for reducing exponentially the consumption of random bits in comparison-based algorithms. In some cases we are able to reduce the number of random bits from linear to nearly constant, without affecting the expected running time. Most of our results are obtained by adjusting or reorganizing existing randomized algorithms to work well with a pairwise or O(1)-wise independent sampler. The prominent exception, and the main focus of this article, is a linear-time randomized minimum spanning tree algorithm that is not derived from the well-known Karger-Klein-Tarjan algorithm. In many ways it resembles more closely the deterministic minimum spanning tree algorithms based on soft heaps. Further, using our algorithm as a guide, we present a unified view of the existing nongreedy minimum spanning tree algorithms. Concepts from the Karger-Klein-Tarjan algorithm, such as F-lightness, MST verification, and sampled graphs, are related to the concepts of edge corruption, subgraph contractibility, and soft heaps, which are the basis of the deterministic MST algorithms of Chazelle and Pettie-Ramachandran. © 2008 ACM.",Graph algorithms; Minimum spanning trees; Random sampling,Algorithms; Problem solving; Random processes; Sensitivity analysis; Graph algorithms; Minimum spanning trees; Random sampling; Trees (mathematics)
Faster approximation schemes for fractional multicommodity flow problems,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149111727&doi=10.1145%2f1328911.1328924&partnerID=40&md5=d231df02baa93f36ff7043fe0d7aea6b,"We present fully polynomial approximation schemes for concurrent multicommodity flow problems that run in time of the minimum possible dependencies on the number of commodities k.We show that by modifying the algorithms by Garg and Könemann [1998] and Fleischer [2000], we can reduce their running time on a graph with n vertices and m edges from Õ (e-2(m2+km)) to Õ (e-2m2) for an implicit representation of the output, or Õ(e-2(m2 + kn)) for an explicit representation, where Õ ( f ) denotes a quantity that is O( f logO(1) m). The implicit representation consists of a set of trees rooted at sources (there can be more than one tree per source), and with sinks as their leaves, together with flow values for the flow directed from the source to the sinks in a particular tree. Given this implicit representation, the approximate value of the concurrent flow is known, but if we want the explicit flow per commodity per edge, we would have to combine all these trees together, and the cost of doing so may be prohibitive. In case we want to calculate explicitly the solution flow, we modify our schemes so that they run in time polylogarithmic in nk (n is the number of nodes in the network). This is within a polylogarithmic factor of the trivial lower bound of time (nk) needed to explicitly write down a multicommodity flow of k commodities in a network of n nodes. Therefore our schemes are within a polylogarithmic factor of the minimum possible dependencies of the running time on the number of commodities k. © 2008 ACM.",Fully-polynomial time approximation schemes; Multicommodity flows,Algorithms; Polynomials; Problem solving; Trees (mathematics); Fully-polynomial time approximation schemes; Multicommodity flows; Approximation theory
Uniform deterministic dictionaries,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149187711&doi=10.1145%2f1328911.1328912&partnerID=40&md5=df11fccac802fdd7498f9adf5329a207,"We present a new analysis of the well-known family of multiplicative hash functions, and improved deterministic algorithms for selecting good hash functions. The main motivation is realization of deterministic dictionaries with fast lookups and reasonably fast updates. The model of computation is the Word RAM, and it is assumed that the machine word-size matches the size of keys in bits. Many of the modern solutions to the dictionary problem are weakly nonuniform, that is, they require a number of constants to be computed at compile time for the stated time bounds to hold. The currently fastest deterministic dictionary uses constants not known to be computable in polynomial time. In contrast, our dictionaries do not require any special constants or instructions, and running times are independent of word (and key) length. Our family of dynamic dictionaries achieves a performance of the following type: lookups in time O(t) and updates in amortized time O(n1/t), for an appropriate parameter function t. Update procedures require division, whereas searching uses multiplication only. © 2008 ACM.",Deterministic algorithms; Perfect hashing,Computation theory; Function evaluation; Mathematical models; Problem solving; Deterministic algorithms; Perfect hashing; Glossaries
Alternation and redundancy analysis of the intersection problem,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149189456&doi=10.1145%2f1328911.1328915&partnerID=40&md5=cb4a352de5a7367ee64e7b4dc306acb3,"The intersection of sorted arrays problem has applications in search engines such as Google. Previous work has proposed and compared deterministic algorithms for this problem, in an adaptive analysis based on the encoding size of a certificate of the result (cost analysis). We define the alternation analysis, based on the nondeterministic complexity of an instance. In this analysis we prove that there is a deterministic algorithm asymptotically performing as well as any randomized algorithm in the comparison model. We define the redundancy analysis, based on a measure of the internal redundancy of the instance. In this analysis we prove that any algorithm optimal in the redundancy analysis is optimal in the alternation analysis, but that there is a randomized algorithm which performs strictly better than any deterministic algorithm in the comparison model. Finally, we describe how these results can be extended beyond the comparison model. © 2008 ACM.",Adaptive analysis; Alternation analysis; Intersection; Intersection of sorted arrays; Randomized algorithm; Redundancy analysis,Adaptive systems; Algorithms; Encoding (symbols); Problem solving; Random processes; Adaptive analysis; Alternation analysis; Intersection of sorted arrays; Randomized algorithms; Redundancy analysis; Search engines
The collective memory of amnesic processes,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149109157&doi=10.1145%2f1328911.1328923&partnerID=40&md5=0a8f307e24cebcb087a9e009c0aae176,"This article considers the problem of robustly emulating a shared atomic memory over a distributed message-passing system where processes can fail by crashing and possibly recover. We revisit the notion of atomicity in the crash-recovery context and introduce a generic algorithm that emulates an atomic memory. The algorithm is instantiated for various settings according to whether processes have access to local stable storage, and whether, in every execution of the algorithm, a sufficient number of processes are assumed not to crash. We establish the optimality of specific instances of our algorithm in terms of resilience, log complexity (number of stable storage accesses needed in every read or write operation), as well as time complexity (number of communication steps needed in every read or write operation). The article also discusses the impact of considering a multiwriter versus a single-writer memory, as well as the impact of weakening the consistency of the memory by providing safe or regular semantics instead of atomicity. © 2008 ACM.",Atomic registers; Crash recovery; Log complexity; Shared-memory emulation,Distributed computer systems; Genetic algorithms; Problem solving; Robustness (control systems); Atomic registers; Crash recovery; Log complexity; Shared-memory emulation; Storage allocation (computer)
Hierarchical bin buffering: Online local moments for dynamic external memory arrays,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149180019&doi=10.1145%2f1328911.1328925&partnerID=40&md5=b11ac5bf444bd769ae5ba98dd19fd2b5,"For a massive I/O array of size n, we want to compute the first N local moments, for some constant N. Our simpler algorithms partition the array into consecutive ranges called bins, and apply not only to local-moment queries, but also to algebraic queries. With N buffers of size n, time complexity drops to O(n). A more sophisticated approach uses hierarchical buffering and has a logarithmic time complexity (O(b logb n)), when using N hierarchical buffers of size n/b. Using overlapped bin buffering, we show that only one buffer is needed, as with wavelet-based algorithms, but using much less storage. © 2008 ACM.",Hierarchical buffers; Polynomial fitting; Statistical queries; Very large arrays,Algorithms; Computational complexity; Query languages; Storage allocation (computer); Hierarchical buffers; Polynomial fitting; Statistical queries; Very large arrays; Hierarchical systems
No sorting? Better searching!,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149125599&doi=10.1145%2f1328911.1328913&partnerID=40&md5=be6f482cc67e15d0c4133bc32b8648b3,"Questions about order versus disorder in systems and models have been fascinating scientists over the years. In computer science, order is intimately related to sorting, commonly meant as the task of arranging keys in increasing or decreasing order with respect to an underlying total order relation. The sorted organization is amenable for searching a set of n keys, since each search requires Θ(log n) comparisons in the worst case, which is optimal if the cost of a single comparison can be considered a constant. Nevertheless, we prove that disorder implicitly provides more information than order does. For the general case of searching an array of multidimensional keys whose comparison cost is proportional to their length (and hence which cannot be considered a constant), we demonstrate that ""suitable"" disorder gives better bounds than those derivable by using the natural lexicographic order. We start from previous work done by Andersson et al. [2001], who proved that θ (k log log n/log log(4 + k log log n/log n) + k + log n) character comparisons (or probes) comprise the tight complexity for searching a plain sorted array of n keys, each of length k, arranged in lexicographic order. We describe a novel permutation of the n keys that is different from the sorted order. When keys are kept ""unsorted"" in the array according to this permutation, the complexity of searching drops to Θ(k + log n) character comparisons (or probes) in theworst case, which is optimal among all possible permutations, up to a constant factor. Consequently, disorder carries more information than does order; this fact was not observable before, since the latter two bounds are Θ(log n) when k = O(1). More implications are discussed in the article, including searching in the bit-probe model. © 2008 ACM.",Implicit data structures; In-place algorithms; Searching; Sorting,Data structures; Information systems; Mathematical models; Societies and institutions; Implicit data structures; In-place algorithms; Search engines
Finding a long directed cycle,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149134923&doi=10.1145%2f1328911.1328918&partnerID=40&md5=137dfedf84fa8157b6096416549e4212,"Consider a digraph with n vertices. For any fixed value k, we present linear- and almost-linear-time algorithms to find a cycle of length k, if one exists. We also find a cycle that has length log n/log log n in polynomial time, if one exists. Under an appropriate complexity assumption it is known to be impossible to improve this guarantee by more than a log log n factor. Our approach is based on depth-first search. © 2008 ACM.",Approximation algorithms; Circumference; Cycles; Hamiltonian cycles; Long cycles,Approximation algorithms; Computational complexity; Hamiltonians; Polynomials; Search engines; Circumference; Hamiltonian cycles; Long cycles; Graph theory
Rectangular layouts and contact graphs,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149145341&doi=10.1145%2f1328911.1328919&partnerID=40&md5=31df60c8ac624d3c659ca03628709f3b,"Contact graphs of isothetic rectangles unify many concepts from applications including VLSI and architectural design, computational geometry, and GIS. Minimizing the area of their corresponding rectangular layouts is a key problem. We study the area-optimization problem and show that it is NP-hard to find a minimum-area rectangular layout of a given contact graph. We present O(n)-time algorithms that construct O(n2)-area rectangular layouts for general contact graphs and O(n log n)-area rectangular layouts for trees. (For trees, this is an O(log n)-approximation algorithm.) We also present an infinite family of graphs (respectively, trees) that require (n2) (respectively, (n log n))area. We derive these results by presenting a new characterization of graphs that admit rectangular layouts, using the related concept of rectangular duals. A corollary to our results relates the class of graphs that admit rectangular layouts to rectangle-of-influence drawings. © 2008 ACM.",Contact graphs; Rectangular duals; Rectangular layouts,Approximation algorithms; Computational geometry; Numerical methods; Optimization; Contact graphs; Rectangular duals; Rectangular layouts; Graph theory
Approximate distance oracles for geometric spanners,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149179298&doi=10.1145%2f1328911.1328921&partnerID=40&md5=4fc765e85421f5b6d2a731864959bb54,"Given an arbitrary real constant ε > 0, and a geometric graph G in d-dimensional Euclidean space with n points, O(n) edges, and constant dilation, our main result is a data structure that answers (1 + ε)-approximate shortest-path-length queries in constant time. The data structure can be constructed in O(n log n) time using O(n log n) space. This represents the first data structure that answers (1 + ε)-approximate shortest-path queries in constant time, and hence functions as an approximate distance oracle. The data structure is also applied to several other problems. In particular, we also show that approximate shortest-path queries between vertices in a planar polygonal domain with rounded obstacles can be answered in constant time. Other applications include query versions of closest-pair problems, and the efficient computation of the approximate dilations of geometric graphs. Finally, we show how to extend the main result to answer (1 + ε)-approximate shortest-path-length queries in constant time for geometric spanner graphs with m = (n) edges. The resulting data structure can be constructed in O(m + n log n) time using O(n log n) space. © 2008 ACM.",Approximation algorithm; Computational geometry; Geometric graphs; Shortest paths; Spanners,Approximation algorithms; Data structures; Graph theory; Query languages; Geometric graphs; Shortest paths; Spanners; Computational geometry
Improved bounds for scheduling conflicting jobs with minsum criteria,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149083933&doi=10.1145%2f1328911.1328922&partnerID=40&md5=472dd6718fc434f01268405bd3e3f02b,"We consider a general class of scheduling problems where a set of conflicting jobs needs to be scheduled (preemptively or nonpreemptively) on a set of machines so as to minimize the weighted sum of completion times. The conflicts among jobs are formed as an arbitrary conflict graph. Building on the framework of Queyranne and Sviridenko [2002b], we present a general technique for reducing the weighted sum of completion-times problem to the classical makespan minimization problem. Using this technique, we improve the best-known results for scheduling conflicting jobs with the min-sum objective, on several fundamental classes of graphs, including line graphs, (k +1)- claw-free graphs, and perfect graphs. In particular, we obtain the first constant-factor approximation ratio for nonpreemptive scheduling on interval graphs.We also improve the results of Kim [2003] for scheduling jobs on line graphs and for resource-constrained scheduling. © 2008 ACM.",Approximation algorithms; Coloring; Linear programming; LP rounding; Scheduling; Sum multicoloring,Approximation algorithms; Coloring; Linear programming; Optimization; Problem solving; LP rounding; Sum multicoloring; Scheduling
Path decomposition under a new cost measure with applications to optical network design,2008,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149087091&doi=10.1145%2f1328911.1328926&partnerID=40&md5=3ff25bdba1dee0e4277953a8253ac04c,"We introduce a problem directly inspired by its application to DWDM (dense wavelength division multiplexing) network design. We are given a set of demands to be carried over a network. Our goal is to choose a route for each demand and to decompose the network into a collection of edge-disjoint simple paths. These paths are called optical line systems. The cost of routing one unit of demand is the number of line systems with which the demand route overlaps; our design objective is to minimize the total cost over all demands. This cost metric is motivated by the need to minimize O-E-O (optical-electrical-optical) conversions in optical transmission. For given line systems, it is easy to find the optimal demand routes. On the other hand, for given demand routes designing the optimal line systems can be NP-hard. We first present a 2-approximation for general network topologies. As optical networks often have low node degrees, we offer an algorithm that finds the optimal solution for the special case in which the node degree is at most 3. Our solution is based on a local greedy approach. If neither demand routes nor line systems are fixed, the situation becomes much harder. Even for a restricted scenario on a 3-regular Hamiltonian network, no efficient algorithm can guarantee a constant approximation better than 2. For general topologies, we offer a simple algorithm with an O(log K)- and an O(log n)-approximation, where K is the number of demands and n the number of nodes. This approximation ratio is almost tight. For rings, a common special topology, we offer a more complex 3/2-approximation algorithm. © 2008 ACM.",Approximation algorithms; Optical network design; Path decomposition,Approximation algorithms; Fiber optic networks; Hamiltonians; Optimization; Wavelength division multiplexing; Dense wavelength division multiplexing (DWDM); Optical line systems; Optical network design; Path decomposition; Domain decomposition methods
Partial fillup and search time in LC tries,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448958036&doi=10.1145%2f1290672.1290681&partnerID=40&md5=6090120294b8bcaf34c975ffb20748ae,"Andersson and Nilsson introduced in 1993 a level-compressed trie (for short, LC trie) in which a full subtree of a node is compressed to a single node of degree being the size of the subtree. Recent experimental results indicated a dramatic improvement when full subtrees are replaced by partially filled subtrees. In this article, we provide a theoretical justification of these experimental results, showing, among others, a rather moderate improvement in search time over the original LC tries. For such an analysis, we assume that n strings are generated independently by a binary memoryless source, with p denoting the probability of emitting a 1 (and q = 1 - p). We first prove that the so-called α-fillup level F n(α) (i.e., the largest level in a trie with α fraction of nodes present at this level) is concentrated on two values with high probability: either F n(α) = k n or F n(α) = k n + 1, where k n = log 1/pq n - ln (p/q)/2 ln 3/2 (1pq) -1 (α) ln n + O(1) is an integer and (x) denotes the normal distribution function. This result directly yields the typical depth (search time) D n(α) in the α-LC tries, namely, we show that with high probability D n(α) ∼ C 2 log log n, where C 2 = 1/log(1 - h/log(1/pq)) for p q and h = -plog p-qlog q is the Shannon entropy rate. This should be compared with recently found typical depth in the original LC tries, which is C 1log log n, where C 1 = 1/log(1-h/log(1/min{p, 1-p})). In conclusion, we observe that α affects only the lower term of the α-fillup level F n(α), and the search time in α-LC tries is of the same order as in the original LC tries.",Digital trees; Level-compressed tries; Partial fillup; Poissonization; Probabilistic analysis; Strings; Trees,Binary codes; Function evaluation; Probability distributions; Digital trees; Level-compressed tries; Partial fillup; Poissonization; Strings; Trees (mathematics)
Routing selfish unsplittable traffic,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448955789&doi=10.1145%2f1290672.1290689&partnerID=40&md5=60502b3301ec1a40abf7f5590c0e27a8,"We consider general resource assignment games involving selfish users/agents in which users compete for resources and try to be assigned to those which maximize their own benefits (e.g., try to route their traffic through links which minimize the latency of their own traffic). We propose and study a mechanism design approach in which an allocation mechanism assigns users to resources and charges the users for using the resources so as to induce each user to truthfully report a private piece of information he/she holds (e.g., how much traffic he/she needs to transmit). This information is crucial for computing optimal (or close to optimal) allocations and an agent could misreport his/her information to induce the underlying allocation algorithm to output a solution which he/she likes more (e.g., which assigns better resources to him/her). For our resource allocation problems, we give an algorithmic characterization of the solutions for which truth-telling is a Nash equilibrium. A natural application of these results is to a scheduling/ routing problem which is the mechanism design counterpart of the selfish routing game of Koutsoupias and Papadimitriou [1999]: Each selfish user wants to route a piece of unsplittable traffic using one of m links of different speeds so as to minimize his/her own latency. Our mechanism design counterpart can be seen as the problem of scheduling selfish jobs on parallel related machines and is the dual of the problem of scheduling (unselfish) jobs on parallel selfish machines studied by Archer and Tardos [2001]. Koutsoupias and Papadimitriou studied an ""anarchic"" scenario in which each user chooses his/her own link, and this may produce Nash equilibria of cost (logm/ log logm) times the optimum. Our mechanism design counterpart is a possible way of reducing the effect of selfish behavior via suitable incentives to the agents (i.e., taxes for using the links).We indeed show that in the resulting game, it is possible to guarantee an approximation factor of 8 for any number of links/machines (this solution also works for online settings). However, it remains impossible to guarantee arbitrarily good approximate solutions, even for 2 links/machines and even if the allocation algorithm is allowed superpolynomial time. This result shows that our scheduling problem with selfish jobs is more difficult than the scheduling problem with selfish machines by Archer and Tardos (which admits exact solutions). We also study some generalizations of this basic problem.",Algorithmic mechanism design; Nash equilibrium; Scheduling; Selfish routing,Approximation theory; Problem solving; Scheduling; Telecommunication links; Telecommunication traffic; Algorithmic mechanism design; Nash equilibrium; Selfish routing; Routing algorithms
"Succinct indexable dictionaries with applications to encoding k-ary trees, prefix sums and multisets",2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448943298&doi=10.1145%2f1290672.1290680&partnerID=40&md5=3f311e255522e8376220e414bf253f1c,"We consider the indexable dictionary problem, which consists of storing a set S ⊆ {0, . . . ,m . 1} for some integer m while supporting the operations of rank(x), which returns the number of elements in S that are less than x if x ∈ S, and -1 otherwise; and select(i), which returns the ith smallest element in S. We give a data structure that supports both operations in O(1) time on the RAM model and requires B(n,m) + o(n) + O(lg lgm) bits to store a set of size n, where B(n,m) = [lg ( m n)] is the minimum number of bits required to store any n-element subset from a universe of size m. Previous dictionaries taking this space only supported (yes/no) membership queries in O(1) time. In the cell probe model we can remove the O(lg lgm) additive term in the space bound, answering a question raised by Fich and Miltersen [1995] and Pagh [2001]. We present extensions and applications of our indexable dictionary data structure, including: an information-theoretically optimal representation of a k-ary cardinal tree that supports standard operations in constant time; a representation of a multiset of size n from {0, . . . ,m.1} in B(n,m+n)+o(n) bits that supports (appropriate generalizations of) rank and select operations in constant time; and + O(lg lgm) a representation of a sequence of n nonnegative integers summing up to m in B(n,m + n) + o(n) bits that supports prefix sum queries in constant time.",Dictionaries; Multisets; Perfect hashing; Prefix sums; Sets; Succinct data structures; Tries,Data structures; Integer programming; Mathematical models; Set theory; Trees (mathematics); Nonnegative integers; Perfect hashing; Prefix sums; Succinct data structures; Encoding (symbols)
Introduction to SODA 2002 and 2003 special issue,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448993377&doi=10.1145%2f1290672.1290673&partnerID=40&md5=fed7c98f0759faff984d064e3696711e,[No abstract available],,
Foreword to special issue on SODA 2003 (articles 37-41),2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448949861&doi=10.1145%2f1290672.1290673&partnerID=40&md5=4a784c4a34bc20826a2387dc52a79e63,[No abstract available],,
Oblivious routing on node-capacitated and directed graphs,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448959813&doi=10.1145%2f1290672.1290688&partnerID=40&md5=083d0a69784a7bc6c7c40f877d47616d,"Oblivious routing algorithms for general undirected networks were introduced by Räcke [2002], and this work has led to many subsequent improvements and applications. Comparatively little is known about oblivious routing in general directed networks, or even in undirected networks with node capacities. We present the first nontrivial upper bounds for both these cases, providing algorithms for k-commodity oblivious routing problems with competitive ratio O(√k log(n)) for undirected nodecapacitated graphs and O(√ k n 1/4 log(n)) for directed graphs. In the special case that all commodities have a common source or sink, our upper bound becomes O(√k log(n)) in both cases, matching the lower bound up to a factor of log(n). The lower bound (which first appeared in Azar et al. [2003]) is obtained on a graph with very high degree.We show that, in fact, the degree of a graph is a crucial parameter for node-capacitated oblivious routing in undirected graphs, by providing an O(Δ polylog(n))- competitive oblivious routing scheme for graphs of degree Δ. For the directed case, however, we show that the lower bound of Ω(√n) still holds in low-degree graphs. Finally, we settle an open question about routing problems in which all commodities share a common source or sink.We show that even in this simplified scenario there are networks in which no oblivious routing algorithm can achieve a competitive ratio better than Ω(log n).",Communication networks; Directed graphs; Node-capacitated graphs; Oblivious routing,Problem solving; Routing algorithms; Telecommunication networks; Directed graphs; Node-capacitated graphs; Oblivious routing; Graph theory
Edge-disjoint paths revisited,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448936789&doi=10.1145%2f1290672.1290683&partnerID=40&md5=bade7948a7a0c7cdeb8e78f1af5dcb75,"The approximability of the maximum edge-disjoint paths problem (EDP) in directed graphs was seemingly settled by an Ω(m 1/2-ε)- hardness result of Guruswami et al. [2003], and an Ω(√m) approximation achievable via a natural multicommodity-flow-based LP relaxation as well as a greedy algorithm. Here m is the number of edges in the graph. We observe that the Ω(m 1/2-ε)-hardness of approximation applies to sparse graphs, and hence when expressed as a function of n, that is, the number of vertices, only an Ω(m 1/2-ε)-hardness follows. On the other hand, O(√m)-approximation algorithms do not guarantee a sublinear (in terms of n) approximation algorithm for dense graphs. We note that a similar gap exists in the known results on the integrality gap of the flow-based LP relaxation: an Ω(√n) lower bound and O(√m) upper bound. Motivated by this discrepancy in the upper and lower bounds, we study algorithms for EDP in directed and undirected graphs and obtain improved approximation ratios. We show that the greedy algorithm has an approximation ratio of O(min(n 2/3, √m)) in undirected graphs and a ratio of O(min(n 4/5, √m)) in directed graphs. For acyclic graphs we give an O(√n ln n) approximation via LP rounding. These are the first sublinear approximation ratios for EDP. The results also extend to EDP with weights and to the uniform-capacity unsplittable flow problem (UCUFP).",Approximation algorithm; Edge-disjoint paths; Greedy algorithm; Multicommodity flow relaxation,Graph theory; Number theory; Problem solving; Edge-disjoint paths; Greedy algorithms; Multicommodity flow relaxation; Uniform capacity unsplittable flow problem (UCUFP); Approximation algorithms
Energy-efficient algorithms for flow time minimization,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448978913&doi=10.1145%2f1290672.1290686&partnerID=40&md5=2b60f7638181c7e65c5fc1637e877433,"We study scheduling problems in battery-operated computing devices, aiming at schedules with low total energy consumption. While most of the previous work has focused on finding feasible schedules in deadline-based settings, in this article we are interested in schedules that guarantee good response times. More specifically, our goal is to schedule a sequence of jobs on a variable-speed processor so as to minimize the total cost consisting of the energy consumption and the total flow time of all jobs. We first show that when the amount of work, for any job, may take an arbitrary value, then no online algorithm can achieve a constant competitive ratio. Therefore, most of the article is concerned with unit-size jobs. We devise a deterministic constant competitive online algorithm and show that the offline problem can be solved in polynomial time.",Competitive analysis; Dynamic programming; Flow time; Offline algorithms; Online algorithms; Variable-speed processor,Algorithms; Dynamic programming; Energy utilization; Problem solving; Scheduling; Flow time; Offline algorithms; Online algorithms; Variable speed processors; Optimization
Optimal parallel selection,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448950585&doi=10.1145%2f1290672.1290675&partnerID=40&md5=d96879b6e6081d0ac6bf6039f43e68f7,We present an optimal parallel selection algorithm on the EREW PRAM. This algorithm runs in O(log n) time with n/log n processors. This complexity matches the known lower bound for parallel selection on the EREW PRAM model. We therefore close this problem which has been open for more than a decade.,EREW PRAM; Parallel algorithms; Selection,Computational complexity; Mathematical models; Problem solving; EREW PRAM; Optimal parallel selection; Parallel algorithms
Finding the k shortest simple paths: A new algorithm and its implementation,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448972779&doi=10.1145%2f1290672.1290682&partnerID=40&md5=4aeea626ceea2ecf9ccd36559a6d8d17,"We describe a newalgorithm to enumerate the k shortest simple (loopless) paths in a directed graph and report on its implementation. Our algorithm is based on a replacement paths algorithm proposed by Hershberger and Suri [2001], and can yield a factor Θ(n) improvement for this problem. But there is a caveat: The fast replacement paths subroutine is known to fail for some directed graphs. However, the failure is easily detected, and so our k shortest paths algorithm optimistically uses the fast subroutine, then switches to a slower but correct algorithm if a failure is detected. Thus, the algorithm achieves its Θ(n) speed advantage only when the optimism is justified. Our empirical results show that the replacement paths failure is a rare phenomenon, and the new algorithm outperforms the current best algorithms; the improvement can be substantial in large graphs. For instance, on GIS map data with about 5,000 nodes and 12,000 edges, our algorithm is 4-8 times faster. In synthetic graphs modeling wireless ad hoc networks, our algorithm is about 20 times faster.",Directed paths; Loop-free paths; Path equivalence class; Replacement paths,Ad hoc networks; Geographic information systems; Graph theory; Problem solving; Directed paths; Loop-free paths; Path equivalence class; Replacement paths; Algorithms
Approximation algorithms and hardness results for cycle packing problems,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36549060611&doi=10.1145%2f1290672.1290685&partnerID=40&md5=b73f78a1629cfd360b1ab08ba85088c9,"The cycle packing number v e(G) of a graph G is the maximum number of pairwise edgedisjointcycles in G. Computing v e(G) is an NP-hard problem.We present approximation algorithms for computing v e(G) in both undirected and directed graphs. In the undirected case we analyze a variant of the modified greedy algorithm suggested by Caprara et al. [2003] and showthat it has approximation ratio Θ(√log n), where n = |V(G)|. This improves upon the previous O(log n) upper bound for the approximation ratio of this algorithm. In the directed case we present √n-approximation algorithm. Finally, we give an O(n 2/3)- approximation algorithm for the problem of finding a maximum number of edge-disjoint cycles that intersect a specified subset S of vertices.We also study generalizations of these problems. Our approximation ratios are the currently best-known ones and, in addition, provide upper bounds on the integrality gap of standard LP-relaxations of these problems. In addition, we give ower bounds for the integrality gap and approximability of v e(G) in directed graphs. Specifically, we prove a lower bound of Ω( log n log log n) for the integrality gap of edge-disjoint cycle packing. We also show that it is quasi-NP-hard to approximate v e(G) within a factor of O(log 1-ε n) for any constant ε > 0. This improves upon the previously known APX-hardness result for this problem.",Approximation algorithms; Cycle packing; Edge-disjoint; Hardness of approximation; Integrality gap,Computational complexity; Graphic methods; Problem solving; Approximation ratio; Cycle packing; Edge disjoint; Integrality gap; Approximation algorithms
Skip graphs,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448982165&doi=10.1145%2f1290672.1290674&partnerID=40&md5=fd724d67eac0ed0e417eba2a67b02855,"Skip graphs are a novel distributed data structure, based on skip lists, that provide the full functionality of a balanced tree in a distributed system where resources are stored in separate nodes that may fail at any time. They are designed for use in searching peer-to-peer systems, and by providing the ability to perform queries based on key ordering, they improve on existing search tools that provide only hash table functionality. Unlike skip lists or other tree data structures, skip graphs are highly resilient, tolerating a large fraction of failed nodes without losing connectivity. In addition, simple and straightforward algorithms can be used to construct a skip graph, insert new nodes into it, search it, and detect and repair errors within it introduced due to node failures.",Overlay networks; Peer-to-peer; Skip lists,Algorithms; Data structures; Distributed database systems; Error analysis; Overlay networks; Peer-to-peer; Skip lists; Graph theory
Algorithms for power savings,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448985715&doi=10.1145%2f1290672.1290678&partnerID=40&md5=d519430d42ec368e4370ed90d2cdfcf2,"This article examines two different mechanisms for saving power in battery-operated embedded systems. The first strategy is that the system can be placed in a sleep state if it is idle. However, a fixed amount of energy is required to bring the system back into an active state in which it can resume work. The second way in which power savings can be achieved is by varying the speed at which jobs are run. We utilize a power consumption curve P(s) which indicates the power consumption level given a particular speed. We assume that P(s) is convex, nondecreasing, and nonnegative for s 0. The problem is to schedule arriving jobs in a way that minimizes total energy use and so that each job is completed after its release time and before its deadline. We assume that all jobs can be preempted and resumed at no cost. Although each problem has been considered separately, this is the first theoretical analysis of systems that can use both mechanisms. We give an offline algorithm that is within a factor of 2 of the optimal algorithm. We also give an online algorithm with a constant competitive ratio.",Dynamic speed scaling; Online algorithms; Power savings,Algorithms; Embedded systems; Energy utilization; Optimization; Problem solving; Dynamic speed scaling; Online algorithms; Power savings; Energy conservation
Guessing secrets efficiently via list decoding,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448962041&doi=10.1145%2f1290672.1290679&partnerID=40&md5=a119e6058d84e435ebc56d40107b341c,"We consider the guessing secrets problem defined by Chung et al. [2001]. This is a variant of the standard 20 questions game where the player has a set of k > 1 secrets from a universe of N possible secrets. The player is asked Boolean questions about the secret. For each question, the player picks one of the k secrets adversarially, and answers according to this secret. We present an explicit set of O(log N) questions together with an efficient (i.e., poly(log N) time) algorithm to solve the guessing secrets problem for the case of 2 secrets. This answers the main algorithmic question left unanswered by Chung et al. [2001]. The main techniques we use are small ε-biased spaces and the notion of list decoding.",20 questions; Biased spaces; Decoding algorithms; Error-correcting codes; K-universal sets,Algorithms; Error correction; Formal logic; Problem solving; Biased spaces; Decoding algorithms; Error-correcting codes; K-universal sets; Decoding
Packing element-disjoint steiner trees,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448941522&doi=10.1145%2f1290672.1290684&partnerID=40&md5=e50aa788d3770df8ec62063f97543e73,"Given an undirected graph G(V, E) with terminal set T V, the problem of packing element-disjoint Steiner trees is to find the maximum number of Steiner trees that are disjoint on the nonterminal nodes and on the edges. The problem is known to be NP-hard to approximate within a factor of (log n), where n denotes V. We present a randomized O(log n)-approximation algorithm for this problem, thus matching the hardness lower bound. Moreover, we show a tight upper bound of O(log n) on the integrality ratio of a natural linear programming relaxation.",Approximation algorithms; Element-disjoint; Hardness of approximation; Packing; Steiner trees,Approximation algorithms; Integration; Linear programming; Number theory; Problem solving; Element disjoint; Steiner trees; Trees (mathematics)
Improved online algorithms for buffer management in QoS switches,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448966129&doi=10.1145%2f1290672.1290687&partnerID=40&md5=f7440adf4a023eb48804054ba330d749,"We consider the following buffer management problem arising in QoS networks: Packets with specified weights and deadlines arrive at a network switch and need to be forwarded so that the total weight of forwarded packets is maximized. Packets not forwarded before their deadlines are lost. The main result of the article is an online 64/33 ≈ 1.939-competitive algorithm, the first deterministic algorithm for this problem with competitive ratio below 2. For the 2-uniform case we give an algorithm with ratio ≈ 1.377 and a matching lower bound.",Online algorithms; Scheduling,Algorithms; Online systems; Packet networks; Scheduling; Switching systems; Buffer management; Network switch; Online algorithms; Quality of service
Minimizing weighted flow time,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448942971&doi=10.1145%2f1290672.1290676&partnerID=40&md5=c38b524cabb3f427e01b659d8188faba,"We consider the problem of minimizing the total weighted flow time on a single machine with preemptions. We give an online algorithm that is O(k)-competitive for k weight classes. This implies an O(log W)-competitive algorithm, where W is the maximum to minimum ratio of weights. This algorithm also implies an O(log n + log P)-approximation ratio for the problem, where P is the ratio of the maximum to minimum job size and n is the number of jobs. We also consider the nonclairvoyant setting where the size of a job is unknown upon its arrival and becomes known to the scheduler only when the job meets its service requirement. We consider the resource augmentation model, and give a (1 + ε)-speed, (1 +1/ε)-competitive online algorithm.",Nonclairvoyant scheduling; Online algorithms; Response time; Scheduling,Algorithms; Approximation theory; Job analysis; Problem solving; Scheduling; Nonclairvoyant scheduling; Online algorithms; Response time; Optimization
The k-traveling repairmen problem,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448949494&doi=10.1145%2f1290672.1290677&partnerID=40&md5=e97f6db7a4d62b6b0becc549d4f276a4,"We consider the k-traveling repairmen problem, also known as the minimum latency problem, to multiple repairmen. We give a polynomial-time 8.497αapproximation algorithm for this generalization, where denotes the best achievable approximation factor for the problem of finding the least-cost rooted tree spanning i vertices of a metric. For the latter problem, a (2+ε)- approximation is known. Our results can be compared with the best-known approximation algorithm using similar techniques for the case k = 1, which is 3.59α. Moreover, recent work of Chaudry et al. [2003] shows how to remove the factor of α, thus improving all of these results by that factor. We are aware of no previous work on the approximability of the present problem. In addition, we give a simple proof of the 3.59α-approximation result that can be more easily extended to the case of multiple repairmen, and may be of independent interest.",Traveling salesman; Vehicle routing,Approximation algorithms; Polynomials; Problem solving; Trees (mathematics); Vehicle routing; Approximability; Approximation algorithm; Least-cost rooted tree spanning; Traveling salesman problem
Windows scheduling as a restricted version of bin packing,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548251827&doi=10.1145%2f1273340.1273344&partnerID=40&md5=e1b1db90ff0cfe9e9419c6638795b6e2,"Given is a sequence of n positive integers w1,w 2,wn that are associated with the items 1,2,n, respectively. In the windows scheduling problem, the goal is to schedule all the items (equal-length information pages) on broadcasting channels such that the gap between two consecutive appearances of page i on any of the channels is at most wi slots (a slot is the transmission time of one page). In the unit-fractions bin packing problem, the goal is to pack all the items in bins of unit size where the size (width) of item i is 1/wi. The optimization objective is to minimize the number of channels or bins. In the offline setting, the sequence is known in advance, whereas in the online setting, the items arrive in order and assignment decisions are irrevocable. Since a page requires at least 1/wi of a channel's bandwidth, it follows that windows scheduling without migration (i.e., all broadcasts of a page must be from the same channel) is a restricted version of unit-fractions bin packing. Let H = ⌈∑i==1n(1/wi) be the bandwidth lower bound on the required number of bins (channels). The best-known offline algorithm for the windows scheduling problem used H + O(ln H) channels. This article presents an offline algorithm for the unit-fractions bin packing problem with at most H + 1 bins. In the online setting, this article presents algorithms for both problems with H + O(H) channels or bins, where the one for the unit-fractions bin packing problem is simpler. On the other hand, this article shows that already for the unit-fractions bin packing problem, any online algorithm must use at least H+Ω(ln H) bins. For instances in which the window sizes form a divisible sequence, an optimal online algorithm is presented. Finally, this article includes a new NP-hardness proof for the windows scheduling problem. © 2007 ACM.",Approximation algorithms; Bin-packing; Online algorithms; Periodic scheduling,Approximation algorithms; Bandwidth; Channel state information; Online systems; Problem solving; Scheduling; Bin-packing; Online algorithms; Periodic scheduling; Integer programming
Constructing pairwise disjoint paths with few links,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548258160&doi=10.1145%2f1273340.1273342&partnerID=40&md5=6ee7caf3a1266c822bd059272e252d6f,"Let P be a simple polygon and let {(u1, u′1), (u2, u′2),(um, u′m)} be a set of m pairs of distinct vertices of P, where for every distinct i, j m, there exist pairwise disjoint (nonintersecting) paths connecting ui to u′i and uj to u′j. We wish to construct m pairwise disjoint paths in the interior of P connecting u i to u′i for i = 1, ,m, with a minimal total number of line segments. We give an approximation algorithm that constructs such a set of paths using O(M) line segments in O(n log m + M log m) time, where M is the number of line segments in the optimal solution and n is the size of the polygon. © 2007 ACM.",Isomorphic triangulations; Link paths; Noncrossing; Polygon,Approximation algorithms; Number theory; Optimal systems; Problem solving; Telecommunication links; Isomorphic triangulations; Link paths; Noncrossing; Polygons; Motion planning
Improved approximation results for the stable marriage problem,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548286312&doi=10.1145%2f1273340.1273346&partnerID=40&md5=b61266c5aeb95a47505a51910f578db6,"The stable marriage problem has recently been studied in its general setting, where both ties and incomplete lists are allowed. It is NP-hard to find a stable matching of maximum size, while any stable matching is a maximal matching and thus trivially we can obtain a 2-approximation algorithm. In this article, we give the first nontrivial result for approximation of factor less than two. Our algorithm achieves an approximation ratio of 2/(1 + L -2) for instances in which only men have ties of length at most L. When both men and women are allowed to have ties but the lengths are limited to two, then we show a ratio of 13/7(<1.858). We also improve the lower bound on the approximation ratio to 21/19(>1.1052). © 2007 ACM.",Approximation algorithms; Incomplete lists; Stable marriage problem; Ties,Branch and bound method; Computational complexity; Numerical methods; Pattern matching; Problem solving; Approximation ratio; Incomplete lists; Stable marriage problem; Approximation algorithms
Nearest-neighbor-preserving embeddings,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548291868&doi=10.1145%2f1273340.1273347&partnerID=40&md5=fee8f5d7a2db8d04a20c970874cb060a,In this article we introduce the notion of nearest-neighbor-preserving embeddings. These are randomized embeddings between two metric spaces which preserve the (approximate) nearest-neighbors. We give two examples of such embeddings for Euclidean metrics with low intrinsic dimension. Combining the embeddings with known data structures yields the best-known approximate nearest-neighbor data structures for such metrics. © 2007 ACM.,Dimensionality reduction; Doubling spaces; Embeddings; Nearest neighbor,Approximation theory; Data structures; Metric system; Problem solving; Random processes; Dimensionality reduction; Doubling spaces; Nearest neighbors; Embedded systems
Novel architectures for P2P applications: The continuous-discrete approach,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548290258&doi=10.1145%2f1273340.1273350&partnerID=40&md5=d4db81d30da9ad9f9fcb3bbe83554005,"We propose a new approach for constructing P2P networks based on a dynamic decomposition of a continuous space into cells corresponding to servers. We demonstrate the power of this approach by suggesting two new P2P architectures and various algorithms for them. The first serves as a DHT (distributed hash table) and the other is a dynamic expander network. The DHT network, which we call Distance Halving, allows logarithmic routing and load while preserving constant degrees. It offers an optimal tradeoff between degree and path length in the sense that degree d guarantees a path length of O(logd n). Another advantage over previous constructions is its relative simplicity. A major new contribution of this construction is a dynamic caching technique that maintains low load and storage, even under the occurrence of hot spots. Our second construction builds a network that is guaranteed to be an expander. The resulting topologies are simple to maintain and implement. Their simplicity makes it easy to modify and add protocols. A small variation yields a DHT which is robust against random Byzantine faults. Finally we show that, using our approach, it is possible to construct any family of constant degree graphs in a dynamic environment, though with worse parameters. Therefore, we expect that more distributed data structures could be designed and implemented in a dynamic environment. © 2007 ACM.",Peer-to-peer networks; Routing,Buffer storage; Continuous time systems; Distributed computer systems; Fault tolerant computer systems; Routing protocols; Distributed hash table (DHT); Dynamic caching; Peer-to-peer networks; Network architecture
Multicommodity demand flow in a tree and packing integer programs,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548230391&doi=10.1145%2f1273340.1273343&partnerID=40&md5=dc5040375e1e77e3b36528b592e2767d,"We consider requests for capacity in a given tree network T = (V, E) where each edge e of the tree has some integer capacity ue. Each request f is a node pair with an integer demand df and a profit wf which is obtained if the request is satisfied. The objective is to find a set of demands that can be feasibly routed in the tree and which provides a maximum profit. This generalizes well-known problems, including the knapsack and b-matching problems. When all demands are 1, we have the integer multicommodity flow problem. Garg et al. [1997] had shown that this problem is NP-hard and gave a 2-approximation algorithm for the cardinality case (all profits are 1) via a primal-dual algorithm. Our main result establishes that the integrality gap of the natural linear programming relaxation is at most 4 for the case of arbitrary profits. Our proof is based on coloring paths on trees and this has other applications for wavelength assignment in optical network routing. We then consider the problem with arbitrary demands. When the maximum demand dmax is at most the minimum edge capacity umin, we show that the integrality gap of the LP is at most 48. This result is obtained by showing that the integrality gap for the demand version of such a problem is at most 11.542 times that for the unit-demand case. We use techniques of Kolliopoulos and Stein [2004, 2001] to obtain this. We also obtain, via this method, improved algorithms for line and ring networks. Applications and connections to other combinatorial problems are discussed. © 2007 ACM.",Approximation algorithm; Integer multicommodity flow; Integrality gap; Packing integer program; Tree,Approximation algorithms; Decision trees; Optimization; Pattern matching; Problem solving; Integer multicommodity flows; Integrality gaps; Packing integer programs; Primal-dual algorithms; Integer programming
Convergence time to Nash equilibrium in load balancing,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548247126&doi=10.1145%2f1273340.1273348&partnerID=40&md5=79be6d2d61a504d8e01fbf67d7517d50,"We study the number of steps required to reach a pure Nash equilibrium in a load balancing scenario where each job behaves selfishly and attempts to migrate to a machine which will minimize its cost. We consider a variety of load balancing models, including identical, restricted, related, and unrelated machines. Our results have a crucial dependence on the weights assigned to jobs. We consider arbitrary weights, integer weights, k distinct weights, and identical (unit) weights. We look both at an arbitrary schedule (where the only restriction is that a job migrates to a machine which lowers its cost) and specific efficient schedulers (e.g., allowing the largest weight job to move first). A by-product of our results is establishing a connection between various scheduling models and the game-theoretic notion of potential games. We show that load balancing in unrelated machines is a generalized ordinal potential game, load balancing in related machines is a weighted potential game, and load balancing in related machines and unit weight jobs is an exact potential game. © 2007 ACM.",Convergence time; Game theory; Nash equilibrium,Automata theory; Convergence of numerical methods; Problem solving; Resource allocation; Scheduling; Convergence time; Nash equilibrium; Unrelated machines; Game theory
Approximate parameterized matching,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548270387&doi=10.1145%2f1273340.1273345&partnerID=40&md5=408fadd60d5e8846d474c65942e6d42d,"Two equal length strings s and s′, over alphabets s and s′, parameterize match if there exists a bijection π : s s′ such that π (s) = s′, where π (s) is the renaming of each character of s via π. Parameterized matching is the problem of finding all parameterized matches of a pattern string p in a text t, and approximate parameterized matching is the problem of finding at each location a bijection π that maximizes the number of characters that are mapped from p to the appropriate p-length substring of t. Parameterized matching was introduced as a model for software duplication detection in software maintenance systems and also has applications in image processing and computational biology. For example, approximate parameterized matching models image searching with variable color maps in the presence of errors. We consider the problem for which an error threshold, k, is given, and the goal is to find all locations in t for which there exists a bijection π which maps p into the appropriate p-length substring of t with at most k mismatched mapped elements. Our main result is an algorithm for this problem with O(nk1.5 + mk log m) time complexity, where m = p and n=t. We also show that when p = t = m, the problem is equivalent to the maximum matching problem on graphs, yielding a O(m + k1.5) solution. © 2007 ACM.",Hamming distance; Maximum matching; Mismatch pair; Parameterize match,Approximation theory; Character recognition; Computer software maintenance; Hamming distance; Parameterization; Problem solving; Maximum matching; Mismatch pairs; Parameterize matches; Pattern matching
Faster algorithms for sorting by transpositions and sorting by block interchanges,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548256218&doi=10.1145%2f1273340.1273341&partnerID=40&md5=9d75f956d44388f812c1391cec48d37a,"In this article, we present a new data structure, called the permutation tree, to improve the running time of sorting permutation by transpositions and sorting permutation by block interchanges. The existing 1.5-approximation algorithm for sorting permutation by transpositions has time complexity O(n 3/2 logn). By means of the permutation tree, we can improve this algorithm to achieve time complexity O(nlogn). We can also improve the algorithm for sorting permutation by block interchanges to take its time complexity from O(n2) down to O(nlogn). © 2007 ACM.",Block interchange; Genome; Permutation; Time complexity; Transposition; Tree,Approximation algorithms; Computational complexity; Decision trees; Sorting; Block interchange; Sorting permutations; Time complexity; Data structures
Problems column,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548282954&doi=10.1145%2f1273340.1273351&partnerID=40&md5=0480bfac6c54cbee886b117b795191b1,"The stable marriage problem, displayed at the Symposium on Discrete Algorithms (SODA), which was held in New Orleans in January, 2007, is presented. The problem involves different sets of men and women, providing a rank-ordered preference list of members of the opposite sex. The aim of the problem is to find a matching, which eliminates blocking pairs. The paper presented by Iwama presents a generalization of the basic problem, where the preference list may have ties and include members of the opposite sex. The model says that finding a stable marriage without blocking pairs with the largest cardinality is NP-Complete. It was also observed that a local improvement-based approach offers an approximation factor of 1.8. The case, which presents G as a bipartite graph is known as Open shop Scheduling.",,Computational complexity; Discrete time control systems; Graph theory; Pattern matching; Problem solving; Approximation factors; Rank-ordered preference; Symposium on Discrete Algorithms (SODA); Approximation algorithms
Routing and scheduling in multihop wireless networks with time-varying channels,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548253854&doi=10.1145%2f1273340.1273349&partnerID=40&md5=9061aecad528b694a72137eef07734d1,"We study routing and scheduling in multihop wireless networks. When data is transmitted from its source node to its destination node it may go through other wireless nodes as intermediate hops. The data transmission is node constrained, that is, every node can transmit data to at most one neighboring node per time step. The transmission rates are time varying as a result of changing wireless channel conditions. In this article, we assume that data arrivals and transmission rates are governed by an adversary. The power of the adversary is limited by an admissibility condition which forbids the adversary from overloading any wireless node a priori. The node-constrained transmission and time-varying nature of the transmission rates make our model different from and harder than the standard adversarial queueing model which relates to wireline networks. For the case in which the adversary specifies the paths that the data must follow, we design scheduling algorithms that ensure network stability. These algorithms try to give priority to the data that is closest to its source node. However, at each time step only a subset of the data queued at a node is eligible for scheduling. One of our algorithms is fully distributed. For the case in which the adversary does not dictate the data paths, we show how to route data so that the admissibility condition is satisfied. We can then schedule data along the chosen paths using our stable scheduling algorithms. © 2007 ACM.",Routing; Scheduling; Stability; Time-varying; Wireless network,Channel capacity; Data communication systems; Routing algorithms; Scheduling; Stability criteria; Time varying networks; Data transmission rates; Intermediate hops; Multihop wireless networks; Wireless channels; Wireless networks
Strongly stable matchings in time O(nm) and extension to the hospitals-residents problem,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250190622&doi=10.1145%2f1240233.1240238&partnerID=40&md5=eaa1a91ea27fd007a5c0eb4dd3ce0e48,"An instance of the stable marriage problem is an undirected bipartite graph G = (X W, E) with linearly ordered adjacency lists with ties allowed in the ordering. A matching M is a set of edges, no two of which share an endpoint. An edge e = (a, b) E M is a blocking edge for M if a is either unmatched or strictly prefers b to its partner in M, and b is unmatched, strictly prefers a to its partner in M, or is indifferent between them. A matching is strongly stable if there is no blocking edge with respect to it. We give an O(nm) algorithm for computing strongly stable matchings, where n is the number of vertices and m the number of edges. The previous best algorithm had running time O(m2). We also study this problem in the hospitals-residents setting, which is a many-to-one extension of the aforementioned problem. We give an O(m ∑hHph) algorithm for computing a strongly stable matching in the hospitals-residents problem, where ph is the quota of a hospital h. The previous best algorithm had running time O(m 2). © 2007 ACM.",Bipartite matching; Level maximal; Stable marriage; Strong stability; Ties,Algorithms; Graph theory; Problem solving; Bipartite graphs; Bipartite matching; Blocking edges; Strong stability; Pattern matching
Retroactive data structures,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250156680&doi=10.1145%2f1240233.1240236&partnerID=40&md5=217219f6cf4b8b1f4a517bb8f15cc23c,"We introduce a new data structuring paradigm in which operations can be performed on a data structure not only in the present, but also in the past. In this new paradigm, called retroactive data structures, the historical sequence of operations performed on the data structure is not fixed. The data structure allows arbitrary insertion and deletion of operations at arbitrary times, subject only to consistency requirements. We initiate the study of retroactive data structures by formally defining the model and its variants. We prove that, unlike persistence, efficient retroactivity is not always achievable. Thus, we present efficient retroactive data structures for queues, doubly ended queues, priority queues, union-find, and decomposable search structures. © 2007 ACM.",History; Persistence; Point location; Rollback; Time travel,Algorithms; Online searching; Queueing networks; Point locations; Retroactive data structures; Rollbacks; Time travel; Data structures
The relative worst order ratio for online algorithms,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250191795&doi=10.1145%2f1240233.1240245&partnerID=40&md5=f021e9731a54eee1c4d3cc618e97ce15,"We define a new measure for the quality of online algorithms, the relative worst order ratio, using ideas from the max/max ratio [Ben-David and Borodin 1994] and from the random order ratio [Kenyon 1996]. The new ratio is used to compare online algorithms directly by taking the ratio of their performances on their respective worst permutations of a worst-case sequence. Two variants of the bin packing problem are considered: the classical bin packing problem, where the goal is to fit all items in as few bins as possible, and the dual bin packing problem, which is the problem of maximizing the number of items packed in a fixed number of bins. Several known algorithms are compared using this new measure, and a new, simple variant of first-fit is proposed for dual bin packing. Many of our results are consistent with those previously obtained with the competitive ratio or the competitive ratio on accommodating sequences, but new separations and easier proofs are found. © 2007 ACM.",Bin packing; Dual bin packing; Online; Quality measure; Relative worst order ratio,Binary sequences; Computational methods; Optimization; Quality control; Bin packing; Online algorithms; Permutations; Algorithms
Sharing the cost more efficiently: Improved approximation for multicommodity rent-or-buy,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250213841&doi=10.1145%2f1240233.1240246&partnerID=40&md5=fad4cdc1b30f716cd90392938de1dd5d,"In the multicommodity rent-or-buy (MROB) network design problems, we are given a network together with a set of k terminal pairs (s1, t1), . . . , (sk , tk ). The goal is to provision the network so that a given amount of flow can be shipped between si and ti for all 1 = i = k simultaneously. In order to provision the network, one can either rent capacity on edges at some cost per unit of flow, or buy them at some larger fixed cost. Bought edges have no incremental, flow-dependent cost. The overall objective is to minimize the total provisioning cost. Recently, Gupta et al. [2003a] presented a 12-approximation for the MROB problem. Their algroithm chooses a subset of the terminal pairs in the graph at random and then buys the edges of an approximate Steiner forest for these pairs. This technique had previously been introduced [Gupta et al. 2003b] for the single-sink rent-or-buy network design problem. In this article we give a 6.828-approximation for the MROB problem by refining the algorithm of Gupta et al. and simplifying their analysis. The improvement in our article is based on a more careful adaptation and simplified analysis of the primal-dual algorithm for the Steiner forest problem due to Agrawal et al. [1995]. Our result significantly reduces the gap between the single-sink and multisink case. © 2007 ACM.",Approximation algorithms; Cost sharing; Network design; Steiner forests,Cost accounting; Graphic methods; Logic design; Network architecture; Optimization; Trees (mathematics); Cost sharing; Network design; Steiner forests; Approximation algorithms
Phase changes in random point quadtrees,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250222447&doi=10.1145%2f1240233.1240235&partnerID=40&md5=9af7e0acf6c748ddc1a23b2f4513c0e6,"We show that a wide class of linear cost measures (such as the number of leaves) in random d-dimensional point quadtrees undergo a change in limit laws: If the dimension d = 1, , 8, then the limit law is normal; if d 9 then there is no convergence to a fixed limit law. Stronger approximation results such as convergence rates and local limit theorems are also derived for the number of leaves, additional phase changes being unveiled. Our approach is new and very general, and also applicable to other classes of search trees. A brief discussion of Devroye's grid trees (covering m-ary search trees and quadtrees as special cases) is given. We also propose an efficient numerical procedure for computing the constants involved to high precision. © 2007 ACM.",Analysis in distribution of algorithms; Asymptotic transfer; Central limit theorems; Depth; Differential equations; Grid trees; Local limit theorems; Mellin transforms; Page usage; Phase transitions; Quadtrees; Total path length,Approximation theory; Differential equations; Grid computing; Numerical methods; Theorem proving; Asymptotic transfer; Central limit theorems; Grid trees; Local limit theorems; Mellin transforms; Total path length; Trees (mathematics)
Compressed representations of sequences and full-text indexes,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250171723&doi=10.1145%2f1240233.1240243&partnerID=40&md5=4f0f4ccd34836a39e16a0a43f19efa52,"Given a sequence S = s1s2 . . . sn of integers smaller than r = O(polylog(n)), we show how S can be represented using nH0(S) + o(n) bits, so that we can know any sq , as well as answer rank and select queries on S, in constant time. H0(S) is the zero-order empirical entropy of S and nH0(S) provides an information-theoretic lower bound to the bit storage of any sequence S via a fixed encoding of its symbols. This extends previous results on binary sequences, and improves previous results on general sequences where those queries are answered in O(log r) time. For larger r , we can still represent S in nH0(S) + o(n log r) bits and answer queries in O(log r/ log log n) time. Another contribution of this article is to show how to combine our compressed representation of integer sequences with a compression boosting technique to design compressed full-text indexes that scale well with the size of the input alphabet. Specifically, we design a variant of the FM-index that indexes a string T [1, n] within nHk (T)+o(n) bits of storage, where Hk (T) is the kth-order empirical entropy of T . This space bound holds simultaneously for all k = a log n, constant 0 < a < 1, and = O(polylog(n)). This index counts the occurrences of an arbitrary pattern P[1, p] as a substring of T in O(p) time; it locates each pattern occurrence in O(log1+e n) time for any constant 0 < e < 1; and reports a text substring of length in O(+ log1+e n) time. Compared to all previous works, our index is the first that removes the alphabet-size dependance from all query times, in particular, counting time is linear in the pattern length. Still, our index uses essentially the same space of the kth-order entropy of the text T, which is the best space obtained in previous work. We can also handle larger alphabets of size = O(n), for any 0 < β < 1, by paying o(n log) extra space and multiplying all query times by O(log/log log n). © 2007 ACM.",Burrows-Wheeler transform; Compression boosting; Entropy; Rank and select; Text compression; Text indexing; Wavelet tree,Binary sequences; Computational methods; Indexing (of information); Information management; Mathematical transformations; Query processing; Text processing; Trees (mathematics); Burrows Wheeler transform; Counting time; Integers; Text compression; Text indexes; Wavelet tree; Data compression
The NP-completeness column: Finding needles in haystacks,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250201081&doi=10.1145%2f1240233.1240247&partnerID=40&md5=bc72db364ff24fe4d78ee6a076a02db5,"This is the 26th edition of a column that covers new developments in the theory of NPcompleteness. The presentation is modeled on that which M. R. Garey and I used in our book ""Computers and Intractability: A Guide to the Theory of NP-Completeness,"" W. H. Freeman & Co., New York, 1979, hereinafter referred to as ""[G&J]."" Previous columns, the first 23 of which appeared in J. Algorithms, will be referred to by a combination of their sequence number and year of appearance, e.g., ""Column 1 [1981]."" Full bibliographic details on the previous columns, as well as downloadable unofficial versions of them, can be found at http://www.research.att.com/~dsj/ columns/. This column discusses the question of whether finding an object can be computationally difficult even when we know that the object exists. © 2007 ACM.",Fixed point; Game theory; Local search; Nash equilibrium; PLS; PPAD,Algorithms; Computational methods; Digital arithmetic; Game theory; Online searching; Nash equilibrium; Computational complexity
A simple entropy-based algorithm for planar point location,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250207348&doi=10.1145%2f1240233.1240240&partnerID=40&md5=0b313975fee02ba0ceb506b56e596173,"Given a planar polygonal subdivision S, point location involves preprocessing this subdivision into a data structure so that given any query point q, the cell of the subdivision containing q can be determined efficiently. Suppose that for each cell z in the subdivision, the probability pz that a query point lies within this cell is also given. The goal is to design the data structure to minimize the average search time. This problem has been considered before, but existing data structures are all quite complicated. It has long been known that the entropy H of the probability distribution is the dominant term in the lower bound on the average-case search time. In this article, we show that a very simple modification of a well-known randomized incremental algorithm can be applied to produce a data structure of expected linear size that can answer point-location queries in O(H) average time. We also present empirical evidence for the practical efficiency of this approach. © 2007 ACM.",Entropy; Expected-case complexity; Point location; Polygonal subdivision; Randomized algorithms; Trapezoidal maps,Computational complexity; Data structures; Optimization; Probability distributions; Query processing; Planar point location; Point location queries; Polygonal subdivision; Randomized incremental algorithms; Trapezoidal maps; Algorithms
Deterministic sampling and range counting in geometric data streams,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250185027&doi=10.1145%2f1240233.1240239&partnerID=40&md5=53f237491e4981c842e6eab463f40a20,"We present memory-efficient deterministic algorithms for constructing ε-nets and ε-approximations of streams of geometric data. Unlike probabilistic approaches, these deterministic samples provide guaranteed bounds on their approximation factors. We show how our deterministic samples can be used to answer approximate online iceberg geometric queries on data streams. We use these techniques to approximate several robust statistics of geometric data streams, including Tukey depth, simplicial depth, regression depth, the Thiel-Sen estimator, and the least median of squares. Our algorithms use only a polylogarithmic amount of memory, provided the desired approximation factors are at least inverse-polylogarithmic. We also include a lower bound for noniceberg geometric queries. © 2007 ACM.",Data streams; Epsilon nets; Geometric data; Iceberg queries; Range counting; Robust statistics; Sampling; Streaming algorithms,Approximation theory; Data structures; Geometry; Online systems; Query processing; Statistics; Deterministic sampling; Epsilon nets; Iceberg queries; Range counting; Streaming algorithms; Algorithms
An algorithm for deciding zero equivalence of nested polynomially recurrent sequences,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250195747&doi=10.1145%2f1240233.1240241&partnerID=40&md5=d875303d754398dd1fb3ec87ff2ba631,"We introduce the class of nested polynomially recurrent sequences which includes a large number of sequences that are of combinatorial interest. We present an algorithm for deciding zero equivalence of these sequences, thereby providing a new algorithm for proving identities among combinatorial sequences: In order to prove an identity, decide by the algorithm whether the difference of lefthand-side and righthand-side is identically zero. This algorithm is able to treat mathematical objects which are not covered by any other known symbolic method for proving combinatorial identities. Despite its theoretical flavor and high complexity, an implementation of the algorithm can be successfully applied to nontrivial examples. © 2007 ACM.",Combinatorial sequences; Nested polynomially recurrent sequences; Symbolic computation; Zero equivalence,Combinatorial mathematics; Computational complexity; Computational methods; Combinatorial sequences; Nested polynomially recurrent sequences; Symbolic computation; Zero equivalence; Algorithms
Improved algorithms for weakly chordal graphs,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250196129&doi=10.1145%2f1240233.1240237&partnerID=40&md5=1e795cb9d899aab7104b6693b5efeccf,"We use a new structural theorem on the presence of two-pairs in weakly chordal graphs to develop improved algorithms. For the recognition problem, we reduce the time complexity from O(mn2) to O(m2) and the space complexity from O(n3) to O(m + n), and also produce a hole or antihole if the input graph is not weakly chordal. For the optimization problems, the complexity of the clique and coloring problems is reduced from O(mn2) to O(n3) and the complexity of the independent set and clique cover problems is improved from O(n4) to O(mn). The space complexity of our optimization algorithms is O(m + n). © 2007 ACM.",Coloring; Graph algorithms; Perfect graphs; Recognition; Weakly chordal,Computational complexity; Graph theory; Optimization; Pattern recognition; Theorem proving; Chordal graphs; Graph algorithms; Space complexity; Structural theorems; Algorithms
Dynamic text and static pattern matching,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250180198&doi=10.1145%2f1240233.1240242&partnerID=40&md5=3a6017bfa2d3734ba37388408d41e508,"In this article, we address a new version of dynamic pattern matching. The dynamic text and static pattern matching problem is the problem of finding a static pattern in a text that is continuously being updated. The goal is to report all new occurrences of the pattern in the text after each text update. We present an algorithm for solving the problem where the text update operation is changing the symbol value of a text location. Given a text of length n and a pattern of length m, our algorithm preprocesses the text in time O(n log log m), and the pattern in time O(m log m). The extra space used is O(n + m log m). Following each text update, the algorithm deletes all prior occurrences of the pattern that no longer match, and reports all new occurrences of the pattern in the text in O(log log m) time. We note that the complexity is not proportional to the number of pattern occurrences, since all new occurrences can be reported in a succinct form. © 2007 ACM.",Border trees; Dynamic text; Static pattern,Algorithms; Computational complexity; Problem solving; Text processing; Trees (mathematics); Border trees; Dynamic texts; Static patterns; Pattern matching
Multiplierless multiple constant multiplication,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250199345&doi=10.1145%2f1240233.1240234&partnerID=40&md5=7221f9fecc983c3d4c5b8feba33f51ea,"A variable can be multiplied by a given set of fixed-point constants using a multiplier block that consists exclusively of additions, subtractions, and shifts. The generation of a multiplier block from the set of constants is known as the multiple constant multiplication (MCM) problem. Finding the optimal solution, namely, the one with the fewest number of additions and subtractions, is known to be NP-complete. We propose a new algorithm for the MCM problem, which produces solutions that require up to 20% less additions and subtractions than the best previously known algorithm. At the same time our algorithm, in contrast to the closest competing algorithm, is not limited by the constant bitwidths. We present our algorithm using a unifying formal framework for the best, graph-based MCM algorithms and provide a detailed runtime analysis and experimental evaluation. We show that our algorithm can handle problem sizes as large as 100 32-bit constants in a time acceptable for most applications. The implementation of the new algorithm is available at www.spiral.net. © 2007 ACM.",Addition chains; Directed graph; FIR filter; Fixed-point arithmetic; Strength reduction,Computational complexity; FIR filters; Graph theory; Problem solving; Addition chains; Directed graphs; Fixed point arithmetic; Strength reduction; Algorithms
Compressed indexes for dynamic text collections,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250206975&doi=10.1145%2f1240233.1240244&partnerID=40&md5=f3903868b0fbefd5dd49facc912ff48e,"Let T be a string with n characters over an alphabet of constant size. A recent breakthrough on compressed indexing allows us to build an index for T in optimal space (i.e., O(n) bits), while supporting very efficient pattern matching [Ferragina and Manzini 2000; Grossi and Vitter 2000]. Yet the compressed nature of such indexes also makes them difficult to update dynamically. This article extends the work on optimal-space indexing to a dynamic collection of texts. Our first result is a compressed solution to the library management problem, where we show an index of O(n) bits for a text collection L of total length n, which can be updated in O(|T | log n) time when a text T is inserted or deleted from L; also, the index supports searching the occurrences of any pattern P in all texts in L in O(|P| log n + occ log2 n) time, where occ is the number of occurrences. Our second result is a compressed solution to the dictionary matching problem, where we show an index of O(d) bits for a pattern collection D of total length d, which can be updated in O(|P| log2 d) time when a pattern P is inserted or deleted fromD; also, the index supports searching the occurrences of all patterns ofD in any text T in O((|T |+occ) log2 d) time. When compared with the O(d log d)-bit suffix-tree-based solution of Amir et al. [1995], the compact solution increases the query time by roughly a factor of log d only. The solution to the dictionary matching problem is based on a new compressed representation of a suffix tree. Precisely, we give an O(n)-bit representation of a suffix tree for a dynamic collection of texts whose total length is n, which supports insertion and deletion of a text T in O(|T | log2 n) time, as well as all suffix tree traversal operations, including forward and backward suffix links. This work can be regarded as a generalization of the compressed representation of static texts. In the study of the aforementioned result, we also derive the first O(n)-bit representation for maintaining n pairs of balanced parentheses in O(log n/ log log n) time per operation, matching the time complexity of the previous O(n log n)-bit solution. © 2007 ACM.",Compressed suffix tree; String matching,Data compression; Digital libraries; Glossaries; Information management; Pattern matching; Text processing; Compressed suffix tree; Dictionary matching; Library management; Static texts; String matching; Indexing (of information)
Entropy-based bounds for online algorithms,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847337001&doi=10.1145%2f1186810.1186817&partnerID=40&md5=56acfa6752688020007ba6e8387e2a03,"We focus in this work on an aspect of online computation that is not addressed by standard competitive analysis, namely, identifying request sequences for which nontrivial online algorithms are useful versus request sequences for which all algorithms perform equally poorly. The motivations for this work are advanced system and architecture designs which allow the operating system to dynamically allocate resources to online protocols such as prefetching and caching. To utilize these features, the operating system needs to identify data streams that can benefit from more resources.Our approach in this work is based on the relation between entropy, compression, and gambling, extensively studied in information theory. It has been shown that in some settings, entropy can either fully or at least partially characterize the expected outcome of an iterative gambling game. Our goal is to study the extent to which the entropy of the input characterizes the expected performance of online algorithms for problems that arise in computer applications. We study bounds based on entropy for three classical online problems - -list accessing, prefetching, and caching. Our bounds relate the performance of the best online algorithm to the entropy, a parameter intrinsic to characteristics of the request sequence. This is in contrast to the competitive ratio parameter of competitive analysis, which quantifies the performance of the online algorithm with respect to an optimal offline algorithm. For the prefetching problem, we give explicit upper and lower bounds for the performance of the best prefetching algorithm in terms of the entropy of the request sequence. In contrast, we show that the entropy of the request sequence alone does not fully capture the performance of online list accessing and caching algorithms. © 2007 ACM.",Caching; Entropy; List accessing; Online algorithms; Performance bounds; Prefetching; Stochastic process,Cache memory; Game theory; Iterative methods; Network protocols; Online systems; Problem solving; Entropy based bounds; List accessing; Online algorithms; Performance bounds; Prefetching; Algorithms
A data structure for a sequence of string accesses in external memory,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847316545&doi=10.1145%2f1186810.1186816&partnerID=40&md5=752794624e064b1271c417dad601447e,"We introduce a new paradigm for querying strings in external memory, suited to the execution of sequences of operations. Formally, given a dictionary of n strings S1, . . ., Sn, we aim at supporting a search sequence for m not necessarily distinct strings T1, T2, . . ., Tm, as well as inserting and deleting individual strings. The dictionary is stored on disk, where each access to a disk page fetches B items, the cost of an operation is the number of pages accessed (I/Os), and efficiency must be attained on entire sequences of string operations rather than on individual ones. Our approach relies on a novel and conceptually simple self-adjusting data structure (SASL) based on skip lists, that is also interesting per se. The search for the whole sequence T1, T2, . . ., Tm can be done in an expected number of I/Os: O (∑ m j=1 |Tj|/B + ∑ni=1 (ni log B m/ni)), where each Tj may or may not be present in the dictionary, and ni is the number of times Si is queried (i.e., the number of Tj s equal to Si ). Moreover, inserting or deleting a string Si takes an expected amortized number O( |Si |/ B + logB n) of I/Os. The term ∑m j =1 |Tj |/ B in the search formula is a lower bound for reading the input, and the term ∑ni=1 ni logB m/ni (entropy of the query sequence) is a standard information-theoretic lower bound.We regard this result as the static optimality theorem for external-memory string access, as compared to Sleator and Tarjan's classical theorem for numerical dictionaries [Sleator and Tarjan 1985]. Finally,we reformulate the search bound if a cache is available, taking advantage of common prefixes among the strings examined in the search. © 2007 ACM.",Caching; External-memory data structure; Sequence of string searches and updates; Skip list,Buffer storage; Information analysis; Number theory; Query languages; Search engines; Disk pages; External memory data structure; Self adjusting data structure (SASL); Skip lists; Data structures
Tight bounds for worst-case equilibria,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847278463&doi=10.1145%2f1186810.1186814&partnerID=40&md5=9a2d3e5b7ab01aa48b7d9ecd4bdd1f03,"We study the problem of traffic routing in noncooperative networks. In such networks, users may follow selfish strategies to optimize their own performance measure and therefore, their behavior does not have to lead to optimal performance of the entire network. In this article we investigate the worst-case coordination ratio, which is a game-theoretic measure aiming to reflect the price of selfish routing. Following a line of previous work, we focus on the most basic networks consisting of parallel links with linear latency functions. Our main result is that the worst-case coordination ratio on m parallel links of possibly different speeds is logm/log log logm. In fact, we are able to give an exact description of the worst-case coordination ratio, depending on the number of links and ratio of speed of the fastest link over the speed of the slowest link. For example, for the special case in which all m parallel links have the same speed, we can prove that the worst-case coordination ratio is (-1)(m) + (1), with denoting the Gamma (factorial) function. Our bounds entirely resolve an open problem posed recently by Koutsoupias and Papadimitriou [1999]. © 2007 ACM.",Coordination ration; Nash equilibria; Noncooperative networks; Selfish strategies; Traffic routing,Function evaluation; Game theory; Optimization; Problem solving; Telecommunication links; User interfaces; Coordination ration; Nash equilibria; Noncooperative networks; Selfish strategies; Traffic routing; Telecommunication networks
Frugal path mechanisms,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847321633&doi=10.1145%2f1186810.1186813&partnerID=40&md5=2e662f94edc76ddad851af659d84c120,"We consider the problem of selecting a low-cost s - t path in a graph where the edge costs are a secret, known only to the various economic agents who own them. To solve this problem, Nisan and Ronen applied the celebrated Vickrey-Clarke-Groves (VCG) mechanism, which pays a premium to induce the edges so as to reveal their costs truthfully. We observe that this premium can be unacceptably high. There are simple instances where the mechanism pays Θ(n) times the actual cost of the path, even if there is an alternate path available that costs only (1 + ε) times as much. This inspires the frugal path problem, which is to design a mechanism that selects a path and induces truthful cost revelation, without paying such a high premium.This article contributes negative results on the frugal path problem. On two large classes of graphs, including those having three node-disjoint s - t paths, we prove that no reasonable mechanism can always avoid paying a high premium to induce truthtelling. In particular, we introduce a general class of min function mechanisms, and show that all min function mechanisms can be forced to overpay just as badly as VCG. Meanwhile, we prove that every truthful mechanism satisfying some reasonable properties is a min function mechanism.Our results generalize to the problem of hiring a team to complete a task, where the analog of a path in the graph is a subset of the agents constituting a team capable of completing the task. © 2007 ACM.",Dominant strategies; Game theory; Overpayment; Truthful mechanism design; Vickrey-Clarke-Groves mechanism,Computational methods; Costs; Function evaluation; Investments; Problem solving; Dominant strategies; Overpayments; Truthful mechanism designs; Vickrey Clarke Groves mechanism (VCG); Graph theory
Algorithmic aspects of bandwidth trading,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847279451&doi=10.1145%2f1186810.1186820&partnerID=40&md5=4431b1b2129317568d37224f54440649,"We study algorithmic problems that are motivated by bandwidth trading in next-generation networks. Typically, bandwidth trading involves sellers (e.g., network operators) interested in selling bandwidth pipes that offer to buyers a guaranteed level of service for a specified time interval. The buyers (e.g., bandwidth brokers) are looking to procure bandwidth pipes to satisfy the reservation requests of end-users (e.g., Internet subscribers). Depending on what is available in the bandwidth exchange, the goal of a buyer is to either spend the least amount of money so as to satisfy all the reservations made by its customers, or to maximize its revenue from whatever reservations can be satisfied.We model this as a real-time nonpreemptive scheduling problem in which machine types correspond to bandwidth pipes and jobs correspond to end-user reservation requests. Each job specifies a time interval during which it must be processed, and a set of machine types on which it can be executed. If necessary, multiple machines of a given type may be allocated, but each must be paid for. Finally, each job has associated with it a revenue, which is realized if the job is scheduled on some machine.There are two versions of the problem that we consider. In the cost minimization version, the goal is to minimize the total cost incurred for scheduling all jobs, and in the revenue maximization version the goal is to maximize the revenue of the jobs that are scheduled for processing on a given set of machines. We consider several variants of the problems that arise in practical scenarios, and provide constant factor approximations. © 2007 ACM.",Approximation algorithms for NP-hard problems; Dynamic storage allocation; General caching; Resource allocation; Scheduling,Bandwidth; Commerce; Computational complexity; Industrial economics; Optimization; Resource allocation; Scheduling; Approximation algorithms; Dynamic storage allocation; General caching; Real-time scheduling; Algorithms
On the difficulty of some shortest path problems,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847293858&doi=10.1145%2f1186810.1186815&partnerID=40&md5=e59b80aeb2a8308f214897996434b087,"We prove superlinear lower bounds for some shortest path problems in directed graphs, where no such bounds were previously known. The central problem in our study is the replacement paths problem: Given a directed graph G with non-negative edge weights, and a shortest path P = {e1, e 2,..., ep} between two nodes s and t, compute the shortest path distances from s to t in each of the p graphs obtained from G by deleting one of the edges ei. We show that the replacement paths problem requires (m n) time in the worst case whenever m = O(n n). Our construction also implies a similar lower bound on the k shortest simple paths problem for a broad class of algorithms that includes all known algorithms for the problem. To put our lower bound in perspective, we note that both these problems (replacement paths and k shortest simple paths) can be solved in near-linear time for undirected graphs. © 2007 ACM.",K shortest simple paths; Lower bound; Replacement path; Shortest path,Algorithms; Computational methods; Linear systems; Problem solving; Set theory; Lower bounds; Replacement paths; Shortest path; Shortest simple paths; Graph theory
The string edit distance matching problem with moves,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847272670&doi=10.1145%2f1186810.1186812&partnerID=40&md5=772f050a0727d5d9fc16681df435a5b3,"The edit distance between two strings S and R is defined to be the minimum number of character inserts, deletes, and changes needed to convert R to S. Given a text string t of length n, and a pattern string p of length m, informally, the string edit distance matching problem is to compute the smallest edit distance between p and substrings of t.We relax the problem so that: (a) we allow an additional operation, namely, substring moves; and (b) we allow approximation of this string edit distance. Our result is a near-linear time deterministic algorithm to produce a factor of O(log n log* n) approximation to the string edit distance with moves. This is the first known significantly subquadratic algorithm for a string edit distance problem in which the distance involves nontrivial alignments. Our results are obtained by embedding strings into L1 vector space using a simplified parsing technique, which we call edit-sensitive parsing (ESP). © 2007 ACM.",Approximate pattern matching; Data streams; Edit distance; Embedding; Similarity search; String matching,Algorithms; Approximation theory; Automata theory; Embedded systems; Nonlinear systems; Problem solving; Approximate pattern matching; Data streams; Edit distance; Similarity search; String matching; Pattern matching
Querying priced information in databases: The conjunctive case,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847263070&doi=10.1145%2f1186810.1186819&partnerID=40&md5=a7d5ce665bf7e029a94f5df17a3bcd45,"Query optimization that involves expensive predicates has received considerable attention in the database community. Typically, the output to a database query is a set of tuples that satisfy certain conditions, and, with expensive predicates, these conditions may be computationally costly to verify. In the simplest case, when the query looks for the set of tuples that simultaneously satisfy k expensive predicates, the problem reduces to ordering the evaluation of the predicates so as to minimize the time to output the set of tuples comprising the answer to the query. We study different cases of the problem: the sequential case, in which a single processor is available to evaluate the predicates, and the distributed case, in which there are k processors available, each dedicated to a different attribute (column) of the database, and there is no communication cost between the processors.For the sequential case, we give a simple and fast deterministic k-approximation algorithm, and prove that k is the best possible approximation ratio for a deterministic algorithm, even if exponential time algorithms are allowed. We also propose a randomized, polynomial time algorithm with expected approximation ratio 1 + 2/2 ≈ 1.707 for k = 2, and prove that 3/2 is the best possible expected approximation ratio for randomized algorithms. We also show that given 0 ε 1, no randomized algorithm achieves approximation ratio smaller than 1 + ε with probability larger than (1 + ε)/2.For the distributed case, we consider two different models: the preemptive model, in which a processor is allowed to interrupt the evaluation of a predicate, and the nonpreemptive model, in which the evaluation of a predicate must be completed once started. We show that k is the best possible approximation ratio for a deterministic algorithm, even if exponential time algorithms are allowed. For the preemptive model, we introduce a polynomial time k-approximation algorithm. For the nonpreemptive model, we introduce a polynomial time O(k log2 k)-approximation algorithm. © 2007 ACM.",Competitive analysis; Online algorithms,Algorithms; Costs; Database systems; Mathematical models; Optimization; Polynomial approximation; Query languages; Competitive analysis; Computational costs; Online algorithms; Querying priced information; Information retrieval systems
Foreword to special issue on SODA 2002,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847305523&doi=10.1145%2f1186810.1186811&partnerID=40&md5=4405658a3094a5cef86cfb926732b6d2,[No abstract available],,
An improved algorithm for radio broadcast,2007,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847305524&doi=10.1145%2f1186810.1186818&partnerID=40&md5=0415e81dac1cdc4101e2265e6cb77371,"We show that for every radio network G = (V, E) and source s ∈ V, there exists a radio broadcast schedule for G of length Rad(G, s) + O(√Rad(G, s) · log2 n) = O(Rad(G, s) + log4 n), where Rad(G, s) is the radius of the radio network G with respect to the source s. This result improves the previously best-known upper bound of O(Rad(G, s)+log5 n) due to Gaber and Mansour [1995]. For graphs with small genus, particularly for planar graphs, we provide an even better upper bound of Rad(G, S) + O(√Rad(G, s) · log n + log3 n) = O(Rad(G, s) + log3 n). © 2007 ACM.",Radio broadcast,Algorithms; Graph theory; Problem solving; Scheduling; Set theory; Radio broadcast scheduling; Radio networks; Upper bound; Radio broadcasting
Experimental analysis of dynamic all pairs shortest path algorithms,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846614452&doi=10.1145%2f1198513.1198519&partnerID=40&md5=211cc5cc3af6afe7556b03fdb0d0cd3d,"We present the results of an extensive computational study on dynamic algorithms for all pairs shortest path problems. We describe our implementations of the recent dynamic algorithms of King [1999] and of Demetrescu and Italiano [2006], and compare them to the dynamic algorithm of Ramalingam and Reps and to static algorithms on random, real-world and hard instances. Our experimental data suggest that some of the dynamic algorithms and their algorithmic techniques can be really of practical value in many situations. © 2006 ACM.",Dynamic graph algorithms; Experimental algorithmics; Shortest paths,Data reduction; Graph theory; Problem solving; Dynamic graph algorithms; Shortest paths problems; Static algorithms; Algorithms
The cyclic multi-peg Tower of Hanoi,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748992000&doi=10.1145%2f1159892.1159893&partnerID=40&md5=19844cdb3199cf2da893615e20aa0b19,"Variants of the classical Tower of Hanoi problem evolved in various directions. Allowing more than 3 pegs, and imposing limitations on the possible moves among the pegs, are two of these. Here, we deal with the case of h ≥ 3 pegs arranged on a circle, where moves are allowed only from a peg to the next peg (in the clockwise direction). Unlike the multi-peg problem without restrictions on moves between pegs, the complexity of this variant as a function of the number of disks is exponential. We find explicit lower and upper bounds for its complexity for any h, and show how this complexity can be estimated arbitrarily well for any specific h. © 2006 ACM.",Multi-peg tower of Hanoi,Computational complexity; Function evaluation; Functions; Hanoi problem; Multi-peg problems; Multi-peg tower of Hanoi; Problem solving
Quasiconvex analysis of multivariate recurrence equations for backtracking algorithms,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846635384&doi=10.1145%2f1198513.1198515&partnerID=40&md5=56248a66f87a1536acad7c612cf7b14b,"We consider a class of multivariate recurrences frequently arising in the worst-case analysis of Davis-Putnam-style exponential-time backtracking algorithms for NP-hard problems. We describe a technique for proving asymptotic upper bounds on these recurrences, by using a suitable weight function to reduce the problem to that of solving univariate linear recurrences; show how to use quasiconvex programming to determine the weight function yielding the smallest upper bound; and prove that the resulting upper bounds are within a polynomial factor of the true asymptotics of the recurrence. We develop and implement a multiple-gradient descent algorithm for the resulting quasiconvex programs, using a real-number arithmetic package for guaranteed accuracy of the computed worst-case time bounds. © 2006 ACM.",Automated analysis of algorithms; Backtracking; Davis - Putnam procedures; Method of feasible directions; Multivariate recurrences; Quasiconvex programming; Worst-case analysis,Algorithms; Asymptotic stability; Digital arithmetic; Linear programming; Polynomials; Problem solving; Real time systems; Backtracking; Davis - Putnam procedures; Multivariate recurrences; Quasiconvex programming; Worst-case analysis; Computational complexity
ACM Transactions on Algorithms: Foreword,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846587374&doi=10.1145%2f1198513.1198514&partnerID=40&md5=230e01f71e6025a017081f5cc571e717,[No abstract available],,
Approximate distance oracles for unweighted graphs in expected O(n 2) time,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846593294&doi=10.1145%2f1198513.1198518&partnerID=40&md5=54a9c257679e7d82ef01cabbc35484ba,"Let G = (V, E) be an undirected graph on n vertices, and let Δ(u, v) denote the distance in G between two vertices u and v. Thorup and Zwick showed that for any positive integer t, the graph G can be preprocessed to build a data structure that can efficiently report t-approximate distance between any pair of vertices. That is, for any u, v V, the distance reported is at least Δ(u, v) and at most tΔ(u, v). The remarkable feature of this data structure is that, for t3, it occupies subquadratic space, that is, it does not store all-pairs distances explicitly, and still it can answer any t-approximate distance query in constant time. They named the data structure approximate distance oracle because of this feature. Furthermore, the trade-off between the stretch t and the size of the data structure is essentially optimal.In this article, we show that we can actually construct approximate distance oracles in expected O(n2) time if the graph is unweighted. One of the new ideas used in the improved algorithm also leads to the first expected linear-time algorithm for computing an optimal size (2, 1)-spanner of an unweighted graph. A (2, 1) spanner of an undirected unweighted graph G = (V, E) is a subgraph (V, ), E, such that for any two vertices u and v in the graph, their distance in the subgraph is at most 2Δ(u, v) + 1. © 2006 ACM.",Approximate distance oracles; Distance queries; Distances; Shortest paths; Spanners,Algorithms; Approximation theory; Computation theory; Data structures; Integer programming; Quadratic programming; Query languages; Approximate distance oracles; Distance queries; Subquadratic space; Unweighted graph; Graph theory
A general approach to online network optimization problems,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846614035&doi=10.1145%2f1198513.1198522&partnerID=40&md5=1a02a6a99a1595fb9d30658210d55630,"We study a wide range of online graph and network optimization problems, focusing on problems that arise in the study of connectivity and cuts in graphs. In a general online network design problem, we have a communication network known to the algorithm in advance. What is not known in advance are the connectivity (bandwidth) or cut demands between vertices in the network which arrive online.We develop a unified framework for designing online algorithms for problems involving connectivity and cuts. We first present a general O(log m)-competitive deterministic algorithm for generating a fractional solution that satisfies the online connectivity or cut demands, where m is the number of edges in the graph. This may be of independent interest for solving fractional online bandwidth allocation problems, and is applicable to both directed and undirected graphs. We then show how to obtain integral solutions via an online rounding of the fractional solution. This part of the framework is problem dependent, and applies various tools including results on approximate max-flow min-cut for multicommodity flow, the Hierarchically Separated Trees (HST) method and its extensions, certain rounding techniques for dependent variables, and Räcke's new hierarchical decomposition of graphs.Specifically, our results for the integral case include an O(log mlog n)-competitive randomized algorithm for the online nonmetric facility location problem and for a generalization of the problem called the multicast problem. In the nonmetric facility location problem, m is the number of facilities and n is the number of clients. The competitive ratio is nearly tight. We also present an O(log2nlog k)-competitive randomized algorithm for the online group Steiner problem in trees and an O(log3nlog k)-competitive randomized algorithm for the problem in general graphs, where n is the number of vertices in the graph and k is the number of groups. Finally, we design a deterministic O(log 3nlog log n)-competitive algorithm for the online multi-cut problem. © 2006 ACM.",Competitive analysis; Facility location; Group Steiner; Multi-cuts; Online network optimization; Randomized rounding,Algorithms; Graph theory; Hierarchical systems; Online systems; Optimization; Trees (mathematics); Competitive analysis; Facility location; Online network optimization; Randomized rounding; Problem solving
Optimally scheduling video-on-demand to minimize delay when sender and receiver bandwidth may differ,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846635383&doi=10.1145%2f1198513.1198523&partnerID=40&md5=0a87d403115fbfec96355e7c114e32c8,"We establish tight bounds on the intrinsic cost (either minimizing delay d for fixed sender and receiver bandwidths, or minimizing sender bandwidth for fixed delay and receiver bandwidth) of broadcasting a video of length m over a channel of bandwidth S in such a way that a receiver (with bandwidth R), starting at an arbitrary time s, can download the video so that it can begin playback at time s + d.Our bounds are realized by a simple just-in-time protocol that partitions the video into a fixed number of segments, partitions the sender bandwidth into an equivalent number of equal bandwidth subchannels, and broadcasts each segment repeatedly on its own subchannel. The protocol is suitable for the broadcast of compressed video and it can be implemented so that video information is packaged into discrete fixed length packets incurring only a modest overhead (measured in terms of increased delay).Our primary contribution is a lower bound on the required delay that applies to all protocols. This lower bound matches the behavior of our just-in-time protocol in the limit as the number of segments approaches infinity, provided the video compression satisfies some uniform upper bound. For a fixed number of segments, our protocol is optimal within a broad class of protocols, even if the video is compressed arbitrarily. © 2006 ACM.",Data compression; Digital video broadcasting; Protocols; Video on demand,Bandwidth; Image compression; Network protocols; Scheduling; Television broadcasting; Bandwidth subchannels; Digital video broadcasting; Video information; Video on demand
Computing equilibria for a service provider game with (Im)perfect information,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846586942&doi=10.1145%2f1198513.1198524&partnerID=40&md5=c5be53b8c671dc8204223b702ce262e2,"We study fundamental algorithmic questions concerning the complexity of market equilibria under perfect and imperfect information by means of a basic microeconomic game. Suppose a provider offers a service to a set of potential customers. Each customer has a particular demand of service and her behavior is determined by a utility function that is nonincreasing in the sum of demands that are served by the provider.Classical game theory assumes complete information: the provider has full knowledge of the behavior of all customers. We present a complete characterization of the complexity of computing optimal pricing strategies and of computing best/worst equilibria in this model. Basically, we show that most of these problems are inapproximable in the worst case but admit an FPAS in the average case. Our average case analysis covers large classes of distributions for customer utilities. We generalize our analysis to robust equilibria in which players change their strategies only when this promises a significant utility improvement.A more realistic model considers providers with incomplete information. Following the game theoretic framework of Bayesian games introduced by Harsanyi, the provider is aware of probability distributions describing the behavior of the customers and aims at estimating its expected revenue under best/worst equilibria. Somewhat counterintuitively, we obtain an FPRAS for the equilibria problem in the model with imperfect information although the problem with perfect information is inapproximable under the worst-case measures. In particular, the worst-case complexity of the considered problems increases with the precision of the available knowledge. © 2006 ACM.",Imperfect information; Market equilibria; Service provider games,Algorithmic languages; Computational complexity; Cost effectiveness; Customer satisfaction; Information analysis; Knowledge acquisition; Probability distributions; Robustness (control systems); Utility programs; Imperfect information; Market equilibria; Service provider games; Utility function; Game theory
The register function for t-ary trees,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749000267&doi=10.1145%2f1159892.1159894&partnerID=40&md5=0f3badcceb1548c16292f6e15bcf238b,"For the register function for t-ary trees, recently introduced by Auber et al., we prove that the average is Iog 4 n + O(1), if all such trees with n internal nodes are considered to be equally likely. This result remains true for rooted trees where the set of possible out-degrees is finite. Furthermore we obtain exponential tail estimates for the distribution of the register function. Thus, the distribution is highly concentrated around the mean value. © 2006 ACM.",Asymptotics; Generating functions; Horton-Strahler numbers; Register function,Asymptotic stability; Function evaluation; Number theory; Trees (mathematics); Horton-Strahler numbers; Register function; Register functions; Functions
Optimal constrained graph exploration,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749000268&doi=10.1145%2f1159892.1159897&partnerID=40&md5=0e6dbc5d9d8f70dddadcab37446039c6,"We address the problem of constrained exploration of an unknown graph G = ( V, E) from a given start node s with either a tethered robot or a robot with a fuel tank of limited capacity, the former being a tighter constraint. In both variations of the problem, the robot can only move along the edges of the graph, for example, it cannot jump between nonadjacent nodes. In the tethered robot case, if the tether (rope) has length l, then the robot must remain within distance l from the start node s. In the second variation, a fuel tank of limited capacity forces the robot to return to s after traversing C edges. The efficiency of algorithms for both variations of the problem is measured by the number of edges traversed during the exploration. We present an algorithm for a tethered robot that explores the graph in Θ(|E|) edge traversals. The problem of exploration using a robot with a limited fuel tank capacity can be solved with a simple reduction from the tethered robot case and also yields a Θ(|E|) algorithm. This improves on the previous best-known bound of O (|E|+ |V| log 2 |V|). Since the lower bound for the graph exploration problems is Ω(|E|), our algorithm is optimal within a constant factor. © 2006 ACM.",Computational learning theory; Graph exploration,Algorithms; Computation theory; Fuel tanks; Problem solving; Robots; Computational learning theory; Graph exploration; Tank capacity; Tethered robots; Graph theory
An approximation algorithm for scheduling malleable tasks under general precedence constraints,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749000266&doi=10.1145%2f1159892.1159899&partnerID=40&md5=a02c080479de7d4118f21396afd94ad8,"In this article, we study the problem of scheduling malleable tasks with precedence constraints. We are given m identical processors and n tasks. For each task the processing time is a function of the number of processors allotted to it. In addition, the tasks must be processed according to the precedence constraints. The goal is to minimize the makespan (maximum completion time) of the resulting schedule. The best previous approximation algorithm (that works in two phases) in Lepère et al. [2002b] has a ratio 3 + √5 ≈ 5.236. We develop an improved approximation algorithm with a ratio at most 100/43 + 100(√4349 - 7)/2451 ≈ 4.730598. We also show that our resulting ratio is asymptotically tight. © 2006 ACM.",Approximation algorithms; Malleable tasks; Precedence constraints; Scheduling,Approximation theory; Constraint theory; Problem solving; Scheduling; Approximation algorithm; Malleable tasks; Precedence constraints; Algorithms
Oracles for bounded-length shortest paths in planar graphs,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749009751&doi=10.1145%2f1159892.1159895&partnerID=40&md5=fb5ce5724d7c80c6c3200cd8481f0774,"We present a new approach for answering short path queries in planar graphs. For any fixed constant k and a given unweighted planar graph G = (V, E), one can build in script capital O(|V|) time a data structure, which allows to check in script capital O(1) time whether two given vertices are at distance at most k in G and if so a shortest path between them is returned. Graph G can be undirected as well as directed. Our data structure works in fully dynamic environment. It can be updated in script capital O(1) time after removing an edge or a vertex while updating after an edge insertion takes polylogarithmic amortized time. Besides deleting elements one can also disable ones for some time. It is motivated by a practical situation where nodes or links of a network may be temporarily out of service. Our results can be easily generalized to other wide classes of graphs-for instance we can take any minor-closed family of graphs. © 2006 ACM.",Bounded length; Dynamic environment; Oracle; Planar graph; Shortest path,Algorithms; Data structures; Bounded length; Dynamic environment; Oracle; Planar graph; Shortest path; Graph theory
Online topological ordering,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748991997&doi=10.1145%2f1159892.1159896&partnerID=40&md5=02854549ee09cfe182af3ea27ce240d7,"It is shown that the problem of maintaining the topological order of the nodes of a directed acyclic graph while inserting m edges can be solved in O(min{/m 3/2log n, m 3/2 + n 2 log n}) time, an improvement over the best known result of O(mn). In addition, we analyze the complexity of the same algorithm with respect to the treewidth k of the underlying undirected graph. We show that the algorithm runs in time O(mk log 2 n) for general k and that it can be implemented to run in O(n log n) time on trees, which is optimal. The algorithm also detects cycles in the input. © 2006 ACM.",Dynamic algorithms; Graphs; Online algorithms; Topological order; Treewidth,Algorithms; Computational complexity; Online systems; Problem solving; Trees (mathematics); Dynamic algorithms; Online algorithms; Topological order; Treewidth; Graph theory
Generic quantum Fourier transforms,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846646884&doi=10.1145%2f1198513.1198525&partnerID=40&md5=007dfcc66aca75a5082f7ef99e18fc44,"The quantum Fourier transform (QFT) is a principal ingredient appearing in many efficient quantum algorithms. We present a generic framework for the construction of efficient quantum circuits for the QFT by quantizing the highly successful separation of variables technique for the construction of efficient classical Fourier transforms. Specifically, we apply Bratteli diagrams, Gel'fand-Tsetlin bases, and strong generating sets of small adapted diameter to provide efficient quantum circuits for the QFT over a wide variety of finite Abelian and non-Abelian groups, including all families of groups for which efficient QFTs are currently known and many new families as well. Moreover, our method provides the first subexponential-size quantum circuits for the QFT over the linear groups GLk(q), SLk(q), and the finite groups of Lie type, for any fixed prime power q. © 2006 ACM.",Group theory; Quantum computation,Algorithms; Computation theory; Fourier transforms; Group theory; Quantum computation; Quantum Fourier transform (QFT); Quantum theory
When indexing equals compression: Experiments with compressing suffix arrays and applications,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846576963&doi=10.1145%2f1198513.1198521&partnerID=40&md5=74f6909c4bdcc73fa0edaeaf6ce1b812,"We report on a new experimental analysis of high-order entropy-compressed suffix arrays, which retains the theoretical performance of previous work and represents an improvement in practice. Our experiments indicate that the resulting text index offers state-of-the-art compression. In particular, we require roughly 20% of the original text size - -without requiring a separate instance of the text. We can additionally use a simple notion to encode and decode block-sorting transforms (such as the Burrows - Wheeler transform), achieving a compression ratio comparable to that of bzip2. We also provide a compressed representation of suffix trees (and their associated text) in a total space that is comparable to that of the text alone compressed with gzip. © 2006 ACM.",Burrows - Wheeler Transform; Entropy; Suffix array; Text indexing,Algorithms; Entropy; Image coding; Indexing (materials working); Mathematical transformations; Trees (mathematics); Burrows Wheeler Transform; Compressing suffix arrays; State-of-the-art compression; Text indexing; Data compression
The NP-completeness column: The many limits on approximation,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748996751&doi=10.1145%2f1159892.1159901&partnerID=40&md5=d12e82dd2b91ce2c9c535eb60be367ac,"This is the 25th edition of a column that covers new developments in the theory of NP-completeness. The presentation is modeled on that which M. R. Garey and I used in our book Computers and Intractability: A Guide to the Theory of NP-Completeness, W. H. Freeman & Co., New York, 1979, hereinafter referred to as ""[G&J]."" Previous columns, the first 23 of which appeared in Journal of Algorithms, will be referred to by a combination of their sequence number and year of appearance, for example, [Col 1, 1981]. Full bibliographic details on the previous columns as well as downloadable unofficial versions of them can be found at http://www.reseach.att.com/~dsj/columns/. This edition of the column discusses the wide range of lower bounds on approximation guarantees for NP-hard optimization problems both in their functional forms and in the hypotheses on which they depend. © 2006 ACM.",Approximation algorithms; Clique; Label cover; Lower bounds; Probabilistically checkable proofs; Set cover; Unique Games Conjecture,Computer simulation; Game theory; Optimization; Probabilistic logics; Problem solving; Approximation algorithms; Lower bounds; Set cover; Unique Games Conjecture; Approximation theory
Secure multiparty computation of approximations,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748996753&doi=10.1145%2f1159892.1159900&partnerID=40&md5=c11c799188388ca51eb31ffbd2e6c558,"Approximation algorithms can sometimes provide efficient solutions when no efficient exact computation is known. In particular, approximations are often useful in a distributed setting where the inputs are held by different parties and may be extremely large. Furthermore, for some applications, the parties want to compute a function of their inputs securely without revealing more information than necessary. In this work, we study the question of simultaneously addressing the above efficiency and security concerns via what we call secure approximations. We start by extending standard definitions of secure (exact) computation to the setting of secure approximations. Our definitions guarantee that no additional information is revealed by the approximation beyond what follows from the output of the function being approximated. We then study the complexity of specific secure approximation problems. In particular, we obtain a sublinear-communication protocol for securely approximating the Hamming distance and a polynomial-time protocol for securely approximating the permanent and related #P-hard problems. © 2006 ACM.",Distributed data processing; Privacy; Sublinear communication,Computation theory; Data privacy; Data processing; Distributed computer systems; Distributed data processing; Hamming distance; Multiparty computation; Sublinear communication; Approximation theory
Faster fixed parameter tractable algorithms for finding feedback vertex sets,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748996754&doi=10.1145%2f1159892.1159898&partnerID=40&md5=be453c4ff788a9f85b4cdf5ea010f905,"A feedback vertex set (fvs) of a graph is a set of vertices whose removal results in an acyclic graph. We show that if an undirected graph on n vertices with minimum degree at least 3 has a fvs on at most 1/3n 1-ε vertices, then there is a cycle of length at most 6/ε (for ε ≥ 1/2, we can even improve this to just 6). Using this, we obtain a O((12 log k/log log k + 6) kn ω algorithm for testing whether an undirected graph on n vertices has a fvs of size at most k. Here n ω is the complexity of the best matrix multiplication algorithm. The previous best parameterized algorithm for this problem took O((2k + 1) kn 2) time. We also investigate the fixed parameter complexity of weighted feedback vertex set problem in weighted undirected graphs. © 2006 ACM.",Feedback vertex set; Girth; Parameterized complexity,Computational complexity; Graph theory; Parameter estimation; Problem solving; Set theory; Feedback vertex set; Girth; Parameterized complexity; Algorithms
Melding priority queues,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846583487&doi=10.1145%2f1198513.1198517&partnerID=40&md5=73d67b3741e0a33f37fa99d2729e56ca,"We show that any priority queue data structure that supports insert, delete, and find-min operations in pq(n) amortized time, where n is an upper bound on the number of elements in the priority queue, can be converted into a priority queue data structure that also supports fast meld operations with essentially no increase in the amortized cost of the other operations. More specifically, the new data structure supports insert, meld and find-min operations in O(1) amortized time, and delete operations in O(pq(n) + a(n)) amortized time, where a(n) is a functional inverse of the Ackermann function, and where n this time is the total number of operations performed on all the priority queues. The construction is very simple. The meldable priority queues are obtained by placing a nonmeldable priority queue at each node of a union-find data structure.We also show that when all keys are integers in the range [1, N], we can replace n in the bound stated previously by min{n, N}. Applying this result to the nonmeldable priority queue data structures obtained recently by Thorup [2002b] and by Han and Thorup [2002] we obtain meldable RAM priority queues with O(log log n)amortized time per operation, or O(vlog log n) expected amortized time per operation, respectively. As a by-product, we obtain improved algorithms for the minimum directed spanning tree problem on graphs with integer edge weights, namely, a deterministic O(m log log n)-time algorithm and a randomized O(mvlog log n)-time algorithm. For sparse enough graphs, these bounds improve on the O(m + n log n) running time of an algorithm by Gabow et al. [1986] that works for arbitrary edge weights. © 2006 ACM.",Disjoint set union; Heaps; Minimum directed spanning trees; Optimum branchings; Priority queues; Union-find; Word RAM model,Algorithms; Computational complexity; Graph theory; Queueing theory; Random access storage; Set theory; Trees (mathematics); Disjoint set union; Minimum directed spanning trees; Optimum branchings; Priority queues; Data structures
Rank-maximal matchings,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846620479&doi=10.1145%2f1198513.1198520&partnerID=40&md5=54a2d849a72ec798da01a11f5105e1c5,"Suppose that each member of a set A of applicants ranks a subset of a set P of posts in an order of preference, possibly involving ties. A matching is a set of (applicant, post) pairs such that each applicant and each post appears in at most one pair. A rank-maximal matching is one in which the maximum possible number of applicants are matched to their first choice post, and subject to that condition, the maximum possible number are matched to their second choice post, and so on. This is a relevant concept in any practical matching situation and it was first studied by Irving [2003]. We give an algorithm to compute a rank-maximal matching with running time O(min(n + C,Cvn)m), where C is the maximal rank of an edge used in a rank-maximal matching, n is the number of applicants and posts and m is the total size of the preference lists. © 2006 ACM.",Bipartite matching; Maximum cardinality matching; One-sided preference lists,Algorithms; Computation theory; Bipartite matching; Maximum cardinality matching; Preference lists; Rank-maximal matchings; Set theory
Succinct ordinal trees with level-ancestor queries,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846642491&doi=10.1145%2f1198513.1198516&partnerID=40&md5=9b7abdcfc086e14feb7b3de4ae9fcb9f,"We consider succinct or space-efficient representations of trees that efficiently support a variety of navigation operations. We focus on static ordinal trees, that is, arbitrary static rooted trees where the children of each node are ordered. The set of operations is essentially the union of the sets of operations supported by previous succinct representations [Jacobson 1989; Munro and Raman 2001; Benoit et al. 1999] to which we add the level-ancestor operation.Our representation takes 2n + o(n) bits to represent an n-node tree, which is within o(n) bits of the information-theoretic minimum, and supports all operations in O(1) time on the RAM model.These operations also provide a mapping from the n nodes of the tree onto the integers {1, . . . , n}. In addition to the existing otivations for studying such data structures, we are motivated by the problem of representing XML documents compactly so that XPath queries can be supported efficiently. © 2006 ACM.",Succinct data structures; XML document representation,Data structures; Information retrieval systems; Navigation systems; Query languages; Random access storage; Set theory; XML; Level-ancestor operation; Static rooted trees; Succinct ordinal trees; XML document representation; Trees (mathematics)
A loopless gray code for rooted trees,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747629848&doi=10.1145%2f1150334.1150335&partnerID=40&md5=b246de2ac8ae23be83e5ca2bceca51f3,Beyer and Hedetniemi [1980] gave the first constant average-time algorithm for the generation of all rooted trees with n nodes. This article presents the first combinatorial Gray code for these trees and a loopless algorithm for its generation. © 2006 ACM.,Gray code; Loopless; Rooted trees,Algorithms; Codes (symbols); Combinatorial circuits; Graph theory; Average-time algorithm; Gray code; Loopless; Rooted trees; Trees (mathematics)
Bipartite roots of graphs,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747691696&doi=10.1145%2f1150334.1150337&partnerID=40&md5=2f33792727a1b6651ff78d26436fb139,"Graph H is a root of graph G if there exists a positive integer k such that x and y are adjacent in G if and only if their distance in H is at most k. Motwani and Sudan [1994] proved the NP-completeness of graph square recognition and conjectured that it is also NP-complete to recognize squares of bipartite graphs. The main result of this article is to show that squares of bipartite graphs can be recognized in polynomial time. In fact, we give a polynomial-time algorithm to count the number of different bipartite square roots of a graph, although this number could be exponential in the size of the input graph. By using the ideas developed, we are able to give a new and simpler linear-time algorithm to recognize squares of trees and a new algorithmic proof that tree square roots are unique up to isomorphism. Finally, we prove the NP-completeness of recognizing cubes of bipartite graphs. © 2006 ACM.",Bipartite graphs; Graph powers; Graph roots,Algorithms; Computational complexity; Computer science; Graphic methods; Polynomials; Trees (mathematics); Bipartite graphs; Graph powers; Graph roots; Isomorphism; Graph theory
Algorithmic construction of sets for k-restrictions,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747718071&doi=10.1145%2f1150334.1150336&partnerID=40&md5=042a588ca678620e5e3865c373c1970e,"This work addresses k-restriction problems, which unify combinatorial problems of the following type: The goal is to construct a short list of strings in σ m that satisfies a given set of k-wise demands. For every k positions and every demand, there must be at least one string in the list that satisfies the demand at these positions. Problems of this form frequently arise in different fields in Computer Science. The standard approach for deterministically solving such problems is via almost k-wise independence or k-wise approximations for other distributions. We offer a generic algorithmic method that yields considerably smaller constructions. To this end, we generalize a previous work of Naor et al. [1995]. Among other results, we enhance the combinatorial objects in the heart of their method, called splitters, and construct multi-way splitters, using a new discrete version of the topological Necklace Splitting Theorem [Alon 1987]. We utilize our methods to show improved constructions for group testing [Ngo and Du 2000] and generalized hashing [Alon et al. 2003], and an improved inapproximability result for SET-COVER under the assumption P ≠ NP. © 2006 ACM.",Almost k-wise independence; Derandomization; Generalized hashing; Group testing; k-restriction; Set-Cover; Splitter,Algorithms; Approximation theory; Combinatorial mathematics; Problem solving; Set theory; Theorem proving; Almost k-wise independence; Derandomization; Generalized hashing; Group testing; K-restriction; Set-Cover; Splitter; Computer science
Minimizing mean flow time for UET tasks,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747647492&doi=10.1145%2f1150334.1150340&partnerID=40&md5=461f87c1921ba26600867251909dcf8d,"We consider the problem of scheduling a set of n unit-execution-time (UET) tasks, with precedence constraints, on m ≥ 1 parallel and identical processors so as to minimize the mean flow time. For two processors, the Coffman-Graham algorithm gives a schedule that simultaneously minimizes the mean flow time and the makespan. The problem becomes strongly NP-hard for an arbitrary number of processors, although the complexity is not known for each fixed m ≥ 3. For arbitrary precedence constraints, we show that the Coffman-Graham algorithm gives a schedule with a worst-case bound no more than 2, and we give examples showing that the bound is tight. For intrees, the problem can be solved in polynomial time for each fixed m ≥ 1, although the complexity is not known for an arbitrary number of processors. We show that Hu's algorithm (which is optimal for the makespan objective) yields a schedule with a worst-case bound no more than 1.5, and we give examples showing that the ratio can approach 1.308999. "" 2006 ACM.",Approximation algorithms; Intrees; Mean flow time; Precedence constraints; Scheduling,Algorithms; Approximation theory; Computational complexity; Parallel processing systems; Polynomials; Program processors; Scheduling; Intrees; Mean flow time; Precedence constraints; Unit-execution-time (UET) tasks; Problem solving
Efficient algorithms for bichromatic separability,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747708183&doi=10.1145%2f1150334.1150338&partnerID=40&md5=ac0eb779924f1aa09ee353f01178d22e,"A closed solid body separates one point set from another if it contains the former and the closure of its complement contains the latter. We present a near-linear algorithm for deciding whether two sets of n points in ℝ 3 can be separated by a prism, near-quadratic algorithms for separating by a slab or a wedge, and a near-cubic algorithm for separating by a double wedge. The latter three algorithms improve the previous best known results by an order of magnitude, while the prism separability algorithm constitutes an improvement of two orders of magnitude. © 2006 ACM.",Arrangements; Geometric algorithms; Separability,Linear systems; Numerical methods; Prisms; Quadratic programming; Set theory; Bichromatic separability; Geometric algorithms; Near-cubic algorithm; Near-quadratic algorithms; Algorithms
This side up!,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747742491&doi=10.1145%2f1150334.1150339&partnerID=40&md5=1743749d7908c58dc0a7c5d01f581cf0,"We consider two- and three-dimensional bin-packing problems where 90° rotations are allowed. We improve all known asymptotic performance bounds for these problems. In particular, we show how to combine ideas from strip packing and two-dimensional bin packing to give a new algorithm for the three-dimensional strip packing problem where boxes can only be rotated sideways. We propose to call this problem ""This side up"". Our algorithm has an asymptotic performance bound of 9/4. © 2006 ACM.",Bin packing; Online algorithms; Rotations,Algorithms; Asymptotic stability; Three dimensional; Asymptotic performance; Bin packing; Online algorithms; Rotations; Problem solving
An improved algorithm for CIOQ switches,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747656424&doi=10.1145%2f1150334.1150342&partnerID=40&md5=ff1e0e9a6f9b256f9498d779ce54fabf,"The problem of maximizing the weighted throughput in various switching settings has been intensively studied recently through competitive analysis. To date, the most general model that has been investigated is the standard CIOQ (Combined Input and Output Queued) switch architecture with internal fabric speedup S ≥ 1. CIOQ switches, that comprise the backbone of packet routing networks, are N × N switches controlled by a switching policy that incorporates two components: Admission control and scheduling. An admission control strategy is essential to determine the packets stored in the FIFO queues in input and output ports, while the scheduling policy conducts the transfer of packets through the internal fabric, from input ports to output ports. The online problem of maximizing the total weighted throughput of CIOQ switches was recently investigated by Kesselman and Rosén [2003]. They presented two different online algorithms for the general problem that achieve non-constant competitive ratios (linear in either the speedup or the number of distinct values, or logarithmic in the value range). We introduce the first constant-competitive algorithm for the general case of the problem, with arbitrary speedup and packet values. Specifically, our algorithm is 8-competitive, and is also simple and easy to implement. © 2006 ACM.",CIOQ; Competitive; On-line; Qos; Switch,Algorithms; Online systems; Problem solving; Quality of service; Routers; Switching theory; Admission control strategy; Combined Input and Output Queued (CIOQ); Constant-competitive algorithm; Non-constant competitive ratios; Switches
Robust subgraphs for trees and paths,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747728231&doi=10.1145%2f1150334.1150341&partnerID=40&md5=996d67683822bb682028d256bd702736,"Consider a graph problem which is associated with a parameter, for example, that of finding a longest tour spanning k vertices. The following question is natural: Is there a small subgraph that contains an optimal or near optimal solution for every possible value of the given parameter? Such a subgraph is said to be robust. In this article we consider the problems of finding heavy paths and heavy trees of k edges. In these two cases, we prove surprising bounds on the size of a robust subgraph for a variety of approximation ratios. For both problems, we show that in every complete weighted graph on n vertices there exists a subgraph with approximately α/1-α 2n edges that contains an α-approximate solution for every k = 1,..., n - 1. In the analysis of the tree problem, we also describe a new result regarding balanced decomposition of trees. In addition, we consider variants in which the subgraph itself is restricted to be a path or a tree. For these problems, we describe polynomial time algorithms and corresponding proofs of negative results. © 2006 ACM.",Approximation algorithms; Decomposition; Robust optimization; Spanning trees,Algorithms; Approximation theory; Polynomials; Problem solving; Robustness (control systems); Trees (mathematics); Optimal solution; Spanning trees; Subgraph; Trees decomposition; Graph theory
Finding 3-shredders efficiently,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745273590&doi=10.1016%2fj.foodchem.2005.01.050&partnerID=40&md5=85adfbca2d147152f0e5c3d48c721e9c,"A shredder in an undirected graph is a set of vertices whose removal results in at least three components. A 3-shredder is a shredder of size three. We present an algorithm that, given a 3-connected graph, finds its 3-shredders in time proportional to the number of vertices and edges, when implemented on a RAM (random access machine). © 2006 ACM.",Depth-first search; Four-connected; Shredder; Vertex cut,Algorithms; Edge detection; Number theory; Random access storage; Set theory; Depth-first search; Four-connected; Shredder; Vertex cut; Graph theory
Generating rooted and free plane trees,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745281216&doi=10.1145%2f1125994.1125995&partnerID=40&md5=69deb49612ae0e2ec08dc1d8a49f9297,"This article has two main results. First, we develop a simple algorithm to list all nonisomorphic rooted plane trees in lexicographic order using a level sequence representation. Then, by selecting a unique centroid to act as the root of a free plane tree, we apply the rooted plane tree algorithm to develop an algorithm to list all nonisomorphic free plane trees. The latter algorithm also uses a level sequence representation and lists all free plane trees with a unique centroid first followed by all free plane trees with two centroids. Both algorithms are proved to run in constant amortized time using straightforward bounding methods. © 2006 ACM.",CAT algorithm; Chord diagram; Free plane tree; Necklace; Planar tree; Rooted plane tree,Algorithms; Boundary conditions; Mathematical models; Theorem proving; CAT algorithm; Chord diagram; Free plane tree; Lexicography; Necklace; Nonisomorphic rooted plane; Planar tree; Rooted plane tree; Tree algorithm; Trees (mathematics)
The minimum generalized vertex cover problem,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745270743&doi=10.1145%2f1125994.1125998&partnerID=40&md5=e917b453889f3faf728dcc2c7978080a,"Let G = (V, E) be an undirected graph, with three numbers d0(e) ≥ d1(e) ≥ d2(e) ≥ 0 for each edge e ∈ E. A solution is a subset U ⊆ V and di(e) represents the cost contributed to the solution by the edge e if exactly i of its endpoints are in the solution. The cost of including a vertex v in the solution is c(v). A solution has cost that is equal to the sum of the vertex costs and the edge costs. The minimum generalized vertex cover problem is to compute a minimum cost set of vertices. We study the complexity of the problem with the costs d0(e) = 1, d1(e) = α and d2(e) = 0 ∀e ∈ E and c-(v) = β ∀v ∈ V, for all possible values of of and β. We also provide 2-approximation algorithms for the general case. © 2006 ACM.",Complexity classification; Local-ratio; Vertex cover,Algorithms; Approximation theory; Computational complexity; Edge detection; Functions; Number theory; Problem solving; Complexity classification; Local-ratio; Vertex cover problem; Graph theory
Online scheduling of splittable tasks,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745277382&doi=10.1145%2f1125994.1125999&partnerID=40&md5=41ecce67dcb9ee0727e1c180eaf91d98,"We consider online scheduling of splittable tasks on parallel machines, where the goal is to minimize the last completion time (the makespan). In our model, each task can be split into a limited number of parts, that can then be scheduled independently and in parallel. We consider both the case where the machines are identical and the case where some subset of the machines have a (fixed) higher speed than the others. We design a class of algorithms that allows us to give tight bounds for a large class of cases where tasks may be split into relatively many parts. For identical machines, we also improve upon the natural greedy algorithm in other classes of cases. © 2006 ACM.",Online algorithms; Scheduling; Splittable tasks,Boundary conditions; Mathematical models; Parallel algorithms; Scheduling; Greedy algorithm; Online algorithms; Online scheduling; Splittable tasks; Online searching
Pattern matching for arc-annotated sequences,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745253368&doi=10.1145%2f1125994.1125997&partnerID=40&md5=84ba37b91bec42c3958bcf267fe6aa06,"We study pattern matching for arc-annotated sequences. An O(nm) time algorithm is given for the problem to determine whether a length m sequence with nested arc annotation is an arc-preserving subsequence (aps) of a length n sequence with nested arc annotation, called APS(NESTED,NESTED). Arc-annotated sequences and, in particular, those with nested arc annotation are motivated by applications in RNA structure comparison. Our algorithm generalizes results for ordered tree inclusion problems and it is useful for recent fixed-parameter algorithms for LAPCS(NESTED,NESTED), which is the problem of computing a longest arc-preserving common subsequence of two sequences with nested arc annotations. In particular, the presented dynamic programming methodology implies a quadratic-time algorithm for an open problem posed by Vialette. © 2006 ACM.",Arc-annotated sequences; Dynamic programming; Pattern matching; RNA secondary structure,Algorithms; Dynamic programming; Problem solving; RNA; Structure (composition); Trees (mathematics); Arc-annotated sequences; Quadratic-time algorithm; RNA secondary structure; Time algorithm; Pattern matching
Problems column,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745230462&doi=10.1145%2f1125994.1126002&partnerID=40&md5=d43392b36dc090f954560b0e68c21a0b,"The open research problems, coming from papers presented at the European Symposium on Algorithms (ESA), held in Mallorca, Spain in Oct 2005 were discussed. A solution to the brain teaser from Donald Knuth related to graph theory was also presented. The problem of multiple stream jitter regulation was also addressed and a model was suggested for permitting a certain amount of cell-dropping, and examining the correlations between buffer size, optimal jitter, and drop ratio. It was suggested that the minimum size, bounded capacity cut (MinSBCC) problem which arises in practical settings, such as in epidemiology, disaster control, as well as finding dense subgraphs and communities in graphs was also described. The solution to the problem of scheduling a set of independent jobs on identical parallel machines subject to migration delays so as to minimize workspan was also elaborated.",Algorithms; Problems,Algorithms; Correlation methods; Disaster prevention; Epidemiology; Jitter; Mathematical models; Problem solving; Disaster control; European Symposium on Algorithms (ESA); Open research problems; Parallel machines; Graph theory
Improved results for data migration and open shop scheduling,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745255065&doi=10.1145%2f1125994.1126001&partnerID=40&md5=5d320cde07edb40da304697ccca21d40,"The data migration problem is to compute an efficient plan for moving data stored on devices in a network from one configuration to another. We consider this problem with the objective of minimizing the sum of completion times of all storage devices. It is modeled by a transfer graph, where vertices represent the storage devices, and the edges indicate the data transfers required between pairs of devices. Each vertex has a nonnegative weight, and each edge has a release time and a processing time. A vertex completes when all the edges incident on it complete; the constraint is that two edges incident on the same vertex cannot be processed simultaneously. The objective is to minimize the sum of weighted completion times of all vertices. Kim (Journal of Algorithms, 55:42-57, 2005) gave a 9-approximation algorithm for the problem when edges have arbitrary processing times and are released at time zero. We improve Kim's result by giving a 5.06-approximation algorithm. We also address the open shop scheduling problem, O|rj| Σ wjC j, and show that it is a special case of the data migration problem. Queyranne and Sviridenko (Journal of Scheduling, 5:287-305, 2002) gave a 5.83 -approximation algorithm for the nonpreemptive version of the open shop problem. They state as an obvious open question whether there exists an algorithm for open shop scheduling that gives a performance guarantee better than 5.83. Our 5.06 algorithm for data migration proves the existence of such an algorithm. Crucial to our improved result is a property of the linear programming relaxation for the problem. Similar linear programs have been used for various other scheduling problems. Our technique may be useful in obtaining improved results for these problems as well. © 2006 ACM.",Approximation algorithms; Data migration; Linear programming; LP rounding; Open shop; Scheduling,Algorithms; Approximation theory; Data storage equipment; Data transfer; Graph theory; Linear programming; Operations research; Problem solving; Approximation algorithms; Data migration; LP rounding; Open shop; Processing time; Scheduling
Minimizing total completion time on uniform machines with deadline constraints,2006,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745238676&doi=10.1145%2f1125994.1126000&partnerID=40&md5=c75631e3f8c67c21946f06d3fd11e112,"Consider n independent jobs and m uniform machines in parallel. Each job has a processing requirement and a deadline. All jobs are available for processing at time t = 0. Job j must complete its processing before or at its deadline and preemptions are allowed. A set of jobs is said to be feasible if there exists a schedule that meets all the deadlines. We present a polynomial-time algorithm that given a feasible set of jobs, constructs a schedule that minimizes the total completion time Σ Cj. In the classical α | β | γ scheduling notation, this problem is referred to as Qm | prmt, d̄j | Σ Cj. It is well known that a generalization of this problem with regard to its machine environment results in an NP-hard problem. © 2006 ACM.",Deadline constraints; Mean flow time; Polynomial-time algorithms; Uniform machines,Algorithms; Parallel processing systems; Polynomials; Problem solving; Queueing theory; Deadline constraints; Machine environment; Mean flow time; Polynomial-time algorithms; Total completion time; Uniform machines; Constraint theory
Problems Column,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024259027&doi=10.1145%2f1077464.1077475&partnerID=40&md5=1958add3012aff0052f2bb8f987c1ac2,[No abstract available],,
Computing Almost Shortest Paths,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016166786&doi=10.1145%2f1103963.1103968&partnerID=40&md5=8a3829a8a237d624b512943409d1688e,"We study the s-sources almost shortest paths (abbreviated s-ASP) problem. Given an unweighted graph G = (V, E), and a subset S ⊆ V of s nodes, the goal is to compute almost shortest paths between all the pairs of nodes S ×V. We devise an algorithm with running time O(|E|nρ+s.n1+ζ) for this problem that computes the paths Pu, w for all pairs (u, w) ∈ S ×V such that the length of Pu, w is at most (1 + ϵ)dG(u, w) + β(ζ, ρ, ϵ), and β(ζ, ρ, ϵ) is constant when ζ, ρ, and ϵ are arbitrarily small constants. We also devise a distributed protocol for the s-ASP problem that computes the paths Pu, w as above, and has time and communication complexities of O(s. Diam(G) + n1+ ζ/2) (respectively, O(s. Diam(G) log3 n + n1+ ζ/2 log n)) and O(|E|n ρ + s. n1+ ζ) (respectively, O(|E|nρ + s. n1+ ζ + n1+ ρ+ ζ (ρ. ζ/2)/2)) in the synchronous (respectively asynchronous) setting. Our sequential algorithm, as well as the distributed protocol, is based on a novel algorithm for constructing (1+ϵ, β(ζ, ρ, ϵ))-spanners of size O(n|1+ ζ), developed in this article. This algorithm has running time of O(|E|n ρ), which is significantly faster than the previously known algorithm given in Elkin and Peleg [2001], whose running time is.O(n2+ ρ).We also develop the first distributed protocol for constructing (1+ϵ, β)-spanners. The communication complexity of this protocol is near optimal. © 2005, ACM. All rights reserved.",Algorithms; Almost shortest paths; Graph algorithms; Spanners; Theory,
A Linear-Time Approximation Algorithm for Weighted Matchings in Graphs,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995706345&doi=10.1145%2f1077464.1077472&partnerID=40&md5=d77bc5b9da2fa5c4d43cd48cbe75ec6a,"Approximation algorithms have so far mainly been studied for problems that are not known to have polynomial time algorithms for solving them exactly. Here we propose an approximation algorithm for the weighted matching problem in graphs which can be solved in polynomial time. The weighted matching problem is to find a matching in an edge weighted graph that has maximum weight. The first polynomial-time algorithm for this problem was given by Edmonds in 1965. The fastest known algorithm for the weighted matching problem has a running time of O(nm +n2 log n). Many real world problems require graphs of such large size that this running time is too costly. Therefore, there is considerable need for faster approximation algorithms for the weighted matching problem.We present a linear-time approximation algorithm for the weighted matching problem with a performance ratio arbitrarily close to 2/3. This improves the previously best performance ratio of 1/2. Our algorithm is not only of theoretical interest, but because it is easy to implement and the constants involved are quite small it is also useful in practice. © 2005, ACM. All rights reserved.",Algorithms; Approximation algorithm; Maximum weight matching; Theory,
A Maiden Analysis of Longest Wait First,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969254462&doi=10.1145%2f1077464.1077467&partnerID=40&md5=abba264cade6ca172a9d04d04824dc62,"We consider server scheduling strategies to minimize average flow time in a multicast pull system where data items have uniform size. The algorithm LongestWait First (LWF) always services the page where the aggregate waiting times of the outstanding requests for that page is maximized. We provide the first non-trivial analysis of the worst case performance of LWF. On the negative side, we show that LWF is not s-speed O(1)-competitive for s < 1+ √5/2. On the positive side, we show that LWF is 6-speed O(1)-competitive. © 2005, ACM. All rights reserved.",Algorithms; Broadcast; Flow Time; Multicast; Peformance; Resource augmentation; Scheduling; Theory,
On Network Design Problems: Fixed Cost Flows and the Covering Steiner Problem,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016686241&doi=10.1145%2f1077464.1077470&partnerID=40&md5=34ce66243f648dc7d2208a74462a8cac,"Network design problems, such as generalizations of the Steiner Tree Problem, can be cast as edge-cost-flow problems. An edge-cost flow problem is a min-cost flow problem in which the cost of the flow equals the sum of the costs of the edges carrying positive flow. We prove a hardness result for the Minimum Edge Cost Flow Problem (MECF). Using the oneround two-prover scenario, we prove that MECF does not admit a 2log1-ε n-ratio approximation, for every constant ε > 0, unless NP ⊆ DTIME(npolylogn). A restricted version of MECF, called Infinite Capacity MECF (ICF), is defined. The ICF problem is defined as follows: (i) all edges have infinite capacity, (ii) there are multiple sources and sinks, where flow can be delivered from every source to every sink, (iii) each source and sink has a supply amount and demand amount, respectively, and (iv) the required total flow is given as part of the input. The goal is to find a minimum edge-cost flow that meets the required total flow while obeying the demands of the sinks and the supplies of the sources. This problem naturally arises in practical scheduling applications, and is equivalent to the special case of single source MECF, with all edges not touching the source or the sink having infinite capacity.The directed ICF generalizes the Covering Steiner Problem in directed and undirected graphs. The undirected version of ICF generalizes several network design problems, such as: Steiner Tree Problem, k-MST, Point-to-point Connection Problem, and the generalized Steiner Tree Problem. An O(log x)-approximation algorithm for undirected ICF is presented. We also present a bi-criteria approximation algorithm for directed ICF. The algorithm for directed ICF finds a flow that delivers half the required flow at a cost that is at most O(nε/ε4) times bigger than the cost of an optimal flow. The running time of the algorithm is O(x2/ε. n1+1/ε), where x denotes the required total flow. Randomized approximation algorithms for the Covering Steiner Problem in directed and undirected graphs are presented. The algorithms are based on a randomized reduction to a problem called 1/2 -Group Steiner. In undirected graphs, the approximation ratio matches the approximation ratio of Konjevod et al. [2002]. However, our algorithm is much simpler. In directed graphs, the algorithm is the first nontrivial approximation algorithm for the Covering Steiner Problem. Deterministic algorithms are obtained by derandomization. © 2005, ACM. All rights reserved.",Algorithms; Approximation; Design; Flow; Graphs; Optimization; Theory; Theory,
Editor's Foreword,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024276541&doi=10.1145%2f1077464.1077465&partnerID=40&md5=b771659b89a678a3c1ee8dd0f20649c1,[No abstract available],,
On a Generalization of the Stable Roommates Problem,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981550710&doi=10.1145%2f1077464.1077474&partnerID=40&md5=7f641989e74ac14fe889798edd3a8155,"We consider two generalizations of the stable roommates problem: a) we allow parallel edges in the underlying graph, and b) we study a problem with multiple partners. We reduce both problems to the classical stable roommates problem and describe an extension of Irving's algorithm that solves the generalized problem efficiently.We give a direct proof of a recent result on the structure of stable many-to-many matchings (so called stable b-matchings) as a by-product of the justification of the algorithm. © 2005, ACM. All rights reserved.",Algorithms; Economics; Irving's algorithm; Stable marriage problem; Table roommates problem,
Individual Displacements for Linear Probing Hashing with Different Insertion Policies,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023626599&doi=10.1145%2f1103963.1103964&partnerID=40&md5=4393560bfb5ae143f4d2068fac74ffd6,"We study the distribution of the individual displacements in hashing with linear probing for three different versions: First Come, Last Come and Robin Hood. Asymptotic distributions and their moments are found when the the size of the hash table tends to infinity with the proportion of occupied cells converging to some α, 0 < α < 1. (In the case of Last Come, the results are more complicated and less complete than in the other cases.) We also show, using the diagonal Poisson transform studied by Poblete, Viola and Munro, that exact expressions for finite m and n can be obtained from the limits as m, n→ ∞. We end with some results, conjectures and questions about the shape of the limit distributions. These have some relevance for computer applications. © 2005, ACM. All rights reserved.",Algorithms; Hashing; Linear probing; Robin Hood hashing,
Exact Distribution of Individual Displacements in Linear Probing Hashing,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249130992&doi=10.1145%2f1103963.1103965&partnerID=40&md5=f3a6bc90aec37ef0d05660eb951715f9,"This paper studies the distribution of individual displacements for the standard and the Robin Hood linear probing hashing algorithms. When the a table of size m has n elements, the distribution of the search cost of a random element is studied for both algorithms. Specifically, exact distributions for fixed m and n are found as well as when the table is α-full, and α strictly smaller than 1. Moreover, for full tables, limit laws for both algorithms are derived. © 2005, ACM. All rights reserved.",Algorithms; Distributional analysis; Individual displacements; Linear probing; Performance; Theory,
Approximate Majorization and Fair Online Load Balancing,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961799654&doi=10.1145%2f1103963.1103970&partnerID=40&md5=c431210cb39ef9c647de37af72f3b1f2,"This article relates the notion of fairness in online routing and load balancing to vector majorization as developed by Hardy et al. [1929]. We define α-supermajorization as an approximate form of vector majorization, and show that this definition generalizes and strengthens the prefix measure proposed by Kleinberg et al. [2001] as well as the popular notion of max-min fairness. The article revisits the problem of online load-balancing for unrelated 1-∞ machines from the viewpoint of fairness. We prove that a greedy approach is O(log n)-supermajorized by all other allocations, where n is the number of jobs. This means the greedy approach is globally O(log n)-fair. This may be contrasted with polynomial lower bounds presented by Goel et al. [2001] for fair online routing. We also define a machine-centric view of fairness using the related concept of submajorization. We prove that the greedy online algorithm is globally O(logm)-balanced, where m is the number of machines. © 2005, ACM. All rights reserved.",Algorithms; Fairness; Load balancing; Theory,
Fast Sparse Matrix Multiplication,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250883179&doi=10.1145%2f1077464.1077466&partnerID=40&md5=dcee6906afae16148ed86190f786346b,"Let A and B two n × n matrices over a ring R (e.g., the reals or the integers) each containing at most m nonzero elements. We present a new algorithm that multiplies A and B using O(m0.7n1.2 + n2+o(1)) algebraic operations (i.e., multiplications, additions and subtractions) over R. The naïve matrix multiplication algorithm, on the other hand, may need to perform Ω(mn) operations to accomplish the same task. For m ≤n1.14, the new algorithm performs an almost optimal number of only n2+o(1) operations. For m ≤ n1.68, the new algorithm is also faster than the best known matrix multiplication algorithm for dense matrices which uses O(n2.38) algebraic operations. The new algorithm is obtained using a surprisingly straightforward combination of a simple combinatorial idea and existing fast rectangular matrix multiplication algorithms.We also obtain improved algorithms for the multiplication of more than two sparse matrices. As the known fast rectangular matrix multiplication algorithms are far from being practical, our result, at least for now, is only of theoretical value. © 2005, ACM. All rights reserved.",Algorithms; Matrix multiplication; Sparse matrices; Theory,
Pricing Multicasting in More Flexible Network Models,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34848860294&doi=10.1145%2f1077464.1077469&partnerID=40&md5=afc514eddd3f9ced057a6170617db207,"The problem of designing efficient algorithms for sharing the cost of multicasting has recently received considerable attention. In this article, we examine the effect on the complexity of pricing when two flexibility-enhancing mechanisms are incorporated into the network model. In particular, we study a model where the session is offered at a number of different rates of transmission, and where there is a cost for enabling multicasting at each node of the network. We consider two techniques that have been used in practice to provide multiple rates: using a layered transmission scheme (called the layered paradigm) and using different multicast groups for each possible rate (called the split session paradigm).We demonstrate that the difference between these two paradigms has a significant impact on the complexity of pricing multicasting. For the layered paradigm, we provide a distributed algorithm for computing pricing efficiently in terms of local computation and message complexity. For the split session paradigm, on the other hand, we demonstrate that this problem can be solved in polynomial time if the number of possible rates is fixed, but if the number of rates is part of the input, then the problem becomes NP-Hard even to approximate. We also examine the effect of delivering the transmissions for the various rates from different locations within the network. We show that, in this case, the pricing problem becomes NPHard for the split session paradigm even for a fixed constant number of possible rates but if layering is used, then it can be solved in polynomial time by formulating the problem as a totally unimodular integer program. © 2005, ACM. All rights reserved.",Algorithms; Multi-Rate multicast; Overlays; Theory,
The Np-Completeness Column,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748035771&doi=10.1145%2f1077464.1077476&partnerID=40&md5=b4f543cf0dbb9b6d27d03665c958b0d9,"This is the 24th edition of a column that covers new developments in the theory of NP-completeness. The presentation is modeled on that which M. R. Garey and I used in our book “Computers and Intractability: A Guide to the Theory of NP-Completeness,” W. H. Freeman & Co., New York, 1979, hereinafter referred to as “[G & J].” Previous columns, the first 23 of which appeared in J. Algorithms, will be referred to by a combination of their sequence number and year of appearance, e.g.“[Col 1, 1981].” This edition of the column describes the history and purpose of the column and the status of the open problems from [G & J] and previous columns. © 2005, ACM. All rights reserved.",Algorithms; Coding theory; Graphs; Lattice bases; Np-Completeness; Open problems; Perfect; Primality testing; Theory,
The Greedy Algorithm for the Minimum Common String Partition Problem,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859096206&doi=10.1145%2f1103963.1103971&partnerID=40&md5=f8c845162e0ad6419c32025c6c737304,"In the Minimum Common String Partition problem (MCSP), we are given two strings on input, and we wish to partition them into the same collection of substrings, minimizing the number of the substrings in the partition. This problem is NP-hard, even for a special case, denoted 2-MCSP, where each letter occurs at most twice in each input string. We study a greedy algorithm for MCSP that at each step extracts a longest common substring from the given strings. We show that the approximation ratio of this algorithm is between Ω(n0.43) and O(n0.69). In the case of 2-MCSP, we show that the approximation ratio is equal to 3. For 4-MCSP, we give a lower bound of Ω(log n). © 2005, ACM. All rights reserved.",Algorithms; Approximation algorithms; String algorithms,
Maintaining Information in Fully Dynamic Trees with Top Trees,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748108785&doi=10.1145%2f1103963.1103966&partnerID=40&md5=df64afa096f124679ef43ba035f3bd09,"We design top trees as a new simpler interface for data structures maintaining information in a fully dynamic forest.We demonstrate how easy and versatile they are to use on a host of different applications. For example, we show how to maintain the diameter, center, and median of each tree in the forest. The forest can be updated by insertion and deletion of edges and by changes to vertex and edge weights. Each update is supported in O(log n) time, where n is the size of the tree(s) involved in the update. Also, we show how to support nearest common ancestor queries and level ancestor queries with respect to arbitrary roots in O(log n) time. Finally, with marked and unmarked vertices, we show how to compute distances to a nearest marked vertex. The latter has applications to approximate nearest marked vertex in general graphs, and thereby to static optimization problems over shortest path metrics. Technically speaking, top trees are easily implemented either with Frederickson's [1997a] topology trees or with Sleator and Tarjan's [1983] dynamic trees. However, we claim that the interface is simpler for many applications, and indeed our new bounds are quadratic improvements over previous bounds where they exist. © 2005, ACM. All rights reserved.",Algorithms; Design; Fully dynamic forest; Theory,
Analysis of Linear Combination Algorithms in Cryptography,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047214694&doi=10.1145%2f1077464.1077473&partnerID=40&md5=94379eaffdbf0e46a5115b331c06ef9a,"Several cryptosystems rely on fast calculations of linear combinations in groups. Oneway to achieve this is to use joint signed binary digit expansions of small “weight.” We study two algorithms, one based on nonadjacent forms of the coefficients of the linear combination, the other based on a certain joint sparse form specifically adapted to this problem. Both methods are sped up using the sliding windows approach combined with precomputed lookup tables.We give explicit and asymptotic results for the number of group operations needed, assuming uniform distribution of the coefficients. Expected values, variances and a central limit theorem are proved using generating functions. Furthermore, we provide a new algorithm that calculates the digits of an optimal expansion of pairs of integers from left to right. This avoids storing the whole expansion, which is needed with the previously known right-to-left methods, and allows an online computation. © 2005, ACM. All rights reserved.",Algorithms; Cryptosystems; Digital Expansions; Elliptic Curve; Hamming Weight; Online Algorithm,
Black Box for Constant-Time Insertion in Priority Queues (Note),2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846640936&doi=10.1145%2f1077464.1077471&partnerID=40&md5=12ee01dfdd2d163d74e869732b4ed54d,"We present a simple black box that takes a priority queue Q which supports find-min, insert, and delete in x-time at most t. Here x-time may be worst-case, expected, or amortized. The black-box transforms Q into a priority queue Q*. that supports find-min in constant time, insert in constant x-time, and delete in x-time O(t). Moreover, if Q supports dec-key in constant time, then so does Q*. © 2005, ACM. All rights reserved.",Algorithms; Priority queues; Shortest paths; Theory,
An O(VE) Algorithm for Ear Decompositions of Matching-Covered Graphs,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954655675&doi=10.1145%2f1103963.1103969&partnerID=40&md5=ef6d67653837d444b9dee37c49606bcd,"Our main result is an O(nm)-time (deterministic) algorithm for constructing an ear decomposition of a matching-covered graph, where n and m denote the number of nodes and edges. The improvement in the running time comes from new structural results that give a sharpened version of Lovász and Plummer's Two-Ear Theorem. Our algorithm is based on O(nm)-time algorithms for two other fundamental problems in matching theory, namely, finding all the allowed edges of a graph, and finding the canonical partition of an elementary graph. To the best of our knowledge, no faster deterministic algorithms are known for these two fundamental problems. © 2005, ACM. All rights reserved.",Algorithms; Ear decompositions; Matching theory; Theory,
Approximation Algorithms for the Capacitated Minimum Spanning Tree Problem and its Variants in Network Design,2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849138128&doi=10.1145%2f1103963.1103967&partnerID=40&md5=3444a238a66140ef131f88891e882a12,"Given an undirected graph G = (V, E) with nonnegative costs on its edges, a root node r ∈ V, a set of demands D ⊆ V with demand v ∈ D wishing to route w(v) units of flow (weight) to r, and a positive number k, the Capacitated Minimum Steiner Tree (CMStT) problem asks for a minimum Steiner tree, rooted at r, spanning the vertices in D ∪{r }, in which the sum of the vertex weights in every subtree connected to r is at most k. When D = V, this problem is known as the Capacitated Minimum Spanning Tree (CMST) problem. Both CMsT and CMST problems are NP-hard. In this article, we present approximation algorithms for these problems and several of their variants in network design. Our main results are the following:—We present a (γρST + 2)-approximation algorithm for the CMStT problem, where γ is the inverse Steiner ratio, and ρST is the best achievable approximation ratio for the Steiner tree problem. Our ratio improves the current best ratio of 2ρST + 2 for this problem.—In particular, we obtain (γ+2)-approximation ratio for theCMSTproblem, which is an improvement over the current best ratio of 4 for this problem. For points in Euclidean and rectilinear planes, our result translates into ratios of 3.1548 and 3.5, respectively.—For instances in the plane, under the Lp norm, with the vertices in D having uniform weights, we present a nontrivial (7/5ρST + 3/2)-approximation algorithm for the CMStT problem. This translates into a ratio of 2.9 for the CMST problem with uniform vertex weights in the Lp metric plane.Our ratio of 2.9 solves the long-standing open problem of obtaining any ratio better than 3 for this case.—For the CMST problem, we show how to obtain a 2-approximation for graphs in metric spaces with unit vertex weights and k = 3, 4.—For the budgeted CMST problem, in which the weights of the subtrees connected to r could be up to αk instead of k (α ≥ 1), we obtain a ratio of γ + 2/α. © 2005, ACM. All rights reserved.",Algorithms; Approximation algorithms; Minimum spanning trees; Network design; Spanning trees; Theory,
"Fixed-Parameter Algorithms for (k, r)-Center in Planar Graphs and Map Graphs",2005,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948770618&doi=10.1145%2f1077464.1077468&partnerID=40&md5=febb081f937c3d703c53e5e671b4ad0d,"The (k, r)-center problem asks whether an input graph G has ≤ k vertices (called centers) such that every vertex of G is within distance ≤ r from some center. In this article, we prove that the (k, r)-center problem, parameterized by k and r, is fixed-parameter tractable (FPT) on planar graphs, i.e., it admits an algorithm of complexity f (k, r)nO(1) where the function f is independent of n. In particular, we show that f (k, r) = 2O(r log r) √ k, where the exponent of the exponential term grows sublinearly in the number of centers. Moreover, we prove that the same type of FPT algorithms can be designed for the more general class of map graphs introduced by Chen, Grigni, and Papadimitriou. Our results combine dynamic-programming algorithms for graphs of small branchwidth and a graphtheoretic result bounding this parameter in terms of k and r. Finally, a byproduct of our algorithm is the existence of a PTAS for the r-domination problem in both planar graphs and map graphs. Our approach builds on the seminal results of Robertson and Seymour on Graph Minors, and as a result is much more powerful than the previous machinery of Alber et al. for exponential speedup on planar graphs. To demonstrate the versatility of our results, we show how our algorithms can be extended to general parameters that are “large” on grids. In addition, our use of branchwidth instead of the usual treewidth allows us to obtain much faster algorithms, and requires more complicated dynamic programming than the standard leaf/introduce/forget/join structure of nice tree decompositions. Our results are also unique in that they apply to classes of graphs that are not minor-closed, namely, constant powers of planar graphs and map graphs. © 2005, ACM. All rights reserved.","(k, r)-center; Algorithms; Domination; Fixed-Parameter algorithms; Graph; Map graph; Planar; Theory",
Introduction to the Special Issue on ACM-SIAM Symposium on Discrete Algorithms (SODA) 2020,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162003726&doi=10.1145%2f3561912&partnerID=40&md5=7ea587e6309fb97d8446539b22e1426b,[No abstract available],,
Tightening Curves on Surfaces Monotonically with Applications,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161986036&doi=10.1145%2f3558097&partnerID=40&md5=3e798286bccaab2b9a1c2d991a42d24e,"We prove the first polynomial bound on the number of monotonic homotopy moves required to tighten a collection of closed curves on any compact orientable surface, where the number of crossings in the curve is not allowed to increase at any time during the process. The best known upper bound before was exponential, which can be obtained by combining the algorithm of De Graaf and Schrijver [J. Comb. Theory Ser. B, 1997] together with an exponential upper bound on the number of possible surface maps. To obtain the new upper bound, we apply tools from hyperbolic geometry, as well as operations in graph drawing algorithms - the cluster and pipe expansions - to the study of curves on surfaces.As corollaries, we present two efficient algorithms for curves and graphs on surfaces. First, we provide a polynomial-time algorithm to convert any given multicurve on a surface into minimal position. Such an algorithm only existed for single closed curves, and it is known that previous techniques do not generalize to the multicurve case. Second, we provide a polynomial-time algorithm to reduce any k-terminal plane graph (and more generally, surface graph) using degree-1 reductions, series-parallel reductions, and ΔY-transformations for arbitrary integer k. Previous algorithms only existed in the planar setting when k ≤ 4, and all of them rely on extensive case-by-case analysis based on different values of k. Our algorithm makes use of the connection between electrical transformations and homotopy moves and thus solves the problem in a unified fashion.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bigon; braid; circular planar; cluster expansion; geodesic; geometric intersection number; hyperbolic trigonometry; medial graph; ΔY transformations,Clustering algorithms; Drawing (graphics); Expansion; Graph theory; Polynomial approximation; Bigon; Braid; Circular planar; Cluster expansion; Geodesic; Geometric intersection number; Hyperbolic trigonometry; Intersection number; Medial graph; ΔY transformation; Reduction
Optimal Bound on the Combinatorial Complexity of Approximating Polytopes,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150794745&doi=10.1145%2f3559106&partnerID=40&md5=6db7209a4cd9130499df695f1f39a9aa,"This article considers the question of how to succinctly approximate a multidimensional convex body by a polytope. Given a convex body K of unit diameter in Euclidean d-dimensional space (where d is a constant) and an error parameter ϵ > 0, the objective is to determine a convex polytope of low combinatorial complexity whose Hausdorff distance from K is at most ϵ. By combinatorial complexity, we mean the total number of faces of all dimensions. Classical constructions by Dudley and Bronshteyn/Ivanov show that O(1/ϵ(d-1)/2) facets or vertices are possible, respectively, but neither achieves both bounds simultaneously. In this article, we show that it is possible to construct a polytope with O(1/ϵ(d-1)/2) combinatorial complexity, which is optimal in the worst case.Our result is based on a new relationship between ϵ-width caps of a convex body and its polar body. Using this relationship, we are able to obtain a volume-sensitive bound on the number of approximating caps that are ""essentially different.""We achieve our main result by combining this with a variant of the witness-collector method and a novel variable-thickness layered construction of the economical cap covering. © 2022 Association for Computing Machinery.",convex polytopes; macbeath regions; Polytope approximation,Combinatorial complexity; Convex body; Convex polytopes; D-dimensional spaces; Error parameters; Euclidean; Macbeath region; Optimal bounds; Polytope approximation; Polytopes; Computational geometry
κ-Apices of Minor-closed Graph Classes. II. Parameterized Algorithms,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134716234&doi=10.1145%2f3519028&partnerID=40&md5=bae01fd7a8ea124a46210292d4025c1f,"Let be a minor-closed graph class. We say that a graph G is a k-Apex of if G contains a set S of at most k vertices such that G\S belongs to . We denote by k () the set of all graphs that are k-Apices of . In the first paper of this series, we obtained upper bounds on the size of the graphs in the minor-obstruction set of k (), i.e., the minor-minimal set of graphs not belonging to k (). In this article, we provide an algorithm that, given a graph G on n vertices, runs in time 2poly(k) (k). n3 and either returns a set S certifying that G €k (G), or reports that G g‰ Ak (G). Here poly is a polynomial function whose degree depends on the maximum size of a minor-obstruction of . In the special case where excludes some apex graph as a minor, we give an alternative algorithm running in 2poly(k)n2-Time.  © 2022 Association for Computing Machinery.",flat wall theorem; Graph minors; graph modification problems; irrelevant vertex technique; parameterized algorithms,Graph theory; Graphic methods; Apex graph; Flat wall theorem; Graph G; Graph minors; Graph modification problems; Irrelevant vertex technique; Minor-closed graph class; Parameterized algorithm; Polynomial functions; Upper Bound; Parameter estimation
Network Design for s-T Effective Resistance,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135714949&doi=10.1145%2f3522588&partnerID=40&md5=9c4c387563cecc7d8a6a0d011913a9af,"We consider a new problem of designing a network with small s-T effective resistance. In this problem, we are given an undirected graph G = (V,E), two designated vertices s,t gV, and a budget k. The goal is to choose a subgraph of G with at most k edges to minimize the s-T effective resistance. This problem is an interpolation between the shortest path problem and the minimum cost flow problem and has applications in electrical network design.We present several algorithmic and hardness results for this problem and its variants. On the hardness side, we show that the problem is NP-hard, and the weighted version is hard to approximate within a factor smaller than two assuming the small-set expansion conjecture. On the algorithmic side, we analyze a convex programming relaxation of the problem and design a constant factor approximation algorithm. The key of the rounding algorithm is a randomized path-rounding procedure based on the optimality conditions and a flow decomposition of the fractional solution. We also use dynamic programming to obtain a fully polynomial time approximation scheme when the input graph is a series-parallel graph, with better approximation ratio than the integrality gap of the convex program for these graphs.  © 2022 Association for Computing Machinery.",Effective resistance; network design,Approximation algorithms; Budget control; Convex optimization; Dynamic programming; Hardness; Polynomial approximation; Algorithmics; Effective resistances; Electrical networks; Graph G; Hardness result; Minimum cost flow problem; Network design; Shortest path problem; Subgraphs; Undirected graph; Undirected graphs
A Generalization of Self-Improving Algorithms,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163778624&doi=10.1145%2f3531227&partnerID=40&md5=2cdec7a58ecf1d84e087439f9a4caad9,"Ailon et al. [SICOMP'11] proposed self-improving algorithms for sorting and Delaunay triangulation (DT) when the input instances x1, .. , xn follow some unknown product distribution. That is, xi is drawn independently from a fixed unknown distribution i. After spending O(n1+ϵ) time in a learning phase, the subsequent expected running time is O((n + H)/ϵ), where H T {HS,HDT}, and HS and HDT are the entropies of the distributions of the sorting and DT output, respectively. In this article, we allow dependence among the xi's under the group product distribution. There is a hidden partition of [1, n] into groups; the xi's in the kth group are fixed unknown functions of the same hidden variable uk; and the uk's are drawn from an unknown product distribution. We describe self-improving algorithms for sorting and DT under this model when the functions that map uk to xi's are well-behaved. After an O(poly(n))-Time training phase, we achieve O(n + HS) and O(nα (n) + HDT) expected running times for sorting and DT, respectively, where α (·) is the inverse Ackermann function. © 2022 Association for Computing Machinery.",Delaunay triangulation; entropy; Expected running time; sorting,Triangulation; Delaunay triangulation; Expected running time; Generalisation; Hidden variable; Inverse Ackermann function; Learning phasis; Product distributions; Training phasis; Entropy
A Lower Bound on Cycle-Finding in Sparse Digraphs,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162002914&doi=10.1145%2f3417979&partnerID=40&md5=20976ed7abec36692dadd2cad408bbda,"We consider the problem of finding a cycle in a sparse directed graph G that is promised to be far from acyclic, meaning that the smallest feedback arc set, i.e., a subset of edges whose deletion results in an acyclic graph, in G is large. We prove an information-theoretic lower bound, showing that for N-vertex graphs with constant outdegree, any algorithm for this problem must make,(N5/9) queries to an adjacency list representation of G. In the language of property testing, our result is an (N5/9) lower bound on the query complexity of one-sided algorithms for testing whether sparse digraphs with constant outdegree are far from acyclic. This is the first improvement on the ω (gs N) lower bound, implicit in the work of Bender and Ron, which follows from a simple birthday paradox argument.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",directed acyclic graphs; graph algorithms; Property testing; randomized algorithms,Information theory; Acyclic graphs; Directed acyclic graph; Feedback arc sets; Graph algorithms; Graph G; Information-theoretic lower bounds; Low bound; Outdegree; Property-testing; Randomized Algorithms; Directed graphs
Tolerant Testers of Image Properties,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160924853&doi=10.1145%2f3531527&partnerID=40&md5=e9d40b5fd379566fdd6627ca7c17d153,"We initiate a systematic study of tolerant testers of image properties or, equivalently, algorithms that approximate the distance from a given image to the desired property. Image processing is a particularly compelling area of applications for sublinear-time algorithms and, specifically, property testing. However, for testing algorithms to reach their full potential in image processing, they have to be tolerant, which allows them to be resilient to noise.We design efficient approximation algorithms for the following fundamental questions: What fraction of pixels have to be changed in an image so it becomes a half-plane? A representation of a convex object? A representation of a connected object? More precisely, our algorithms approximate the distance to three basic properties (being a half-plane, convexity, and connectedness) within a small additive error ϵ, after reading poly(1/ϵ) pixels, independent of the image size. We also design an efficient agnostic proper PAC learner of convex sets (continuous and discrete) in two dimensions under the uniform distribution.Our algorithms require very simple access to the input: uniform random samples for the half-plane property and convexity, and samples from uniformly random blocks for connectedness. However, the analysis of the algorithms, especially for convexity, requires many geometric and combinatorial insights. For example, in the analysis of the algorithm for convexity, we define a set of reference polygons Pϵ such that (1) every convex image has a nearby polygon in Pϵ and (2) one can use dynamic programming to quickly compute the smallest empirical distance to a polygon in Pϵ.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Computational geometry; connectedness; convexity; half-plane; property testing; tolerant property testing,Approximation algorithms; Dynamic programming; Pixels; Set theory; Connectedness; Convexity; Half-planes; Image properties; Images processing; Property; Property-testing; Systematic study; Tolerant property testing; Computational geometry
Maintaining the Union of Unit Discs under Insertions with Near-Optimal Overhead,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163813713&doi=10.1145%2f3527614&partnerID=40&md5=6c7f12bae54e83cfa77769577482f3d7,"We present efficient dynamic data structures for maintaining the union of unit discs and the lower envelope of pseudo-lines in the plane. More precisely, we present three main results in this paper: (i)We present a linear-size data structure to maintain the union of a set of unit discs under insertions. It can insert a disc and update the union in O((k+1)log2 n) time, where n is the current number of unit discs and k is the combinatorial complexity of the structural change in the union due to the insertion of the new disc. It can also compute, within the same time bound, the area of the union after the insertion of each disc.(ii)We propose a linear-size data structure for maintaining the lower envelope of a set of x-monotone pseudo-lines. It can handle insertion/deletion of a pseudo-line in O(log2n) time; for a query point x0g.,R2, it can report, in O(log n) time, the point on the lower envelope with x-coordinate x0; and for a query point qg.,R2, it can return all k pseudo-lines lying below q in time O(log n+klog2 n).(iii)We present a linear-size data structure for storing a set of circular arcs of unit radius (not necessarily on the boundary of the union of the corresponding discs), so that for a query unit disc D, all input arcs intersecting D can be reported in O(n1/2+I + k) time, where k is the output size and I> 0 is an arbitrarily small constant. A unit-circle arc can be inserted or deleted in O(log2 n) time. © 2022 Association for Computing Machinery.",dynamic data structures; intersection searching; Lower envelopes; pseudo-lines; tentative binary search; unit discs,'current; Binary search; Dynamic data structure; Intersection searching; Lower envelopes; Near-optimal; Pseudo-lines; Query points; Tentative binary search; Unit disk; Data structures
Deterministic Leader Election in Anonymous Radio Networks,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163800927&doi=10.1145%2f3527171&partnerID=40&md5=6956fa3355aacdec12ea74a8febc1ea8,"Leader election is a fundamental task in distributed computing. It is a symmetry breaking problem, calling for one node of the network to become the leader, and for all other nodes to become non-leaders. We consider leader election in anonymous radio networks modeled as simple undirected connected graphs. Nodes communicate in synchronous rounds. In each round, a node can either transmit a message to all its neighbours, or stay silent and listen. A node v hears a message from a neighbour w in a given round if v listens in this round and if w is its only neighbour transmitting in this round. If v listens in a round in which more than one neighbour transmits, then v hears noise that is different from any message and different from silence.We assume that nodes are identical (anonymous) and execute the same deterministic algorithm. Under this scenario, symmetry can be broken only in one way: by different wake-up times of the nodes. In which situations is it possible to break symmetry and elect a leader using time as symmetry breaker? In order to answer this question, we consider configurations. A configuration is the underlying graph with nodes tagged by non-negative integers with the following meaning. A node can either wake up spontaneously in the round shown on its tag, according to some global clock, or can be woken up hearing a message sent by one of its already awoken neighbours. The local clock of a node starts at its wakeup and nodes do not have access to the global clock determining their tags. A configuration is feasible if there exists a distributed algorithm that elects a leader for this configuration.Our main result is a complete algorithmic characterization of feasible configurations. More precisely, we design a centralized decision algorithm, working in polynomial time, whose input is a configuration and which decides if the configuration is feasible. Using this algorithm we also provide a dedicated deterministic distributed leader election algorithm for each feasible configuration that elects a leader for this configuration in time O(n2σ, where n is the number of nodes and σ is the difference between the largest and smallest tag of the configuration. We then ask the question whether there exists a universal deterministic distributed algorithm electing a leader for all feasible configurations. The answer turns out to be no, and we show that such a universal algorithm cannot exist even for the class of 4-node feasible configurations. We also prove that a distributed version of our decision algorithm cannot exist. © 2022 Association for Computing Machinery.",algorithm; anonymous radio network; graph; Leader election,Audition; Distributed computer systems; Graph theory; Polynomial approximation; Radio; Wakes; Anonymous radio network; Decision algorithms; Deterministics; Global clocks; Graph; Leader election; Radio networks; Simple++; Symmetry breakings; Undirected connected graphs; Clocks
Detecting Feedback Vertex Sets of Size k in Og(2.7k) Time,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162035382&doi=10.1145%2f3504027&partnerID=40&md5=e4abc887d0e26b7c56b1e41f7e147328,"In the Feedback Vertex Set (FVS) problem, one is given an undirected graph G and an integer k, and one needs to determine whether there exists a set of k vertices that intersects all cycles of G (a so-called feedback vertex set). Feedback Vertex Set is one of the most central problems in parameterized complexity: It served as an excellent testbed for many important algorithmic techniques in the field such as Iterative Compression [Guo et al. (JCSS'06)], Randomized Branching [Becker et al. (J. Artif. Intell. Res'00)] and Cut Count [Cygan et al. (FOCS'11)]. In particular, there has been a long race for the smallest dependence f(k) in run times of the type Og (f(k)), where the Og notation omits factors polynomial in n. This race seemed to have reached a conclusion in 2011, when a randomized Og (3k) time algorithm based on Cut Count was introduced.In this work, we show the contrary and give a Og (2.7k) time randomized algorithm. Our algorithm combines all mentioned techniques with substantial new ideas: First, we show that, given a feedback vertex set of size k of bounded average degree, a tree decomposition of width (1-ω (1))k can be found in polynomial time. Second, we give a randomized branching strategy inspired by the one from [Becker et al. (J. Artif. Intell. Res'00)] to reduce to the aforementioned bounded average degree setting. Third, we obtain significant run time improvements by employing fast matrix multiplication.  © 2022 Association for Computing Machinery.",Feedback vertex set; parameterized complexity; treewidth,Computational complexity; Iterative methods; Parallel processing systems; Polynomial approximation; Trees (mathematics); Algorithmic techniques; Average degree; Central problems; Feedback vertex set; Feedback Vertex Set problems; Graph G; Parameterized complexity; Runtimes; Tree-width; Undirected graph; Undirected graphs
Generic Techniques for Building Top-k Structures,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162019063&doi=10.1145%2f3546074&partnerID=40&md5=147b891db50faaaba75e08d711f6b8ce,"A reporting query returns the objects satisfying a predicate q from an input set. In prioritized reporting, each object carries a real-valued weight (which can be query dependent), and a query returns the objects that satisfy q and have weights at least a threshold τ. A top-k query finds, among all the objects satisfying q, the k ones of the largest weights; a max query is a special instance with k = 1. We want to design data structures of small space to support queries (and possibly updates) efficiently.Previous work has shown that a top-k structure can also support max and prioritized queries with no performance deterioration. This article explores the opposite direction: do prioritized queries, possibly combined with max queries, imply top-k search? Subject to mild conditions, we provide affirmative answers with two reduction techniques. The first converts a prioritized structure into a static top-k structure with the same space complexity and only a logarithmic blowup in query time. If a max structure is available in addition, our second reduction yields a top-k structure with no degradation in expected performance (this holds for the space, query, and update complexities). Our techniques significantly simplify the design of top-k structures because structures for max and prioritized queries are often easier to obtain. We demonstrate this by developing top-k structures for interval stabbing, 3D dominance, halfspace reporting, linear ranking, and L∞ nearest neighbor search in the RAM and the external memory computation models.  © 2022 Association for Computing Machinery.",algorithms; data structures; reductions; Top-k,Data structures; Deterioration; Random access storage; Structural design; % reductions; Condition; Design data; Input set; Max-query; Performance deterioration; Reduction techniques; Space complexity; Top-k; Top-k query; Nearest neighbor search
Improving the Dilation of a Metric Graph by Adding Edges,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163721560&doi=10.1145%2f3517807&partnerID=40&md5=02db83986031236e1c95cfec8173ef49,"Most of the literature on spanners focuses on building the graph from scratch. This article instead focuses on adding edges to improve an existing graph. A major open problem in this field is: Given a graph embedded in a metric space, and a budget of k edges, which k edges do we add to produce a minimum-dilation graph? The special case where k=1 has been studied in the past, but no major breakthroughs have been made for k > 1. We provide the first positive result, an O(k)-Approximation algorithm that runs in O(n3 log n) time.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximation algorithms; Computational geometry; greedy spanner,Budget control; Computational geometry; Greedy spanne; K-approximation algorithm; Metric spaces; Approximation algorithms
Exponential Separations in Local Privacy,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162226206&doi=10.1145%2f3459095&partnerID=40&md5=2677689a1cad2ae8b495889e45aa251c,"We prove a general connection between the communication complexity of two-player games and the sample complexity of their multi-player locally private analogues. We use this connection to prove sample complexity lower bounds for locally differentially private protocols as straightforward corollaries of results from communication complexity. In particular, we (1) use a communication lower bound for the hidden layers problem to prove an exponential sample complexity separation between sequentially and fully interactive locally private protocols, and (2) use a communication lower bound for the pointer chasing problem to prove an exponential sample complexity separation between k-round and (k+1)-round sequentially interactive locally private protocols, for every k.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",communication complexity; Local differential privacy,Computational complexity; Communication complexity; Differential privacies; Exponentials; Hidden layers; Local differential privacy; Low bound; Private protocols; Sample complexity; Two-player games; Game theory
Sticky Brownian Rounding and its Applications to Constraint Satisfaction Problems,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159825331&doi=10.1145%2f3459096&partnerID=40&md5=d2ade9ff7990c34f76795d2e049370d3,"Semidefinite programming is a powerful tool in the design and analysis of approximation algorithms for combinatorial optimization problems. In particular, the random hyperplane rounding method of Goemans and Williamson [31] has been extensively studied for more than two decades, resulting in various extensions to the original technique and beautiful algorithms for a wide range of applications. Despite the fact that this approach yields tight approximation guarantees for some problems, e.g., Max-Cut, for many others, e.g., Max-SAT and Max-DiCut, the tight approximation ratio is still unknown. One of the main reasons for this is the fact that very few techniques for rounding semi-definite relaxations are known. In this work, we present a new general and simple method for rounding semi-definite programs, based on Brownian motion. Our approach is inspired by recent results in algorithmic discrepancy theory. We develop and present tools for analyzing our new rounding algorithms, utilizing mathematical machinery from the theory of Brownian motion, complex analysis, and partial differential equations. Focusing on constraint satisfaction problems, we apply our method to several classical problems, including Max-Cut, Max-2SAT, and Max-DiCut, and derive new algorithms that are competitive with the best known results. To illustrate the versatility and general applicability of our approach, we give new approximation algorithms for the Max-Cut problem with side constraints that crucially utilizes measure concentration results for the Sticky Brownian Motion, a feature missing from hyperplane rounding and its generalizations.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Brownian motion; constraint satisfaction problems; Semidefinite programming,Approximation algorithms; Combinatorial optimization; Computation theory; Constraint satisfaction problems; Geometry; Brownian; Combinatorial optimization problems; Constraint-satisfaction problems; Design and analysis; ITS applications; MAX CUT; Max-cut; Max-SAT; Rounding method; Semi-definite programming; Brownian movement
Hypergraph Isomorphism for Groups with Restricted Composition Factors,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141636056&doi=10.1145%2f3527667&partnerID=40&md5=a73a0f60111cf89f220c6490f044c3f3,"We consider the isomorphism problem for hypergraphs taking as input two hypergraphs over the same set of vertices V and a permutation group T""over domain V, and asking whether there is a permutation γϵ T""that proves the two hypergraphs to be isomorphic. We show that for input groups, all of whose composition factors are isomorphic to a subgroup of the symmetric group on d points, this problem can be solved in time (n + m)O((log d)c) for some absolute constant c where n denotes the number of vertices and m the number of hyperedges. In particular, this gives the currently fastest isomorphism test for hypergraphs in general. The previous best algorithm for this problem due to Schweitzer and Wiebking (STOC 2019) runs in time nO(d)mO(1).As an application of this result, we obtain, for example, an algorithm testing isomorphism of graphs excluding K3,h (h ≥ 3) as a minor in time nO((log h)c). In particular, this gives an isomorphism test for graphs of Euler genus at most g running in time nO((log g)c). © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bounded genus graphs; Graph isomorphism; groups with restricted composition factors; hypergraphs,Graph theory; Bounded-genus graphs; Graph isomorphism; Group with restricted composition factor; Hyper graph; Hyperedges; Hypergraph isomorphisms; Isomorphism problems; Permutation group; Schweitzer; Symmetric groups; Set theory
Online Algorithms for Weighted Paging with Predictions,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162064393&doi=10.1145%2f3548774&partnerID=40&md5=89520ca5f1642fdb4b1c16c65f107f19,"In this article, we initiate the study of the weighted paging problem with predictions. This continues the recent line of work in online algorithms with predictions, particularly that of Lykouris and Vassilvitski (ICML 2018) and Rohatgi (SODA 2020) on unweighted paging with predictions. We show that unlike unweighted paging, neither a fixed lookahead nor a knowledge of the next request for every page is sufficient information for an algorithm to overcome the existing lower bounds in weighted paging. However, a combination of the two, which we call strong per request prediction (SPRP), suffices to give a 2-competitive algorithm. We also explore the question of gracefully degrading algorithms with increasing prediction error, and give both upper and lower bounds for a set of natural measures of prediction error.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",algorithms with predictions; Weighted paging,Algorithm with prediction; Competitive algorithms; Low bound; Natural measure; On-line algorithms; Prediction errors; Upper and lower bounds; Weighted paging; Forecasting
A Faster Algorithm for Finding Tarski Fixed Points,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147332544&doi=10.1145%2f3524044&partnerID=40&md5=4989ee2be47dcfa392e9b540ded25e95,"Dang et al. have given an algorithm that can find a Tarski fixed point in a k-dimensional lattice of width n using O(log k n) queries [2]. Multiple authors have conjectured that this algorithm is optimal [2, 7], and indeed this has been proven for two-dimensional instances [7]. We show that these conjectures are false in dimension three or higher by giving an O(log2 n) query algorithm for the three-dimensional Tarski problem. We also give a new decomposition theorem for k-dimensional Tarski problems which, in combination with our new algorithm for three dimensions, gives an O(log2 [k/3]permil; n) query algorithm for the k-dimensional problem.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Query complexity; Tarski fixed points; total function problem,Computational geometry; Decomposition theorems; Fast algorithms; Fixed points; Multiple authors; Query algorithms; Query complexity; Tarski fixed point; Three dimensions; Total function problem; Two-dimensional; Domain decomposition methods
Rapid Mixing from Spectral Independence beyond the Boolean Domain,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161034893&doi=10.1145%2f3531008&partnerID=40&md5=b5a8d07e776d510c38c3926167764855,"We extend the notion of spectral independence (introduced by Anari, Liu, and Oveis Gharan [4]) from the Boolean domain to general discrete domains. This property characterises distributions with limited correlations and implies that the corresponding Glauber dynamics is rapidly mixing. As a concrete application, we show that Glauber dynamics for sampling proper q-colourings mixes in polynomial-Time for the family of triangle-free graphs with maximum degree Δprovided q≥ (α∗ +)Δwhere α∗≈ 1.763 is the unique solution to α∗ = exp (1/α∗) and THORN 0 is any constant. This is the first efficient algorithm for sampling proper q-colourings in this regime with possibly unbounded "". Our main tool of establishing spectral independence is the recursive coupling by Goldberg, Martin, and Paterson [25]. © 2022 Association for Computing Machinery.",Graph colouring; Markov chain Monte Carlo; spectral independence,Markov processes; Mixing; Boolean domain; Discrete domains; Glauber dynamics; Graph colorings; Markov chain Monte Carlo; Markov Chain Monte-Carlo; Property; Rapid mixing; Rapidly mixing; Spectral independence; Polynomial approximation
Dynamic Geometric Set Cover and Hitting Set,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148979163&doi=10.1145%2f3551639&partnerID=40&md5=f0e530199f2511b2f813f2ffb581560a,"We investigate dynamic versions of geometric set cover and hitting set where points and ranges may be inserted or deleted, and we want to efficiently maintain an (approximately) optimal solution for the current problem instance. While their static versions have been extensively studied in the past, surprisingly little is known about dynamic geometric set cover and hitting set. For instance, even for the most basic case of one-dimensional interval set cover and hitting set, no nontrivial results were known. The main contribution of our article are two frameworks that lead to efficient data structures for dynamically maintaining set covers and hitting sets in g1 and g2. The first framework uses bootstrapping and gives a (1 + ϵ)-approximate data structure for dynamic interval set cover in g1 with O(nα / ϵ) amortized update time for any constant α > 0; in g2, this method gives O(1)-approximate data structures for unit-square set cover and hitting set with O(n1/2+α) amortized update time. The second framework uses local modification and leads to a (1 + ϵ)-approximate data structure for dynamic interval hitting set in g1 with Õ(1/ϵ) amortized update time; in g2, it gives O(1)-approximate data structures for unit-square set cover and hitting set in the partially dynamic settings with Õ(1) amortized update time.  © 2022 Association for Computing Machinery.",dynamic data structures; geometric hitting set; Geometric set cover,Computation theory; Geometry; Current problems; Dynamic data structure; Geometric hitting set; Geometric set cover; Geometric sets; Hitting sets; Interval sets; Optimal solutions; Set-cover; Unit squares; Data structures
Solving Connectivity Problems Parameterized by Treewidth in Single Exponential Time,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128292072&doi=10.1145%2f3506707&partnerID=40&md5=1d72b2ec11e8ffc3c2b2bf33e3e1e136,"For the vast majority of local problems on graphs of small treewidth (where, by local we mean that a solution can be verified by checking separately the neighbourhood of each vertex), standard dynamic programming techniques give ctw |V|O(1) time algorithms, where tw is the treewidth of the input graph G = (V,E) and c is a constant. On the other hand, for problems with a global requirement (usually connectivity) the best-known algorithms were naive dynamic programming schemes running in at least twtw time.We bridge this gap by introducing a technique we named Cut&Count that allows to produce ctw |V|O(1) time Monte-Carlo algorithms for most connectivity-Type problems, including Hamiltonian Path , Steiner Tree , Feedback Vertex Set and Connected Dominating Set . These results have numerous consequences in various fields, like parameterized complexity, exact and approximate algorithms on planar and H-minor-free graphs and exact algorithms on graphs of bounded degree. The constant c in our algorithms is in all cases small, and in several cases we are able to show that improving those constants would cause the Strong Exponential Time Hypothesis to fail. In all these fields we are able to improve the best-known results for some problems. Also, looking from a more theoretical perspective, our results are surprising since the equivalence relation that partitions all partial solutions with respect to extendability to global solutions seems to consist of at least twtw equivalence classes for all these problems. Our results answer an open problem raised by Lokshtanov, Marx and Saurabh [SODA'11].In contrast to the problems aimed at minimizing the number of connected components that we solve using Cut&Count as mentioned above, we show that, assuming the Exponential Time Hypothesis, the aforementioned gap cannot be bridged for some problems that aim to maximize the number of connected components like Cycle Packing .  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",connectivity problems; feedback vertex set; hamilton path/cycle; Isolation Lemma; steiner tree; Treewidth,C (programming language); Equivalence classes; Monte Carlo methods; Parameterization; Trees (mathematics); Connected component; Connectivity problems; Exact algorithms; Feedback vertex set; Hamilton path; Hamilton path/cycle; Isolation lemmata; Parameterized; Steiner trees; Tree-width; Dynamic programming
Fully Dynamic (Δ+1)-Coloring in O(1) Update Time,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128271484&doi=10.1145%2f3494539&partnerID=40&md5=fb8395da9a52bc9e837a9da96fb9e09d,"The problem of (Δ+1)-vertex coloring a graph of maximum degree Δhas been extremely well studied over the years in various settings and models. Surprisingly, for the dynamic setting, almost nothing was known until recently. In SODA'18, Bhattacharya, Chakrabarty, Henzinger and Nanongkai devised a randomized algorithm for maintaining a (Δ+1)-coloring with O(log "") expected amortized update time. In this article, we present an improved randomized algorithm for (Δ+1)-coloring that achieves O(1) amortized update time and show that this bound holds not only in expectation but also with high probability.Our starting point is the state-of-The-Art randomized algorithm for maintaining a maximal matching (Solomon, FOCS'16). We carefully build on the approach of Solomon, but, due to inherent differences between the maximal matching and (Δ+1)-coloring problems, we need to deviate significantly from it in several crucial and highly nontrivial points.1  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic graph algorithms; Graph coloring,Coloring; Coloring problems; Dynamic graph algorithms; Dynamic settings; Graph colorings; High probability; Maximal matchings; Maximum degree; Randomized Algorithms; State of the art; Vertex coloring; Graph theory
4 vs 7 Sparse Undirected Unweighted Diameter Is SETH-hard at Timen4/3,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128253017&doi=10.1145%2f3494540&partnerID=40&md5=b0f206fdd9516a3c0b50c9d0e04dfb41,"We show, assuming the Strong Exponential Time Hypothesis, that for every ϵ > 0, approximating undirected unweighted Diameter on n-vertex m-edge graphs within ratio 7/4-ϵ requires m4/3-o(1) time, even when m = Õ(n). This is the first result that conditionally rules out a near-linear time 5/3-Approximation for undirected Diameter.  © 2022 Association for Computing Machinery.",Diameter; inapproximability; k-Orthogonal Vectors; SETH lower bounds,Diameter; Inapproximability; K-orthogonal vector; Low bound; Near-linear time; Orthogonal vectors; SETH low bound; Strong exponential time hypothesis
Exact Distance Oracles for Planar Graphs with Failing Vertices,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127324375&doi=10.1145%2f3511541&partnerID=40&md5=2c38b397e41be893458e994d149a089c,"We consider exact distance oracles for directed weighted planar graphs in the presence of failing vertices. Given a source vertex u, a target vertex v and a set X of k failed vertices, such an oracle returns the length of a shortest u-To-v path that avoids all vertices in X. We propose oracles that can handle any number k of failures. We show several tradeoffs between space, query time, and preprocessing time. In particular, for a directed weighted planar graph with n vertices and any constant k, we show an Õ(n)-size, Õ(gš n)-query-Time oracle.1 We then present a space vs. query time tradeoff: for any q ϵ [ 1,gš n ], we propose an oracle of size nk+1+o(1)/q2k that answers queries in Õ(q) time. For single vertex failures (k = 1), our n2+o(1)/q2-size, Õ(q)-query-Time oracle improves over the previously best known tradeoff of Baswana et al. SODA 2012 by polynomial factors for q ≥ nt, for any t g (0,1/2]. For multiple failures, no planarity exploiting results were previously known.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",distance oracles; Planar graphs; shortest paths; Voronoi diagrams,Computation theory; Directed graphs; Distance oracle; Multiple failures; Planar graph; Polynomial factor; Preprocessing time; Query time; Short-path; Single vertex; Voronoi diagrams; Weighted planar graphs; Graphic methods
Optimal Parameterized Algorithms for Planar Facility Location Problems Using Voronoi Diagrams,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128289010&doi=10.1145%2f3483425&partnerID=40&md5=94386518e4def10c28a64dce7115f463,"We study a general family of facility location problems defined on planar graphs and on the two-dimensional plane. In these problems, a subset of k objects has to be selected, satisfying certain packing (disjointness) and covering constraints. Our main result is showing that, for each of these problems, the nO(gšk) time brute force algorithm of selecting k objects can be improved to nO(gšk) time. The algorithm is based on an idea that was introduced recently in the design of geometric QPTASs, but was not yet used for exact algorithms and for planar graphs. We focus on the Voronoi diagram of a hypothetical solution of k objects, guess a balanced separator cycle of this Voronoi diagram to obtain a set that separates the solution in a balanced way, and then recurse on the resulting subproblems.The following list is an exemplary selection of concrete consequences of our main result. We can solve each of the following problems in time nO(gš k), where n is the total size of the input:d-Scattered Set: find k vertices in an edge-weighted planar graph that pairwise are at distance at least d from each other (d is part of the input).d-Dominating Set (or (k,d)-Center): find k vertices in an edge-weighted planar graph such that every vertex of the graph is at distance at most d from at least one selected vertex (d is part of the input).Given a set D of connected vertex sets in a planar graph G, find k disjoint vertex sets in D.Given a set D of disks in the plane (of possibly different radii), find k disjoint disks in D.Given a set D of simple polygons in the plane, find k disjoint polygons in D.Given a set D of disks in the plane (of possibly different radii) and a set P of points, find k disks in D that together cover the maximum number of points in P.Given a set D of axis-parallel squares in the plane (of possibly different sizes) and a set P of points, find k squares in D that together cover the maximum number of points in P.It is known from previous work that, assuming the Exponential Time Hypothesis (ETH), there is no f(k)no(gš k) time algorithm for any computable function f for any of these problems. Furthermore, we give evidence that packing problems have nO(gš k) time algorithms for a much more general class of objects than covering problems have. For example, we show that, assuming ETH, the problem where a set D of axis-parallel rectangles and a set P of points are given, and the task is to select k rectangles that together cover the entire point set, does not admit an f(k)no(k) time algorithm for any computable function f.  © 2022 Association for Computing Machinery.",Facility location; packing problems; parameterized complexity; Voronoi diagrams,Computational geometry; Constraint satisfaction problems; Graph theory; Location; Parameter estimation; Parameterization; Exponential time hypothesis; Facilities locations; Facility location problem; Packing problems; Parameterized complexity; Planar graph; Time algorithms; Vertex set; Voronoi diagrams; Weighted planar graphs; Graphic methods
Quasipolynomial Multicut-mimicking Networks and Kernels for Multiway Cut Problems,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128323016&doi=10.1145%2f3501304&partnerID=40&md5=07663b68827cdd96f25d517db0d52406,"We show the existence of an exact mimicking network of kO(log k) edges for minimum multicuts over a set of terminals in an undirected graph, where k is the total capacity of the terminals, i.e., the sum of the degrees of the terminal vertices. Furthermore, using the best available approximation algorithm for Small Set Expansion, we show that a mimicking network of kO(log3 k) edges can be computed in randomized polynomial time. As a consequence, we show quasipolynomial kernels for several problems, including Edge Multiway Cut, Group Feedback Edge Set for an arbitrary group, and Edge Multicut parameterized by the solution size and the number of cut requests. The result combines the matroid-based irrelevant edge approach used in the kernel for s-Multiway Cut with a recursive decomposition and sparsification of the graph along sparse cuts. This is the first progress on the kernelization of Multiway Cut problems since the kernel for s-Multiway Cut for constant value of s (Kratsch and Wahlström, FOCS 2012).  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph separation; Kernelization; sparsification,Approximation algorithms; Computation theory; Undirected graphs; Graph separation; Kernelization; Minimum multicut; Multicuts; Multiway cut; MULTIWAY CUT problems; Quasipolynomials; Sparsification; Terminal vertices; Undirected graph; Polynomial approximation
Constant-Time Dynamic (Δ+1)-Coloring,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128282128&doi=10.1145%2f3501403&partnerID=40&md5=929a70bb3ae8d6a056bee65cb0fa2a8a,"We give a fully dynamic (Las-Vegas style) algorithm with constant expected amortized time per update that maintains a proper (Δ+1)-vertex coloring of a graph with maximum degree at most "". This improves upon the previous O(log "")-Time algorithm by Bhattacharya et al. (SODA 2018). Our algorithm uses an approach based on assigning random ranks to vertices and does not need to maintain a hierarchical graph decomposition. We show that our result does not only have optimal running time but is also optimal in the sense that already deciding whether a ""-coloring exists in a dynamically changing graph with maximum degree at most Δtakes ω (log n) time per operation.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Dynamic graph algorithms; graph coloring; random sampling,Computation theory; Amortized time; Constant time; Dynamic graph algorithms; Graph colorings; Las Vegas; Maximum degree; Random sampling; Time algorithms; Time dynamic; Vertex coloring; Graph theory
Computing the Inverse Geodesic Length in Planar Graphs and Graphs of Bounded Treewidth,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128316283&doi=10.1145%2f3501303&partnerID=40&md5=83028547b444c151f55ae4a8ff0a7ee1,"The inverse geodesic length of a graph G is the sum of the inverse of the distances between all pairs of distinct vertices of G. In some domains, it is known as the Harary index or the global efficiency of the graph. We show that, if G is planar and has n vertices, then the inverse geodesic length of G can be computed in roughly O(n9/5) time. We also show that, if G has n vertices and treewidth at most k, then the inverse geodesic length of G can be computed in O(n log O(k)n) time. In both cases, we use techniques developed for computing the sum of the distances, which does not have ""inverse""component, together with batched evaluations of rational functions.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",algebraic tools; bounded treewidth; distances in graphs; IGL; planar graphs,Computation theory; Geodesy; Graphic methods; Rational functions; Algebraic tool; Bounded treewidth; Distance in graphs; Global efficiency; Graph G; IGL; Inverse component; Planar graph; Tree-width; Graph theory
Efficient Shortest Paths in Scale-Free Networks with Underlying Hyperbolic Geometry,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128315056&doi=10.1145%2f3516483&partnerID=40&md5=4a9ef1e02e08a5ae53a8e8f3b05d2614,"A standard approach to accelerating shortest path algorithms on networks is the bidirectional search, which explores the graph from the start and the destination, simultaneously. In practice this strategy performs particularly well on scale-free real-world networks. Such networks typically have a heterogeneous degree distribution (e.g., a power-law distribution) and high clustering (i.e., vertices with a common neighbor are likely to be connected themselves). These two properties can be obtained by assuming an underlying hyperbolic geometry.To explain the observed behavior of the bidirectional search, we analyze its running time on hyperbolic random graphs and prove that it is Õ(n2-1/α + n1/(2α) + δmax) with high probability, where α g (1/2, 1) controls the power-law exponent of the degree distribution, and max is the maximum degree. This bound is sublinear, improving the obvious worst-case linear bound. Although our analysis depends on the underlying geometry, the algorithm itself is oblivious to it.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bidirectional shortest path; hyperbolic geometry; Random graphs; scale-free networks,Complex networks; Computation theory; Graph theory; Probability distributions; Bi-directional search; Bidirectional short path; Degree distributions; Hyperbolic geometry; Random graphs; Real-world networks; Scale free networks; Scale-free; Short-path; Shortest path algorithms; Geometry
Time Dependent Biased Random Walks,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128314252&doi=10.1145%2f3498848&partnerID=40&md5=17c92083f0e03db7d6ad5c5e763dc9f8,"We study the biased random walk where at each step of a random walk a ""controller""can, with a certain small probability, move the walk to an arbitrary neighbour. This model was introduced by Azar et al. [STOC'1992]; we extend their work to the time dependent setting and consider cover times of this walk. We obtain new bounds on the cover and hitting times. Azar et al. conjectured that the controller can increase the stationary probability of a vertex from p to p1-ϵ; while this conjecture is not true in full generality, we propose a best-possible amended version of this conjecture and confirm it for a broad class of graphs. We also consider the problem of computing an optimal strategy for the controller to minimise the cover time and show that for directed graphs determining the cover time is PSPACE-complete.  © 2022 Association for Computing Machinery.",cover time; Markov chain; Markov decision process; PSPACE; Random walk,Directed graphs; Markov processes; Biased random walk; Cover time; Hitting time; Markov Decision Processes; Optimal strategies; PSPACE; PSPACE-complete; Random Walk; Time dependent; Controllers
I/O-Efficient Algorithms for Topological Sort and Related Problems,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151842896&doi=10.1145%2f3418356&partnerID=40&md5=7e43a9bd609659c936b44b127f52e77b,"This article presents I/O-efficient algorithms for topologically sorting a directed acyclic graph and for the more general problem identifying and topologically sorting the strongly connected components of a directed graph G = (V, E). Both algorithms are randomized and have I/O-costs O(sort(E) · poly(log V)), with high probability, where sort(E) = O(E/B log M/B(E/B)) is the I/O cost of sorting an |E|-element array on a machine with size-B blocks and size-M cache/internal memory. These are the first algorithms for these problems that do not incur at least one I/O per vertex, and as such these are the first I/O-efficient algorithms for sparse graphs. By applying the technique of time-forward processing, these algorithms also imply I/O-efficient algorithms for most problems on directed acyclic graphs, such as shortest paths, as well as the single-source reachability problem on arbitrary directed graphs.  © 2022 Association for Computing Machinery.",external memory; graph algorithm; I/O efficient; strongly connected components; topological sort,Cache memory; Graph algorithms; Graphic methods; Acyclic graphs; Element array; External memory; Graph algorithms; Graph G; High probability; I/O efficient; Internal memory; Strongly connected component; Topological sort; Directed graphs
SETH-based Lower Bounds for Subset Sum and Bicriteria Path,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125689407&doi=10.1145%2f3450524&partnerID=40&md5=75bb858f2b5a4c9071d048b5ed4264d8,"Subset Sumand k-SAT are two of the most extensively studied problems in computer science, and conjectures about their hardness are among the cornerstones of fine-grained complexity. An important open problem in this area is to base the hardness of one of these problems on the other.Our main result is a tight reduction from k-SAT to Subset Sum on dense instances, proving that Bellman's 1962 pseudo-polynomial O∗(T)-time algorithm for Subset Sum on n numbers and target T cannot be improved to time T1-ϵ · 2o(n) for any ϵ > 0, unless the Strong Exponential Time Hypothesis (SETH) fails.As a corollary, we prove a ""Direct-OR""theorem for Subset Sum under SETH, offering a new tool for proving conditional lower bounds: It is now possible to assume that deciding whether one out of N given instances of Subset Sum is a YES instance requires time (N T)1-o(1). As an application of this corollary, we prove a tight SETH-based lower bound for the classical Bicriteria s,t-Path problem, which is extensively studied in Operations Research. We separate its complexity from that of Subset Sum: On graphs with m edges and edge lengths bounded by L, we show that the O(Lm) pseudo-polynomial time algorithm by Joksch from 1966 cannot be improved to Õ(L + m), in contrast to a recent improvement for Subset Sum (Bringmann, SODA 2017).  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bicriteria shortest path; fine-grained complexity; Strong Exponential Time Hypothesis; Subset sum,Operations research; Polynomial approximation; Set theory; Bi-criteria; Bicriterion shortest paths; Fine grained; Fine-grained complexity; Low bound; Path problems; Strong exponential time hypothesis; Subset sum; Tight reductions; Time algorithms; Hardness
"Tight Bounds for ĝ.,""1Oblivious Subspace Embeddings",2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151799913&doi=10.1145%2f3477537&partnerID=40&md5=9f033f285268512d49f8cee04bb55b85,"An oblivious subspace embedding is a distribution over matrices such that for any fixed matrix A, (equation presented) where r is the dimension of the embedding, is the distortion of the embedding, and for an n-dimensional vector y, is the -norm. Another important property is the sparsity of , that is, the maximum number of non-zero entries per column, as this determines the running time of computing . While for there are nearly optimal tradeoffs in terms of the dimension, distortion, and sparsity, for the important case of , much less was known. In this article, we obtain nearly optimal tradeoffs for oblivious subspace embeddings, as well as new tradeoffs for . Our main results are as follows:(1)We show for every , any oblivious subspace embedding with dimension r has distortion (equation presented) When in applications, this gives a lower bound, and shows the oblivious subspace embedding of Sohler and Woodruff (STOC, 2011) for is optimal up to factors.(2)We give sparse oblivious subspace embeddings for every . Importantly, for , we achieve , and non-zero entries per column. The best previous construction with is due to Woodruff and Zhang (COLT, 2013), giving or and ; in contrast our and are optimal up to factors even for dense matrices. We also give (1) oblivious subspace embeddings with an expected number of non-zero entries per column for arbitrarily small , and (2) the first oblivious subspace embeddings for with -distortion and dimension independent of n. Oblivious subspace embeddings are crucial for distributed and streaming environments, as well as entrywise low-rank approximation. Our results give improved algorithms for these applications.  © 2022 Association for Computing Machinery.","linear regression; Subspace embedding; ĝ.,""<sub>p</sub>norm","Approximation theory; Commerce; Computation theory; Matrix algebra; Vectors; Dimensional vectors; Embeddings; Low bound; matrix; Optimal tradeoffs; Property; Running time; Subspace embedding; Tight bound; Ĝ,""pnorm; Embeddings"
A Learned Approach to Design Compressed Rank/Select Data Structures,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160596513&doi=10.1145%2f3524060&partnerID=40&md5=f746ae85b67a61c4f87d37c35d3cd38a,"We address the problem of designing, implementing, and experimenting with compressed data structures that support rank and select queries over a dictionary of integers. We shine a new light on this classical problem by showing a connection between the input integers and the geometry of a set of points in a Cartesian plane suitably derived from them. We then build upon some results in computational geometry to introduce the first compressed rank/select dictionary based on the idea of “learning” the distribution of such points via proper linear approximations (LA). We therefore call this novel data structure the la_vector. We prove time and space complexities of the la_vector in several scenarios: in the worst case, in the case of input distributions with finite mean and variance, and taking into account the kth order entropy of some of its building blocks. We also discuss improved hybrid data structures, namely, ones that suitably orchestrate known compressed rank/select dictionaries with the la_vector. We corroborate our theoretical results with a large set of experiments over datasets originating from a variety of applications (Web search, DNA sequencing, information retrieval, and natural language processing) and show that our approach provides new interesting space-time tradeoffs with respect to many well-established compressed rank/select dictionary implementations. In particular, we show that our select is the fastest, and our rank is on the space-time Pareto frontier. © 2022 Copyright held by the owner/author(s).",algorithm engineering; Compressed data structures; high order entropy; piecewise linear approximations; rank/select dictionaries,Approximation algorithms; Data structures; DNA sequences; Entropy; Gene encoding; Large dataset; Learning algorithms; Natural language processing systems; Piecewise linear techniques; Query processing; Sensitive data; Algorithm engineering; Cartesian plane; Classical problems; Compressed data structures; High order entropy; High-order; Higher-order; Linear approximations; Piecewise linear approximations; Rank/select dictionary; Computational geometry
Introduction to the ACM-SIAM Symposium on Discrete Algorithms (SODA) 2019 Special Issue,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151903312&doi=10.1145%2f3508460&partnerID=40&md5=0e30606db807d3af0ab8c230a6a17a50,[No abstract available],,
Polynomial-time Algorithm for Maximum Weight Independent Set on P6-free Graphs,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151847768&doi=10.1145%2f3414473&partnerID=40&md5=7a6db5291dfab5eeb7ef51ca5b7a2674,"In the classic Maximum Weight Independent Set problem, we are given a graph G with a nonnegative weight function on its vertices, and the goal is to find an independent set in G of maximum possible weight. While the problem is NP-hard in general, we give a polynomial-time algorithm working on any P6-free graph, that is, a graph that has no path on 6 vertices as an induced subgraph. This improves the polynomial-time algorithm on P5-free graphs of Lokshtanov et al. [15] and the quasipolynomial-time algorithm on P6-free graphs of Lokshtanov et al. [14]. The main technical contribution leading to our main result is enumeration of a polynomial-size family ĝ.,± of vertex subsets with the following property: For every maximal independent set I in the graph, ĝ.,± contains all maximal cliques of some minimal chordal completion of G that does not add any edge incident to a vertex of I.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",maximum independent set; P<sub>6</sub>-free graphs; potential maximal clique,Graph theory; Polynomial approximation; Set theory; Free graphs; Graph G; Maximal clique; Maximum independent sets; Maximum weight independent set problems; Maximum weight independent sets; Non negatives; P6-free graph; Polynomial-time algorithms; Potential maximal clique; Graphic methods
Max Flows in Planar Graphs with Vertex Capacities,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141197361&doi=10.1145%2f3504032&partnerID=40&md5=22258f12c28dad1cdb93de828d067f21,"We consider the maximum flow problem in directed planar graphs with capacities on both vertices and arcs and with multiple sources and sinks. We present three algorithms when the capacities are integers. The first algorithm runs in O(min {k2 n, n log3 n + kn}) time when all capacities are bounded by a constant, where n is the number of vertices in the graph, and k is the number of terminals. This algorithm is the first to solve the vertex-disjoint paths problem in linear time when k is fixed but larger than 2. The second algorithm runs in O(k5 Δn polylog (nU)) time, where each arc capacity and finite vertex capacity is bounded by U, and Δis the maximum degree of the graph. Finally, when k = 3, we present an algorithm that runs in O(n log n) time; this algorithm works even when the capacities are arbitrary reals. Our algorithms improve on the fastest previously known algorithms when k and Δare fixed and U is bounded by a polynomial in n. Prior to this result, the fastest algorithms ran in O(n4/3+o(1)) time for unit capacities; in the smallest of O(n3/2log n log U), Õ(n10/7U1/7), O(n11/8+o(1)U1/4), and O(n4/3 + o(1)U1/3) time for integer capacities; and in O(n2/log n) time for real capacities, even when k = 3.  © 2022 Association for Computing Machinery.",integer capacities; maximum flow; multiple sources and sinks; Planar graphs; scaling algorithm; vertex capacities,Flow graphs; Graph algorithms; Integer capacity; Maximum flow problems; Maximum flows; Multiple sinks; Multiple source; Planar graph; Scaling algorithm; Source and sink; Vertex capacity; Vertex disjoint paths; Graphic methods
Optimal Las Vegas Approximate Near Neighbors in ℓp,2022,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151798354&doi=10.1145%2f3461777&partnerID=40&md5=27d7a8928d883929bbd9facc9e3e7ba7,"We show that approximate near neighbor search in high dimensions can be solved in a Las Vegas fashion (i.e., without false negatives) for ℓp (1≤ p≤ 2) while matching the performance of optimal locality-sensitive hashing. Specifically, we construct a data-independent Las Vegas data structure with query time O(dnρ) and space usage O(dn1+ρ) for (r, c r)-approximate near neighbors in Rd under the ℓp norm, where ρ = 1/cp + o(1). Furthermore, we give a Las Vegas locality-sensitive filter construction for the unit sphere that can be used with the data-dependent data structure of Andoni et al. (SODA 2017) to achieve optimal space-time tradeoffs in the data-dependent setting. For the symmetric case, this gives us a data-dependent Las Vegas data structure with query time O(dnρ) and space usage O(dn1+ρ) for (r, c r)-approximate near neighbors in Rd under the ℓp norm, where ρ = 1/(2cp - 1) + o(1).Our data-independent construction improves on the recent Las Vegas data structure of Ahle (FOCS 2017) for ℓp when 1 < p≤ 2. Our data-dependent construction performs even better for ℓp for all pϵ [1, 2] and is the first Las Vegas approximate near neighbors data structure to make use of data-dependent approaches. We also answer open questions of Indyk (SODA 2000), Pagh (SODA 2016), and Ahle by showing that for approximate near neighbors, Las Vegas data structures can match state-of-the-art Monte Carlo data structures in performance for both the data-independent and data-dependent settings and across space-time tradeoffs.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximate near neighbor search; locality-sensitive hashing; recall; similarity search,Commerce; Query processing; Approximate Nearest Neighbor Search; Data dependent; Las Vegas; Locality sensitive hashing; Performance; Query time; Recall; Similarity search; Space usage; Space-time tradeoffs; Data structures
Dynamic Distribution-Sensitive Point Location,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151779564&doi=10.1145%2f3487403&partnerID=40&md5=a4027b07e7a14279c9888bb4a15a1873,"We propose a dynamic data structure for the distribution-sensitive point location problem in the plane. Suppose that there is a fixed query distribution within a convex subdivision S, and we are given an oracle that can return in O(1) time the probability of a query point falling into a polygonal region of constant complexity. We can maintain S such that each query is answered in Oopt(S)) expected time, where opt (S) is the expected time of the best linear decision tree for answering point location queries in S. The space and construction time are O(nlog2n), where n is the number of vertices of S. An update of S as a mixed sequence of k edge insertions and deletions takes O(klog4 n) amortized time. As a corollary, the randomized incremental construction of the Voronoi diagram of n sites can be performed in O(nlog4 n) expected time so that, during the incremental construction, a nearest neighbor query at any time can be answered optimally with respect to the intermediate Voronoi diagram at that time.  © 2021 Association for Computing Machinery.",convex subdivision; Dynamic planar point location; linear decision tree,Computational geometry; Graphic methods; Location; Nearest neighbor search; Probability distributions; Query processing; Convex subdivision; Dynamic data structure; Dynamic distribution; Dynamic planar point location; Expected time; Linear decision trees; Planar point location; Point location; Point location problems; Voronoi diagrams; Decision trees
Algorithms for Weighted Independent Transversals and Strong Colouring,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139665645&doi=10.1145%2f3474057&partnerID=40&md5=f2ef12fda1d85c49072ec876e657768a,"An independent transversal (IT) in a graph with a given vertex partition is an independent set consisting of one vertex in each partition class. Several sufficient conditions are known for the existence of an IT in a given graph and vertex partition, which have been used over the years to solve many combinatorial problems. Some of these IT existence theorems have algorithmic proofs, but there remains a gap between the best existential bounds and the bounds obtainable by efficient algorithms.Recently, Graf and Haxell (2018) described a new (deterministic) algorithm that asymptotically closes this gap, but there are limitations on its applicability. In this article, we develop a randomized algorithm that is much more widely applicable, and demonstrate its use by giving efficient algorithms for two problems concerning the strong chromatic number of graphs.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Independent transversal; strong colouring,Algorithmics; Combinatorial problem; Condition; Deterministic algorithms; Existence theorem; Graph partition; Independent set; Independent transversal; Strong coloring; Vertex partitions; Graph theory
A Simple Algorithm for Optimal Search Trees with Two-way Comparisons,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119128260&doi=10.1145%2f3477910&partnerID=40&md5=2b703256d80b0a4e08ea0eb090c42779,"We present a simple O(n4)-time algorithm for computing optimal search trees with two-way comparisons. The only previous solution to this problem, by Anderson et al., has the same running time but is significantly more complicated and is restricted to the variant where only successful queries are allowed. Our algorithm extends directly to solve the standard full variant of the problem, which also allows unsuccessful queries and for which no polynomial-time algorithm was previously known. The correctness proof of our algorithm relies on a new structural theorem for two-way-comparison search trees.  © 2021 held by the owner/author(s).",algorithms; Data structures; optimal search trees,Computation theory; Database systems; Polynomial approximation; Structural optimization; Optimal search; Optimal search tree; Polynomial-time algorithms; Running time; Search trees; SIMPLE algorithm; Simple++; SIMPLER algorithms; Time algorithms; Two ways; Trees (mathematics)
Querying a Matrix through Matrix-Vector Products,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117118315&doi=10.1145%2f3470566&partnerID=40&md5=c7a82b57af7dd0380ce62d994f051f60,"We consider algorithms with access to an unknown matrix M ϵ F n×d via matrix-vector products, namely, the algorithm chooses vectors v1, ĝ , vq, and observes Mv1, ĝ , Mvq. Here the vi can be randomized as well as chosen adaptively as a function of Mv1, ĝ , Mvi-1. Motivated by applications of sketching in distributed computation, linear algebra, and streaming models, as well as connections to areas such as communication complexity and property testing, we initiate the study of the number q of queries needed to solve various fundamental problems. We study problems in three broad categories, including linear algebra, statistics problems, and graph problems. For example, we consider the number of queries required to approximate the rank, trace, maximum eigenvalue, and norms of a matrix M; to compute the AND/OR/Parity of each column or row of M, to decide whether there are identical columns or rows in M or whether M is symmetric, diagonal, or unitary; or to compute whether a graph defined by M is connected or triangle-free. We also show separations for algorithms that are allowed to obtain matrix-vector products only by querying vectors on the right, versus algorithms that can query vectors on both the left and the right. We also show separations depending on the underlying field the matrix-vector product occurs in. For graph problems, we show separations depending on the form of the matrix (bipartite adjacency versus signed edge-vertex incidence matrix) to represent the graph.Surprisingly, very few works discuss this fundamental model, and we believe a thorough investigation of problems in this model would be beneficial to a number of different application areas.  © 2021 Association for Computing Machinery.",Communication complexity; linear algebra; sketching,Eigenvalues and eigenfunctions; Matrix algebra; Separation; Vectors; Well testing; Communication complexity; Distributed computations; Eigen-value; Graph problems; matrix; Matrix-vector products; Maximum norm; Property-testing; Sketchings; Streaming model; Computational complexity
Approximation Algorithms for the Bottleneck Asymmetric Traveling Salesman Problem,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117127297&doi=10.1145%2f3478537&partnerID=40&md5=952565a6112b6b57df03b63a49c8217b,"We present the first nontrivial approximation algorithm for the bottleneck asymmetric traveling salesman problem. Given an asymmetric metric cost between n vertices, the problem is to find a Hamiltonian cycle that minimizes its bottleneck (or maximum-length edge) cost. We achieve an O(log n/ log log n) approximation performance guarantee by giving a novel algorithmic technique to shortcut Eulerian circuits while bounding the lengths of the shortcuts needed. This allows us to build on a related result of Asadpour, Goemans, M ...dry, Oveis Gharan, and Saberi to obtain this guarantee. Furthermore, we show how our technique yields stronger approximation bounds in some cases, such as the bounded orientable genus case studied by Oveis Gharan and Saberi. We also explore the possibility of further improvement upon our main result through a comparison to the symmetric counterpart of the problem.  © 2021 Association for Computing Machinery.",Approximation algorithms; bottleneck optimization; traveling salesman problem,Hamiltonians; Traveling salesman problem; 'Dry' [; Algorithmic techniques; Approximation performance; Asymmetric travelling salesman problems; Bottleneck optimisation; Edge costs; Eulerian circuits; Hamiltonian cycle; Metric cost; Performance guarantees; Approximation algorithms
Approximating Geometric Knapsack via L-packings,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117074090&doi=10.1145%2f3473713&partnerID=40&md5=aa66ed0eb8d041d2660e8c4084ebeba8,"We study the two-dimensional geometric knapsack problem, in which we are given a set of n axis-aligned rectangular items, each one with an associated profit, and an axis-aligned square knapsack. The goal is to find a (non-overlapping) packing of a maximum profit subset of items inside the knapsack (without rotating items). The best-known polynomial-time approximation factor for this problem (even just in the cardinality case) is 2+ϵ [Jansen and Zhang, SODA 2004]. In this article we present a polynomial-time 17/9+ϵ < 1.89-approximation, which improves to 558/325+ϵ < 1.72 in the cardinality case.Prior results pack items into a constant number of rectangular containers that are filled via greedy strategies. We deviate from this setting and show that there exists a large profit solution where items are packed into a constant number of containers plus one L-shaped region at the boundary of the knapsack containing narrow-high items and thin-wide items. These items may interact in complex manners at the corner of the L. The best-known approximation ratio for the subproblem in the L-shaped region is 2+ϵ (via a trivial reduction to one-dimensional knapsack); hence, as a second major result we present a PTAS for this case that we believe might be of broader utility.We also consider the variant with rotations, where items can be rotated by 90 degrees. Again, the best-known polynomial-time approximation factor (even for the cardinality case) is 2+ϵ [Jansen and Zhang, SODA 2004]. We present a polynomial-time (3/2+ϵ)-approximation for this setting, which improves to 4/3+ϵ in the cardinality case.  © 2021 Association for Computing Machinery.",approximation algorithms; Geometric knapsack; rectangle packing,Approximation algorithms; Combinatorial optimization; Computational geometry; Containers; Profitability; Approximation factor; Cardinalities; Geometric knapsack; Knapsack problems; Knapsacks; L-shaped; Polynomial time approximation; Polynomial-time; Rectangle packing; Two-dimensional; Polynomial approximation
Zip Trees,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117112427&partnerID=40&md5=3a0dc8e0618859e5a77cfae987cf79e3,"We introduce the zip tree,1 a form of randomized binary search tree that integrates previous ideas into one practical, performant, and pleasant-to-implement package. A zip tree is a binary search tree in which each node has a numeric rank and the tree is (max)-heap-ordered with respect to ranks, with rank ties broken in favor of smaller keys. Zip trees are essentially treaps [8], except that ranks are drawn from a geometric distribution instead of a uniform distribution, and we allow rank ties. These changes enable us to use fewer random bits per node.We perform insertions and deletions by unmerging and merging paths (unzipping and zipping) rather than by doing rotations, which avoids some pointer changes and improves efficiency. The methods of zipping and unzipping take inspiration from previous top-down approaches to insertion and deletion by Stephenson [10], Martínez and Roura [5], and Sprugnoli [9].From a theoretical standpoint, this work provides two main results. First, zip trees require only O(log log n) bits (with high probability) to represent the largest rank in an n-node binary search tree; previous data structures require O(log n) bits for the largest rank. Second, zip trees are naturally isomorphic to skip lists [7], and simplify Dean and Jones' mapping between skip lists  © 2021 Association for Computing Machinery.",binary search tree; randomized algorithm; skip list; treap; Zip tree,Binary trees; Forestry; Trees (mathematics); Binary search trees; Geometric distribution; Insertions and deletions; Random bits; Randomized Algorithms; Skip listes; Top down approaches; Treap; Uniform distribution; Zip tree; Probability distributions
Better Distance Preservers and Additive Spanners,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117117544&doi=10.1145%2f3490147&partnerID=40&md5=7fca1038f83d546e9ec36198777154af,"We study two popular ways to sketch the shortest path distances of an input graph. The first is distance preservers, which are sparse subgraphs that agree with the distances of the original graph on a given set of demand pairs. Prior work on distance preservers has exploited only a simple structural property of shortest paths, called consistency, stating that one can break shortest path ties such that no two paths intersect, split apart, and then intersect again later. We prove that consistency alone is not enough to understand distance preservers, by showing both a lower bound on the power of consistency and a new general upper bound that polynomially surpasses it. Specifically, our new upper bound is that any p demand pairs in an n-node undirected unweighted graph have a distance preserver on O(n2/3p2/3 + np1/3 edges. We leave a conjecture that the right bound is O(n2/3p2/3 + n) or better.The second part of this paper leverages these distance preservers in a new construction of additive spanners, which are subgraphs that preserve all pairwise distances up to an additive error function. We give improved error bounds for spanners with relatively few edges; for example, we prove that all graphs have spanners on O(n) edges with +O(n3/7 + ϵ ) error. Our construction can be viewed as an extension of the popular path-buying framework to clusters of larger radii.  © 2021 Association for Computing Machinery.",distance preservers; Spanners; sparsification,Additives; Error analysis; Undirected graphs; Additive spanners; Distance preserver; Input graphs; Low bound; Short-path; Simple++; Spanner; Sparse subgraphs; Sparsification; Two paths; Graph theory
"Smaller Cuts, Higher Lower Bounds",2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117160294&doi=10.1145%2f3469834&partnerID=40&md5=a22428252486f672e8f0716d3b569706,"This article proves strong lower bounds for distributed computing in the congest model, by presenting the bit-gadget: a new technique for constructing graphs with small cuts.The contribution of bit-gadgets is twofold. First, developing careful sparse graph constructions with small cuts extends known techniques to show a near-linear lower bound for computing the diameter, a result previously known only for dense graphs. Moreover, the sparseness of the construction plays a crucial role in applying it to approximations of various distance computation problems, drastically improving over what can be obtained when using dense graphs.Second, small cuts are essential for proving super-linear lower bounds, none of which were known prior to this work. In fact, they allow us to show near-quadratic lower bounds for several problems, such as exact minimum vertex cover or maximum independent set, as well as for coloring a graph with its chromatic number. Such strong lower bounds are not limited to NP-hard problems, as given by two simple graph problems in P, which are shown to require a quadratic and near-quadratic number of rounds. All of the above are optimal up to logarithmic factors. In addition, in this context, the complexity of the all-pairs-shortest-paths problem is discussed.Finally, it is shown that graph constructions for congest lower bounds translate to lower bounds for the semi-streaming model, despite being very different in its nature.  © 2021 Association for Computing Machinery.",distributed algorithms; Lower bounds,Computational complexity; Chromatic number; Computation problems; Dense graphs; Distance computation; Graph construction; High-low; Low bound; Maximum independent sets; Minimum vertex cover; Sparse graphs; Graph theory
Sublinear Random Access Generators for Preferential Attachment Graphs,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117077673&doi=10.1145%2f3464958&partnerID=40&md5=0c9b020300f806e1337c7a77a110b2e1,"We consider the problem of sampling from a distribution on graphs, specifically when the distribution is defined by an evolving graph model, and consider the time, space, and randomness complexities of such samplers.In the standard approach, the whole graph is chosen randomly according to the randomized evolving process, stored in full, and then queries on the sampled graph are answered by simply accessing the stored graph. This may require prohibitive amounts of time, space, and random bits, especially when only a small number of queries are actually issued. Instead, we propose a setting where one generates parts of the sampled graph on-the-fly, in response to queries, and therefore requires amounts of time, space, and random bits that are a function of the actual number of queries. Yet, the responses to the queries correspond to a graph sampled from the distribution in question.Within this framework, we focus on two random graph models: the Barabási-Albert Preferential Attachment model (BA-graphs) (Science, 286 (5439):509-512) (for the special case of out-degree 1) and the random recursive tree model (Theory of Probability and Mathematical Statistics, (51):1-28). We give on-the-fly generation algorithms for both models. With probability 1-1/poly(n), each and every query is answered in polylog(n) time, and the increase in space and the number of random bits consumed by any single query are both polylog(n), where n denotes the number of vertices in the graph.Our work thus proposes a new approach for the access to huge graphs sampled from a given distribution, and our results show that, although the BA random graph model is defined by a sequential process, efficient random access to the graph's nodes is possible. In addition to the conceptual contribution, efficient on-the-fly generation of random graphs can serve as a tool for the efficient simulation of sublinear algorithms over large BA-graphs, and the efficient estimation of their on such graphs.  © 2021 Association for Computing Machinery.",local computation algorithms; preferential attachment graphs; Random Graph Generator; sublinear algorithms,Computation theory; Statistics; Trees (mathematics); Computation algorithm; Local computation; Local computation algorithm; Preferential attachment graph; Preferential attachments; Random bits; Random graph generator; Random graphs; Sublinear algorithm; Time-space; Graphic methods
The Complexity of the Ideal Membership Problem for Constrained Problems over the Boolean Domain,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117158235&doi=10.1145%2f3449350&partnerID=40&md5=93bc3601da4692d4e8c8c53a8f1cd83e,"Given an ideal I and a polynomial f the Ideal Membership Problem (IMP) is to test if f μ I. This problem is a fundamental algorithmic problem with important applications and notoriously intractable.We study the complexity of the IMP for combinatorial ideals that arise from constrained problems over the Boolean domain. As our main result, we identify the borderline of tractability. By using Gröbner bases techniques, we extend Schaefer's dichotomy theorem [STOC, 1978] which classifies all Constraint Satisfaction Problems (CSPs) over the Boolean domain to be either in P or NP-hard. Moreover, our result implies necessary and sufficient conditions for the efficient computation of Theta Body Semi-Definite Programming (SDP) relaxations, identifying therefore the borderline of tractability for constraint language problems.This article is motivated by the pursuit of understanding the recently raised issue of bit complexity of Sum-of-Squares (SoS) proofs [O'Donnell, ITCS, 2017]. Raghavendra and Weitz [ICALP, 2017] show how the IMP tractability for combinatorial ideals implies bounded coefficients in SoS proofs.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",constraint satisfaction problems; Gröbner basis; Ideal membership problem; Sum-of-Squares; Theta Bodies,Systems engineering; Algorithmic problems; Boolean domain; Constrained problem; Constraint-satisfaction problems; Dichotomy theorem; Gröbne base; Ideal membership problem; Membership problem; Sums of squares; Theta body; Constraint satisfaction problems
A Deamortization Approach for Dynamic Spanner and Dynamic Maximal Matching,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117101433&doi=10.1145%2f3469833&partnerID=40&md5=37424f6f5604033dd023006e88262012,"Many dynamic graph algorithms have an amortized update time, rather than a stronger worst-case guarantee. But amortized data structures are not suitable for real-time systems, where each individual operation has to be executed quickly. For this reason, there exist many recent randomized results that aim to provide a guarantee stronger than amortized expected. The strongest possible guarantee for a randomized algorithm is that it is always correct (Las Vegas) and has high-probability worst-case update time, which gives a bound on the time for each individual operation that holds with high probability.In this article, we present the first polylogarithmic high-probability worst-case time bounds for the dynamic spanner and the dynamic maximal matching problem.(1)For dynamic spanner, the only known o(n) worst-case bounds were O(n3/4) high-probability worst-case update time for maintaining a 3-spanner and O(n5/9) for maintaining a 5-spanner. We give a O(1)k log3 (n) high-probability worst-case time bound for maintaining a (2k-1)-spanner, which yields the first worst-case polylog update time for all constant k. (All the results above maintain the optimal tradeoff of stretch 2k-1 and Õ(n1+1/k) edges.)(2)For dynamic maximal matching, or dynamic 2-approximate maximum matching, no algorithm with o(n) worst-case time bound was known and we present an algorithm with O(log 5 (n)) high-probability worst-case time; similar worst-case bounds existed only for maintaining a matching that was (2+ μ)-approximate, and hence not maximal.Our results are achieved using a new approach for converting amortized guarantees to worst-case ones for randomized data structures by going through a third type of guarantee, which is a middle ground between the two above: An algorithm is said to have worst-case expected update time I' if for every update σ, the expected time to process σ is at most I'. Although stronger than amortized expected, the worst-case expected guarantee does not resolve the fundamental problem of amortization: A worst-case expected update time of O(1) still allows for the possibility that every 1/f(n) updates requires ϴ (f(n)) time to process, for arbitrarily high f(n). In this article, we present a black-box reduction that converts any data structure with worst-case expected update time into one with a high-probability worst-case update time: The query time remains the same, while the update time increases by a factor of O(log 2(n)).Thus, we achieve our results in two steps: (1) First, we show how to convert existing dynamic graph algorithms with amortized expected polylogarithmic running times into algorithms with worst-case expected polylogarithmic running times. (2) Then, we use our black-box reduction to achieve the polylogarithmic high-probability worst-case time bound. All our algorithms are Las-Vegas-type algorithms.  © 2021 Association for Computing Machinery.",maximal matching; Spanners,Interactive computer systems; Probability; Real time systems; Black-box reductions; Dynamic graph algorithms; High probability; Las Vegas; Maximal matchings; Polylogarithmic; Running time; Spanner; Time bound; Worst-case time; Data structures
"Approximate Counting of k-Paths: Simpler, Deterministic, and in Polynomial Space",2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112070968&doi=10.1145%2f3461477&partnerID=40&md5=574ebc189ee5eb8adb1cd3eebaec0a96,"Recently, Brand et al. [STOC 2018] gave a randomized mathcal O(4kmϵ-2-time exponential-space algorithm to approximately compute the number of paths on k vertices in a graph G up to a multiplicative error of 1 ± ϵ based on exterior algebra. Prior to our work, this has been the state-of-the-art. In this article, we revisit the algorithm by Alon and Gutner [IWPEC 2009, TALG 2010], and obtain the following results: •We present a deterministic 4k+ O(sk(log k+log2ϵ-1))m-time polynomial-space algorithm. This matches the running time of the best known deterministic polynomial-space algorithm for deciding whether a given graph G has a path on k vertices. •Additionally, we present a randomized 4k+mathcal O(logk(logk+logϵ-1))m-time polynomial-space algorithm. Our algorithm is simple - we only make elementary use of the probabilistic method. Here, n and m are the number of vertices and the number of edges, respectively. Additionally, our approach extends to approximate counting of other patterns of small size (such as q-dimensional p-matchings).  © 2021 ACM.",k-path; Parameterized complexity; parameterized counting problems,Graph algorithms; Polynomials; Approximate counting; Exterior algebra; Multiplicative errors; Polynomial space; Probabilistic methods; Space algorithms; State of the art; Time polynomials; Graph theory
Editorial,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112063455&doi=10.1145%2f3462270&partnerID=40&md5=6a6a0bb9a96649ea42810a2b0c111a14,[No abstract available],,
On β-Plurality Points in Spatial Voting Games,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112026071&doi=10.1145%2f3459097&partnerID=40&md5=e4c6ea7eff53e13cda54842bcb4c7455,"Let V be a set of n points in mathcal Rd, called voters. A point p mathcal Rd is a plurality point for V when the following holds: For every q mathcal Rd, the number of voters closer to p than to q is at least the number of voters closer to q than to p. Thus, in a vote where each vV votes for the nearest proposal (and voters for which the proposals are at equal distance abstain), proposal p will not lose against any alternative proposal q. For most voter sets, a plurality point does not exist. We therefore introduce the concept of β-plurality points, which are defined similarly to regular plurality points, except that the distance of each voter to p (but not to q) is scaled by a factor β, for some constant 0< β 1/2 1. We investigate the existence and computation of β-plurality points and obtain the following results. •Define β∗d := {β : any finite multiset V in mathcal Rd admits a β-plurality point. We prove that β∗d = s3/2, and that 1/s d 1/2 β∗d 1/2 s 3/2 for all d3/4 3. •Define β (p, V) := sup {β : p is a β -plurality point for V}. Given a voter set V in mathcal R2, we provide an algorithm that runs in O(n log n) time and computes a point p such that β (p, V) 3/4 β∗b. Moreover, for d3/4 2, we can compute a point p with β (p,V) 3/4 1/s d in O(n) time. •Define β (V) := sup { β : V admits a β -plurality point}. We present an algorithm that, given a voter set V in mathcal Rd, computes an ((1-I)A β (V))-plurality point in time On2I 3d-2 A log n I d-1 A log 2 1I).  © 2021 ACM.",Computational geometry; computational social choice; plurality point; spatial voting theory,Algorithms; Multiset; Voting game; Mathematical techniques
Online Service with Delay,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112083298&doi=10.1145%2f3459925&partnerID=40&md5=306b5fa0a534d85898cc52e0ccc910c1,"In this article, we introduce the online service with delay problem. In this problem, there are n points in a metric space that issue service requests over time, and there is a server that serves these requests. The goal is to minimize the sum of distance traveled by the server and the total delay (or a penalty function thereof) in serving the requests. This problem models the fundamental tradeoff between batching requests to improve locality and reducing delay to improve response time, which has many applications in operations management, operating systems, logistics, supply chain management, and scheduling. Our main result is to show a poly-logarithmic competitive ratio for the online service with delay problem. This result is obtained by an algorithm that we call the preemptive service algorithm. The salient feature of this algorithm is a process called preemptive service, which uses a novel combination of (recursive) time forwarding and spatial exploration on a metric space. We also generalize our results to k > 1 servers and obtain stronger results for special metrics such as uniform and star metrics that correspond to (weighted) paging problems.  © 2021 ACM.",competitive ratio; k-server; online algorithms; Paging; weighted paging,Set theory; Supply chain management; Topology; Competitive ratio; On-line service; Operations management; Penalty function; Salient features; Service requests; Spatial explorations; Sum of distances; Scheduling
Discrete Fréchet Distance under Translation,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112052592&doi=10.1145%2f3460656&partnerID=40&md5=8978be817e727c373e58fbb615a41fa1,"The discrete Fréchet distance is a popular measure for comparing polygonal curves. An important variant is the discrete Fréchet distance under translation, which enables detection of similar movement patterns in different spatial domains. For polygonal curves of length n in the plane, the fastest known algorithm runs in time Õ(n5) [12]. This is achieved by constructing an arrangement of disks of size Õ(n4), and then traversing its faces while updating reachability in a directed grid graph of size N := Õ(n5), which can be done in time Õ(s N) per update [27]. The contribution of this article is two-fold. First, although it is an open problem to solve dynamic reachability in directed grid graphs faster than Õ(s N), we improve this part of the algorithm: We observe that an offline variant of dynamic s-t-reachability in directed grid graphs suffices, and we solve this variant in amortized time Õ(N1/3) per update, resulting in an improved running time of Õ(N4.66) for the discrete Fréchet distance under translation. Second, we provide evidence that constructing the arrangement of size Õ(N4) is necessary in the worst case by proving a conditional lower bound of n4 - o(1) on the running time for the discrete Fréchet distance under translation, assuming the Strong Exponential Time Hypothesis.  © 2021 Owner/Author.",conditional lower bounds; Fréchet distance,Graph algorithms; Amortized time; Lower bounds; Movement pattern; Polygonal curve; Reachability; Running time; Spatial domains; Strong exponential time hypothesis; Directed graphs
The Infinite Server Problem,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112057769&doi=10.1145%2f3456632&partnerID=40&md5=3d3dfdb2d2b552af5019f0496423fc87,"We study a variant of the k-server problem, the infinite server problem, in which infinitely many servers reside initially at a particular point of the metric space and serve a sequence of requests. In the framework of competitive analysis, we show a surprisingly tight connection between this problem and the resource augmentation version of the k-server problem, also known as the (h,k)-server problem, in which an online algorithm with k servers competes against an offline algorithm with h servers. Specifically, we show that the infinite server problem has bounded competitive ratio if and only if the (h,k)-server problem has bounded competitive ratio for some k=O(h). We give a lower bound of 3.146 for the competitive ratio of the infinite server problem, which holds even for the line and some simple weighted stars. It implies the same lower bound for the (h,k)-server problem on the line, even when k/h → ∞, improving on the previous known bounds of 2 for the line and 2.4 for general metrics. For weighted trees and layered graphs, we obtain upper bounds, although they depend on the depth. Of particular interest is the infinite server problem on the line, which we show to be equivalent to the seemingly easier case in which all requests are in a fixed bounded interval. This is a special case of a more general reduction from arbitrary metric spaces to bounded subspaces. Unfortunately, classical approaches (double coverage and generalizations, work function algorithm, balancing algorithms) fail even for this special case.  © 2021 ACM.",competitive analysis; k-server; Online algorithms; resource augmentation,Set theory; Topology; Balancing algorithms; Classical approach; Competitive analysis; Competitive ratio; Off-line algorithm; On-line algorithms; Resource augmentation; Work function algorithms; Trees (mathematics)
The Quest for Strong Inapproximability Results with Perfect Completeness,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112054535&doi=10.1145%2f3459668&partnerID=40&md5=5138c4d482195d6d407d5f3d5fca7bee,"The Unique Games Conjecture has pinned down the approximability of all constraint satisfaction problems (CSPs), showing that a natural semidefinite programming relaxation offers the optimal worst-case approximation ratio for any CSP. This elegant picture, however, does not apply for CSP instances that are perfectly satisfiable, due to the imperfect completeness inherent in the Unique Games Conjecture. This work is motivated by the pursuit of a better understanding of the approximability of perfectly satisfiable instances of CSPs. We prove that an ""almost Unique""version of Label Cover can be approximated within a constant factor on satisfiable instances. Our main conceptual contribution is the formulation of a (hypergraph) version of Label Cover that we call V Label Cover. Assuming a conjecture concerning the inapproximability of V Label Cover on perfectly satisfiable instances, we prove the following implications: •There is an absolute constant c0 such that for k ≥ 3, given a satisfiable instance of Boolean k-CSP, it is hard to find an assignment satisfying more than c0k2/2k fraction of the constraints. •Given a k-uniform hypergraph, k ≥ 2, for all ϵ > 0, it is hard to tell if it is q-strongly colorable or has no independent set with an ϵ fraction of vertices, where q=CE k+ sk-1/2 CE‰. •Given a k-uniform hypergraph, k ≥ 3, for all ϵ > 0, it is hard to tell if it is (k-1)-rainbow colorable or has no independent set with an ϵ fraction of vertices.  © 2021 ACM.",constraint satisfaction; dictatorship testing; hardness of approximation; hypergraph coloring; Inapproximability,Graph theory; Approximability; Approximation ratios; Constant factors; Hypergraph; Inapproximability; Independent set; Semi-definite programming relaxations; Unique games conjecture; Constraint satisfaction problems
The Combined Basic LP and Affine IP Relaxation for Promise VCSPs on Infinite Domains,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112096471&doi=10.1145%2f3458041&partnerID=40&md5=bbb19bea40f3c1ffd770d87491bf83ed,"Convex relaxations have been instrumental in solvability of constraint satisfaction problems (CSPs), as well as in the three different generalisations of CSPs: valued CSPs, infinite-domain CSPs, and most recently promise CSPs. In this work, we extend an existing tractability result to the three generalisations of CSPs combined: We give a sufficient condition for the combined basic linear programming and affine integer programming relaxation for exact solvability of promise valued CSPs over infinite-domains. This extends a result of Brakensiek and Guruswami (SODA'20) for promise (non-valued) CSPs (on finite domains).  © 2021 ACM.",convex relaxations; polymorphisms; Promise constraint satisfaction; valued constraint satisfaction,Integer programming; Linear programming; Relaxation processes; Convex relaxation; Finite domains; Infinite domains; Constraint satisfaction problems
The Complexity of Approximately Counting Retractions to Square-free Graphs,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112075381&doi=10.1145%2f3458040&partnerID=40&md5=60444c284b69d7bcb5e8b6e4b29ead68,"A retraction is a homomorphism from a graph G to an induced subgraph H of G that is the identity on H. In a long line of research, retractions have been studied under various algorithmic settings. Recently, the problem of approximately counting retractions was considered. We give a complete trichotomy for the complexity of approximately counting retractions to all square-free graphs (graphs that do not contain a cycle of length 4). It turns out there is a rich and interesting class of graphs for which this problem is complete in the class #BIS. As retractions generalise homomorphisms, our easiness results extend to the important problem of approximately counting homomorphisms. By giving new #BIS-easiness results, we now settle the complexity of approximately counting homomorphisms for a whole class of non-trivial graphs that were previously unresolved.  © 2021 ACM.",Approximate counting; counting complexity; graph homomorphisms; retractions; square-free graphs,Algebra; Graphic methods; Free graphs; Graph G; Induced subgraphs; Long line; Non-trivial; Graph theory
Improving the Smoothed Complexity of FLIP for Max Cut Problems,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112084750&doi=10.1145%2f3454125&partnerID=40&md5=9b025fe2b00f37eb5eb84336b92ec7bd,"Finding locally optimal solutions for MAX-CUT and MAX-k-CUT are well-known PLS-complete problems. An instinctive approach to finding such a locally optimum solution is the FLIP method. Even though FLIP requires exponential time in worst-case instances, it tends to terminate quickly in practical instances. To explain this discrepancy, the run-time of FLIP has been studied in the smoothed complexity framework. Etscheid and Röglin (ACM Transactions on Algorithms, 2017) showed that the smoothed complexity of FLIP for max-cut in arbitrary graphs is quasi-polynomial. Angel, Bubeck, Peres, and Wei (STOC, 2017) showed that the smoothed complexity of FLIP for max-cut in complete graphs is (Oφ5n15.1), where φ is an upper bound on the random edge-weight density and φ is the number of vertices in the input graph. While Angel, Bubeck, Peres, and Wei's result showed the first polynomial smoothed complexity, they also conjectured that their run-time bound is far from optimal. In this work, we make substantial progress toward improving the run-time bound. We prove that the smoothed complexity of FLIP for max-cut in complete graphs is O(φ n7.83). Our results are based on a carefully chosen matrix whose rank captures the run-time of the method along with improved rank bounds for this matrix and an improved union bound based on this matrix. In addition, our techniques provide a general framework for analyzing FLIP in the smoothed framework. We illustrate this general framework by showing that the smoothed complexity of FLIP for MAX-3-CUT in complete graphs is polynomial and for MAX-k-CUT in arbitrary graphs is quasi-polynomial. We believe that our techniques should also be of interest toward showing smoothed polynomial complexity of FLIP for MAX-k-CUT in complete graphs for larger constants k.  © 2021 ACM.",k-cut; local search; max-cut; Smoothed analysis,Graph algorithms; Graphic methods; Polynomial approximation; Arbitrary graphs; Complete problems; Exponential time; Optimal solutions; Polynomial complexity; Quasi-poly-nomial; Random edge weights; Worst-case instances; Graph theory
Parameterized Approximation Algorithms for Bidirected Steiner Network Problems,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107813086&doi=10.1145%2f3447584&partnerID=40&md5=04f76619ad90c9b83121b27311774782,"The Directed Steiner Network (DSN) problem takes as input a directed graph G=(V, E) with non-negative edge-weights and a set D⊆ V × V of k demand pairs. The aim is to compute the cheapest network N⊆ G for which there is an s\rightarrow t path for each (s, t)∈ D. It is known that this problem is notoriously hard, as there is no k1/4-o(1)-approximation algorithm under Gap-ETH, even when parametrizing the runtime by k [Dinur & Manurangsi, ITCS 2018]. In light of this, we systematically study several special cases of DSN and determine their parameterized approximability for the parameter k. For the bi-DSNPlanar problem, the aim is to compute a solution N⊆ G whose cost is at most that of an optimum planar solution in a bidirected graph G, i.e., for every edge uv of G the reverse edge vu exists and has the same weight. This problem is a generalization of several well-studied special cases. Our main result is that this problem admits a parameterized approximation scheme (PAS) for k. We also prove that our result is tight in the sense that (a) the runtime of our PAS cannot be significantly improved, and (b) no PAS exists for any generalization of bi-DSNPlanar, under standard complexity assumptions. The techniques we use also imply a polynomial-sized approximate kernelization scheme (PSAKS). Additionally, we study several generalizations of bi-DSNPlanar and obtain upper and lower bounds on obtainable runtimes parameterized by k. One important special case of DSN is the Strongly Connected Steiner Subgraph (SCSS) problem, for which the solution network N⊆ G needs to strongly connect a given set of k terminals. It has been observed before that for SCSS a parameterized 2-approximation exists for parameter k [Chitnis et al., IPEC 2013]. We give a tight inapproximability result by showing that for k no parameterized (2 - ϵ)-approximation algorithm exists under Gap-ETH. Additionally, we show that when restricting the input of SCSS to bidirected graphs, the problem remains NP-hard but becomes FPT for k.  © 2021 ACM.",bidirected graphs; Directed Steiner network; parameterized approximations; planar graphs; strongly connected Steiner subgraph,Approximation algorithms; Directed graphs; NP-hard; Parameterization; Approximation scheme; Bidirected graph; Complexity assumptions; Inapproximability; Planar Solutions; Steiner network; Strongly connected; Upper and lower bounds; Parameter estimation
Finding a Shortest Odd Hole,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107757606&doi=10.1145%2f3447869&partnerID=40&md5=7dadaca37d99b660e9132cce95ed532a,"An odd hole in a graph is an induced cycle with odd length greater than 3. In an earlier paper (with Sophie Spirkl), solving a longstanding open problem, we gave a polynomial-time algorithm to test if a graph has an odd hole. We subsequently showed that, for every t, there is a polynomial-time algorithm to test whether a graph contains an odd hole of length at least t. In this article, we give an algorithm that finds a shortest odd hole, if one exists.  © 2021 ACM.",Induced subgraph; recognition algorithm; shortest odd hole,Polynomial approximation; Induced cycle; Odd length; Polynomial-time algorithms; Graph algorithms
Navigating in Trees with Permanently Noisy Advice,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107804564&doi=10.1145%2f3448305&partnerID=40&md5=e674bbad39276e1f02c611ae68bde57e,"We consider a search problem on trees in which an agent starts at the root of a tree and aims to locate an adversarially placed treasure, by moving along the edges, while relying on local, partial information. Specifically, each node in the tree holds a pointer to one of its neighbors, termed advice. A node is faulty with probability q. The advice at a non-faulty node points to the neighbor that is closer to the treasure, and the advice at a faulty node points to a uniformly random neighbor. Crucially, the advice is permanent, in the sense that querying the same node again would yield the same answer. Let Δdenote the maximum degree. For the expected number of moves (edge traversals) until finding the treasure, we show that a phase transition occurs when the noise parameter q is roughly 1 g"". Below the threshold, there exists an algorithm with expected number of moves O(D g""), where D is the depth of the treasure, whereas above the threshold, every search algorithm has an expected number of moves, which is both exponential in D and polynomial in the number of nodes n. In contrast, if we require to find the treasure with probability at least 1 -then for every fixed I> 0, if q < 1/""I, then there exists a search strategy that with probability 1 - δfinds the treasure using (Δ-1D)O(1/ϵ) moves. Moreover, we show that (Δ-1D)ω(1/ϵ) moves are necessary.  © 2021 ACM.",average case analysis; computation in unreliable conditions; expectation versus high probability performances; Navigation with advice; worst-case analysis,Probability; Edge traversal; Faulty node; Maximum degree; Noise parameters; Partial information; Search Algorithms; Search problem; Search strategies; Forestry
2-Approximating Feedback Vertex Set in Tournaments,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107748812&doi=10.1145%2f3446969&partnerID=40&md5=1cd5dfebb74aecb8acb81ce292978a93,"A tournament is a directed graph T such that every pair of vertices is connected by an arc. A feedback vertex set is a set S of vertices in T such that T - S is acyclic. We consider the Feedback Vertex Set problem in tournaments. Here, the input is a tournament T and a weight function w : V(T) → N, and the task is to find a feedback vertex set S in T minimizing w(S) = ∑v∈S w(v). Rounding optimal solutions to the natural LP-relaxation of this problem yields a simple 3-approximation algorithm. This has been improved to 2.5 by Cai et al. [SICOMP 2000], and subsequently to 7/3 by Mnich et al. [ESA 2016]. In this article, we give the first polynomial time factor 2-approximation algorithm for this problem. Assuming the Unique Games Conjecture, this is the best possible approximation ratio achievable in polynomial time.  © 2021 ACM.",Approximation algorithm; branching; feedback vertex set; local ratio; tournament,Directed graphs; Approximation ratios; Feedback vertex set; Feedback Vertex Set problems; LP relaxation; Optimal solutions; Polynomial-time; Unique games conjecture; Weight functions; Polynomial approximation
Graph Sparsification for Derandomizing Massively Parallel Computation with Low Space,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107741468&doi=10.1145%2f3451992&partnerID=40&md5=75be056f514012c8936b6cae1f77020e,"The Massively Parallel Computation (MPC) model is an emerging model that distills core aspects of distributed and parallel computation, developed as a tool to solve combinatorial (typically graph) problems in systems of many machines with limited space. Recent work has focused on the regime in which machines have sublinear (in n, the number of nodes in the input graph) space, with randomized algorithms presented for the fundamental problems of Maximal Matching and Maximal Independent Set. However, there have been no prior corresponding deterministic algorithms. A major challenge underlying the sublinear space setting is that the local space of each machine might be too small to store all edges incident to a single node. This poses a considerable obstacle compared to classical models in which each node is assumed to know and have easy access to its incident edges. To overcome this barrier, we introduce a new graph sparsification technique that deterministically computes a low-degree subgraph, with the additional property that solving the problem on this subgraph provides significant progress towards solving the problem for the original input graph. Using this framework to derandomize the well-known algorithm of Luby [SICOMP'86], we obtain O(log Δ+ log log n)-round deterministic MPC algorithms for solving the problems of Maximal Matching and Maximal Independent Set with O(nI) space on each machine for any constant I > 0. These algorithms also run in O(log "") rounds in the closely related model of CONGESTED CLIQUE, improving upon the state-of-the-art bound of O(log 2"") rounds by Censor-Hillel et al. [DISC'17].  © 2021 ACM.",derandomization; Massively parallel computation; maximal independent set; maximal matching; sparsification,Algorithms; Mathematical techniques; Deterministic algorithms; Graph sparsification; Massively parallels; Maximal independent set; Maximal matchings; Parallel Computation; Randomized Algorithms; State of the art; Graph algorithms
Sparse Backbone and Optimal Distributed SINR Algorithms,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107780021&doi=10.1145%2f3452937&partnerID=40&md5=2a01413bdbe04b50fbaf35b3627a00cb,"We develop randomized distributed algorithms for many of the most fundamental communication problems in wireless networks under the Signal to Interference and Noise Ratio (SINR) model of communication, including (multi-message) broadcast, local broadcast, coloring, Maximal Independent Set, and aggregation. The complexity of our algorithms is optimal up to polylogarithmic preprocessing time. It shows - contrary to expectation - that the plain vanilla SINR model is just as powerful and fast (modulo the preprocessing) as various extensions studied, including power control, carrier sense, collision detection, free acknowledgements, and geolocation knowledge. Central to these results is an efficient construction of a constant-density backbone structure over the network, which is of independent interest. This is achieved using an indirect sensing technique, where message non-reception is used to deduce information about relative node-distances.  © 2021 ACM.",distributed algorithm; plain SINR; Wireless network,Computational complexity; Power control; Signal interference; Backbone structures; Collision detection; Communication problems; Efficient construction; Maximal independent set; Preprocessing time; Randomized distributed algorithms; Signal-to-interference and noise ratios; Signal to noise ratio
Node-weighted Network Design in Planar and Minor-closed Families of Graphs,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107775247&doi=10.1145%2f3447959&partnerID=40&md5=b25dcf6164a7b575fb57f7e32f826a48,"We consider node-weighted survivable network design (SNDP) in planar graphs and minor-closed families of graphs. The input consists of a node-weighted undirected graph G = (V, E) and integer connectivity requirements r(uv) for each unordered pair of nodes uv. The goal is to find a minimum weighted subgraph H of G such that H contains r(uv) disjoint paths between u and v for each node pair uv. Three versions of the problem are edge-connectivity SNDP (EC-SNDP), element-connectivity SNDP (Elem-SNDP), and vertex-connectivity SNDP (VC-SNDP), depending on whether the paths are required to be edge, element, or vertex disjoint, respectively. Our main result is an O(k)-approximation algorithm for EC-SNDP and Elem-SNDP when the input graph is planar or more generally if it belongs to a proper minor-closed family of graphs; here, k = max uvr(uv) is the maximum connectivity requirement. This improves upon the O(klog n)-approximation known for node-weighted EC-SNDP and Elem-SNDP in general graphs [31]. We also obtain an O(1) approximation for node-weighted VC-SNDP when the connectivity requirements are in {0, 1, 2}; for higher connectivity our result for Elem-SNDP can be used in a black-box fashion to obtain a logarithmic factor improvement over currently known general graph results. Our results are inspired by, and generalize, the work of Demaine, Hajiaghayi, and Klein [13], who obtained constant factor approximations for node-weighted Steiner tree and Steiner forest problems in planar graphs and proper minor-closed families of graphs via a primal-dual algorithm.  © 2021 ACM.",Approximation algorithms; element/vertex connectivity; network design; primal-dual algorithms,Approximation algorithms; Graph algorithms; Graphic methods; Constant factor approximation; Element connectivity; Node-weighted networks; Primal dual algorithms; Steiner forest problem; Survivable network design; Vertex connectivity; Weighted undirected graph; Graph theory
Memory-Adjustable Navigation Piles with Applications to Sorting and Convex Hulls,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107744601&doi=10.1145%2f3452938&partnerID=40&md5=9e05d2d6c0215ef09d3aa286d1bf59b8,"We consider space-bounded computations on a random-access machine, where the input is given on a read-only random-access medium, the output is to be produced to a write-only sequential-access medium, and the available workspace allows random reads and writes but is of limited capacity. The length of the input is N elements, the length of the output is limited by the computation, and the capacity of the workspace is O(S) bits for some predetermined parameter S ≥ lg N. We present a state-of-the-art priority queue - called an adjustable navigation pile - for this restricted model. This priority queue supports Minimum in O(1) time, Construct in O(N) time, and Extract-min in O(N/S + lg S) time for any S ≥ lg N. The priority queue can be further augmented in O(N) time to deal with a batch of at most S elements in a specified range of values at a time, and allow to Insert (activate) or Extract (deactivate) an element among these elements, such that Insert and Extract take O(N/S + lg S) time for any S ≥ lg N. We show how to use our data structure to sort N elements and to compute the convex hull of N points in the Euclidean plane in O(N2/S + N lg S) time for any S ≥ lg N. Following a known lower bound for the space-time product of any branching program for finding unique elements, both our sorting and convex-hull algorithms are optimal. The adjustable navigation pile has turned out to be useful when designing other space-efficient algorithms, and we expect that it will find its way to yet other applications.  © 2021 ACM.",convex hull; Priority queue; read-only data; space-restricted random-access machine; space-time trade-off,Computational geometry; Navigation; Queueing theory; Sorting; Bounded computation; Branching programs; Convex hull algorithm; Euclidean planes; Random access machines; Restricted models; Sequential access; Space efficient algorithms; Piles
Optimal Substring Equality Queries with Applications to Sparse Text Indexing,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100319398&doi=10.1145%2f3426870&partnerID=40&md5=0eff5efc51dd6907b338f1dc242f47e7,"We consider the problem of encoding a string of length n from an integer alphabet of size so access, substring equality, and Longest Common Extension (LCE) queries can be answered efficiently. We describe a new space-optimal data structure supporting logarithmic-time queries. Access and substring equality query times can furthermore be improved to the optimal O(1) if O(log n) additional precomputed words are allowed in the total space. Additionally, we provide in-place algorithms for converting between the string and our data structure. Using this new string representation, we obtain the first in-place subquadratic algorithms for several string-processing problems in the restore model: The input string is rewritable and must be restored before the computation terminates. In particular, we describe the first in-place subquadratic Monte Carlo solutions to the sparse suffix sorting, sparse LCP array construction, and suffix selection problems. With the sole exception of suffix selection, our algorithms are also the first running in sublinear time for small enough sets of input suffixes. Combining these solutions, we obtain the first sublinear-time Monte Carlo algorithm for building the sparse suffix tree in compact space. We also show how to build a correct version of our data structure using small working space. This leads to the first Las Vegas in-place algorithm computing the full LCP array in O(nlog n) time w.h.p. and to the first Las Vegas in-place algorithms solving the sparse suffix sorting and sparse LCP array construction problems in O(n1.5 s log σ) time w.h.p.  © 2020 ACM.",in-place; Substring equality queries; suffix sorting,Data structures; Monte Carlo methods; Restoration; Sorting; Trees (mathematics); Construction problem; In-place algorithms; Integer alphabets; Longest common extensions; Monte carlo algorithms; Selection problems; String processing; Subquadratic algorithms; Structural optimization
Zeros of Holant Problems,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100316796&doi=10.1145%2f3418056&partnerID=40&md5=aefb338f4fdf176f9ef75fc8f607238c,"We present fully polynomial-time (deterministic or randomised) approximation schemes for Holant problems, defined by a non-negative constraint function satisfying a generalised second-order recurrence modulo in a couple of exceptional cases. As a consequence, any non-negative Holant problem on cubic graphs has an efficient approximation algorithm unless the problem is equivalent to approximately counting perfect matchings, a central open problem in the area. This is in sharp contrast to the computational phase transition shown by two-state spin systems on cubic graphs. Our main technique is the recently established connection between zeros of graph polynomials and approximate counting.  © 2020 ACM.",Approximate counting; Holant problem; holographic algorithms; zero-free region,Approximation algorithms; Graph algorithms; Polynomial approximation; Spin fluctuations; Approximate counting; Approximation scheme; Computational phase; Efficient approximation algorithms; Graph polynomials; Holant problems; Perfect matchings; Polynomial-time; Constraint satisfaction problems
Tight Bounds for Online TSP on the Line,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100290676&doi=10.1145%2f3422362&partnerID=40&md5=123670385d70d90f3f1281c7d08f1eda,"We consider the online traveling salesperson problem (TSP), where requests appear online over time on the real line and need to be visited by a server initially located at the origin. We distinguish between closed and open online TSP, depending on whether the server eventually needs to return to the origin or not. While online TSP on the line is a very natural online problem that was introduced more than two decades ago, no tight competitive analysis was known to date. We settle this problem by providing tight bounds on the competitive ratios for both the closed and the open variant of the problem. In particular, for closed online TSP, we provide a 1.64-competitive algorithm, thus matching a known lower bound. For open online TSP, we give a new upper bound as well as a matching lower bound that establish the remarkable competitive ratio of 2.04. Additionally, we consider the online DIAL-A-RIDE problem on the line, where each request needs to be transported to a specified destination. We provide an improved non-preemptive lower bound of 1.75 for this setting, as well as an improved preemptive algorithm with competitive ratio 2.41. Finally, we generalize known and give new complexity results for the underlying offline problems. In particular, we give an algorithm with running time O(n) for closed offline TSP on the line with release dates and show that both variants of offline DIAL-A-RIDE on the line are NP-hard for any capacity c≥ 2 of the server.  © 2020 ACM.",approximation algorithms; competitive analysis; computational complexity; online algorithms; TSP,Algorithms; Mathematical techniques; Competitive algorithms; Competitive analysis; Competitive ratio; Complexity results; Off-line problems; Online dial-a-ride problems; Preemptive algorithm; Traveling salesperson problem; NP-hard
Journey to the Center of the Point Set,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100299929&doi=10.1145%2f3431285&partnerID=40&md5=8d27dfb0d02da79bef32208365a18eef,"Let P be a set of n points in R d. For a parameter α (0,1), an α-centerpoint of P is a point p R d such that all closed halfspaces containing P also contain at least α n points of P. We revisit an algorithm of Clarkson et al. [1996] that computes (roughly) a 1/(4d)-centerpoint in Õ(d) randomized time, where Õ hides polylogarithmic terms. We present an improved algorithm that can compute centerpoints with quality arbitrarily close to 1/d and runs in randomized time Õ(d). While the improvements are (arguably) mild, it is the first refinement of the algorithm by Clarkson et al. [1996] in over 20 years. The new algorithm is simpler, and the running time bound follows by a simple random walk argument, which we believe to be of independent interest. We also present several new applications of the improved centerpoint algorithm.  © 2020 ACM.",centerpoints; Computational geometry; random walks,Algorithms; Mathematical techniques; Half spaces; New applications; Point set; Polylogarithmic; Running time; Simple random walk; Geometry
Oblivious Resampling Oracles and Parallel Algorithms for the Lopsided Lovász Local Lemma,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100291041&doi=10.1145%2f3392035&partnerID=40&md5=8818cbb7cf4c66079864bc2de348ef98,"The Lovász Local Lemma (LLL) shows that, for a collection of ""bad""events B in a probability space that are not too likely and not too interdependent, there is a positive probability that no events in B occur. Moser and Tardos (2010) gave sequential and parallel algorithms that transformed most applications of the variable-assignment LLL into efficient algorithms. A framework of Harvey and Vondrák (2015) based on ""resampling oracles""extended this to sequential algorithms for other probability spaces satisfying a generalization of the LLL known as the Lopsided Lovász Local Lemma (LLLL). We describe a new structural property that holds for most known resampling oracles, which we call ""obliviousness.""Essentially, it means that the interaction between two bad-events B, B′ depends only on the randomness used to resample B and not the precise state within B itself. This property has two major consequences. First, combined with a framework of Kolmogorov (2016), it leads to a unified parallel LLLL algorithm, which is faster than previous, problem-specific algorithms of Harris (2016) for the variable-assignment LLLL and of Harris and Srinivasan (2014) for permutations. This gives the first RNC algorithms for rainbow perfect matchings and rainbow Hamiltonian cycles of Kn. Second, this property allows us to build LLLL probability spaces from simpler ""atomic""events. This gives the first resampling oracle for rainbow perfect matchings on the complete s-uniform hypergraph Kn(s) and the first commutative resampling oracle for Hamiltonian cycles of Kn.  © 2020 Public Domain.",lexicographically first MIS; LFMIS; LLL; LLLL; Lopsided Lovász Local Lemma; Lovász Local Lemma; resampling,Computational complexity; Hamiltonians; Parallel algorithms; Hamiltonian cycle; Local lemmata; Perfect matchings; Positive probability; Probability spaces; Problem-specific algorithms; Sequential algorithm; Variable assignment; Probability
R-Simple k-Path and Related Problems Parameterized by k/r,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100304277&doi=10.1145%2f3439721&partnerID=40&md5=513b77e1178eaad8dc64525e73197ce2,"Abasi et al. (2014) introduced the following two problems. In the r-Simple k-Path problem, given a digraph G on n vertices and positive integers r, k, decide whether G has an r-simple k-path, which is a walk where every vertex occurs at most r times and the total number of vertex occurrences is k. In the (r, k)-Monomial Detection problem, given an arithmetic circuit that succinctly encodes some polynomial P on n variables and positive integers k, r, decide whether P has a monomial of total degree k where the degree of each variable is at most r. Abasi et al. obtained randomized algorithms of running time 4(k/r)log r⋅ nO(1) for both problems. Gabizon et al. (2015) designed deterministic 2O((k/r)log r)⋅ nO(1)-time algorithms for both problems (however, for the (r, k)-Monomial Detection problem the input circuit is restricted to be non-canceling). Gabizon et al. also studied the following problem. In the P-Set (r, q)-Packing Problem, given a universe V, positive integers (p, q, r), and a collection H of sets of size P whose elements belong to V, decide whether there exists a subcollection H′ of H of size q where each element occurs in at most r sets of H′. Gabizon et al. obtained a deterministic 2O((pq/r)log r)nO(1)-time algorithm for P-Set (r, q)-Packing. The above results prove that the three problems are single-exponentially fixed-parameter tractable (FPT) parameterized by the product of two parameters, that is, k/r and log r, where k=pq for P-Set (r, q)-Packing. Abasi et al. and Gabizon et al. asked whether the log r factor in the exponent can be avoided. Bonamy et al. (2017) answered the question for (r, k)-Monomial Detection by proving that unless the Exponential Time Hypothesis (ETH) fails there is no 2o((k/r) log r) (n + log k)O(1)-time algorithm for (r, k)-Monomial Detection, i.e., (r, k)-Monomial Detection is unlikely to be single-exponentially FPT when parameterized by k/r alone. The question remains open for r-Simple k-Path and P-Set (r, q)-Packing. We consider the question from a wider perspective: are the above problems FPT when parameterized by k/r only, i.e., whether there exists a computable function f such that the problems admit a f(k/r)(n+log k)O(1)-time algorithm Since r can be substantially larger than the input size, the algorithms of Abasi et al. and Gabizon et al. do not even show that any of these three problems is in XP parameterized by k/r alone. We resolve the wider question by (a) obtaining a 2O((k/r) log(k/r))(n + log k)O(1)-time algorithm for r-Simple k-Path on digraphs and a 2O(k/r) sdot (n + log k)O(1)-time algorithm for r-Simple k-Path on undirected graphs (i.e., for undirected graphs, we answer the original question in affirmative), (b) showing that P-Set (r, q)-Packing is FPT (in contrast, we prove that P-Multiset (r, q)-Packing is W[1]-hard), and (c) proving that (r, k)-Monomial Detection is para-NP-hard even if only two distinct variables are in polynomial P and the circuit is non-canceling. For the special case of (r, k)-Monomial Detection where k is polynomially bounded by the input size (which is in XP), we show W[1]-hardness. Along the way to solve P-Set (r, q)-Packing, we obtain a polynomial kernel for any fixed P, which resolves a question posed by Gabizon et al. regarding the existence of polynomial kernels for problems with relaxed disjointness constraints. All our algorithms are deterministic.  © 2021 ACM.",(r; k)-monomial detection; p-set (r; parameterized complexity; q)-packing; r-simple k-path,Directed graphs; Graph algorithms; NP-hard; Parameterization; Polynomials; Arithmetic circuit; Computable functions; Detection problems; Exponential time hypothesis; Following problem; Polynomial kernels; Positive integers; Randomized Algorithms; Parameter estimation
"Deterministic APSP, Orthogonal Vectors, and More",2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100315604&doi=10.1145%2f3402926&partnerID=40&md5=278f11aa63aa449783a37caced3528dc,"We show how to solve all-pairs shortest paths on n nodes in deterministic n3>/2>ω (s log n) time, and how to count the pairs of orthogonal vectors among n 0-1 vectors in d = clog n dimensions in deterministic n2-1/O(log c) time. These running times essentially match the best known randomized algorithms of Williams [46] and Abboud, Williams, and Yu [8], respectively, and the ability to count was open even for randomized algorithms. By reductions, these two results yield faster deterministic algorithms for many other problems. Our techniques can also be used to deterministically count k-satisfiability (k-SAT) assignments on n variable formulas in 2n-n/O(k) time, roughly matching the best known running times for detecting satisfiability and resolving an open problem of Santhanam [24]. A key to our constructions is an efficient way to deterministically simulate certain probabilistic polynomials critical to the algorithms of prior work, carefully applying small-biased sets and modulus-amplifying polynomials.  © 2020 ACM.",All-pairs shortest paths; derandomization; polynomial method; satisfiability,Algorithms; Mathematical techniques; 0-1 vectors; All pairs shortest paths; Deterministic algorithms; K-satisfiability; Orthogonal vectors; Randomized Algorithms; Running time; Satisfiability; Formal logic
Randomized Contractions Meet Lean Decompositions,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100302264&doi=10.1145%2f3426738&partnerID=40&md5=6083a2473e1f3b627892bac19655ad4a,"We show an algorithm that, given an n-vertex graph G and a parameter k, in time 2O(k log k) n O(1) finds a tree decomposition of G with the following properties: - every adhesion of the tree decomposition is of size at most k, and - every bag of the tree decomposition is (i,i)-unbreakable in G for every 1 1/2 i 1/2 k. Here, a set X V(G) is (a,b)-unbreakable in G if for every separation (A,B) of order at most b in G, we have |A \cap X| 1/2 a or |B X| 1/2 a. The resulting tree decomposition has arguably best possible adhesion size bounds and unbreakability guarantees. Furthermore, the parametric factor in the running time bound is significantly smaller than in previous similar constructions. These improvements allow us to present parameterized algorithms for MINIMUM BISECTION, STEINER CUT, and STEINER MULTICUT with improved parameteric factor in the running time bound. The main technical insight is to adapt the notion of lean decompositions of Thomas and the subsequent construction algorithm of Bellenbaum and Diestel to the parameterized setting.  © 2020 ACM.",Lean decompositions; minimum bisection; randomized contractions,Adhesion; Graph algorithms; Trees (mathematics); Construction algorithms; Multicuts; N-vertex graph; Parameterized; Parameterized algorithm; Randomized contraction; Running time; Tree decomposition; Parameter estimation
Fine-grained Complexity Analysis of Two Classic TSP Variants,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100299108&doi=10.1145%2f3414845&partnerID=40&md5=dac519bd458b1394e69f530baaca5ef9,"We analyze two classic variants of the TRAVELING SALESMAN PROBLEM (TSP) using the toolkit of fine-grained complexity. Our first set of results is motivated by the BITONIC TSP problem: given a set of n points in the plane, compute a shortest tour consisting of two monotone chains. It is a classic dynamic-programming exercise to solve this problem in O(n) time. While the near-quadratic dependency of similar dynamic programs for LONGEST COMMON SUBSEQUENCE and DISCRETE Fréchet Distance has recently been proven to be essentially optimal under the Strong Exponential Time Hypothesis, we show that bitonic tours can be found in subquadratic time. More precisely, we present an algorithm that solves bitonic TSP in O(nlog n) time and its bottleneck version in O(nlog 3 n) time. In the more general pyramidal TSP problem, the points to be visited are labeled 1,... ,n and the sequence of labels in the solution is required to have at most one local maximum. Our algorithms for the bitonic (bottleneck) TSP problem also work for the pyramidal TSP problem in the plane. Our second set of results concerns the popular k-OPT heuristic for TSP in the graph setting. More precisely, we study the k-OPT decision problem, which asks whether a given tour can be improved by a k-OPT move that replaces k edges in the tour by k new edges. A simple algorithm solves k-OPT in O(nk) time for fixed k. For 2-OPT, this is easily seen to be optimal. For k=3, we prove that an algorithm with a runtime of the form Õ(n3-I) exists if and only if ALL-PAIRS SHORTEST PATHS in weighted digraphs has such an algorithm. For general k-OPT, it is known that a runtime of f(k) · no(k/ log k) would contradict the Exponential Time Hypothesis. The results for k=2,3 may suggest that the actual time complexity of k-OPT is i (nk). We show that this is not the case, by presenting an algorithm that finds the best k-move in O(n s 2k/3 +1) time for fixed k ≥ 3. This implies that 4-OPT can be solved in O(n3) time, matching the best-known algorithm for 3-OPT. Finally, we show how to beat the quadratic barrier for k=2 in two important settings, namely, for points in the plane and when we want to solve 2-OPT repeatedly.  © 2020 ACM.",bitonic TSP; fine-grained complexity; k-OPT; Traveling salesman problem,Dynamic programming; Graph theory; All pairs shortest paths; Best-known algorithms; Complexity analysis; Decision problems; Exponential time hypothesis; Longest common subsequences; Programming exercise; Strong exponential time hypothesis; Traveling salesman problem
Optimal-Time Dictionary-Compressed Indexes,2021,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098570683&doi=10.1145%2f3426473&partnerID=40&md5=f5c31bff8b050e9c08ef0b11ae24bcfb,"We describe the first self-indexes able to count and locate pattern occurrences in optimal time within a space bounded by the size of the most popular dictionary compressors. To achieve this result, we combine several recent findings, including string attractors - new combinatorial objects encompassing most known compressibility measures for highly repetitive texts - and grammars based on locally consistent parsing. More in detail, letγbe the size of the smallest attractor for a text T of length n. The measureγis an (asymptotic) lower bound to the size of dictionary compressors based on Lempel-Ziv, context-free grammars, and many others. The smallest known text representations in terms of attractors use space O(γlog (n/i3)), and our lightest indexes work within the same asymptotic space. Let ϵ > 0 be a suitably small constant fixed at construction time, m be the pattern length, and occ be the number of its text occurrences. Our index counts pattern occurrences in O(m+log 2+ϵ n) time and locates them in O(m+(occ+1)log ϵ n) time. These times already outperform those of most dictionary-compressed indexes, while obtaining the least asymptotic space for any index searching within O((m+occ),polylog, n) time. Further, by increasing the space to O(γlog (n/i3)log ϵ n), we reduce the locating time to the optimal O(m+occ), and within O(γlog (n/i3)log n) space we can also count in optimal O(m) time. No dictionary-compressed index had obtained this time before. All our indexes can be constructed in O(n) space and O(nlog n) expected time. As a by-product of independent interest, we show how to build, in O(n) expected time and without knowing the sizeγof the smallest attractor (which is NP-hard to find), a run-length context-free grammar of size O(γlog (n/i3)) generating (only) T. As a result, our indexes can be built without knowingi3.  © 2020 ACM.",attractors; compressed text indexes; grammar compression; locally consistent parsing; Repetitive string collections,Context free grammars; Dynamical systems; NP-hard; Asymptotic space; Combinatorial objects; Construction time; Dictionary compressors; Expected time; Locally consistent parsing; Pattern length; Text representation; Optimization
Approximate Single-Source Fault Tolerant Shortest Path,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092567730&doi=10.1145%2f3397532&partnerID=40&md5=f2d73aa5433701d6b078547dc9ad14b6,"Let G=(V,E) be an n-vertices m-edges directed graph with edge weights in the range [1,W] for some parameter W, and sμ V be a designated source. In this article, we address several variants of the problem of maintaining the (1+ϵ)-approximate shortest path from s to each vμ V{s} in the presence of a failure of an edge or a vertex. From the graph theory perspective, we show that G has a subgraph H with Õ(ϵ -1} nlog W) edges such that for any x,vμ V, the graph H \ x contains a path whose length is a (1+ϵ)-approximation of the length of the shortest path from s to v in G \ x. We show that the size of the subgraph H is optimal (up to logarithmic factors) by proving a lower bound of ω (ϵ -1 n log W) edges. Demetrescu, Thorup, Chowdhury, and Ramachandran (SICOMP 2008) showed that the size of a fault tolerant exact shortest path subgraph in weighted directed/undirected graphs is ω (m). Parter and Peleg (ESA 2013) showed that even in the restricted case of unweighted undirected graphs, the size of any subgraph for the exact shortest path is at least ω (n1.5). Therefore, a (1+ϵ)-approximation is the best one can hope for. We consider also the data structure problem and show that there exists an φ(ϵ -1 n log W) size oracle that for any vμ V reports a (1+ϵ)-approximate distance of v from s on a failure of any xμ V in O(log log 1+ϵ (nW)) time. We show that the size of the oracle is optimal (up to logarithmic factors) by proving a lower bound of ω (ϵ -1 nlog W log -1 n). Finally, we present two distributed algorithms. We present a single-source routing scheme that can route on a (1+ϵ)-approximation of the shortest path from a fixed source s to any destination t in the presence of a fault. Each vertex has a label and a routing table of φ(ϵ -1 log W) bits. We present also a labeling scheme that assigns each vertex a label of φ(ϵ -1log W) bits. For any two vertices x,vμ V, the labeling scheme outputs a (1+ϵ)-approximation of the distance from s to v in G \ x using only the labels of x and v.  © 2020 ACM.",approximate-distances; Fault-tolerant; oracle; routing; subgraph,Directed graphs; Fault tolerance; Graph algorithms; Edge weights; Fault-tolerant; Fixed source; Labeling scheme; Routing table; Shortest path; Single source; Undirected graph; Graph structures
Tree Edit Distance Cannot be Computed in Strongly Subcubic Time (Unless APSP Can),2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092596964&doi=10.1145%2f3381878&partnerID=40&md5=64f827c6de48465e7b286db6ee4f798d,"The edit distance between two rooted ordered trees with n nodes labeled from an alphabet is the minimum cost of transforming one tree into the other by a sequence of elementary operations consisting of deleting and relabeling existing nodes, as well as inserting new nodes. Tree edit distance is a well-known generalization of string edit distance. The fastest known algorithm for tree edit distance runs in cubic O(n3) time and is based on a similar dynamic programming solution as string edit distance. In this article, we show that a truly subcubic O(n3-ϵ) time algorithm for tree edit distance is unlikely: For || = ω (n), a truly subcubic algorithm for tree edit distance implies a truly subcubic algorithm for the all pairs shortest paths problem. For || = O(1), a truly subcubic algorithm for tree edit distance implies an O(nk-ϵ) algorithm for finding a maximum weight k-clique. Thus, while in terms of upper bounds string edit distance and tree edit distance are highly related, in terms of lower bounds string edit distance exhibits the hardness of the strong exponential time hypothesis (Backurs, Indyk STOC'15) whereas tree edit distance exhibits the hardness of all pairs shortest paths. Our result provides a matching conditional lower bound for one of the last remaining classic dynamic programming problems.  © 2020 ACM.",Conditional lower bound; string edit distance; tree edit distance,Dynamic programming; Forestry; Hardness; All pairs shortest paths; Dynamic programming problems; Elementary operations; Programming solutions; Strong exponential time hypothesis; Subcubic algorithms; Time algorithms; Tree edit distance; Trees (mathematics)
Deterministic Sparse Suffix Sorting in the Restore Model,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092591837&doi=10.1145%2f3398681&partnerID=40&md5=8a016260ad19e7b9f5c350ca5615be6a,"Given a text T of length n, we propose a deterministic online algorithm computing the sparse suffix array and the sparse longest common prefix array of T in O(c g lg n + m lg m lg n lg∗ n) time with O(m) words of space under the premise that the space of T is rewritable, where m ≤ n is the number of suffixes to be sorted (provided online and arbitrarily), and c is the number of characters with m ≤ c ≤ n that must be compared for distinguishing the designated suffixes.  © 2020 ACM.",alphabet reduction; deterministic algorithms; edit-sensitive parsing; online algorithms; Sparse suffix sorting,Algorithms; Deterministic online algorithms; Longest common prefixes; Rewritable; Suffix arrays; Suffix sorting; Mathematical techniques
Symmetry Exploitation for Online Machine Covering with Bounded Migration,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092588047&doi=10.1145%2f3397535&partnerID=40&md5=5f4e057fa49a1753fe79371aac4f0488,"Online models that allow recourse can be highly effective in situations where classical online models are too pessimistic. One such problem is the online machine covering problem on identical machines. In this setting, jobs arrive one by one and must be assigned to machines with the objective of maximizing the minimum machine load. When a job arrives, we are allowed to reassign some jobs as long as their total size is (at most) proportional to the processing time of the arriving job. The proportionality constant is called the migration factor of the algorithm. Using a rounding procedure with useful structural properties for online packing and covering problems, we design first a simple (1.7 + ϵ)-competitive algorithm using a migration factor of O(1/ϵ), which maintains at every arrival a locally optimal solution with respect to the Jump neighborhood. After that, we present as our main contribution a more involved (4/3+ϵ)-competitive algorithm using a migration factor of Å (1/ϵ 3). At every arrival, we run an adaptation of the Largest Processing Time first (LPT) algorithm. Since the new job can cause a complete change of the assignment of smaller jobs in both cases, a low migration factor is achieved by carefully exploiting the highly symmetric structure obtained by the rounding procedure.  © 2020 ACM.",bounded migration; LPT; Machine covering; online; scheduling,Algorithms; Competitive algorithms; Identical machines; Jump neighborhoods; Largest processing time; Optimal solutions; Packing and coverings; Rounding procedures; Symmetric structures; Mathematical techniques
Parameterized Hardness of Art Gallery Problems,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092571336&doi=10.1145%2f3398684&partnerID=40&md5=c59af15b0b127c7a9975757085a75c60,"Given a simple polygon P on n vertices, two points x, y in P are said to be visible to each other if the line segment between x and y is contained in P. The Point Guard Art Gallery problem asks for a minimum set S such that every point in P is visible from a point in S. The Vertex Guard Art Gallery problem asks for such a set S subset of the vertices of P. A point in the set S is referred to as a guard. For both variants, we rule out any f(k)no(k log k) algorithm, where k := |S| is the number of guards, for any computable function f, unless the exponential time hypothesis fails. These lower bounds almost match the nO(k) algorithms that exist for both problems.  © 2020 ACM.",art gallery; Computational geometry; ETH lower bound; intractability; parameterized complexity,Algorithms; Art gallery problem; Computable functions; Exponential time hypothesis; Line segment; Lower bounds; Parameterized; Simple polygon; Vertex guards; Mathematical techniques
Dynamic Parameterized Problems and Algorithms,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092548457&doi=10.1145%2f3395037&partnerID=40&md5=5bee205dfb4ae8a3dfed71fede43770d,"Fixed-parameter algorithms and kernelization are two powerful methods to solve NP-hard problems. Yet so far those algorithms have been largely restricted to static inputs. In this article, we provide fixed-parameter algorithms and kernelizations for fundamental NP-hard problems with dynamic inputs. We consider a variety of parameterized graph and hitting set problems that are known to have f(k)n1+o(1) time algorithms on inputs of size n, and we consider the question of whether there is a data structure that supports small updates (such as edge/vertex/set/element insertions and deletions) with an update time of g(k)no(1); such an update time would be essentially optimal. Update and query times independent of n are particularly desirable. Among many other results, we show that FEEDBACK VERTEX SET and k-PATH admit dynamic algorithms with f(k)log O(1) update and query times for some function f depending on the solution size k only. We complement our positive results by several conditional and unconditional lower bounds. For example, we show that unlike their undirected counterparts, DIRECTED FEEDBACK VERTEX SET and DIRECTED k-PATH do not admit dynamic algorithms with no(1) update and query times even for constant solution sizes k ≤ 3, assuming popular hardness hypotheses. We also show that unconditionally, in the cell probe model, DIRECTED FEEDBACK VERTEX SET cannot be solved with update time that is purely a function of k.  © 2020 ACM.",cell probe lower bounds; dynamic algorithms; Fixed-parameter algorithms,Graph algorithms; Graph theory; NP-hard; Constant solution; Dynamic algorithm; Feedback vertex set; Fixed-parameter algorithms; Hitting set problems; Insertions and deletions; Parameterized graphs; Parameterized problems; Parameter estimation
Polylogarithmic Approximation Algorithms for Weighted-F-deletion Problems,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092564934&doi=10.1145%2f3389338&partnerID=40&md5=53f01c456f3cc3855dee46c5fb6d4bb7,"For a family of graphs F, the Weighted F Vertex Deletion problem, is defined as follows: given an n-vertex undirected graph G and a weight function w: V(G)→ℝ F, find a minimum weight subset S¢V(G) such that G-S belongs to F. We devise a recursive scheme to obtain O(logO(1) n)-approximation algorithms for such problems, building upon the classical technique of finding balanced separators. We obtain the first O(logO(1) n)-approximation algorithms for the following problems. •Let F be a finite set of graphs containing a planar graph, and F=G(F) be the maximal family of graphs such that every graph HϵG(F) excludes all graphs in F as minors. The vertex deletion problem corresponding to F=G(F) is the Weighted Planar F-Minor-Free Deletion (WP F-MFD) problem. We give a randomized and a deterministic approximation algorithms for WP F-MFD with ratios O(log1.5 n) and O(log2 n), respectively. Prior to our work, a randomized constant factor approximation algorithm for the unweighted version was known [FOCS 2012]. After our work, a deterministic constant factor approximation algorithm for the unweighted version was also obtained [SODA 2019]. •We give an O(log2 n)-factor approximation algorithm for Weighted Chordal Vertex Deletion, the vertex deletion problem to the family of chordal graphs. On the way to this algorithm, we also obtain a constant factor approximation algorithm for Multicut on chordal graphs. We give an O(log3 n)-factor approximation algorithm for WeightedDistance Hereditary Vertex Deletion. We believe that our recursive scheme can be applied to obtain O(logO(1) n)-approximation algorithms for many other problems as well.  © 2020 ACM.",Approximation algorithm; balanced separators; chordal graphs; distance hereditary graphs; F-Vertex Deletion; Planar F-minor-free graphs,Graph algorithms; Graph theory; Graphic methods; Balanced separators; Classical techniques; Constant-factor approximation algorithms; Deterministic approximation; Factor approximation algorithms; Following problem; Polylogarithmic approximation; Vertex deletion problems; Approximation algorithms
Point-Width and Max-CSPs,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092564416&doi=10.1145%2f3409447&partnerID=40&md5=be5a2defeefc13fb930ac8bdf53fc673,"The complexity of (unbounded-arity) Max-CSPs under structural restrictions is poorly understood. The two most general hypergraph properties known to ensure tractability of Max-CSPs, β-acyclicity and bounded (incidence) MIM-width, are incomparable and lead to very different algorithms. We introduce the framework of point decompositions for hypergraphs and use it to derive a new sufficient condition for the tractability of (structurally restricted) Max-CSPs, which generalises both bounded MIM-width and β-acyclicity. On the way, we give a new characterisation of bounded MIM-width and discuss other hypergraph properties which are relevant to the complexity of Max-CSPs, such as β-hypertreewidth.  © 2020 ACM.",constraint satisfaction; hypergraphs; hypertree width; maximum constraint satisfaction; β-acyclicity,Algorithms; Acyclicity; Hypergraph; Structural restrictions; Mathematical techniques
Maximum Matching in the Online Batch-arrival Model,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092565595&doi=10.1145%2f3399676&partnerID=40&md5=c631bc89bf6aedd6472ae4f41b8f781e,"Consider a two-stage matching problem, where edges of an input graph are revealed in two stages (batches) and in each stage we have to immediately and irrevocably extend our matching using the edges from that stage. The natural greedy algorithm is half competitive. Even though there is a huge literature on online matching in adversarial vertex arrival model, no positive results were previously known in adversarial edge arrival model. For two-stage bipartite matching problem, we show that the optimal competitive ratio is exactly 2/3 in both the fractional and the randomized-integral models. Furthermore, our algorithm for fractional bipartite matching is instance optimal, i.e., it achieves the best competitive ratio for any given first stage graph. We also study natural extensions of this problem to general graphs and to s stages and present randomized-integral algorithms with competitive ratio 12 + 2-O(s). Our algorithms use a novel Instance-Optimal-LP and combine graph decomposition techniques with online primal-dual analysis.  © 2020 ACM.",competitive ratio; Edmonds-Gallai decomposition; matching; OnlineAlgorithms; primal-dual analysis; semi-streaming,Algorithms; Mathematical techniques; Bipartite matching problems; Bipartite matchings; Graph decompositions; Greedy algorithms; Maximum matchings; Natural extension; Online primal duals; Optimal competitive ratios; Graph algorithms
The Non-Uniform k-Center Problem,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092598543&doi=10.1145%2f3392720&partnerID=40&md5=07b754538782c1c66fbedb13dc546f83,"In this article, we introduce and study the Non-Uniform k-Center (NUkC) problem. Given a finite metric space (X, d) and a collection of balls of radii { r1 ≥ ... ≥ rk}, the NUkC problem is to find a placement of their centers in the metric space and find the minimum dilation α, such that the union of balls of radius α ⋅ ri around the ith center covers all the points in X. This problem naturally arises as a min-max vehicle routing problem with fleets of different speeds. The NUkC problem generalizes the classic k-center problem, wherein all the k radii are the same (which can be assumed to be 1 after scaling). It also generalizes the k-center with outliers (kCwO for short) problem, in which there are k balls of radius 1 and ℓ (number of outliers) balls of radius 0. Before this work, there was a 2-approximation and 3-approximation algorithm known for these problems, respectively; the former is best possible unless P=NP. We first observe that no O(1)-approximation to the optimal dilation is possible unless P=NP, implying that the NUkC problem is harder than the above two problems. Our main algorithmic result is an (O(1), O(1))-bi-criteria approximation result: We give an O(1)-approximation to the optimal dilation; however, we may open Θ(1) centers of each radii. Our techniques also allow us to prove a simple (uni-criterion), optimal 2-approximation to the kCwO problem improving upon the long-standing 3-factor approximation for this problem. Our main technical contribution is a connection between the NUkC problem and the so-called firefighter problems on trees that have been studied recently in the TCS community. We show NUkC is at least as hard as the firefighter problem. While we do nt know whether the converse is true, we are able to adapt ideas from recent works [1, 3] in non-trivial ways to obtain our constant factor bi-criteria approximation.  © 2020 ACM.",Clustering algorithms; firefighting on trees; outliers,Fire extinguishers; Set theory; Topology; Vehicle routing; Approximation results; Constant factors; Different speed; Finite metric spaces; K-center problem; Technical contribution; Union of balls; Vehicle Routing Problems; Statistics
A Complexity Theoretical Study of Fuzzy K-Means,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092570395&doi=10.1145%2f3409385&partnerID=40&md5=b74e3d0ccdfd43a3173b74600182fef2,"The fuzzy K-means problem is a popular generalization of the well-known K-means problem to soft clusterings. In this article, we present the first algorithmic study of the problem going beyond heuristics. Our main result is that, assuming a constant number of clusters, there is a polynomial time approximation scheme for the fuzzy K-means problem. As a part of our analysis, we also prove the existence of small coresets for fuzzy K-means. At the heart of our proofs are two novel techniques developed to analyze the otherwise notoriously difficult fuzzy K-means objective function.  © 2020 ACM.",approximation algorithms; Clustering; coresets; fuzzy K-means,Polynomial approximation; Clusterings; Core set; Fuzzy k-means; Novel techniques; Number of clusters; Objective functions; Polynomial time approximation schemes; Theoretical study; K-means clustering
A Colored Path Problem and Its Applications,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092545567&doi=10.1145%2f3396573&partnerID=40&md5=b5c78553786e6245e89510a107130346,"Given a set of obstacles and two points in the plane, is there a path between the two points that does not cross more than k different obstacles? Equivalently, can we remove k obstacles so that there is an obstacle-free path between the two designated points? This is a fundamental NP-hard problem that has undergone a tremendous amount of research work. The problem can be formulated and generalized into the following graph problem: Given a planar graph G whose vertices are colored by color sets, two designated vertices s, t ∈ V(G), and k ∈ N, is there an s-t path in G that uses at most k colors? If each obstacle is connected, then the resulting graph satisfies the color-connectivity property, namely that each color induces a connected subgraph. We study the complexity and design algorithms for the above graph problem with an eye on its geometric applications. We prove a set of hardness results, including a result showing that the color-connectivity property is crucial for any hope for fixed-parameter tractable (FPT) algorithms. We also show that our hardness results translate to the geometric instances of the problem. We then focus on graphs satisfying the color-connectivity property. We design an FPT algorithm for this problem parameterized by both k and the treewidth of the graph and extend this result further to obtain an FPT algorithm for the parameterization by both k and the length of the path. The latter result implies and explains previous FPT results for various obstacle shapes.  © 2020 ACM.",barrier coverage; barrier resilience; colored path; minimum constraint removal; motion planning; Parameterized complexity and algorithms; planar graphs,Color; Graph algorithms; Graph structures; Hardness; NP-hard; Parameter estimation; Color connectivity; Connected Subgraph; Designated points; Geometric applications; Graph problems; Hardness result; ITS applications; Obstacle shapes; Graph theory
Edge Estimation with Independent Set Oracles,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092559148&doi=10.1145%2f3404867&partnerID=40&md5=cc3fd797e523c6fd0c6c838ab16df477,"We study the task of estimating the number of edges in a graph, where the access to the graph is provided via an independent set oracle. Independent set queries draw motivation from group testing and have applications to the complexity of decision versus counting problems. We give two algorithms to estimate the number of edges in an n-vertex graph, using (i) polylog(n) bipartite independent set queries or (ii) n2/3 polylog(n) independent set queries.  © 2020 ACM.",graph parameter estimation; Independent set queries,Algorithms; Mathematical techniques; Counting problems; Edge estimation; Group testing; Independent set; N-vertex graph; Graph algorithms
Approximating Spanners and Directed Steiner Forest,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088690469&doi=10.1145%2f3381451&partnerID=40&md5=05a1a8b72e28d539b58faf179241ace8,"It was recently found that there are very close connections between the existence of additive spanners (subgraphs where all distances are preserved up to an additive stretch), distance preservers (subgraphs in which demand pairs have their distance preserved exactly), and pairwise spanners (subgraphs in which demand pairs have their distance preserved up to a multiplicative or additive stretch) [Abboud-Bodwin SODA'16 8 J.ACM'17, Bodwin-Williams SODA'16]. We study these problems from an optimization point of view, where rather than studying the existence of extremal instances, we are given an instance and are asked to find the sparsest possible spanner/preserver. We give an O(n3/5 + ϵ)-approximation for distance preservers and pairwise spanners (for arbitrary constant ϵ > 0). This is the first nontrivial upper bound for either problem, both of which are known to be as hard to approximate as Label Cover. We also prove Label Cover hardness for approximating additive spanners, even for the cases of additive 1 stretch (where one might expect a polylogarithmic approximation, since the related multiplicative 2-spanner problem admits an O(log n)-approximation) and additive polylogarithmic stretch (where the related multiplicative spanner problem has an O(1)-approximation). Interestingly, the techniques we use in our approximation algorithm extend beyond distance-based problem to pure connectivity network design problems. In particular, our techniques allow us to give an O(n3/5 + ϵ)-approximation for the Directed Steiner Forest problem (for arbitrary constant ϵ > 0) when all edges have uniform costs, improving the previous best O(n2/3 + ϵ)-approximation due to Berman et al. [ICALP'11] (which holds for general edge costs).  © 2020 ACM.",Approximation algorithms; directed spanner; directed Steiner forest; hardness of approximation; network design,Additives; Forestry; Additive spanners; Arbitrary constants; Distance-based; Network design problems; Polylogarithmic; Polylogarithmic approximation; Steiner forest problem; Steiner forests; Approximation algorithms
The Complexity of Cake Cutting with Unequal Shares,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088709681&doi=10.1145%2f3380742&partnerID=40&md5=5cef06f1621c187a71fa16acceec1817,"An unceasing problem of our prevailing society is the fair division of goods. The problem of proportional cake cutting focuses on dividing a heterogeneous and divisible resource, the cake, among n players who value pieces according to their own measure function. The goal is to assign each player a not necessarily connected part of the cake that the player evaluates at least as much as her proportional share. In this article, we investigate the problem of proportional division with unequal shares, where each player is entitled to receive a predetermined portion of the cake. Our main contribution is threefold. First we present a protocol for integer demands, which delivers a proportional solution in fewer queries than all known protocols. By giving a matching lower bound, we then show that our protocol is asymptotically the fastest possible. Finally, we turn to irrational demands and solve the proportional cake cutting problem by reducing it to the same problem with integer demands only. All results remain valid in a highly general cake cutting model, which can be of independent interest.  © 2020 ACM.",Cake cutting; fair division; proportional division; unequal shares,Algorithms; Cake cuttings; Fair divisions; Lower bounds; Measure function; Proportional-share; Mathematical techniques
Improved Dynamic Graph Coloring,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088706915&doi=10.1145%2f3392724&partnerID=40&md5=986715e4a453ab4d7b3a70a6cd3a9ac0,"This article studies the fundamental problem of graph coloring in fully dynamic graphs. Since the problem of computing an optimal coloring, or even approximating it to within n1-ϵ for any ϵ > 0, is NP-hard in static graphs, there is no hope to achieve any meaningful computational results for general graphs in the dynamic setting. It is therefore only natural to consider the combinatorial aspects of dynamic coloring or alternatively, study restricted families of graphs. Toward understanding the combinatorial aspects of this problem, one may assume a black-box access to a static algorithm for C-coloring any subgraph of the dynamic graph, and investigate the trade-off between the number of colors and the number of recolorings per update step. Optimizing the number of recolorings, sometimes referred to as the recourse bound, is important for various practical applications. In WADS '17, Barba et al. devised two complementary algorithms: for any β > 0, the first (respectively, second) maintains an O(Cβn1/β) (respectively, O(Cβ)-coloring while recoloring O(β) (respectively, O(βn1/β)) vertices per update. Barba et al. also showed that the second trade-off appears to exhibit the right behavior, at least for β = O(1): any algorithm that maintains a C-coloring of an n-vertex dynamic forest must recolor ω (n2C(C-1)) vertices per update, for any constant C ≥ 2. Our contribution is twofold: •We devise a new algorithm for general graphs that improves significantly upon the first trade-off in a wide range of parameters: for any β > 0, we get a Ô (Cβlog2 n)-coloring with O(β) recolorings per update, where the Ô notation suppresses polyloglog(n) factors. In particular, for β = O(1), we get constant recolorings with polylog(n) colors; not only is this an exponential improvement over the previous bound but also it unveils a rather surprising phenomenon: the trade-off between the number of colors and recolorings is highly non-symmetric. •For uniformly sparse graphs, we use low out-degree orientations to strengthen the preceding result by bounding the update time of the algorithm rather than the number of recolorings. Then, we further improve this result by introducing a new data structure that refines bounded out-degree edge orientations and is of independent interest. From this data structure, we get a deterministic algorithm for graphs of arboricity I' that maintains an O(I' log2 n)-coloring in amortized O(1) time.  © 2020 ACM.",Coloring; dynamic graph algorithms; edge orientations; graph arboricity,Color; Coloring; Data structures; Economic and social effects; Graph algorithms; Graphic methods; Optimization; Combinatorial aspect; Computational results; Deterministic algorithms; Dynamic settings; Edge orientations; Graph colorings; Optimal coloring; Static algorithms; Graph theory
Tight Bounds on Online Checkpointing Algorithms,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088707298&doi=10.1145%2f3379543&partnerID=40&md5=891a67de90cde875992cbe707e30818b,"The problem of online checkpointing is a classical problem with numerous applications that has been studied in various forms for almost 50 years. In the simplest version of this problem, a user has to maintain k memorized checkpoints during a long computation, where the only allowed operation is to move one of the checkpoints from its old time to the current time, and his goal is to keep the checkpoints as evenly spread out as possible at all times. Bringmann, Doerr, Neumann, and Sliacan studied this problem as a special case of an online/offline optimization problem in which the deviation from uniformity is measured by the natural discrepancy metric of the worst case ratio between real and ideal segment lengths. They showed this discrepancy is smaller than 1.59-o(1) for all k and smaller than ln 4-o(1)≈ 1.39 for the sparse subset of k's, which are powers of 2. In addition, they obtained upper bounds on the achievable discrepancy for some small values of k. In this article, we solve the main problems left open in the above-mentioned paper by proving that ln 4 is a tight upper and lower bound on the asymptotic discrepancy for all large k and by providing tight upper and lower bounds (in the form of provably optimal checkpointing algorithms, some of which are in fact better than those of Bringmann et al.) for all the small values of k ≤ 10. In the last part of the article, we describe some new applications of this online checkpointing problem.  © 2020 ACM.",Checkpoints; competitive analysis; online algorithms,Algorithms; Check pointing; Check-pointing algorithms; Classical problems; New applications; Optimal checkpointing; Optimization problems; Segment lengths; Upper and lower bounds; Mathematical techniques
A PTAS for Euclidean TSP with Hyperplane Neighborhoods,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088690278&doi=10.1145%2f3383466&partnerID=40&md5=774e2b9e1f9aa7db96ca0019ff2fe55e,"In the Traveling Salesperson Problem with Neighborhoods (TSPN), we are given a collection of geometric regions in some space. The goal is to output a tour of minimum length that visits at least one point in each region. Even in the Euclidean plane, TSPN is known to be APX-hard [27{, which gives rise to studying more tractable special cases of the problem. In this article, we focus on the fundamental special case of regions that are hyperplanes in the d-dimensional Euclidean space. This case contrasts the much-better understood case of so-called fat regions [20, 40{. While for d = 2, an exact algorithm with a running time of O(n5) is known [34{, settling the exact approximability of the problem for d = 3 has been repeatedly posed as an open question [29, 30, 40, 47{. To date, only an approximation algorithm with guarantee exponential in d is known [30{, and NP-hardness remains open. For arbitrary fixed d, we develop a Polynomial Time Approximation Scheme (PTAS) that works for both the tour and path version of the problem. Our algorithm is based on approximating the convex hull of an optimal tour by a convex polytope of bounded complexity. After enumerating a number of structural properties of these polytopes, a linear program finds one of them that minimizes the length of the tour. As the approximation guarantee approaches 1, our scheme adjusts the complexity of the considered polytopes accordingly. In the analysis of our approximation scheme, we show that our search space includes a sufficiently good approximation of the optimum. To do so, we develop a novel and general sparsification technique that transforms an arbitrary convex polytope into one with a constant number of vertices, and, subsequently, into one of bounded complexity in the above sense. We show that this transformation does not increase the tour length by too much, while the transformed tour visits any hyperplane that it visited before the transformation.  © 2020 ACM.",approximation algorithms; computational geometry; hyperplane neighborhoods; TSP; TSPN,Geometry; Linear programming; Polynomial approximation; Topology; Approximation scheme; Bounded complexity; Convex polytopes; Euclidean planes; Euclidean spaces; Exact algorithms; Polynomial time approximation schemes; Traveling salesperson problem with neighborhoods; Approximation algorithms
Periods of Iterations of Functions with Restricted Preimage Sizes,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088694635&doi=10.1145%2f3378570&partnerID=40&md5=8eb5c58880366172aa67daae1a929d96,"Let [n{ = {1, ..., n} and let ωn be the set of all mappings from [n{ to itself. Let f be a random uniform element of ωn and let T(f) and B(f) denote, respectively, the least common multiple and the product of the length of the cycles of f. Harris proved in 1973 that T converges in distribution to a standard normal distribution and, in 2011, Schmutz obtained an asymptotic estimate on the logarithm of the expectation of T and B over all mappings on n nodes. We obtain analogous results for random uniform mappings on n = kr nodes with preimage sizes restricted to a set of the form {0,k}, where k = k(r) ≥ 2. This is motivated by the use of these classes of mappings as heuristic models for the statistics of polynomials of the form xk + a over the integers modulo p, with p 1 (mod k). We exhibit and discuss our numerical results on this heuristic.  © 2020 ACM.",computations in finite fields; Enumeration; permutations and combinations,Normal distribution; Asymptotic estimates; Heuristic model; Least common multiple; Numerical results; Pre images; Standard normal distributions; Mapping
Nearly ETH-tight Algorithms for Planar Steiner Tree with Terminals on Few Faces,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088691082&doi=10.1145%2f3371389&partnerID=40&md5=f3403b3eeb7e4ebd7b9856bc2202adb5,"The STEINER TREE problem is one of the most fundamental NP-complete problems, as it models many network design problems. Recall that an instance of this problem consists of a graph with edge weights and a subset of vertices (often called terminals); the goal is to find a subtree of the graph of minimum total weight that connects all terminals. A seminal paper by Erickson et al. [Math. Oper. Res., 1987{ considers instances where the underlying graph is planar and all terminals can be covered by the boundary of k faces. Erickson et al. show that the problem can be solved by an algorithm using nO(k) time and nO(k) space, where n denotes the number of vertices of the input graph. In the past 30 years there has been no significant improvement of this algorithm, despite several efforts. In this work, we give an algorithm for PLANAR STEINER TREE with running time 2O(k)nO(k) with the above parameterization, using only polynomial space. Furthermore, we show that the running time of our algorithm is almost tight: We prove that there is no f(k)no(k) algorithm for PLANAR STEINER TREE for any computable function f, unless the Exponential Time Hypothesis fails.  © 2020 ACM.",exact algorithms; exponential time hypothesis; lower bound; parameterized algorithms; Planar graphs; Steiner tree,Computational complexity; Graph algorithms; Graph structures; Computable functions; Exponential time hypothesis; Minimum total weights; Network design problems; Polynomial space; Steiner tree problem; Steiner trees; Underlying graphs; Trees (mathematics)
Structure Learning of H-Colorings,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088696004&doi=10.1145%2f3382207&partnerID=40&md5=3512c5a7dec396d02b713d7d58ba6db9,"We study the following structure learning problem for H-colorings. For a fixed (and known) constraint graph H with q colors, given access to uniformly random H-colorings of an unknown graph G=(V,E), how many samples are required to learn the edges of G? We give a characterization of the constraint graphs H for which the problem is identifiable for every G and show that there are identifiable constraint graphs for which one cannot hope to learn every graph G efficiently. We provide refined results for the case of proper vertex q-colorings of graphs of maximum degree d. In particular, we prove that in the tree uniqueness region (i.e., when q≤ d), the problem is identifiable and we can learn G in poly(d,q)× O(n2 log n) time. In the tree non-uniqueness region (i.e., when q≤ d), we show that the problem is not identifiable and thus G cannot be learned. Moreover, when q ≤ d - d + Θ(1), we establish that even learning an equivalent graph (any graph with the same set of H-colorings) is computationally hard - sample complexity is exponential in n in the worst case. We further explore the connection between the efficiency/hardness of the structure learning problem and the uniqueness/non-uniqueness phase transition for general H-colorings and prove that under a well-known uniqueness condition in statistical physics, we can learn G in poly(d,q)× O(n2 log n) time.  © 2020 ACM.",H-colorings; identifiability; Markov random fields; spin systems; Structure learning,Coloring; Forestry; Statistical Physics; Constraint graph; Graph G; Maximum degree; Nonuniqueness; Sample complexity; Structure-learning; Trees (mathematics)
An Improved Isomorphism Test for Bounded-tree-width Graphs,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088696800&doi=10.1145%2f3382082&partnerID=40&md5=f8e0c0eb4311b4d6b978206684d2e505,"We give a new FPT algorithm testing isomorphism of n-vertex graphs of tree-width k in time 2kpolylog(k)n3, improving the FPT algorithm due to Lokshtanov, Pilipczuk, Pilipczuk, and Saurabh (FOCS 2014), which runs in time 2O(k5 log k)n5. Based on an improved version of the isomorphism-invariant graph decomposition technique introduced by Lokshtanov et al., we prove restrictions on the structure of the automorphism groups of graphs of tree-width k. Our algorithm then makes heavy use of the group theoretic techniques introduced by Luks (JCSS 1982) in his isomorphism test for bounded degree graphs and Babai (STOC 2016) in his quasipolynomial isomorphism test. In fact, we even use Babai's algorithm as a black box in one place. We also give a second algorithm that, at the price of a slightly worse running time 2O(k2 log k)n3, avoids the use of Babai's algorithm and, more importantly, has the additional benefit that it can also be used as a canonization algorithm.  © 2020 ACM.",decompositions; graph canonization; Graph isomorphism; tree-width,Forestry; Graph algorithms; Graphic methods; Group theory; Automorphism groups; Babai's algorithms; Bounded degree graphs; FPT algorithms; Graph decompositions; N-vertex graph; Quasi-poly-nomial; Running time; Trees (mathematics)
Covering Small Independent Sets and Separators with Applications to Parameterized Algorithms,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088702843&doi=10.1145%2f3379698&partnerID=40&md5=795187175c1c2b0b1cda64ebe13ae862,"We present two new combinatorial tools for the design of parameterized algorithms. The first is a simple linear time randomized algorithm that given as input a d-degenerate graph G and an integer k, outputs an independent set Y, such that for every independent set X in G of size at most k, the probability that X is a subset of Y is at least (((d+1)kk) . k(d+1))-1. The second is a new (deterministic) polynomial time graph sparsification procedure that given a graph G, a set T = s1, t1 , s2, t2, .... , s , t of terminal pairs, and an integer k, returns an induced subgraph G∗ of G that maintains all the inclusion minimal multicuts of G of size at most k and does not contain any (k+2)-vertex connected set of size 2O(k). In particular, G∗ excludes a clique of size 2O(k) as a topological minor. Put together, our new tools yield new randomized fixed parameter tractable (FPT) algorithms for STABLE s-t SEPARATOR, STABLE ODD CYCLE TRANSVERSAL, and STABLE MULTICUT on general graphs, and for STABLE DIRECTED FEEDBACK VERTEX SET on d-degenerate graphs, resolving two problems left open by Marx et al. [ACM Transactions on Algorithms, 2013{. All of our algorithms can be derandomized at the cost of a small overhead in the running time.  © 2020 ACM.",Independece covering family; parameterized algorithms; stable multicut; stable OCT; stable s-t separator,Graph algorithms; Parameter estimation; Polynomial approximation; Separators; Combinatorial tools; Feedback vertex set; Graph sparsification; Induced subgraphs; Odd cycle transversals; Parameterized algorithm; Randomized Algorithms; Topological-minor; Graph theory
Clustering in Hypergraphs to Minimize Average Edge Service Time,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088695388&doi=10.1145%2f3386121&partnerID=40&md5=ed94a57cfc66f6a80ba3ee5ad4a9eed4,"We study the problem of clustering the vertices of a weighted hypergraph such that on average the vertices of each edge can be covered by a small number of clusters. This problem has many applications, such as for designing medical tests, clustering files on disk servers, and placing network services on servers. The edges of the hypergraph model groups of items that are likely to be needed together, and the optimization criteria that we use can be interpreted as the average delay (or cost) to serve the items of a typical edge. We describe and analyze algorithms for this problem for the case in which the clusters have to be disjoint and for the case where clusters can overlap. The analysis is often subtle and reveals interesting structure and invariants that one can utilize.  © 2020 ACM.",average cover time; Clustering; hypergraphs; set cover,Average delay; Edge services; Hypergraph; Hypergraph model; Network services; Number of clusters; Optimization criteria; Graph theory
Random Walks on Small World Networks,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088707183&doi=10.1145%2f3382208&partnerID=40&md5=6e3819f1830d442829150027169b1f75,"We study the mixing time of random walks on small-world networks modelled as follows: starting with the 2-dimensional periodic grid, each pair of vertices {u,v} with distance d> 1 is added as a ""long-range""edge with probability proportional to d-r, where r≥ 0 is a parameter of the model. Kleinberg [33{ studied a close variant of this network model and proved that the (decentralised) routing time is O((log n)2) when r=2 and nω (1) when rĝ‰ 2. Here, we prove that the random walk also undergoes a phase transition at r=2, but in this case, the phase transition is of a different form. We establish that the mixing time is (log n) for r< 2, O((log n)4) for r=2, and nω (1) for r> 2.  © 2020 ACM.",conductance; mixing time; phase transition; random walk; Small world,Mixing; Random processes; Decentralised; Mixing time; Network modeling; Probability proportional; Random Walk; Small-world networks
Enumerating Minimal Dominating Sets in Kt-free Graphs and Variants,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088694965&doi=10.1145%2f3386686&partnerID=40&md5=e6911f48ec00758ce4c9ded6fe63b31a,"It is a long-standing open problem whether the minimal dominating sets of a graph can be enumerated in output-polynomial time. In this article we investigate this problem in graph classes defined by forbidding an induced subgraph. In particular, we provide output-polynomial time algorithms for Kt-free graphs and for several related graph classes. This answers a question of Kanté et al. about enumeration in bipartite graphs.  © 2020 ACM.",Algorithmic enumeration; forbidden induced subgraphs; minimal dominating sets,Polynomial approximation; Bipartite graphs; Free graphs; Graph class; Induced subgraphs; Minimal dominating sets; Polynomial-time; Polynomial-time algorithms; Graph algorithms
Time- And Space-optimal Algorithm for the Many-visits TSP,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088691960&doi=10.1145%2f3382038&partnerID=40&md5=08e7616cf9f13de09718b9d954c29930,"The many-visits traveling salesperson problem (MV-TSP) asks for an optimal tour of n cities that visits each city c a prescribed number kc of times. Travel costs may be asymmetric, and visiting a city twice in a row may incur a non-zero cost. The MV-TSP problem finds applications in scheduling, geometric approximation, and Hamiltonicity of certain graph families. The fastest known algorithm for MV-TSP is due to Cosmadakis and Papadimitriou (SICOMP, 1984). It runs in time nO(n) + O(n3c kc) and requires ná¶¿(n) space. An interesting feature of the Cosmadakis-Papadimitriou algorithm is its logarithmic dependence on the total length ckc of the tour, allowing the algorithm to handle instances with very long tours. The superexponential dependence on the number of cities in both the time and space complexity, however, renders the algorithm impractical for all but the narrowest range of this parameter. In this article, we improve upon the Cosmadakis-Papadimitriou algorithm, giving an MV-TSP algorithm that runs in time 2O(n), i.e., single-exponential in the number of cities, using polynomial space. The space requirement of our algorithm is (essentially) the size of the output, and assuming the Exponential-Time Hypothesis (ETH), the problem cannot be solved in time 2o(n). Our algorithm is deterministic, and arguably both simpler and easier to analyze than the original approach of Cosmadakis and Papadimitriou. It involves an optimization over directed spanning trees and a recursive, centroid-based decomposition of trees.  © 2020 ACM.",high-multiplicity scheduling; spanning trees; TSP,Algorithms; Directed spanning trees; Exponential time hypothesis; Geometric approximations; Logarithmic dependence; Space requirements; Super-exponential; Time and space complexity; Traveling salesperson problem; Mathematical techniques
Testing Bounded Arboricity,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084762512&doi=10.1145%2f3381418&partnerID=40&md5=8948a2cbb008cedc8daac76f98c83080,"In this article, we consider the problem of testing whether a graph has bounded arboricity. The class of graphs with bounded arboricity includes many important graph families (e.g., planar graphs and randomly generated preferential attachment graphs). Graphs with bounded arboricity have been studied extensively in the past, particularly because for many problems, they allow for much more efficient algorithms and/or better approximation ratios. We present a tolerant tester in the general-graphs model. The general-graphs model allows access to degree and neighbor queries, and the distance is defined with respect to the actual number of edges. Namely, we say that a graph G is ϵ-close to having arboricity α if by removing at most an ϵ-fraction of its edges, we can obtain a graph G′ that has arboricity α, and otherwise we say that G is ϵ-far. Our algorithm distinguishes between graphs that are ϵ-close to having arboricity α and graphs that are c á ϵ-far from having arboricity 3α, where c is an absolute small constant. The query complexity and running time of the algorithm are Õ (n / ϵm) + (1 / ϵ)O(log(1/ϵ)), where n denotes the number of vertices and m denotes the number of edges (we use the notation Õ to hide poly-logarithmic factors in n). In terms of the dependence on n and m, this bound is optimal up to poly-logarithmic factors since ω(n / m) queries are necessary. © 2020 ACM.",graph arboricity; graph degeneracy; Property testing; tolerant testing,Approximation algorithms; Graph theory; Graphic methods; Approximation ratios; General graph; Neighbor query; Planar graph; Poly-logarithmic factors; Preferential attachments; Query complexity; Running time; Graph algorithms
Lower Bounds for the Parameterized Complexity of Minimum Fill-in and Other Completion Problems,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084761790&doi=10.1145%2f3381426&partnerID=40&md5=f96e9952eb1ca2738168d5fc0cbe5890,"In this work, we focus on several completion problems for subclasses of chordal graphs: MINIMUM FILL-IN, INTERVAL COMPLETION, PROPER INTERVAL COMPLETION, TRIVIALLY PERFECT COMPLETION, and THRESHOLD COMPLETION. In these problems, the task is to add at most k edges to a given graph to obtain a chordal, interval, proper interval, threshold, or trivially perfect graph, respectively. We prove the following lower bounds for all these problems, as well as for the related CHAIN COMPLETION problem: •Assuming the Exponential Time Hypothesis, none of these problems can be solved in time 2O(n1/2/logc n) or 2O(k1/4/logc k)· nO(1), for some integer c. •Assuming the non-existence of a subexponential-time approximation scheme for MIN BISECTION on d-regular graphs, for some constant d, none of these problems can be solved in time 2o(n) or 2ok)}· nO(1). For all the aforementioned completion problems, apart from PROPER INTERVAL COMPLETION, FPT algorithms with running time of the form 2O(k log k)· nO(1) are known. Thus, the second result proves that a significant improvement of any of these algorithms would lead to a surprising breakthrough in the design of approximation algorithms for MIN BISECTION. To prove our results, we use a reduction methodology based on combining the classic approach of starting with a sparse instance of 3-SAT, prepared using the Sparsification Lemma, with the existence of almost linear-size Probabilistically Checkable Proofs. Apart from our main results, we also obtain lower bounds excluding the existence of subexponential algorithms for the OPTIMUM LINEAR ARRANGEMENT problem, as well as improved, yet still not tight, lower bounds for FEEDBACK ARC SET IN TOURNAMENTS. © 2020 ACM.",Graph completion problems,Graph theory; Approximation scheme; Exponential time hypothesis; Feedback arc set in tournaments; Interval completion; Parameterized complexity; Probabilistically checkable proof; Sparsification lemmata; Sub-exponential algorithms; Approximation algorithms
Submatrix Maximum Queries in Monge and Partial Monge Matrices Are Equivalent to Predecessor Search,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084764980&doi=10.1145%2f3381416&partnerID=40&md5=3116054a602c603d610026ac9e423baa,"We present an optimal data structure for submatrix maximum queries in n× n Monge matrices. Our result is a two-way reduction showing that the problem is equivalent to the classical predecessor problem in a universe of polynomial size. This gives a data structure of O(n) space that answers submatrix maximum queries in O(log log n) time, as well as a matching lower bound, showing that O(log log n) query-time is optimal for any data structure of size O(npolylog(n)). Our result settles the problem, improving on the O(log2 n) query time in SODA'12, and on the O(log n) query-time in ICALP'14. In addition, we show that partial Monge matrices can be handled in the same bounds as full Monge matrices. In both previous results, partial Monge matrices incurred additional inverse-Ackermann factors. © 2020 ACM.",Monge matrix; predecessor search; range queries,Data structures; Structural optimization; Lower bounds; Monge matrices; Polynomial size; Predecessor problems; Query time; Submatrix; Two ways; Matrix algebra
A Unified PTAS for Prize Collecting TSP and Steiner Tree Problem in Doubling Metrics,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084759955&doi=10.1145%2f3378571&partnerID=40&md5=1ba5d5fdbd7d745ea23a9a94392de74b,"We present a unified (randomized) polynomial-time approximation scheme (PTAS) for the prize collecting traveling salesman problem (PCTSP) and the prize collecting Steiner tree problem (PCSTP) in doubling metrics. Given a metric space and a penalty function on a subset of points known as terminals, a solution is a subgraph on points in the metric space whose cost is the weight of its edges plus the penalty due to terminals not covered by the subgraph. Under our unified framework, the solution subgraph needs to be Eulerian for PCTSP, while it needs to be a tree for PCSTP. Before our work, even a QPTAS for the problems in doubling metrics is not known. Our unified PTAS is based on the previous dynamic programming frameworks proposed in Talwar (STOC 2004) and Bartal, Gottlieb, Krauthgamer (STOC 2012). However, since it is unknown which part of the optimal cost is due to edge lengths and which part is due to penalties of uncovered terminals, we need to develop new techniques to apply previous divide-and-conquer strategies and sparse instance decompositions. © 2020 ACM.",Doubling dimension; polynomial time approximation scheme; prize collecting; Steiner tree problem; traveling salesman problem,Dynamic programming; Polynomial approximation; Set theory; Topology; Traveling salesman problem; Divide and conquer; Penalty function; Polynomial time approximation schemes; Prize collecting Steiner tree problem; Prize-collecting; Programming framework; Steiner tree problem; Unified framework; Trees (mathematics)
Doubly Balanced Connected Graph Partitioning,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084766574&doi=10.1145%2f3381419&partnerID=40&md5=3bb3305472d8d8d464a5f03607c2b30c,"We introduce and study the doubly balanced connected graph partitioning problem: Let G=(V, E) be a connected graph with a weight (supply/demand) function p : V → {-1, +1} satisfying p(V)=∑ j&isin V p(j) = 0. The objective is to partition G into (V1,V2) such that G[V1] and G[V2] are connected, |p(V1)|,|p(V2)|≤ cp, and max{ |V1 / V2|,|V2 / V1|} ≤ cs, for some constants cp and cs. When G is 2-connected, we show that a solution with cp=1 and cs=2 always exists and can be found in randomized polynomial time. Moreover, when G is 3-connected, we show that there is always a ""perfect"" solution (a partition with p(V1)=p(V2)=0 and |V1|=|V2|, if |V| 0 (mod 4)), and it can be found in randomized polynomial time. Our techniques can be extended, with similar results, to the case in which the weights are arbitrary (not necessarily ±1), and to the case that p(V) 0 and the excess supply/demand should be split evenly. They also apply to the problem of partitioning a graph with two types of nodes into two large connected subgraphs that preserve approximately the proportion of the two types. © 2020 ACM.",Graph partitioning; power grid islanding,Polynomial approximation; Connected graph; Connected subgraphs; Polynomial-time; Graph theory
Fully Dynamic MIS in Uniformly Sparse Graphs,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084759345&doi=10.1145%2f3378025&partnerID=40&md5=e0d216c88a2a8528a24b4b018b63bb09,"We consider the problem of maintaining a maximal independent set in a dynamic graph subject to edge insertions and deletions. Recently, Assadi et al. (at STOC'18) showed that a maximal independent set can be maintained in sublinear (in the dynamically changing number of edges) amortized update time. In this article, we significantly improve the update time for uniformly sparse graphs. Specifically, for graphs with arboricity α, the amortized update time of our algorithm is O(α2 ⋅ log2 n), where n is the number of vertices. For low arboricity graphs, which include, for example, minor-free graphs and some classes of ""real-world"" graphs, our update time is polylogarithmic. Our update time improves the result of Assadi et al. for all graphs with arboricity bounded by m3/8-ϵ, for any constant ϵ > 0. This covers much of the range of possible values for arboricity, as the arboricity of a general graph cannot exceed m1/2. © 2020 ACM.",dynamic graph algorithms; edge orientations; graph arboricity; Maximal independent set,Graph algorithms; Graphic methods; Dynamic graph; Free graphs; General graph; Insertions and deletions; Maximal independent set; Polylogarithmic; Real-world; Sparse graphs; Graph theory
A Linear-Time Algorithm for Seeds Computation,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084752156&doi=10.1145%2f3386369&partnerID=40&md5=44f732f25d4fd6b8567e33e3d54473e5,"A seed in a word is a relaxed version of a period in which the occurrences of the repeating subword may overlap. Our first contribution is a linear-time algorithm computing a linear-size representation of all seeds of a word (the number of seeds might be quadratic). In particular, one can easily derive the shortest seed and the number of seeds from our representation. Thus, we solve an open problem stated in a survey by Smyth from 2000 and improve upon a previous O(n log n)-time algorithm by Iliopoulos et al. from 1996. Our approach is based on combinatorial relations between seeds and subword complexity (used here for the first time in the context of seeds). In previous papers, compact representations of seeds consisted of two independent parts operating on the suffix tree of the input word and the suffix tree of its reverse, respectively. Our second contribution is a novel and significantly simpler representation of all seeds that avoids dealing with the suffix tree of the reversed word. This result is also of independent interest from a combinatorial point of view. A preliminary version of this work, with a much more complex algorithm constructing a representation of seeds on two suffix trees, was presented at the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA'12). © 2020 ACM.",quasiperiodicity; Seed of a word; subword complexity; suffix tree,Clustering algorithms; Forestry; Compact representation; Complex algorithms; Discrete algorithms; Linear-time algorithms; Sub words; Subword complexity; Suffix-trees; Time algorithms; Trees (mathematics)
Subexponential Algorithms for Rectilinear Steiner Tree and Arborescence Problems,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084754589&doi=10.1145%2f3381420&partnerID=40&md5=6d83a44e83ccda5c02684fa07d1165bb,"A rectilinear Steiner tree for a set K of points in the plane is a tree that connects k using horizontal and vertical lines. In the Rectilinear Steiner Tree problem, the input is a set K={z1,z2,..., zn} of n points in the Euclidean plane (R2), and the goal is to find a rectilinear Steiner tree for k of smallest possible total length. A rectilinear Steiner arborescence for a set k of points and a root r ∈ K is a rectilinear Steiner tree T for K such that the path in T from r to any point z ∈ K is a shortest path. In the Rectilinear Steiner Arborescence problem, the input is a set K of n points in R2, and a root r ∈ K, and the task is to find a rectilinear Steiner arborescence for K, rooted at r of smallest possible total length. In this article, we design deterministic algorithms for these problems that run in 2O(nlog n) time. © 2020 ACM.",rectilinear Steiner arborescence; Rectilinear Steiner tree; subexponential exact algorithm; treewidth algorithm,Geometry; Deterministic algorithms; Euclidean planes; Rectilinear Steiner arborescences; Rectilinear Steiner tree problems; Rectilinear steiner trees; Shortest path; Sub-exponential algorithms; Vertical lines; Trees (mathematics)
Holant Clones and the Approximability of Conservative Holant Problems,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084747536&doi=10.1145%2f3381425&partnerID=40&md5=5703721a0921dd82fe56dc8a3209b40b,"We construct a theory of holant clones to capture the notion of expressibility in the holant framework. Their role is analogous to the role played by functional clones in the study of weighted counting Constraint Satisfaction Problems. We explore the landscape of conservative holant clones and determine the situations in which a set F of functions is ""universal in the conservative case,"" which means that all functions are contained in the holant clone generated by F together with all unary functions. When F is not universal in the conservative case, we give concise generating sets for the clone. We demonstrate the usefulness of the holant clone theory by using it to give a complete complexity-theory classification for the problem of approximating the solution to conservative holant problems. We show that approximation is intractable exactly when F is universal in the conservative case. © 2020 ACM.",approximate counting; Computational counting problem; functional clone; Holant problem,Constraint satisfaction problems; Approximability; Clone theory; Complexity theory; Expressibility; Generating set; Holant problems; Unary functions; Cloning
Ramsey Spanning Trees and Their Applications,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084754046&doi=10.1145%2f3371039&partnerID=40&md5=a8612da836318c0f2ab8af96122e9df2,"The metric Ramsey problem asks for the largest subset S of a metric space that can be embedded into an ultrametric (more generally into a Hilbert space) with a given distortion. Study of this problem was motivated as a non-linear version of Dvoretzky theorem. Mendel and Naor [29] devised the so-called Ramsey Partitions to address this problem, and showed the algorithmic applications of their techniques to approximate distance oracles and ranking problems. In this article, we study the natural extension of the metric Ramsey problem to graphs, and introduce the notion of Ramsey Spanning Trees. We ask for the largest subset S ⊆ V of a given graph G=(V, E), such that there exists a spanning tree of G that has small stretch for S. Applied iteratively, this provides a small collection of spanning trees, such that each vertex has a tree providing low stretch paths to all other vertices. The union of these trees serves as a special type of spanner, a tree-padding spanner. We use this spanner to devise the first compact stateless routing scheme with O(1) routing decision time, and labels that are much shorter than in all currently existing schemes. We first revisit the metric Ramsey problem and provide a new deterministic construction. We prove that for every k, any n-point metric space has a subset S of size at least n1-1/k that embeds into an ultrametric with distortion 8k. We use this result to obtain the state-of-the-art deterministic construction of a distance oracle. Building on this result, we prove that for every k, any n-vertex graph G=(V, E) has a subset S of size at least n1-1/k, and a spanning tree of G, that has stretch O(k log log n) between any point in S and any point in V. © 2020 ACM.",compact routing; distance oracles; Distortion; metric embedding; spanning trees,Iterative methods; Set theory; Topology; Algorithmic applications; Metric Ramsey problem; Natural extension; Nonlinear versions; Ranking problems; Routing decisions; State of the art; Stateless routing; Trees (mathematics)
K-center Clustering under Perturbation Resilience,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084756016&doi=10.1145%2f3381424&partnerID=40&md5=d50a8242b45b5520c72fb1112c162c79,"The k-center problem is a canonical and long-studied facility location and clustering problem with many applications in both its symmetric and asymmetric forms. Both versions of the problem have tight approximation factors on worst case instances: a 2-approximation for symmetric k-center and an O(log∗(k))-approximation for the asymmetric version. Therefore, to improve on these ratios, one must go beyond the worst case. In this work, we take this approach and provide strong positive results both for the asymmetric and symmetric k-center problems under a natural input stability (promise) condition called α-perturbation resilience [15], which states that the optimal solution does not change under any α-factor perturbation to the input distances. We provide algorithms that give strong guarantees simultaneously for stable and non-stable instances: Our algorithms always inherit the worst-case guarantees of clustering approximation algorithms and output the optimal solution if the input is 2-perturbation resilient. In particular, we show that if the input is only perturbation resilient on part of the data, our algorithm will return the optimal clusters from the region of the data that is perturbation resilient while achieving the best worst-case approximation guarantee on the remainder of the data. Furthermore, we prove that our result is tight by showing symmetric k-center under (2 - ϵ)-perturbation resilience is hard unless NP = RP. The impact of our results is multifaceted. First, to our knowledge, asymmetric k-center is the first problem that is hard to approximate to any constant factor in the worst case, yet can be optimally solved in polynomial time under perturbation resilience for a constant value of α. This is also the first tight result for any problem under perturbation resilience, i.e., this is the first time the exact value of α for which the problem switches from being NP-hard to efficiently computable has been found. Furthermore, our results illustrate a surprising relationship between symmetric and asymmetric k-center instances under perturbation resilience. Unlike approximation ratio, for which symmetric k-center is easily solved to a factor of 2 but asymmetric k-center cannot be approximated to any constant factor, both symmetric and asymmetric k-center can be solved optimally under resilience to 2-perturbations. Finally, our guarantees in the setting where only part of the data satisfies perturbation resilience make these algorithms more applicable to real-life instances. © 2020 Owner/Author.",Beyond worst-case analysis; clustering; perturbation resilience,Approximation algorithms; Optimal systems; Optimization; Polynomial approximation; Time switches; Approximation factor; Approximation ratios; Clustering problems; Constant factors; Facility locations; K-center problem; Optimal solutions; Worst-case instances; Clustering algorithms
Linear-time String Indexing and Analysis in Small Space,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084757908&doi=10.1145%2f3381417&partnerID=40&md5=1c137f123a6d38dfc96a566ec535d9a6,"The field of succinct data structures has flourished over the past 16 years. Starting from the compressed suffix array by Grossi and Vitter (STOC 2000) and the FM-index by Ferragina and Manzini (FOCS 2000), a number of generalizations and applications of string indexes based on the Burrows-Wheeler transform (BWT) have been developed, all taking an amount of space that is close to the input size in bits. In many large-scale applications, the construction of the index and its usage need to be considered as one unit of computation. For example, one can compare two genomes by building a common index for their concatenation and by detecting common substructures by querying the index. Efficient string indexing and analysis in small space lies also at the core of a number of primitives in the data-intensive field of high-throughput DNA sequencing. We report the following advances in string indexing and analysis: We show that the BWT of a string T ∈ {1,...,σ}n can be built in deterministic O(n) time using just O(n log σ) bits of space, where σ ≤ n. Deterministic linear time is achieved by exploiting a new partial rank data structure that supports queries in constant time and that might have independent interest. Within the same time and space budget, we can build an index based on the BWT that allows one to enumerate all the internal nodes of the suffix tree of T. Many fundamental string analysis problems, such as maximal repeats, maximal unique matches, and string kernels, can be mapped to such enumeration and can thus be solved in deterministic O(n) time and in O(n log σ) bits of space from the input string by tailoring the enumeration algorithm to some problem-specific computations. We also show how to build many of the existing indexes based on the BWT, such as the compressed suffix array, the compressed suffix tree, and the bidirectional BWT index, in randomized O(n) time and in O(n log σ) bits of space. The previously fastest construction algorithms for BWT, compressed suffix array and compressed suffix tree, which used O(n log σ) bits of space, took O(n log log σ) time for the first two structures and O(n log ϵn) time for the third, where ϵ is any positive constant smaller than one. Alternatively, the BWT could be previously built in linear time if one was willing to spend O(n log σ log log σ n) bits of space. Contrary to the state-of-the-art, our bidirectional BWT index supports every operation in constant time per element in its output. © 2020 ACM.",bidirectional BWT index; Burrows-Wheeler transform; Compact data structures; compressed indexes; compressed suffix array; compressed suffix tree; matching statistics; maximal exact match; maximal repeat; maximal unique match; minimal absent word; monotone minimal perfect hash function; partial rank query; string kernel; suffix array; suffix tree; suffix-link tree,Budget control; Data structures; DNA sequences; Forestry; Gene encoding; Indexing (of information); Trees (mathematics); Burrows-Wheeler Transform; Compressed suffix array; Compressed suffix trees; Construction algorithms; Enumeration algorithms; Large-scale applications; Positive constant; Succinct data structure; Backward wave tubes
A time- And message-optimal distributed algorithm for minimum spanning trees,2020,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075640860&doi=10.1145%2f3365005&partnerID=40&md5=04c538a6ed7c9d1e8b9126b7a29b1aa8,"This article presents a randomized (Las Vegas) distributed algorithm that constructs a minimum spanning tree (MST) in weighted networks with optimal (up to polylogarithmic factors) time and message complexity. This algorithm runs in O (D + √ n) time and exchanges O (m) messages (both with high probability), where n is the number of nodes of the network, D is the hop-diameter, and m is the number of edges. This is the first distributed MST algorithm that matches simultaneously the time lower bound of Ω (D + √ n) [10] and the message lower bound of Ω(m) [31], which both apply to randomized Monte Carlo algorithms. The prior time and message lower bounds are derived using two completely different graph constructions; the existing lower-bound construction that shows one lower bound does not work for the other. To complement our algorithm, we present a new lower-bound graph construction for which any distributed MST algorithm requires both Ω (D + √ n) rounds and Ω(m) messages. © 2019 Association for Computing Machinery. All rights reserved.",Distributed algorithms; Minimum spanning trees,Computational complexity; Graph algorithms; Parallel algorithms; Graph construction; High probability; Lower bounds; Message complexity; Minimum spanning trees; Monte carlo algorithms; Poly-logarithmic factors; Weighted networks; Trees (mathematics)
Randomized memoryless algorithms for theweighted and the generalized k-server problems,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076806043&doi=10.1145%2f3365002&partnerID=40&md5=182c92f23e4b814572e9d0e8a28d6e14,"The weighted k-server problem is a generalization of the k-server problem wherein the cost of moving a server of weight βi through a distance d is βi • d. On uniform metric spaces, this models caching with caches having different page replacement costs. A memoryless algorithm is an online algorithm whose behavior is independent of the history given the positions of its k servers. In this article, we develop a framework to analyze the competitiveness of randomized memoryless algorithms. The key technical contribution is a method for working with potential functions defined implicitly as the solution of a linear system. Using this, we establish tight bounds on the competitive ratio achievable by randomized memoryless algorithms for the weighted k-server problem on uniform metrics. We first prove that there is an αk-competitive memoryless algorithm for this problem, where αk = α2 k.1 + 3αk.1 + 1; α1 = 1.We complement this result by proving that no randomized memoryless algorithm can have a competitive ratio less than αk. Finally,we prove that the above bounds also hold for the generalized k-server problem onweighted uniform metrics. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Competitive analysis; Online algorithms; Weighted k-server problem,Algorithms; Mathematical techniques; Competitive analysis; Competitive ratio; K-server problem; On-line algorithms; Page replacement; Potential function; Technical contribution; Uniform metric; Linear systems
Faster Replacement Paths and Distance Sensitivity Oracles,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077014692&doi=10.1145%2f3365835&partnerID=40&md5=3e10bef0578cd0d5ad45a01cb865afbf,"Shortest paths computation is one of the most fundamental problems in computer science. An important variant of the problem is when edges can fail, and one needs to compute shortest paths that avoid a (failing) edge. More formally, given a source node s, a target node t, and an edge e, a replacement path for the triple (s, t, e) is a shortest s-t path avoiding edge e. Replacement paths computation can be seen either as a static problem or as a data structure problem. In the static setting, a typical goal is to compute for fixed s and t, for every possible failed edge e, the length of the best replacement path around e (replacement paths problem). In the data structure setting, a typical goal is to design a data structure (distance sensitivity oracle) that, after some preprocessing, quickly answers queries of the form: What is the length of the replacement path for the triple (s, t, e)? In this article, we focus on n-node directed graphs with integer edge weights in [.M,M], and present improved replacement paths algorithms and distance sensitivity oracles based on fast matrix multiplication. In more detail, we obtain the following main results: . We describe a replacement paths algorithm with runtime Õ (Mnω), where ω < 2.373 is the fast matrix multiplication exponent. For a comparison, the previous fastest algorithms have runtime Õ (Mn1+2ω/3) [Weimann,Yuster.FOCS'10] and, in the unweighted case, Õ (n2.5) [Roditty, Zwick. ICALP'05]. Our result shows that, at least for small integer weights, the replacement paths problem in directed graphs may be easier than the related all-pairs shortest paths problem, as the current best runtime for the latter is Õ (M 1/4-ω n2+ 1/4-ω): this is Ω(n2.5) even if ω = 2. Our algorithm also implies that the k shortest simple s-t paths can be computed in Õ (kMnω) time. • We consider the single-source generalization of the replacement paths problem, where only the source s is fixed. We show how to solve this problem in all-pairs shortest paths time, currently Õ (M1/4-ω n2+ 1/4-ω). Our runtime reduces to Õ (Mnω) for positive weights, hence matching our mentioned result for the simpler replacement paths case (that, however, holds also for nonpositive weights). One of the ingredients that we use is an algorithm to compute the distances from a set S of source nodes to a set T of target nodes in Õ (Mnω + |S| · |T | · (Mn)1/4-ω) time. This improves on a result in Yuster,Zwick.FOCS'05. • We present the first distance sensitivity oracle that achieves simultaneously subcubic preprocessing time and sublinear query time. More precisely, for a given parameter α ∞ [0, 1], our oracle has preprocessing time Õ (Mnω+1/2 + Mnω+α (4-ω)) and query time Õ (n1-α). The previous best oracle for small integer weights has Õ (Mnω+1-α) preprocessing time and (superlinear) Õ (n1+α) query time [Weimann,Yuster-FOCS'10]. From a technical point of view, an interesting and novel aspect of our oracle is that it exploits as a subroutine our single-source replacement paths algorithm. We also present an oracle with the same preprocessing time as in Weimann,Yuster.FOCS'10 and with smaller query time Õ (n1-1-α/4-ω + n2α ). Copyright © 2019 held by the owner/author(s).",Distance sensitivity oracles; Replacement paths; Shortest paths,Data structures; Directed graphs; All pairs shortest paths; Distance sensitivity oracles; Fast matrix multiplication; Preprocessing time; Replacement paths; Shortest path; Shortest paths computation; Sub-linear queries; Matrix algebra
Introduction to the special issue on SODA'18,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076967270&doi=10.1145%2f3368307&partnerID=40&md5=27d94f39f04a72e15458873b88cd3d81,[No abstract available],,
Tight analysis of parallel randomized greedy MIS,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076802116&doi=10.1145%2f3326165&partnerID=40&md5=9bd52571b470d9b812909e2d6c6d2295,"We provide a tight analysis that settles the round complexity of the well-studied parallel randomized greedy MIS algorithm, thus answering the main open question of Blelloch, Fineman, and Shun [SPAA'12]. The parallel/distributed randomized greedy Maximal Independent Set (MIS) algorithm works as follows. An order of the vertices is chosen uniformly at random. Then, in each round, all vertices that appear before their neighbors in the order are added to the independent set and removed from the graph along with their neighbors. The main question of interest is the number of rounds it takes until the graph is empty. This algorithm has been studied since 1987, initiated by Coppersmith, Raghavan, and Tompa [FOCS'87], and the previously best known boundswereO(logn) rounds in expectation for Erdõs-Rényi random graphs by Calkin and Frieze [Random Struc. Alg.'90] andO(log2 n) rounds with high probability for general graphs by Blelloch, Fineman, and Shun [SPAA'12]. We prove a high probability upper bound of O(logn) on the round complexity of this algorithm in general graphs and that this bound is tight. This also shows that parallel randomized greedy MIS is as fast as the celebrated algorithm of Luby [STOC'85, JALG'86]. © 2019 Association for Computing Machinery.",Greedy algorithm; Maximal independent set; parallel algorithms; PRAM,Graph theory; Parallel algorithms; General graph; Greedy algorithms; High probability; Independent set; Maximal independent set; PRAM; Random graphs; Round complexity; Computational complexity
Optimal streaming and tracking distinct elements with high probability,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076800387&doi=10.1145%2f3309193&partnerID=40&md5=fd79d5f4c1e1fe3f0f6e8e380ea5e0a6,"The distinct elements problem is one of the fundamental problems in streaming algorithms-given a stream of integers in the range {1,..,n},wewish to provide a (1 + ϵ) approximation to the number of distinct elements in the input. After a long line of research an optimal solution for this problem with constant probability of success, using O(1 ϵ2 + lgn) bits of space, was given by Kane, Nelson, and Woodruff in 2010. The standard approach used to achieve low failure probability δ is to take the median of lg δ.1 parallel repetitions of the original algorithm. We show that such a multiplicative space blow-up is unnecessary: We provide an optimal algorithm using O(lg δ.1 ϵ2 + lgn) bits of space-matching known lower bounds for this problem. That is, the lg δ.1 factor does not multiply the lgn term. This settles completely the space complexity of the distinct elements problem with respect to all standard parameters. We consider also the strong tracking (or continuous monitoring) variant of the distinct elements problem, where we want an algorithm that provides an approximation of the number of distinct elements seen so far, at all times of the stream. We show that this variant can be solved using O(lg lg n+lg δ.1 ϵ2 + lgn) bits of space, which we show to be optimal. ©2019. Optimal Streaming and Tracking Distinct Elements with High Probability. ACMTrans.",Continuous monitoring; Distinct elements; Streaming algorithms,Monitoring; Probability; Continuous monitoring; Distinct elements; Failure Probability; Optimal solutions; Original algorithms; Parallel repetition; Probability of success; Streaming algorithm; Approximation algorithms
Approximation schemes for low-rank binary matrix approximation problems,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075631023&doi=10.1145%2f3365653&partnerID=40&md5=96e17971cbaa9c9a17565196a3bf67ab,"We provide a randomized linear time approximation scheme for a generic problem about clustering of binary vectors subject to additional constraints. The new constrained clustering problem generalizes a number of problems and by solving it, we obtain the first linear time-approximation schemes for a number of well-studied fundamental problems concerning clustering of binary vectors and low-rank approximation of binary matrices. Among the problems solvable by our approach are Low GF(2)-Rank Approximation, Low Boolean-Rank Approximation, and various versions of Binary Clustering. For example, for Low GF(2)- Rank Approximation problem, where for anm × n binary matrix A and integer r > 0, we seek for a binary matrix B of GF(2) rank at most r such that the ∂0-norm of matrix A - B is minimum, our algorithm, for any ∈ > 0 in time f (r , ∈ ) nm, where f is some computable function, outputs a (1 + ∈ )-approximate solution with probability at least (1 - 1 e ). This is the first linear time approximation scheme for these problems. We also give (deterministic) PTASes for these problems running in time nf (r ) 1 ∈2 log 1 ∈ , where f is some function depending on the problem. Our algorithm for the constrained clustering problem is based on a novel sampling lemma, which is interesting on its own. © 2019 Association for Computing Machinery. All rights reserved.",Approximation scheme; Binary matrix factorization; Clustering; Random sampling,Approximation algorithms; Approximation theory; Clustering algorithms; Factorization; Approximation problems; Approximation scheme; Binary matrix; Clustering; Constrained clustering; Linear-time approximation; Low rank approximations; Random sampling; Matrix algebra
A faster algorithm for minimum-cost bipartite perfect matching in planar graphs,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075639505&doi=10.1145%2f3365006&partnerID=40&md5=7a7e37a4095f118478b382c26289fc9e,"Given a weighted planar bipartite graph G(A ∪ B, E) where each edge has an integer edge cost, we give an O (n4/3 lognC) time algorithm to compute minimum-cost perfect matching; hereC is the maximum edge cost in the graph. The previous best-known planarity exploiting algorithm has a running time of O(n3/2 logn) and is achieved by using planar separators (Lipton and Tarjan '80). Our algorithm is based on the bit-scaling paradigm (Gabow and Tarjan '89). For each scale, our algorithm first executes O(n1/3) iterations of Gabow and Tarjan's algorithm in O(n4/3) time leaving only O(n2/3) vertices unmatched. Next, it constructs a compressed residual graphH withO(n2/3) vertices andO(n) edges. This is achieved by using an r -division of the planar graph G with r = n2/3. For each partition of the r -division, there is an edge between two vertices of H if and only if they are connected by a directed path inside the partition. Using existing efficient shortest-path data structures, the remaining O(n2/3) vertices are matched by iteratively computing a minimum-cost augmenting path, each taking O (n2/3) time. Augmentation changes the residual graph, so the algorithm updates the compressed representation for each partition affected by the change in O (n2/3) time. We bound the total number of affected partitions over all the augmenting paths by O(n2/3 logn). Therefore, the total time taken by the algorithm is O (n4/3). © 2019 Association for Computing Machinery. All rights reserved.",Minimum-cost matching; Primal-dual; Scaling algorithms,Graphic methods; Iterative methods; Augmenting path; Bipartite graphs; Minimum cost matching; Perfect matchings; Planar separators; Primal-dual; Scaling algorithm; Time algorithms; Graph theory
Algorithms to approximate column-sparse packing problems,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075640726&doi=10.1145%2f3355400&partnerID=40&md5=a3e22f1b49c6005eec4d152b12d11473,"Column-sparse packing problems arise in several contexts in both deterministic and stochastic discrete optimization. We present two unifying ideas, (non-uniform) attenuation and multiple-chance algorithms, to obtain improved approximation algorithms for some well-known families of such problems. As three main examples, we attain the integrality gap, up to lower-order terms, for known LP relaxations for k-column-sparse packing integer programs (Bansal et al., Theory of Computing, 2012) and stochastic k-set packing (Bansal et al., Algorithmica, 2012), and go ""half the remaining distance"" to optimal for a major integrality-gap conjecture of Furedi, Kahn, and Seymour on hypergraph matching (Combinatorica, 1993). © 2019 Association for Computing Machinery. All rights reserved.",Approximation algorithms; Packing programs; Randomized algorithms,Computation theory; Integer programming; Stochastic systems; Discrete optimization; Integrality gaps; Lower order terms; LP relaxation; Non-uniform; Packing integer programs; Packing problems; Randomized Algorithms; Approximation algorithms
"More logarithmic-factor speedups for 3sum, (median,+)-convolution, and some geometric 3sum-hard problems",2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075633809&doi=10.1145%2f3363541&partnerID=40&md5=01df0808dfc591f9391b95e667e4071c,"This article presents an algorithm that solves the 3SUM problem for n real numbers in O((n2/ log2 n) (log logn)O(1) ) time, improving previous solutions by about a logarithmic factor. Our framework for shaving off two logarithmic factors can be applied to other problems, such as (median,+)-convolution/matrix multiplication and algebraic generalizations of 3SUM. This work also obtains the first subquadratic results on some 3SUM-hard problems in computational geometry, for example, deciding whether (the interiors of) a constant number of simple polygons have a common intersection. © 2019 Association for Computing Machinery. All rights reserved.",3SUM; Computational geometry; Convolution; Matrix multiplication,Computational geometry; 3SUM; 3sum-hard problems; MAtrix multiplication; Real number; Simple polygon; Convolution
Proximity results and faster algorithms for integer programming using the steinitz lemma,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075639199&doi=10.1145%2f3340322&partnerID=40&md5=8b6296af68556063c69dfd683fd7606c,"We consider integer programming problems in standard form max{cT x : Ax = b, x =0, x ∈ Zn } where A ∈ Zm×n, b ∈ Zm, andc ∈ Zn.We show that such an integer program can be solved in time (m Δ)O(m) ∥b∥2∞ , where Δ is an upper bound on each absolute value of an entry in A. This improves upon the longstanding best bound of Papadimitriou [27] of (m Δ)O(m2 ) , where in addition, the absolute values of the entries of b also need to be bounded by Δ. Our result relies on a lemma of Steinitz that states that a set of vectors in Rm that is contained in the unit ball of a norm and that sum up to zero can be ordered such that all partial sums are of norm bounded bym. We also use the Steinitz lemma to show that the ∂1-distance of an optimal integer and fractional solution, also under the presence of upper bounds on the variables, is bounded by m (2m Δ + 1)m. Here Δ is again an upper bound on the absolute values of the entries of A. The novel strength of our bound is that it is independent of n. We provide evidence for the significance of our bound by applying it to general knapsack problems where we obtain structural and algorithmic results that improve upon the recent literature. © 2019 Association for Computing Machinery. All rights reserved.",Dynamic program; Integer programming,Combinatorial optimization; Absolute values; Dynamic programs; Fractional solutions; Integer program; Integer programming problems; Knapsack problems; Partial sums; Upper Bound; Integer programming
A new algorithm for fast generalized DFTs,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075638704&doi=10.1145%2f3301313&partnerID=40&md5=9076648eaf2a50f4dc3c0c1be9ec4f7d,"We give an new arithmetic algorithm to compute the generalized Discrete Fourier Transform (DFT) over finite groups G. The new algorithm uses O(|G|ω/2+o(1) ) operations to compute the generalized DFT over finite groups of Lie type, including the linear, orthogonal, and symplectic families and their variants, as well as all finite simple groups of Lie type. Here ω is the exponent of matrix multiplication, so the exponent ω/2 is optimal if ω = 2. Previously, ""exponent one"" algorithms were known for supersolvable groups and the symmetric and alternating groups. No exponent one algorithms were known, even under the assumption ω = 2, for families of linear groups of fixed dimension, and indeed the previous best-known algorithm for SL2 (Fq ) had exponent 4/3 despite being the focus of significant effort. We unconditionally achieve exponent at most 1.19 for this group and exponent one if ω = 2. Our algorithm also yields an improved exponent for computing the generalized DFT over general finite groups G, which beats the longstanding previous best upper bound for any ω. In particular, assuming ω = 2, we achieve exponent √ 2, while the previous best was 3/2. © 2019 Association for Computing Machinery. All rights reserved.",Discrete Fourier transform; Finite groups of Lie type,Discrete Fourier transforms; Arithmetic algorithms; Best-known algorithms; Finite groups; Finite groups of Lie type; Finite simple group; Generalized discrete Fourier transform; Linear group; MAtrix multiplication; Lie groups
Distributed edge coloring and a special case of the constructive lovász local lemma,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075638436&doi=10.1145%2f3365004&partnerID=40&md5=d15acbb7592c3e36f1471674f221d266,"The complexity of distributed edge coloring depends heavily on the palette size as a function of the maximum degree Δ. In this article, we explore the complexity of edge coloring in the LOCAL model in different palette size regimes. Our results are as follows. Lower Bounds: First, we simplify the round elimination technique of Brandt et al. [16] and prove that (2Δ - 2)-edge coloring requires Ω(logΔ logn) time with high probability and Ω(logΔ n) time deterministically, even on trees. Second, we show that a natural approach to computing (Δ + 1)-edge colorings (Vizing's theorem), namely, extending an arbitrary partial coloring by iteratively recoloring subgraphs, requires Ω(Δlogn) time. Upper Bounds on General Graphs: We give a randomized edge coloring algorithm that can use palette sizes as small as Δ + O ( √ Δ), which is a natural barrier for randomized approaches. The running time of our (1 + ∈)Δ-edge coloring algorithm is usually dominated by O(log ∈ -1) calls to a distributed Lovasz local lemma (LLL) algorithm. For example, using the Chung-Pettie-Su LLL algorithm, we compute a (1 + ∈)Δ-edge coloring in O(logn) time when ∈ ≥ (log3 Δ)/ √ Δ, or O(logΔ n) + (log logn)3+o(1) time when ∈ = Ω(1). When Δ is sublogarithmic in n the performance is improved with the Ghaffari-Harris-Kuhn LLL algorithm. Upper Bounds on Trees: We show that the Ω(logΔ logn) lower bound can be nearly matched on trees. To establish this result, we develop a new distributed Lovasz local lemma algorithm for tree-structured dependency graphs, which arise naturally from O(1)-round probabilistic algorithms run on trees. Specifically, our (1 + ∈)Δ-edge coloring algorithm for trees takes O(log(1/∈ )) max{ log log n log log log n , loglog Δ logn} time when∈ ≥ (log3 Δ)/ √ Δ, orO(max{ log log n log log log n , logΔ logn}) time when ∈ = Ω(1). © 2019 Association for Computing Machinery. All rights reserved.",Distributed algorithms; Edge coloring; LOCAL model; Lovasz local lemma,Coloring; Forestry; Iterative methods; Parallel algorithms; Redundant manipulators; Distributed edge coloring; Edge coloring; Edge-coloring algorithms; Elimination techniques; Local model; Lovasz local lemma; Probabilistic algorithm; Randomized approach; Trees (mathematics)
Scheduling when you do not know the number of machines,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075633395&doi=10.1145%2f3340320&partnerID=40&md5=d46be199f8a44398988db23756448809,"Often in a scheduling problem, there is uncertainty about the jobs to be processed. The issue of uncertainty regarding the machines has been much less studied. In this article, we study a scheduling environment in which jobs first need to be grouped into some sets before the number of machines is known, and then the sets need to be scheduled on machines without being separated. To evaluate algorithms in such an environment, we introduce the idea of an α-robust algorithm, one that is guaranteed to return a schedule on any number m of machines that is within an α factor of the optimal schedule on m machine, where the optimum is not subject to the restriction that the sets cannot be separated. Under such environment, we give a ( 5 3 + ∈ )- robust algorithm for scheduling on parallel machines to minimize makespan and show a lower bound 43 . For the special case when the jobs are infinitesimal, we give a 1.233-robust algorithm with an asymptotic lower bound of 1.207. We also study a case of fair allocation, where the objective is to minimize the difference between the maximum and minimum machine load. © 2019 Association for Computing Machinery. All rights reserved.",Robust algorithms,Scheduling; Fair allocation; Lower bounds; Machine loads; On-machines; Optimal schedule; Parallel machine; Robust algorithm; Scheduling problem; Scheduling algorithms
Solving the sigma-tau problem,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075638845&doi=10.1145%2f3359589&partnerID=40&md5=5592c5b1981058b46db47f24e0518aa5,"Knuth assigned the following open problem a difficulty rating of 48/50 in The Art of Computer Programming Volume 4A: For odd n ≥ 3, can the permutations of {1, 2, . ,n} be ordered in a cyclic list so that each permutation is transformed into the next by applying either the operation σ, a rotation to the left, or τ , a transposition of the first two symbols? The Sigma-Tau problem is equivalent to finding a Hamilton cycle in the directed Cayley graph generated by σ = (1 2 n) and τ = (1 2). In this article, we solve the Sigma-Tau problem by providing a simple O(n)- time successor rule to generate successive permutations of a Hamilton cycle in the aforementioned Cayley graph. © 2019 Association for Computing Machinery. All rights reserved.",Cayley graph; Hamilton cycle; Permutations; Sigma-tau problem; Successor rule,Arts computing; Computer programming; Graph Databases; Cayley graphs; Hamilton cycle; Permutations; Sigma-tau problem; Successor rule; Directed graphs
Spanning circuits in regular matroids,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073219444&doi=10.1145%2f3355629&partnerID=40&md5=bbd145c0a8e7b24cf237441ce579dd12,"We consider the fundamental Matroid Theory problem of finding a circuit in a matroid containing a set T of given terminal elements. For graphic matroids, this corresponds to the problem of finding a simple cycle passing through a set of given terminal edges in a graph. The algorithmic study of the problem on regular matroids, a superclass of graphic matroids, was initiated by Gavenčiak, Král', and Oum [ICALP'12], who proved that the case of the problem with |T| = 2 is fixed-parameter tractable (FPT) when parameterized by the length of the circuit. We extend the result of Gavenčiak, Král', and Oum by showing that for regular matroids • the Minimum Spanning Circuit problem, deciding whether there is a circuit with at most ℓ elements containing T, is FPT parameterized by k = ℓ − |T|; • the Spanning Circuit problem, deciding whether there is a circuit containingT, is FPT parameterized by |T|. We note that extending our algorithmic findings to binary matroids, a superclass of regular matroids, is highly unlikely: Minimum Spanning Circuit parameterized by ℓ is W[1]-hard on binary matroids even when |T| = 1. We also show a limit to how far our results can be strengthened by considering a smaller parameter. More precisely, we prove that Minimum Spanning Circuit parameterized by |T| is W[1]-hard even on cographic matroids, a proper subclass of regular matroids. © 2019 Association for Computing Machinery.",Parameterized complexity; Regular matroids; Spanning circuit,Electric network analysis; Matrix algebra; Parameterization; Timing circuits; Binary matroids; Cographic matroids; Matroid theory; Parameterized; Parameterized complexity; Combinatorial mathematics
Heavy hitters and the structure of local privacy,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073259741&doi=10.1145%2f3344722&partnerID=40&md5=b8e31f4dd6bb81a3c0c5bf80746e162f,"We present a new locally differentially private algorithm for the heavy hitters problem that achieves optimal worst-case error as a function of all standardly considered parameters. Prior work obtained error rates that depend optimally on the number of users, the size of the domain, and the privacy parameter but depend sub-optimally on the failure probability. We strengthen existing lower bounds on the error to incorporate the failure probability and show that our new upper bound is tight with respect to this parameter as well. Our lower bound is based on a new understanding of the structure of locally private protocols. We further develop these ideas to obtain the following general results beyond heavy hitters. • Advanced Grouposition: In the local model, group privacy for k users degrades proportionally to ≈√k instead of linearly in k as in the central model. Stronger group privacy yields improved max-information guarantees, as well as stronger lower bounds (via “packing arguments”), over the central model. • Building on a transformation of Bassily and Smith (STOC 2015), we give a generic transformation from any non-interactive approximate-private local protocol into a pure-private local protocol. Again in contrast with the central model, this shows that we cannot obtain more accurate algorithms by moving from pure to approximate local privacy. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Differential privacy; Heavy hitters; Local model,Algorithms; Mathematical techniques; Differential privacies; Failure Probability; Generic transformations; Heavy-hitter; Local model; Lower bounds; Private protocols; Worst case error; Errors
Improving TSP tours using dynamic programming over tree decompositions,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073252959&doi=10.1145%2f3341730&partnerID=40&md5=d10e873f88ccb6acf93ba42a27751a3f,"Given a traveling salesman problem (TSP) tour H in graph G, a k-move is an operation that removes k edges from H and adds k edges of G so that a new tour H' is formed. The popular k-OPT heuristic for TSP finds a local optimum by starting from an arbitrary tour H and then improving it by a sequence of k-moves. Until 2016, the only known algorithm to find an improving k-move for a given tour was the naive solution in timeO(nk). At ICALP'16, de Berg, Buchin, Jansen, and Woeginger showed anO(n⌊2k/3⌋+1)-time algorithm. We show an algorithm that runs in O(n(1/4+ϵk)k) time, where limk→∞ ϵk = 0. It improves over the state of the art for every k ≥ 5. For the most practically relevant case k = 5, we provide a slightly refined algorithm running in O(n3.4) time. We also show that for the k = 4 case, improving over the O(n3)-time algorithm of de Berg et al. would be a major breakthrough: An O(n3−ϵ)-time algorithm for any ϵ > 0 would imply an O(n3−δ)-time algorithm for the All Pairs Shortest Paths problem, for some δ > 0. © 2019 Association for Computing Machinery.",K-OPT; Local search; Traveling salesman problem; Treewidth,Graph theory; Traveling salesman problem; All pairs shortest paths; Dynamic programming over tree decompositions; Local optima; Local search; Running-in; State of the art; Time algorithms; Tree-width; Dynamic programming
Exponential separations in the energy complexity of leader election,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073218642&doi=10.1145%2f3341111&partnerID=40&md5=685f9644a38e4478f7dd8e9eff6bb962,"Energy is often the most constrained resource for battery-powered wireless devices, and most of the energy is often spent on transceiver usage (i.e., transmitting and receiving packets) rather than computation. In this article, we study the energy complexity of fundamental problems in several models of wireless radio networks. It turns out that energy complexity is very sensitive to whether the devices can generate random bits and their ability to detect collisions. We consider four collision detection models: Strong-CD (in which transmitters and listeners detect collisions), Sender-CD (in which only transmitters detect collisions), Receiver-CD (in which only listeners detect collisions), and No-CD (in which no one detects collisions). The take-away message of our results is quite surprising. For randomized algorithms, there is an exponential gap between the energy complexity of Sender-CD and Receiver-CD: Randomized: No-CD = Sender-CD ≫ Receiver-CD = Strong-CD and for deterministic algorithms, there is another exponential gap in energy complexity, but in the reverse direction: Deterministic: No-CD = Receiver-CD ≫ Sender-CD = Strong-CD Precisely, the randomized energy complexity of Leader Election is Θ(log∗ n) in Sender-CD but Θ(log(log∗ n)) in Receiver-CD, where n is the number of devices, which is unknown to the devices at the beginning; the deterministic complexity of Leader Election is Θ(log N) in Receiver-CD but Θ(log log N) in Sender-CD, where N is the size of the ID space. There is a tradeoff between time and energy. We provide a new upper bound on the time-energy tradeoff curve for randomized algorithms. A critical component of this algorithm is a new deterministic Leader Election algorithm for dense instances, when n = Θ(N), with inverse Ackermann energy complexity. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Energy complexity; Leader election; Wireless network,Radio transceivers; Wireless networks; Collision detection; Constrained resources; Deterministic algorithms; Energy complexity; Leader election; Leader election algorithm; Randomized Algorithms; Wireless radio networks; Complex networks
A 4/3-approximation algorithm for the minimum 2-edge connected subgraph problem,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073252161&doi=10.1145%2f3341599&partnerID=40&md5=e55073f3d669bb70fc4ea2b0c51367cb,"We present a factor 43 approximation algorithm for the problem of finding a minimum 2-edge connected spanning subgraph of a given undirected multigraph. The algorithm is based upon a reduction to a restricted class of graphs. In these graphs, the approximation algorithm constructs a 2-edge connected spanning subgraph by modifying the smallest 2-edge cover. © 2019 Association for Computing Machinery.",Approximation Algorithms; Graph Connectivity,Algorithms; Mathematical techniques; Connected Subgraph; Graph connectivity; Multigraphs; Subgraphs; Approximation algorithms
Recognizing weak embeddings of graphs,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073199542&doi=10.1145%2f3344549&partnerID=40&md5=a015044ba989cde5d5d2d3fabd51de90,"We present an efficient algorithm for a problem in the interface between clustering and graph embeddings. An embedding φ : G → M of a graph G into a 2-manifold M maps the vertices in V (G) to distinct points and the edges in E(G) to interior-disjoint Jordan arcs between the corresponding vertices. In applications in clustering, cartography, and visualization, nearby vertices and edges are often bundled to the same point or overlapping arcs due to data compression or low resolution. This raises the computational problem of deciding whether a given map φ : G → M comes from an embedding. A map φ : G → M is a weak embedding if it can be perturbed into an embedding ψε : G → M with ∣∣ φ − ψε ∣∣ < ε for every ε > 0, where ∣∣.∣∣ is the unform norm. A polynomial-time algorithm for recognizing weak embeddings has recently been found by Fulek and Kynčl. It reduces the problem to solving a system of linear equations over Z2. It runs in O(n2ω ) ≤ O(n4.75) time, where ω ∈ [2, 2.373) is the matrix multiplication exponent and n is the number of vertices and edges of G. We improve the running time to O(n log n). Our algorithm is also conceptually simpler: We perform a sequence of local operations that gradually “untangles” the image φ(G) into an embedding ψ (G) or reports that φ is not a weak embedding. It combines local constraints on the orientation of subgraphs directly, thereby eliminating the need for solving large systems of linear equations. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph embedding; Graph operations,Clustering algorithms; Data visualization; Embeddings; Linear equations; Maps; Polynomial approximation; Computational problem; Corresponding vertices; Graph embeddings; Graph operations; Local constraints; MAtrix multiplication; Polynomial-time algorithms; System of linear equations; Graph theory
"Ordered level planarity and its relationship to geodesic planarity, bi-monotonicity, and variations of level planarity",2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073226831&doi=10.1145%2f3359587&partnerID=40&md5=576bf8c5c9989f23db8910afa45285dc,"We introduce and study the problem Ordered Level Planarity, which asks for a planar drawing of a graph such that vertices are placed at prescribed positions in the plane and such that every edge is realized as a y-monotone curve. This can be interpreted as a variant of Level Planarity in which the vertices on each level appear in a prescribed total order. We establish a complexity dichotomy with respect to both the maximum degree and the level-width, that is, the maximum number of vertices that share a level. Our study of Ordered Level Planarity is motivated by connections to several other graph drawing problems. Geodesic Planarity asks for a planar drawing of a graph such that vertices are placed at prescribed positions in the plane and such that every edge e is realized as a polygonal path p composed of line segments with two adjacent directions from a given set S of directions that is symmetric with respect to the origin. Our results on Ordered Level Planarity imply N P-hardness for any S with |S| ≥ 4, even if the given graph is a matching. Manhattan Geodesic Planarity is the special case where S contains precisely the horizontal and vertical directions. Katz, Krug, Rutter, and Wolff claimed that Manhattan Geodesic Planarity can be solved in polynomial time for the special case of matchings [GD'09]. Our results imply that this is incorrect unless P = N P. Our reduction extends to settle the complexity of the Bi-Monotonicity problem, which was proposed by Fulek, Pelsmajer, Schaefer, and Štefankovič. Ordered Level Planarity turns out to be a special case of T-Level Planarity, Clustered Level Planarity, and Constrained Level Planarity. Thus, our results strengthen previous hardness results. In particular, our reduction to Clustered Level Planarity generates instances with only two non-trivial clusters. This answers a question posed by Angelini, Da Lozzo, Di Battista, Frati, and Roselli. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph drawing; Level Planarity; NP-hardness; Orthogeodesic drawings; Point-set em-bedability; Upward drawings,Drawing (graphics); Geodesy; Hardness; Polynomial approximation; Reduction; Graph drawing; NP-hardness; Planarity; Point set; Upward drawing; Graph theory
Faster approximation schemes for the two-dimensional knapsack problem,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073147618&doi=10.1145%2f3338512&partnerID=40&md5=539b7f658e585282ee2b7c13c59b07e5,"For geometric optimization problems we often understand the computational complexity on a rough scale, but not very well on a finer scale. One example is the two-dimensional knapsack problem for squares. There is a polynomial time (1 + ϵ)-approximation algorithm for it (i.e., a PTAS) but the running time of this algorithm is triple exponential in 1/ϵ, i.e., Ω(n221/ϵ ). A double or triple exponential dependence on 1/ϵ is inherent in how this and other algorithms for other geometric problems work. In this article, we present an efficient PTAS (EPTAS) for knapsack for squares, i.e., a (1 + ϵ)-approximation algorithm with a running time of Oϵ (1) · nO(1). In particular, the exponent of n in the running time does not depend on ϵ at all! Since there can be no fully polynomial time approximation scheme (FPTAS) for the problem (unless P = NP), this is the best kind of approximation scheme we can hope for. To achieve this improvement, we introduce two new key ideas: We present a fast method to guess the Ω(221/ϵ ) relatively large squares of a suitable near-optimal packing instead of using brute-force enumeration. Secondly, we introduce an indirect guessing framework to define sizes of cells for the remaining squares. In the previous PTAS, each of these steps needs a running time of Ω(n221/ϵ and we improve both to Oϵ (1) · nO(1). ) We complete our result by giving an algorithm for two-dimensional knapsack for rectangles under (1 + ϵ)resource augmentation. We improve the previous double-exponential PTAS to an EPTAS and compute even a solution with optimal weight, while the previous PTAS computes only an approximation. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximation algorithm; EPTAS; Geometric knapsack problem,Combinatorial optimization; Geometry; Polynomial approximation; Approximation scheme; EPTAS; Exponential dependence; Fully polynomial time approximation schemes; Geometric optimization; Knapsack problems; Resource augmentation; Two-dimensional knapsack problem; Approximation algorithms
Faster carry bit computation for adder circuits with prescribed arrival times,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070066919&doi=10.1145%2f3340321&partnerID=40&md5=0b5187ac1a809461f9267b67d08e1aed,"We consider the fundamental problem of constructing fast circuits for the carry bit computation in binary addition. Up to a small additive constant, the carry bit computation reduces to computing an And-Or path, i.e., a formula of type t0 ∧ (t1 ∨ (t2 ∧ (... tm−1 )...) or t0 ∨ (t1 ∧ (t2 ∨ (... tm−1 )...). We present an algorithm that computes the fastest known Boolean circuit for an And-Or path with given arrival times a(t0 ),..., a(tm−1 ) for the input signals. Our objective function is delay, a natural generalization of depth with respect to arrival times. The maximum delay of the circuit we compute is log2 W + log2 log2 m + log2 log2 log2 m + 4.3, where W := mi=−01 2a(ti). Note that log2 W is a lower bound on the delay of any circuit depending on inputs t0,..., tm−1 with prescribed arrival times. Our method yields the fastest circuits for And-Or paths, carry bit computation, and adders in terms of delay known so far. © 2019 Copyright held by the owner/author(s).",Adders; And-or paths; Carry bits; Delay; Non-uniform arrival times,Adders; Timing circuits; Adder circuit; And-or paths; Arrival time; Binary additions; Boolean circuit; Delay; Natural generalization; Objective functions; Delay circuits
An optimal O(nm) algorithm for enumerating all walks common to all closed edge-covering walks of a graph,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070062714&doi=10.1145%2f3341731&partnerID=40&md5=777cba6d6f1b8d516a3724afceb6d533,[No abstract available],,
Dynamic beats fixed: On phase-based algorithms for file migration,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070107592&doi=10.1145%2f3340296&partnerID=40&md5=9e3361bbfdd3ec277dd3a0c5e7959518,"We construct a deterministic 4-competitive algorithm for the online file migration problem, beating the currently best 20-year-old, 4.086-competitive Move-To-Local-Min (Mtlm) algorithm by Bartal et al. (SODA 1997). Like Mtlm, our algorithm also operates in phases, but it adapts their lengths dynamically depending on the geometry of requests seen so far. The improvement was obtained by carefully analyzing a linear model (factor-revealing linear program) of a single phase of the algorithm. We also show that if an online algorithm operates in phases of fixed length and the adversary is able to modify the graph between phases, then the competitive ratio is at least 4.086. © 2019 Copyright held by the owner/author(s).",Competitive analysis; Factor-revealing linear programs; File migration; Online algorithms,Algorithms; Mathematical techniques; Competitive analysis; Competitive ratio; File migration; Linear modeling; Linear programs; On-line algorithms; Phase based; Single phase; Linear programming
A tractable class of binary vcsps via m-convex intersection,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069492431&doi=10.1145%2f3329862&partnerID=40&md5=1b68d741dd8b6b754d49c1941e7a5970,"A binary VCSP is a general framework for the minimization problem of a function represented as the sum of unary and binary cost functions. An important line of VCSP research is to investigate what functions can be solved in polynomial time. Cooper and Živn?classified the tractability of binary VCSP instances according to the concept of ""triangle,"" and showed that the only interesting tractable case is the one induced by the joint winner property (JWP). Recently, Iwamasa, Murota, and Živn?made a link between VCSP and discrete convex analysis, showing that a function satisfying the JWPcan be transformed into a function represented as the sum of two quadratic M-convex functions, which can be minimized in polynomial time via an M-convex intersection algorithm if the value oracle of each M-convex function is given. In this article,we give an algorithmic answer to a natural question:What binary finite-valued CSP instances can be represented as the sum of two quadratic M-convex functions and can be solved in polynomial time via an M-convex intersection algorithm? We solve this problem by devising a polynomial-Time algorithm for obtaining a concrete form of the representation in the representable case. Our result presents a larger tractable class of binary finite-valued CSPs, which properly contains the JWP class. © 2019 Association for Computing Machinery.",discrete convex analysis; Mconvexity; Valued constraint satisfaction problems,Cost functions; Polynomial approximation; Discrete convex analysis; Intersection algorithms; M-convex functions; Mconvexity; Minimization problems; Polynomial-time algorithms; Tractable class; Valued constraint satisfaction problems; Constraint satisfaction problems
"Derandomized Concentration Bounds for Polynomials, and Hypergraph Maximal Independent Set",2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069541719&doi=10.1145%2f3326171&partnerID=40&md5=a154de6d171f422f9feecdcee9b63007,"A parallel algorithm for maximal independent set (MIS) in hypergraphs has been a long-standing algorithmic challenge, dating back nearly 30 years to a survey of Karp and Ramachandran (1990). The best randomized parallel algorithm for hypergraphs of fixed rank r was developed by Beame and Luby (1990) and Kelsen (1992), running in time roughly (logn)r !. We improve the randomized algorithm of Kelsen, reducing the runtime to roughly (logn)2r and simplifying the analysis through the use of more-modern concentration inequalities. We also give a method for derandomizing concentration bounds for low-degree polynomials, which are the key technical tool used to analyze that algorithm. This leads to a deterministic PRAM algorithm also running in (logn)2r+3 time and poly(m,n) processors. This is the first deterministic algorithm with sub-polynomial runtime for hypergraphs of rank r 3. Our analysis can also apply when r is slowly growing; using this in conjunction with a strategy of Bercea et al. (2015) gives a deterministic MIS algorithm running in time exp(O( log(mn) log log(mn) )). © 2019 Association for Computing Machinery.",Concentration bounds; hypergraph; maximal independent set; MIS,Graph theory; Management information systems; Parallel algorithms; Concentration bounds; Concentration inequality; Deterministic algorithms; Hypergraph; Maximal independent set; PRAM algorithms; Randomized Algorithms; Technical tools; Polynomials
Generalized Center Problems with Outliers,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069474922&doi=10.1145%2f3338513&partnerID=40&md5=d0f46b3939b926d4b07d72166c3f56d3,"We study theF-center problem with outliers: Given a metric space (X,d), a general down-closed familyF of subsets of X, and a parameterm, we need to locate a subset S F of centers such that the maximum distance among the closestm points in X to S is minimized. Our main result is a dichotomy theorem. Colloquially, we prove that there is an efficient 3-Approximation for the F-center problem with outliers if and only if we can efficiently optimize a poly-bounded linear function over F subject to a partition constraint. One concrete upshot of our result is a polynomial time 3-Approximation for the knapsack center problem with outliers for which no (true) approximation algorithm was known. © 2019 Association for Computing Machinery.",Approximation algorithms; clustering; k-center problem,Clustering algorithms; Color centers; Polynomial approximation; Statistics; Center problems; clustering; Dichotomy theorem; K-center problem; Linear functions; Maximum distance; Metric spaces; Polynomial-time; Approximation algorithms
Sparse Approximation via Generating Point Sets,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069446623&doi=10.1145%2f3302249&partnerID=40&md5=b66835b4e7a79c11ddd205cedd8cbcdb,"For a set P of n points in the unit ball b Rd , consider the problem of finding a small subset T P such that its convex-hull ϵ-Approximates the convex-hull of the original set. Specifically, the Hausdorff distance between the convex hull of T and the convex hull of P should be at most ϵ.We present an efficient algorithm to compute such an ϵ-Approximation of size kalg, where ϵ - is a function of ϵ and kalg is a function of the minimum size kopt of such an ϵ-Approximation. Surprisingly, there is no dependence on the dimension d in either of the bounds. Furthermore, every point of P can be ϵ-Approximated by a convex-combination of points of T that is O(1/ϵ2)-sparse. Our result can be viewed as a method for sparse, convex autoencoding: Approximately representing the data in a compact way using sparse combinations of a small subset T of the original data. The new algorithm can be kernelized, and it preserves sparsity in the original input. © 2019 Association for Computing Machinery.",Convex hull; coreset; sparse approximation,Computational geometry; Convex combinations; Convex hull; coreset; Hausdorff distance; Point set; Sparse approximations; Unit ball; Approximation algorithms
Distributed Dominating Set Approximations beyond Planar Graphs,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069436654&doi=10.1145%2f3326170&partnerID=40&md5=6cd5bf1a589338df6aaff2d12d170098,"The Minimum Dominating Set (MDS) problem is a fundamental and challenging problem in distributed computing. While it is well known thatminimum dominating sets cannot be well approximated locally on general graphs, in recent years there has been much progress on computing good local approximations on sparse graphs and in particular on planar graphs. In this article, we study distributed and deterministic MDS approximation algorithms for graph classes beyond planar graphs. In particular, we show that existing approximation bounds for planar graphs can be lifted to bounded genus graphs and more general graphs, which we call locally embeddable graphs, and present (1) a local constant-Time, constant-factor MDS approximation algorithm on locally embeddable graphs, and (2) a local O(log n)-Time (1)-Approximation scheme for any 0 on graphs of bounded genus. Our main technical contribution is a new analysis of a slightly modified variant of an existing algorithm by Lenzen et al. [21]. Interestingly, unlike existing proofs for planar graphs, our analysis does not rely on direct topological arguments but on combinatorial density arguments only. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximation algorithms; Distributed computing; dominating set,Approximation algorithms; Distributed computer systems; Graphic methods; Approximation bounds; Approximation scheme; Bounded-genus graphs; Constant factors; Dominating sets; Local approximation; Minimum dominating set; Technical contribution; Graph theory
Faster Pseudopolynomial Time Algorithms for Subset Sum,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069450842&doi=10.1145%2f3329863&partnerID=40&md5=5d06f78b0499255a0657f97a152fd7ed,"Given a (multi) set S of n positive integers and a target integer u, the subset sum problem is to decide if there is a subset of S that sums up to u. We present a series of new algorithms that compute and return all the realizable subset sums up to the integer u in O (min{ nu,u5/4, }), where is the sum of all elements of S and O hides polylogarithmic factors. We also present a modified algorithm for integers modulo m, which computes all the realizable subset sums modulom in O (min{ nm,m5/4}) time. Our contributions improve upon the standard dynamic programming algorithm that runs in O(nu) time. To the best of our knowledge, the new algorithms are the fastest deterministic algorithms for this problem. The new results can be employed in various algorithmic problems, from graph bipartition to computational social choice. Finally, we also improve a result on covering Zm, which might be of independent interest. © 2019 Association for Computing Machinery.",convolution; Subset sum,Convolution; Dynamic programming; Algorithmic problems; Computational social choices; Deterministic algorithms; Dynamic programming algorithm; Modified algorithms; Poly-logarithmic factors; Pseudo-polynomial time algorithms; Subset sum; Set theory
Deciding the Confusability ofWords under Tandem Repeats in Linear Time,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069525623&doi=10.1145%2f3338514&partnerID=40&md5=4717df1af5a57952632f964cfe3c488e,"Tandem duplication in DNA is the process of inserting a copy of a segment of DNA adjacent to the original position.Motivated by applications that store data in living organisms, Jain et al. (2016) proposed the study of codes that correct tandem duplications to improve the reliability of data storage. We investigate algorithms associated with the study of these codes. Two words are said to be k-confusable if there exists a sequence of tandem duplications for each word, where each duplication is of length at most k, such that the resulting two words after duplications are equal. For k = 3, we demonstrate that the problem of deciding whether two words is 3-confusable is linear-Time solvable through a characterisation that can be checked efficiently. Combining with previous results, the decision problem is linear-Time solvable for k 3. We conjecture that this problem is undecidable for k 3. Using insights gained from the algorithm, we study the size of tandem-duplication codes. We improve the previous known upper bound and then construct codes with larger sizes as compared to the previous constructions. We determine the sizes of optimal tandem-duplication codes for lengths up to 20, develop recursive methods to construct tandem-duplication codes for all word lengths, and compute explicit lower bounds for the size of optimal tandem-duplication codes for lengths from 21 to 30. © 2019 Association for Computing Machinery.",DNA-based data storage; Tandem duplications,Biology; Digital storage; DNA; Data storage; Decision problems; Living organisms; Lower bounds; Recursive methods; Tandem duplications; Tandem repeats; Upper Bound; Codes (symbols)
Online Vertex-Weighted Bipartite Matching: Beating 1-1 e with Random Arrivals,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069471846&doi=10.1145%2f3326169&partnerID=40&md5=9325b00b38ab5d489cc679597da84bf0,"We introduce a weighted version of the ranking algorithm by Karp et al. (STOC 1990), and we prove a competitive ratio of 0.6534 for the vertex-weighted online bipartite matching problem when online vertices arrive in random order. Our result shows that random arrivals help beating the 1-1/e barrier even in the vertexweighted case.We build on the randomized primal-dual framework by Devanur et al. (SODA 2013) and design a two dimensional gain sharing function, which depends not only on the rank of the offline vertex, but also on the arrival time of the online vertex. To our knowledge, this is the first competitive ratio strictly larger than 1-1/e for an online bipartite matching problem achieved under the randomized primal-dual framework. Our algorithm has a natural interpretation that offline vertices offer a larger portion of their weights to the online vertices as time increases, and each online vertex matches the neighbor with the highest offer at its arrival. © 2019 Association for Computing Machinery.",online bipartite matching; randomized primal-dual; Vertex weighted,Algorithms; Mathematical techniques; Arrival time; Bipartite matching problems; Bipartite matchings; Competitive ratio; Primal-dual; Ranking algorithm; Vertex weighted; Weighted bipartite matchings; Wages
A Lottery Model for Center-Type ProblemsWith Outliers,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069510889&doi=10.1145%2f3311953&partnerID=40&md5=4ec2aef657e2b46bec04c04cc6a46760,"In this article, we give tight approximation algorithms for the k-center and matroid center problems with outliers. Unfairness arises naturally in this setting: certain clients could always be considered as outliers. To address this issue, we introduce a lottery model in which each client j is allowed to submit a parameter pj ∊ [0, 1] and we look for a random solution that covers every client j with probability at least pj. Our techniques include a randomized rounding procedure to round a point inside a matroid intersection polytope to a basis plus at most one extra item such that all marginal probabilities are preserved and such that a certain linear function of the variables does not decrease in the process with probability one. © 2019 Association for Computing Machinery.",Approximation algorithms; randomized rounding,Combinatorial mathematics; Probability; Statistics; Center problems; Linear functions; Lottery models; Marginal probability; Matroid intersection; Polytopes; Random solutions; Randomized rounding; Approximation algorithms
Online submodular maximization with preemption,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067228134&doi=10.1145%2f3309764&partnerID=40&md5=89a8c74c577e4f04abd0b80f7b79baba,"Submodular function maximization has been studied extensively in recent years under various constraints and models. The problem plays a major role in various disciplines. We study a natural online variant of this problem in which elements arrive one by one and the algorithm has to maintain a solution obeying certain constraints at all times. Upon arrival of an element, the algorithm has to decide whether to accept the element into its solution and may preempt previously chosen elements. The goal is tomaximize a submodular function over the set of elements in the solution. We study two special cases of this general problem and derive upper and lower bounds on the competitive ratio. Specifically, we design a 1/e-competitive algorithm for the unconstrained case in which the algorithm may hold any subset of the elements, and constant competitive ratio algorithms for the case where the algorithm may hold at most k elements in its solution. © 2019 Association for Computing Machinery.",Competitive analysis; Online algorithms; Preemption; Submodular maximization,Algorithms; Competitive analysis; Competitive ratio; K elements; On-line algorithms; Preemption; Submodular; Submodular functions; Upper and lower bounds; Mathematical techniques
Bloom filters in adversarial environments,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067263132&doi=10.1145%2f3306193&partnerID=40&md5=fb2d73a53e72f78202a4cd15209fd559,"Many efficient data structures use randomness, allowing them to improve upon deterministic ones. Usually, their efficiency and correctness are analyzed using probabilistic tools under the assumption that the inputs and queries are independent of the internal randomness of the data structure. In this work, we consider data structures in a more robust model, which we call the adversarial model. Roughly speaking, this model allows an adversary to choose inputs and queries adaptively according to previous responses. Specifically, we consider a data structure known as a ""Bloom filter"" and prove a tight connection between Bloom filters in this model and cryptography. A Bloom filter represents a set S of elements approximately by using fewer bits than a precise representation. The price for succinctness is allowing for some errors: For any x ∈ S, it should always answer Yes, and for any x S it should answer Yes only with small probability. In the adversarial model, we consider both efficient adversaries (that run in polynomial time) and computationally unbounded adversaries that are only bounded in the number of queries they can make. For computationally bounded adversaries, we show that non-trivial (memory-wise) Bloom filters exist if and only if one-way functions exist. For unbounded adversaries, we show that there exists a Bloom filter for sets of size n and error ϵ that is secure against t queries and uses only O(n log 1 ϵ + t ) bits of memory. In comparison, n log 1 ϵ is the best possible under a non-adaptive adversary. © 2019 Association for Computing Machinery.",Adaptive inputs; Bloom filter; Pseudorandom functions; Streaming algorithm,Polynomial approximation; Random processes; Adaptive adversary; Adaptive inputs; Adversarial environments; Bloom filters; Efficient data structures; One-way functions; Pseudo-random functions; Streaming algorithm; Data structures
Introduction to the special issue on SODA 2017,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065761251&doi=10.1145%2f3319426&partnerID=40&md5=1654865ecaa7fba2b76fb4b95eee6a9d,[No abstract available],,
Fully polynomial FPT algorithms for some classes of bounded clique-width graphs,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067232092&doi=10.1145%2f3310228&partnerID=40&md5=e7c12b60695f67c2a763d8b44df17856,"Recently, hardness results for problems in P were achieved using reasonable complexity-theoretic assumptions such as the Strong Exponential TimeHypothesis.According to these assumptions,many graph-theoretic problems do not admit truly subquadratic algorithms. A central technique used to tackle the difficulty of the above-mentioned problems is fixed-parameter algorithms with polynomial dependency in the fixed parameter (P-FPT). Applying this technique to clique-width, an important graph parameter, remained to be done. In this article, we study several graph-theoretic problems for which hardness results exist such as cycle problems, distance problems, and maximum matching. We give hardness results and P-FPT algorithms, using clique-width and some of its upper bounds as parameters. We believe that our most important result is an algorithm in O(k4 n +m)-time for computing a maximum matching, where k is either the modular-width of the graph or the P4-sparseness. The latter generalizes many algorithms that have been introduced so far for specific subclasses such as cographs. Our algorithms are based on preprocessing methods using modular decomposition and split decomposition. Thus they can also be generalized to some graph classes with unbounded clique-width. © 2019 Association for Computing Machinery.",Clique-width; Fully polynomial FPT; Hardness in P; Modular decomposition; Neighbourhood diversity; Primeval decomposition; Split decomposition,Hardness; Parameter estimation; Polynomials; Clique-width; Fully polynomial FPT; Modular decomposition; Neighbourhood; Split decomposition; Graph theory
Locating errors in faulty formulas,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067234263&doi=10.1145%2f3313776&partnerID=40&md5=de5e0f1b1e91494c4b291230fe493973,"Given a drawing of a read-once formula (called the blueprint), and a blackbox implementation with the same topology as the blueprint that purports to compute the formula, can we tell if it does? Under a fault model, where the only faults in the implementation are gates that complement their outputs, we show that there is an efficient algorithm that makes a linear number of probes to the blackbox implementation and determines if the blueprint and implementation are identical. We also show a matching lower bound. We further ask whether we can diagnose where the faults are, using blackbox testing. We prove that if the implementation has a property called polynomial balance, then it is possible to do this efficiently. To complement this result, we show that even if the blueprint is polynomially balanced and there are only logarithmically many errors in the implementation, the implementation could be unbalanced and the diagnosis problem provably requires super-polynomially many tests.We point out that this problem is one instance of a general class of problems of learning deviations from a blueprint, which we call conformance learning. Conformance learning seems worthy of further investigation in a broader context. © 2019 Association for Computing Machinery.",Fault diagnosis; Fault-tolerant computing; Learning,Algorithms; Failure analysis; Fault tolerant computer systems; Mathematical techniques; Black boxes; Diagnosis problem; Fault model; General class; Learning; Locating error; Lower bounds; Read-once formulas; Black-box testing
Ascending-price algorithms for unknown markets,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067230590&doi=10.1145%2f3319394&partnerID=40&md5=19f88bbc4ab88616305daed80e7d3382,"We design a simple ascending-price algorithm to compute a (1 + ϵ )-approximate equilibrium in Arrow- Debreu markets with weak gross substitute property. It applies to an unknown market setting without exact knowledge about the number of agents, their individual utilities, and endowments. Instead, our algorithm only uses price queries to a global demand oracle. This is the first polynomial-time algorithm for most of the known tractable classes of Arrow-Debreu markets, which computes such an equilibrium with a number of calls to the demand oracle that is polynomial in log 1/ϵ and avoids heavy machinery such as the ellipsoid method. Demands can be real-valued functions of prices, but the oracles only return demand values of bounded precision. Due to this more realistic assumption, precision and representation of prices and demands become a major technical challenge, and we develop new tools and insights that may be of independent interest. Furthermore, we give the first polynomial-time algorithm to compute an exact equilibrium for markets with spending constraint utilities. This resolves an open problem posed by Duan and Mehlhorn. © 2019 Association for Computing Machinery.",Equilibrium computation; Market equilibrium; Spending constraint utilities; Weak gross substitutes,Costs; Machinery; Polynomial approximation; Approximate equilibriums; Ellipsoid method; Market equilibria; Polynomial-time algorithms; Real-valued functions; Spending constraint utilities; Technical challenges; Weak gross substitutes; Commerce
Editorial,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065801409&doi=10.1145%2f3325824&partnerID=40&md5=3d73916204d645602b8c36a4435254fd,[No abstract available],,
Domination when the stars are out,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065775605&doi=10.1145%2f3301445&partnerID=40&md5=c97a5f4cf8dcbd981fd718d2ab484cd8,"We algorithmize the structural characterization for claw-free graphs by Chudnovsky and Seymour. Building on this result, we show that Dominating Set on claw-free graphs is (i) fixed-parameter tractable and (ii) even possesses a polynomial kernel. To complement these results, we establish that Dominating Set is unlikely to be fixed-parameter tractable on the slightly larger class of graphs that exclude K1,4 as an induced subgraph (K1,4-free graphs). We show that our algorithmization can also be used to show that the related Connected Dominating Set problem is fixed-parameter tractable on claw-free graphs. To complement that result, we show that Connected Dominating Set is unlikely to have a polynomial kernel on claw-free graphs and is unlikely to be fixed-parameter tractable on K1,4-free graphs. Combined, our results provide a dichotomy for Dominating Set and Connected Dominating Set on K1-free graphs and show that the problem is fixed-parameter tractable if and only if ≤ 3. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Claw-free graphs; Connected dominating set; Dominating set; Fixed-parameter tractable; Polynomial kernel,Graphic methods; Polynomials; Claw-free graphs; Connected Dominating Set; Dominating sets; Fixed-parameter tractable; Polynomial kernels; Graph theory
Metastability of the logit dynamics for asymptotically well-behaved potential games,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062336540&doi=10.1145%2f3301315&partnerID=40&md5=013eb8da89bd21ff9b48964a9f03a789,"Convergence rate and stability of a solution concept are classically measured in terms of “eventually” and “forever,” respectively. In the wake of recent computational criticisms to this approach, we study whether these timeframes can be updated to have states computed “quickly” and stable for “long enough”. Logit dynamics allows irrationality in players' behavior and may take time exponential in the number of players n to converge to a stable state (i.e., a certain distribution over pure strategy profiles). We prove that every potential game, for which the behavior of the logit dynamics is not chaotic as n increases, admits distributions stable for a super-polynomial number of steps in n no matter the players' irrationality and the starting profile of the dynamics. The convergence rate to these metastable distributions is polynomial in n when the players are not too rational. Our proofs build upon the new concept of partitioned Markov chains, which might be of independent interest, and a number of involved technical contributions. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximate equilibria; Bounded rationality; Logit dynamics; Potential games,Markov processes; Approximate equilibria; Bounded rationality; Convergence rates; Metastabilities; Potential games; Solution concepts; Super-polynomials; Technical contribution; Dynamics
"The (H, k)-server problem on bounded depth trees",2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062348462&doi=10.1145%2f3301314&partnerID=40&md5=67e9d5299e3f9d71e7a82306da2ee17e,"We study the k-server problem in the resource augmentation setting, i.e., when the performance of the online algorithm with k servers is compared to the offline optimal solution with h ≤ k servers. The problem is very poorly understood beyond uniform metrics. For this special case, the classic k-server algorithms are roughly (1 + 1/ϵ)-competitive when k = (1 + ϵ)h, for any ϵ > 0. Surprisingly, however, no o(h)-competitive algorithm is known even for HSTs of depth 2 and even when k/h is arbitrarily large. We obtain several new results for the problem. First, we show that the known k-server algorithms do not work even on very simple metrics. In particular, the Double Coverage algorithm has competitive ratio Ω(h) irrespective of the value of k, even for depth-2 HSTs. Similarly, the Work Function Algorithm, which is believed to be optimal for all metric spaces when k = h, has competitive ratio Ω(h) on depth-3 HSTs even if k = 2h. Our main result is a new algorithm that is O(1)-competitive for constant depth trees, whenever k = (1 + ϵ)h for any ϵ > 0. Finally, we give a general lower bound that any deterministic online algorithm has competitive ratio at least 2.4 even for depth-2 HSTs and when k/h is arbitrarily large. This gives a surprising qualitative separation between uniform metrics and depth-2 HSTs for the (h, k)-server problem. © 2019 Association for Computing Machinery.",Competitive analysis; K-server problem; Online algorithms; Resource augmentation,Forestry; Competitive algorithms; Competitive analysis; Deterministic online algorithms; K-server problem; On-line algorithms; Qualitative separation; Resource augmentation; Work function algorithms; Trees (mathematics)
Approximation schemes for clustering with outliers,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062337035&doi=10.1145%2f3301446&partnerID=40&md5=9c54cfd43275a86bcba8fd50244896e0,"                             Clustering problems are well studied in a variety of fields, such as data science, operations research, and computer science. Such problems include variants of center location problems, k-median and k-means to name a few. In some cases, not all data points need to be clustered; some may be discarded for various reasons. For instance, some points may arise from noise in a dataset or one might be willing to discard a certain fraction of the points to avoid incurring unnecessary overhead in the cost of a clustering solution. We study clustering problems with outliers. More specifically, we look at uncapacitated facility location (UFL), k-median, and k-means. In these problems, we are given a set X of data points in a metric space δ(., .), a set C of possible centers (each maybe with an opening cost), maybe an integer parameter k, plus an additional parameter z as the number of outliers. In uncapacitated facility location with outliers, we have to open some centers, discard up to z points of X, and assign every other point to the nearest open center, minimizing the total assignment cost plus center opening costs. In k-median and k-means, we have to open up to k centers, but there are no opening costs. In k-means, the cost of assigning j to i is δ                             2                             (j,i). We present several results. Our main focus is on cases where δ is a doubling metric (this includes fixed dimensional Euclidean metrics as a special case) or is the shortest path metrics of graphs from a minor-closed family of graphs. For uniform-cost UFL with outliers on such metrics, we show that a multiswap simple local search heuristic yields a PTAS. With a bit more work, we extend this to bicriteria approximations for the k-median and k-means problems in the same metrics where, for any constant ϵ > 0, we can find a solution using (1 + ϵ)k centers whose cost is at most a (1 + ϵ)-factor of the optimum and uses at most z outliers. Our algorithms are all based on natural multiswap local search heuristics. We also show that natural local search heuristics that do not violate the number of clusters and outliers for k-median (or k-means) will have unbounded gap even in Euclidean metrics. Furthermore, we show how our analysis can be extended to general metrics for k-means with outliers to obtain a (25 + ϵ, 1 + ϵ)-approximation: an algorithm that uses at most (1 + ϵ)k clusters and whose cost is at most 25 + ϵ of optimum and uses no more than z outliers.                          © 2019 Association for Computing Machinery.",K-means; Local search; Outliers,Approximation algorithms; Cost benefit analysis; Graph theory; Heuristic algorithms; Local search (optimization); Location; Operations research; Statistics; Approximation scheme; Bicriteria approximation; Center location problem; K-means; Local search; Local search heuristics; Outliers; Uncapacitated facility locations; K-means clustering
Sparse dynamic programming on DAGs with small width,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062363208&doi=10.1145%2f3301312&partnerID=40&md5=dd26d311c74f720091cb52e396dcd4c2,"The minimum path cover problem asks us to find a minimum-cardinality set of paths that cover all the nodes of a directed acyclic graph (DAG). We study the case when the size k of a minimum path cover is small, that is, when the DAG has a small width. This case is motivated by applications in pan-genomics, where the genomic variation of a population is expressed as a DAG. We observe that classical alignment algorithms exploiting sparse dynamic programming can be extended to the sequence-against-DAG case by mimicking the algorithm for sequences on each path of a minimum path cover and handling an evaluation order anomaly with reachability queries. Namely, we introduce a general framework for DAG-extensions of sparse dynamic programming. This framework produces algorithms that are slower than their counterparts on sequences only by a factor k. We illustrate this on two classical problems extended to DAGs: longest increasing subsequence and longest common subsequence. For the former, we obtain an algorithm with running time O(k|E| log |V |). This matches the optimal solution to the classical problem variant when the input sequence is modeled as a path. We obtain an analogous result for the longest common subsequence problem. We then apply this technique to the co-linear chaining problem, which is a generalization of the above two problems. The algorithm for this problem turns out to be more involved, needing further ingredients, such as an FM-index tailored for large alphabets and a two-dimensional range search tree modified to support range maximum queries. We also study a general sequence-to-DAG alignment formulation that allows affine gap costs in the sequence. The main ingredient of the proposed framework is a new algorithm for finding a minimum path cover of a DAG (V, E) in O(k|E| log |V |) time, improving all known time-bounds when k is small and the DAG is not too dense. In addition to boosting the sparse dynamic programming framework, an immediate consequence of this new minimum path cover algorithm is an improved space/time tradeoff for reachability queries in arbitrary directed graphs. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Co-linear chaining; Longest common subsequence; Pan-genomics; Pattern matching,Directed graphs; Graph theory; Pattern matching; Trees (mathematics); Alignment algorithms; Arbitrary directed graphs; Directed acyclic graph (DAG); Genomics; Longest common subsequence problem; Longest common subsequences; Longest increasing subsequences; Sparse dynamic programming; Dynamic programming
Approximation schemes for machine scheduling with resource (in-)dependent processing times,2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065769831&doi=10.1145%2f3302250&partnerID=40&md5=917f365397291e892f18103c5a79a5c1,"We consider two related scheduling problems: single resource-constrained scheduling on identical parallel machines and a generalization with resource-dependent processing times. In both problems, jobs require a certain amount of an additional resource and have to be scheduled on machines minimizing the makespan, while at every point in time a given resource capacity is not exceeded. In the first variant of the problem, the processing times and resource amounts are fixed, while in the second the former depends on the latter. Both problems contain bin packing with cardinality constraint as a special case, and, therefore, these problems are strongly NP-complete even for a constant number of machines larger than three, which can be proven by a reduction from 3-Partition. Furthermore, if the number of machines is part of the input, then we cannot hope for an approximation algorithm with absolute approximation ratio smaller than 3/2. We present asymptotic fully polynomial time approximation schemes (AFPTAS) for the problems: For any ε > 0, a schedule of length at most (1 + ε) times the optimum plus an additive term of O(pmax log(1/ε)/ε) is provided, and the running time is polynomially bounded in 1/ε and the input length. Up to now, only approximation algorithms with absolute approximation ratios were known. Furthermore, the AFPTAS for resource-constrained scheduling on identical parallel machines directly improves the additive term of the best AFPTAS for bin packing with cardinality constraint so far. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Scheduling with resource-dependent processing times; Single resource-constrained scheduling,Additives; Polynomial approximation; Scheduling; Approximation ratios; Approximation scheme; Cardinality constraints; Fully polynomial time approximation schemes; Identical parallel machines; Resource constrained scheduling; Resource-dependent processing time; Scheduling problem; Approximation algorithms
"On problems equivalent to (min,+)-convolution",2019,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061209306&doi=10.1145%2f3293465&partnerID=40&md5=6da67d910051284f18786bcefd179aee,"In recent years, significant progress has been made in explaining the apparent hardness of improving upon the naive solutions for many fundamental polynomially solvable problems. This progress has come in the form of conditional lower bounds-reductions from a problem assumed to be hard. The hard problems include 3SUM, All-Pairs Shortest Path, SAT, Orthogonal Vectors, and others. In the (min, +)-convolution problem, the goal is to compute a sequence (c[i])n-1 i=0 , wherec[k] = mini=0, k {a[i] + b[k -i]}, given sequences (a[i])n-1 i=0 and (b[i])n-1 i=0 . This can easily be done in O(n2) time, but no O(n2-ϵ ) algorithm is known for ϵ > 0. In this article, we undertake a systematic study of the (min, +)- convolution problem as a hardness assumption. First, we establish the equivalence of this problem to a group of other problems, including variants of the classic knapsack problem and problems related to subadditive sequences. The (min, +)-convolution problem has been used as a building block in algorithms for many problems, notably problems in stringology. It has also appeared as an ad hoc hardness assumption. Second, we investigate some of these connections and provide new reductions and other results. We also explain why replacing this assumption with the Strong Exponential Time Hypothesis might not be possible for some problems. © 2018 held by the owner/author(s). Publication rights licensed to ACM.","(min,+)- convolution; conditional lower bounds; Fine-grained complexity; knapsack; subquadratic equivalence",Combinatorial optimization; Convolution; Hardness; All pairs shortest paths; Fine grained; knapsack; Lower bounds; Orthogonal vectors; Polynomially solvable; Strong exponential time hypothesis; subquadratic equivalence; Problem solving
Subquadratic algorithms for the diameter and the sum of pairwise distances in planar graphs,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061210333&doi=10.1145%2f3218821&partnerID=40&md5=dd5dc98892dbe9157663db013bad8e6f,"In this article, we show how to compute for n-vertex planar graphs in O(n11/6 polylog(n)) expected time the diameter and the sum of the pairwise distances. The algorithms work for directed graphs with real weights and no negative cycles. In O(n15/8 polylog(n)) expected time, we can also compute the number of pairs of vertices at distances smaller than a given threshold. These are the first algorithms for these problems using time O(nc ) for some constant c < 2, even when restricted to undirected, unweighted planar graphs. © 2018 Copyright is held by the owner/author(s).",diameter; distance counting; distances in graphs; Planar graph; Voronoi diagram; Wiener index,Directed graphs; Graphic methods; diameter; distance counting; distances in graphs; Planar graph; Voronoi diagrams; Wiener index; Graph theory
Feedback vertex set inspired kernel for chordal vertex deletion,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059642979&doi=10.1145%2f3284356&partnerID=40&md5=e6b5a2513526c299bd3f2bb3149e05bd,"                             Given a graph G and a parameter k, the Chordal Vertex Deletion (CVD) problem asks whether there exists a subset U ⊆ V(G) of size at most k that hits all induced cycles of size at least 4. The existence of a polynomial kernel for CVD was a well-known open problem in the field of Parameterized Complexity. Recently, Jansen and Pilipczuk resolved this question affirmatively by designing a polynomial kernel for 1 CVD of size O(k                             161                              log                             58                              k) and asked whether one can design a kernel of size O(k                             10                             ) [Jansen an Pilipczuk, SODA 2017]. While we do not completely resolve this question, we design a significantly smaller kernel of size O(k                             12                              log                             10                              k), inspired by the O(k                             2                             )-size kernel for Feedback Vertex Set [Thomassé, TALG 2010]. Furthermore, we introduce the notion of the independence degree of a vertex, which is our main conceptual contribution.                          © 2018 Association for Computing Machinery.",Chordal graph; Chordal vertex deletion; Kernelization; Parameterized complexity,Algorithms; Mathematical techniques; Can design; Chordal graphs; Chordal vertex deletion; Feedback vertex set; Induced cycle; Kernelization; Parameterized complexity; Polynomial kernels; Graph theory
A (2 + ϵ )-approximation for maximum weight matching in the semi-streaming model,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061205271&doi=10.1145%2f3274668&partnerID=40&md5=438e30da8fd3a7b90d9223c5014f865c,"We present a simple deterministic single-pass (2 + ϵ )-approximation algorithm for the maximum weight matching problem in the semi-streaming model. This improves on the currently best known approximation ratio of (4 + ϵ ). Our algorithm usesO(n log2 n) bits of space for constant values of ϵ. It relies on a variation of the local-ratio theorem, which may be of use for other algorithms in the semi-streaming model as well. © 2018 Association for Computing Machinery.",approximation algorithms; local-ratio; matchings; Semi-streaming algorithms,Algorithms; Mathematical techniques; Approximation ratios; Constant values; Local ratio; Matchings; Maximum weight matching; Single pass; Streaming algorithm; Streaming model; Approximation algorithms
A Dual Descent Algorithm for Node-capacitated Multiflow Problems and Its Applications,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059652891&doi=10.1145%2f3291531&partnerID=40&md5=3d93b2413ee7e511a74e3dfbb4105ff6,"                             In this article, we develop an O((m log k)MSF(n, m, 1))-time algorithm to find a half-integral node-capacitated multiflow of the maximum total flow-value in a network with n nodes, m edges, and k terminals, where MSF(n, m, γ ) denotes the time complexity of solving the maximum submodular flow problem in a network with n nodes, m edges, and the complexity γ of computing the exchange capacity of the submodular function describing the problem. By using Fujishige-Zhang algorithm for submodular flow, we can find a maximum half-integral multiflow in O(mn                             3                              log k) time. This is the first combinatorial strongly polynomial time algorithm for this problem. Our algorithm is built on a developing theory of discrete convex functions on certain graph structures. Applications include “ellipsoid-free” combinatorial implementations of a 2-approximation algorithm for the minimum node-multiway cut problem by Garg, Vazirani, and Yannakakis.                          © 2018 Association for Computing Machinery.",Discrete convex analysis; Node-capacitated multiflow; Node-multiway cut; Submodular flow,Complex networks; Functions; Polynomial approximation; Discrete convex analysis; Discrete convex function; Multi flows; Multiway cut; MULTIWAY CUT problems; Strongly polynomial time algorithm; Submodular flow; Submodular functions; Combinatorial mathematics
Firefighting on trees beyond integrality gaps,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061174962&doi=10.1145%2f3173046&partnerID=40&md5=3a085ec7bb4f4b12c201414e5e4ffa36,"The Firefighter problem and a variant of it, known as Resource Minimization for Fire Containment (RMFC), are natural models for optimal inhibition of harmful spreading processes. Despite considerable progress on several fronts, the approximability of these problems is still badly understood. This is the case even when the underlying graph is a tree, which is one of the most-studied graph structures in this context and the focus of this article. In their simplest version, a fire spreads fromone fixed vertex step by step from burning to adjacent non-burning vertices, and at each time step B many non-burning vertices can be protected from catching fire. The Firefighter problem asks, for a given B, to maximize the number of vertices that will not catch fire, whereas RMFC (on a tree) asks to find the smallest B that allows for saving all leaves of the tree. Prior to this work, the best known approximation ratios were an O(1)-approximation for the Firefighter problem and an O(log n)-approximation for RMFC, both being LP-based and essentially matching the integrality gaps of two natural LP relaxations. We improve on both approximations by presenting a PTAS for the Firefighter problem and an O(1)- approximation for RMFC, both qualitatively matching the known hardness results. Our results are obtained through a combination of the known LPs with several new techniques, which allow for efficiently enumerating over super-constant size sets of constraints to strengthen the natural LPs. © 2018 Association for Computing Machinery.",Approximation algorithms; combinatorial optimization; propagation models; rounding techniques,Approximation algorithms; Combinatorial optimization; Fire extinguishers; Fires; Forestry; Approximability; Approximation ratios; Fire containment; Graph structures; Integrality gaps; Propagation models; rounding techniques; Underlying graphs; Trees (mathematics)
Beating approximation factor two forweighted tree augmentation with bounded costs,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061197044&doi=10.1145%2f3182395&partnerID=40&md5=f2b3e19bbd7ea966fc5d8a0d075e6409,"The Weighted Tree Augmentation Problem (WTAP) is a fundamental well-studied problem in the field of network design. Given an undirected tree G = (V, E), an additional set of edges L ⊆ V ×lV disjoint from E called links and a cost vector c ϵ RL ≥0, WTAP asks to find a minimum-cost set F ⊆ L with the property that (V, E ∪ F ) is 2-edge connected. The special case where c∂ = 1 for all ∂ ϵ L is called the Tree Augmentation Problem (TAP). For the class of bounded cost vectors, we present the first improved approximation algorithm for WTAP in more than three decades. Concretely, for any M ϵ R≥1 and ϵ > 0, we present an LP based (μ + ϵ )- approximation for WTAP restricted to cost vectors c in [1,M]L for μ ∼ 1.96417. More generally, our result is a (μ + ϵ )-approximation algorithm with running time nrO(1) , where r = cmax/cmin is the ratio between the largest and the smallest cost of any link. For the special case of TAP, we improve this factor to 53 + ϵ. Our results rely on several new ideas, including a new LP relaxation of WTAP and a two-phase rounding algorithm. In the first phase, the algorithm uses the fractional LP solution to guide a simple decomposition method that breaks the tree into well-structured trees and equips each with a part of the fraction LP solution. In the second phase, the fractional solution in each part of the decomposition is rounded to an integral solution with two rounding procedures and the best outcome is included in the solution. One rounding procedure exploits the constraints in the new LP, while the other one exploits a connection to the Edge Cover Problem. We show that both procedures cannot have a bad approximation guarantee simultaneously to obtain the claimed approximation factor. © 2018 Association for Computing Machinery.",approximation algorithm; linear programming; network design; rounding technique; Tree augmentation problem,Approximation algorithms; Forestry; Linear programming; Approximation factor; Decomposition methods; Fractional solutions; Integral solutions; Network design; Rounding procedures; rounding technique; Tree augmentation; Trees (mathematics)
Common tangents of two disjoint polygons in linear time and constant workspace,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059642257&doi=10.1145%2f3284355&partnerID=40&md5=0aa54dab79055830bcbadb3284628455,"We provide a remarkably simple algorithm to compute all (at most four) common tangents of two disjoint simple polygons. Given each polygon as a read-only array of its corners in cyclic order, the algorithm runs in linear time and constant workspace and is the first to achieve the two complexity bounds simultaneously. The set of common tangents provides basic information about the convex hulls of the polygons-whether they are nested, overlapping, or disjoint-and our algorithm thus also decides this relationship. © 2018 Copyright held by the owner/author(s).",Common tangent; Constant workspace; Simple polygon,Algorithms; Mathematical techniques; Common tangent; Complexity bounds; Constant workspace; Convex hull; Linear time; SIMPLE algorithm; Simple polygon; Geometry
Completeness for first-order properties on sparse structures with algorithmic applications,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061178115&doi=10.1145%2f3196275&partnerID=40&md5=758d04334553ba3365000ee3660e528c,"Properties definable in first-order logic are algorithmically interesting for both theoretical and pragmatic reasons. Many of the most studied algorithmic problems, such as Hitting Set and Orthogonal Vectors, are first-order, and the first-order properties naturally arise as relational database queries. A relatively straightforward algorithm for evaluating a propertywith k + 1 quantifiers takes timeO(mk ) and, assuming the Strong Exponential Time Hypothesis (SETH), some such properties require O(mk-ϵ ) time for any ϵ > 0. (Here, m represents the size of the input structure, i.e., the number of tuples in all relations.) We give algorithms for every first-order property that improves this upper bound to mk /2Θ( √ log n) , i.e., an improvement by a factor more than any poly-log, but less than the polynomial required to refute SETH. Moreover,we showthat further improvement is equivalent to improving algorithms for sparse instances of the well-studied Orthogonal Vectors problem. Surprisingly, both results are obtained by showing completeness of the Sparse Orthogonal Vectors problem for the class of first-order properties under fine-grained reductions. To obtain improved algorithms, we apply the fast Orthogonal Vectors algorithm of References [3, 16]. While fine-grained reductions (reductions that closely preserve the conjectured complexities of problems) have been used to relate the hardness of disparate specific problems both within P and beyond, this is the first such completeness result for a standard complexity class. © 2018 Copyright held by the owner/author(s).",Fine-grained complexity; first-order model checking; orthogonal vectors,Model checking; Query languages; Algorithmic applications; Algorithmic problems; Fine grained; First-order models; Orthogonal vectors; Relational Database; Specific problems; Strong exponential time hypothesis; Vectors
Even delta-matroids and the complexity of planar boolean CSPs,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061215080&doi=10.1145%2f3230649&partnerID=40&md5=60adcdcfb2144c21942dc7c148aa762f,"The main result of this article is a generalization of the classical blossom algorithm for finding perfect matchings. Our algorithm can efficiently solve Boolean CSPs where each variable appears in exactly two constraints (we call it edge CSP) and all constraints are even Δ-matroid relations (represented by lists of tuples). As a consequence of this, we settle the complexity classification of planar Boolean CSPs started by Dvorak and Kupec. Using a reduction to even Δ-matroids, we then extend the tractability result to larger classes of Δ-matroids that we call efficiently coverable. It properly includes classes that were known to be tractable before, namely, co-independent, compact, local, linear, and binary, with the following caveat:We represent Δ-matroids by lists of tuples, while the last two use a representation by matrices. Since an n ×n matrix can represent exponentially many tuples, our tractability result is not strictly stronger than the known algorithm for linear and binary Δ-matroids. 2018 Copyright held by the owner/author(s). © 2018 held by the owner/author(s). Publication rights licensed to ACM.",blossom algorithm; Constraint satisfaction problem; delta-matroid,Combinatorial mathematics; Matrix algebra; Coverable; N-matrix; Perfect matchings; Constraint satisfaction problems
Polynomial kernels and wideness properties of nowhere dense graph classes,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057169785&doi=10.1145%2f3274652&partnerID=40&md5=b52a392a7ef5ea3d44b9335a1b4491ec,"Nowhere dense classes of graphs [21, 22] are very general classes of uniformly sparse graphs with several seemingly unrelated characterisations. From an algorithmic perspective, a characterisation of these classes in terms of uniform quasi-wideness, a concept originating in finite model theory, has proved to be particularly useful. Uniform quasi-wideness is used in many fpt-algorithms on nowhere dense classes. However, the existing constructions showing the equivalence of nowhere denseness and uniform quasi-wideness imply a non-elementary blow up in the parameter dependence of the fpt-algorithms, making them infeasible in practice. As a first main result of this article, we use tools from logic, in particular from a sub-field of model theory known as stability theory, to establish polynomial bounds for the equivalence of nowhere denseness and uniform quasi-wideness. A powerful method in parameterized complexity theory is to compute a problem kernel in a pre-computation step, that is, to reduce the input instance in polynomial time to a sub-instance of size bounded in the parameter only (independently of the input graph size). Our new tools allow us to obtain for every fixed radius r ∈ N a polynomial kernel for the distance-r dominating set problem on nowhere dense classes of graphs. This result is particularly interesting, as it implies that for every class C of graphs that is closed under taking subgraphs, the distance-r dominating set problem admits a kernel on C for every value of r if, and only if, it already admits a polynomial kernel for every value of r (under the standard assumption of parameterized complexity theory that FPT W[2]). © 2018 Copyright held by the owner/author(s).",Graph structure theory; Nowhere denseness; Stability theory; Uniform quasi-wideness,C (programming language); Computational complexity; Graphic methods; Parameter estimation; Polynomial approximation; Dominating set problems; Graph structures; Nowhere denseness; Parameter dependence; Parameterized complexity; Stability theories; Standard assumptions; Uniform quasi-wideness; Graph theory
Stream sampling framework and application for frequency cap statistics,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054167049&doi=10.1145%2f3234338&partnerID=40&md5=70442f58ef1bb996da66c3cd0aa718ef,"Unaggregated data, in a streamed or distributed form, are prevalent and come from diverse sources such as interactions of users with web services and IP traffic. Data elements have keys (cookies, users, queries), and elements with different keys interleave. Analytics on such data typically utilizes statistics expressed as a sum over keys in a specified segment of a function f applied to the frequency (the total number of occurrences) of the key. In particular, Distinct is the number of active keys in the segment, Sum is the sum of their frequencies, and both are special cases of frequency cap statistics, which cap the frequency by a parameter T. Random samples can be very effective for quick and efficient estimation of statistics at query time. Ideally, to estimate statistics for a given function f , our sample would include a key with frequency w with probability roughly proportional to f (w). The challenge is that while such “gold-standard” samples can be easily computed after aggregating the data (computing the set of key-frequency pairs), this aggregation is costly: It requires structure of size that is proportional to the number of active keys, which can be very large. We present a sampling framework for unaggregated data that uses a single pass (for streams) or two passes (for distributed data) and structure size proportional to the desired sample size. Our design unifies classic solutions for Distinct and Sum. Specifically, our -capped samples provide nonnegative unbiased estimates of any monotone non-decreasing frequency statistics and statistical guarantees on quality that are close to gold standard for cap statistics with T = Θ(). Furthermore, our multi-objective samples provide these statistical guarantees on quality for all concave sub-linear statistics (the nonnegative span of cap functions) while incurring only a logarithmic overhead on sample size. © 2018 Association for Computing Machinery.",Distributed aggregation; Frequency statistics; Stream processing,Frequency estimation; Statistics; Web services; Distributed data; Efficient estimation; Multi objective; Statistical guarantee; Stream processing; Stream sampling; Structure sizes; Unbiased estimates; Sampling
Windrose planarity: Embedding graphs with direction-constrained edges,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053899700&doi=10.1145%2f3239561&partnerID=40&md5=500eeb8099d333d89ecaee595fd293b2,"Given a planar graph G and a partition of the neighbors of each vertex v in four sets v, v, v, and v, the problem Windrose Planarity asks to decide whether G admits a windrose-planar drawing, that is, a planar drawing in which (i) each neighbor u ∈ v is above and to the right of v, (ii) each neighbor u ∈ v is above and to the left of v, (iii) each neighbor u ∈ v is below and to the left of v, (iv) each neighbor u ∈ v is below and to the right of v, and (v) edges are represented by curves that are monotone with respect to each axis. By exploiting both the horizontal and the vertical relationship among vertices, windrose-planar drawings allow us to simultaneously visualize two partial orders defined by means of the edges of the graph. Although the problem is N P-hard in the general case, we give a polynomial-time algorithm for testing whether there exists a windrose-planar drawing that respects a given combinatorial embedding. This algorithm is based on a characterization of the plane triangulations admitting a windrose-planar drawing. Furthermore, for any embedded graph with n vertices that has a windrose-planar drawing, we can construct one with at most one bend per edge and with at most 2n − 5 bends in total, which lies on the 3n × 3n grid. The latter result contrasts with the fact that straight-line windrose-planar drawings may require exponential area. © 2018 Association for Computing Machinery.",Algorithms; Combinatorial embeddings; Graph drawing; Planar graphs; Upward planarity,Algorithms; Drawing (graphics); Polynomial approximation; Embedded graphs; Embeddings; Exponential areas; Graph drawing; Planar graph; Planarity; Polynomial-time algorithms; Vertical relationships; Graph theory
Online submodular maximization with free disposal,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053920490&doi=10.1145%2f3242770&partnerID=40&md5=8afb4bfdac0d968bc2031a8bb77fd1f4,"We study the online submodular maximization problem with free disposal under a matroid constraint. Elements from some ground set arrive one by one in rounds, and the algorithm maintains a feasible set that is independent in the underlying matroid. In each round when a new element arrives, the algorithm may accept the new element into its feasible set and possibly remove elements from it, provided that the resulting set is still independent. The goal is to maximize the value of the final feasible set under some monotone submodular function, to which the algorithm has oracle access. For k-uniform matroids, we give a deterministic algorithm with competitive ratio at least 0.2959, and the ratio approaches α                             1                             ∞ ≈ 0.3178 as k approaches infinity, improving the previous best ratio of 0.25 by Chakrabarti and Kale (IPCO 2014), Buchbinder et al. (SODA 2015), and Chekuri et al. (ICALP 2015). We also show that our algorithm is optimal among a class of deterministic monotone algorithms that accept a new arriving element only if the objective is strictly increased. Further, we prove that no deterministic monotone algorithm can be strictly better than 0.25-competitive even for partition matroids, the most modest generalization of k-uniform matroids, matching the competitive ratio by Chakrabarti and Kale (IPCO 2014) and Chekuri et al. (ICALP 2015). Interestingly, we show that randomized algorithms are strictly more powerful by giving a (non-monotone) randomized algorithm for partition matroids with ratio α                             1                             ∞ ≈ 0.3178. © 2018 Association for Computing Machinery.",Free disposal; Online submodular maximization; Uniform matroid,Algorithms; Mathematical techniques; Competitive ratio; Deterministic algorithms; Deterministic monotone algorithms; Maximization problem; Oracle access; Randomized Algorithms; Submodular; Submodular functions; Combinatorial mathematics
A mazing 2+ε approximation for unsplittable flow on a path,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053900610&doi=10.1145%2f3242769&partnerID=40&md5=b1c839d3521453e69a737cf307106b55,"We study the problem of unsplittable flow on a path (UFP), which arises naturally in many applications such as bandwidth allocation, job scheduling, and caching. Here we are given a path with nonnegative edge capacities and a set of tasks, which are characterized by a subpath, a demand, and a profit. The goal is to find the most profitable subset of tasks whose total demand does not violate the edge capacities. Not surprisingly, this problem has received a lot of attention in the research community. If the demand of each task is at most a small-enough fraction δ of the capacity along its subpath (δ-small tasks), then it has been known for a long time [Chekuri et al., ICALP 2003] how to compute a solution of value arbitrarily close to the optimum via LP rounding. However, much remains unknown for the complementary case, that is, when the demand of each task is at least some fraction δ > 0 of the smallest capacity of its subpath (δ-large tasks). For this setting, a constant factor approximation is known, improving on an earlier logarithmic approximation [Bonsma et al., FOCS 2011]. In this article, we present a polynomial-time approximation scheme (PTAS) for δ-large tasks, for any constant δ > 0. Key to this result is a complex geometrically inspired dynamic program. Each task is represented as a segment underneath the capacity curve, and we identify a proper maze-like structure so that each corridor of the maze is crossed by only O(1) tasks in the optimal solution. The maze has a tree topology, which guides our dynamic program. Our result implies a 2 + ε approximation for UFP, for any constant ε > 0, improving on the previously best 7 + ε approximation by Bonsma et al. We remark that our improved approximation algorithm matches the best known approximation ratio for the considerably easier special case of uniform edge capacities. © 2018 Association for Computing Machinery.",Approximation algorithms,Polynomial approximation; Profitability; Approximation ratios; Constant factor approximation; Dynamic programs; Logarithmic approximation; Optimal solutions; Polynomial time approximation schemes; Research communities; Unsplittable flow; Approximation algorithms
Deterministic parallel algorithms for fooling polylogarithmic juntas and the Lovász Local Lemma,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053436935&doi=10.1145%2f3230651&partnerID=40&md5=bd5782b978c4b0e8e1735b05503fc5ee,"Many randomized algorithms can be derandomized efficiently using either the method of conditional expectations or probability spaces with low (almost-) independence. A series of papers, beginning with Luby (1993) and continuing with Berger and Rompel (1991) and Chari et al. (2000), showed that these techniques can be combined to give deterministic parallel algorithms for combinatorial optimization problems involving sums of w-juntas. We improve these algorithms through derandomized variable partitioning, reducing the processor complexity to essentially independent of w and time complexity to linear in w. As a key subroutine, we give a new algorithm to generate a probability space which can fool a given set of neighborhoods. Schulman (1992) gave an NC algorithm to do so for neighborhoods of size w ≤ O(log n). Our new algorithm is in NC1, with essentially optimal time and processor complexity, when w = O(log n); it remains in NC up to w = polylog(n). This answers an open problem of Schulman. One major application of these algorithms is an NC algorithm for the Lovász Local Lemma. Previous NC algorithms, including the seminal algorithm of Moser and Tardos (2010) and the work of Chandrasekaran et. al (2013), required that (essentially) the bad-events could span only O(log n) variables; we relax this to polylog(n) variables. We use this for an NC2 algorithm for defective vertex coloring, which works for arbitrary degree graphs. © 2018 ACM.",Derandomization; Lovász local lemma; Method of conditional expectations; NC algorithms,Combinatorial optimization; Parallel algorithms; Arbitrary degree; Combinatorial optimization problems; Conditional expectation; Derandomization; Local lemmata; Probability spaces; Randomized Algorithms; Time complexity; Parallel processing systems
Network sparsification for steiner problems on planar and bounded-genus graphs,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053931448&doi=10.1145%2f3239560&partnerID=40&md5=f1635a407caf6887308ca1771db70461,"We propose polynomial-time algorithms that sparsify planar and bounded-genus graphs while preserving optimal or near-optimal solutions to Steiner problems. Our main contribution is a polynomial-time algorithm that, given an unweighted undirected graph G embedded on a surface of genus д and a designated face f bounded by a simple cycle of length k, uncovers a set F ⊆ E(G) of size polynomial in д and k that contains an optimal Steiner tree for any set of terminals that is a subset of the vertices of f . We apply this general theorem to prove that: - Given an unweighted graph G embedded on a surface of genus д and a terminal set S ⊆ V (G), one can in polynomial time find a set F ⊆ E(G) that contains an optimal Steiner tree T for S and that has size polynomial in д and |E(T ) |. - An analogous result holds for an optimal Steiner forest for a set S of terminal pairs. - Given an unweighted planar graph G and a terminal set S ⊆ V (G), one can in polynomial time find a set F ⊆ E(G) that contains an optimal (edge) multiway cut C separating S (i.e., a cutset that intersects any path with endpoints in different terminals from S) and that has size polynomial in |C|. In the language of parameterized complexity, these results imply the first polynomial kernels for Steiner Tree and Steiner Forest on planar and bounded-genus graphs (parameterized by the size of the tree and forest, respectively) and for (Edge) Multiway Cut on planar graphs (parameterized by the size of the cutset). Additionally, we obtain a weighted variant of our main contribution: a polynomial-time algorithm that, given an undirected plane graph G with positive edge weights, a designated face f bounded by a simple cycle of weight w(f ), and an accuracy parameter ε > 0, uncovers a set F ⊆ E(G) of total weight at most poly(ε−1)w(f ) that, for any set of terminal pairs that lie on f , contains a Steiner forest within additive error εw(f ) from the optimal Steiner forest. © 2018 Association for Computing Machinery.",Kernelization; Planar graphs; Polynomial kernel; Sparsification; Steiner tree,Forestry; Graphic methods; Parameterization; Polynomial approximation; Kernelization; Planar graph; Polynomial kernels; Sparsification; Steiner trees; Graph theory
Streaming algorithms for estimating the matching size in planar graphs and beyond,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053447233&doi=10.1145%2f3230819&partnerID=40&md5=2638569b8c234b32acb6565d88375a07,"We consider the problem of estimating the size of a maximum matching when the edges are revealed in a streaming fashion. When the input graph is planar, we present a simple and elegant streaming algorithm that, with high probability, estimates the size of a maximum matching within a constant factor using Õ(n2/3) space, where n is the number of vertices. The approach generalizes to the family of graphs that have bounded arboricity, which include graphs with an excluded constant-size minor. To the best of our knowledge, this is the first result for estimating the size of a maximum matching in the adversarial-order streaming model (as opposed to the random-order streaming model) in o(n) space. We circumvent the barriers inherent in the adversarial-order model by exploiting several structural properties of planar graphs, and more generally, graphs with bounded arboricity. We further reduce the required memory size to Õ(n) for three restricted settings: (i) when the input graph is a forest; (ii) when we have 2-passes and the input graph has bounded arboricity; and (iii) when the edges arrive in random order and the input graph has bounded arboricity. Finally, we design a reduction from the Boolean Hidden Matching Problem to show that there is no randomized streaming algorithm that estimates the size of the maximum matching to within a factor better than 3/2 and uses only o(n1/2) bits of space. Using the same reduction, we show that there is no deterministic algorithm that computes this kind of estimate in o(n) bits of space. The lower bounds hold even for graphs that are collections of paths of constant length. © 2018 ACM.",Bounded arboricity; Estimating matching size; Maximal matching; Planar graphs; Streaming algorithms,Graphic methods; Bounded arboricity; Estimating matching size; Maximal matchings; Planar graph; Streaming algorithm; Graph theory
An efficient algorithm for computing high-quality paths amid polygonal obstacles,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052557642&doi=10.1145%2f3230650&partnerID=40&md5=8318d48aea704f2ffc6f816678546c3b,"We study a path-planning problem amid a set O of obstacles in R2, in which we wish to compute a short path between two points while also maintaining a high clearance from O; the clearance of a point is its distance from a nearest obstacle in O. Specifically, the problem asks for a path minimizing the reciprocal of the clearance integrated over the length of the path. We present the first polynomial-time approximation scheme for this problem. Let n be the total number of obstacle vertices and let ε ∈ (0, 1]. Our algorithm computes in time O(n                             ε                             2                             2 logn                             ε ) a path of total cost at most (1 + ε) times the cost of the optimal path. © 2018 ACM.",Approximation; Bicriteria objective; Geometry; Motion planning,Geometry; Polynomial approximation; Approximation; Bi-criteria; High quality; Optimal paths; Path planning problems; Polynomial time approximation schemes; Short-path; Two-point; Motion planning
Graph reconstruction and verification,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051816065&doi=10.1145%2f3199606&partnerID=40&md5=11176a7414260b9514a5b7aa2de49573,"How efficiently can we find an unknown graph using distance or shortest path queries between its vertices? We assume that the unknown graph G is connected, unweighted, and has bounded degree. In the reconstruction problem, the goal is to find the graph G. In the verification problem, we are given a hypothetical graph Ĝ and want to check whether G is equal to Ĝ. We provide a randomized algorithm for reconstruction using O (n3/2) distance queries, based on Voronoi cell decomposition. Next, we analyze natural greedy algorithms for reconstruction using a shortest path oracle and also for verification using either oracle, and showthat their query complexity is n1+o(1).We further improve the query complexitywhen the graph is chordal or outerplanar. Finally,we showsome lower bounds, and consider an approximate version of the reconstruction problem. © 2018 ACM.",Network Tomography; Phrases: Reconstruction; Verification,Concrete beams and girders; Verification; Bounded degree; Greedy algorithms; Network tomography; Query complexity; Randomized Algorithms; Reconstruction problems; Shortest path queries; Verification problems; Graph theory
Batched point location in SINR Diagrams via algebraic tools,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049792366&doi=10.1145%2f3209678&partnerID=40&md5=6fddeef9562e73a3dc34043a1f148aa1,"The SINR (Signal to Interference plus Noise Ratio) model for the quality of wireless connections has been the subject of extensive recent study. It attempts to predict whether a particular transmitter is heard at a specific location, in a setting consisting of n simultaneous transmitters and background noise. The SINR model gives rise to a natural geometric object, the SINR diagram, which partitions the space into n regions where each of the transmitters can be heard and the remaining space where no transmitter can be heard. Efficient point location in the SINR diagram, i.e., being able to build a data structure that facilitates determining, for a query point, whether any transmitter is heard there, and if so, which one, has been recently investigated in several articles. These planar data structures are constructed in time at least quadratic in n and support logarithmic-time approximate queries.Moreover, the performance of some of the proposed structures depends strongly not only on the number n of transmitters and on the approximation parameter, but also on some geometric parameters that cannot be bounded a priori as a function of n or. In this article, we address the question of batched point location queries, i.e., answering many queries simultaneously. Specifically, in one dimension, we can answer n queries exactly in amortized polylogarithmic time per query, while in the plane we can do it approximately. In another result, we show how to answer n2 queries exactly in amortized polylogarithmic time per query, assuming the queries are located on a possibly non-uniform n ~ n grid. All these results can handle arbitrary power assignments to the transmitters. Moreover, the amortized query time in these results depends only on n and We also show how to speed up the preprocessing in a previously proposed point-location structure in SINR diagram for uniform-power sites, by almost a full order of magnitude. For this, we obtain results on the sensitivity of the reception regions to slight changes in the reception threshold, which are of independent interest. Finally, these results demonstrate the (so far underutilized) power of combining algebraic tools with those of computational geometry and other fields. © 2018 ACM.",Algebraic Methods; Batched Point Location; Fast Polynomial Multiplication; Fast Polynomial Multipoint Evaluation; Range Searching; Sinr Diagram; Sinr Model; Wireless Networks,Algebra; Computational geometry; Data structures; Location; Signal interference; Signal to noise ratio; Transmitters; Wireless networks; Algebraic method; Multipoint evaluation; Point location; Polynomial multiplication; Range searching; Sinr Diagram; Sinr models; Query processing
Packing groups of items into multiple knapsacks,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052602353&doi=10.1145%2f3233524&partnerID=40&md5=001606fba8be1514893ab398cf720979,"We consider a natural generalization of the classical multiple knapsack problem in which instead of packing single items we are packing groups of items. In this problem, we have multiple knapsacks and a set of items partitioned into groups. Each item has an individual weight, while the profit is associated with groups rather than items. The profit of a group can be attained if and only if every item of this group is packed. Such a general model finds applications in various practical problems, e.g., delivering bundles of goods. The tractability of this problem relies heavily on how large a group could be. Deciding if a group of items of total weight 2 could be packed into two knapsacks of unit capacity is already NP-hard and it thus rules out a constant-approximation algorithm for this problem in general. We then focus on the parameterized version where the total weight of items in each group is bounded by a factor δ of the total capacity of all knapsacks. Both approximation and inapproximability results with respect to δ are derived. We also show that, depending on whether the number of knapsacks is a constant or part of the input, the approximation ratio for the problem, as a function on δ, changes substantially, which has a clear difference from the classical multiple knapsack problem. © 2018 ACM.",Approximation schemes; Bin packing; Lower bounds; Multiple knapsack,Combinatorial optimization; Profitability; Approximation ratios; Approximation scheme; Bin packing; Constant approximation algorithms; Lower bounds; Multiple knapsack; Multiple knapsack problem; Natural generalization; Approximation algorithms
Adaptive computation of the swap-insert correction distance,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051759027&doi=10.1145%2f3232057&partnerID=40&md5=a179e792e49ffc75aed841b7ef8c4874,"The Swap-Insert Correction distance from a string S of length n to another string L of length m≥n on the alphabet [1.δ] is the minimum number of insertions, and swaps of pairs of adjacent symbols, converting S into L. Contrarily to other correction distances, computing it is NP-Hard in the size δ of the alphabet. We describe an algorithm computing this distance in time within O(δ2nmtδ.1), where for each [1.δ] there are occurrences of in S,mϵoccurrences of ¿ in L, and where ""t = max [1.δ] min is a new parameter of the analysis, measuring one aspect of the difficulty of the instance. The difficulty ""t is bounded by above by various terms, such as the length n of the shortest string S, and by the maximum number of occurrences of a single character in S (max[1.δ]). This result illustrates how, in many cases, the correction distance between two strings can be easier to compute than in the worst case scenario. © 2018 ACM.",Adaptive; Dynamic Programming; Edit Distance; Insert; Swap,Algorithms; Mathematical techniques; Adaptive; Adaptive computations; Algorithm computing; Edit distance; Insert; New parameters; Swap; Worst case scenario; Dynamic programming
Approximation guarantees for the minimum linear arrangement problem by higher eigenvalues,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052589719&doi=10.1145%2f3228342&partnerID=40&md5=5b0fce14aa9e888d58153c00c9c13b02,"Given an n-vertex undirected graph G = (V, E) and positive edge weights {we}e∈E, a linear arrangement is a permutation π : V → {1, 2, . . .,n}. The value of the arrangement is val(G, π):=n                             1                             e={u                             ,v}∈E we|π(u) − π(v)|. In the minimum linear arrangement problem, the goal is to find a linear arrangement π∗ that achieves val(G, π∗) = MLA(G):= minπ val(G, π). In this article, we show that for any ϵ > 0 and positive integer r, there is an nO(r/ϵ)-time randomized algorithm that, given a graph G, returns a linear arrangement π, such that (equation presented) with high probability, where L is the normalized Laplacian of G and λr (L) is the rth smallest eigenvalue of L. Our algorithm gives a constant factor approximation for regular graphs that are weak expanders. © 2018 ACM.",Expander graph; Graph Laplacian; Lasserre hierarchy; Ordering problem; Semidefinite programming,Approximation algorithms; Eigenvalues and eigenfunctions; Laplace transforms; Expander graphs; Graph Laplacian; Lasserre hierarchy; Order problems; Semi-definite programming; Graph theory
Dynamic time warping and geometric edit distance: Breaking the quadratic barrier,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052564898&doi=10.1145%2f3230734&partnerID=40&md5=4bac2fd8880cde22b58b9a44f8946247,"Dynamic Time Warping (DTW) and Geometric Edit Distance (GED) are basic similarity measures between curves or general temporal sequences (e.g., time series) that are represented as sequences of points in some metric space (X, dist). The DTW and GED measures are massively used in various fields of computer science and computational biology. Consequently, the tasks of computing these measures are among the core problems in P. Despite extensive efforts to find more efficient algorithms, the best-known algorithms for computing the DTW or GED between two sequences of points in X = Rd are long-standing dynamic programming algorithms that require quadratic runtime, even for the one-dimensional case d = 1, which is perhaps one of the most used in practice. In this article, we break the nearly 50-year-old quadratic time bound for computing DTW or GED between two sequences of n points in R by presenting deterministic algorithms that run in O(n2 log log log n/ log log n) time. Our algorithms can be extended to work also for higher-dimensional spaces Rd, for any constant d, when the underlying distance-metric dist is polyhedral (e.g., L1, L∞). © 2018 ACM.",Dynamic time warping; Geometric edit distance; Geometric matching; Point matching; Time series,Geometry; One dimensional; Time series; Best-known algorithms; Computational biology; Deterministic algorithms; Dynamic programming algorithm; Dynamic time warping; Edit distance; Geometric matching; Point-matching; Dynamic programming
Conditional lower bounds for all-pairs max-flow,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052564778&doi=10.1145%2f3212510&partnerID=40&md5=bc2af832bd1575908ef8540ad3d8d1da,"We provide evidence that computing the maximum flow value between every pair of nodes in a directed graph on n nodes, m edges, and capacities in the range [1..n], which we call the All-Pairs Max-Flow problem, cannot be solved in time that is significantly faster (i.e., by a polynomial factor) than O(n3) even for sparse graphs, namely m = O(n); thus for general m, it cannot be solved significantly faster than O(n2m). Since a single maximum st-flow can be solved in time Õ (mn) [Lee and Sidford, FOCS 2014], we conclude that the all-pairs version might require time equivalent to Ω( n3/2) computations of maximum st-flow, which strongly separates the directed case from the undirected one. Moreover, if maximum st-flow can be solved in time Õ (m), then the runtime of Ω( n2) computations is needed. This is in contrast to a conjecture of Lacki, Nussbaum, Sankowski, and Wulff-Nilsen [FOCS 2012] that All-Pairs Max-Flow in general graphs can be solved faster than the time of O(n2) computations of maximum st-flow. Specifically, we show that in sparse graphs G = (V, E, w), if one can compute the maximum st-flow from every s in an input set of sources S ⊆ V to every t in an input set of sinks T ⊆ V in time O((|S||T |m)1−ε ), for some |S|, |T | and a constant ε > 0, then MAX-CNF-SAT (maximum satisfiability of conjunctive normal form formulas) with n variables and m clauses can be solved in time mO(1)2(1−δ)n for a constant δ(ε) > 0, a problem for which not even 2n/poly(n) algorithms are known. Such running time for MAX-CNF-SAT would in particular refute the Strong Exponential Time Hypothesis (SETH). Hence, we improve the lower bound of Abboud, Vassilevska-Williams, and Yu [STOC 2015], who showed that for every fixed ε > 0 and |S| = |T | = O(n), if the above problem can be solved in time O(n3/2−ε ), then some incomparable (and intuitively weaker) conjecture is false. Furthermore, a larger lower bound than ours implies strictly super-linear time for maximum st-flow problem, which would be an amazing breakthrough. In addition, we show that All-Pairs Max-Flow in uncapacitated networks with every edge-density m = m(n) cannot be computed in time significantly faster than O(mn), even for acyclic networks. The gap to the fastest known algorithm by Cheung, Lau, and Leung [FOCS 2011] is a factor of O(mω−1/n), and for acyclic networks it is O(nω−1), where ω is the matrix multiplication exponent. Finally, we extend our lower bounds to the version that asks only for the maximum-flow values below a given threshold (over all source-sink pairs). © 2018 ACM.",All-pairs maximum flow; Conditional lower bounds; Hardness in P; Strong exponential time hypothesis,Graph theory; Conjunctive normal forms; Lower bounds; MAtrix multiplication; Maximum flows; Maximum satisfiability; Polynomial factor; Source-sink pairs; Strong exponential time hypothesis; Flow graphs
An efficient representation for filtrations of simplicial complexes,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052569679&doi=10.1145%2f3229146&partnerID=40&md5=2d6aa216ffa2b88feb7262ff94dec6e8,"A filtration over a simplicial complex K is an ordering of the simplices of K such that all prefixes in the ordering are subcomplexes of K. Filtrations are at the core of Persistent Homology, a major tool in Topological Data Analysis. To represent the filtration of a simplicial complex, the entire filtration can be appended to any data structure that explicitly stores all the simplices of the complex such as the Hasse diagram or the recently introduced Simplex Tree [Algorithmica'14]. However, with the popularity of various computational methods that need to handle simplicial complexes, and with the rapidly increasing size of the complexes, the task of finding a compact data structure that can still support efficient queries is of great interest. This direction has been recently pursued for the case of maintaining simplicial complexes. For instance, Boissonnat et al. [Algorithmica'17] considered storing the simplices that are maximal with respect to inclusion and Attali et al. [IJCGA'12] considered storing the simplices that block the expansion of the complex. Nevertheless, so far there has been no data structure that compactly stores the filtration of a simplicial complex, while also allowing the efficient implementation of basic operations on the complex. In this article, we propose a new data structure called the Critical Simplex Diagram (CSD), which is a variant of the Simplex Array List [Algorithmica'17]. Our data structure allows one to store in a compact way the filtration of a simplicial complex and allows for the efficient implementation of a large range of basic operations. Moreover, we prove that our data structure is essentially optimal with respect to the requisite storage space. Finally, we show that the CSD representation admits fast construction algorithms for Flag complexes and relaxed Delaunay complexes. © 2018 ACM.",Compact data structures; Filtration; Simplicial complex,Data structures; Filtration; Structural optimization; Basic operation; Compact data structure; Construction algorithms; Efficient implementation; Hasse diagrams; Persistent homology; Simplicial complex; Topological data analysis; Digital storage
The dependent doors problem: An investigation into sequential decisions without feedback,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052591375&doi=10.1145%2f3218819&partnerID=40&md5=830b73684b2b72ef67f85df1fc2cf6a4,"We introduce the dependent doors problem as an abstraction for situations in which one must perform a sequence of dependent decisions, without receiving feedback information on the effectiveness of previously made actions. Informally, the problem considers a set of d doors that are initially closed, and the aim is to open all of them as fast as possible. To open a door, the algorithm knocks on it, and it might open or not according to some probability distribution. This distribution may depend on which other doors are currently open, as well as on which other doors were open during each of the previous knocks on that door. The algorithm aims to minimize the expected time until all doors open. Crucially, it must act at any time without knowing whether or which other doors have already opened. In this work, we focus on scenarios where dependencies between doors are both positively correlated and acyclic. The fundamental distribution of a door describes the probability it opens in the best of conditions (with respect to other doors being open or closed). We show that if in two configurations of d doors corresponding doors share the same fundamental distribution, then these configurations have the same optimal running time up to a universal constant, no matter what the dependencies between doors and what the distributions. We also identify algorithms that are optimal up to a universal constant factor. For the case in which all doors share the same fundamental distribution, we additionally provide a simpler algorithm and a formula to calculate its running time. We furthermore analyse the price of lacking feedback for several configurations governed by standard fundamental distributions. In particular, we show that the price is logarithmic in d for memoryless doors but can potentially grow to be linear in d for other distributions. We then turn our attention to investigate precise bounds. Even for the case of two doors, identifying the optimal sequence is an intriguing combinatorial question. Here, we study the case of two cascading memoryless doors. That is, the first door opens on each knock independently with probability p1. The second door can only open if the first door is open, in which case it will open on each knock independently with probability p2. We solve this problem almost completely by identifying algorithms that are optimal up to an additive term of 1. © 2018 ACM.",Combinatorics; Multi-armed bandit; No feedback; Search,Problem solving; Combinatorics; Feed back information; Identify algorithms; Multi armed bandit; No feedbacks; Search; Sequential decisions; Universal constants; Probability distributions
Editorial: ACM-SIAM symposium on discrete algorithms (SODA) 2016 special issue,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052607817&doi=10.1145%2f3230647&partnerID=40&md5=58b535f3a472d05791f24638c08ac33a,[No abstract available],,
Data structures for weighted matching and extensions to b-matching and f-factors,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052606738&doi=10.1145%2f3183369&partnerID=40&md5=a5ed0864897eb36d053c542031bde431,"This article shows the weighted matching problem on general graphs can be solved in time O(n(m + n logn)) for n and m the number of vertices and edges, respectively. This was previously known only for bipartite graphs. The crux is a data structure for blossom creation. It uses a dynamic nearest-common-ancestor algorithm to simplify blossom steps, so they involve only back edges rather than arbitrary nontree edges. The rest of the article presents direct extensions of Edmonds' blossom algorithm to weighted b-matching and f -factors. Again, the time bound is the one previously known for bipartite graphs: for b-matching the time is O(min{b(V),n logn}(m + n logn)) and for f -factors the time is O(min{f (V),m logn}(m + n logn)), where b(V) and f (V) both denote the sum of all degree constraints. Several immediate applications of the f -factor algorithm are given: The generalized shortest path structure of Reference [19], i.e., the analog of the shortest-paths tree for conservative undirected graphs, is shown to be a version of the blossom structure for f -factors. This structure is found in time O(|N|(m + n logn)) for N, the set of negative edges (0 < |N| < n). A shortest T-join is found in time O(n(m + n logn)) or O(|T|(m + n logn)) when all costs are nonnegative. These bounds are all slight improvements of previously known ones, and are simply achieved by proper initialization of the f -factor algorithm. © 2018 ACM.",B-matching; Blossom; Conservative graph; Degree-constrained subgraph; F-factor; Matching; Shortest-paths tree; T-join,Data structures; Forestry; Graphic methods; Trees (mathematics); B-matching; Blossom; Conservative graph; F factor; Matching; Shortest Paths Tree; Subgraphs; T-joins; Graph theory
Deterministic algorithms for submodular maximization problems,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052585613&doi=10.1145%2f3184990&partnerID=40&md5=579bfc4d592e5b9958368dfa9bb4551d,"Randomization is a fundamental tool used in many theoretical and practical areas of computer science. We study here the role of randomization in the area of submodular function maximization. In this area, most algorithms are randomized, and in almost all cases the approximation ratios obtained by current randomized algorithms are superior to the best results obtained by known deterministic algorithms. Derandomization of algorithms for general submodular function maximization seems hard since the access to the function is done via a value oracle. This makes it hard, for example, to apply standard derandomization techniques such as conditional expectations. Therefore, an interesting fundamental problem in this area is whether randomization is inherently necessary for obtaining good approximation ratios. In this work, we give evidence that randomization is not necessary for obtaining good algorithms by presenting a new technique for derandomization of algorithms for submodular function maximization. Our high level idea is to maintain explicitly a (small) distribution over the states of the algorithm, and carefully update it using marginal values obtained from an extreme point solution of a suitable linear formulation. We demonstrate our technique on two recent algorithms for unconstrained submodular maximization and for maximizing a submodular function subject to a cardinality constraint. In particular, for unconstrained submodular maximization we obtain an optimal deterministic 1/2-approximation showing that randomization is unnecessary for obtaining optimal results for this setting. © 2018 ACM.",Derandomization; Submodular function maximization; Unconstrained submodular maximization,Random processes; Cardinality constraints; Conditional expectation; Derandomization; Deterministic algorithms; Maximization problem; Randomized Algorithms; Submodular; Submodular functions; Approximation algorithms
Exploring the complexity of layout parameters in tournaments and semicomplete digraphs,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052583974&doi=10.1145%2f3196276&partnerID=40&md5=7c0e2edeff8f72c1db5e8cbf69934962,"A simple digraph is semicomplete if for any two of its vertices u and v, at least one of the arcs (u,v) and (v,u) is present. We study the complexity of computing two layout parameters of semicomplete digraphs: cutwidth and optimal linear arrangement (Ola). We prove the following: • Both parameters are NP-hard to compute and the known exact and parameterized algorithms for them have essentially optimal running times, assuming the Exponential Time Hypothesis. • The cutwidth parameter admits a quadratic Turing kernel, whereas it does not admit any polynomial kernel unless NP ⊆ coNP/poly. By contrast, Ola admits a linear kernel. These results essentially complete the complexity analysis of computing cutwidth and Ola on semicomplete digraphs (with respect to standard parameters). Our techniques also can be used to analyze the sizes of minimal obstructions for having a small cutwidth under the induced subdigraph relation. © 2018 ACM.",Cutwidth; Phrases: Kernelization theory; Semicomplete digraphs; Tournaments; Words,Directed graphs; Graph theory; Optimization; Cutwidths; Kernelization; Semicomplete digraph; Tournaments; Words; Parameter estimation
Subexponential parameterized algorithm for interval completion,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052599880&doi=10.1145%2f3186896&partnerID=40&md5=b02c18c56580b0e47f4873da0f17faa3,"In the Interval Completion problem we are given an n-vertex graph G and an integer k, and the task is to transform G by making use of at most k edge additions into an interval graph. This is a fundamental graph modification problem with applications in sparse matrix multiplication and molecular biology. The question about fixed-parameter tractability of Interval Completion was asked by Kaplan et al. [FOCS 1994; SIAM J. Comput. 1999] and was answered affirmatively more than a decade later by Villanger et al. [STOC 2007; SIAM J. Comput. 2009], who presented an algorithm with running time O(k2kn3m). We give the first subexponential parameterized algorithm solving Interval Completion in time kO(k)nO(1). This adds Interval Completion to a very small list of parameterized graph modification problems solvable in subexponential time. © 2018 ACM.",Completion problems; Graph modification problems; Interval graphs; Subexponential algorithms,Graph theory; Molecular biology; Parameter estimation; Parameterization; Completion problem; Fixed-parameter tractability; Graph modification problems; Interval completion; Interval graph; Parameterized algorithm; Parameterized graphs; Sub-exponential algorithms; Problem solving
On the integrality gap of degree-4 sum of squares for planted clique,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052591350&doi=10.1145%2f3178538&partnerID=40&md5=a6e0c05e28d63ec04da865884da17581,"The problem of finding large cliques in random graphs and its “planted” variant, where one wants to recover a clique of size ω log (n) added to an Erdős-Rényi graph G ∼ G(n,1                             2 ), have been intensely studied. Nevertheless, existing polynomial time algorithms can only recover planted cliques of size ω = Ω(n). By contrast, information theoretically, one can recover planted cliques so long as ω log (n). In this work, we continue the investigation of algorithms from the Sum of Squares hierarchy for solving the planted clique problem begun by Meka, Potechin, and Wigderson [2] and Deshpande and Montanari [25]. Our main result is that degree four SoS does not recover the planted clique unless ω n/ polylog n, improving on the bound ω n1/3 due to Reference [25]. An argument of Kelner shows that the this result cannot be proved using the same certificate as prior works. Rather, our proof involves constructing and analyzing a new certificate that yields the nearly tight lower bound by “correcting” the certificate of References [2, 25, 27]. © 2018 ACM.",Planted clique; Random matrices; Sum of Squares method,Graph theory; Information theory; Polynomial approximation; Integrality gaps; Investigation of algorithm; Lower bounds; Planted cliques; Polynomial-time algorithms; Random graphs; Random matrices; Sum of squares; Recovery
Near-optimal light spanners,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052604593&doi=10.1145%2f3199607&partnerID=40&md5=7e1d66f518cf070bcdefc0888aeb171f,"A spanner H of a weighted undirected graph G is a “sparse” subgraph that approximately preserves distances between every pair of vertices in G. We refer to H as a δ-spanner of G for some parameter δ ≥ 1 if the distance in H between every vertex pair is at most a factor δ bigger than in G. In this case, we say that H has stretch δ. Two main measures of the sparseness of a spanner are the size (number of edges) and the total weight (the sum of weights of the edges in the spanner). It is well-known that for any positive integer k, one can efficiently construct a (2k − 1)-spanner of G with O(n1+1/k ) edges where n is the number of vertices [2]. This size-stretch tradeoff is conjectured to be optimal based on a girth conjecture of Erdős [17]. However, the current state of the art for the second measure is not yet optimal. Recently Elkin, Neiman and Solomon [ICALP 14] presented an improved analysis of the greedy algorithm, proving that the greedy algorithm admits (2k − 1) · (1 + ϵ) stretch and total edge weight of Oϵ ((k/ log k) · ω(MST (G)) · n1/k ), where ω(MST (G)) is the weight of a MST of G. The previous analysis by Chandra et al. [SOCG 92] admitted (2k − 1) · (1 + ϵ) stretch and total edge weight of Oϵ (kω(MST (G))n1/k ). Hence, Elkin et al. improved the weight of the spanner by a log k factor. In this article, we completely remove the k factor from the weight, presenting a spanner with (2k − 1) · (1 + ϵ) stretch, Oϵ (ω(MST (G))n1/k ) total weight, and O(n1+1/k ) edges. Up to a (1 + ϵ) factor in the stretch this matches the girth conjecture of Erdős [17]. © 2018 ACM.",Light spanner,Algorithms; Mathematical techniques; Edge weights; Greedy algorithms; Near-optimal; Positive integers; State of the art; Subgraphs; Vertex pairs; Weighted undirected graph; Graph theory
Improved deterministic algorithms for linear programming in low dimensions,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052556526&doi=10.1145%2f3155312&partnerID=40&md5=f93f27c8d3c0467286203ad28303ada6,"Chazelle and Matoušek [J. Algorithms, 1996] presented a derandomization of Clarkson's sampling-based algorithm [J. ACM, 1995] for solving linear programs with n constraints and d variables in d(7+o(1))dn deterministic time. The time bound can be improved to d(5+o(1))dn with subsequent work by Brönnimann, Chazelle, and Matoušek [SIAM J. Comput., 1999]. We first point out a much simpler derandomization of Clarkson's algorithm that avoids ε-approximations and runs in d(3+o(1))dn time. We then describe a few additional ideas that eventually improve the deterministic time bound to d(1/2+o(1))dn. © 2018 ACM.",Computational geometry; Derandomization; Epsilon-nets; Linear programming,Approximation algorithms; Computational geometry; Derandomization; Deterministic algorithms; Epsilon-nets; Linear programs; Sampling-based algorithms; Time bound; Linear programming
Fully polynomial-time parameterized computations for graphs and matrices of low treewidth,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052576471&doi=10.1145%2f3186898&partnerID=40&md5=7fb1b4eb937e696f3f5050c43a1c49b2,"We investigate the complexity of several fundamental polynomial-time solvable problems on graphs and on matrices, when the given instance has low treewidth; in the case of matrices, we consider the treewidth of the graph formed by non-zero entries. In each of the considered cases, the best known algorithms working on general graphs run in polynomial time; however, the exponent of the polynomial is large. Therefore, our main goal is to construct algorithms with running time of the form poly(k) · n or poly(k) · n log n, where k is the width of the tree decomposition given on the input. Such procedures would outperform the best known algorithms for the considered problems already for moderate values of the treewidth, like O (n1/c ) for a constant c. Our results include the following: - an algorithm for computing the determinant and the rank of an n × n matrix using O (k3 · n) time and arithmetic operations; - an algorithm for solving a system of linear equations using O (k3 · n) time and arithmetic operations; - an O (k3 · n log n)-time randomized algorithm for finding the cardinality of a maximum matching in a graph; - an O (k4 · n log2 n)-time randomized algorithm for constructing a maximum matching in a graph; - an O (k2 · n log n)-time algorithm for finding a maximum vertex flow in a directed graph. Moreover, we give an approximation algorithm for treewidth with time complexity suited to the running times as above. Namely, the algorithm, when given a graph G and integer k, runs in time O (k7 · n log n) and either correctly reports that the treewidth of G is larger than k, or constructs a tree decomposition of G of width O (k2 ). © 2018 ACM.",,Approximation algorithms; Flow graphs; Graph theory; Matrix algebra; Polynomial approximation; Algorithm for solving; Arithmetic operations; Best-known algorithms; Maximum matchings; Parameterized computation; Randomized Algorithms; System of linear equations; Tree decomposition; Trees (mathematics)
Erratum: Approximating minimum-cost connectivity problems via uncrossable bifamilies (ACM Transactions on Algorithms (2012) 9:1 (1) DOI: 10.1145/2390176.2390177),2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052002503&doi=10.1145%2f3186991&partnerID=40&md5=6ce4891a616f13fc2e29a63205a6bd92,"There are two errors in our article “Approximating Minimum-Cost Connectivity Problems via Uncrossable Bifamilies” (ACM Transactions on Algorithms (TALG), 9(1), Article No. 1, 2012). In that article, we consider the (undirected) Survivable Network problem. The input consists of a graph G = (V, E) with edge-costs, a set T ⊆ V of terminals, and connectivity demands {rst > 0: st ∈ D ⊆ T × T}. The goal is to find a minimum cost subgraph of G that for all st ∈ D contains rst pairwise internally disjoint st-paths. We claimed ratios O(k ln k) for rooted demands when the set D of demand pairs form a star, where k = maxst ∈D rst is the maximum demand. This ratio is correct when the requirements are rst = k for all t ∈ T \ {s}, but for general rooted demands our article implies only ratio O(k2) (which, however, is still the currently best-known ratio for the problem). We also obtained various ratios for the node-weighted version of the problem. These results are valid, but the proof needs a correction described here. © 2018 ACM.",,
Faster algorithms for computing plurality points,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052562251&doi=10.1145%2f3186990&partnerID=40&md5=e3e1a43597379a77eab4f02ce020c109,"Let V be a set of n points in Rd, which we call voters. A point p ∈ Rd is preferred over another point p' ∈ Rd by a voter v ∈ V if dist(v, p) < dist(v, p'). A point p is called a plurality point if it is preferred by at least as many voters as any other point p'. We present an algorithm that decides in O(n log n) time whether V admits a plurality point in the L2 norm and, if so, finds the (unique) plurality point. We also give efficient algorithms to compute a minimum-cost subset W ⊂ V such that V \ W admits a plurality point, and to compute a so-called minimum-radius plurality 6 ball. Finally, we consider the problem in the personalized L1 norm, where each point v ∈ V has a preference vector w1 (v), . . ., wd (v) and the distance from v to any point p ∈ Rd is given by d                             i=1 wi (v) · |xi (v) − xi (p) |. For this case we can compute in O(nd                             −1 ) time the set of all plurality points of V. When all preference vectors are equal, the running time improves to O(n). © 2018 ACM.",Computational geometry; Computational social choice; Condorcet points; Plurality points; Voronoi games; Voting theory,Computation theory; Computational social choices; Condorcet points; Plurality points; Voronoi; Voting theory; Computational geometry
A faster subquadratic algorithm for finding outlier correlations,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052553224&doi=10.1145%2f3174804&partnerID=40&md5=60618981fbaf9dd84e03926971a4c689,"We study the problem of detecting outlier pairs of strongly correlated variables among a collection of n variables with otherwise weak pairwise correlations. After normalization, this task amounts to the geometric task where we are given as input a set of n vectors with unit Euclidean norm and dimension d, and for some constants 0 < τ < ρ < 1, we are asked to find all the outlier pairs of vectors whose inner product is at least ρ in absolute value, subject to the promise that all but at most q pairs of vectors have inner product at most τ in absolute value. Improving on an algorithm of Valiant [FOCS 2012; J. ACM 2015], we present a randomized algorithm that for Boolean inputs ({−1, 1}-valued data normalized to unit Euclidean length) runs in time (Equation presented), where 0 < γ < 1 is a constant tradeoff parameter and M(μ, ν) is the exponent to multiply an [nμ] × [nν ] matrix with an [nν] × [nμ] matrix and Δ = 1/(1 − logτ ρ). As corollaries we obtain randomized algorithms that run in time (Equation presented) and in time (Equation presented), where 2 ≤ ω < 2.38 is the exponent for square matrix multiplication and 0.3 < α ≤ 1 is the exponent for rectangular matrix multiplication. The notation Õ (·) hides polylogarithmic factors in n and d whose degree may depend on ρ and τ. We present further corollaries for the light bulb problem and for learning sparse Boolean functions. © 2018 ACM.",Correlation; Fast matrix multiplication; Light bulb problem; Rectangular matrix multiplication; Similarity search,Correlation methods; Incandescent lamps; Statistics; Fast matrix multiplication; Light bulbs; Pairwise correlation; Poly-logarithmic factors; Randomized Algorithms; Rectangular matrix; Similarity search; Subquadratic algorithms; Matrix algebra
Subtree isomorphism revisited,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052567713&doi=10.1145%2f3093239&partnerID=40&md5=ece18012a691c49666dad062de90e21f,"The Subtree Isomorphism problem asks whether a given tree is contained in another given tree. The problem is of fundamental importance and has been studied since the 1960s. For some variants, e.g., orderedtrees, near-linear time algorithms are known, but for the general case truly subquadratic algorithms remain elusive. Our first result is a reduction from the Orthogonal Vectors problem to Subtree Isomorphism, showing that a truly subquadratic algorithm for the latter refutes the Strong Exponential Time Hypothesis (SETH). In light of this conditional lower bound, we focus on natural special cases for which no truly subquadratic algorithms are known. We classify these cases against the quadratic barrier, showing in particular that: • Even for binary, rooted trees, a truly subquadratic algorithm refutes SETH. • Even for rooted trees of depthO(log logn), wheren is the total number of vertices, a truly subquadratic algorithm refutes SETH. • For every constant d, there is a constant εd > 0 and a randomized, truly subquadratic algorithm for degree-d rooted trees of depth at most (1 + εd) logd n. In particular, there is an O(min{2.85h,n2}) algorithm for binary trees of depth h. Our reductions utilize new “tree gadgets” that are likely useful for future SETH-based lower bounds for problems on trees. Our upper bounds apply a folklore result from randomized decision tree complexity. © 2018 ACM.",Fine-grained complexity; SETH; Subtree Isomorphism,Binary trees; Clustering algorithms; Decision trees; Set theory; Fine grained; Near-linear time; Orthogonal vectors; Randomized decisions; SETH; Strong exponential time hypothesis; Subquadratic algorithms; Subtree isomorphism; Trees (mathematics)
CoveringLSH: Locality-sensitive hashing without false negatives,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052578848&doi=10.1145%2f3155300&partnerID=40&md5=eefcb3f6d1f4dc77dbb8219876bc6a2c,"We consider a new construction of locality-sensitive hash functions for Hamming space that is covering in the sense that is it guaranteed to produce a collision for every pair of vectors within a given radius r. The construction is efficient in the sense that the expected number of hash collisions between vectors at distance cr, for a given c > 1, comes close to that of the best possible data independent LSH without the covering guarantee, namely, the seminal LSH construction of Indyk and Motwani (STOC'98). The efficiency of the new construction essentially matches their bound when the search radius is not too large-e.g., when cr = o(log(n)/ log log n), where n is the number of points in the dataset, and when cr = log(n)/k, where k is an integer constant. In general, it differs by at most a factor ln(4) in the exponent of the time bounds. As a consequence, LSH-based similarity search in Hamming space can avoid the problem of false negatives at little or no cost in efficiency. © 2018 ACM.",High-dimensional; Locality-sensitive hashing; Recall; Similarity search,Efficiency; Hamming distance; Hash functions; False negatives; Hash collisions; High-dimensional; Locality sensitive hash; Locality sensitive hashing; New constructions; Recall; Similarity search; Vector spaces
Randomized embeddings with slack and high-dimensional approximate nearest neighbor,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048991210&doi=10.1145%2f3178540&partnerID=40&md5=effa4ebde8a40b108aa9039c6f76b856,"Approximate nearest neighbor search (ϵ-ANN) in high dimensions has been mainly addressed by Locality Sensitive Hashing (LSH), which has complexity with polynomial dependence in dimension, sublinear query time, but subquadratic space requirement. We introduce a new “low-quality” embedding for metric spaces requiring that, for some query, there exists an approximate nearest neighbor among the pre-images of its k > 1 approximate nearest neighbors in the target space. In Euclidean spaces, we employ random projections to a dimension inversely proportional to k. Our approach extends to the decision problem with witness of checking whether there exists an approximate near neighbor; this also implies a solution for ϵ-ANN. After dimension reduction, we store points in a uniform grid of side length ϵ/√d', where d is the reduced dimension. Given a query, we explore cells intersecting the unit ball around the query. This data structure requires linear space and query time in O(dnρ ), ρ ≈ 1 − ϵ2/ log(1/ϵ), where n denotes input cardinality and d space dimension. Bounds are improved for doubling subsets via r-nets. We present our implementation for ϵ-ANN in C++ and experiments for d ≤ 960, n ≤ 106, using synthetic and real datasets, which confirm the theoretical analysis and, typically, yield better practical performance. We compare to FALCONN, the state-of-the-art implementation of multi-probe LSH: our prototype software is essentially comparable in terms of preprocessing, query time, and storage usage. © 2018 ACM.",Approximate nearest neighbor; Curse of dimensionality; Doubling dimension; Experimental study; Johnson-Lindenstrauss Lemma; Randomized embeddings,C++ (programming language); Digital storage; Image retrieval; Software prototyping; Curse of dimensionality; Doubling dimensions; Embeddings; Experimental study; Johnson-Lindenstrauss lemmata; Nearest neighbor search
Computing the gromov-hausdorff distance for metric trees,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052602785&doi=10.1145%2f3185466&partnerID=40&md5=3080ac62a61b475145402804c9c82943,"The Gromov-Hausdorff (GH) distance is a natural way to measure distance between two metric spaces. We prove that it is NP-hard to approximate the GH distance better than a factor of 3 for geodesic metrics on a pair of trees. We complement this result by providing a polynomial time O(min{n, rn})-approximation algorithm for computing the GH distance between a pair of metric trees, where r is the ratio of the longest edge length in both trees to the shortest edge length. For metric trees with unit length edges, this yields an O(n)-approximation algorithm.1                          © 2018 ACM.",Embeddings; Metric spaces,Approximation algorithms; Forestry; Polynomial approximation; Set theory; Topology; Edge length; Embeddings; Hausdorff distance; Longest edge; Measure distance; Metric spaces; Metric trees; Polynomial-time; Trees (mathematics)
Approximation algorithms for minimum-load k-facility location,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052604967&doi=10.1145%2f3173047&partnerID=40&md5=483c72133f1886fbcdf51cb47d4a2a3a,"We consider a facility-location problem that abstracts settings where the cost of serving the clients assigned to a facility is incurred by the facility. Formally, we consider the minimum-load k-facility location (MLkFL) problem, which is defined as follows. We have a set F of facilities, a set C of clients, and an integer k ≥ 0. Assigning client j to a facility f incurs a connection cost d(f , j). The goal is to open a set F ⊆ F of k facilities and assign each client j to a facility f (j) ∈ F so as to minimize maxf ∈F Σj ∈ C:f (j)=f d(f , j); we callΣ                             j ∈ C:f (j)=f d(f , j) the load of facility f . This problem was studied under the name of min-max star cover in References [3, 7], who (among other results) gave bicriteria approximation algorithms for MLkFL for when F = C. MLkFL is rather poorly understood, and only an O(k)-approximation is currently known for MLkFL, even for line metrics. Our main result is the first polytime approximation scheme (PTAS) for MLkFL on line metrics (note that no non-trivial true approximation of any kind was known for this metric). Complementing this, we prove that MLkFL is strongly NP-hard on line metrics. We also devise a quasi-PTAS for MLkFL on tree metrics. MLkFL turns out to be surprisingly challenging even on line metrics and resilient to attack by a variety of techniques that have been successfully applied to facility-location problems. For instance, we show that (a) even a configuration-style LP-relaxation has a bad integrality gap and (b) a multi-swap k-median style local-search heuristic has a bad locality gap. Thus, we need to devise various novel techniques to attack MLkFL. Our PTAS for line metrics consists of two main ingredients. First, we prove that there always exists a near-optimal solution possessing some nice structural properties. A novel aspect of this proof is that we first move to a mixed-integer LP (MILP) encoding of the problem and argue that a MILP-solution minimizing a certain. © 2018 ACM.",Approximation algorithms; Lower bound; Min-max star cover; Minimum load k-facility location; Polynomial time approximation scheme,Heuristic algorithms; Integer programming; Location; Polynomial approximation; Site selection; Stars; Bicriteria approximation; Facility location problem; Facility locations; Local search heuristics; Lower bounds; Min-max; Near-optimal solutions; Polynomial time approximation schemes; Approximation algorithms
Known algorithms on graphs of bounded treewidth are probably optimal,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052551415&doi=10.1145%2f3170442&partnerID=40&md5=da450b177c41e5e127e57d0f2bf134ea,"We obtain a number of lower bounds on the running time of algorithms solving problems on graphs of bounded treewidth. We prove the results under the Strong Exponential Time Hypothesis of Impagliazzo and Paturi. In particular, assuming that n-variablem-clause SAT cannot be solved in time (2 − ϵ)nmO(1), we show that for any ϵ > 0: • Independent Set cannot be solved in time (2 − ϵ)tw(G)|V(G)|O(1), • Dominating Set cannot be solved in time (3 − ϵ)tw(G)|V(G)|O(1), • Max Cut cannot be solved in time (2 − ϵ)tw(G)|V(G)|O(1), • Odd Cycle Transversal cannot be solved in time (3 − ϵ)tw(G)|V(G)|O(1), • For any fixed q ≥ 3, q-Coloring cannot be solved in time (q − ϵ)tw(G)|V(G)|O(1), • Partition Into Triangles cannot be solved in time (2 − ϵ)tw(G)|V(G)|O(1). Our lower bounds match the running times for the best known algorithms for the problems, up to the ϵ in the base. © 2018 ACM.",Lower bounds; SETH; Treewidth,Algorithms; Best-known algorithms; Bounded treewidth; Dominating sets; Lower bounds; Odd cycle transversals; SETH; Strong exponential time hypothesis; Tree-width; Mathematical techniques
Computing 2-walks in polynomial time,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052605526&doi=10.1145%2f3183368&partnerID=40&md5=e5227218d0756b508dc0c90dedeb2767,"A 2-walk of a graph is a walk visiting every vertex at least once and at most twice. By generalizing decompositions of Tutte and Thomassen, Gao, Richter, and Yu proved that every 3-connected planar graph contains a closed 2-walk such that all vertices visited twice are contained in 3-separators. This seminal result generalizes Tutte's theorem that every 4-connected planar graph is Hamiltonian, as well as Barnette's theorem that every 3-connected planar graph has a spanning tree with maximum degree at most 3. The algorithmic challenge of finding such a closed 2-walk is to overcome big overlapping subgraphs in the decomposition, which are also inherent in Tutte's and Thomassen's decompositions. We solve this problem by extending the decomposition of Gao, Richter, and Yu in such a way that all pieces into which the graph is decomposed are edge-disjoint. This implies the first polynomial-time algorithm that computes the closed 2-walk just mentioned. Its running time is O(n3). © 2018 ACM.",2-walks; 3-connected planar graphs; 3-trees; Algorithms; Data structures; Tutte paths,Algorithms; Computation theory; Data structures; Graphic methods; Polynomial approximation; Trees (mathematics); 2-walks; 3-connected planar graphs; 3-trees; Maximum degree; Polynomial-time; Polynomial-time algorithms; Spanning tree; Tutte path; Graph theory
Analyzing node-weighted oblivious matching problem via continuous LP with jump discontinuity,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052584929&doi=10.1145%2f3168008&partnerID=40&md5=ccddf54e8226a73ee3f4c5a649ff033c,"We prove the first non-trivial performance ratio strictly above 0.5 for the weighted Ranking algorithm on the oblivious matching problem where nodes in a general graph can have arbitrary weights. We have discovered a new structural property of the ranking algorithm: if a node has two unmatched neighbors, then it will still be matched even when its rank is demoted to the bottom. This property allows us to form LP constraints for both the weighted and the unweighted versions of the problem. Using a new class of continuous linear programming (LP), we prove that the ratio for the weighted case is at least 0.501512, and we improve the ratio for the unweighted case to 0.526823 (from the previous best 0.523166 in SODA 2014). Unlike previous continuous LP, in which the primal solution must be continuous everywhere, our new continuous LP framework allows the monotone component of the primal function to have jump discontinuities, and the other primal components to take non-conventional forms, such as the Dirac δ function. © 2018 ACM.",Linear programming; Oblivious algorithms; Ranking; Weighted matching,Delta functions; Continuous linear programming; Dirac delta function; Jump discontinuities; Matching problems; Oblivious algorithms; Ranking; Ranking algorithm; Weighted matching; Linear programming
Selection and sorting in the “restore” model,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052580446&doi=10.1145%2f3168005&partnerID=40&md5=39d62673881e8fd01633fd1026e67245,"We consider the classical selection and sorting problems in a model where the initial permutation of the input has to be restored after completing the computation. Such algorithms are useful for designing space-efficient algorithms, when one encounters subproblems that have to be solved by subroutines. It is important that these subroutines leave the array in its original state after they finish so that the computation can be properly resumed. Algorithms in this model can also be relevant for saving communication time, in case the data is distributed among several machines and would need to be copied to further machines for execution of the subroutine. Although the requirement of the restoration is stringent compared to the classical versions of the problems, this model is more relaxed than a read-only memory where the input elements are not allowed to be moved within the input array. We first show that for a sequence of n integers, selection (finding the median or more generally the k-th smallest element for a given k) can be done in O(n) time using O(lg n) words1 of extra space in this model. In contrast, no linear-time selection algorithm is known that uses polylogarithmic space in the read-only memory model. For sorting n integers in this model, we first present an O(n lg n)-time algorithm using O(lg n) words of extra space that outputs (in a write only tape) the given sequence in sorted order while restoring the order of the original input in the input tape. When the universe size U is polynomial in n, we give a faster O(n)-time algorithm (analogous to radix sort) that uses O(nε ) words of extra space for an arbitrarily small constant ε > 0. More generally, we show how to match the time bound of any word-RAM integer sorting algorithms using O(nε ) words of extra space. In sharp contrast, there is an Ω(n2/S)-time lower bound for integer sorting using O(S) bits of space in the read-only memory model. Extension of our results to arbitrary input types beyond integers is not possible: for “indivisible” input elements, we can prove the same Ω(n2/S) lower bound for sorting in our model. We also describe space-efficient algorithms to count the number of inversions in a given sequence in this model. En route, we develop linear-time in-place algorithms to extract leading bits of the input array and to compress and decompress strings with low entropy; these techniques may be of independent interest. © 2018 ACM.",In-place extraction; Read-only memory; Restore; Selection problem; Sorting integers,Random access storage; Restoration; ROM; Subroutines; Communication time; In-place algorithms; Number of inversions; Restore; Selection algorithm; Selection problems; Sorting algorithm; Space efficient algorithms; Sorting
The alternating stock size problem and the gasoline puzzle,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052550245&doi=10.1145%2f3178539&partnerID=40&md5=10e72472afb5ecc4b494a8b2665d5534,"Given a set S of integers whose sum is zero, consider the problem of finding a permutation of these integers such that (i) all prefix sums of the ordering are nonnegative and (ii) the maximum value of a prefix sum is minimized. Kellerer et al. call this problem the stock size problem and showed that it can be approximated to within 3/2. They also showed that an approximation ratio of 2 can be achieved via several simple algorithms. We consider a related problem, which we call the alternating stock size problem, where the numbers of positive and negative integers in the input set S are equal. The problem is the same as that shown earlier, but we are additionally required to alternate the positive and negative numbers in the output ordering. This problem also has several simple 2-approximations. We show that it can be approximated to within 1.79. Then we show that this problem is closely related to an optimization version of the gasoline puzzle due to Lovász, in which we want to minimize the size of the gas tank necessary to go around the track. We present a 2-approximation for this problem, using a natural linear programming relaxation whose feasible solutions are doubly stochastic matrices. Our novel rounding algorithm is based on a transformation that yields another doubly stochastic matrix with special properties, from which we can extract a suitable permutation. © 2018 ACM.",Approximation algorithms; Gasoline puzzle; Stock size problem,Approximation algorithms; Linear programming; Linear transformations; Matrix algebra; Stochastic systems; Approximation ratios; Doubly stochastic; Doubly stochastic matrix; Feasible solution; Linear programming relaxation; Rounding algorithm; Special properties; Stock size; Gasoline
Exact algorithms for terrain guarding,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052570934&doi=10.1145%2f3186897&partnerID=40&md5=c03e64f5284f384f886cc8ccf005728f,"Given a 1.5-dimensional terrain T, also known as an x-monotone polygonal chain, the Terrain Guarding problem seeks a set of points of minimum size onT that guards all of the points onT. Here, we say that a point p guards a point q if no point of the line segment pq is strictly below T. The Terrain Guarding problem has been extensively studied for over 20 years. In 2005 it was already established that this problem admits a constant-factor approximation algorithm (SODA 2005). However, only in 2010 King and Krohn (SODA 2010) finally showed that Terrain Guarding is NP-hard. In spite of the remarkable developments in approximation algorithms for Terrain Guarding, next to nothing is known about its parameterized complexity. In particular, the most intriguing open questions in this direction ask whether, if parameterized by the size k of a solution guard set, it admits a subexponential-time algorithm and whether it is fixed-parameter tractable. In this article, we answer the first question affirmatively by developing an nO(k)-time algorithm for both Discrete Terrain Guarding and Continuous Terrain Guarding. We also make non-trivial progress with respect to the second question: we show that Discrete Orthogonal Terrain Guarding, a well-studied special case of Terrain Guarding, is fixed-parameter tractable. © 2018 ACM.",Art gallery; Exponential-time algorithms; Terrain guarding,Landforms; Parameter estimation; Polynomials; Art gallery; Constant-factor approximation algorithms; Exact algorithms; Exponential time algorithm; Parameterized complexity; Polygonal chains; Subexponential time; Terrain guarding; Approximation algorithms
Perfect phylogenies via branchings in acyclic digraphs and a generalization of Dilworth's theorem,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052602691&doi=10.1145%2f3182178&partnerID=40&md5=ceec597b65645faa58a063c814c1b52d,"Motivated by applications in cancer genomics and following the work of Hajirasouliha and Raphael (WABI 2014), Hujdurović et al. (IEEE TCBB, 2018) introduced the minimum conflict-free row split (MCRS) problem: split each row of a given binary matrix into a bitwise OR of a set of rows so that the resulting matrix corresponds to a perfect phylogeny and has the minimum possible number of rows among all matrices with this property. Hajirasouliha and Raphael also proposed the study of a similar problem, in which the task is to minimize the number of distinct rows of the resulting matrix. Hujdurović et al. proved that both problems are NP-hard, gave a related characterization of transitively orientable graphs, and proposed a polynomial-time heuristic algorithm for the MCRS problem based on coloring cocomparability graphs. We give new, more transparent formulations of the two problems, showing that the problems are equivalent to two optimization problems on branchings in a derived directed acyclic graph. Building on these formulations, we obtain new results on the two problems, including (1) a strengthening of the heuristic by Hujdurović et al. via a new min-max result in digraphs generalizing Dilworth's theorem, which may be of independent interest; (2) APX-hardness results for both problems; (3) approximation algorithms; and (4) exponential-time algorithms solving the two problems to optimality faster than the naïve brute-force approach. Our work relates to several well-studied notions in combinatorial optimization: chain partitions in partially ordered sets, laminar hypergraphs, and (classical and weighted) colorings of graphs. © 2018 ACM.",Acyclic digraph; Approximation algorithm; APX-hardness; Branching; Chain partition; Dilworth's theorem; Min-max theorem; Minimum conflict-free row split problem; Perfect phylogeny,Approximation algorithms; Biology; Chains; Combinatorial optimization; Directed graphs; Graph theory; Hardness; Heuristic algorithms; Matrix algebra; Optimization; Polynomial approximation; Set theory; Acyclic digraph; APX-Hardness; Branching; Conflict free; Dilworth's theorem; Min-max theorem; Perfect phylogeny; Problem solving
Tight space bounds for two-dimensional approximate range counting,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052578851&doi=10.1145%2f3205454&partnerID=40&md5=dcbe7da219584a11f715341810e3a9be,"We study the problem of two-dimensional orthogonal range counting with additive error. Given a set P of n points drawn from an n × n grid and an error parameter ε, the goal is to build a data structure, such that for any orthogonal range R, it can return the number of points in P ∩ R with additive error εn. A well-known solution for this problem is obtained by using ε-approximation, which is a subset A ⊆ P that can estimate the number of points in P ∩ R with the number of points in A ∩ R. It is known that an ε-approximation of size O(ε                             1 log2.5                             ε                             1 ) exists for any P with respect to orthogonal ranges, and the best lower bound is Ω(ε                             1 log ε                             1 ). The ε-approximation is a rather restricted data structure, as we are not allowed to store any information other than the coordinates of the points. In this article, we explore what can be achieved without any restriction on the data structure. We first describe a simple data structure that uses O(ε                             1 (log2                             ε                             1 + log n)) bits and answers queries with error εn. We then prove a lower bound that any data structure that answers queries with error εn will have to use Ω(ε                             1 (log2                             ε                             1 + log n)) bits. Our lower bound is information-theoretic: We show that there is a collection of 2Ω(n log n) point sets with large union combinatorial discrepancy and thus are hard to distinguish unless we use Ω(n log n) bits. © 2018 ACM.",Data structures; Discrepancy theory; Range counting,Errors; Information theory; Additive errors; Discrepancy theory; Error parameters; Lower bounds; Point set; Range counting; Space bounds; Data structures
Efficient computation of middle levels gray codes,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049084102&doi=10.1145%2f3170443&partnerID=40&md5=f8f95be872aa578ea09d1405de1e6f48,"For any integer n ≥ 1, a middle levels Gray code is a cyclic listing of all bitstrings of length 2n + 1 that have either n or n + 1 entries equal to 1 such that any two consecutive bitstrings in the list differ in exactly one bit. The question whether such a Gray code exists for every n ≥ 1 has been the subject of intensive research during the past 30 years and has been answered affirmatively only recently [T. Mütze. Proof of the middle levels conjecture. Proc. London Math. Soc., 112(4):677-713, 2016]. In this work, we provide the first efficient algorithm to compute a middle levels Gray code. For a given bitstring, our algorithm computes the next bitstrings in the Gray code in time O(n(1 +n )), which is O(n) on average per bitstring provided that = Ω(n). © 2018 ACM.",Gray code; Middle levels conjecture,Algorithms; Mathematical techniques; Bit-strings; Efficient computation; Gray codes; Integer-N; Intensive research; Middle levels conjecture; Codes (symbols)
Distributed online and stochastic queueing on a multiple access channel,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052575218&doi=10.1145%2f3182396&partnerID=40&md5=84727479d1e27f7cd07bc54d87f4dce7,"We consider the problems of online and stochastic packet queueing in a distributed system of n nodes with queues, where the communication between the nodes is done via a multiple access channel. In the online setting, in each round, an arbitrary number of packets can be injected to nodes' queues. Two measures of performance are considered: the total number of packets in all queues, called the total load, and the maximum queue size, called the maximum load. We develop a deterministic distributed algorithm that is asymptotically 1 optimal with respect to both complexity measures, in a competitive way. More precisely, the total load of our algorithm is bigger than the total load of any other algorithm, including centralized online solutions, by only an additive term of O(n2), whereas the maximum queue size of our algorithm is at most n times bigger than the maximum queue size of any other algorithm, with an extra additive O(n). The optimality for both measures is justified by proving the corresponding lower bounds, which also separates nearly exponentially distributed solutions from the centralized ones. Next, we show that our algorithm is also stochastically stable for any expected injection rate smaller or equal to 1. This is the first solution to the stochastic queueing problem on a multiple access channel that achieves such stability for the (highest possible) rate equal to 1. © 2018 ACM.",Contention resolution; Distributed algorithms; Multiple access channel; Online algorithms; Shared channel; Stability; Stochastic queueing,Convergence of numerical methods; Parallel algorithms; Stochastic systems; Contention resolution; Multiple access channels; On-line algorithms; Shared channel; Stochastic queueing; Queueing theory
Deterministic Truncation of Linear Matroids,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044003841&doi=10.1145%2f3170444&partnerID=40&md5=f799fb8df5f6111b40684cfc54ad3297,"Let M = (E, I) be a matroid of rank n. A k-truncation of M is a matroid M = (E, I) such that for any A ⊆ E, A ∊ ∊ Í if and only if |A| = k and A - I. Given a linear representation, A, of M, we consider the problem of finding a linear representation, Ak, of the k-truncation of M. A common way to compute Ak is to multiply the matrix A with a random k × n matrix, yielding a simple randomized algorithm. Thus, a natural question is whether we can compute Ak deterministically. In this article, we settle this question for matrices over any field in which the field operations can be done efficiently. This includes any finite field and the field of rational numbers (Q). Our algorithms are based on the properties of the classical Wronskian determinant, and the folded Wronskian determinant, which was recently introduced by Guruswami and Kopparty [23, 24] and Forbes and Shpilka [14]. Our main conceptual contribution in this article is to show that the Wronskian determinant can also be used to obtain a representation of the truncation of a linear matroid in deterministic polynomial time. An important application of our result is a deterministic algorithm to compute representative sets over linear matroids, which derandomizes a result of Fomin et al. [11, 12]. This result derandomizes several parameterized algorithms, including an algorithm for-Matroid Parity to which several problems, such as-Matroid Intersection, can be reduced. © 2018 ACM. All Rights Reserved.",Matroid parity; Matroid truncation; Representative sets; Wronskian matrix,Combinatorial mathematics; Differential equations; Polynomial approximation; Deterministic algorithms; Field operation; Linear representation; Matroid intersection; Parameterized algorithm; Randomized Algorithms; Rational numbers; Representative sets; Matrix algebra
Incremental exact min-cut in polylogarithmic amortized update time,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043995266&doi=10.1145%2f3174803&partnerID=40&md5=bf0ad5588437295893e3f7df8f015b7a,"We present a deterministic incremental algorithm for exactly maintaining the size of a minimum cut with O(log3 n log log2 n) amortized time per edge insertion and O(1) query time. This result partially answers an open question posed by Thorup (2007). It also stays in sharp contrast to a polynomial conditional lower bound for the fully dynamic weighted minimum cut problem. Our algorithm is obtained by combining a sparsification technique of Kawarabayashi and Thorup (2015) or its recent improvement by Henzinger, Rao, and Wang (2017), and an exact incremental algorithm of Henzinger (1997). We also study space-efficient incremental algorithms for the minimum cut problem. Concretely, we show that there exists an O(n logn/e2) space Monte Carlo algorithm that can process a stream of edge insertions starting from an empty graph, and with high probability, the algorithm maintains a (1 + e )-approximation to the minimum cut. The algorithm has O((a (n) log3 n)/e2) amortized update time and constant query time, where a (n) stands for the inverse of Ackermann function. © 2018 ACM.",Edge connectivity; Minimum cut; Space-efficient dynamic graph algorithms,Algorithms; Mathematical techniques; Ackermann functions; Dynamic graph algorithms; Edge connectivity; High probability; Incremental algorithm; Minimum cut; Monte carlo algorithms; Space efficient; Approximation algorithms
Efficient algorithms for constructing very sparse spanners and emulators,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058350417&doi=10.1145%2f3274651&partnerID=40&md5=47b7aa85c0038d1c5f05b8c2c7c67008,"Miller et al. [48] devised a distributed1 algorithm in the CONGEST model that, given a parameter k = 1, 2, . . . , constructs an O(k)-spanner of an input unweighted n-vertex graph with O(n1+1/k ) expected edges in O(k) rounds of communication. In this article, we improve the result of Reference [48] by showing a k-round distributed algorithm in the same model that constructs a (2k - 1)-spanner with O(n1+1/k /∈ ) edges, with probability 1 - ∈ for any ∈ > 0. Moreover, when k = ω(logn), our algorithm produces (still in k rounds) ultrasparse spanners, i.e., spanners of size n(1 + o(1)), with probability 1 - o(1). To our knowledge, this is the first distributed algorithm in the CONGEST or in the PRAM models that constructs spanners or skeletons (i.e., connected spanning subgraphs) that are sparse. Our algorithm can also be implemented in linear time in the standard centralized model, and for large k, it provides spanners that are sparser than any other spanner given by a known (near-)linear time algorithm. We also devise improved bounds (and algorithms realizing these bounds) for (1 + ∈, β)-spanners and emulators. In particular, we show that for any unweighted n-vertex graph and any ∈ > 0, there exists a (1 + ∈, ( log log n/∈)log log n )-emulator withO(n) edges. All previous constructions of (1 + ∈, β)-spanners and emulators employ a superlinear number of edges for all choices of parameters. Finally, we provide some applications of our results to approximate shortest paths' computation in unweighted graphs. © 2018 Association for Computing Machinery.",CONGEST; Shortest path; Spanners,Clustering algorithms; Centralized models; CONGEST; Linear-time algorithms; N-vertex graph; Shortest path; Spanners; Spanning subgraphs; Unweighted graphs; Graph theory
Subquadratic kernels for implicit 3-hitting set and 3-set packing problems,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061188865&doi=10.1145%2f3293466&partnerID=40&md5=036f0036a6bbfaac1de672bf6616e8d5,"We consider four well-studied NP-complete packing/covering problems on graphs: Feedback Vertex Set in Tournaments (FVST), Cluster Vertex Deletion (CVD), Triangle Packing in Tournaments (TPT) and Induced P3-Packing. For these four problems, kernels with O(k2) vertices have been known for a long time. In fact, such kernels can be obtained by interpreting these problems as finding either a packing of k pairwise disjoint sets of size 3 (3-Set Packing) or a hitting set of size at most k for a family of sets of size at most 3 (3-Hitting Set). In this article, we give the first kernels for FVST, CVD, TPT, and Induced P3-Packing with a subquadratic number of vertices. Specifically, we obtain the following results. • FVST admits a kernel with O(k 32 ) vertices. • CVD admits a kernel with O(k 53 ) vertices. • TPT admits a kernel with O(k 32 ) vertices. • Induced P3-Packing admits a kernel with O(k 53 ) vertices. Our results resolve an open problem from WorKer 2010 on the existence of kernels with O(k2-ϵ ) vertices for FVST and CVD. All of our results are based on novel uses of old and new ""expansion lemmas"" and a weak form of crown decomposition where (i) almost all of the head is used by the solution (as opposed to all), (ii) almost none of the crown is used by the solution (as opposed to none), and (iii) if H is removed from G, then there is almost no interaction between the head and the rest (as opposed to no interaction at all). © 2019 Association for Computing Machinery.",Implicit 3-Hitting Set; Implicit 3-Set Packing; Kernelization; parameterized complexity,Algorithms; Disjoint sets; Feedback vertex set; Hitting sets; Implicit 3-Hitting Set; Kernelization; NP Complete; Packing problems; Parameterized complexity; Mathematical techniques
An O(log k)-competitive algorithm for generalized caching,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058288309&doi=10.1145%2f3280826&partnerID=40&md5=c6b23d8d4ca436967ed86347505566c8,"In the generalized caching problem, we have a set of pages and a cache of size k. Each page p has a size wp ≥ 1 and fetching cost cp for loading the page into the cache. At any point in time, the sum of the sizes of the pages stored in the cache cannot exceed k. The input consists of a sequence of page requests. If a page is not present in the cache at the time it is requested, it has to be loaded into the cache, incurring a cost of cp . We give a randomizedO(log k)-competitive online algorithm for the generalized caching problem, improving the previous bound ofO(log2 k) by Bansal, Buchbinder, and Naor (STOC'08). This improved bound is tight and of the same order as the known bounds for the classic paging problem with uniform weights and sizes. We use the same LP-based techniques as Bansal et al. but provide improved and slightly simplified methods for rounding fractional solutions online. © 2018 Association for Computing Machinery.",Knapsack cover inequalities; Online primal dual,Algorithms; Mathematical techniques; Competitive algorithms; Fractional solutions; Knapsack cover inequalities; On-line algorithms; Online primal duals; Simplified method; Cache memory
Deterministic graph exploration with advice,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058304277&doi=10.1145%2f3280823&partnerID=40&md5=1262b7aa31c3e3998fa38ebb91cddd86,"We consider the fundamental task of graph exploration. An n-node graph has unlabeled nodes, and all ports at any node of degree d are arbitrarily numbered 0, . . . ,d - 1. A mobile agent, initially situated at some starting node v, has to visit all nodes and stop. The time of the exploration is the number of edge traversals. We consider the problem of how much knowledge the agent has to have a priori, to explore the graph in a given time, using a deterministic algorithm. Following the paradigm of algorithms with advice, this a priori information (advice) is provided to the agent by an oracle, in the form of a binary string, whose length is called the size of advice. We consider two types of oracles. The instance oracle knows the entire instance of the exploration problem, i.e., the port-numbered map of the graph and the starting node of the agent in this map. The map oracle knows the port-numbered map of the graph but does not know the starting node of the agent. What is the minimum size of advice that must be given to the agent by each of these oracles, so that the agent explores the graph in a given time? We first determine the minimum size of advice to achieve exploration in polynomial time. We prove that some advice of size log log logn - c, for any constant c, is sufficient for polynomial exploration, and that no advice of size log log logn - Φ(n), where Φ is any function diverging to infinity, can help to do this. These results hold both for the instance and for the map oracles. On the other side of the spectrum, when advice is large, there are two natural time thresholds: θ(n2) for a map oracle, and θ(n) for an instance oracle. This is because, in both cases, these time benchmarks can be achieved with sufficiently large advice (advice of size O(n logn) suffices). We show that, with a map oracle, time θ(n2) cannot be improved in general, regardless of the size of advice. What is then the smallest advice to achieve time θ(nθ) with a map oracle? We show that this smallest size of advice is larger than nδ , for any δ < 1/3. For large advice, the situation changes significantly when we allow an instance oracle instead of a map oracle. In this case, advice of size O(n logn) is enough to achieve time O(n). Is such a large advice needed to achieve linear time? We answer this question affirmatively. Indeed, we show more: with any advice of size o(n logn), the time of exploration must be at least n∈ , for any ∈ < 2, and with any advice of size O(n), the time must be Ω(n2). We finally look at Hamiltonian graphs, as for them it is possible to achieve the absolutely optimal exploration time n - 1, when sufficiently large advice (of size O(n logn)) is given by an instance oracle. We show that a map oracle cannot achieve this: regardless of the size of advice, the time of exploration must be Ω(n2), for some Hamiltonian graphs. However, even for the instance oracle, with advice of size o(n logn), optimal time n - 1 cannot be achieved: Indeed, we show that the time of exploration with such advice must sometimes exceed the optimal time n - 1 by a summand n∈ , for any ∈ < 1. © 2018 Association for Computing Machinery.",Advice; Algorithm; Exploration; Graph; Mobile agent,Algorithms; Hamiltonians; Mobile agents; Natural resources exploration; Polynomial approximation; Advice; Deterministic algorithms; Edge traversal; Graph; Graph exploration; Hamiltonian graph; Polynomial-time; Priori information; Graph theory
The simplex Algorithm Is NP-mighty,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058329326&doi=10.1145%2f3280847&partnerID=40&md5=3dd698e173c279ab614c51950f0f1129,"We show that the Simplex Method, the Network Simplex Method-both with Dantzig's original pivot rule- and the Successive Shortest Path Algorithm are NP-mighty. That is, each of these algorithms can be used to solve, with polynomial overhead, any problem in NP implicitly during the algorithm's execution. This result casts a more favorable light on these algorithms' exponential worst-case running times. Furthermore, as a consequence of our approach, we obtain several novel hardness results. For example, for a given input to the Simplex Algorithm, deciding whether a given variable ever enters the basis during the algorithm's execution and determining the number of iterations needed are both NP-hard problems. Finally, we close a long-standing open problem in the area of network flows over time by showing that earliest arrival flows are NP-hard to obtain. © 2018 Association for Computing Machinery.",Earliest arrival flows; Network simplex; NP-mightiness; Simplex algorithm; Successive shortest paths,Computational complexity; Graph theory; Earliest arrival; Hardness result; Network flows; Number of iterations; Shortest path; Shortest path algorithms; Simplex algorithm; Simplex methods; Linear programming
Clique-width III: Hamiltonian cycle and the odd case of graph coloring,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058268764&doi=10.1145%2f3280824&partnerID=40&md5=47bc8352e58170a95db1bc67a7c7028e,"Max-Cut, Edge Dominating Set, Graph Coloring, and Hamiltonian Cycle on graphs of bounded cliquewidth have received significant attention as they can be formulated inMSO2 (and, therefore, have linear-time algorithms on bounded treewidth graphs by the celebrated Courcelle's theorem), but cannot be formulated in MSO1 (which would have yielded linear-time algorithms on bounded clique-width graphs by a well-known theorem of Courcelle, Makowsky, and Rotics). Each of these problems can be solved in time g(k)nf(k) on graphs of clique-width k. Fomin et al. (2010) showed that the running times cannot be improved to g(k)nO(1) assuming W[1]≠FPT. However, this does not rule out non-trivial improvements to the exponent f (k) in the running times. In a follow-up paper, Fomin et al. (2014) improved the running times for EdgeDominating Set and Max-Cut to n O(k) , and proved that these problems cannot be solved in time g(k)no(k) unless ETH fails. Thus, prior to this work, Edge Dominating Set and Max-Cut were known to have tight nθ(k) algorithmic upper and lower bounds. In this article, we provide lower bounds for Hamiltonian Cycle and Graph Coloring. For Hamiltonian Cycle, our lower bound g(k)no(k) matches asymptotically the recent upper bound nO(k) due to Bergougnoux, Kanté, and Kwon (2017). As opposed to the asymptotically tight nθ(k) bounds for Edge Dominating Set, Max-Cut, and Hamiltonian Cycle, the Graph Coloring problem has an upper bound of nO(2k ) and a lower bound of merely no( 4vk) (implicit from the W[1]-hardness proof). In this article, we close the gap for Graph Coloring by proving a lower bound of n2 o(k). This shows that Graph Coloring behaves qualitatively different from the other three problems. To the best of our knowledge, Graph Coloring is the first natural problem known to require exponential dependence on the parameter in the exponent of n. © 2018 Association for Computing Machinery.",,Clustering algorithms; Coloring; Graphic methods; Hamiltonians; Bounded-treewidth graphs; Courcelle's theorems; Edge dominating set; Exponential dependence; Graph coloring problem; Hamiltonian cycle; Linear-time algorithms; Upper and lower bounds; Graph theory
Improved analysis of deterministic load-balancing schemes,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058351525&doi=10.1145%2f3282435&partnerID=40&md5=5280c20b947c150149d1e07fd43b9993,"We consider the problem of deterministic load balancing of tokens in the discrete model. A set of n processors is connected into a d-regular undirected network. In every timestep, each processor exchanges some of its tokens with each of its neighbors in the network. The goal is tominimize the discrepancy between the number of tokens on the most-loaded and the least-loaded processor as quickly as possible. In this work, we identify some natural conditions on deterministic load-balancing algorithms to improve upon the long-standing results of Rabani et al. (1998). Specifically,we introduce the notion of cumulatively fair load-balancing algorithms where in any interval of consecutive timesteps, the total number of tokens sent out over an edge by a node is the same (up to constants) for all adjacent edges.We prove that algorithms that are cumulatively fair and where every node retains a sufficient part of its load in each step, achieve a discrepancy of O(d min{logn/μ,vn}) in time O(T ), where μ is the spectral gap of the transition matrix of the graph.We also show that, in general, neither of these assumptions may be omitted without increasing discrepancy. We then show, by a combinatorial potential reduction argument, that any cumulatively fair scheme satisfying some additional assumptions achieves a discrepancy of O(d) almost as quickly as the continuous diffusion process. This positive result applies to some of the simplest and most natural discrete load balancing schemes. © 2018 Copyright is held by the owner/author(s).",Deterministic load-balancing; Mixing time; Networks,Algorithms; Mathematical techniques; Networks (circuits); Discrete modeling; Load balancing algorithms; Load-balancing schemes; Mixing time; Natural conditions; Potential reduction; Transition matrices; Undirected network; Mixer circuits
Entropy and optimal compression of some general plane trees,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056774240&doi=10.1145%2f3275444&partnerID=40&md5=8e3c9f9a39d53bc641888f010833090e,"We continue developing the information theory of structured data. In this article, we study models generating d-ary trees (d ≥ 2) and trees with unrestricted degree. We first compute the entropy which gives us the fundamental lower bound on compression of such trees. Then we present efficient compression algorithms based on arithmetic encoding that achieve the entropy within a constant number of bits. A naïve implementation of these algorithms has a prohibitive time complexity of O(nd) elementary arithmetic operations (each corresponding to a number f (n,d) of bit operations), but our efficient algorithms run in O(n2) of these operations, where n is the number of nodes. It turns out that extending source coding (i.e., compression) from sequences to advanced data structures such as degree-unconstrained trees is mathematically quite challenging and leads to recurrences that find ample applications in the information theory of general structures (e.g., to analyze the information content of degree-unconstrained non-plane trees). © 2018 Association for Computing Machinery.",Arithmetic coding; Plane recursive trees; Random trees,Entropy; Information theory; Arithmetic Coding; Arithmetic operations; Compression algorithms; General structures; Information contents; Optimal compression; Plane recursive trees; Random tree; Trees (mathematics)
The complexity of independent set reconfiguration on bipartite graphs,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056798239&doi=10.1145%2f3280825&partnerID=40&md5=9f8323cb3c34038eac5374942c72d6e5,"We settle the complexity of the Independent Set Reconfiguration problem on bipartite graphs under all three commonly studied reconfiguration models. We show that under the token jumping or token addition/ removal model, the problem is NP-complete. For the token sliding model, we show that the problem remains PSPACE-complete. © 2018 Association for Computing Machinery.",Bipartite graphs; Independent set; Reconfiguration; Solution space; Vertex cover,Algorithms; Mathematical techniques; Bipartite graphs; Independent set; Reconfiguration; Solution space; Vertex cover; Graph theory
An optimal algorithm for1-heavy hitters in insertion streams and related problems,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056775275&doi=10.1145%2f3264427&partnerID=40&md5=8966794b66af3da39d54bc03c6e6747d,"We give the first optimal bounds for returning the1-heavy hitters in a data stream of insertions, together with their approximate frequencies, closing a long line of work on this problem. For a stream of m items in {1, 2, . . ., n} and parameters 0 < ε < φ 1, let fi denote the frequency of item i, i.e., the number of times item i occurs in the stream. With arbitrarily large constant probability, our algorithm returns all items i for which fi φm, returns no items j for which fj (φ − ε)m, and returns approximations fi with | fi − fi | εm for each item i that it returns. Our algorithm uses O(ε−1 log φ−1 + φ−1 log n + log log m) bits of space, processes each stream update in O(1) worst-case time, and can report its output in time linear in the output size. We also prove a lower bound, which implies that our algorithm is optimal up to a constant factor in its space complexity. A modification of our algorithm can be used to estimate the maximum frequency up to an additive εm error in the above amount of space, resolving Question 3 in the IITK 2006 Workshop on Algorithms for Data Streams for the case of1-heavy hitters. We also introduce several variants of the heavy hitters and maximum frequency problems, inspired by rank aggregation and voting schemes, and show how our techniques can be applied in such settings. Unlike the traditional heavy hitters problem, some of these variants look at comparisons between items rather than numerical values to determine the frequency of an item. © 2018 Association for Computing Machinery.",Heavy hitters; Streaming algorithms; Voting; Winner determination,Algorithms; Mathematical techniques; Constant factors; Heavy-hitter; Maximum frequency; Numerical values; Optimal algorithm; Streaming algorithm; Voting; Winner determination; Approximation algorithms
Distribution-free Junta Testing,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054205765&doi=10.1145%2f3264434&partnerID=40&md5=2350e83ce2d091e795a07403822257cc,"We study the problem of testing whether an unknown n-variable Boolean function is a k-junta in the distribution-free property testing model, where the distance between functions is measured with respect to an arbitrary and unknown probability distribution over {0, 1}n. Our first main result is that distribution-free k-junta testing can be performed, with one-sided error, by an adaptive algorithm that uses Õ(k2)/ϵ queries (independent of n). Complementing this, our second main result is a lower bound showing that any non-adaptive distribution-free k-junta testing algorithm must make Ω(2k/3) queries even to test to accuracy ϵ = 1/3. These bounds establish that while the optimal query complexity of non-adaptive k-junta testing is 2Θ(k), for adaptive testing it is poly(k), and thus show that adaptivity provides an exponential improvement in the distribution-free query complexity of testing juntas. © 2018 Association for Computing Machinery.",Distribution-free model; Property testing,Adaptive algorithms; Boolean functions; Adaptive testing; Distribution-free; Distribution-free property testing; Lower bounds; Optimal query; Property-testing; Query complexity; Testing algorithm; Probability distributions
Scaling algorithms for weighted matching in general graphs,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042531911&doi=10.1145%2f3155301&partnerID=40&md5=e5ed2d6340221515b6034311d2f6ba6f,"We present a new scaling algorithm for maximum (or minimum) weight perfect matching on general, edge weighted graphs. Our algorithm runs inO(mnlog(nN)) time,O(mn) per scale, which matches the running time of the best cardinality matching algorithms on sparse graphs [16, 20, 36, 37]. Here, m,n, and N bound the number of edges, vertices, and magnitude, respectively, of any integer edge weight. Our result improves on a 25-year-old algorithm of Gabow and Tarjan, which runs in O(mnlognα(m,n) log(nN)) time. © 2018 ACM.",Matching polytope; Non-bipartite graphs; Scaling algorithm,Graphic methods; Bipartite graphs; Cardinality matching; Edge-weighted graph; General graph; Matching polytope; Perfect matchings; Scaling algorithm; Weighted matching; Graph theory
Fault-Tolerant approximate BFS structures,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042526306&doi=10.1145%2f3022730&partnerID=40&md5=1574e6cbd3fad307de92bd442964f0e8,"A fault-tolerant structure for a network is required to continue functioning following the failure of some of the network’s edges or vertices. This article addresses the problem of designing a fault-tolerant (α, β) approximate BFS structure (or FT-ABFS structure for short), namely, a subgraph H of the network G such that subsequent to the failure of some subset F of edges or vertices, the surviving part of H (namely, H \ F) still contains an approximate BFS spanning tree for (the surviving part of) G, satisfying dist(s, v, H \ F ) ≤ α · dist(s, v, G \ F ) + β for every v ∈ V. Our first result is an algorithm that given an n-vertex unweighted undirected graph G and a source s constructs a multiplicative (3, 0) FT-ABFS structure rooted at s resilient to a failure of a single edge with at most 4n edges (improving by an O(log n) factor on the near-tight result of Baswana and Khanna (2010) for the special case of edge failures). This was recently improved to 2n edges by Bilò et al. (2014). Next, we consider the multiple edge faults case, for a constant integer f > 1, we prove that there exists a (polynomial-time constructible) (3 f , f log n) FT-ABFS structure with O(f n) edges that is resilient against f faults. We also show the existence of a (3 f + 1, 0) FT-ABFS structure with O(f logf n · n) edges. We then consider additive (1, β) FT-ABFS structures and demonstrate an interesting dichotomy between multiplicative and additive spanners. In contrast to the linear size of (α, 0) FT-ABFS structures, we show that for every n, there exist δ, ϵ > 0, and n-vertex graphs G with a source s for which any (1, nδ ) FT-ABFS structure rooted at s has Ω (n7/6−ϵ ) edges. For the case of additive stretch 3, we show that (1, 3) FT-ABFS structures admit a lower bound of Ω (n5/4 ) edges. © 2018 ACM.",Additive spanners; Fault tolerance; Replacement paths,Fault tolerance; Polynomial approximation; Trees (mathematics); Additive spanners; Fault-tolerant; Fault-tolerant structures; N-vertex graph; Polynomial-time; Replacement paths; Spanning tree; Undirected graph; Graph theory
Reducing curse of dimensionality: Improved PTAS for TSP (with neighborhoods) in doubling metrics,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042521255&doi=10.1145%2f3158232&partnerID=40&md5=a6ece53d87b6c6bf546ad1940d2c7580,"We consider the Traveling Salesman Problem with Neighborhoods (TSPN) in doubling metrics. The goal is to find the shortest tour that visits each of a given collection of subsets (regions or neighborhoods) in the underlying metric space. We give a randomized polynomial-time approximation scheme (PTAS) when the regions are fat weakly disjoint. This notion of regions was first defined when a QPTAS was given for the problem in SODA 2010 (Chan and Elbassioni 2010). The regions are partitioned into a constant number of groups, where in each group, regions should have a common upper bound on their diameters and each region designates one point within it such that these points are far away from one another. We combine the techniques in the previous work, together with the recent PTAS for TSP (STOC 2012: Bartal, Gottlieb, and Krauthgamer 2012) to achieve a PTAS for TSPN. However, several nontrivial technical hurdles need to be overcome for applying the PTAS framework to TSPN: (1) Heuristic to detect sparse instances. In the STOC 2012 paper, a minimum spanning tree heuristic is used to estimate the portion of an optimal tour within some ball. However, for TSPN, it is not known if an optimal tour would use points inside the ball to visit regions that intersect the ball. (2) Partially cut regions in the recursion. After a sparse ball is identified by the heuristic, the PTAS framework for TSP uses dynamic programming to solve the instance restricted to the sparse ball and recurse on the remaining instance. However, for TSPN, it is an important issue to decide whether each region partially intersecting the sparse ball should be solved in the sparse instance or considered in the remaining instance. Surprisingly, we show that both issues can be resolved by conservatively making the ball in question responsible for all intersecting regions. In particular, a sophisticated charging argument is needed to bound the cost of combining tours in the recursion. Moreover, more refined procedures are used to improve the dependence of the running time on the doubling dimension k from the previous exp[(O(1))k2] (even for just TSP) to exp[2O(k log k)]. © 2018 ACM.",Approximation scheme; Doubling dimension; Metric space; Traveling salesman problem with neighborhoods,Approximation theory; Dynamic programming; Heuristic programming; Optimization; Polynomial approximation; Set theory; Topology; Approximation scheme; Curse of dimensionality; Doubling dimensions; Doubling metrics; Metric spaces; Minimum spanning trees; Optimal tours; Polynomial time approximation schemes; Traveling salesman problem
Linear time parameterized algorithms for subset feedback vertex set,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042531896&doi=10.1145%2f3155299&partnerID=40&md5=53ab99ecc4c8c170ad73fdcee136a6bd,"In the Subset Feedback Vertex Set (Subset FVS) problem, the input is a graph G on n vertices and m edges, a subset of vertices T, referred to as terminals, and an integer k. The objective is to determine whether there exists a set of at most k vertices intersecting every cycle that contains a terminal. The study of parameterized algorithms for this generalization of the Feedback Vertex Set problem has received significant attention over the past few years. In fact, the parameterized complexity of this problem was open until 2011, when two groups independently showed that the problem is fixed parameter tractable. Using tools from graph minors„ Kawarabayashi and Kobayashi obtained an algorithm for Subset FVS running in time O(f (k) · n2m) [SODA 2012, JCTB 2012]. Independently, Cygan et al. [ICALP 2011, SIDMA 2013] designed an algorithm for Subset FVS running in time 2O(k log k) · nO(1). More recently, Wahlström obtained the first single exponential time algorithm for Subset FVS, running in time 4k · nO(1) [SODA 2014]. While the 2O(k) dependence on the parameter k is optimal under the Exponential Time Hypothesis, the dependence of this algorithm as well as those preceding it, on the input size is at least quadratic. In this article, we design the first linear time parameterized algorithms for Subset FVS. More precisely, we obtain the following new algorithms for Subset FVS. —A randomized algorithm for Subset FVS running in time O(25.6k · (n + m)). —A deterministic algorithm for Subset FVS running in time 2O(k log k) · (n + m). Since it is known that assuming the Exponential Time Hypothesis, Subset FVS cannot have an algorithm running in time 2o(k)nO(1), our first algorithm obtains the best possible asymptotic dependence on both the parameter as well as the input size. Both of our algorithms are based on “cut centrality,” in the sense that solution vertices are likely to show up in minimum size cuts between vertices sampled from carefully chosen distributions. © 2018 ACM.",Feedback vertex set; Graph separation problems; Linear-time FPT algorithms,Parameter estimation; Parameterization; Set theory; Deterministic algorithms; Exponential time algorithm; Exponential time hypothesis; Feedback vertex set; Feedback Vertex Set problems; FPT algorithms; Parameterized complexity; Separation problems; Graph theory
Kernels for (Connected) dominating set on graphs with excluded topological minors,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042495249&doi=10.1145%2f3155298&partnerID=40&md5=8ed1aadd1509b18cef586189ad77b9c2,"We give the first linear kernels for the Dominating Set and Connected Dominating Set problems on graphs excluding a fixed graph H as a topological minor. In other words, we prove the existence of polynomial time algorithms that, for a given H-topological-minor-free graph G and a positive integer k, output an Htopological-minor-free graph G on O(k) vertices such that G has a (connected) dominating set of size k if and only if G has one. Our results extend the known classes of graphs on which the Dominating Set and Connected Dominating Set problems admit linear kernels. Prior to our work, it was known that these problems admit linear kernels on graphs excluding a fixed apex graph H as a minor. Moreover, for Dominating Set, a kernel of size kc(H), where c(H) is a constant depending on the size of H, follows from a more general result on the kernelization of Dominating Set on graphs of bounded degeneracy. Alon and Gutner explicitly asked whether one can obtain a linear kernel for Dominating Set on H-minor-free graphs. We answer this question in the affirmative and in fact prove a more general result. For Connected Dominating Set no polynomial kernel even on H-minor-free graphs was known prior to our work. On the negative side, it is known that Connected Dominating Set on 2-degenerated graphs does not admit a polynomial kernel unless coNP ⊆ NP/poly. Our kernelization algorithm is based on a non-trivial combination of the following ingredients • The structural theorem of Grohe and Marx [STOC 2012] for graphs excluding a fixed graph H as a topological minor; • A novel notion of protrusions, different than the one defined in [FOCS 2009]; • Our results are based on a generic reduction rule that produces an equivalent instance (in case the input graph is H-minor-free) of the problem, with treewidth O(k). The application of this rule in a divide-and-conquer fashion, together with the new notion of protrusions, gives us the linear kernels. A protrusion in a graph [FOCS 2009] is a subgraph of constant treewidth which is separated from the rest of the graph by at most a constant number of vertices. In our variant of protrusions, instead of stipulating that the subgraph be of constant treewidth, we ask that it contains a constant number of vertices from a solution. We believe that this new take on protrusions would be useful for other graph problems and in different algorithmic settings. © 2018 ACM.",Connected dominating set; Kernelization; Topological minor free graphs,Graphic methods; Polynomial approximation; Polynomials; Topology; Connected Dominating Set; Connected dominating set problems; Divide and conquer; Free graphs; Kernelization; Polynomial kernels; Polynomial-time algorithms; Topological-minor; Graph theory
Cache-Oblivious buffer heap and cache-Efficient computation of shortest paths in graphs,2018,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042495264&doi=10.1145%2f3147172&partnerID=40&md5=c0f2a539ce1eef6dd20a50c6eb159a60,"We present the buffer heap, a cache-oblivious priority queue that supports Delete-Min, Delete, and a hybrid Insert/Decrease-Key operation in O(B1 log2 MN ) amortized block transfers from main memory, where M and B are the (unknown) cache size and block size, respectively, and N is the number of elements in the queue. We introduce the notion of a slim data structure that captures the situation when only a limited portion of the cache, which we call a slim cache, is available to the data structure to retain data between data structural operations. We show that a buffer heap automatically adapts to such an environment and supports all operations in O(λ1 +B1 log2Nλ ) amortized block transfers each when the size of the slim cache is λ. Our results provide substantial improvements over known trivial cache performance bounds for cache-oblivious priority queues with Decrease-Keys. Using the buffer heap, we present cache-oblivious implementations of Dijkstra’s algorithm for undirected and directed single-source shortest path (SSSP) problems for graphs with non-negative real edge-weights. On a graph with n vertices and m edges, our algorithm for the undirected case performs O(n +mB log2 Mn ) block transfers and for the directed case performs O((n + mB ) · log2 Bn ) block transfers. These results give the first non-trivial cache-oblivious bounds for the SSSP problem on general graphs. For the all-pairs shortest path (APSP) problem on weighted undirected graphs, we incorporate slim buffer heaps into multi-buffer-buffer-heaps and use these to improve the cache-aware cache complexity. We also present a simple cache-oblivious APSP algorithm for unweighted undirected graphs that performs O(mnB logM/B Bn ) block transfers. This matches the cache-aware bound and is a substantial improvement over the previous cache-oblivious bound for the problem. © 2018 ACM.",Buffer heap; Cache-efficient; Cache-oblivious; Decrease-key; Priority queue; Shortest paths,Cache memory; Data structures; Directed graphs; Graphic methods; Queueing theory; Buffer heap; Cache-efficient; Cache-oblivious; Decrease-key; Priority queues; Shortest path; Graph theory
Sublinear-Time Maintenance of Breadth-First Spanning Trees in Partially Dynamic Networks,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041442206&doi=10.1145%2f3146550&partnerID=40&md5=b0a4dbfd26f0f370f707d988a346bb52,"We study the problem of maintaining a breadth-first spanning tree (BFS tree) in partially dynamic distributed networks modeling a sequence of either failures or additions of communication links (but not both). We present deterministic (1 + ∈ )-approximation algorithms whose amortized time (over some number of link changes) is sublinear in D, the maximum diameter of the network. Our technique also leads to a deterministic (1 + ∈ )-approximate incremental algorithm for single-source shortest paths in the sequential (usual RAM) model. Prior to our work, the state of the art was the classic exact algorithm of Even and Shiloach (1981), which is optimal under some assumptions (Roditty and Zwick 2011; Henzinger et al. 2015). Our result is the first to show that, in the incremental setting, this bound can be beaten in certain cases if some approximation is allowed. © 2017 ACM.",Distributed algorithms; Dynamic algorithms,Algorithms; Mathematical techniques; Parallel algorithms; Amortized time; Dynamic algorithm; Dynamic distributed networks; Dynamic network; Exact algorithms; Incremental algorithm; Single source shortest paths; State of the art; Approximation algorithms
Parallel algorithms and concentration bounds for the lovász local lemma via witness DAGs,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041438911&doi=10.1145%2f3147211&partnerID=40&md5=5982d82aa69ec899d6302475f7e0ca30,"The Lovász Local Lemma (LLL) is a cornerstone principle in the probabilistic method of combinatorics, and a seminal algorithm of Moser and Tardos (2010) provides an efficient randomized algorithm to implement it. This can be parallelized to give an algorithm that uses polynomially many processors and runs in O(log3 n) time on an EREW PRAM, stemming fromO(logn) adaptive computations of amaximal independent set (MIS). Chung et al. (2014) developed faster local and parallel algorithms, potentially running in time O(log2 n), but these algorithms require more stringent conditions than the LLL.We give a new parallel algorithm that works under essentially the same conditions as the original algorithm of Moser and Tardos but uses only a single MIS computation, thus running in O(log2 n) time on an EREW PRAM. This can be derandomized to give an NC algorithm running in time O(log2 n) as well, speeding up a previous NC LLL algorithm of Chandrasekaran et al. (2013). We also provide improved and tighter bounds on the runtimes of the sequential and parallel resampling-based algorithms originally developed byMoser and Tardos. These apply to any problem instance in which the tighter Shearer LLL criterion is satisfied. © 2017 ACM.",Lovász Local Lemma; Resampling Algorithm; Shearer's criterion,Algorithms; Mathematical techniques; Adaptive computations; Concentration bounds; Local lemmata; Original algorithms; Probabilistic methods; Randomized Algorithms; Resampling algorithms; Shearer's criterion; Parallel algorithms
"Separate, measure and conquer: Faster polynomial-space algorithms for Max 2-CSP and counting dominating sets",2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041446083&doi=10.1145%2f3111499&partnerID=40&md5=cafc789a4869e8bb3816b3659a014ed1,"We show a method resulting in the improvement of several polynomial-space, exponential-time algorithms. The method capitalizes on the existence of small balanced separators for sparse graphs,which can be exploited for branching to disconnect an instance into independent components. For this algorithm design paradigm, the challenge to date has been to obtain improvements in worst-case analyses of algorithms, compared with algorithms that are analyzed with advanced methods, notably Measure and Conquer. Our contribution is the design of a general method to integrate the advantage from the separator-branching into Measure and Conquer, for a more precise and improved running time analysis. We illustrate the method with improved algorithms for Max (r,2)-CSP and #Dominating Set. An instance of the problem Max (r,2)-CSP, or simply Max 2-CSP, is parameterized by the domain size r (often 2), the number of variables n (vertices in the constraint graph G), and the number of constraints m (edges in G). When G is cubic, and omitting sub-exponential terms here for clarity, we give an algorithm running in time r (1/5) n = r (2/15)m; the previous best was r (1/4) n = r (1/6)m. By known results, this improvement for the cubic case results in an algorithm running in time r (9/50)m for general instances; the previous best was r (19/100)m.We show that the analysis of the earlier algorithm was tight: our improvement is in the algorithm, not just the analysis. The same running time improvements hold for Max Cut, an important special case of Max 2-CSP, and for Polynomial and Ring CSP, generalizations encompassing graph bisection, the Ising model, and counting. We also give faster algorithms for #Dominating Set, counting the dominating sets of every cardinality 0, . . . ,n for a graph G of order n. For cubic graphs, our algorithm runs in time 3(1/5) n; the previous best was 2(1/2) n. For general graphs, we give an unrelated algorithm running in time 1.5183n; the previous best was 1.5673n. The previous best algorithms for these problems all used local transformations and were analyzed by the Measure and Conquer method. Our new algorithms capitalize on the existence of small balanced separators for cubic graphs - a non-local property - and the ability to tailor the local algorithms always to ""pivot"" on a vertex in the separator. The new algorithms perform much as the old ones until the separator is empty, at which point they gain because the remaining vertices are split into two independent problem instances that can be solved recursively. It is likely that such algorithms can be effective for other problems too, and we present their design and analysis in a general framework. © 2017 ACM.",Counting dominating sets; Graph separators; Max 2-CSP; Measure & conquer,Constraint satisfaction problems; Graphic methods; Ising model; Polynomials; Separators; Dominating sets; Exponential time algorithm; Graph separators; Independent components; Local transformations; Max 2-CSP; Measure and conquer; Running time analysis; Graph theory
Approximation algorithms for computing maximin share allocations,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041433832&doi=10.1145%2f3147173&partnerID=40&md5=3976cc7d07f87c8daae5f1d5207434a3,"We study the problem of computing maximin share allocations, a recently introduced fairness notion. Given a set of n agents and a set of goods, the maximin share of an agent is the best she can guarantee to herself, if she is allowed to partition the goods in any way she prefers, into n bundles, and then receive her least desirable bundle. The objective then is to find a partition, where each agent is guaranteed her maximin share. Such allocations do not always exist, hence we resort to approximation algorithms. Our main result is a 2/3- approximation that runs in polynomial time for any number of agents and goods. This improves upon the algorithm of Procaccia andWang (2014), which is also a 2/3-approximation but runs in polynomial time only for a constant number of agents. To achieve this, we redesign certain parts of the algorithm in Procaccia and Wang (2014), exploiting the construction of carefully selected matchings in a bipartite graph representation of the problem. Furthermore, motivated by the apparent difficulty in establishing lower bounds, we undertake a probabilistic analysis.We prove that in randomly generated instances, maximin share allocations exist with high probability. This can be seen as a justification of previously reported experimental evidence. Finally, we provide further positive results for two special cases arising from previous works. The first is the intriguing case of three agents, where we provide an improved 7/8-approximation. The second case is when all item values belong to {0, 1, 2}, where we obtain an exact algorithm. © 2017 ACM.",Fair division; Maximin share,Polynomial approximation; Bipartite graphs; Exact algorithms; Experimental evidence; Fair divisions; High probability; Lower bounds; Maximin; Polynomial-time; Approximation algorithms
Primal dual gives almost optimal energy-efficient online algorithms,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041436301&doi=10.1145%2f3155297&partnerID=40&md5=a65c94808c98c6bf2af6abcb1256346b,"We consider the problem of online scheduling of jobs on unrelated machines with dynamic speed scaling to minimize the sum of energy and weighted flow-time. We give an algorithm with an almost optimal competitive ratio for arbitrary power functions. (No earlier results handled arbitrary power functions for unrelated machines.) For power functions of the form f (s ) = sα for some constant α > 1, we get a competitive ratio of O( α/log α ), improving upon a previous competitive ratio of O(α2) by Anand et al. (2012), along with a matching lower bound of ω( α/log α ). Further, in the resource augmentation model, with a 1+ ∈ speed up, we give a 2( 1/∈ + 1) competitive algorithm, with essentially the same techniques, improving the bound of 1 + O( 1/∈2 ) by Gupta et al. (2010) and matching the bound of Anand et al. (2012) for the special case of fixed speed unrelated machines. Unlike the previous results most of which used an amortized local competitiveness argument or dual fitting methods, we use a primal-dual method, which is useful not only to analyze the algorithms but also to design the algorithm itself. © 2017 ACM.",Energy efficiency; Flow-time; Online algorithm; Scheduling,Aluminum; Scheduling; Scheduling algorithms; Competitive algorithms; Dynamic speed scaling; Flow-time; On-line algorithms; Optimal competitive ratios; Primal-dual methods; Resource augmentation; Unrelated machines; Energy efficiency
"Binary adder circuits of asymptotically minimum depth, linear size, and fan-out two",2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041441394&doi=10.1145%2f3147215&partnerID=40&md5=b2fe61d4f252e086f6f1b6089ce3a222,"We consider the problem of constructing fast and small binary adder circuits. Among widely used adders, the Kogge-Stone adder is often considered the fastest, because it computes the carry bits for two n-bit numbers (where n is a power of two) with a depth of 2 log2 n logic gates, size 4n log2 n, and all fan-outs bounded by two. Fan-outs of more than two are disadvantageous in practice, because they lead to the insertion of repeaters for repowering the signal and additional depth in the physical implementation. However, the depth bound of the Kogge-Stone adder is off by a factor of two from the lower bound of log2 n. Two separate constructions by Brent and Krapchenko achieve this lower bound asymptotically. Brent's construction gives neither a bound on the fan-out nor the size, while Krapchenko's adder has linear size, but can have up to linear fan-out. With a fan-out bound of two, neither construction achieves a depth of less than 2 log2 n. In a further approach, Brent and Kung proposed an adder with linear size and fan-out two but twice the depth of the Kogge-Stone adder. These results are 33-43 years old and no substantial theoretical improvement for has beenmade since then. In this article, we integrate the individual advantages of all previous adder circuits into a new family of full adders, the first to improve on the depth bound of 2 log2 n while maintaining a fan-out bound of two. Our adders achieve an asymptotically optimum logic gate depth of log2 n + o(log2 n) and linear size O(n). © 2017 ACM.",Binary addition; Circuit; Combinational complexity; Depth; Fan-out; Parallel; Size,Bins; Carry logic; Computer circuits; Logic gates; Networks (circuits); Timing circuits; Binary additions; Combinational complexity; Depth; Fan Out; Parallel; Size; Adders
Distributed private data analysis: Lower bounds and practical constructions,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041441128&doi=10.1145%2f3146549&partnerID=40&md5=e96b9dc36a268bb16212aff262bd46cb,"We consider a distributed private data analysis setting, where multiple parties each hold some sensitive data and they wish to run a protocol to learn some aggregate statistics over the distributed dataset, while protecting each user's privacy. As an initial effort, we consider a distributed summation problem. We first show a lower bound, that is, under information-theoretic differential privacy, any multi-party protocol with a small number of messages must have large additive error. We then show that by adopting a computational differential privacy notion, one can circumvent this lower bound and design practical protocols for the periodic distributed summation problem. Our construction has several desirable features. First, it works in the clientserver model and requires no peer-to-peer communication among the clients. Second, our protocol is fault tolerant and can output meaningful statistics even when a subset of the participants fail to respond. Our constructions guarantee the privacy of honest parties even when a fraction of the participants may be compromised and colluding. In addition, we propose a new distributed noise addition mechanism that guarantees small total error. © 2017 ACM.",Differential privacy; Distributed private data analysis; Periodic aggregation; Untrusted aggregator,Data handling; Information analysis; Information theory; Client-server models; Desirable features; Differential privacies; Distributed noise additions; Multi-party protocols; Peer-to-peer communications; Private data analysis; Untrusted aggregator; Data privacy
Independence and efficient domination on P6-free graphs,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038591268&doi=10.1145%2f3147214&partnerID=40&md5=e082df336b54c5bb313691e6565ef3f2,"In the Maximum Weight Independent Set problem, the input is a graphG, every vertex has a non-negative integer weight, and the task is to find a set S of pairwise nonadjacent vertices, maximizing the total weight of the vertices in S. We give an n O(log2 n) time algorithm for this problem on graphs excluding the path P6 on 6 vertices as an induced subgraph. Currently, there is no constant k known for which Maximum Weight Independent Set on Pκ-free graphs becomes NP-hard, and our result implies that if such a κ exists, then k κ6 unless all problems in NP can be decided in quasi-polynomial time. Using the combinatorial tools that we develop for this algorithm, we also give a polynomial-Time algorithm for Maximum Weight Efficient Dominating Set on P6-free graphs. In this problem, the input is a graph G, every vertex has an integer weight, and the objective is to find a set S of maximum weight such that every vertex inG has exactly one vertex in S in its closed neighborhood or to determine that no such set exists. Prior to our work, the class of P6-free graphs was the only class of graphs defined by a single forbidden induced subgraph on which the computational complexity of Maximum Weight Efficient Dominating Set was unknown. CCS Concepts.",(Quasi)polynomialtime algorithms; Efficient domination; Independence; P<sub>6</sub>-free graphs,Graphic methods; Polynomial approximation; Efficient dominating set; Efficient domination; Free graphs; Independence; Maximum weight independent set problems; Polynomial-time algorithms; Quasi-polynomial time; Weight independent sets; Graph theory
The parametric closure problem,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038599889&doi=10.1145%2f3147212&partnerID=40&md5=968b730b75e1b2420f13636f4da0c28f,"We define the parametric closure problem, in which the input is a partially ordered set whose elements have linearly varying weights and the goal is to compute the sequence of minimum-weight downsets of the partial order as the weights vary.We give polynomial time solutions to many important special cases of this problem including semiorders, reachability orders of bounded-Treewidth graphs, partial orders of bounded width, and series-parallel partial orders. Our result for series-parallel orders provides a significant generalization of a previous result of Carlson and Eppstein on bicriterion subtree problems. © 2017 ACM 1549-6325/2017/12-ART2 $15.00.",Bicriterion optimization; Closure problem; Parametric optimization,Polynomial approximation; Polynomials; Bi-criterion optimizations; Bounded-treewidth graphs; Closure problem; Minimum weight; Parametric optimization; Partially ordered set; Polynomial-time; Series-parallel; Set theory
Toward optimal self-adjusting heaps,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041446639&doi=10.1145%2f3147138&partnerID=40&md5=3a9f6b80be5a6908326a2a3aa5820a42,"We give a variant of the pairing heaps that achieves the following amortized costs: O(1) per find-min and insert, O(log logn) per decrease-key and meld, O(logn) per delete-min; where n is the number of elements in the resulting heap on which the operation is performed. These bounds are the best known for any selfadjusting heap and match two lower bounds, one established by Fredman and the other by Iacono and Özkan, for a family of self-adjusting heaps that generalizes the pairing heaps but do not include our variant. We further show how to reduce the amortized cost for meld to be paid by the other operations, on the expense of increasing that of delete-min toO(logn + log log N ), where N is the total number of elements in the collection of heaps of the data structure (not just the heap under consideration by the operation). © 2017 ACM.",Amortized analysis; Heaps; Pairing heaps; Priority queues,Algorithms; Amortized analysis; Heaps; Lower bounds; Pairing heaps; Priority queues; Self-adjusting; Mathematical techniques
Maximizing polynomials subject to assignment constraints,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041443675&doi=10.1145%2f3147137&partnerID=40&md5=76a56b6cbe52a578e329b49305d1edf9,"We study the q-adic assignment problem. We first give an O(n(q-1)/2)-approximation algorithm for the Koopmans-Beckman version of the problem, improving upon the result of Barvinok. Then, we introduce a new family of instances satisfying ""tensor triangle inequalities"" and give a constant factor approximation algorithm for them.We showthat many classical optimization problems can be modeled by q-adic assignment problems from this family. Finally, we give several integrality gap examples for the natural LP relaxations of the problem. © 2017 ACM.",Approximation algorithms; Quadratic assignment problem,Combinatorial optimization; Optimization; Assignment problems; Classical optimization; Constant-factor approximation algorithms; Integrality gaps; LP relaxation; Quadratic assignment problems; Triangle inequality; Approximation algorithms
Spider covers for prize-collecting network activation problem,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032896162&doi=10.1145%2f3132742&partnerID=40&md5=7839c327979275f18114f9a19a1f598c,"In the network activation problem, each edge in a graph is associated with an activation function that decides whether the edge is activated from weights assigned to its end nodes. The feasible solutions of the problem are node weights such that the activated edges form graphs of required connectivity, and the objective is to find a feasible solution minimizing its total weight. In this article, we consider a prize-collecting version of the network activation problem and present the first nontrivial approximation algorithms. Our algorithms are based on a newlinear programming relaxation of the problem. They round optimal solutions for the relaxation by repeatedly computing node weights activating subgraphs, called spiders, which are known to be useful for approximating the network activation problem. For the problem with node-connectivity requirements, we also present a new potential function on uncrossable biset families and use it to analyze our algorithms. © 2017 ACM.",LP rounding algorithm; Network design; Wireless network,Approximation algorithms; Graph theory; Wireless networks; Activation functions; Feasible solution; LP-rounding; Network activations; Network design; Node connectivity; Potential function; Programming relaxations; Chemical activation
Exact and asymptotic solutions of a divide-and-conquer recurrence dividing at half: Theory and applications,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032894165&doi=10.1145%2f3127585&partnerID=40&md5=094acd1ba35b7b3bc8acbc7a5f9d0478,"Divide-and-conquer recurrences of the form f(n) = f ([n/2)] + f ([n/2)] + g(n) (n ≥ 2), with g(n) and f (1) given, appear very frequently in the analysis of computer algorithms and related areas. While most previous methods and results focus on simpler crude approximation to the solution, we show that the solution always satisfies the simple identity f(n) = nP (log2 n) - Q(n) under an optimum (iff) condition on g(n). This form is not only an identity but also an asymptotic expansion because Q(n) is of a smaller order than linearity. Explicit forms for the continuous periodic function P are provided. We show how our results can be easily applied to many dozens of concrete examples collected from the literature and how they can be extended in various directions. Our method of proof is surprisingly simple and elementary but leads to the strongest types of results for all examples to which our theory applies. © 2017 ACM.",Additivity; Analysis of algorithms; Asymptotic approximation; Asymptotic linearity; Functional equation; Identity; Master theorems; Periodic oscillation; Recurrence relation; Sensitivity analysis; Uniform continuity,Approximation algorithms; Additivity; Analysis of algorithms; Asymptotic approximation; Asymptotic linearity; Functional equation; Identity; Master theorems; Periodic oscillation; Recurrence relations; Uniform continuity; Sensitivity analysis
Linear-time parameterized algorithms via skew-symmetric multicuts,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030327357&doi=10.1145%2f3128600&partnerID=40&md5=97a73a44208879c8012715a334604423,"A skew-symmetric graph (D = (V,A), s ) is a directed graph D with an involution s on the set of vertices and arcs. Flows on skew-symmetric graphs have been used to generalize maximum flow and maximum matching problems on graphs, initially by Tutte and later by Goldberg and Karzanov. In this article, we introduce a separation problem, d-Skew-Symmetric Multicut, where we are given a skew-symmetric graph D, a family T of d-size subsets of vertices, and an integer k. The objective is to decide whether there is a set X ? A of k arcs such that every set J in the family has a vertex v such that v and s (v) are in different strongly connected components ofD= (V,A \ (X ? s (X)). In thiswork,we give an algorithm ford-Skew-Symmetric Multicut that runs in time O((4d)k (m + n +)), wherem is the number of arcs in the graph, n is the number of vertices, andis the length of the family given in the input. This problem, apart from being independently interesting, also captures the main combinatorial difficulty of numerous classical problems. Our algorithm for d-Skew-Symmetric Multicut paves the way for the first linear-time parameterized algorithms for several problems. We demonstrate its utility by obtaining the following linear-time parameterized algorithms:-We show that Almost 2-SAT is a special case of 1-Skew-Symmetric Multicut, resulting in an algorithm for Almost 2-SAT that runs in time O(4kk4), where k is the size of the solution andis the length of the input formula. Then, using linear-time parameter-preserving reductions to Almost 2-SAT, we obtain algorithms for Odd Cycle Transversal and Edge Bipartization that run in time O(4kk4 (m + n)) and O(4kk5 (m + n)), respectively, where k is the size of the solution, and m and n are the number of edges and vertices respectively. This resolves an open problem posed by Reed et al. and improves on the earlier almost-linear-time algorithm of Kawarabayashi and Reed.-We show that Deletion q-Horn Backdoor Set Detection is a special case of 3-Skew-Symmetric Multicut, giving us an algorithm for Deletion q-Horn Backdoor Set Detection that runs in time O(12kk5), where k is the size of the solution andis the length of the input formula. This gives the first fixed-parameter tractable algorithm for this problem answering a question posed in a work by Narayanaswamy et al. Using this result, we get an algorithm for Satisfiability that runs in time O(12kk5), where k is the size of the smallest q-Horn deletion backdoor set, with being the length of the input formula. © 2017 ACM.",Backdoors to satisfiability; Graph bipartization; Graph separation problems,Clustering algorithms; Directed graphs; Flow graphs; Formal logic; Graph theory; Parameterization; Fixed-parameter tractable algorithms; Graph bipartization; Linear-time algorithms; Odd cycle transversals; Parameterized algorithm; Satisfiability; Separation problems; Strongly connected component; Parameter estimation
A data structure for nearest common ancestors with linking,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030319579&doi=10.1145%2f3108240&partnerID=40&md5=bbd4458e9d7616c591b1177ef6a61084,"Consider a forest that evolves via link operations that make the root of one tree the child of a node in another tree. Intermixed with link operations are nca operations, which return the nearest common ancestor of two given nodes when such exists. This article shows that a sequence of m such nca and link operations on a forest of n nodes can be processed online in time O(ma(m,n) + n). This was previously known only for a restricted type of link operation. The special case where a link only extends a tree by adding a new leaf occurs in Edmonds' algorithm for finding a maximum weight matching on a general graph. Incorporating our algorithm into the implementation of Edmonds' algorithm in [9] achieves time O(n(m + n logn)) for weighted matching, an arguably optimum asymptotic bound (n andm are the number of vertices and edges, respectively). Our data structure also provides a simple alternative implementation of the incremental-tree set merging algorithm of Gabow and Tarjan [11]. © 2017 ACM.",Disjoint-set data structure; Least common ancestors; Matching algorithms; Set merging,Data structures; Forestry; Asymptotic bounds; General graph; Least common ancestors; Matching algorithm; Maximum weight matching; Merging algorithms; Weighted matching; Merging
Counting thin subgraphs via packings faster than meet-in-the-middle time,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030330604&doi=10.1145%2f3125500&partnerID=40&md5=6c09ff61275aba757c62e9b141c4dd90,"Vassilevska and Williams (STOC'09) showed how to count simple paths on k vertices and matchings on k/2 edges in ann-vertex graph in time nk/2+O(1). In the same year, two different algorithms with the same runtime were given by Koutis and Williams (ICALP'09), and Bjorklund et al. (ESA'09), via nst/2+O(1)-time algorithms for counting t-tuples of pairwise disjoint sets drawn from a given family of s-sized subsets of an n-element universe. Shortly afterwards, Alon and Gutner (TALG'10) showed that these problems have O(n st/2) and O(nk/2) lower bounds when counting by color coding. Here, we show that one can do better-we show that the ""meet-in-the-middle"" exponent st/2 can be beaten and give an algorithm that counts in time n0.45470382st+O(1) for t a multiple of three. This implies algorithms for counting occurrences of a fixed subgraph on k vertices and pathwidth pk in an n-vertex graph in n0.45470382k+2p+O(1) time, improving on the three mentioned algorithms for paths and matchings, and circumventing the color-coding lower bound. We also give improved bounds for counting t-tuples of disjoint s-sets for s = 2, 3, 4. Our algorithms use fast matrix multiplication. We show an argument that this is necessary to go below the meet-in-the-middle barrier. © 2017 ACM.",Counting low pathwidth graphs; Counting matchings; Counting packings; Counting paths; FPT algorithms; Matrix multiplication; Meet in the middle,Matrix algebra; Counting paths; FPT algorithms; Matchings; MAtrix multiplication; Meet-in-the-middle; Pathwidth; Graph theory
Tight Kernel bounds for problems on graphs with small degeneracy,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028546128&doi=10.1145%2f3108239&partnerID=40&md5=d11600598041b5d806695dee142e72a5,"Kernelization is a strong and widely applied technique in parameterized complexity. In a nutshell, a kernelization algorithm for a parameterized problem transforms in polynomial time a given instance of the problem into an equivalent instance whose size depends solely on the parameter. Recent years have seen major advances in the study of both upper and lower bound techniques for kernelization, and by now this area has become one of the major research threads in parameterized complexity. In this article, we consider kernelization for problems on d-degenerate graphs, that is, graphs such that any subgraph contains a vertex of degree at most d. This graph class generalizes many classes of graphs for which effective kernelization is known to exist, for example, planar graphs, H-minor free graphs, and H-topological-minor free graphs. We show that for several natural problems on d-degenerate graphs the best-known kernelization upper bounds are essentially tight. In particular, using intricate constructions of weak compositions, we prove that unless coNP ⊆ NP/poly: • DOMINATING SET has no kernels of size O(k(d-1)(d-3)-ϵ) for any ϵ > 0. The current best upper bound is O(k(d+1)2). • INDEPENDENT DOMINATING SET has no kernels of size O(kd-4-ϵ) for any ϵ > 0. The current best upper bound is O(kd+1). • INDUCED MATCHING has no kernels of size O(kd-3-ϵ) for any ϵ > 0. The current best upper bound is O(kd). To the best of our knowledge, DOMINATING SET is the the first problem where a lower bound with superlinear dependence on d (in the exponent) can be proved. In the last section of the article, we also give simple kernels for CONNECTED VERTEX COVER and CAPACITATED VERTEX COVER of size O(kd) and O(kd+1), respectively. We show that the latter problem has no kernels of size O(kd-ϵ) unless coNP ⊆ NP/poly by a simple reduction from d-EXACT SET COVER (the same lower bound for CONNECTED VERTEX COVER on d-degenerate graphs is already known). © 2017 ACM.",Degenerate graphs; Kernelization; Kernelization lower bounds; Parameterized complexity; Sparse graphs; Weak compositions,Graphic methods; Parameterization; Polynomial approximation; Topology; Degenerate graphs; Kernelization; Lower bounds; Parameterized complexity; Sparse graphs; Weak compositions; Graph theory
"Max-sum diversification, monotone submodular functions, and dynamic updates",2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026527724&doi=10.1145%2f3086464&partnerID=40&md5=dd3a166e2f0c2b8d75487df466d64d41,"Result diversification is an important aspect in web-based search, document summarization, facility location, portfolio management, and other applications. Given a set of ranked results for a set of objects (e.g., web documents, facilities, etc.) with a distance between any pair, the goal is to select a subset S satisfying the following three criteria: (a) the subset S satisfies some constraint (e.g., bounded cardinality), (b) the subset contains results of high “quality,” and (c) the subset contains results that are “diverse” relative to the distance measure. The goal of result diversification is to produce a diversified subset while maintaining high quality as much as possible. We study a broad class of problems where the distances are a metric, where the constraint is given by independence in a matroid, where quality is determined by a monotone submodular function and diversity is defined as the sum of distances between objects in S. Our problem is a generalization of the max-sum diversification problem studied in Gollapudi and Sharma [2009], which in turn is a generalization of the max-sum p-dispersion problem studied extensively in location theory. It is NP-hard even with the triangle inequality. We propose two simple and natural algorithms: a greedy algorithm for a cardinality constraint and a local search algorithm for an arbitrary matroid constraint. We prove that both algorithms achieve constant approximation ratios. © 2017 ACM",Dispersion; Diversification; Greedy algorithms; Local search; Submodular functions,C (programming language); Combinatorial mathematics; Dispersion (waves); Dispersions; Financial data processing; Investments; Local search (optimization); Set theory; Cardinality constraints; Diversification; Document summarization; Greedy algorithms; Local search; Local search algorithm; Portfolio managements; Submodular functions; Approximation algorithms
Hollow heaps,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027324777&doi=10.1145%2f3093240&partnerID=40&md5=6c05b383e80d7e28199c7006c169d781,"We introduce the hollow heap, a very simple data structure with the same amortized efficiency as the classical Fibonacci heap. All heap operations except delete and delete-min take O(1) time, worst case as well as amortized; delete and delete-min take O(logn) amortized time on a heap of n items. Hollow heaps are the simplest structure to achieve these bounds. Hollow heaps combine two novel ideas: the use of lazy deletion and re-insertion to do decrease-key operations and the use of a dag (directed acyclic graph) instead of a tree or set of trees to represent a heap. Lazy deletion produces hollow nodes (nodes without items), giving the data structure its name. © 2017 ACM.",Amortized analysis; Data structures; Heaps; Priority queues,Data structures; Directed graphs; Forestry; Amortized analysis; Amortized time; Directed acyclic graph (DAG); Fibonacci heap; Heaps; Key operations; Priority queues; Trees (mathematics)
Improved approximation algorithm for steiner k-forest with nearly uniform weights,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026509697&doi=10.1145%2f3077581&partnerID=40&md5=d2c05bcee80a15bde11dd0822779e01c,"In the Steiner k-Forest problem, we are given an edge weighted graph, a collection D of node pairs, and an integer k . |D|. The goal is to find amin-weight subgraph that connects at least k pairs. The best known ratio for this problem is min{O( ã n), O( ã k)} [Gupta et al. 2010]. In Gupta et al. [2010], it is also shown that ratio ƒÏ for Steiner k-Forest implies ratio O(ƒÏ E log2 n) for the related Dial-a-Ride problem. The only other algorithm known for Dial-a-Ride, besides the one resulting from Gupta et al. [2010], has ratio O( ã n) [Charikar and Raghavachari 1998]. We obtain approximation ratio n0.448 for Steiner k-Forest and Dial-a-Ride with unit weights, breaking the O( ã n) approximation barrier for this natural case. We also show that if the maximum edge-weight is O(nε), then one can achieve ratio O(n(1+ε)E0.448), which is less than ã n if ε is small enough. The improvement for Dial-a-Ride is the first progress for this problem in 15 years. To prove our main result, we consider the following generalization of the Minimum k-Edge Subgraph (Mk-ES) problem, which we call Min-Cost ℓ-Edge- Profit Subgraph (MCℓ-EPS): Given a graph G = (V, E) with edge-profits p = {pe : e ¸ E} and node-costs c = {cv : v ¸ V}, and a lower profit bound ℓ, find a minimum node-cost subgraph of G of edge-profit at least ℓ. The Mk-ES problem is a special case of MCℓ-EPS with unit node costs and unit edge profits. The currently best known ratio for Mk-ES is n3.2 ã 2+ε [Chlamtac et al. 2012]. We extend this ratio to MCℓ-EPS for general node costs and profits bounded by a polynomial in n, which may be of independent interest.",Dial-a-ride; Minimum k-edge subgraph,Approximation algorithms; Costs; Forestry; Profitability; Approximation ratios; Dial-a-Ride; Dial-a-ride problem; Edge weights; Edge-weighted graph; Node pairs; Subgraphs; Unit weight; Graph theory
Cost-oblivious storage reallocation,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027027131&doi=10.1145%2f3070693&partnerID=40&md5=21eeb6d26f94f40bd287e3837524859c,"Databases allocate and free blocks of storage on disk. Freed blocks introduce holes where no data is stored. Allocation systems attempt to reuse such deallocated regions in order to minimize the footprint on disk. When previously allocated blocks cannot be moved, this problem is called the memory allocation problem. The competitive ratio for this problem has matching upper and lower bounds that are logarithmic in the number of requests and in the ratio of the largest to smallest requests. This article defines the storage reallocation problem, where previously allocated blocks can be moved, or reallocated, but at some cost. This cost is determined by the allocation/reallocation cost function. The objective is to minimize the storage footprint, that is, the largest memory address containing an allocated object, while simultaneously minimizing the reallocation costs. This article gives asymptotically optimal algorithms for storage reallocation, in which the storage footprint is atmost (1+∈) times optimal, and the reallocation cost is O((1/∈) log(1/∈)) times the original allocation cost, that is, it is within a constant factor of optimal when ∈ is a constant. The algorithms are cost oblivious, which means they achieve these bounds with no knowledge of the allocation/reallocation cost function, as long as the cost function is subadditive. © 2017 ACM.",Cost oblivious; Physical layout; Reallocation; Scheduling; Storage allocation,Cost functions; Digital storage; Scheduling; Storage allocation (computer); Allocation problems; Allocation systems; Asymptotically optimal; Competitive ratio; Constant factors; Physical layout; Reallocation; Upper and lower bounds; Costs
Tight bounds on vertex connectivity under sampling,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027018420&doi=10.1145%2f3086465&partnerID=40&md5=674fde2d810433c155870f6130da427c,"A fundamental result by Karger [10] states that for any λ-edge-connected graph with n nodes, independently sampling each edge with probability p = Ω(log(n)/λ) results in a graph that has edge connectivity Ω(λp), with high probability. This article proves the analogous result for vertex connectivity, when either vertices or edges are sampled. We show that for any k-vertex-connected graph G with n nodes, if each node is independently sampled with probability p = Ω(√log(n)/k), then the subgraph induced by the sampled nodes has vertex connectivity Ω(kp2), with high probability. If edges are sampled with probability p = Ω(log(n)/k), then the sampled subgraph has vertex connectivity Ω(kp), with high probability. Both bounds are existentially optimal. © 2017 ACM.",Graph sampling; Vertex connectivity,Probability; Connected graph; Edge connectivity; Graph samplings; High probability; K-vertex connected graphs; Probability p; Under-sampling; Vertex connectivity; Graph theory
Maximizing symmetric submodular functions,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027001294&doi=10.1145%2f3070685&partnerID=40&md5=a82279c5df89e0c65103a6a959b9192d,"Symmetric submodular functions are an important family of submodular functions capturing many interesting cases, including cut functions of graphs and hypergraphs. Maximization of such functions subject to various constraints receives little attention by current research, unlike similar minimization problems that have been widely studied. In this work, we identify a few submodular maximization problems for which one can get a better approximation for symmetric objectives than the state-of-the-art approximation for general submodular functions. We first consider the problem of maximizing a non-negative symmetric submodular function f : 2N → ℝ+ subject to a down-monotone solvable polytope ℘ ⊆ [0, 1]N. For this problem, we describe an algorithm producing a fractional solution of value at least 0.432 · f (OPT), where OPT is the optimal integral solution. Our second result considers the problem max{f(S) : |S| = k} for a non-negative symmetric submodular function f : 2N → ℝ+. For this problem, we give an approximation ratio that depends on the value k/|N| and is always at least 0.432. Our method can also be applied to non-negative non-symmetric submodular functions, in which case it produces 1/e - o(1) approximation, improving over the best-known result for this problem. For unconstrained maximization of a non-negative symmetric submodular function, we describe a deterministic linear-time 1/2-approximation algorithm. Finally, we give a [1 - (1 - 1/k)k-1]-approximation algorithm for Submodular Welfare with k players having identical non-negative submodular utility functions and show that this is the best possible approximation ratio for the problem. © 2017 ACM.",Cardinality constraint; Matroid constraint; Submodular welfare; Symmetric submodular functions,Algorithms; Mathematical techniques; Approximation ratios; Cardinality constraints; Fractional solutions; Graphs and hypergraphs; Maximization problem; Minimization problems; Submodular; Submodular functions; Approximation algorithms
Combinatorial algorithm for restricted max-min fair allocation,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027055525&doi=10.1145%2f3070694&partnerID=40&md5=66901992c90691e47012549c94393182,"We study the basic allocation problem of assigning resources to players to maximize fairness. This is one of the few natural problems that enjoys the intriguing status of having a better estimation algorithm than approximation algorithm. Indeed, a certain Configuration-LP can be used to estimate the value of the optimal allocation to within a factor of 4 + ϵ. In contrast, however, the best-known approximation algorithm for the problem has an unspecified large constant guarantee. In this article, we significantly narrow this gap by giving a 13-approximation algorithm for the problem. Our approach develops a local search technique introduced by Haxell [13] for hypergraph matchings and later used in this context by Asadpour, Feige, and Saberi [2]. For our local search procedure to terminate in polynomial time, we introduce several new ideas, such as lazy updates and greedy players. Besides the improved approximation guarantee, the highlight of our approach is that it is purely combinatorial and uses the Configuration-LP only in the analysis. © 2017 ACM.",Approximation algorithms; Efficient local search; Fair allocation,Local search (optimization); Polynomial approximation; Allocation problems; Combinatorial algorithm; Estimation algorithm; Fair allocation; Local search; Local search techniques; Optimal allocation; Polynomial-time; Approximation algorithms
Editorial,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017185769&doi=10.1145%2f3038922&partnerID=40&md5=4aaada857fde1f40d39be60ef053f6c7,[No abstract available],,
A fast and simple surface reconstruction algorithm,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017200372&doi=10.1145%2f3039242&partnerID=40&md5=ac69cda9ad1b54e47eff1632bf385dd8,"We present an algorithm for surface reconstruction from a point cloud. It runs in O(nlog n) time, where n is the number of sample points, and this is optimal in the pointer machine model. The only existing O(nlog n)-Time algorithm is due to Funke and Ramos, and it uses some sophisticated data structures. The key task is to extract a locally uniform subsample from the input points. Our algorithm is much simpler and it is based on a variant of the standard octree. We built a prototype that runs an implementation of our algorithm to extract a locally uniform subsample, invokes Cocone to reconstruct a surface from the subsample, and adds back the sample points absent from the subsample via edge flips. In our experiments with some nonuniform samples, the subsample extraction step is fast and effective, and the prototype gives a 51% to 68% speedup over using Cocone alone. The prototype also runs faster on locally uniform samples.",Delaunay triangulation; Octree; Sampling; Surface reconstruction,Extraction; Sampling; Delau-nay triangulations; Nonuniform samples; Number of samples; Octree; Pointer machines; Surface from; Surface reconstruction algorithms; Time algorithms; Surface reconstruction
An improved approximation for k-median and positive correlation in budgeted optimization,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017170753&doi=10.1145%2f2981561&partnerID=40&md5=b2862c3ece9ee68e749a9f87cec6fcb9,"Dependent rounding is a useful technique for optimization problems with hard budget constraints. This framework naturally leads to negative correlation properties. However, what if an application naturally calls for dependent rounding on the one hand and desires positive correlation on the other? More generally, we develop algorithms that guarantee the known properties of dependent rounding but also have nearly bestpossible behavior-near-independence, which generalizes positive correlation-on ""small"" subsets of the variables. The recent breakthrough of Li and Svensson for the classical k-median problem has to handle positive correlation in certain dependent rounding settings, and does so implicitly. We improve upon Li-Svensson's approximation ratio for k-median from 2.732 + ϵ to 2.675 + ϵ by developing an algorithm that improves upon various aspects of their work. Our dependent rounding approach helps us improve the dependence of the runtime on the parameter ϵ from Li-Svensson's No(1/ϵ2) to No(1/ϵ)log(1/ϵ)). © 2017 ACM.",Approximation algorithms; Combinatorial optimization; Dependent rounding; Facility location problems; K-median,Approximation algorithms; Budget control; Combinatorial optimization; Lithium; Approximation ratios; Budget constraint; Dependent rounding; Facility location problem; K-median; Negative correlation; Optimization problems; Positive correlations; Optimization
"Submatrix maximum queries in monge matrices and partial monge matrices, and their applications",2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017170593&doi=10.1145%2f3039873&partnerID=40&md5=6511862790974e29b996398654c07655,"We describe a data structure for submatrix maximum queries in Monge matrices or partial Monge matrices, where a query seeks the maximum element in a contiguous submatrix of the given matrix. The structure, for an n×n Monge matrix, takes O(nlog n) space and O(nlog n) preprocessing time, and answers queries in O(log2 n) time. For partial Mongematrices, the space grows by α(n), the preprocessing grows by α(n) logn(α(n) is the inverse Ackermann function), and the query remains O(log2 n). Our design exploits an interpretation of the column maxima in a Monge (partial Monge, respectively) matrix as an upper envelope of pseudo-lines (pseudo-segments, respectively). We give two applications: (1) For a planar set of n points in an axis-parallel rectangle B, we build a data structure, in O(nα(n) log4 n) time and O(nα(n) log3 n) space, that returns, for a query point p, the largest-Area empty axis-parallel rectangle contained in Band containing p, in O(log4 n) time. This improves substantially the nearly quadratic storage and preprocessing obtained by Augustine et al. [2010]. (2) Given an n-node arbitrarily weighted planar digraph, with possibly negative edge weights, we build, in O(nlog2 n/ log log n) time, a linear-size data structure that supports edge-weight updates and graph-distance queries between arbitrary pairs of nodes in O(n2/3 log5/3 n) time per operation. This improves a previous algorithm of Fakcharoenphol and Rao [2006]. Our data structure has already been applied in a recent maximum flow algorithm for planar graphs in Borradaile et al. [2011]. © 2017 ACM 1549-6325/2017/03-ART26 15.00.",Dynamic Distance Oracle; Empty Rectangles; Monge Matrix; Planar Graphs; Pseudo-Line; Pseudo-Segment; Range Query,Data structures; Digital storage; Flow graphs; Geometry; Graph theory; Graphic methods; Inverse problems; Empty Rectangles; Monge matrices; Planar graph; Pseudo-lines; Pseudo-Segment; Range query; Matrix algebra
Smoothed analysis of local search for the maximum-cut problem,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017179697&doi=10.1145%2f3011870&partnerID=40&md5=47471b6737e691803f80b8cb7bdfd612,"Even though local search heuristics are the method of choice in practice for many well-studied optimization problems, most of them behave poorly in the worst case. This is, in particular, the case for the Maximum-Cut Problem, for which local search can take an exponential number of steps to terminate and the problem of computing a local optimum is PLS-complete. To narrow the gap between theory and practice, we study local search for the Maximum-Cut Problem in the framework of smoothed analysis in which inputs are subject to a small amount of random noise. We show that the smoothed number of iterations is quasi-polynomial, that is, it is bounded from above by a polynomial in nlog n and f, where n denotes the number of nodes and f denotes the perturbation parameter. This shows that worst-case instances are fragile, and it is a first step in explaining why they are rarely observed in practice. © 2017 ACM.",Local search; Maximum-cut problem; Smoothed analysis,Computation theory; Heuristic algorithms; Heuristic methods; Local search (optimization); Traveling salesman problem; Local search; Local search heuristics; Maximum cut problems; Number of iterations; Optimization problems; Perturbation parameters; Smoothed analysis; Worst-case instances; Optimization
Property testing on product distributions: Optimal testers for bounded derivative properties,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017174577&doi=10.1145%2f3039241&partnerID=40&md5=5d30e572de6166ef7dd3a1daa15ead89,"The primary problem in property testing is to decide whether a given function satisfies a certain property or is far from any function satisfying it. This crucially requires a notion of distance between functions. The most prevalent notion is the Hamming distance over the uniform distribution on the domain. This restriction to uniformity is rather limiting, and it is important to investigate distances induced by more general distributions. In this article, we provide simple and optimal testers for bounded derivative properties over arbitrary product distributions. Bounded derivative properties include fundamental properties, such as monotonicity and Lipschitz continuity. Our results subsume almost all known results (upper and lower bounds) on monotonicity and Lipschitz testing over arbitrary ranges. We prove an intimate connection between bounded derivative property testing and binary search trees (BSTs). We exhibit a tester whose query complexity is the sum of expected depths of optimal BSTs for each marginal. Furthermore, we show that this sum-of-depths is also a lower bound. A technical contribution of our work is an optimal dimension reduction theorem for all bounded derivative properties that relates the distance of a function from the property to the distance of restrictions of the function to random lines. Such a theorem has been elusive even for monotonicity, and our theorem is an exponential improvement to the previous best-known result. © 2017 ACM.",Lipschitz continuity; Monotonicity; Property testing,Binary trees; Fundamental properties; Lipschitz continuity; Monotonicity; Product distributions; Property-testing; Technical contribution; Uniform distribution; Upper and lower bounds; Hamming distance
Representative families of product families,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017143623&doi=10.1145%2f3039243&partnerID=40&md5=68e448c9cd21f8f85619896fb2485e17,"A subfamily F' of a set family F is said to q-represent F if for every A s F and B of size q such that A n B = φ there exists a set A' s F' such that A' ∩ B = φ. Recently, we provided an algorithm that, for a given family F of sets of size p together with an integer q, efficiently computes a q-representative family F of F of size approximately (p+qp). In this article, we consider the efficient computation of q-representative families for product families F. A family F is a product family if there exist families A and B such that F = {A ∪ B: A ϵ A, B ϵ B, A n B = φ}. Our main technical contribution is an algorithm that, given A, B and q, computes a q-representative family F' of F. The running time of our algorithm is sublinear in ·F· for many choices of A, B, and q that occur naturally in several dynamic programming algorithms. We also give an algorithm for the computation of q-representative families for product families F in the more general setting where q-representation also involves independence in a matroid in addition to disjointness. This algorithm considerably outperforms the naive approach where one first computes F from A and B and then computes the q-representative family F' from F. We give two applications of our new algorithms for computing q-representative families for product families. The first is a 3.8408k no(1) deterministic algorithm for the Multilinear Monomial Detection (k-MlD) problem. The second is a significant improvement of deterministic dynamic programming algorithms for ""connectivity problems"" on graphs of bounded treewidth. © 2017 ACM.",Matroids; Multilinear monomial detection; Parameterized algorithms; Representative families; Tree-width bounded graphs,Combinatorial mathematics; Matrix algebra; Trees (mathematics); Connectivity problems; Deterministic algorithms; Dynamic programming algorithm; Efficient computation; Parameterized algorithm; Representative families; Technical contribution; Tree-width; Dynamic programming
Uniform kernelization complexity of hitting forbidden minors,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017114059&doi=10.1145%2f3029051&partnerID=40&md5=09430254ab2a75e1bed923afd7d0562c,"The F-MINOR-FREE DELETION problem asks, for a fixed set F and an input consisting of a graph G and integer k, whether κ vertices can be removed from G such that the resulting graph does not contain any member of F as a minor. At FOCS 2012, Fomin et al. showed that the special case when F contains at least one planar graph has a kernel of size f (F) · κg(F) for some functions f and g. They left open whether this PLANAR F-MINOR-FREE DELETION problem has kernels whose size is uniformly polynomial, of the form f (F) · κc for some universal constant c. We prove that some PLANAR F-MINOR-FREE DELETION problems do not have uniformly polynomial kernels (unless NP ⊆ coNP/poly), not even when parameterized by the vertex cover number. On the positive side, we consider the problem of determining whether κ vertices can be removed to obtain a graph of treedepth at most η. We prove that this problem admits uniformly polynomial kernels with O(κ6) vertices for every fixed η. © 2017 ACM.",Kernelization; Lower bounds; Minor-free deletion; Treedepth,Forestry; Polynomials; Kernelization; Lower bounds; Minor-free deletion; Parameterized; Polynomial kernels; Positive sides; Tree-depth; Universal constants; Graph theory
Discovering archipelagos of tractability for constraint satisfaction and counting,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017135911&doi=10.1145%2f3014587&partnerID=40&md5=8f30e9165a9ca729c9a15c32c8c98cb7,"The Constraint Satisfaction Problem (CSP) is a central and generic computational problem which provides a common framework for many theoretical and practical applications. A central line of research is concerned with the identification of classes of instances for which CSP can be solved in polynomial time; such classes are often called ""islands of tractability."" A prominent way of defining islands of tractability for CSP is to restrict the relations that may occur in the constraints to a fixed set, called a constraint language, whereas a constraint language is conservative if it contains all unary relations. Schaefer's famous Dichotomy Theorem (STOC 1978) identifies all islands of tractability in terms of tractable constraint languages over a Boolean domain of values. Since then,many extensions and generalizations of this result have been obtained. Recently, Bulatov (TOCL 2011, JACM 2013) gave a full characterization of all islands of tractability for CSP and the counting version #CSP that are defined in terms of conservative constraint languages. This article addresses the general limit of the mentioned tractability results for CSP and #CSP, that they only apply to instances where all constraints belong to a single tractable language (in general, the union of two tractable languages is not tractable). We show that we can overcome this limitation as long as we keep some control of how constraints over the various considered tractable languages interact with each other. For this purpose, we utilize the notion of a strong backdoor of a CSP instance, as introduced byWilliams et al. (IJCAI 2003), which is a set of variables that when instantiated, moves the instance to an island of tractability, that is, to a tractable class of instances. We consider strong backdoors into scattered classes, consisting of CSP instances where each connected component belongs entirely to some class from a list of tractable classes. Figuratively speaking, a scattered class constitutes an archipelago of tractability. The main difficulty lies in finding a strong backdoor of given size k; once it is found, we can try all possible instantiations of the backdoor variables and apply the polynomial time algorithms associated with the islands of tractability on the list component-wise. Ourmain result is an algorithm that, given a CSP instance with nvariables, finds in time f (k)nO(1) a strong backdoor into a scattered class (associated with a list of finite conservative constraint languages) of size k or correctly decides that there is not such a backdoor. This also gives the running time for solving (#)CSP, provided that (#)CSP is polynomial-time tractable for the considered constraint languages. Our result makes significant progress towards the main goal of the backdoor-based approach to CSPs-the identification of maximal base classes for which small backdoors can be detected efficiently. © 2017 ACM.",Backdoor sets; Constraint languages; Constraint satisfaction; Counting; Islands of tractability,Computation theory; Polynomial approximation; Polynomials; Backdoor sets; Constraint language; Constraint Satisfaction; Counting; Islands of tractability; Constraint satisfaction problems
Asymptotically optimal encodings of range data structures for selection and top-k queries,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017109865&doi=10.1145%2f3012939&partnerID=40&md5=f3772a57283f478842b02457a90b2ab4,"Given an array A[1,n] of elements with a total order,we consider the problem of building a data structure that solves two queries: a) selection queries receive a range [i,j] and an integer k and return the position of the kth largest element in A[i,j], b) top-k queries receive [i,j] and k and return the positions of the k largest elements in A[i,j]. These problems can be solved in optimal time,O(1 + lg k/lg lg n) and O(k),respectively,using linear-space data structures. We provide the first study of the encoding data structures for the above problems,where A cannot be accessed at query time. Several applications are interested in the relative order of the entries of A,and their positions,rather their actual values,and thus we do not need to keep A at query time. In those cases,encodings save storage space: we first show that any encoding answering such queries requires nlg k - O(n + klg k) bits of space,then,we design encodings using O(nlg k) bits,that is,asymptotically optimal up to constant factors,while preserving optimal query time. ©2017 ACM.",Encoding data structures; Range minimum queries; Range search data structures; Succinct data structures,Data structures; Encoding (symbols); Information retrieval; Query processing; Asymptotically optimal; Constant factors; Encoding data; Range minimum queries; Range search; Relative order; Storage spaces; Succinct data structure; Digital storage
Time vs. information tradeoffs for leader election in anonymous trees,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014669777&doi=10.1145%2f3039870&partnerID=40&md5=c8a4d0445417fede2fd3c5efe5ffa6cc,"Leader election is one of the fundamental problems in distributed computing. It calls for all nodes of a network to agree on a single node, called the leader. If the nodes of the network have distinct labels, then agreeing on a single node means that all nodes have to output the label of the elected leader. If the nodes of the network are anonymous, the task of leader election is formulated as follows: every node v of the network must output a simple path, which is coded as a sequence of port numbers, such that all these paths end at a common node, the leader. In this article, we study deterministic leader election in anonymous trees. Our aim is to establish tradeoffs between the allocated time φ and the amount of information that has to be given apriorito the nodes to enable leader election in time τ in all trees for which leader election in this time is at all possible. Following the framework of algorithms with advice, this information (a single binary string) is provided to all nodes at the start by an oracle knowing the entire tree. The length of this string is called the size of advice. For a given time τ allocated to leader election, we give upper and lower bounds on the minimum size of advice sufficient to perform leader election in time τ. For most values of τ, our upper and lower bounds are either tight up to multiplicative constants, or they differ only by a logarithmic factor. Let T be an n-node tree of diameter diam ≤ D. While leader election in time diam can be performed without any advice, for time diam - 1 we give tight upper and lower bounds of Θ(log D). For time diam - 2 we give tight upper and lower bounds of Θ(log D) for even values of diam, and tight upper and lower bounds of Θ(log n) for odd values of diam. Moving to shorter time, in the interval [β · diam, diam - 3] for constant β > 1/2, we prove an upper bound of O(n log n/D) and a lower bound of Ω(n/D), the latter being valid whenever diam is odd or when the time is at most diam - 4. Hence, with the exception of the special case when diam is even and time is exactly diam - 3, our bounds leave only a logarithmic gap in this time interval. Finally, for time α · diam for any constant α < 1/2 (except for the case of very small diameters), we again give tight upper and lower bounds, this time Θ(n). © 2017 ACM.",Advice; Deterministic distributed algorithm; Leader election; Time; Trees,Commerce; Distributed computer systems; Forestry; Trees (mathematics); Advice; Deterministic distributed algorithms; Leader election; Time; Trees; Binary trees
For-all sparse recovery in near-optimal time,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014734583&doi=10.1145%2f3039872&partnerID=40&md5=d8b44f839902a0727dd5df4795962a7f,"An approximate sparse recovery system in ℓ1 norm consists of parameters k, ε, N; an m-by-N measurement Φ; and a recovery algorithm R. Given a vector, x, the system approximates x by x̂ = R(Ωx), which must satisfy ||x̂-x||1 ≤ (1+ε)||x-xk||1. We consider the ""for all"" model, in which a single matrix Φ, possibly ""constructed"" non-explicitly using the probabilistic method, is used for all signals x. The best existing sublinear algorithm by Porat and Strauss [2012] uses O(ε-3k log(N/k)) measurements and runs in time O(k1-αNα) for any constant α > 0. In this article, we improve the number of measurements to O(ε-2k log(N/k)), matching the best existing upper bound (attained by super-linear algorithms), and the runtime to O(k1+β poly(log N, 1/ε)), with a modest restriction that k ≤ N1-α and ε ≤ (log k/ log N)γ for any constants α, β, γ > 0. When k ≤ logc N for some c > 0, the runtime is reduced to O(k poly(N, 1/ε)). With no restrictions on ε, we have an approximation recovery system with m = O(k/ε log(N/k)((log N/ log k)γ + 1/ε)) measurements. The overall architecture of this algorithm is similar to that of Porat and Strauss [2012] in that we repeatedly use a weak recovery system (with varying parameters) to obtain a top-level recovery algorithm. The weak recovery system consists of a two-layer hashing procedure (or with two unbalanced expanders for a deterministic algorithm). The algorithmic innovation is a novel encoding procedure that is reminiscent of network coding and that reflects the structure of the hashing stages. The idea is to encode the signal position index i by associating it with a unique message mi, which will be encoded to a longer message mi′ (in contrast to Porat and Strauss [2012] in which the encoding is simply the identity). Portions of the message mi′ correspond to repetitions of the hashing, and we use a regular expander graph to encode the linkages among these portions. The decoding or recovery algorithm consists of recovering the portions of the longer messages mi′ and then decoding to the original messages mi, all the while ensuring that corruptions can be detected and/or corrected. The recovery algorithm is similar to list recovery introduced in Indyk et al. [2010] and used in Gilbert et al. [2013]. In our algorithm, the messages {mi} are independent of the hashing, which enables us to obtain a better result. © 2017 ACM.",Compressive sensing; List decoding; Sparse recovery,Decoding; Encoding (symbols); Network coding; Recovery; Compressive sensing; Deterministic algorithms; List decoding; Probabilistic methods; Recovery algorithms; Sparse recovery; Sublinear algorithm; Varying parameters; Parameter estimation
Nearly optimal deterministic algorithm for sparse walsh-hadamard transform,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014651142&doi=10.1145%2f3029050&partnerID=40&md5=fd2611972fb6e158f33cef045a87cb5d,"For every fixed constant α > 0, we design an algorithm for computing the k-sparse Walsh-Hadamard transform (i.e., Discrete Fourier Transform over the Boolean cube) of an N-dimensional vector x ∈ ℝN in time k1+α(log N)O(1). Specifically, the algorithm is given query access to x and computes a k-sparse x̃ ∈ ℝN satisfying ||x̃ - x̂||1 ≤ c||x̂ - Hk(x̂)||1 for an absolute constant c > 0, where x̂ is the transform of x and Hk(x̂) is its best k-sparse approximation. Our algorithm is fully deterministic and only uses nonadaptive queries to x (i.e., all queries are determined and performed in parallel when the algorithm starts). An important technical tool that we use is a construction of nearly optimal and linear lossless condensers, which is a careful instantiation of the GUV condenser (Guruswami et al. [2009]). Moreover, we design a deterministic and nonadaptive ℓ1/ℓ1 compressed sensing scheme based on general lossless condensers that is equipped with a fast reconstruction algorithm running in time k1+α(log N)O(1) (for the GUV-based condenser) and is of independent interest. Our scheme significantly simplifies and improves an earlier expander-based construction due to Berinde, Gilbert, Indyk, Karloff, and Strauss [Berinde et al. 2008]. Our methods use linear lossless condensers in a black box fashion; therefore, any future improvement on explicit constructions of such condensers would immediately translate to improved parameters in our framework (potentially leading to k(log N)O(1) reconstruction time with a reduced exponent in the polylogarithmic factor, and eliminating the extra parameter α). By allowing the algorithm to use randomness while still using nonadaptive queries, the runtime of the algorithm can be improved to Õ(k log3 N). © 2017 ACM.",Explicit constructions; Pseudorandomness; Sketching; Sparse Fourier transform; Sparse recovery; Sublinear time algorithms,Approximation algorithms; Discrete Fourier transforms; Mathematical transformations; Random processes; Explicit constructions; Pseudorandomness; Sketching; Sparse recovery; Sublinear time algorithms; Hadamard transforms
"Generating random permutations by coin tossing: Classical algorithms, new analysis, and modern implementation",2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017247870&doi=10.1145%2f3009909&partnerID=40&md5=4de92734fa409d1779776469274ed12f,"Several simple, classical, little-known algorithms in the statistics and computer science literature for generating random permutations by coin tossing are examined, analyzed, and implemented. These algorithms are either asymptotically optimal or close to being so in terms of the expected number of times the random bits are generated. In addition to asymptotic approximations to the expected complexity, we also clarify the corresponding variances, as well as the asymptotic distributions. A brief comparative discussion with numerical computations in a multicore system is also given.",Analysis of algorithms; Asymptotic distribution; Generating functions; Hardware random number generator; Mellin transform; Multithreading; Random number generator; Random permutation; Uniform distribution; Variance,Asymptotic analysis; Number theory; Statistical tests; Analysis of algorithms; Asymptotic distributions; Generating functions; Mellin transform; Multi-threading; Random number generators; Random permutations; Uniform distribution; Variance; Random number generation
Algorithmic and enumerative aspects of the Moser-Tardos distribution,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014695109&doi=10.1145%2f3039869&partnerID=40&md5=73cd6e713f8c36c88079316aacd4fb1c,"Moser and Tardos have developed a powerful algorithmic approach (henceforth MT) to the Lovász Local Lemma (LLL); the basic operation done in MT and its variants is a search for ""bad"" events in a current configuration. In the initial stage of MT, the variables are set independently. We examine the distributions on these variables that arise during intermediate stages of MT. We show that these configurations have a more or less ""random"" form, building further on the MT-distribution concept of Haeupler et al. in understanding the (intermediate and) output distribution of MT. This has a variety of algorithmic applications; the most important is that bad events can be found relatively quickly, improving on MT across the complexity spectrum. It makes some polynomial-time algorithms sublinear (e.g., for Latin transversals, which are of basic combinatorial interest), gives lower-degree polynomial runtimes in some settings, transforms certain superpolynomial-time algorithms into polynomial-time algorithms, and leads to Las Vegas algorithms for some coloring problems for which only Monte Carlo algorithms were known. We show that, in certain conditions when the LLL condition is violated, a variant of the MT algorithm can still produce a distribution that avoids most of the bad events. We show in some cases that this MT variant can run faster than the original MT algorithm itself and develop the first-known criterion for the case of the asymmetric LLL. This can be used to find partial Latin transversals - improving on earlier bounds of Stein (1975) - among other applications. We furthermore give applications in enumeration, showing that most applications (for which we aim for all or most of the bad events to be avoided) have large solution sets. We do this by showing that the MT distribution has large Rényi entropy. © 2017 ACM.",Combinatorial enumeration; Graph coloring; Latin transversals; LLL distribution; Lovász local lemma; Moser-Tardos algorithm; MT distribution; Satisfiability,Parallel processing systems; Polynomial approximation; Polynomials; Program processors; Combinatorial enumeration; Graph colorings; Latin transversals; LLL distribution; Local lemmata; MT distribution; Satisfiability; Combinatorial mathematics
Faster and simpler sketches of valuation functions,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014644669&doi=10.1145%2f3039871&partnerID=40&md5=d1b867f88f468a01126a5563899895bc,"We present fast algorithms for sketching valuation functions. Let N (|N| = n) be some ground set and v : 2N → ℝ be a function. We say that ṽ : 2N → ℝ is an α-sketch of v if for every set S we have that v(S)/α ≤ ṽ(S) ≤ v(S) and ṽ can be described in poly(n) bits. Goemans et al. [SODA'09] showed that if v is submodular then there exists an Õ(√n)-sketch that can be constructed using polynomially many value queries (this is essentially the best possible, as Balcan and Harvey [STOC'11] show that no submodular function admits an n1/3-ε-sketch). Based on their work, Balcan et al. [COLT'12] and Badanidiyuru et al. [SODA'12] show that if v is subadditive, then there exists an Õ(√n)-sketch that can be constructed using polynomially many demand queries. All previous sketches are based on complicated geometric constructions. The first step in their constructions is proving the existence of a good sketch by finding an ellipsoid that ""approximates"" v well (this is done by applying John's theorem to ensure the existence of an ellipsoid that is ""close"" to the polymatroid that is associated with v). The second step is to show that this ellipsoid can be found efficiently, and this is done by repeatedly solving a certain convex program to obtain better approximations of John's ellipsoid. In this article, we give a significantly simpler, nongeometric proof for the existence of good sketches and utilize the proof to obtain much faster algorithms that match the previously obtained approximation bounds. Specifically, we provide an algorithm that finds Õ(√n)-sketch of a submodular function with only Õ(n3/2) value queries, and we provide an algorithm that finds Õ(√n)-sketch of a subadditive function with O(n) demand and value queries. © 2017 ACM.",Function sketching; Submodular optimization,Aluminum; Approximation algorithms; Convex optimization; Optimization; Approximation bounds; Convex programs; Fast algorithms; Geometric construction; Subadditive function; Submodular functions; Submodular optimizations; Valuation function; Query processing
Dynamic facility location via exponential clocks,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017153248&doi=10.1145%2f2928272&partnerID=40&md5=95eab8b725a0ae6a7111755d7e93ec70,"The dynamic facility location problem is a generalization of the classic facility location problem proposed by Eisenstat, Mathieu, and Schabanel to model the dynamics of evolving social/infrastructure networks. The generalization lies in that the distance metric between clients and facilities changes over time. This leads to a trade-off between optimizing the classic objective function and the ""stability"" of the solution: There is a switching cost charged every time a client changes the facility to which it is connected. While the standard linear program (LP) relaxation for the classic problem naturally extends to this problem, traditional LP-rounding techniques do not, as they are often sensitive to small changes in the metric resulting in frequent switches. We present a new LP-rounding algorithm for facility location problems, which yields the first constant approximation algorithm for the dynamic facility location problem. Our algorithm installs competing exponential clocks on the clients and facilities and connects every client by the path that repeatedly follows the smallest clock in the neighborhood. The use of exponential clocks gives rise to several properties that distinguish our approach from previous LP roundings for facility location problems. In particular, we use no clustering and we allow clients to connect through paths of arbitrary lengths. In fact, the clustering-free nature of our algorithm is crucial for applying our LP-rounding approach to the dynamic problem. © 2017 ACM.",Approximation algorithms; Exponential clocks; Facility location problems,Approximation algorithms; Clocks; Economic and social effects; Linear programming; Location; Site selection; Constant approximation algorithms; Distance metrics; Dynamic problem; Facility location problem; Facility locations; Linear programs; Objective functions; Switching costs; Clustering algorithms
On uniform capacitated k-median beyond the natural LP relaxation,2017,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010015625&doi=10.1145%2f2983633&partnerID=40&md5=fc9ba00c00ee7979b3d6b7d65721e382,"In this article, we study the uniform capacitated k-median (CKM) problem. In the problem, we are given a set F of potential facility locations, a set C of clients, a metric d over F ∪ C, an upper bound k on the number of facilities that we can open, and an upper bound u on the number of clients that each facility can serve. We need to open a subset S ⊆ F of k facilities and connect clients in C to facilities in S so that each facility is connected by at most u clients. The goal is to minimize the total connection cost over all clients. Obtaining a constant approximation algorithm for this problem is a notorious open problem; most previous works gave constant approximations by either violating the capacity constraints or the cardinality constraint. Notably, all of these algorithms are based on the natural LP relaxation for the problem. The LP relaxation has unbounded integrality gap, even when we are allowed to violate the capacity constraints or the cardinality constraint by a factor of 2 - ϵ. Our result is an exp(O(1/ϵ2))-approximation algorithm for the problem that violates the cardinality constraint by a factor of 1 + ϵ. In other words, we find a solution that opens at most (1 + ϵ)k facilities whose cost is at most exp(O(1/ϵ2)) times the optimum solution when at most k facilities can be open. This is already beyond the capability of the natural LP relaxation, as it has unbounded integrality gap even if we are allowed to open (2 - ϵ)k facilities. Indeed, our result is based on a novel LP for this problem. It is our hope that this LP is the first step toward a constant approximation for CKM. The version that we described is the hard capacitated version of the problem, as we can only open one facility at each location. This is as opposed to the soft capacitated version, in which we are allowed to open more than one facility at each location. The hard capacitated version is more general, since one can convert a soft capacitated instance to a hard capacitated instance by making enough copies of each facility location. We give a simple proof that in the uniform capacitated case, the soft capacitated version and the hard capacitated version are actually equivalent, up to a small constant loss in the approximation ratio. © 2017 ACM.",Approximation algorithms; Capacitated k-median; Pseudoapproximation; Theory,Location; Approximation ratios; Capacity constraints; Cardinality constraints; Constant approximation algorithms; Facility locations; K-median; Pseudoapproximation; Theory; Approximation algorithms
A new approach to online scheduling: Approximating the optimal competitive ratio,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008889412&doi=10.1145%2f2996800&partnerID=40&md5=fdf3fd3e40583fdecd0a099da1c471d1,"We propose a new approach to competitive analysis in online scheduling by introducing the novel concept of competitive-ratio approximation schemes. Such a scheme algorithmically constructs an online algorithm with a competitive ratio arbitrarily close to the best possible competitive ratio for any online algorithm. We study the problem of scheduling jobs online to minimize the weighted sum of completion times on parallel, related, and unrelated machines, and we derive both deterministic and randomized algorithms that are almost best possible among all online algorithms of the respective settings. We also generalize our techniques to arbitrary monomial cost functions and apply them to the makespan objective. Our method relies on an abstract characterization of online algorithms combined with various simplifications and transformations. We also contribute algorithmic means to compute the actual value of the best possible competitive ratio up to an arbitrary accuracy. This strongly contrasts with nearly all previous manually obtained competitiveness results, and, most importantly, it reduces the search for the optimal competitive ratio to a question that a computer can answer. We believe that our concept can also be applied to many other problems and yields a new perspective on online algorithms in general. © 2016 ACM.","Algorithms; Competitive analysis; Design; F.1.2 [computation by abstract devices]: modes of Computation - online computation; F.2.2 [analysis of algorithms and problem complexity]: nonnumerical algorithms and problems - computations on discrete structures; G.2.2 [discrete mathematics]: combinatorics - combinatorial algorithms; G.2.3 [discrete mathematics]: applications; I.2.8 [artifical intelligence]: problem solving, control methods and search - scheduling; Jobs arrive over time; Makespan; Min-sum objective; Online scheduling; Theory",Algorithms; Computational complexity; Cost functions; Design; Scheduling; Combinatorics; Competitive analysis; Control methods; Discrete mathematics; Jobs arrive over time; Makespan; Min-sum; Online computations; Online scheduling; Problem complexity; Theory; Problem solving
Lopsidependency in the Moser-Tardos framework: Beyond the Lopsided Lovász Local Lemma,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008871245&doi=10.1145%2f3015762&partnerID=40&md5=27c953f6dd0836506310c5738faf8189,"The Lopsided Lovász Local Lemma (LLLL) is a powerful probabilistic principle that has been used in a variety of combinatorial constructions. While this principle began as a general statement about probability spaces, it has recently been transformed into a variety of polynomial-time algorithms. The resampling algorithm of Moser and Tardos [2010] is the most well-known example of this. A variety of criteria have been shown for the LLLL; the strongest possible criterion was shown by Shearer, and other criteria that are easier to use computationally have been shown by Bissacot et al. [2011], Pegden [2014], Kolipaka and Szegedy [2011], and Kolipaka et al. [2012]. We show a new criterion for the Moser-Tardos algorithm to converge. This criterion is stronger than the LLLL criterion, and, in fact, can yield better results even than the full Shearer criterion. This is possible because it does not apply in the same generality as the original LLLL; yet, it is strong enough to cover many applications of the LLLL in combinatorics. We show a variety of new bounds and algorithms. A noteworthy application is for k-SAT, with bounded occurrences of variables. As shown in Gebauer et al. [2011], a k-SAT instance in which every variable appears L ≤ 2k+1/e(k+1) times, is satisfiable. Although this bound is asymptotically tight (in k), we improve it to L ≤ 2k+1(1-1/k)k/k-1 - 2/k, which can be significantly stronger when k is small. We introduce a new parallel algorithm for the LLLL. While Moser and Tardos described a simple parallel algorithm for the Lovász Local Lemma and described a simple sequential algorithm for a form of the Lopsided Lemma, they were not able to combine the two. Our new algorithm applies in nearly all settings in which the sequential algorithm works - this includes settings covered by our new, stronger LLLL criterion. © 2016 ACM.",Hamiltonian cycle; Independent transversals; Lopsided Lovasz local lemma; Lovasz local lemma; Moser-Tardos algorithm; Ramsey number,Hamiltonians; Parallel algorithms; Polynomial approximation; Redundant manipulators; Sequential switching; Hamiltonian cycle; Independent transversals; Lovasz local lemma; Polynomial-time algorithms; Probability spaces; Ramsey numbers; Resampling algorithms; Sequential algorithm; Aluminum
Semi-streaming set cover,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997080090&doi=10.1145%2f2957322&partnerID=40&md5=04d1b29eba402ac11fff101e961fc1ae,"This article studies the set cover problem under the semi-streaming model. The underlying set system is formalized in terms of a hypergraph G = (V, E) whose edges arrive one by one, and the goal is to construct an edge cover F ⊆ E with the objective of minimizing the cardinality (or cost in the weighted case) of F. We further consider a parameterized relaxation of this problem, where, given some 0 ≤ ε < 1, the goal is to construct an edge (1 - ε)-cover, namely, a subset of edges incident to all but an ε-fraction of the vertices (or their benefit in the weighted case). The key limitation imposed on the algorithm is that its space is limited to (poly)logarithmically many bits per vertex. Our main result is an asymptotically tight tradeoff between ε and the approximation ratio: We design a semi-streaming algorithm that on input hypergraph G constructs a succinct data structure D such that for every 0 ≤ ε < 1, an edge (1 - ε)-cover that approximates the optimal edge (1-)cover within a factor of f (ε, n) can be extracted from D (efficiently and with no additional space requirements), where f (ε, n) = {O(1/ε), if ε > 1/√n O(√n), otherwise. In particular, for the traditional set cover problem, we obtain an O(√n)-approximation. This algorithm is proved to be best possible by establishing a family (parameterized by ε) of matching lower bounds. © 2016 ACM.",Algorithms; F.2.0 [analysis of algorithms and complexity]: general; Lower bounds; Set cover; Streaming algorithms; Theory,Algorithms; Computational complexity; Analysis of algorithms; Lower bounds; Set cover; Streaming algorithm; Theory; Approximation algorithms
Cheeger-type approximation for sparsest st-cut,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997170293&doi=10.1145%2f2996799&partnerID=40&md5=3b683c22fdd3271f0103790b06a6aafa,"We introduce the st-cut version of the sparsest-cut problem, where the goal is to find a cut of minimum sparsity in a graph G(V, E) among those separating two distinguished vertices s, t ∈ V . Clearly, this problem is at least as hard as the usual (non-st) version. Our main result is a polynomial-time algorithm for the product-demands setting that produces a cut of sparsity O(√OPT), where OPT ≤ 1 denotes the optimum when the total edge capacity and the total demand are assumed (by normalization) to be 1. Our result generalizes the recent work of Trevisan [arXiv, 2013] for the non-st version of the same problem (sparsest cut with product demands), which in turn generalizes the bound achieved by the discrete Cheeger inequality, a cornerstone of Spectral Graph Theory that has numerous applications. Indeed, Cheeger's inequality handles graph conductance, the special case of product demands that are proportional to the vertex (capacitated) degrees. Along the way, we obtain an O(log|V|) approximation for the general-demands setting of sparsest st-cut. © 2016 ACM.",Cheeger inequality; Sparsest cut,Polynomial approximation; Cheeger; Cheeger inequalities; Cheeger's inequalities; Edge capacity; Polynomial-time algorithms; Product demand; Sparsest cut; Spectral graph theory; Graph theory
Algorithms for hub label optimization,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997498563&doi=10.1145%2f2996593&partnerID=40&md5=528880f7fb5bbecad91233d040d19b85,"We consider the hub label optimization problem, which arises in designing fast preprocessing-based shortest-path algorithms. We give O(log n)-approximation algorithms for the objectives of minimizing the maximum label size (ℓ∞-norm) and simultaneously minimizing a constant number of ℓp-norms. Prior to this, an O(log n)-approximation algorithm was known [Cohen et al. 2003] only for minimizing the total label size (ℓ1-norm). © 2016 ACM.",Distance oracles; Labeling algorithms,Optimization; Distance oracles; Labeling algorithms; Optimization problems; Shortest path algorithms; Approximation algorithms
Waste makes haste: Bounded time algorithms for envy-free cake cutting with free disposal,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997120385&doi=10.1145%2f2988232&partnerID=40&md5=0ab5768982f2e9990ba0b39fc0a3fd70,"We consider the classic problem of envy-free division of a heterogeneous good (""cake"") among several agents. It is known that, when the allotted pieces must be connected, the problem cannot be solved by a finite algorithm for three or more agents. The impossibility result, however, assumes that the entire cake must be allocated. In this article, we replace the entire-allocation requirement with a weaker partial-proportionality requirement: the piece given to each agent must be worth for it at least a certain positive fraction of the entire cake value. We prove that this version of the problem is solvable in bounded time even when the pieces must be connected. We present simple, bounded-time envy-free cake-cutting algorithms for (1) giving each of n agents a connected piece with a positive value; (2) giving each of three agents a connected piece worth at least 1/3; (3) giving each of four agents a connected piece worth at least 1/7; (4) giving each of four agents a disconnected piece worth at least 1/4; and (5) giving each of n agents a disconnected piece worth at least (1 - ε)/n for any positive ε. © 2016 ACM.",Cake-cutting; Envy-free; Fair division; Finite algorithm; Perfect matching,Problem solving; Waste disposal; Cake cuttings; Envy free; Fair divisions; Finite algorithm; Perfect matchings; Software agents
Minimum latency submodular cover,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997120333&doi=10.1145%2f2987751&partnerID=40&md5=c86897474129b82867ab027e21b534f8,"We study the Minimum Latency Submodular Cover (MLSC) problem, which consists of a metric (V, d) with source r ∈ V and m monotone submodular functions f1, f2,..., fm : 2V → [0, 1]. The goal is to find a path originating at r that minimizes the total ""cover time"" of all functions. This generalizes well-studied problems, such as Submodular Ranking [Azar and Gamzu 2011] and the Group Steiner Tree [Garg et al. 2000]. We give a polynomial time O(log1/ε · log2+δ |V|)-approximation algorithm for MLSC, where ε > 0 is the smallest non-zero marginal increase of any {fi}mi=1 and δ > 0 is any constant. We also consider the Latency Covering Steiner Tree (LCST) problem, which is the special case of MLSC where the fis are multi-coverage functions. This is a common generalization of the Latency Group Steiner Tree [Gupta et al. 2010; Chakrabarty and Swamy 2011] and Generalized Min-sum Set Cover [Azar et al. 2009; Bansal et al. 2010] problems. We obtain an O(log2 |V|)-approximation algorithm for LCST. Finally, we study a natural stochastic extension of the Submodular Ranking problem and obtain an adaptive algorithm with an O(log 1/ε)-approximation ratio, which is best possible. This result also generalizes some previously studied stochastic optimization problems, such as Stochastic Set Cover [Goemans and Vondrák 2006] and Shared Filter Evaluation [Munagala et al. 2007; Liu et al. 2008]. © 2016 ACM.",Approximation; Covering Steiner tree; Sequencing and scheduling; Stochastic optimization; Submodular,Adaptive algorithms; Aluminum; Approximation algorithms; Optimization; Polynomial approximation; Stochastic systems; Approximation; Sequencing and scheduling; Steiner trees; Stochastic optimizations; Submodular; Trees (mathematics)
How good is multi-pivot quicksort?,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992176837&doi=10.1145%2f2963102&partnerID=40&md5=b800b9e7ddfc18091d2b0baa35faff3f,"Multi-Pivot Quicksort refers to variants of classical quicksort where in the partitioning step k pivots are used to split the input into k + 1 segments. For many years, multi-pivot quicksort was regarded as impractical, but in 2009 a two-pivot approach by Yaroslavskiy, Bentley, and Bloch was chosen as the standard sorting algorithm in Sun's Java 7. In 2014 at ALENEX, Kushagra et al. introduced an even faster algorithm that uses three pivots. This article studies what possible advantages multi-pivot quicksort might offer in general. The contributions are as follows: Natural comparison-optimal algorithms for multi-pivot quicksort are devised and analyzed. The analysis shows that the benefits of using multiple pivots with respect to the average comparison count are marginal and these strategies are inferior to simpler strategies such as the wellknown median-of-k approach. A substantial part of the partitioning cost is caused by rearranging elements. A rigorous analysis of an algorithm for rearranging elements in the partitioning step is carried out, observing mainly how often array cells are accessed during partitioning. The algorithm behaves best if three to five pivots are used. Experiments show that this translates into good cache behavior and is closest to predicting observed running times of multi-pivot quicksort algorithms. Finally, it is studied how choosing pivots from a sample affects sorting cost. The study is theoretical in the sense that although the findings motivate design recommendations for multipivot quicksort algorithms that lead to running-time improvements over known algorithms in an experimental setting, these improvements are small. © 2016 ACM.",Multi-pivot; Quicksort; Sorting,Algorithms; Sorting; Cache behavior; Design recommendations; Multi-pivot; nocv1; Optimal algorithm; Partitioning step; Quicksort; Rigorous analysis; Sorting algorithm; Mathematical techniques
Better balance by being biased: A 0.8776-approximation for max bisection,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992195833&doi=10.1145%2f2907052&partnerID=40&md5=f0779a7c2b06e6962f02c0494ddc5b10,"Recently, Raghavendra and Tan (SODA 2012) gave a 0.85-approximation algorithm for the MAX BISECTION problem. We improve their algorithm to a 0.8776-approximation. As MAX BISECTION is hard to approximate within αGW + ε ≈ 0.8786 under the Unique Games Conjecture (UGC), our algorithm is nearly optimal. We conjecture that MAX BISECTION is approximable within αGW - ε, that is, that the bisection constraint (essentially) does not make MAX CUT harder. We also obtain an optimal algorithm (assuming the UGC) for the analogous variant of MAX 2-SAT. Our approximation ratio for this problem exactly matches the optimal approximation ratio for MAX 2-SAT, that is, αLLZ + ε ≈ 0.9401, showing that the bisection constraint does not make MAX 2-SAT harder. This improves on a 0.93-approximation for this problem from Raghavendra and Tan. © 2016 ACM.",Approximation algorithms; Max-bisection; Semidefinite programming,Algorithms; Mathematical techniques; Approximation ratios; MAX CUT; Max-bisection; nocv1; Optimal algorithm; Optimal approximation; Semi-definite programming; Unique games conjecture; Approximation algorithms
2-Edge connectivity in directed graphs,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992175762&doi=10.1145%2f2968448&partnerID=40&md5=1678699fb4a252398f07e912c54e7a6f,"Edge and vertex connectivity are fundamental concepts in graph theory. While they have been thoroughly studied in the case of undirected graphs, surprisingly, not much has been investigated for directed graphs. In this article, we study 2-edge connectivity problems in directed graphs and, in particular, we consider the computation of the following natural relation: We say that two vertices v and w are 2-edge-connected if there are two edge-disjoint paths from v to w and two edge-disjoint paths from w to v. This relation partitions the vertices into blocks such that all vertices in the same block are 2-edge-connected. Differently from the undirected case, those blocks do not correspond to the 2-edge-connected components of the graph. The main result of this article is an algorithm for computing the 2-edge-connected blocks of a directed graph in linear time. Besides being asymptotically optimal, our algorithm improves significantly over previous bounds. Once the 2-edge-connected blocks are available, we can test in constant time if two vertices are 2-edge-connected. Additionally, when two query vertices v and w are not 2-edge-connected, we can produce in constant time a ""witness"" of this property by exhibiting an edge that is contained in all paths from v to w or in all paths from w to v. We are also able to compute in linear time a sparse certificate for this relation, i.e., a subgraph of the input graph that has O(n) edges and maintains the same 2-edge-connected blocks as the input graph, where n is the number of vertices. © 2016 ACM.",Connectivity; Directed graph; Dominators; Flow graph,Computation theory; Graphic methods; Undirected graphs; Connectivity; Constant time; Dominator; Edge connectivity; Edge disjoint paths; Flow-graphs; Fundamental concepts; Input graphs; Linear time; Vertex connectivity; Flow graphs
Nearest-neighbor searching under uncertainty II,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992168545&doi=10.1145%2f2955098&partnerID=40&md5=306d61dd5bdce49a6c9a39a5f50d0eed,"Nearest-neighbor search, which returns the nearest neighbor of a query point in a set of points, is an important and widely studied problem in many fields, and it has a wide range of applications. In many of them, such as sensor databases, location-based services, face recognition, and mobile data, the location of data is imprecise. We therefore study nearest-neighbor queries in a probabilistic framework in which the location of each input point is specified as a probability distribution function. We present efficient algorithms for (i) computing all points that are nearest neighbors of a query point with nonzero probability and (ii) estimating the probability of a point being the nearest neighbor of a query point, either exactly or within a specified additive error. © 2016 ACM.",Approximate nearest neighbor; Indexing uncertain data; Probabilistic nearest neighbor; Threshold queries,Distribution functions; Face recognition; Location; Location based services; Probability; Probability distributions; Telecommunication services; Additive errors; Indexing uncertain datum; Nearest neighbor queries; Nearest neighbors; Nearest-neighbor searching; Non-zero probability; Probabilistic framework; Threshold queries; Nearest neighbor search
Sparse fault-tolerant BFS structures,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992122587&doi=10.1145%2f2976741&partnerID=40&md5=3fd85dd5c063aac0ef10e4b2d03e8a23,"A fault-tolerant structure for a network is required for continued functioning following the failure of some of the network's edges or vertices. This article considers breadth-first search (BFS) spanning trees and addresses the problem of designing a sparse fault-tolerant BFS structure (FT-BFS structure), namely, a sparse subgraph T of the given network G such that subsequent to the failure of a single edge or vertex, the surviving part T? of T still contains a BFS spanning tree for (the surviving part of) G. For a source node s, a target node t, and an edge e ∈ G, the shortest s - t path Ps,t,e that does not go through e is known as a replacement path. Thus, our FT-BFS structure contains the collection of all replacement paths Ps,t,e for every t ∈ V (G) and every failed edge e ∈ E(G). Our main results are as follows. We present an algorithm that for every n-vertex graph G and source node s constructs a (single edge failure) FT-BFS structure rooted at s with O(n · min{Depth(s), √n}) edges, where Depth(s) is the depth of the BFS tree rooted at s. This result is complemented by a matching lower bound, showing that there exist n-vertex graphs with a source node s for which any edge (or vertex) FT-BFS structure rooted at s has Ω(n3/2) edges. We then consider fault-tolerant multi-source BFS structures (FT-MBFS structures), aiming to provide (following a failure) a BFS tree rooted at each source s ∈ S for some subset of sources S ⊆ V . Again, tight bounds are provided, showing that there exists a poly-time algorithm that for every n-vertex graph and source set S ⊆ V of size σ constructs a (single failure) FT-MBFS structure T∗(S) from each source si ∈ S, with O(√σ · n3/2) edges, and, on the other hand, there exist n-vertex graphs with source sets S ⊆ V of cardinality σ, on which any FT-MBFS structure from S has Ω(√σ · n3/2) edges. Finally, we propose an O(log n) approximation algorithm for constructing FT-BFS and FT-MBFS structures. The latter is complemented by a hardness result stating that there exists no Ω(log n) approximation algorithm for these problems under standard complexity assumptions. In comparison with previous constructions, our algorithm is deterministic and may improve the number of edges by a factor of up to √n for some instances. All our algorithms can be extended to deal with one vertex failure as well, with the same performance. © 2016 ACM.",Algorithms; C.2.1 [computer-communication networks]: Network architecture and design; Design; Fault tolerance; Reliability; Replacement paths; Single source shortest paths,Algorithms; Approximation algorithms; Complex networks; Design; Fault tolerance; Network architecture; Reliability; Trees (mathematics); Breadth-first search; Complexity assumptions; Computer communication networks; Fault-tolerant structures; Hardness result; Poly-time algorithms; Replacement paths; Single source shortest paths; Graph theory
Tabulating pseudoprimes and tabulating liars,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989177848&doi=10.1145%2f2957759&partnerID=40&md5=c561453eee4b702dea09f6f1801d5e9d,"This article explores the asymptotic complexity of two problems related to the Miller-Rabin-Selfridge primality test. The first problem is to tabulate strong pseudoprimes to a single fixed base a. It is now proven that tabulating up to x requires O(x) arithmetic operations and O(x log x) bits of space. The second problem is to find all strong liars and witnesses, given a fixed odd composite n. This appears to be unstudied, and a randomized algorithm is presented that requires an expected O((log n)2 + /S(n)/) operations (here S(n) is the set of strong liars). Although interesting in their own right, a notable application is the search for sets of composites with no reliable witnesses. © 2016 ACM.",Miller-Rabin primality; Reliable witness; Strong liars; Strong pseudoprimes,Algorithms; Arithmetic operations; Asymptotic complexity; Primality; Primality tests; Pseudoprimes; Randomized Algorithms; Reliable witness; Strong liars; Mathematical techniques
Inapproximability of the multilevel uncapacitated facility location problem,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989880662&doi=10.1145%2f2907050&partnerID=40&md5=ba170f51f08a54a9afb1e7c7f477a7ce,"In this article, we present improved inapproximability results for the k-level uncapacitated facility location problem. In particular, we show that there is no polynomial time approximation algorithm with performance guarantee better than 1.539 unless P = NP for the case when k = 2. For the case of general k (tending to infinity), we obtain a better hardness factor of 1.61. Interestingly, our results show that the two-level problem is computationally harder than the well-known uncapacitated facility location problem (k = 1) since the best-known approximation guarantee for the latter problem is 1.488 due to Li [2013], and our inapproximability is a factor of 1.539 for the two-level problem. The only inapproximability result known before for this class of metric facility location problems is the bound of 1.463 due to Guha and Khuller [1999], which holds even for the case of k = 1. © 2016 ACM.",Approximation algorithms; Hardness of approximation,Hardness; Location; Polynomial approximation; Site selection; Facility location problem; Hardness factor; Hardness of approximation; Inapproximability; Performance guarantees; Polynomial time approximation algorithms; Two-level problems; Uncapacitated facility locations; Approximation algorithms
Smoothed analysis of the 2-opt algorithm for the general TSP,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989183336&doi=10.1145%2f2972953&partnerID=40&md5=7425329f6f56a0156801f06cc1d2555e,"2-Opt is a simple local search heuristic for the traveling salesperson problem that performs very well in experiments with respect to both running time and solution quality. In contrast to this, there are instances on which 2-Opt may need an exponential number of steps to reach a local optimum. To understand why 2-Opt usually finds local optima quickly in experiments, we study its expected running time in the model of smoothed analysis, which can be considered as a less-pessimistic variant of worst-case analysis in which the adversarial input is subject to a small amount of random noise. In our probabilistic input model, an adversary chooses an arbitrary graph G and a probability density function for each edge according to which its length is chosen. We prove that in this model the expected number of local improvements is O(mnφ·16 lnm) = m1+o(1)nφ, where n and mdenote the number of vertices and edges of G, respectively, and φ denotes an upper bound on the density functions. © 2016 ACM.",2-opt; Local search; Probabilistic analysis; Smoothed analysis; Traveling salesperson problem,Heuristic algorithms; Local search (optimization); Optimization; Traveling salesman problem; 2-opt; Local search; Probabilistic analysis; Smoothed analysis; Traveling salesperson problem; Probability density function
Tight lower bound for the channel assignment problem,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988019767&doi=10.1145%2f2876505&partnerID=40&md5=c165ae09b91260d525343bc14b4e7060,"We study the complexity of the CHANNEL ASSIGNMENT problem. An open problem asks whether CHANNEL ASSIGNMENT admits an O(cn) (times a polynomial in the bit size) time algorithm, where n is a number of the vertices, for a constant c independent of the weights on the edges. We answer this question in the negative. Indeed, we show that in the standard Word RAM model, there is no 2o(nlog n) (times a polynomial in the bit size) time algorithm solving CHANNEL ASSIGNMENT unless the exponential time hypothesis fails. Note that the currently best known algorithm works in time O∗(n!) = 2O(nlog n), so our lower bound is tight (where the O∗( ) notation suppresses polynomial factors). © 2016 ACM.",Channel assignment; Exponential time hypothesis; Lower bounds,Polynomials; Best-known algorithms; Channel Assignment; Channel assignment problem; Exponential time hypothesis; Lower bounds; Polynomial factor; RAM model; Time algorithms; Combinatorial optimization
Space-constrained interval selection,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988014752&doi=10.1145%2f2886102&partnerID=40&md5=de88d297fdb39e61c6d2e025f9b4dc78,"We study streaming algorithms for the interval selection problem: finding a maximum cardinality subset of disjoint intervals on the line. A deterministic 2-approximation streaming algorithm for this problem is developed, together with an algorithm for the special case of proper intervals, achieving improved approximation ratio of 3/2.We complement these upper bounds by proving that they are essentially the best possible in the streaming setting: It is shown that an approximation ratio of 2 - ϵ (or 3/2 - ϵ for proper intervals) cannot be achieved unless the space is linear in the input size. In passing, we also answer an open question of Adler and Azar (J. Scheduling 2003) regarding the space complexity of constant-competitive randomized preemptive online algorithms for the same problem. © 2016 ACM.",Interval selection; Lower bounds; Online algorithms; Streaming algorithms,Algorithms; Mathematical techniques; Approximation ratios; Cardinalities; Interval selection; Lower bounds; On-line algorithms; Space complexity; Streaming algorithm; Upper Bound; Approximation algorithms
Deletion without rebalancing in binary search trees,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987933792&doi=10.1145%2f2903142&partnerID=40&md5=bdf4f6f80ad1e0d213f29e7680cdb6ae,"We address the vexing issue of deletions in balanced trees. Rebalancing after a deletion is generally more complicated than rebalancing after an insertion. Textbooks neglect deletion rebalancing, and many B-tree- based database systems do not do it. We describe a relaxation of AVL trees in which rebalancing is done after insertions but not after deletions, yet worst-case access time remains logarithmic in the number of insertions. For any application of balanced trees in which the number of updates is polynomial in the tree size, our structure offers performance competitive with that of classical balanced trees. With the addition of periodic rebuilding, the performance of our structure is theoretically superior to that of many, if not all, classic balanced tree structures. Our structure needs lg lgm+1 bits of balance information per node, wherem is the number of insertions and lg is the base-two logarithm, or lg lg n+O(1) with periodic rebuilding, where n is the number of nodes. An insertion takes up to two rotations and O(1) amortized time, not counting the time to find the insertion position. This is the same as in standard AVL trees. Using an analysis that relies on an exponential potential function, we show that rebalancing steps occur with a frequency that is exponentially small in the height of the affected node. Our techniques apply to other types of balanced trees, notably B-trees, as we show in a companion article, and particularly red-black trees, which can be viewed as a special case of B-trees. © 2016 ACM.",Algorithm; Amortized complexity; Balanced trees; Data structure; Database access methods; Exponential potential function,Algorithms; Binary trees; Computational complexity; Data structures; Periodic structures; Amortized complexity; Amortized time; Balanced trees; Binary search trees; Database access; Insertion position; Potential function; Red black tree; Trees (mathematics)
On the tradeoff between stability and fit,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989846284&doi=10.1145%2f2963103&partnerID=40&md5=f948eda548ecce20df70d8de91a13416,"In computing, as in many aspects of life, changes incur cost. Many optimization problems are formulated as a one-time instance starting from scratch. However, a common case that arises is when we already have a set of prior assignments and must decide how to respond to a new set of constraints, given that each change from the current assignment comes at a price. That is, we would like to maximize the fitness or efficiency of our system, but we need to balance it with the changeout cost from the previous state. We provide a precise formulation for this tradeoff and analyze the resulting stable extensions of some fundamental problems in measurement and analytics. Our main technical contribution is a stable extension of Probability Proportional to Size (PPS) weighted random sampling, with applications to monitoring and anomaly detection problems. We also provide a general framework that applies to top-k, minimum spanning tree, and assignment. In both cases, we are able to provide exact solutions and discuss efficient incremental algorithms that can find new solutions as the input changes. © 2016 ACM.",Stable minimum spanning tree; Stable top-k; Stable weighted samples,Optimization; Anomaly detection; Incremental algorithm; Minimum spanning trees; Optimization problems; Probability proportional; Random sampling; Technical contribution; Weighted samples; Costs
An improved approximation algorithm for the edge-disjoint paths problem with congestion two,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989830437&doi=10.1145%2f2960410&partnerID=40&md5=54518fcc5191915c93d85c894d76e96c,"In the maximum edge-disjoint paths problem, we are given a graph and a collection of pairs of vertices, and the objective is to find the maximum number of pairs that can be routed by edge-disjoint paths. An r-approximation algorithm for this problem is a polynomial-time algorithm that finds at least OPT/r edgedisjoint paths, where OPT denotes the maximum possible number of pairs that can be routed in a given instance. For a long time, an O(n1/2)-approximation algorithm has been best known for this problem even if a congestion of two is allowed, that is, each edge is allowed to be used in at most two of the paths. In this article, we give a randomized O(n3/7· poly(logn))-approximation algorithm with congestion two. This is the first result that breaks the O(n1/2)-approximation algorithm. In particular, we prove the following: (1) If we have a (randomized) polynomial-time algorithm for finding Ω(OPT1/p/polylog(n)) edge-disjoint paths for some p > 1, then we can give a randomized O(n1/2-α)-approximation algorithm for the edge-disjoint paths problem by using Rao-Zhou's algorithm for some α > 0. (2) Based on the Chekuri-Khanna-Shepherd well-linked decomposition, we show that there is a randomized algorithm for finding Ω(OPT1/4/(logn)3/2) edge-disjoint paths connecting given terminal pairs with congestion two. Our framework for this algorithm is more general in the following sense. Indeed, the above two ingredients also work for the maximum edge-disjoint paths problem (with congestion one) if there is a (randomized) polynomial-time algorithm for finding Ω(OPT1/p) edge-disjoint paths connecting given terminal pairs for some p > 1. © 2016 ACM.",Chekuri-Khanna-shepherd well-linked decomposition; Disjoint paths problem; Rao-zhou algorithm,Graph theory; Polynomial approximation; Disjoint paths problem; Edge disjoint paths; Polynomial-time algorithms; R-approximation algorithms; Randomized Algorithms; Approximation algorithms
Adaptive and approximate orthogonal range counting,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988037173&doi=10.1145%2f2830567&partnerID=40&md5=3697b62a956bdffa2dd5991d242738a5,"We present three new results on one of the most basic problems in geometric data structures, 2-D orthogonal range counting. All the results are in the w-bit word RAM model. -It is well known that there are linear-space data structures for 2-D orthogonal range counting with worstcase optimal query time O(log n/ log log n). We give an O(nlog log n)-space adaptive data structure that improves the query time to O(log log n+ log k/ log log n), where k is the output count. When k = O(1), our bounds match the state of the art for the 2-D orthogonal range emptiness problem [Chan et al., 2011]. -We give an O(nlog log n)-space data structure for approximate 2-D orthogonal range counting that can compute a (1+d)-factor approximation to the count in O(log log n) time for any fixed constant d > 0. Again, our bounds match the state of the art for the 2-D orthogonal range emptiness problem. -Last, we consider the 1-D range selection problem, where a query in an array involves finding the kth least element in a given subarray. This problem is closely related to 2-D 3-sided orthogonal range counting. Recently, Jørgensen and Larsen [2011] presented a linear-space adaptive data structure with query time O(log log n+ log k/ log log n). We give a new linear-space structure that improves the query time to O(1 + log k/ log log n), exactly matching the lower bound proved by Jørgensen and Larsen. © 2016 ACM.",Adaptive; Approximate; Computational geometry; Data structure; Orthogonal; Range counting; Word RAM,Computational geometry; Adaptive; Approximate; Emptiness problem; Geometric data structures; Orthogonal; Range counting; Selection problems; Space data structures; Data structures
Approximation algorithms for movement repairmen,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981309757&doi=10.1145%2f2908737&partnerID=40&md5=5d7fa53afc4cffa90e8f9ec2865cbf39,"In the Movement Repairmen (MR) problem, we are given a metric space (V, d) along with a set R of k repairmen r1 , r2,..., rk with their start depots s1 , s2,..., sk ∈ V and speeds v1, v2,... ,vk ≥ 0, respectively, and a set C of m clients c1, c2,..., cm having start locations s1′, s2′,..., sm′ ∈ V and speeds v1′, v2′,..., vm′ ≥ 0, respectively. If t is the earliest time a client cj is collocated with any repairman (say, ri) at a node u, we say that the client is served by ri at u and that its latency is t. The objective in the (SUM-MR) problem is to plan the movements for all repairmen and clients to minimize the sum (average) of the clients' latencies. The motivation for this problem comes, for example, from Amazon Locker Delivery [Amazon 2010] and USPS gopost [Service 2010]. We give the first O(log n)-approximation algorithm for the SUM-MR problem. In order to approximate SUM-MR, we formulate an LP for the problem and bound its integrality gap. Our LP has exponentially many variables; therefore, we need a separation oracle for the dual LP. This separation oracle is an instance of the Neighborhood Prize Collecting Steiner Tree (NPCST) problem in which we want to find a tree with weight at most L collecting the maximum profit from the clients by visiting at least one node from their neighborhoods. The NPCST problem, even with the possibility to violate both the tree weight and neighborhood radii, is still very hard to approximate. We deal with this difficulty by using LP with geometrically increasing segments of the timeline, and by giving a tricriteria approximation for the problem. The rounding needs a relatively involved analysis. We give a constant approximation algorithm for SUM- MR in Euclidean Space where the speed of the clients differs by a constant factor. We also give a constant approximation for the makespan variant. © 2016 ACM.",Linear programs; Movement repairman; Randomized rounding; Routing,Algorithms; Linear programming; Constant approximation algorithms; Constant factors; Integrality gaps; Linear programs; Movement repairman; Prize collecting Steiner tree; Randomized rounding; Routing; Approximation algorithms
On hierarchical routing in doubling metrics,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032340465&doi=10.1145%2f2915183&partnerID=40&md5=e34cd16f7e908f36375491e52bb394f6,"We study the problem of routing in doubling metrics and show how to perform hierarchical routing in such metrics with small stretch and compact routing tables (i.e., with a small amount of routing information stored at each vertex). We say that a metric (X, d) has doubling dimension dim(X) at most α if every ball can be covered by 2α balls of half its radius. (A doubling metric is one whose doubling dimension dim(X) is a constant.) We consider the metric space induced by the shortest-path distance in an underlying undirected graph G. We show how to perform (1 + τ)-stretch routing on such a metric for any 0 < τ ≤ 1 with routing tables of size at most (α/τ)O(α) log log δ bits with only (α/τ)O(α) log entries, where is the diameter of the graph, and δ is the maximum degree of the graph G; hence, the number of routing table entries is just τ−O(1) log for doubling metrics. These results extend and improve on those of Talwar (2004). We also give better constructions of sparse spanners for doubling metrics than those obtained from the routing tables earlier; for τ > 0, we give algorithms to construct (1 + τ)-stretch spanners for a metric (X, d) with maximum degree at most (2 + 1/τ)O(dim(X)), matching the results of Das et al. for Euclidean metrics. © 2016 ACM",Doubling metrics; Hierarchical routing; Spanners,Graph theory; Compact routing tables; Doubling dimensions; Doubling metrics; Euclidean metrics; Hierarchical routings; Routing information; Spanners; Undirected graph; Routing algorithms
Compressed cache-oblivious string B-tree,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981303057&doi=10.1145%2f2903141&partnerID=40&md5=744fa74cba58f4b6c4d4d3597411d686,"In this article, we study three variants of the well-known prefix-search problem for strings, and we design solutions for the cache-oblivious model which improve the best known results. Among these contributions, we close (asymptotically) the classic problem, which asks for the detection of the set of strings that share the longest common prefix with a queried pattern by providing an I/O-optimal solution that matches the space lower bound for tries up to a constant multiplicative factor of the form (1 + ∈), for ∈ > 0. Our solutions hinge upon a novel compressed storage scheme that adds the ability to decompress prefixes of the stored strings I/O-optimally to the elegant locality-preserving front coding (Bender et al. 2006) still preserving its space bounds. © 2016 ACM.",Compressed index; Data compression; Indexing data structure; Pattern matching; String dictionary,Cache memory; Digital storage; Pattern matching; Cache-oblivious; Cache-oblivious model; Compressed index; Design solutions; Locality-preserving; Longest common prefixes; Multiplicative factors; Storage schemes; Data compression
Agnostic learning in permutation-invariant domains,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981352299&doi=10.1145%2f2963169&partnerID=40&md5=9d60dd9e041461bd6d9eb581e9b0cb6e,"We generalize algorithms from computational learning theory that are successful under the uniform distribution on the Boolean hypercube {0, 1}n to algorithms successful on permutation-invariant distributions, distributions that stay invariant constant on permutating the coordinates in the instances. While the tools in our generalization mimic those used for the Boolean hypercube, the fact that permutation-invariant distributions are not product distributions presents a significant obstacle. We prove analogous results for permutation-invariant distributions; more generally, we work in the domain of the symmetric group. We define noise sensitivity in this setting and show that noise sensitivity has a nice combinatorial interpretation in terms of Young tableaux. The main technical innovations involve techniques from the representation theory of the symmetric group, especially the combinatorics of Young tableaux. We show that low noise sensitivity implies concentration on ""simple"" components of the Fourier spectrum and that this fact will allow us to agnostically learn halfspaces under permutation-invariant distributions to constant accuracy in roughly the same time as in the uniform distribution over the Boolean hypercube case. © 2016 ACM.",Algorithms; F.2.m [analysis of algorithms and problem complexity]: miscellaneous; Fourier analysis; Learning theory; Representation theory of the symmetric group; Theory,Algebra; Algorithms; Computational complexity; Fourier analysis; Geometry; Learning algorithms; Combinatorics of young tableaux; Computational learning theory; Invariant distribution; Learning Theory; Problem complexity; Product distributions; Symmetric groups; Theory; Group theory
A linear-size logarithmic stretch path-reporting distance oracle for general graphs,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981320404&doi=10.1145%2f2888397&partnerID=40&md5=b634984343c49e48b4f4a227607358ec,"Thorup and Zwick [2001a] proposed a landmark distance oracle with the following properties. Given an n-vertex undirected graph G = (V, E) and a parameter k = 1, 2,...,their oracle has size O(kn1+1/k), and upon a query (u, v) it constructs a path Π between u and v of length δ(u, v) such that dG(u,v) ≤ δ(u, v) ≤ (2k-1)dG(u, v). The query time of the oracle from Thorup and Zwick [2001a] is O(k) (in addition to the length of the returned path), and it was subsequently improved to O(1) [Wulff-Nilsen 2012; Chechik 2014]. A major drawback of the oracle of Thorup and Zwick [2001a] is that its space is Ω(n · log n). Mendel and Naor [2006] devised an oracle with space O(n1+1/k) and stretch O(k), but their oracle can only report distance estimates and not actual paths. In this article, we devise a path-reporting distance oracle with size O(n1+1/k), stretch O(k), and query time O(n∈), for an arbitrarily small constant ∈ > 0. In particular, for k = log n, our oracle provides logarithmic stretch using linear size. Another variant of our oracle has size O(n log log n), polylogarithmic stretch, and query time O(log log n). For unweighted graphs, we devise a distance oracle with multiplicative stretch O(1), additive stretch O(β(k)), for a function β(·), space O(n1+1/k), and query time O(n∈), for an arbitrarily small constant ∈ > 0. The tradeoff between multiplicative stretch and size in these oracles is far below Erdo{combining double acute accent}s's girth conjecture threshold (which is stretch 2k - 1 and size O(n1+1/k)). Breaking the girth conjecture tradeoff is achieved by exhibiting a tradeoff of different nature between additive stretch β(k) and size O(n1+1/k). A similar type of tradeoff was exhibited by a construction of (1 + ∈, β)-spanners due to Elkin and Peleg [2001]. However, so far (1 + ∈, β)-spanners had no counterpart in the distance oracles' world. An important novel tool that we develop on the way to these results is a distance-preserving path-reporting oracle. We believe that this oracle is of independent interest. © 2016 ACM.",Distance oracles; Distance preservers,Query processing; Distance oracles; Distance preservers; General graph; Polylogarithmic; Query time; Undirected graph; Unweighted graphs; Graph theory
Improved approximation algorithms for matroid and knapsack median problems and applications,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981334974&doi=10.1145%2f2963170&partnerID=40&md5=d3053edfa023e859245bb12ba4e00bcb,"We consider the matroid median problem [Krishnaswamy et al. 2011], wherein we are given a set of facilities with opening costs and a matroid on the facility-set, and clients with demands and connection costs, and we seek to open an independent set of facilities and assign clients to open facilities so as to minimize the sum of the facility-opening and client-connection costs. We give a simple 8-approximation algorithm for this problem based on LP-rounding, which improves upon the 16-approximation in Krishnaswamy et al. [2011]. We illustrate the power and versatility of our techniques by deriving (a) an 8-approximation for the twomatroid median problem, a generalization of matroid median that we introduce involving two matroids; and (b) a 24-approximation algorithm for matroid median with penalties, which is a vast improvement over the 360-approximation obtained in Krishnaswamy et al. [2011]. We show that a variety of seemingly disparate facility-location problems considered in the literature-data placement problem, mobile facility location, k-median forest, metric uniform minimum-latency Uncapacitated Facility Location (UFL)-in fact reduce to the matroid median or two-matroid median problems, and thus obtain improved approximation guarantees for all these problems. Our techniques also yield an improvement for the knapsack median problem. © 2016 ACM.",Algorithms; Approximation algorithms; F.2.2 [analysis of algorithms and problem complexity]: non-numerical algorithms and problems - computations on discrete structures; Facility location k-median and matroid median; G.1.6 [numerical analysis]: optimization; G.2 [discrete mathematics]: miscellaneous; Knapsack constraints; Linear programming and LP rounding; Submodular and matroid polyhedra; Theory,Algorithms; Aluminum; Combinatorial mathematics; Combinatorial optimization; Computational complexity; Costs; Linear programming; Location; Optimization; Discrete mathematics; K-median; Knapsack constraints; LP-rounding; Problem complexity; Submodular; Theory; Approximation algorithms
Data structures for path queries,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017909121&doi=10.1145%2f2905368&partnerID=40&md5=f27bd79adeb5ba659854b515806b4401,"Consider a tree T on n nodes, each having a weight drawn from [1..σ]. In this article, we study the problem of supporting various path queries over the tree T. The path counting query asks for the number of the nodes on a query path whose weights are in a query range, while the path reporting query requires to report these nodes. The path median query asks for the median weight on a path between two given nodes, and the path selection query returns the k-th smallest weight. We design succinct data structures to encode T using nH(WT ) + 2n + o(nlg σ) bits of space, such that we can support path counting queries in O(lg σ/lg lg n + 1) time, path reporting queries in O((occ + 1)(lg σ/lg lg n + 1)) time, and path median and path selection queries in O(lg σ/lg lg σ) time, where H(WT ) is the entropy of the multiset of the weights of the nodes in T and occ is the size of the output. Our results not only greatly improve the best known data structures [Chazelle 1987; Krizanc et al. 2005], but also match the lower bounds for path counting, median, and selection queries [Pǎtraşcu 2007, 2011; Jørgensen and Larsen 2011] when σ = (n/polylog(n)). © 2016 ACM",Path counting; Path median; Path queries; Path reporting; Path selection; Succinct data structures; Tree extraction,Data structures; Path counting; Path median; Path queries; Path reporting; Path selection; Succinct data structure; Tree extraction; Trees (mathematics)
Maximizing k-submodular functions and beyond,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981295028&doi=10.1145%2f2850419&partnerID=40&md5=2a7c163d5ca19ca01718c822b5065ac8,"We consider the maximization problem in the value oracle model of functions defined on k-tuples of sets that are submodular in every orthant and r-wise monotone, where k ≥ 2 and 1 ≤ r ≤ k. We give an analysis of a deterministic greedy algorithm that shows that any such function can be approximated to a factor of 1/(1 + r). For r = k, we give an analysis of a randomized greedy algorithm that shows that any such function can be approximated to a factor of 1/(1 + √k/2). In the case of k = r = 2, the considered functions correspond precisely to bisubmodular functions, in which case we obtain an approximation guarantee of 1/2. We show that, as in the case of submodular functions, this result is the best possible both in the value query model and under the assumption that NP ≠ RP. Extending a result of Ando et al., we show that for any k ≥ 3, submodularity in every orthant and pairwise monotonicity (i.e., r = 2) precisely characterize k-submodular functions. Consequently, we obtain an approximation guarantee of 1/3 (and thus independent of k) for the maximization problem of k-submodular functions. © 2016 ACM.",Bisubmodularity; k-submodularity; Submodularity,Algorithms; Mathematical techniques; Bisubmodularity; Greedy algorithms; Maximization problem; Monotonicity; Oracle model; Query model; Submodular functions; Submodularity; Factor analysis
A faster algorithm for computing straight skeletons,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968813732&doi=10.1145%2f2898961&partnerID=40&md5=aa22b41fe9fbe4b12f7eba0dd35485af,"We present a new algorithm for computing the straight skeleton of a polygon. For a polygon with n vertices, among which r are reflex vertices, we give a deterministic algorithm that reduces the straight skeleton computation to a motorcycle graph computation in O(n(log n) logr) time. It improves on the previously best known algorithm for this reduction, which is randomized, and runs in expected O(n √ h+ 1 log2n) time for a polygon with h holes. Using known motorcycle graph algorithms, our result yields improved time bounds for computing straight skeletons. In particular, we can compute the straight skeleton of a nondegenerate polygon in O(n(log n) logr + r4/3+ϵ) time for any ϵ > 0. On degenerate input, our time bound increases to O(n(log n) logr + r17/11+ϵ ). © 2016 ACM.",Motorcycle graph; Straight skeleton,Geometry; Motorcycles; Musculoskeletal system; Best-known algorithms; Deterministic algorithms; Graph algorithms; Nondegenerate; Straight skeleton; Time bound; Algorithms
Scale-free compact routing schemes in networks of low doubling dimension,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976359351&doi=10.1145%2f2876055&partnerID=40&md5=da1fcc00f0ae1ce8a959281619589c9c,"We consider compact routing schemes in networks of low doubling dimension, where the doubling dimension is the least value α such that any ball in the network can be covered by at most 2α balls of half radius. There are two variants of routing-scheme design: (i) labeled (name-dependent) routing, in which the designer is allowed to rename the nodes so that the names (labels) can contain additional routing information, for example, topological information; and (ii) name-independent routing, which works on top of the arbitrary original node names in the network, that is, the node names are independent of the routing scheme. In this article, given any constant ∈ ∈ (0, 1) and an n-node edge-weighted network of doubling dimension α ∈ O(loglogn), we present-a (1+∈)-stretch labeled compact routing scheme with [log n] -bit routing labels, O(log2 n/loglogn)-bit packet headers, and ((1/∈)O(α) log3n)-bit routing information at each node; -a (9 + ∈)-stretch name-independent compact routing scheme with O(log2 n/loglogn)-bit packet headers, and ((1/∈)O(α) log3n)-bit routing information at each node. In addition, we prove a lower bound: any name-independent routing scheme with o(n(∈/60)2) bits of storage at each node has stretch no less than 9 - ∈ for any ∈ ∈ (0, 8). Therefore, our name-independent routing scheme achieves asymptotically optimal stretch with polylogarithmic storage at each node and packet headers. Note that both schemes are scale-free in the sense that their space requirements do not depend on the normalized diameter Δ of the network. We also present a simpler nonscale-free (9 + ∈)-stretch name-independent compact routing scheme with improved space requirements if Δ is polynomial in n. © 2016 ACM.",Compact routing; Doubling dimension; Labeled routing; Name-independent routing; Scale free,Routing protocols; Compact routing; Doubling dimensions; Labeled routing; Name-independent routing; Scale-free; Network routing
A simple and efficient algorithm for computing market equilibria,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974577504&doi=10.1145%2f2905372&partnerID=40&md5=656e619b6dfe76203ef4b61858f528f6,"We give a new mathematical formulation of market equilibria in exchange economies using an indirect utility function: the function of prices and income that gives the maximum utility achievable. The formulation is a convex program and can be solved when the indirect utility function is convex in prices. We illustrate that many economies, including: - Homogeneous utilities of degree α ∈ [0, 1] in Fisher economies - this includes Linear, Leontief, Cobb-Douglas - Resource allocation utilities like multi-commodity flows satisfy this condition and can be efficiently solved. Further, we give a natural tâtonnement type price-adjusting algorithm in these economies. Our algorithm, which is applicable to a larger class of utility functions than previously known weak gross substitutes, mimics the natural dynamics for the markets as suggested by Walras: it iteratively adjusts a good's price upward when the demand for that good under current prices exceeds its supply; and downward when its supply exceeds its demand. The algorithm computes an approximate equilibrium in a number of iterations that is independent of the number of traders and is almost linear in the number of goods. © 2016 ACM.",Approximation algorithms; Market equilibrium,Approximation algorithms; Commerce; Convex optimization; Costs; Economic analysis; Economics; Electronic trading; Functions; Iterative methods; Approximate equilibriums; Class of utility functions; Exchange economies; Market equilibria; Mathematical formulation; Multi-commodity flow; Number of iterations; Simple and efficient algorithms; Algorithms
Limits and applications of group algebras for parameterized problems,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974575110&doi=10.1145%2f2885499&partnerID=40&md5=77ca1009515d19784ce8c5acea1c478f,"The fastest known randomized algorithms for several parameterized problems use reductions to the k-MLD problem: detection of multilinear monomials of degree k in polynomials presented as circuits. The fastest known algorithm for k-MLD is based on 2k evaluations of the circuit over a suitable algebra. We use communication complexity to show that it is essentially optimal within this evaluation framework. On the positive side, we give additional applications of the method: finding a copy of a given tree on k nodes, a minimum set of nodes that dominate at least t nodes, and an m-dimensional k-matching. In each case, we achieve a faster algorithm than what was known before. We also apply the algebraic method to problems in exact counting. Among other results, we show that a variation of it can break the trivial upper bounds for the disjoint summation problem. © 2016 ACM.",Algebraic algorithms; Color coding; Exact counting; Multilinear detection; Permanent,Algorithms; Reconfigurable hardware; Algebraic algorithms; Color coding; Communication complexity; Evaluation framework; Exact counting; Parameterized problems; Permanent; Randomized Algorithms; Algebra
On problems as hard as CNF-SAT,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973862703&doi=10.1145%2f2925416&partnerID=40&md5=28d566089ba997da62cc6b0cfca93994,"The field of exact exponential time algorithms for non-deterministic polynomial-time hard problems has thrived since the mid-2000s. While exhaustive search remains asymptotically the fastest known algorithm for some basic problems, non-trivial exponential time algorithms have been found for a myriad of problems, including GRAPH COLORING, HAMILTONIAN PATH, DOMINATING SET, and 3-CNF-SAT. In some instances, improving these algorithms further seems to be out of reach. The CNF-SAT problem is the canonical example of a problem for which the trivial exhaustive search algorithm runs in time O(2n), where n is the number of variables in the input formula. While there exist non-trivial algorithms for CNF-SAT that run in time o(2n), no algorithm was able to improve the growth rate 2 to a smaller constant, and hence it is natural to conjecture that 2 is the optimal growth rate. The strong exponential time hypothesis (SETH) by Impagliazzo and Paturi [JCSS 2001] goes a little bit further and asserts that, for every ∈ < 1, there is a (large) integer k such that k-CNF-SAT cannot be computed in time 2∈n. In this article, we show that, for every ∈ < 1, the problems HITTING SET, SET SPLITTING, and NAE-SAT cannot be computed in time O(2∈n) unless SETH fails. Here n is the number of elements or variables in the input. For these problems, we actually get an equivalence to SETH in a certain sense. We conjecture that SETH implies a similar statement for SET COVER and prove that, under this assumption, the fastest known algorithms for STEINER TREE, CONNECTED VERTEX COVER, SET PARTITIONING, and the pseudo-polynomial time algorithm for SUBSET SUM cannot be significantly improved. Finally, we justify our assumption about the hardness of SET COVER by showing that the parity of the number of solutions to SET COVER cannot be computed in time O(2∈n) for any ∈ < 1 unless SETH fails. © 2016 ACM.",Optimal growth rate; Reduction; Satisfiability; Strong exponential time hypothesis,Graph theory; Growth rate; Polynomial approximation; Reduction; Trees (mathematics); Connected vertex cover; Exact exponential-time algorithms; Exhaustive search algorithms; Exponential time algorithm; Optimal growth; Pseudo-polynomial time algorithms; Satisfiability; Strong exponential time hypothesis; Algorithms
Approximation algorithms for stochastic submodular set cover with applications to boolean function evaluation and min-knapsack,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968895040&doi=10.1145%2f2876506&partnerID=40&md5=f4e90fb4fca67cdbb3f4876eb2cc57e3,"We present a new approximation algorithm for the stochastic submodular set cover (SSSC) problem called adaptive dual greedy. We use this algorithm to obtain a 3-approximation algorithm solving the stochastic Boolean function evaluation (SBFE) problem for linear threshold formulas (LTFs). We also obtain a 3- approximation algorithm for the closely related stochastic min-knapsack problem and a 2-approximation for a variant of that problem. We prove a new approximation bound for a previous algorithm for the SSSC problem, the adaptive greedy algorithm of Golovin and Krause. We also consider an approach to approximating SBFE problems using the adaptive greedy algorithm,which we call the Q-value approach. This approach easily yields a new result for evaluation of CDNF (conjunctive / disjunctive normal form) formulas, and we apply variants of it to simultaneous evaluation problems and a ranking problem. However, we show that the Q-value approach provably cannot be used to obtain a sublinear approximation factor for the SBFE problem for LTFs or read-once disjunctive normal form formulas. © 2016 ACM.",Boolean function evaluation; Sequential testing,Algorithms; Boolean functions; Combinatorial optimization; Function evaluation; Stochastic systems; Adaptive greedy algorithms; Approximation bounds; Approximation factor; Disjunctive Normal Form formulas; Evaluation problems; Sequential testing; Stochastic boolean functions; Submodular set covers; Approximation algorithms
Optimal orthogonal graph drawing with convex bend costs,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968835190&doi=10.1145%2f2838736&partnerID=40&md5=33ff21dc2cd5a400bde62e13f3bff5f3,"Traditionally, the quality of orthogonal planar drawings is quantified by the total number of bends or the maximum number of bends per edge. However, this neglects that, in typical applications, edges have varying importance. We consider the problem OPTIMALFLEXDRAW that is defined as follows. Given a planar graph G on n vertices with maximum degree 4 (4-planar graph) and for each edge e a cost function coste : ℕ0 → ℝ defining costs depending on the number of bends e has, compute a planar orthogonal drawing of G of minimum cost. In this generality OPTIMALFLEXDRAW is NP-hard. We show that it can be solved efficiently if (1) the cost function of each edge is convex and (2) the first bend on each edge does not cause any cost. Our algorithm takes time O(n · Tflow(n)) and O(n2 · Tflow(n)) for biconnected and connected graphs, respectively, where Tflow(n) denotes the time to compute a minimum-cost flow in a planar network with multiple sources and sinks. Our result is the first polynomial-time bend-optimization algorithm for general 4-planar graphs optimizing over all embeddings. Previous work considers restricted graph classes and unit costs. © 2016 ACM.",Efficient algorithm; Orthogonal graph drawing; Planar embedding,Algorithms; Cost accounting; Cost functions; Drawing (graphics); Flow graphs; Graph theory; Optimization; Orthogonal functions; Polynomial approximation; Connected graph; Minimum cost flows; Optimization algorithms; Orthogonal drawings; Orthogonal graphs; Planar embedding; Planar networks; Typical application; Costs
Point line cover: The easy kernel is essentially tight,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968760304&doi=10.1145%2f2832912&partnerID=40&md5=303defaf5463d9af14fceb5ad5f09fb6,"The input to the NP-hard POINT LINE COVER problem (PLC) consists of a set P of n points on the plane and a positive integer k; the question is whether there exists a set of at most k lines that pass through all points in P. By straightforward reduction rules, one can efficiently reduce any input to one with at most k2 points. We show that this easy reduction is already essentially tight under standard assumptions.More precisely, unless the polynomial hierarchy collapses to its third level, for any ϵ > 0, there is no polynomial-time algorithm that reduces every instance (P, k) of PLC to an equivalent instance with O(k2-ϵ ) points. This answers, in the negative, an open problem posed by Lokshtanov [2009]. Our proof uses the notion of a kernel from parameterized complexity, and the machinery for deriving lower bounds on the size of kernels developed by Dell and van Melkebeek [2010, 2014]. It has two main ingredients: We first show, by reduction from VERTEX COVER, that-unless the polynomial hierarchy collapses-PLC has no kernel of total size O(k2-ϵ) bits. This does not directly imply the claimed lower bound on the number of points, since the best-known polynomialtime encoding of a PLC instance with n points requires ω(n2) bits. To get around this hurdle, we build on work of Alon [1986] and devise an oracle communication protocol of cost O(nlog n) for PLC. This protocol, together with the lower bound on the total size (which also holds for such protocols), yields the stated lower bound on the number of points. While a number of essentially tight polynomial lower bounds on total sizes of kernels are known, our result is-to the best of our knowledge-the first to show a nontrivial lower bound for structural/secondary parameters. It is also the first example of a lower bound for kernelization that makes use of the full power of the oracle communication protocol lower bounds that can be obtained from the work of Dell and van Melkebeek. We combine the main abstract ideas of our proof to derive a general recipe that could be used to obtain such lower bounds for other problems with unknown or insufficiently strong encodings. © 2016 ACM.",Kernelization lower bounds; Parameterized complexity,Algorithms; Encoding (symbols); Machinery; Polynomial approximation; Line-cover problems; Lower bounds; Parameterized complexity; Polynomial hierarchies; Polynomial-time; Polynomial-time algorithms; Positive integers; Reduction rules; Polynomials
"The traveling salesman problem for lines, balls, and planes",2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968867143&doi=10.1145%2f2850418&partnerID=40&md5=a39d84fc276bbc45107aceca205e683d,"We revisit the traveling salesman problem with neighborhoods (TSPN) and propose several new approximation algorithms. These constitute either first approximations (for hyperplanes, lines, and balls in Rd, for d ≥ 3) or improvements over previous approximations achievable in comparable times (for unit disks in the plane). (I) Given a set of n hyperplanes in Rd, a traveling salesman problem (TSP) tour whose length is at most O(1) times the optimal can be computed in O(n) time when d is constant. (II) Given a set of n lines in Rd, a TSP tour whose length is at most O(log3 n) times the optimal can be computed in polynomial time for all d. (III) Given a set of n unit balls in Rd, a TSP tour whose length is at most O(1) times the optimal can be computed in polynomial time when d is constant. © 2016 ACM.",Approximation algorithm; Group Steiner tree; Hyperplanes; Linear programming; Minimum-perimeter rectangular box; Traveling salesman; Unit disks and balls,Algorithms; Approximation algorithms; Geometry; Linear programming; Polynomial approximation; Trees (mathematics); Group Steiner tree; Hyperplanes; Rectangular box; Traveling salesman; Unit disk; Traveling salesman problem
All-or-nothing generalized assignment with application to scheduling advertising campaigns,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968873950&doi=10.1145%2f2843944&partnerID=40&md5=49be8ab2c29b66bf5a9e17a48448fa23,"We study a variant of the generalized assignment problem (GAP), which we label all-or-nothing GAP (AGAP). We are given a set of items, partitioned into n groups, and a set of mbins. Each item ℓ has size sℓ > 0, and utility α ℓj ≥ 0 if packed in bin j. Each bin can accommodate at most one item from each group; the total size of the items in a bin cannot exceed its capacity. A group of items is satisfied if all of its items are packed. The goal is to find a feasible packing of a subset of the items in the bins such that the total utility from satisfied groups is maximized. We motivate the study of AGAP by pointing out a central application in scheduling advertising campaigns. Our main result is an O(1)-approximation algorithm for AGAP instances arising in practice, in which each group consists of at most m/2 items. Our algorithm uses a novel reduction of AGAP to maximizing submodular function subject to a matroid constraint. For AGAP instances with a fixed number of bins, we develop a randomized polynomial time approximation scheme (PTAS), relying on a nontrivial LP relaxation of the problem. We present a (3+ϵ)-approximation as well as PTASs for other special cases of AGAP, where the utility of any item does not depend on the bin in which it is packed. Finally, we derive hardness results for the different variants of AGAP studied in this paper. © 2016 ACM.",Ad placement; Approximation algorithms; Generalized assignment problem; Group packing,Bins; Combinatorial optimization; Marketing; Polynomial approximation; Scheduling; Ad placement; Advertising campaign; All or nothings; Generalized assignment problem; Generalized assignments; Hardness result; Polynomial time approximation schemes; Submodular functions; Approximation algorithms
Sparse text indexing in small space,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968724530&doi=10.1145%2f2836166&partnerID=40&md5=5f6b42e9a95a8b7ddfa43f5450a4d782,"In this work, we present efficient algorithms for constructing sparse suffix trees, sparse suffix arrays, and sparse position heaps for b arbitrary positions of a text T of length n while using only O(b) words of space during the construction. Attempts at breaking the naïve bound of ω(nb) time for constructing sparse suffix trees in O(b) space can be traced back to the origins of string indexing in 1968. First results were not obtained until 1996, but only for the case in which the b suffixes were evenly spaced in T. In this article, there is no constraint on the locations of the suffixes. Our main contribution is to show that the sparse suffix tree (and array) can be constructed in O(nlog2 b) time. To achieve this, we develop a technique that allows one to efficiently answer b longest common prefix queries on suffixes of T, using only O(b) space.We expect that this technique will prove useful in many other applications in which space usage is a concern. Our first solution is Monte Carlo, and outputs the correct tree with high probability. We then give a Las Vegas algorithm, which also uses O(b) space and runs in the same time bounds with high probability when b = O( √ n). Additional trade-offs between space usage and construction time for the Monte Carlo algorithm are given. Finally, we show that, at the expense of slower pattern queries, it is possible to construct sparse position heaps in O(n+ blog b) time and O(b) space. © 2016 ACM.",Karp Rabin fingerprints; Sparse position heap; Sparse suffix array; Sparse suffix sorting; Sparse suffix tree; Sparse text indexing,Algorithms; Economic and social effects; Forestry; Indexing (of information); Monte Carlo methods; Karp Rabin fingerprints; Sparse position heap; Suffix arrays; Suffix sorting; Suffix-trees; Text-indexing; Query processing
Fast constructions of lightweight spanners for general graphs,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968735339&doi=10.1145%2f2836167&partnerID=40&md5=754446a961313820267ca60a5fe6d1ce,"It is long known that for every weighted undirected n-vertex m-edge graph G = (V, E, ω), and every integer k ≥ 1, there exists a ((2k- 1) · (1 + ϵ))-spanner with O(n1+1/k) edges and weight O(k · n1/k · ω(MST(G)), for an arbitrarily small constant ϵ > 0. (Here ω(MST(G)) stands for the weight of the minimum spanning tree of G.) To our knowledge, the only algorithms for constructing sparse and lightweight spanners for general graphs admit high running times. Most notable in this context is the greedy algorithm of Althöfer et al. [1993], analyzed by Chandra et al. [1992], which requires O(m· (n1+1/k + n · log n)) time. In this article, we devise an efficient algorithm for constructing sparse and lightweight spanners. Specifically, our algorithm constructs ((2k - 1) · (1 + ϵ))-spanners with O(k · n1+1/k) edges and weight O(k · n1/k) · ω(MST(G)), where ϵ > 0 is an arbitrarily small constant. The running time of our algorithm is O(k ·m+ min{n · log n,m· α(n)}). Moreover, by slightly increasing the running time we can reduce the other parameters. These results address an open problem by Roditty and Zwick [2004]. © 2016 ACM.",Graph spanners; Light spanners; Sparse graphs,Algorithms; Trees (mathematics); General graph; Graph G; Graph spanners; Greedy algorithms; Lightweight spanners; Minimum spanning trees; Running time; Sparse graphs; Graph theory
The two-edge connectivity survivable-network design problem in planar graphs,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968834214&doi=10.1145%2f2831235&partnerID=40&md5=154c4347ddf98b79c4b822478f8f92da,"Consider the following problem: given a graph with edge costs and a subset Qof vertices, find aminimum-cost subgraph in which there are two edge-disjoint paths connecting every pair of vertices in Q. The problem is a failure-resilient analog of the Steiner tree problem arising, for example, in telecommunications applications. We study a more generalmixed-connectivity formulation, also employed in telecommunications optimization. Given a number (or requirement) r(v) ϵ {0, 1, 2} for each vertex v in the graph, find a minimum-cost subgraph in which there are min{r(u), r(v)} edge-disjoint u-to-v paths for every pair u, v of vertices. We address the problem in planar graphs, considering a popular relaxation in which the solution is allowed to use multiple copies of the input-graph edges (paying separately for each copy). The problem is max SNP-hard in general graphs and strongly NP-hard in planar graphs. We give the first polynomial-time approximation scheme in planar graphs. The running time is O(nlog n).Under the additional restriction that the requirements are only non-zero for vertices on the boundary of a single face of a planar graph, we give a polynomial-time algorithm to find the optimal solution. © 2016 ACM.",Planar graphs; Polynomial time approximation scheme; Survivable network design,Algorithms; Approximation theory; Costs; Graphic methods; Optimization; Polynomial approximation; Polynomials; Trees (mathematics); Edge connectivity; Edge disjoint paths; Planar graph; Polynomial time approximation schemes; Polynomial-time algorithms; Steiner tree problem; Survivable network design; Telecommunications applications; Graph theory
Distributed algorithms for end-to-end packet scheduling in wireless ad hoc networks,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968866122&doi=10.1145%2f2812811&partnerID=40&md5=7ba0c585e2eb44420dc85f363e2a92f9,"Packet scheduling is a particular challenge in wireless networks due to interference from nearby transmissions. A distance-2 interference model serves as a useful abstraction here, and we study packet routing and scheduling under this model of interference. The main focus of our work is the development of fully distributed (decentralized) protocols. We present polylogarithmic/constant factor approximation algorithms for various families of disk graphs (which capture the geometric nature of wireless-signal propagation), as well as near-optimal approximation algorithms for general graphs. A basic distributed coloring procedure, originally due to Luby [1993] (Journal of Computer and System Sciences, 47:250-286, 1993), underlies many of our algorithms. The work of Finocchi et al. [2002] (Proc. ACM-SIAM Symposium on Discrete Algorithms, 2002) showed that a natural modification of this algorithm leads to improved performance. A rigorous explanation of this was left as an open question, and we prove that the modified algorithm is indeed provably better in the worst case. © 2016 ACM.",Distributed algorithms; Packet scheduling; Wireless interference,Ad hoc networks; Approximation algorithms; Distributed computer systems; Packet networks; Parallel algorithms; Scheduling algorithms; Telecommunication networks; Wireless ad hoc networks; Discrete algorithms; Factor approximation algorithms; Interference modeling; Modified algorithms; Packet scheduling; Polylogarithmic; Wireless interference; Wireless signal propagation; Algorithms
A constant factor approximation algorithm for fault-tolerant k -median,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968884479&doi=10.1145%2f2854153&partnerID=40&md5=347984fdd80b5bef94333c1cc638bb35,"In this article, we consider the fault-tolerant k-median problem and give the first constant factor approximation algorithm for it. In the fault-tolerant generalization of the classical k-median problem, each client j needs to be assigned to at least rj ≥ 1 distinct open facilities. The service cost of j is the sum of its distances to the rj facilities, and the k-median constraint restricts the number of open facilities to atmost k. Previously, a constant factor was known only for the special case when all rjs are the same, and alogarithmic approximation ratio was known for the general case. In addition, we present the first polynomial time algorithm for the fault-tolerant k-median problem on a path or an HST by showing that the corresponding LP always has an integral optimal solution. We also consider the fault-tolerant facility location problem, in which the service cost of j can be a weighted sum of its distance to the rj facilities.We give a simple constant factor approximation algorithm, generalizing several previous results that work only for nonincreasing weight vectors. © 2016 ACM.",Approximation algorithms; Facility location; Fault-tolerance; K-median,Algorithms; Fault tolerance; Polynomial approximation; Polynomials; Approximation ratios; Constant factors; Constant-factor approximation algorithms; Facility location problem; Facility locations; K-median; Optimal solutions; Polynomial-time algorithms; Approximation algorithms
On the expected complexity of Voronoi diagrams on terrains,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968820734&doi=10.1145%2f2846099&partnerID=40&md5=8928bf5139b947668617901808a8bdb4,"We investigate the combinatorial complexity of geodesic Voronoi diagrams on polyhedral terrains using a probabilistic analysis. Aronov et al. [2008] prove that, if one makes certain realistic input assumptions on the terrain, this complexity is ω(n+m √n) in the worst case, where n denotes the number of triangles that define the terrain and mdenotes the number of Voronoi sites. We prove that, under a relaxed set of assumptions, the Voronoi diagram has expected complexity O(n+m), given that the sites are sampled uniformly at random from the domain of the terrain (or the surface of the terrain). Furthermore, we present a construction of a terrain that implies a lower bound of ω (nm2/3) on the expected worst-case complexity if these assumptions on the terrain are dropped. As an additional result, we show that the expected fatness of a cell in a random planar Voronoi diagram is bounded by a constant. © 2016 ACM.",Random inputs; Voronoi diagrams on terrains,Computational geometry; Graphic methods; Combinatorial complexity; Geodesic voronoi diagram; Number of triangles; Polyhedral terrains; Probabilistic analysis; Random input; Voronoi diagrams; Worst-case complexity; Landforms
Approximating semi-matchings in streaming and in two-party communication,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968763978&doi=10.1145%2f2898960&partnerID=40&md5=07d508914f9b4b1ded1389c36939b385,"We study the streaming complexity and communication complexity of approximating unweighted semimatchings. A semi-matching in a bipartite graph G = (A, B, E) with n = |A| is a subset of edges S ⊆ E that matches all A vertices to B vertices with the goal usually being to do this as fairly as possible. While the term semi-matching was coined in 2003 by Harvey et al. [2003], the problem had already previously been studied in the scheduling literature under different names. We present a deterministic one-pass streaming algorithm that for any 0 ≤ ϵ ≤ 1 uses space O(n1+ϵ) and computes an O(n(1-ϵ)/2)-approximation to the semi-matching problem. Furthermore, with O(log n) passes it is possible to compute an O(log n)-approximation with space Õ (n). In the one-way two-party communication setting, we show that for every ϵ > 0, deterministic communication protocols for computing an O(n 1 (1+ϵ)c+1 )-approximation require a message of size more than cn bits. We present two deterministic protocols communicating n and 2n edges that compute an O( √ n) and an O(n1/3)-approximation, respectively. Finally, we improve on the results of Harvey et al. [2003] and prove new links between semi-matchings and matchings.While it was known that an optimal semi-matching contains a maximum matching, we show that there is a hierarchical decomposition of an optimal semi-matching into maximum matchings. A similar result holds for semi-matchings that do not admit length-two degree-minimizing paths. © 2016 ACM.",Communication complexity; Job assignment; Semi-matchings; Streaming algorithms,Approximation algorithms; Computational complexity; Image retrieval; Communication complexity; Deterministic communications; Deterministic protocols; Hierarchical decompositions; Job assignments; Matchings; Maximum matchings; Streaming algorithm; Graph theory
Families with infants: Speeding up algorithms for NP-hard problems using FFT,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968782878&doi=10.1145%2f2847419&partnerID=40&md5=4a20fc14db8e0692e761a74fc43cceb3,"Assume that a group of n people is going to an excursion and our task is to seat them into buses with several constraints each saying that a pair of people does not want to see each other in the same bus. This is a well-known graph coloring problem (with n being the number of vertices) and it can be solved in O∗(2n) time by the inclusion-exclusion principle as shown by Björklund, Husfeldt, and Koivisto in 2009. Another approach to solve this problem in O∗ (2n) time is to use the Fast Fourier Transform (FFT). For this, given a graph G one constructs a polynomial PG(x) of degree O∗ (2n) with the following property: G is k-colorable if and only if the coefficient of xm (for some particular value of m) in the k-th power of P(x) is nonzero. Then, it remains to compute this coefficient using FFT. Assume now that we have additional constraints: the group of people contains several infants and these infants should be accompanied by their relatives in a bus. We show that if the number of infants is linear, then the problem can be solved in O∗ ((2 - ϵ)n) time, where ϵ is a positive constant independent of n. We use this approach to improve known bounds for several NP-hard problems (the traveling salesman problem, the graph coloring problem, the problem of counting perfect matchings) on graphs of bounded average degree, as well as to simplify the proofs of several known results. © 2016 ACM.",Algorithms; Chromatic number; Counting perfect matchings; Fast Fourier transform; NP-hard problem; Traveling salesman,Algorithms; Buses; Computational complexity; Fast Fourier transforms; Graph theory; Traveling salesman problem; Average degree; Chromatic number; Graph coloring problem; Graph G; Inclusion exclusion principle; Perfect matchings; Positive constant; Traveling salesman; Problem solving
Forbidden-set distance labels for graphs of bounded doubling dimension,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964389607&doi=10.1145%2f2818694&partnerID=40&md5=42bcdf0b1328c0719ad4039d5657583d,"This article proposes a forbidden-set labeling scheme for the family of unweighted graphs with doubling dimension bounded by α. For an n-vertex graph G in this family, and for any desired precision parameter ϵ > 0, the labeling scheme stores an O(1 + ϵ-1)2α log2 n-bit label at each vertex. Given the labels of two end-vertices s and t, and the labels of a set F of ""forbidden"" vertices and/or edges, our scheme can compute, in O(1 + ϵ)2α · |F|2 log n time, a 1 + ϵ stretch approximation for the distance between s and t in the graph G \ F. The labeling scheme can be extended into a forbidden-set labeled routing scheme with stretch 1 + ϵ for graphs of bounded doubling dimension. © 2016 ACM.",Compact routing; Distance labeling; Doubling dimension; Fault-tolerance; Forbidden sets,Fault tolerance; Compact routing; Distance labeling; Doubling dimensions; Forbidden set; Labeling scheme; N-vertex graph; Precision parameter; Unweighted graphs; Graph theory
Anarchy is free in network creation,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964771624&doi=10.1145%2f2729978&partnerID=40&md5=634b5dd06326dfccf8d0c9e482d027ff,"The Internet has emerged as perhaps the most important network in modern computing, but rather miraculously, it was created through the individual actions of a multitude of agents rather than by a central planning authority. This motivates the game-theoretic study of network formation, and our article considers one of the most well-studied models, originally proposed by Fabrikant et al. In the model, each of n agents corresponds to a vertex, which can create edges to other vertices at a cost of α each, for some parameter α. Every edge can be freely used by every vertex, regardless of who paid the creation cost. To reflect the desire to be close to other vertices, each agent's cost function is further augmented by the sum total of all (graph-theoretic) distances to all other vertices. Previous research proved that for many regimes of the (α, n) parameter space, the total social cost (sum of all agents' costs) of every Nash equilibrium is bounded by at most a constant multiple of the optimal social cost. In algorithmic game-theoretic nomenclature, this approximation ratio is called the price of anarchy. In our article, we significantly sharpen some of those results, proving that for all constant nonintegral α > 2, the price of anarchy is in fact 1 + o(1); that is, not only is it bounded by a constant, but also it tends to 1 as n→∞. For constant integral α ≥ 2, we show that the price of anarchy is bounded away from 1. We provide quantitative estimates on the rates of convergence for both results. © 2016 ACM.",Algorithmic game theory; Extremal graph theory; Network formation,Computation theory; Cost functions; Costs; Game theory; Algorithmic Game Theory; Approximation ratios; Extremal graph theory; Network formation; Parameter spaces; Price of anarchy; Quantitative estimates; Rates of convergence; Graph theory
Width of points in the streaming model,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964575487&doi=10.1145%2f2847259&partnerID=40&md5=1f24b70c46b6ef5f192f372936ba6a90,"In this article, we show how to compute the width of a dynamic set of low-dimensional points in the streaming model. In particular, we assume that the stream contains both insertions of points and deletions of points to a set S, and the goal is to compute the width of the set S, namely the minimal distance between two parallel hyperplanes sandwiching the point set S. Our algorithm (1+ϵ) approximates the width of the set S using space polylogarithmic in the size of S and the aspect ratio of S. This is the first such algorithm that supports both insertions and deletions of points to the set S: previous algorithms for approximating the width of a point set only supported additions [Agarwal et al. 2004; Chan 2006], or a sliding window [Chan and Sadjad 2006]. This solves an open question from the ""2009 Kanpur list"" of open problems in data streams, property testing, and related topics [Indyk et al. 2011].",Computational geometry; Corset; Streaming; Width,Acoustic streaming; Aspect ratio; Geometry; Corset; Insertions and deletions; Low dimensional; Minimal distance; Polylogarithmic; Property-testing; Streaming model; Width; Computational geometry
Computing the distance between piecewise-linear bivariate functions,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964424955&doi=10.1145%2f2847257&partnerID=40&md5=d42170c37e5242b0a238d25a803597da,"We consider the problem of computing the distance between two piecewise-linear bivariate functions f and g defined over a common domain M, induced by the L2 norm-that is, ∥ f - g∥2 = √ ∫M( f - g)2. If f is defined by linear interpolation over a triangulation of M with n triangles and g is defined over another such triangulation, the obvious naive algorithm requires Θ (n2) arithmetic operations to compute this distance.We show that it is possible to compute it in O(nlog4 nlog log n) arithmetic operations by reducing the problem to multipoint evaluation of a certain type of polynomials. We also present several generalizations and an application to terrain matching. © 2016 ACM.",Multipoint evaluation; Piecewise-linear function; Polyhedral terrain,Function evaluation; Surveying; Triangulation; Arithmetic operations; Bivariate functions; Linear Interpolation; Multipoint evaluation; Piece-wise linear functions; Piecewise linear; Polyhedral terrains; Terrain matching; Piecewise linear techniques
Editorial to the special issue on SODA'12,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969850649&doi=10.1145%2f2846001&partnerID=40&md5=18f520006c45b18e6bffbc13f3791f6f,[No abstract available],,
The minset-poset approach to representations of graph connectivity,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964689141&doi=10.1145%2f2764909&partnerID=40&md5=c60aeb5824d4e5377b2768f57db367eb,"Various instances of the minimal-set poset (minset-poset for short) have been proposed in the literature, e.g., the representation of Picard and Queyranne for all st-minimum cuts of a flow network. We begin with an explanation of why this poset structure is common. We show any family of sets F that can be defined by a ""labelling algorithm"" (e.g., the Ford-Fulkerson labelling algorithm for maximum network flow) has an algorithm that constructs the minset poset for F.We implement this algorithm to efficiently find the nodes of the poset when F is the family of minimum edge cuts of an unweighted graph; we also give related algorithms to construct the entire poset for weighted graphs. The rest of the article discusses applications to edge- and vertex connectivity, both combinatorial and algorithmic, that we now describe. For digraphs, a natural interpretation of the minset poset represents all minimum edge cuts. In the special case of undirected graphs, the minset poset is proved to be a variant of the well-known cactus representation of all mincuts. We use the poset algorithms to construct the cactus representation for unweighted graphs in time O(m+ λ2nlog (n/λ)) (λ is the edge connectivity) improving the previous bound O(λn2) for all but the densest graphs. We also construct the cactus representation for weighted graphs in time O(nmlog (n2/m)), the same bound as a previously known algorithm but in linear space O(m). The latter bound also holds for constructing the minset poset for any weighted digraph; the former bound also holds for constructing the nodes of that poset for any unweighted digraph. The poset is used in algorithms to increase the edge connectivity of a graph by adding the fewest edges possible. For directed and undirected graphs, weighted and unweighted, we achieve the time of the preceding two bounds, i.e., essentially the best-known bounds to compute the edge connectivity itself. Some constructions ofminset posets for graph rigidity are also sketched. For vertex connectivity, the minset poset is proved to be a slight variant of the dominator tree. This leads to an algorithm to construct the dominator tree in time O(m) on a RAM. (The algorithm is included in the appendix, since other linear-time algorithms of similar simplicity have recently been presented.).",Cactus representation; Code optimization; Connectivity augmentation; Dominators; Posets,Algorithms; Clustering algorithms; Directed graphs; Flow graphs; Forestry; Graphic methods; Optimization; Set theory; Trees (mathematics); Cactus representation; Code optimization; Connectivity augmentation; Dominators; Posets; Graph theory
Bypassing UGC from some optimal geometric inapproximability results,2016,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964769119&doi=10.1145%2f2737729&partnerID=40&md5=32a79c0fa3406a8def8799fed6e8eb1f,"The Unique Games Conjecture (UGC) has emerged in recent years as the starting point for several optimal inapproximability results.While for none of these results a reverse reduction to Unique Games is known, the assumption of bijective projections in the Label Cover instance nevertheless seems critical in these proofs. In this work, we bypass the need for UGC assumption in inapproximability results for two geometric problems, obtaining a tight NP-hardness result in each case. The first problem, known as Lp Subspace Approximation, is a generalization of the classic least squares regression problem. Here, the input consists of a set of points X = {a1, . . . , am} ⊆ Rn and a parameter k (possibly depending on n). The goal is to find a subspace H of Rn of dimension k that minimizes the lp norm of the Euclidean distances to the points in X. For p = 2, k = n-1, this reduces to the least squares regression problem, while for p= ∞, k = 0 it reduces to the problem of finding a ball of minimum radius enclosing all the points.We show that for any fixed p ∈ (2,∞), and for k = n-1, it is NP-hard to approximate this problem to within a factor of γp - ϵ for constant ϵ > 0, where γp is the pth norm of a standard Gaussian random variable. This matches the γp approximation algorithm obtained by Deshpande, Tulsiani, and Vishnoi who also showed the same hardness result under the UGC. The second problem we study is the related Lp Quadratic GrothendieckMaximization Problem, considered by Kindler, Naor, and Schechtman. Here, the input is a multilinear quadratic form Σni, j=1aij xixj and the goal is to maximize the quadratic form over the lp unit ball, namely, all x with Σn i=1 |xi |p ≤ 1. The problem is polynomial time solvable for p = 2. We show that for any constant p ∈ (2,∞), it is NP-hard to approximate the quadratic form to within a factor of γ 2 p - ϵ for any ϵ > 0. The same hardness factor was shown under the UGC by Kindler et al. We also obtain a γ 2 p-approximation algorithm for the problem using the convex relaxation of the problem defined by Kindler et al. A γ 2 p approximation algorithm has also been independently obtained by Naor and Schechtman. These are the first approximation thresholds, proven under P ≠ NP, that involve the Gaussian random variable in a fundamental way. Note that the problem statements themselves do not explicitly involve the Gaussian distribution. © 2016 ACM.",Grothendieck; Label cover; NP-hardness; Subspace approximation,Gaussian distribution; Hardness; Least squares approximations; Number theory; Optimization; Polynomial approximation; Random variables; Relaxation processes; Gaussian random variable; Grothendieck; Least squares regression; NP-hardness; Np-hardness results; Optimal inapproximability; Subspace approximation; Unique games conjecture; Approximation algorithms
True Contraction Decomposition and Almost ETH-Tight Bipartization for Unit-Disk Graphs,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197788860&doi=10.1145%2f3656042&partnerID=40&md5=1067f428b05e128bc69ed63907a0796c,"We prove a structural theorem for unit-disk graphs, which (roughly) states that given a set D of = unit disks inducing a unit-disk graph Di\D and a number p ϵ [n] one can partition D into p subsets D1, ⋯, Dp such that for every i ϵ [p] and every D ⊆ D the graph obtained from GD by contracting all edges between the vertices in Di\D admits a tree decomposition in which each bag consists of O(p+|D|) cliques. Our theorem can be viewed as an analog for unit-disk graphs of the structural theorems for planar graphs and almost-embeddable graphs proved recently by Marx et al. [SODA '22] and Bandyapadhyay et al. [SODA '22]. By applying our structural theorem, we give several new combinatorial and algorithmic results for unit-disk graphs. On the combinatorial side, we obtain the first Contraction Decomposition Theorem for unit-disk graphs, resolving an open question in the work by Panolan et al. [SODA '19]. On the algorithmic side, we obtain a new algorithm for bipartization (also known as odd cycle transversal) on unit-disk graphs, which runs in 2O(√klogk). nO(1)time, where : denotes the solution size. Our algorithm significantly improves the previous slightly subexponential-time algorithm given by Lokshtanov et al. [SODA '22] which runs in 2O(k27/28). nO(1) time. We also show that the problem cannot be solved in 2O(√k). nO(1) time assuming the Exponential Time Hypothesis, which implies that our algorithm is almost optimal. Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",bipartization; contraction decomposition; Unit-disk graphs,Domain decomposition methods; Trees (mathematics); Algorithmics; Bipartization; Contraction decomposition; Decomposition theorems; Odd cycle transversals; Planar graph; Subexponential time; Tree decomposition; Unit disc graphs; Unit disk; Graphic methods
Introduction: ACM-SIAM Symposium on Discrete Algorithms (SODA) 2022 Special Issue,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197818674&doi=10.1145%2f3655622&partnerID=40&md5=7ef72d82af8431272367feebe9ba37c1,[No abstract available],,
An Improved Algorithm for The k-Dyck Edit Distance Problem,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192385565&doi=10.1145%2f3627539&partnerID=40&md5=ede38bbbae692615418353410c6abdf0,"A Dyck sequence is a sequence of opening and closing parentheses (of various types) that is balanced. The Dyck edit distance of a given sequence of parentheses S is the smallest number of edit operations (insertions, deletions, and substitutions) needed to transform S into a Dyck sequence. We consider the threshold Dyck edit distance problem, where the input is a sequence of parentheses S and a positive integer k, and the goal is to compute the Dyck edit distance of S only if the distance is at most k, and otherwise report that the distance is larger than k. Backurs and Onak [PODS'16] showed that the threshold Dyck edit distance problem can be solved in O(n+k16) time. In this work, we design new algorithms for the threshold Dyck edit distance problem which costs O(n+k4.544184) time with high probability or O(n+k4.853059) deterministically. Our algorithms combine several new structural properties of the Dyck edit distance problem, a refined algorithm for fast (min, +) matrix product, and a careful modification of ideas used in Valiant's parsing algorithm.  © 2024 Copyright held by the owner/author(s).",Dyck language; edit distance; fine-grained complexity,Dyck language; Edit distance; Fine grained; Fine-grained complexity; High probability; Improved * algorithm; Matrix products; Parsing algorithm; Positive integers; Computational complexity
Greedy Spanners in Euclidean Spaces Admit Sublinear Separators,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197783749&doi=10.1145%2f3590771&partnerID=40&md5=088851d6c31f9a3f9c15e2fe5e848a8b,"The greedy spanner in a low-dimensional Euclidean space is a fundamental geometric construction that has been extensively studied over three decades, as it possesses the two most basic properties of a good spanner: constant maximum degree and constant lightness. Recently, Eppstein and Khodabandeh [28] showed that the greedy spanner in ℝ2 admits a sublinear separator in a strong sense: Any subgraph of k vertices of the greedy spanner in ℝ2 has a separator of size O(√k). Their technique is inherently planar and is not extensible to higher dimensions. They left showing the existence of a small separator for the greedy spanner in ℝd for any constant d ≤ 3 as an open problem. In this article, we resolve the problem of Eppstein and Khodabandeh [28] by showing that any subgraph of k vertices of the greedy spanner in ℝd has a separator of size O(k1-1/d). We introduce a new technique that gives a simple criterion for any geometric graph to have a sublinear separator that we dub τ-lanky: A geometric graph is τ-lanky if any ball of radius r cuts at most τ edges of length at least r in the graph. We show that any τ -lanky geometric graph of n vertices in ℝd has a separator of sizeO(τn1.1/d ).We then derive our main result by showing that the greedy spanner is O(1)-lanky. We indeed obtain a more general result that applies to unit ball graphs and point sets of low fractal dimensions in ℝd. Our technique naturally extends to doubling metrics. We use the τ-lanky criterion to show that there exists a (1 +ϵ )-spanner for doubling metrics of dimension d with a constant maximum degree and a separator of size O(n1-1|d); this result resolves an open problem posed by Abam and Har-Peled [1] a decade ago. We then introduce another simple criterion for a graph in doubling metrics of dimension d to have a sublinear separator. We use the new criterion to show that the greedy spanner of an n-point metric space of doubling dimension d has a separator of size O((n1-1|d) + log Δ) where Δ is the spread of the metric; the factor log(Δ) is tightly connected to the fact that, unlike its Euclidean counterpart, the greedy spanner in doubling metrics has unbounded maximum degree. Finally, we discuss algorithmic implications of our results.  Copyright © 2023 held by the owner/author(s). Publication rights licensed to ACM.",geometric graph; greedy spanner; Spanner; sublinear separator,Fractal dimension; Graph theory; Doubling metrics; Euclidean spaces; Geometric graphs; Greedy spanne; Maximum degree; Simple++; Spanne; Subgraphs; Sublinear; Sublinear separator; Separators
"Hopcroft's Problem, Log∗Shaving, Two-dimensional Fractional Cascading, and Decision Trees",2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188263071&doi=10.1145%2f3591357&partnerID=40&md5=9aa19fba1c7bf3227642cb0216489c74,"We revisit Hopcroft's problem and related fundamental problems about geometric range searching. Given n points and n lines in the plane, we show how to count the number of point-line incidence pairs or the number of point-above-line pairs in O(n4/3) time, which matches the conjectured lower bound and improves the best previous time bound of obtained almost 30 years ago by Matoušek [58].We describe two interesting and different ways to achieve the result: The first is randomized and uses a new two-dimensional version of fractional cascading for arrangements of lines; the second is deterministic and uses decision trees in a manner inspired by the sorting technique of Fredman [42]. The second approach extends to any constant dimension. Many consequences follow from these new ideas: For example, we obtain an O(n4/3)-time algorithm for line segment intersection counting in the plane, O(n4/3)-time randomized algorithms for distance selection in the plane and bichromatic closest pair and Euclidean minimum spanning tree in three or four dimensions, and a randomized data structure for halfplane range counting in the plane with O(n4/3) preprocessing time and space and query time.  Copyright © 2023 held by the owner/author(s). Publication rights licensed to ACM.",decision trees; fractional cascading; geometric data structures; incidences; Range searching,Data structures; Deterministics; Fractional cascading; Geometric data structures; Hopcroft; Incidence; Low bound; Range searching; Sorting techniques; Time bound; Two-dimensional; Decision trees
Collapsing the Tower - On the Complexity of Multistage Stochastic IPs,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197795489&doi=10.1145%2f3604554&partnerID=40&md5=cc992fd833681f160905ec5c9780e93b,"In this article, we study the computational complexity of solving a class of block structured integer programs (IPs), the so-called multistage stochastic IPs. A multistage stochastic IP is an IP of the form min {cTx | Ax = b, x ≥ 0, x integral} where the constraint matrix consists of small block matrices ordered on the diagonal line, and for each stage there are larger blocks with few columns connecting the blocks in a treelike fashion. Over the past few years there was enormous progress in the area of block structured IPs. For many of the known block IP classes, such as n-fold, tree-fold, and two-stage stochastic IPs, nearly matching upper and lower bounds are known concerning their computational complexity. One of the major gaps that remained, however, was the parameter dependency in the running time for an algorithm solving multistage stochastic IPs. Previous algorithms require a tower of t exponentials, where t is the number of stages. In contrast, only a double exponential lower bound was known based on the exponential time hypothesis. In this article, we show that the tower of t exponentials is actually not necessary. We show an improved running time of for the algorithm solving multistage stochastic IPs, where d is the sum of columns in the connecting blocks and rn is the number of rows. Hence, we obtain the first bound by an elementary function for the running time of an algorithm solving multistage stochastic IPs. In contrast to previous works, our algorithm has only a triple exponential dependency on the parameters and only doubly exponential for every constant t. By this, we come very close to the known double exponential bound that holds already for two-stage stochastic IPs, i.e., multistage stochastic IPs with two stages. The improved running time of the algorithm is based on new bounds for the proximity of multistage stochastic IPs. The idea behind the bound is based on generalization of a structural lemma originally used for two-stage stochastic IPs. While the structural lemma requires iteration to be applied to multistage stochastic IPs, our generalization directly applies to inherent combinatorial properties of multiple stages. Already a special case of our lemma yields an improved bound for the Graver complexity of multistage stochastic IPs.  Copyright © 2023 held by the owner/author(s). Publication rights licensed to ACM.",integer programming; Multistage stochastic; parameterized complexity; stochastic programming,Computational complexity; Iterative methods; Matrix algebra; Parameter estimation; Stochastic programming; Stochastic systems; Block structured; Double exponential; Exponentials; Integer program; Integer Program- ming; Multi-stages; Multistage stochastic; Parameterized complexity; Running time; Stochastics; Integer programming
Better Sum Estimation via Weighted Sampling,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197765827&doi=10.1145%2f3650030&partnerID=40&md5=ce2afe2e64956bf8a2c0fc62075b5b80,"Given a large set U where each item α ∈ U has weight w(α), we want to estimate the total weight W = Σ α ∈ U w(α) to within factor of 1± ϵ with some constant probability > 1/2. Since n=|U| is large, we want to do this without looking at the entire set U. In the traditional setting in which we are allowed to sample elements from U uniformly, sampling ω (n) items is necessary to provide any non-trivial guarantee on the estimate. Therefore, we investigate this problem in different settings: in the proportional setting we can sample items with probabilities proportional to their weights, and in the hybrid setting we can sample both proportionally and uniformly. These settings have applications, for example, in sublinear-time algorithms and distribution testing. Sum estimation in the proportional and hybrid setting has been considered before by Motwani, Panigrahy, and Xu [ICALP, 2007]. In their article, they give both upper and lower bounds in terms of n. Their bounds are near-matching in terms of n, but not in terms of ϵ. In this article, we improve both their upper and lower bounds. Our bounds are matching up to constant factors in both settings, in terms of both n and ϵ. No lower bounds with dependency on ϵ were known previously. In the proportional setting, we improve their Õ(√n/ϵ7/2) algorithm to O(√n/ϵ). In the hybrid setting, we improve to Õ Õ (3√n/ϵ9/2) to O(3√n/ϵ4/3). Our algorithms are also significantly simpler and do not have large constant factors. We then investigate the previously unexplored scenario in which n is not known to the algorithm. In this case, we obtain a O(√n/ϵ+log n/ϵ2) algorithm for the proportional setting, and a O(√n/ϵ)algorithm for the hybrid setting. This means that in the proportional setting, we may remove the need for advice without greatly increasing the complexity of the problem, while there is a major difference in the hybrid setting. We prove that this difference in the hybrid setting is necessary, by showing a matching lower bound. Our algorithms have applications in the area of sublinear-time graph algorithms. Consider a large graph G=(V, E) and the task of (1 ± ϵ)-approximating |E|. We consider the (standard) settings where we can sample uniformly from E or from both E and V. This relates to sum estimation as follows: we set U = V and the weights to be equal to the degrees. Uniform sampling then corresponds to sampling vertices uniformly. Proportional sampling can be simulated by taking a random edge and picking one of its endpoints at random. If we can only sample uniformly from E, then our results immediately give a O(√\V\/ϵ) algorithm. When we may sample both from E and V, our results imply an algorithm with complexity O(3√|V|/ϵ4/3). Surprisingly, one of our subroutines provides an (1 ± ϵ)-approximation of |E| Õ(d/ϵ2) using expected samples, where d is the average degree, under the mild assumption that at least a constant fraction of vertices are non-isolated. This subroutine works in the setting where we can sample uniformly from both V and E. We find this remarkable since it is O(1/ϵ2) for sparse graphs.  Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",edge counting; lower bound; sublinear algorithms; Weighted sampling,Constant factors; Edge countings; Low bound; Matchings; Non-trivial; Probability proportional; Sublinear algorithm; Sublinear time algorithms; Upper and lower bounds; Weighted sampling
Isomorphism Testing for Graphs Excluding Small Topological Subgraphs,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197752661&doi=10.1145%2f3651986&partnerID=40&md5=5bc51fd5b99ac33c370dc376c9311a9d,"We give an isomorphism test that runs in time npolylog(h) on all n-vertex graphs excluding some h-vertex graph as a topological subgraph. Previous results state that isomorphism for such graphs can be tested in time npolylog(h) (Babai, STOC 2016) and n{f(h) for some function f (Grohe and Marx, SIAM J. Comp., 2015).Our result also unifies and extends previous isomorphism tests for graphs of maximum degree d running in time npolylog(d) (SIAM J. Comp., 2023) and for graphs of Hadwiger number h running in time npolylog(h) (SIAM J. Comp., 2023).  Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",automorphism groups; Graph isomorphism; topological subgraphs; Weisfeiler-Leman algorithm,Graph theory; Graphic methods; Automorphism groups; Graph isomorphism; Isomorphism testing; N-vertex graph; Polylogs; Running-in; Subgraphs; Topological subgraph; Vertex graphs; Weisfeile-leman algorithm; Set theory
The Fine-Grained Complexity of Graph Homomorphism Parameterized by Clique-Width,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196943364&doi=10.1145%2f3652514&partnerID=40&md5=08ac45f48c2b26e1f3367bdae2de8bb0,"The generic homomorphism problem, which asks whether an input graph G admits a homomorphism into a fixed target graph H, has been widely studied in the literature. In this article, we provide a fine-grained complexity classification of the running time of the homomorphism problem with respect to the clique-width of G (denoted cw) for virtually all choices of H under the Strong Exponential Time Hypothesis. In particular, we identify a property of H called the signature number B(H) and show that for each H, the homomorphism problem can be solved in time O∗(B(H)cw). Crucially, we then show that this algorithm can be used to obtain essentially tight upper bounds. Specifically, we provide a reduction that yields matching lower bounds for each H that is either a projective core or a graph admitting a factorization with additional properties-allowing us to cover all possible target graphs under long-standing conjectures. © 2024 Copyright held by the owner/author(s).",clique-width; fine-grained complexity; Homomorphism,Clique-width; Fine grained; Fine-grained complexity; Fixed targets; Graph G; Graph homomorphisms; Homomorphism; Input graphs; Parameterized; Property; Graph theory
On the Two-Dimensional Knapsack Problem for Convex Polygons,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191654234&doi=10.1145%2f3644390&partnerID=40&md5=4f35c929e41eae12026a7eaf028f58a0,"We study the two-dimensional geometric knapsack problem for convex polygons. Given a set of weighted convex polygons and a square knapsack, the goal is to select the most profitable subset of the given polygons that fits non-overlappingly into the knapsack. We allow to rotate the polygons by arbitrary angles. We present a quasi-polynomial time O(1)-approximation algorithm for the general case and a pseudopolynomial time O(1)-approximation algorithm if all input polygons are triangles, both assuming polynomially bounded integral input data. Additionally, we give a quasi-polynomial time algorithm that computes a solution of optimal weight under resource augmentation - that is, we allow to increase the size of the knapsack by a factor of 1+δfor some δ> 0 but compare ourselves with the optimal solution for the original knapsack. To the best of our knowledge, these are the first results for two-dimensional geometric knapsack in which the input objects are more general than axis-parallel rectangles or circles and in which the input polygons can be rotated by arbitrary angles. © 2024 Copyright held by the owner/author(s).",Approximation algorithms; geometric knapsack problem; polygons; rotation,Combinatorial optimization; Computational geometry; Polynomial approximation; Arbitrary angles; Convex polygon; Geometric knapsack problem; Knapsack problems; Knapsacks; Polygon; Pseudopolynomials; Quasi-polynomial time; Two-dimensional; Two-dimensional knapsack problem; Approximation algorithms
Generic Non-recursive Suffix Array Construction,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191659770&doi=10.1145%2f3641854&partnerID=40&md5=25ebe8ee69086e821d5d8df5cdbffb0a,"The suffix array is arguably one of the most important data structures in sequence analysis and consequently there is a multitude of suffix sorting algorithms. However, to this date the GSACA algorithm introduced in 2015 is the only known non-recursive linear-time suffix array construction algorithm (SACA). Despite its interesting theoretical properties, there has been little effort in improving GSACA's non-competitive real-world performance. There is a super-linear algorithm DSH, which relies on the same sorting principle and is faster than DivSufSort, the fastest SACA for over a decade. The purpose of this article is twofold: We analyse the sorting principle used in GSACA and DSH and exploit its properties to give an optimised linear-time algorithm, and we show that it can be very elegantly used to compute both the original extended Burrows-Wheeler transform (eBWT) and a bijective version of the Burrows-Wheeler transform (BBWT) in linear time. We call the algorithm ""generic,""since it can be used to compute the regular suffix array and the variants used for the BBWT and eBWT. Our suffix array construction algorithm is not only significantly faster than GSACA but also outperforms DivSufSort and DSH. Our BBWT-algorithm is faster than or competitive with all other tested BBWT construction implementations on large or repetitive data, and our eBWT-algorithm is faster than all other programs on data that is not extremely repetitive. © 2024 Copyright held by the owner/author(s).",bijective Burrows-Wheeler transform; string algorithms; Suffix array; suffix sorting,Sorting; Bijective burrow-wheeler transform; Burrows-Wheeler Transform; Construction algorithms; Linear time; Property; Sequence analysis; String algorithms; Suffix arrays; Suffix sorting; Transform algorithm; Clustering algorithms
The Complexity of Finding Fair Many-to-One Matchings,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191652462&doi=10.1145%2f3649220&partnerID=40&md5=55f53634dbd667ea84e83f55a12c2f94,"We analyze the (parameterized) computational complexity of ""fair""variants of bipartite many-to-one matching, where each vertex from the ""left""side is matched to exactly one vertex and each vertex from the ""right""side may be matched to multiple vertices. We want to find a ""fair""matching, in which each vertex from the right side is matched to a ""fair""set of vertices. Assuming that each vertex from the left side has one color modeling its ""attribute"", we study two fairness criteria. For instance, in one of them, we deem a vertex set fair if for any two colors, the difference between the numbers of their occurrences does not exceed a given threshold. Fairness is, for instance, relevant when finding many-to-one matchings between students and colleges, voters and constituencies, and applicants and firms. Here colors may model sociodemographic attributes, party memberships, and qualifications, respectively.We show that finding a fair many-to-one matching is NP-hard even for three colors and maximum degree five. Our main contribution is the design of fixed-parameter tractable algorithms with respect to the number of vertices on the right side. Our algorithms make use of a variety of techniques including color coding. At the core lie integer linear programs encoding Hall like conditions. We establish the correctness of our integer programs, based on Frank's separation theorem [Frank, Discrete Math. 1982]. We further obtain complete complexity dichotomies regarding the number of colors and the maximum degree of each side. © 2024 Copyright held by the owner/author(s).",algorithmic fairness; color coding; FPT; Graph theory; ILP; NP-hardness; polynomial-time algorithms; submodular and supermodular functions,Graph theory; Integer programming; Polynomial approximation; Students; Algorithmic fairness; Algorithmics; Colour coding; FPT; ILP; Matchings; NP-hardness; Polynomial-time algorithms; Submodular functions; Supermodular functions; Color
Contraction Decomposition in Unit Disk Graphs and Algorithmic Applications in Parameterized Complexity,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191663292&doi=10.1145%2f3648594&partnerID=40&md5=eec1b3e158cba6e6b88dc72b716a0f77,"We give a new decomposition theorem in unit disk graphs (UDGs) and demonstrate its applicability in the fields of Structural Graph Theory and Parameterized Complexity. First, our new decomposition theorem shows that the class of UDGs admits an ""almost""Contraction Decomposition Theorem. Prior studies on this topic exhibited that the classes of planar graphs [Klein, SICOMP, 2008], graphs of bounded genus [Demaine, Hajiaghayi and Mohar, Combinatorica 2010], and H-minor free graphs [Demaine, Hajiaghayi and Kawarabayashi, STOC 2011] admit a Contraction Decomposition Theorem. Even bounded-degree UDGs can contain arbitrarily large cliques as minors, and therefore our result is a significant advance in the study of contraction decompositions. Additionally, this result answers an open question posed by Hajiaghayi (www.youtube.com/watch?v=2Bq2gy1N01w) regarding the existence of contraction decompositions for classes of graphs beyond H-minor free graphs though under a relaxation of the original formulation.Second, we present a ""parameteric version""of our new decomposition theorem. We prove that there is an algorithm that, given a UDG G and a positive integer k, runs in polynomial time and outputs a collection of tree decompositions of G with the following properties. Each bag in any of these tree decompositions can be partitioned into connected pieces (we call this measure the chunkiness of the tree decomposition). Moreover, for any subset S of at most k edges in G, there is a tree decomposition in the collection such that S is well preserved in the decomposition in the following sense. For any bag in the tree decomposition and any edge in S with both endpoints in the bag, either its endpoints lie in different pieces or they lie in a piece that is a clique. Having this decomposition at hand, we show that the design of parameterized algorithms for some cut problems becomes elementary. In particular, our algorithmic applications include single-exponential (or slightly super-exponential) algorithms for well-studied problems such as Min Bisection, Steiner Cut, s-Way Cut, and Edge Multiway Cut-Uncut on UDGs; these algorithms are substantially faster than the best-known algorithms for these problems on general graphs. © 2024 Association for Computing Machinery.",Additional Key Words and PhrasesParameterized complexity; contraction decomposition; Min bisection; unit disk graph,Computational complexity; Domain decomposition methods; Parallel processing systems; Parameterization; Polynomial approximation; Trees (mathematics); Additional key word and phrasesparameterized complexity; Algorithmic applications; Contraction decomposition; Decomposition theorems; Free graphs; Key words; MIN BISECTION; Parameterized complexity; Tree decomposition; Unit disc graphs; Graphic methods
Width Helps and Hinders Splitting Flows,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191659847&doi=10.1145%2f3641820&partnerID=40&md5=ec24e05aca3d9f6ff2f95500a76e8bcb,"Minimum flow decomposition (MFD) is the NP-hard problem of finding a smallest decomposition of a network flow/circulation X on a directed graph G into weighted source-to-sink paths whose weighted sum equals X. We show that, for acyclic graphs, considering the width of the graph (the minimum number of paths needed to cover all of its edges) yields advances in our understanding of its approximability. For the version of the problem that uses only non-negative weights, we identify and characterise a new class of width-stable graphs, for which a popular heuristic is a O(log Val (X))-approximation (Val(X) being the total flow of X), and strengthen its worst-case approximation ratio from to ω (m/log m) for sparse graphs, where m is the number of edges in the graph. We also study a new problem on graphs with cycles, Minimum Cost Circulation Decomposition (MCCD), and show that it generalises MFD through a simple reduction. For the version allowing also negative weights, we give a (glog g-X g-g+1)-approximation (g-X g-being the maximum absolute value of X on any edge) using a power-of-two approach, combined with parity fixing arguments and a decomposition of unitary circulations (g-X g-≤ 1), using a generalised notion of width for this problem. Finally, we disprove a conjecture about the linear independence of minimum (non-negative) flow decompositions posed by Kloster et al. [2018], but show that its useful implication (polynomial-time assignments of weights to a given set of paths to decompose a flow) holds for the negative version. © 2024 Copyright held by the owner/author(s).",Flow decomposition; graph width,Computational complexity; Flow graphs; Graphic methods; Flow circulation; Flow decomposition; Graph G; Graph width; Minimum flow; Networks flows; Non negatives; Source to sinks; Splittings; Weighted Sum; Polynomial approximation
Flow-augmentation II: Undirected Graphs,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191885863&doi=10.1145%2f3641105&partnerID=40&md5=eecaafef60b4483b3df399503294db7a,"We present an undirected version of the recently introduced flow-augmentation technique: Given an undirected multigraph G with distinguished vertices s,t g V(G) and an integer k, one can in randomized k(1) g (|V(G)| + |E(G)|) time sample a set A gS† such that the following holds: for every inclusion-wise minimal st-cut Z in G of cardinality at most k, Z becomes a minimum-cardinality cut between s and t in G+A (i.e., in the multigraph G with all edges of A added) with probability 2-(k log k).Compared to the version for directed graphs [STOC 2022], the version presented here has improved success probability (2-(k log k) instead of 2-(k4 log k)), linear dependency on the graph size in the running time bound, and an arguably simpler proof.An immediate corollary is that the Bi-objective st-Cut problem can be solved in randomized FPT time 2(k log k) (|V(G)|+|E(G)|) on undirected graphs. Copyright © 2024 held by the owner/author(s).",Additional Key Words and PhrasesFixed-parameter tractability; flow-augmentation; graph separation problems,Flow graphs; Additional key word and phrasesfixed-parameter tractability; Augmentation techniques; Cardinalities; Flow-augmentation; Graph separation problem; Key words; Multigraphs; Parameter tractability; Separation problems; Undirected graph; Undirected graphs
Map Matching Queries on Realistic Input Graphs under the Fréchet Distance,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191654731&doi=10.1145%2f3643683&partnerID=40&md5=cb9ac99f5149f869154629d66ce575b1,"Map matching is a common preprocessing step for analysing vehicle trajectories. In the theory community, the most popular approach for map matching is to compute a path on the road network that is the most spatially similar to the trajectory, where spatial similarity is measured using the Fréchet distance. A shortcoming of existing map matching algorithms under the Fréchet distance is that every time a trajectory is matched, the entire road network needs to be reprocessed from scratch. An open problem is whether one can preprocess the road network into a data structure, so that map matching queries can be answered in sublinear time.In this article, we investigate map matching queries under the Fréchet distance. We provide a negative result for geometric planar graphs. We show that, unless SETH fails, there is no data structure that can be constructed in polynomial time that answers map matching queries in O((pq)1-δ) query time for any δ> 0, where p and q are the complexities of the geometric planar graph and the query trajectory, respectively. We provide a positive result for realistic input graphs, which we regard as the main result of this article. We show that for c-packed graphs, one can construct a data structure of size that can answer (1+ϵ)-approximate map matching queries in time, where hides lower-order factors and dependence on ϵ. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximation algorithms; Computational geometry; Fréchet distance.; map matching,Computation theory; Computational geometry; Data structures; Graph theory; Graphic methods; Polynomial approximation; Query processing; Roads and streets; Trajectories; Frechet distance; Frechet distance.; Input graphs; Map matching; Matching query; Planar graph; Pre-processing step; Road network; Spatial similarity; Vehicle trajectories; Approximation algorithms
Counting List Homomorphisms from Graphs of Bounded Treewidth: Tight Complexity Bounds,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191793252&doi=10.1145%2f3640814&partnerID=40&md5=07a027deb345fae4ed018d08374ef322,"The goal of this work is to give precise bounds on the counting complexity of a family of generalized coloring problems (list homomorphisms) on bounded-treewidth graphs. Given graphs G, H, and lists L (v) gS† V(H) for every vg V(G), a f:V(G) V(H) that preserves the edges (i.e., uvg E(G) implies f(u)f(v)g E(H)) and respects the lists (i.e., f(v)g L(v)). Standard techniques show that if G is given with a tree decomposition of width t, then the number of list homomorphisms can be counted in time |V(H)|tgn(1). Our main result is determining, for every fixed graph H, how much the base |V(H)| in the running time can be improved. For a connected graph H, we define irr(H) in the following way: if H has a loop or is nonbipartite, then irr(H) is the maximum size of a set SgS† V(H) where any two vertices have different neighborhoods; if H is bipartite, then irr(H) is the maximum size of such a set that is fully in one of the bipartition classes. For disconnected H, we define irr(H) as the maximum of irr(C) over every connected component C of H. It follows from earlier results that if irr(H)=1, then the problem of counting list homomorphisms to H is polynomial-time solvable, and otherwise it is #P-hard. We show that, for every fixed graph H, the number of list homomorphisms from (G,L) to H - can be counted in time if a tree decomposition of G having width at most t is given in the input, and, - given that , cannot be counted in time for any , even if a tree decomposition of G having width at most t is given in the input, unless the Counting Strong Exponential-Time Hypothesis (#SETH) fails. Thereby, we give a precise and complete complexity classification featuring matching upper and lower bounds for all target graphs with or without loops. Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGraph homomorphism; counting complexity; list homomorphism; parameterized complexity; tight bounds; treewidth,Graphic methods; Trees (mathematics); Additional key word and phrasesgraph homomorphism; Bounded treewidth; Counting complexity; Fixed graphs; Key words; List-homomorphisms; Parameterized complexity; Tight bound; Tree decomposition; Tree-width; Polynomial approximation
Scalable High-Quality Hypergraph Partitioning,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184762372&doi=10.1145%2f3626527&partnerID=40&md5=44a630d99c0509113cc6ffd918e766f6,"Balanced hypergraph partitioning is an NP-hard problem with many applications, e.g., optimizing communication in distributed data placement problems. The goal is to place all nodes across k different blocks of bounded size, such that hyperedges span as few parts as possible. This problem is well-studied in sequential and distributed settings, but not in shared-memory. We close this gap by devising efficient and scalable shared-memory algorithms for all components employed in the best sequential solvers without compromises with regards to solution quality. This work presents the scalable and high-quality hypergraph partitioning framework Mt-KaHyPar. Its most important components are parallel improvement algorithms based on the FM algorithm and maximum flows, as well as a parallel clustering algorithm for coarsening – which are used in a multilevel scheme with log(n) levels. As additional components, we parallelize the n-level partitioning scheme, devise a deterministic version of our algorithm, and present optimizations for plain graphs. We evaluate our solver on more than 800 graphs and hypergraphs, and compare it with 25 different algorithms from the literature. Our fastest configuration outperforms almost all existing hypergraph partitioners with regards to both solution quality and running time. Our highest-quality configuration achieves the same solution quality as the best sequential partitioner KaHyPar, while being an order of magnitude faster with ten threads. Thus, two of our configurations occupy all fronts of the Pareto curve for hypergraph partitioning. Furthermore, our solvers exhibit good speedups, e.g., 29.6x in the geometric mean on 64 cores (deterministic), 22.3x (log(n)-level), and 25.9x (n-level). © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clustering; community detection; concurrent gain computations; determinism; FM algorithm; Graph and hypergraph partitioning; high-quality; maximum flows; multilevel algorithm; shared-memory; work-stealing,Coarsening; Computational complexity; Flow graphs; Graphic methods; Memory architecture; Optimization; Clusterings; Community detection; Concurrent gain computation; Determinism; FM algorithm; Graph Partitioning; Graphs and hypergraphs; High quality; Hypergraph partitioning; Maximum flows; Multilevel algorithm; Shared memory; Work-stealing; Clustering algorithms
Approximating Sparsest Cut in Low-treewidth Graphs via Combinatorial Diameter,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184758212&doi=10.1145%2f3632623&partnerID=40&md5=797f35f88e2e8a04590b7abef04cc615,"The fundamental Sparsest Cut problem takes as input a graph G together with edge capacities and demands and seeks a cut that minimizes the ratio between the capacities and demands across the cuts. For n-vertex graphs G of treewidth k, Chlamtáč, Krauthgamer, and Raghavendra (APPROX’10) presented an algorithm that yields a factor-22k approximation in time 2O(k) · nO(1). Later, Gupta, Talwar, and Witmer (STOC’13) showed how to obtain a 2-approximation algorithm with a blown-up runtime of nO(k). An intriguing open question is whether one can simultaneously achieve the best out of the aforementioned results, that is, a factor-2 approximation in time 2O(k) · nO(1). In this article, we make significant progress towards this goal via the following results: (i) A factor-O(k2) approximation that runs in time 2O(k) ·nO(1), directly improving the work of Chlamtáč et al. while keeping the runtime single-exponential in k. (ii) For any ε ∈ (0, 1], a factor-O(1/ε2) approximation whose runtime is 2O(k1+ε/ε) · nO(1), implying a constant-factor approximation whose runtime is nearly single-exponential in k and a factor-O(log2 k) approximation in time kO(k) · nO(1). Key to these results is a new measure of a tree decomposition that we call combinatorial diameter, which may be of independent interest. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Trees (mathematics); Constant factor approximation; Edge capacity; Exponentials; Graph G; N-vertex graph; Runtimes; Sparsest cut; Tree decomposition; Tree-width; Approximation algorithms
Fast and Perfect Sampling of Subgraphs and Polymer Systems,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184780876&doi=10.1145%2f3632294&partnerID=40&md5=e877301a043186eee5a112856953a215,"We give an efficient perfect sampling algorithm for weighted, connected induced subgraphs (or graphlets) of rooted, bounded degree graphs. Our algorithm utilizes a vertex-percolation process with a carefully chosen rejection filter and works under a percolation subcriticality condition. We show that this condition is optimal in the sense that the task of (approximately) sampling weighted rooted graphlets becomes impossible in finite expected time for infinite graphs and intractable for finite graphs when the condition does not hold. We apply our sampling algorithm as a subroutine to give near linear-time perfect sampling algorithms for polymer models and weighted non-rooted graphlets in finite graphs, two widely studied yet very different problems. This new perfect sampling algorithm for polymer models gives improved sampling algorithms for spin systems at low temperatures on expander graphs and unbalanced bipartite graphs, among other applications. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximate counting; polymer models; Sampling algorithms; spin systems; subgraphs,Data mining; Graph theory; Graphic methods; Learning algorithms; Spin dynamics; Approximate counting; Condition; Finite graphs; Graphlets; Perfect sampling; Polymer models; Sampling algorithm; Spin systems; Spin-s systems; Subgraphs; Solvents
Fast Sampling via Spectral Independence Beyond Bounded-degree Graphs,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184774890&doi=10.1145%2f3631354&partnerID=40&md5=06c6bca7811ddac6c9c2e72b164d508c,"Spectral independence is a recently developed framework for obtaining sharp bounds on the convergence time of the classical Glauber dynamics. This new framework has yielded optimal O(n log n) sampling algorithms on bounded-degree graphs for a large class of problems throughout the so-called uniqueness regime, including, for example, the problems of sampling independent sets, matchings, and Ising-model configurations. Our main contribution is to relax the bounded-degree assumption that has so far been important in establishing and applying spectral independence. Previous methods for avoiding degree bounds rely on using Lp-norms to analyse contraction on graphs with bounded connective constant (Sinclair, Srivastava, and Yin, FOCS’13). The non-linearity of Lp-norms is an obstacle to applying these results to bound spectral independence. Our solution is to capture the Lp-analysis recursively by amortising over the subtrees of the recurrence used to analyse contraction. Our method generalises previous analyses that applied only to bounded-degree graphs. As a main application of our techniques, we consider the random graph G(n, d/n), where the previously known algorithms run in time nO(log d) or applied only to large d. We refine these algorithmic bounds significantly, and develop fast nearly linear algorithms based on Glauber dynamics that apply to all constant d, throughout the uniqueness regime. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Glauber dynamics; sparse random graphs; spectral independence,Dynamics; Graph theory; Graphic methods; Bounded degree graphs; Convergence time; Fast sampling; Glauber dynamics; Independent set; Random graphs; Sampling algorithm; Sharp bounds; Sparse random graph; Spectral independence; Ising model
On the External Validity of Average-case Analyses of Graph Algorithms,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184778661&doi=10.1145%2f3633778&partnerID=40&md5=e2981de222ce0206c6447c5726e42a97,"The number one criticism of average-case analysis is that we do not actually know the probability distribution of real-world inputs. Thus, analyzing an algorithm on some random model has no implications for practical performance. At its core, this criticism doubts the existence of external validity; i.e., it assumes that algorithmic behavior on the somewhat simple and clean models does not translate beyond the models to practical performance real-world input. With this article, we provide a first step toward studying the question of external validity systematically. To this end, we evaluate the performance of six graph algorithms on a collection of 2,740 sparse real-world networks depending on two properties: heterogeneity (variance in the degree distribution) and locality (tendency of edges to connect vertices that are already close). We compare this with the performance on generated networks with varying locality and heterogeneity. We find that the performance in the idealized setting of network models translates surprisingly well to real-world networks. Moreover, heterogeneity and locality appear to be the core properties impacting the performance of many graph algorithms. © 2024 Copyright held by the owner/author(s).",Average case; empirical evaluation; network models,Average-case; Average-case analysis; Empirical evaluations; External validities; Graph algorithms; Network models; Performance; Property; Real-world; Real-world networks; Probability distributions
Popular Matchings with One-Sided Bias,2024,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184759511&doi=10.1145%2f3638764&partnerID=40&md5=a70798acb1e117a04d84dfd0aa892a5c,"Let G = (A∪B, E) be a bipartite graph where the set A consists of agents or main players and the set B consists of jobs or secondary players. Every vertex in A ∪ B has a strict ranking of its neighbors. A matching M is popular if for any matching N, the number of vertices that prefer M to N is at least the number that prefer N to M. Popular matchings always exist in G since every stable matching is popular. A matching M is A-popular if for any matching N, the number of agents (i.e., vertices in A) that prefer M to N is at least the number of agents that prefer N to M. Unlike popular matchings, A-popular matchings need not exist in a given instance G and there is a simple linear time algorithm to decide if G admits an A-popular matching and compute one, if so. We consider the problem of deciding if G admits a matching that is both popular and A-popular and finding one, if so. We call such matchings fully popular. A fully popular matching is useful when A is the more important side—so along with overall popularity, we would like to maintain “popularity within the set A”. A fully popular matching is not necessarily a min-size/max-size popular matching and all known polynomial-time algorithms for popular matching problems compute either min-size or max-size popular matchings. Here we show a linear time algorithm for the fully popular matching problem, thus our result shows a new tractable subclass of popular matchings. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bipartite graphs; LP-duality; stable matchings,Graph theory; Polynomial approximation; Bipartite graphs; Linear-time algorithms; LP-duality; Matchings; Polynomial-time algorithms; Popular matching; Popular matching problems; Simplest linear; Stable matching; Clustering algorithms
"Genome Assembly, from Practice to Theory: Safe, Complete and Linear-Time",2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184753756&doi=10.1145%2f3632176&partnerID=40&md5=e3079470bb1790efed7a1e67f585538d,"Genome assembly asks to reconstruct an unknown string from many shorter substrings of it. Even though it is one of the key problems in Bioinformatics, it is generally lacking major theoretical advances. Its hardness stems both from practical issues (size and errors of real data), and from the fact that problem formulations inherently admit multiple solutions. Given these, at their core, most state-of-the-art assemblers are based on finding non-branching paths (unitigs) in an assembly graph. While such paths constitute only partial assemblies, they are likely to be correct. More precisely, if one defines a genome assembly solution as a closed arc-covering walk of the graph, then unitigs appear in all solutions, being thus safe partial solutions. Until recently, it was open what are all the safe walks of an assembly graph. Tomescu and Medvedev (RECOMB 2016) characterized all such safe walks (omnitigs), thus giving the first safe and complete genome assembly algorithm. Even though maximal omnitig finding was later improved to quadratic time by Cairo et al. (ACM Trans. Algorithms 2019), it remained open whether the crucial linear-time feature of finding unitigs can be attained with omnitigs. We answer this question affirmatively, by describing a surprising O(m)-time algorithm to identify all maximal omnitigs of a graph with n nodes and m arcs, notwithstanding the existence of families of graphs with Θ(mn) total maximal omnitig size. This is based on the discovery of a family of walks (macrotigs) with the property that all the non-trivial omnitigs are univocal extensions of subwalks of a macrotig. This has two consequences: (1) A linear-time output-sensitive algorithm enumerating all maximal omnitigs. (2) A compact O(m) representation of all maximal omnitigs, which allows, e.g., for O(m)-time computation of various statistics on them. Our results close a long-standing theoretical question inspired by practical genome assemblers, originating with the use of unitigs in 1995. We envision our results to be at the core of a reverse transfer from theory to practical and complete genome assembly programs, as has been the case for other key Bioinformatics problems. © 2023 Copyright held by the owner/author(s).",,Computation theory; Genes; Graph theory; Program assemblers; All solutions; Assembly solution; Complete genomes; Genome assembly; Linear time; Multiple-solutions; Practical issues; Problem formulation; State of the art; Sub-strings; Bioinformatics
Cluster Editing Parameterized above Modification-disjoint P3-packings,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177740994&doi=10.1145%2f3626526&partnerID=40&md5=aa1c3990c3857287ce97429f2a00c58a,"Given a graph G = (V, E) and an integer k, the Cluster Editing problem asks whether we can transform G into a union of vertex-disjoint cliques by at most k modifications (edge deletions or insertions). In this paper, we study the following variant of Cluster Editing. We are given a graph G = (V, E), a packing H of modification-disjoint induced P3s (no pair of P3s in H share an edge or non-edge) and an integer ℓ. The task is to decide whether G can be transformed into a union of vertex-disjoint cliques by at most ℓ + |H | modifications (edge deletions or insertions). We show that this problem is NP-hard even when ℓ = 0 (in which case the problem asks to turn G into a disjoint union of cliques by performing exactly one edge deletion or insertion per element of H) and when each vertex is in at most 23 P3s of the packing. This answers negatively a question of van Bevern, Froese, and Komusiewicz (CSR 2016, ToCS 2018), repeated by C. Komusiewicz at Shonan meeting no. 144 in March 2019. We then initiate the study to find the largest integer c such that the problem remains tractable when restricting to packings such that each vertex is in at most c packed P3s. Here packed P3s are those belonging to the packing H . Van Bevern et al. showed that the case c = 1 is fixed-parameter tractable with respect to ℓ and we show that the case c = 2 is solvable in |V |2ℓ+O(1) time. © 2023 Copyright held by the owner/author(s).",fixed-parameter tractability; Graph algorithms; parameterized complexity,Clustering algorithms; Computational complexity; Graph theory; Cluster editing; Fixed-parameter tractability; Graph algorithms; Graph G; K modification; NP-hard; Parameterized; Parameterized complexity; Vertex disjoint; Parameter estimation
Additive Sparsification of CSPs,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184065851&doi=10.1145%2f3625824&partnerID=40&md5=e8293762de1feedf69aac9c227fa0c99,"Multiplicative cut sparsifiers, introduced by Benczúr and Karger [STOC’96], have proved extremely influential and found various applications. Precise characterisations were established for sparsifiability of graphs with other 2-variable predicates on Boolean domains by Filtser and Krauthgamer [SIDMA’17] and non-Boolean domains by Butti and Živný [SIDMA’20]. Bansal, Svensson and Trevisan [FOCS’19] introduced a weaker notion of sparsification termed “additive sparsification”, which does not require weights on the edges of the graph. In particular, Bansal et al. designed algorithms for additive sparsifiers for cuts in graphs and hypergraphs. As our main result, we establish that all Boolean Constraint Satisfaction Problems (CSPs) admit an additive sparsifier; that is, for every Boolean predicate P : {0, 1}k → {0, 1} of a fixed arity k, we show that CSP(P) admits an additive sparsifier. Under our newly introduced notion of all-but-one sparsification for non-Boolean predicates, we show that CSP(P) admits an additive sparsifier for any predicate P : Dk → {0, 1} of a fixed arity k on an arbitrary finite domain D. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Constraint satisfaction, sparsification","Constraint satisfaction problems; Boolean constraint; Boolean domain; Boolean predicates; Constraint Satisfaction; Constraint satisfaction, sparsification; Constraint-satisfaction problems; Finite domains; Graphs and hypergraphs; Sparsification; Additives"
Shortest Cycles with Monotone Submodular Costs,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184756345&doi=10.1145%2f3626824&partnerID=40&md5=c6a34cca1c10a0697726ce0200d2a9d6,"We introduce the following submodular generalization of the Shortest Cycle problem. For a nonnegative monotone submodular cost function f defined on the edges (or the vertices) of an undirected graph G, we seek for a cycle C in G of minimum cost OPT = f (C). We give an algorithm that given an n-vertex graph G, parameter ε > 0, and the function f represented by an oracle, in time nO(log 1/ε) finds a cycle C in G with f (C) ≤ (1 + ε) · OPT. This is in sharp contrast with the non-approximability of the closely related Monotone Submodular Shortest (s, t)-Path problem, which requires exponentially many queries to the oracle for finding an n2/3−ε-approximation Goel et al. [7], FOCS 2009. We complement our algorithm with a matching lower bound. We show that for every ε > 0, obtaining a (1 + ε)-approximation requires at least nΩ(log 1/ε) queries to the oracle. When the function f is integer-valued, our algorithm yields that a cycle of cost OPT can be found in time nO(log OPT). In particular, for OPT = nO(1) this gives a quasipolynomial-time algorithm computing a cycle of minimum submodular cost. Interestingly, while a quasipolynomial-time algorithm often serves as a good indication that a polynomial time complexity could be achieved, we show a lower bound that nO(log n) queries are required even when OPT = O(n). We also consider special cases of monotone submodular functions, corresponding to the number of different color classes needed to cover a cycle in an edge-colored multigraph G. For special cases of the corresponding minimization problem, we obtain fixed-parameter tractable algorithms and polynomial-time algorithms, when restricted to certain classes of inputs. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",polynomial-time approximation schemes; shortest cycle; Submodular functions,Approximation algorithms; Parameter estimation; Polynomial approximation; Undirected graphs; Generalisation; Graph G; Low bound; Non negatives; Polynomial time approximation schemes; Quasi-polynomial time; Short cycle; Submodular; Submodular functions; Time algorithms; Cost functions
A Cubic Algorithm for Computing the Hermite Normal Form of a Nonsingular Integer Matrix,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178909513&doi=10.1145%2f3617996&partnerID=40&md5=c8c8face91ca5968a88ab78e9e0c00aa,"A Las Vegas randomized algorithm is given to compute the Hermite normal form of a nonsingular integer matrix A of dimension n. The algorithm uses quadratic integer multiplication and cubic matrix multiplication and has running time bounded by O(n3(logn + log ||A||)2(logn)2) bit operations, where ||A|| = maxi j|Ai j| denotes the largest entry of A in absolute value. A variant of the algorithm that uses pseudo-linear integer multiplication is given that has running time (n3log ||A||)1+o(1) bit operations, where the exponent ""+o(1)"" captures additional factors c1(logn)c2(loglog ||A||)c3for positive real constants c1, c2, c3. © 2023 Association for Computing Machinery. All rights reserved.",Hermite normal form; Howell normal form; integer matrix; Smith massager,Bits operation; Cubic algorithm; Hermite normal form; Howell normal form; Integer matrix; Integer multiplications; Nonsingular; Normal form; Running time; Smith massager; Matrix algebra
"Minimum+1 (s, t)-cuts and Dual-edge Sensitivity Oracle",2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175136551&doi=10.1145%2f3623271&partnerID=40&md5=3af50a1e0efd2eade9725f0a423e7780,"Let G be a directed multi-graph on n vertices andm edges with a designated source vertex s and a designated sink vertex t .We study the (s, t)-cuts of capacity minimum+1 and as an important application of them,we give a solution to the dual-edge sensitivity for (s, t)-mincuts - reporting an (s, t)-mincut upon failure or insertion of any pair of edges. Picard and Queyranne [Mathematical Programming Studies, 13(1): 8-16 (1980)] showed that there exists a directed acyclic graph (DAG) that compactly stores all minimum (s, t)-cuts of G. This structure also acts as an oracle for the single-edge sensitivity of minimum (s, t )-cut. For undirected multi-graphs, Dinitz and Nutov [STOC, 509-518 (1995)] showed that there exists an O(n) size 2-level Cactus model that stores all global cuts of capacity minimum+1. However, for minimum+1 (s, t )-cuts, no such compact structure exists till date. We present the following structural and algorithmic results on minimum+1 (s, t )-cuts. (1) Structure: There is an O(m) size 2-level DAG structure that stores all minimum+1 (s, t )-cuts ofG such that each minimum+1 (s, t)-cut appears as 3-transversal cut - it intersects any path in this structure at most thrice. We also show that there is an O(mn) size structure for storing and characterizing all minimum+1 (s, t )-cuts in terms of 1-transversal cuts. (2) Data structure: There exists an O(n2) size data structure that, given a pair of vertices {u,v} that are not separated by an (s, t)-mincut, can determine in O(1) time if there exists a minimum+1 (s, t )-cut, say (A, B), such that s,u ϵ A and v, t ϵ B; the corresponding cut can be reported in O(|B|) time. (3) Sensitivity oracle: There exists an O(n2) size data structure that solves the dual-edge sensitivity problem for (s, t )-mincuts. It takes O(1) time to report the capacity of a resulting (s, t )-mincut (A, B) and O(|B|) time to report the cut. (4) Lower bounds: For the data structure problems addressed in results (2) and (3) above, we also provide a matching conditional lower bound. We establish a close relationship among three seemingly unrelated problems - all-pairs directed reachability problem, the dual-edge sensitivity problem for (s, t)-mincuts, and the problem of reporting the capacity of ({x,y}, {u,v})-mincut for any four vertices x,y,u,v in G. Assuming the Directed Reachability Hypothesis by Patrascu [SIAM J. Computing, 827-847 (2011)] and Goldstein et al. [WADS, 421-436 (2017)], this leads to ω (n2) lower bounds on the space for the latter two problems. © 2023 Association for Computing Machinery. All rights reserved.","(s,t)-cut; characterization of cuts; fault tolerant; graph structures; maximum flow; minimum cuts; Minimum+1 cuts; sensitivity oracle","Flow graphs; Graphic methods; Mathematical programming; Undirected graphs; (s,t)-cut; Characterization of cut; Edge sensitivity; Fault-tolerant; Graph structures; Maximum flows; Min-cut; Minimum cut; Minimum+1 cut; Sensitivity oracle; Data structures"
Static and Streaming Data Structures for Fréchet Distance Queries,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176771471&doi=10.1145%2f3610227&partnerID=40&md5=7bc5b1fc4f5ad690af2f67b646fdd786,"Given a curve P with points in ℝd in a streaming fashion, and parameters ϵ > 0 and k, we construct a distance oracle that uses space, and given a query curve Q with k points in ℝd returns in time a 1+ϵ approximation of the discrete Fréchet distance between Q and P.In addition, we construct simplifications in the streaming model, oracle for distance queries to a sub-curve (in the static setting), and introduce the zoom-in problem. Our algorithms work in any dimension d, and therefore we generalize some useful tools and algorithms for curves under the discrete Fréchet distance to work efficiently in high dimensions.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","distance oracle; Fréchet distance; high dimension; simplification; streaming algorithm; the ""zoom in"" problem","Computational geometry; Distance oracle; Frechet distance; Higher dimensions; K points; Simplification; Static datum; Streaming algorithm; Streaming data; Streaming model; The ""zoom in"" problem; Query processing"
Synchronized Planarity with Applications to Constrained Planarity Problems,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176764457&doi=10.1145%2f3607474&partnerID=40&md5=910e791a19fe31c40564a91000645558,"We introduce the problem Synchronized Planarity. Roughly speaking, its input is a loop-free multi-graph together with synchronization constraints that, e.g., match pairs of vertices of equal degree by providing a bijection between their edges. Synchronized Planarity then asks whether the graph admits a crossing-free embedding into the plane such that the orders of edges around synchronized vertices are consistent. We show, on the one hand, that Synchronized Planarity can be solved in quadratic time, and, on the other hand, that it serves as a powerful modeling language that lets us easily formulate several constrained planarity problems as instances of Synchronized Planarity. In particular, this lets us solve Clustered Planarity in quadratic time, where the most efficient previously known algorithm has an upper bound of O(n8). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",atomic embeddability; cluster planarity; constrained planarity; PhrasesPlanarity testing,Graph theory; Synchronization; Atomic embeddability; Bijections; Cluster planarity; Constrained planarity; Loop free; Phrasesplanarity testing; Planarity; Quadratic time; Synchronization constraints; Modeling languages
"Recognizing k-Leaf Powers in Polynomial Time, for Constant k",2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176778355&doi=10.1145%2f3614094&partnerID=40&md5=f2a1615c368458678c2bf184a7fb26a1,"A graph G is a k-leaf power if there exists a tree T whose leaf set is V(G), and such that uv ĝ E(G) if and only if the distance between u and v in T is at most k (and u ≠ v). The graph classes of k-leaf powers have several applications in computational biology, but recognizing them has remained a challenging algorithmic problem for the past two decades. The best known result is that 6-leaf powers can be recognized in polynomial time. In this article, we present an algorithm that decides whether a graph G is a k-leaf power in time O(nf(k) for some function f that depends only on k (but has the growth rate of a power tower function).Our techniques are based on the fact that either a k-leaf power has a corresponding tree of low maximum degree, in which case finding it is easy, or every corresponding tree has large maximum degree. In the latter case, large-degree vertices in the tree imply that G has redundant substructures which can be pruned from the graph. In addition to solving a long-standing open problem, it is our hope that the structural results presented in this work can lead to further results on k-leaf powers and related classes.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Algorithms; graph theory; leaf powers,Computation theory; Growth rate; Polynomial approximation; Algorithmic problems; Computational biology; Graph class; Graph G; K-leaf power; Leaf power; Maximum degree; Polynomial-time; Power towers; Trees (mathematics)
Approximating Nash Social Welfare under Submodular Valuations through (Un)Matchings,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176763954&doi=10.1145%2f3613452&partnerID=40&md5=6a8b27b605fb83959a2d163f8dbf1d41,"We study the problem of approximating maximum Nash social welfare (NSW) when allocating m indivisible items among n asymmetric agents with submodular valuations. The NSW is a well-established notion of fairness and efficiency, defined as the weighted geometric mean of agents' valuations. For special cases of the problem with symmetric agents and additive(-like) valuation functions, approximation algorithms have been designed using approaches customized for these specific settings, and they fail to extend to more general settings. Hence, no approximation algorithm with a factor independent of m was known either for asymmetric agents with additive valuations or for symmetric agents beyond additive(-like) valuations before this work.In this article, we extend our understanding of the NSW problem to far more general settings. Our main contribution is two approximation algorithms for asymmetric agents with additive and submodular valuations. Both algorithms are simple to understand and involve non-trivial modifications of a greedy repeated matchings approach. Allocations of high-valued items are done separately by un-matching certain items and re-matching them by different processes in both algorithms. We show that these approaches achieve approximation factors of O(n) and O(n log n) for additive and submodular cases, independent of the number of items. For additive valuations, our algorithm outputs an allocation that also achieves the fairness property of envy-free up to one item (EF1).Furthermore, we show that the NSW problem under submodular valuations is strictly harder than all currently known settings with an factor of the hardness of approximation, even for constantly many agents. For this case, we provide a different approximation algorithm that achieves a factor of , hence resolving it completely.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",asymmetric agents; Nash social welfare; submodular valuations,Approximation algorithms; Software agents; Asymmetric agent; Functions approximations; Geometric mean; Matchings; Nash social welfare; Social welfare; Submodular; Submodular valuation; Symmetrics; Valuation function; Additives
String Indexing with Compressed Patterns,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176761655&doi=10.1145%2f3607141&partnerID=40&md5=8981a759ea6da6c49c735b759a955d46,"Given a string S of length n, the classic string indexing problem is to preprocess S into a compact data structure that supports efficient subsequent pattern queries. In this article, we consider the basic variant where the pattern is given in compressed form and the goal is to achieve query time that is fast in terms of the compressed size of the pattern. This captures the common client-server scenario, where a client submits a query and communicates it in compressed form to a server. Instead of the server decompressing the query before processing it, we consider how to efficiently process the compressed query directly. Our main result is a novel linear space data structure that achieves near-optimal query time for patterns compressed with the classic Lempel-Ziv 1977 (LZ77) compression scheme. Along the way, we develop several data structural techniques of independent interest, including a novel data structure that compactly encodes all LZ77 compressed suffixes of a string in linear space and a general decomposition of tries that reduces the search time from logarithmic in the size of the trie to logarithmic in the length of the pattern.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",String indexing,Indexing (materials working); Indexing (of information); Query processing; Structural optimization; Client /server; Compact data structure; Linear spaces; Near-optimal; Optimal query; Pattern query; Preprocess; Query time; Space data structures; String indexing; Data structures
Near-Optimal Time-Energy Tradeoffs for Deterministic Leader Election,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175707417&doi=10.1145%2f3614429&partnerID=40&md5=591b03ab8f61bd7d826dba874307ac6c,"We consider the energy complexity of the leader election problem in the single-hop radio network model, where each device v has a unique identifier ID(v) ϵ{ 1, 2, ⋯ , N} . Energy is a scarce resource for small battery-powered devices. For such devices, most of the energy is often spent on communication, not on computation. To approximate the actual energy cost, the energy complexity of an algorithm is defined as the maximum over all devices of the number of time slots where the device transmits or listens.Much progress has been made in understanding the energy complexity of leader election in radio networks, but very little is known about the tradeoff between time and energy. Chang et al. [STOC 2017] showed that the optimal deterministic energy complexity of leader election is θ(log log N) if each device can simultaneously transmit and listen but still leaving the problem of determining the optimal time complexity under any given energy constraint.Time-energy tradeoff: For any k ≥ log log N, we show that a leader among at most n devices can be elected deterministically in O(k · n1+ϵ) + O(k · N1/k) time and O(k) energy if each device can simultaneously transmit and listen, where ϵ > 0 is any small constant. This improves upon the previous O(N)-time O(log log N)-energy algorithm by Chang et al. [STOC 2017]. We provide lower bounds to show that the time-energy tradeoff of our algorithm is near-optimal.Dense instances: For the dense instances where the number of devices is n = θ(N), we design a deterministic leader election algorithm using only O(1) energy. This improves upon the O(log∗N)-energy algorithm by Jurdziński, Kutyłowski, and Zatopiański [PODC 2002] and the O(α (N))-energy algorithm by Chang et al. [STOC 2017]. More specifically, we show that the optimal deterministic energy complexity of leader election is if each device cannot simultaneously transmit and listen, and it is if each device can simultaneously transmit and listen.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Leader election; radio network; time-energy tradeoff,Computational complexity; Radio; Deterministics; Energy tradeoff; Leader election; Leader Election Problem; Near-optimal; Optimal time; Radio networks; Time energy; Time-energy tradeoff; Complex networks
A Polynomial-Time Algorithm for 1/3-Approximate Nash Equilibria in Bimatrix Games,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176777042&doi=10.1145%2f3606697&partnerID=40&md5=529fe1cfa03ac8e18ab1fc68c13c27b0,"Since the celebrated PPAD-completeness result for Nash equilibria in bimatrix games, a long line of research has focused on polynomial-time algorithms that compute ϵ-approximate Nash equilibria. Finding the best possible approximation guarantee that we can have in polynomial time has been a fundamental and non-trivial pursuit on settling the complexity of approximate equilibria. Despite a significant amount of effort, the algorithm of Tsaknakis and Spirakis [38], with an approximation guarantee of (0.3393+d), remains the state of the art over the last 15 years. In this paper, we propose a new refinement of the Tsaknakis-Spirakis algorithm, resulting in a polynomial-time algorithm that computes a -Nash equilibrium, for any constant δ> 0. The main idea of our approach is to go beyond the use of convex combinations of primal and dual strategies, as defined in the optimization framework of [38], and enrich the pool of strategies from which we build the strategy profiles that we output in certain bottleneck cases of the algorithm.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximate Nash equilibria; Bimatrix games,Approximation algorithms; Approximate equilibriums; Approximate nash equilibrium; Bimatrix games; Convex combinations; Long line; Nash equilibria; Non-trivial; Polynomial-time; Polynomial-time algorithms; State of the art; Polynomial approximation
Tiling with Squares and Packing Dominos in Polynomial Time,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166390685&doi=10.1145%2f3597932&partnerID=40&md5=6f482fd8e84c53342c75dec2b567a7c2,"A polyomino is a polygonal region with axis-parallel edges and corners of integral coordinates, which may have holes. In this paper, we consider planar tiling and packing problems with polyomino pieces and a polyomino container P. We give polynomial-time algorithms for deciding if P can be tiled with k × k squares for any fixed k which can be part of the input (that is, deciding if P is the union of a set of non-overlapping k × k squares) and for packing P with a maximum number of non-overlapping and axis-parallel 2 × 1 dominos, allowing rotations by 90°. As packing is more general than tiling, the latter algorithm can also be used to decide if P can be tiled by 2 × 1 dominos.These are classical problems with important applications in VLSI design, and the related problem of finding a maximum packing of 2 × 2 squares is known to be NP-hard [6]. For our three problems there are known pseudo-polynomial-time algorithms, that is, algorithms with running times polynomial in the area or perimeter of P. However, the standard, compact way to represent a polygon is by listing the coordinates of the corners in binary. We use this representation, and thus present the first polynomial-time algorithms for the problems. Concretely, we give a simple O(n log n)-time algorithm for tiling with squares, where n is the number of corners of P. We then give a more involved algorithm that reduces the problems of packing and tiling with dominos to finding a maximum and perfect matching in a graph with O(n3) vertices. This leads to algorithms with running times and , respectively.  © 2023 Copyright held by the owner/author(s).",Packing; polyominos; tiling,Classical problems; Packing problems; Planar tiling; Polygonal regions; Polynomial-time; Polynomial-time algorithms; Polyominoes; Running time; Tiling; VLSI design; Polynomial approximation
Fréchet Distance for Uncertain Curves,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166389801&doi=10.1145%2f3597640&partnerID=40&md5=871538ba21ea54106bfdb9f5f33f2307,"In this article, we study a wide range of variants for computing the (discrete and continuous) Fréchet distance between uncertain curves. An uncertain curve is a sequence of uncertainty regions, where each region is a disk, a line segment, or a set of points. A realisation of a curve is a polyline connecting one point from each region. Given an uncertain curve and a second (certain or uncertain) curve, we seek to compute the lower and upper bound Fréchet distance, which are the minimum and maximum Fréchet distance for any realisations of the curves. We prove that both problems are NP-hard for the Fréchet distance in several uncertainty models, and that the upper bound problem remains hard for the discrete Fréchet distance. In contrast, the lower bound (discrete [5] and continuous) Fréchet distance can be computed in polynomial time in some models. Furthermore, we show that computing the expected (discrete and continuous) Fréchet distance is #P-hard in some models.On the positive side, we present an FPTAS in constant dimension for the lower bound problem when Δ/δis polynomially bounded, where δis the Fréchet distance and Δbounds the diameter of the regions. We also show a near-linear-time 3-approximation for the decision problem on roughly δ-separated convex regions. Finally, we study the setting with Sakoe-Chiba time bands, where we restrict the alignment between the curves, and give polynomial-time algorithms for the upper bound and expected discrete and continuous Fréchet distance for uncertainty modelled as point sets.  © 2023 Copyright held by the owner/author(s).",Curves; Fréchet distance; hardness; uncertainty,Computational geometry; Curve; Frechet distance; Line-segments; Low bound; Lower and upper bounds; NP-hard; Polyline; Uncertainty; Uncertainty regions; Polynomial approximation
Matching on the line admits no o(√log n)-competitive algorithm,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166387212&doi=10.1145%2f3594873&partnerID=40&md5=c54281889fd477bd34fa9731b7c5329c,"We present a simple proof that no randomized online matching algorithm for the line can be (√log2(n+1)/15)-competitive against an oblivious adversary for any n = 2i - 1 : i ∈ N. This is the first super-constant lower bound for the problem, and disproves as a corollary a recent conjecture on the topology-parametrized competitiveness achievable on generic spaces.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Competitive Analysis; Metric Matching,Competitive algorithms; Competitive analysis; Generic space; Low bound; Matching algorithm; Matchings; Metric matching; Oblivious adversaries; On-line matching; Simple++; Computation theory
Competitive Online Search Trees on Trees,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166385193&doi=10.1145%2f3595180&partnerID=40&md5=b4ac23c1f0b0f7e6bf0e278d60966848,"We consider the design of adaptive data structures for searching elements of a tree-structured space. We use a natural generalization of the rotation-based online binary search tree model in which the underlying search space is the set of vertices of a tree. This model is based on a simple structure for decomposing graphs, previously known under several names including elimination trees, vertex rankings, and tubings. The model is equivalent to the classical binary search tree model exactly when the underlying tree is a path. We describe an online O(log log n)-competitive search tree data structure in this model, where n is the number of vertices. This matches the best-known competitive ratio of binary search trees. Our method is inspired by Tango trees, an online binary search tree algorithm, but critically needs several new notions including one that we call Steiner-closed search trees, which may be of independent interest. Moreover, our technique is based on a novel use of two levels of decomposition, first from search space to a set of Steiner-closed trees and, second, from these trees into paths.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic optimality; Search trees on trees; tango trees,Trees (mathematics); Binary search trees; Dynamic optimality; Natural generalization; Search spaces; Search tree on tree; Search trees; Steiner; Tango tree; Tree modeling; Tree-structured; Binary trees
Towards Optimal Moment Estimation in Streaming and Distributed Models,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165617614&doi=10.1145%2f3596494&partnerID=40&md5=66adffe4268446258297c98827e3c591,"One of the oldest problems in the data stream model is to approximate the pth moment of an underlying non-negative vector , which is presented as a sequence of updates to its coordinates. Of particular interest is when . Although a tight space bound of bits is known for this problem when both positive and negative updates are allowed, surprisingly, there is still a gap in the space complexity of this problem when all updates are positive. Specifically, the upper bound is bits, while the lower bound is only bits. Recently, an upper bound of bits was obtained under the assumption that the updates arrive in a random order. We show that for , the random order assumption is not needed. Namely, we give an upper bound for worst-case streams of bits for estimating . Our techniques also give new upper bounds for estimating the empirical entropy in a stream. However, we show that for , in the natural coordinator and blackboard distributed communication topologies, there is an bit max-communication upper bound based on a randomized rounding scheme. Our protocols also give rise to protocols for heavy hitters and approximate matrix product. We generalize our results to arbitrary communication topologies G, obtaining an max-communication upper bound, where d is the diameter of G. Interestingly, our upper bound rules out natural communication complexity-based approaches for proving an bit lower bound for for streaming algorithms. In particular, any such lower bound must come from a topology with large diameter.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",message passing; moment estimation; Streaming,Topology; Communication topologies; Data stream model; Distributed models; Low bound; Message-passing; Moment estimation; Non negatives; Streaming; Streaming model; Upper Bound; Message passing
Efficient and Near-optimal Algorithms for Sampling Small Connected Subgraphs,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166394873&doi=10.1145%2f3596495&partnerID=40&md5=5745541bc1b317bc2ee8e87e64e3c07d,"We study the following problem: Given an integer k ≥ 3 and a simple graph G, sample a connected induced k-vertex subgraph of G uniformly at random. This is a fundamental graph mining primitive with applications in social network analysis, bioinformatics, and more. Surprisingly, no efficient algorithm is known for uniform sampling; the only somewhat efficient algorithms available yield samples that are only approximately uniform, with running times that are unclear or suboptimal. In this work, we provide: (i) a near-optimal mixing time bound for a well-known random walk technique, (ii) the first efficient algorithm for truly uniform graphlet sampling, and (iii) the first sublinear-time algorithm for ϵ-uniform graphlet sampling.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",random walks; Subgraph sampling; sublinear algorithms,Graph theory; Connected Subgraph; Following problem; Graph G; Graphlets; Near-optimal algorithms; Random Walk; Simple++; Subgraph sampling; Subgraphs; Sublinear algorithm; Random processes
Almost-Optimal Deterministic Treasure Hunt in Unweighted Graphs,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166392754&doi=10.1145%2f3588437&partnerID=40&md5=56ce9931354a8bdd49fab59a0731c827,"A mobile agent navigating along edges of a simple connected unweighted graph, either finite or countably infinite, has to find an inert target (treasure) hidden in one of the nodes. This task is known as treasure hunt. The agent has no a priori knowledge of the graph, of the location of the treasure, or of the initial distance to it. The cost of a treasure hunt algorithm is the worst-case number of edge traversals performed by the agent until finding the treasure. Awerbuch et al. [3] considered graph exploration and treasure hunt for finite graphs in a restricted model where the agent has a fuel tank that can be replenished only at the starting node s. The size of the tank is B = 2 (1+α) r, for some positive real constant α, where r, called the radius of the graph, is the maximum distance from s to any other node. The tank of size B allows the agent to make at most [B] edge traversals between two consecutive visits at node s.Let e(d) be the number of edges whose at least one endpoint is at distance less than d from s. Awerbuch et al. [3] conjectured that it is impossible to find a treasure hidden in a node at distance at most d at cost nearly linear in e(d). We first design a deterministic treasure hunt algorithm working in the model without any restrictions on the moves of the agent at cost (e(d) log d) and then show how to modify this algorithm to work in the model from Awerbuch et al. [3] with the same complexity. Thus, we refute the preceding 20-year-old conjecture. We observe that no treasure hunt algorithm can beat cost θ (e(d)) for all graphs, and thus our algorithms are also almost optimal.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph; mobile agent; Treasure hunt,Fuel tanks; Graphic methods; Mobile agents; Deterministics; Edge traversal; Finite graphs; Graph; Graph exploration; Initial distance; Priori knowledge; Simple++; Treasure hunt; Unweighted graphs; Graph theory
Hitting Topological Minor Models in Planar Graphs is Fixed Parameter Tractable,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166384548&doi=10.1145%2f3583688&partnerID=40&md5=ee6440832aaa7f494ddecfadc2828b99,"For a finite collection of graphs F, the F-TM-Deletion problem has as input an n-vertex graph G and an integer k and asks whether there exists a set S ⊆ V(G) with |S| ≤ k such that G \ S does not contain any of the graphs in F as a topological minor. We prove that for every such F, F-TM-Deletion is fixed parameter tractable on planar graphs. Our algorithm runs in a 2(k2) · n2 time, or, alternatively, in 2(k) · n4 time. Our techniques can easily be extended to graphs that are embeddable on any fixed surface.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",irrelevant vertex technique; Topological minors; treewidth; vertex deletion problems,Graph theory; Fixed surfaces; Graph G; Irrelevant vertex technique; N-vertex graph; Planar graph; Topological-minor; Tree-width; Vertex deletion problems; Graphic methods
Load Thresholds for Cuckoo Hashing with Overlapping Blocks,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166392081&doi=10.1145%2f3589558&partnerID=40&md5=e10c90c710051334bdca3357fce3209e,"We consider a natural variation of cuckoo hashing proposed by Lehman and Panigrahy (2009). Each of cn objects is assigned k = 2 intervals of size ĝ.,""in a linear hash table of size n and both starting points are chosen independently and uniformly at random. Each object must be placed into a table cell within its intervals, but each cell can only hold one object. Experiments suggested that this scheme outperforms the variant with blocks in which intervals are aligned at multiples of ĝ.,"". In particular, the load threshold is higher, i.e., the load c that can be achieved with high probability. For instance, Lehman and Panigrahy (2009) empirically observed the threshold for ĝ.,""= 2 to be around 96.5% as compared to roughly 89.7% using blocks. They pinned down the asymptotics of the thresholds for large ĝ.,"", but the precise values resisted rigorous analysis. We establish a method to determine these load thresholds for all ĝ.,""≥ 2, and, in fact, for general k ≥ 2. For instance, for k = ĝ.,""= 2, we get ≈ 96.4995%. We employ a theorem due to Leconte, Lelarge, and Massoulié (2013), which adapts methods from statistical physics to the world of hypergraph orientability. In effect, the orientability thresholds for our graph families are determined by belief propagation equations for certain graph limits. As a side note, we provide experimental evidence suggesting that placements can be constructed in linear time using an adapted version of an algorithm by Khosla (2013).  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cuckoo hashing; hypergraph orientability; load thresholds; randomised algorithms; unaligned blocks,Statistical Physics; Cuckoo hashing; Hash table; High probability; Hyper graph; Hypergraph orientability; Load threshold; Natural variation; Orientability; Randomized Algorithms; Unaligned block; Belief propagation
On Coalescence Time in Graphs: When Is Coalescing as Fast as Meeting?,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159235302&doi=10.1145%2f3576900&partnerID=40&md5=4281eeb9c5c770b69b0e36d543da56f7,"Coalescing random walks is a fundamental distributed process, where a set of particles perform independent discrete-time random walks on an undirected graph. Whenever two or more particles meet at a given node, they merge and continue as a single random walk. The coalescence time is defined as the expected time until only one particle remains, starting from one particle at every node. Despite recent progress such as that of Cooper et al., the coalescence time for graphs, such as binary trees, d-dimensional tori, hypercubes, and, more generally, vertex-transitive graphs, remains unresolved. We provide a powerful toolkit that results in tight bounds for various topologies including the aforementioned ones. The meeting time is defined as the worst-case expected time required for two random walks to arrive at the same node at the same time. As a general result, we establish that for graphs whose meeting time is only marginally larger than the mixing time (a factor of log2 n), the coalescence time of n random walks equals the meeting time up to constant factors. This upper bound is complemented by the construction of a graph family demonstrating that this result is the best possible up to constant factors. Finally, we prove a tight worst-case bound for the coalescence time of O(n3). By duality, our results yield identical bounds on the voter model. Our techniques also yield a new bound on the hitting time and cover time of regular graphs, improving and tightening previous results by Broder and Karlin, as well as those by Aldous and Fill.  © 2023 Copyright held by the owner/author(s).",Coalescing random walks; population protocols; voter model,Flocculation; Graphic methods; Random processes; Undirected graphs; Coalescence time; Coalescing random walk; Constant factors; Discrete-time random walk; Distributed process; Expected time; Population protocol; Random Walk; Undirected graph; Voter models; Binary trees
Online Metric Algorithms with Untrusted Predictions,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159189484&doi=10.1145%2f3582689&partnerID=40&md5=9bbea6632ab2aa1d963d2f40a943bc2f,"Machine-learned predictors, although achieving very good results for inputs resembling training data, cannot possibly provide perfect predictions in all situations. Still, decision-making systems that are based on such predictors need not only benefit from good predictions, but should also achieve a decent performance when the predictions are inadequate. In this article, we propose a prediction setup for arbitrary metrical task systems (MTS) (e.g., caching, k-server, and convex body chasing) and online matching on the line. We utilize results from the theory of online algorithms to show how to make the setup robust. Specifically, for caching, we present an algorithm whose performance, as a function of the prediction error, is exponentially better than what is achievable for general MTS. Finally, we present an empirical evaluation of our methods on real-world datasets, which suggests practicality.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",caching; competitive analysis; Metrical task systems,Decision making; Online systems; Caching; Competitive analysis; Convex body; Decision-making systems; K-server; Metrical task systems; On-line algorithms; On-line matching; Performance; Training data; Forecasting
Minimum Cut and Minimum k-Cut in Hypergraphs via Branching Contractions,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159208882&doi=10.1145%2f3570162&partnerID=40&md5=379f9abdd8e97b2f5b14b84b1d385a21,"On hypergraphs with m hyperedges and n vertices, where p denotes the total size of the hyperedges, we provide the following results: • We give an algorithm that runs in Õ(mn2k-2) time for finding a minimum k-cut in hypergraphs of arbitrary rank. This algorithm betters the previous best running time for the minimum k-cut problem, for k > 2. • We give an algorithm that runs in Õ(nmax{r,2k-2}) time for finding a minimum k-cut in hypergraphs of constant rank r. This algorithm betters the previous best running times for both the minimum cut and minimum k-cut problems for dense hypergraphs. Both of our algorithms are Monte Carlo, i.e., they return a minimum k-cut (or minimum cut) with high probability. These algorithms are obtained as instantiations of a generic branching randomized contraction technique on hypergraphs, which extends the celebrated work of Karger and Stein on recursive contractions in graphs. Our techniques and results also extend to the problems of minimum hedge-cut and minimum hedge-k-cut on hedgegraphs, which generalize hypergraphs.  © 2023 Association for Computing Machinery.",hypergraph connectivity; Hypergraph cut; k-cut; minimum cut,Graph theory; Constant rank; Dense hypergraph; Hyper graph; Hyperedges; Hypergraph connectivity; Hypergraph cut; K-cut; Minimum cut; Running time; Wooden fences
Polynomial Kernel for Interval Vertex Deletion,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159168025&doi=10.1145%2f3571075&partnerID=40&md5=d9a833984394df2c0325b854e4d0548d,"Given a graph G and an integer k, the Interval Vertex Deletion (IVD) problem asks whether there exists a subset S ⊆ V(G) of size at most k such that G-S is an interval graph. This problem is known to be NP-complete (according to Yannakakis at STOC 1978). Originally in 2012, Cao and Marx showed that IVD is fixed parameter tractable: they exhibited an algorithm with running time 10k nO(1). The existence of a polynomial kernel for IVD remained a well-known open problem in parameterized complexity. In this article, we settle this problem in the affirmative.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Interval Vertex Deletion; kernelization; parameterized complexity; polynomial kernel,Graph theory; Parameter estimation; Parameterization; Graph G; Interval graph; Interval vertex deletion; Kernelization; NP Complete; Parameterized complexity; Polynomial kernels; Running time; Vertex deletion problems; Polynomials
On the Complexity of String Matching for Graphs,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161165900&doi=10.1145%2f3588334&partnerID=40&md5=d82e07049285bd20704641f8002933d2,"Exact string matching in labeled graphs is the problem of searching paths of a graph G=(V, E) such that the concatenation of their node labels is equal to a given pattern string P[1.m]. This basic problem can be found at the heart of more complex operations on variation graphs in computational biology, of query operations in graph databases, and of analysis operations in heterogeneous networks.We prove a conditional lower bound stating that, for any constant ϵ > 0, an O(|E|1 - ϵ m) time, or an O(|E| m1 - ϵ)time algorithm for exact string matching in graphs, with node labels and pattern drawn from a binary alphabet, cannot be achieved unless the Strong Exponential Time Hypothesis (SETH) is false. This holds even if restricted to undirected graphs with maximum node degree 2 - that is, to zig-zag matching in bidirectional strings, or to deterministic directed acyclic graphs whose nodes have maximum sum of indegree and outdegree 3. These restricted cases make the lower bound stricter than what can be directly derived from related bounds on regular expression matching (Backurs and Indyk, FOCS'16). In fact, our bounds are tight in the sense that lowering the degree or the alphabet size yields linear time solvable problems.An interesting corollary is that exact and approximate matching are equally hard (i.e., quadratic time) in graphs under SETH. In comparison, the same problems restricted to strings have linear time vs quadratic time solutions, respectively (approximate pattern matching having also a matching SETH lower bound (Backurs and Indyk, STOC'15)).  © 2023 Copyright held by the owner/author(s).",Exact pattern matching; graph query; graph search; heterogeneous networks; labeled graphs; string matching; string search; Strong Exponential Time Hypothesis; variation graphs,Complex networks; Directed graphs; Graph Databases; Graphic methods; Pattern matching; Query processing; Undirected graphs; Exact-pattern matching; Graph queries; Graph search; Labeled graphs; Low bound; Matchings; String matching; String search; Strong exponential time hypothesis; Variation graph; Heterogeneous networks
Robust Algorithms for TSP and Steiner Tree,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159230243&doi=10.1145%2f3570957&partnerID=40&md5=c4a56022f60c10c65a636921daf947f1,"Robust optimization is a widely studied area in operations research, where the algorithm takes as input a range of values and outputs a single solution that performs well for the entire range. Specifically, a robust algorithm aims to minimize regret, defined as the maximum difference between the solution's cost and that of an optimal solution in hindsight once the input has been realized. For graph problems in P, such as shortest path and minimum spanning tree, robust polynomial-time algorithms that obtain a constant approximation on regret are known. In this paper, we study robust algorithms for minimizing regret in NP-hard graph optimization problems, and give constant approximations on regret for the classical traveling salesman and Steiner tree problems.  © 2023 Copyright held by the owner/author(s).",Steiner tree; traveling salesman,Approximation algorithms; Optimization; Polynomial approximation; Trees (mathematics); Graph problems; Maximum differences; Minimums panning tree; Operation research; Optimal solutions; Robust algorithm; Robust optimization; Short-path; Steiner trees; Travelling salesman; Operations research
"Subcubic Equivalences between Graph Centrality Problems, APSP, and Diameter",2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85157989875&doi=10.1145%2f3563393&partnerID=40&md5=a876331e50dfc5b9b6cf9436c83e927b,"Measuring the importance of a node in a network is a major goal in the analysis of social networks, biological systems, transportation networks, and so forth. Different centrality measures have been proposed to capture the notion of node importance. For example, the center of a graph is a node that minimizes the maximum distance to any other node (the latter distance is the radius of the graph). The median of a graph is a node that minimizes the sum of the distances to all other nodes. Informally, the betweenness centrality of a node w measures the fraction of shortest paths that have w as an intermediate node. Finally, the reach centrality of a node w is the smallest distance r such that any s-t shortest path passing through w has either s or t in the ball of radius r around w. The fastest known algorithms to compute the center and the median of a graph and to compute the betweenness or reach centrality even of a single node take roughly cubic time in the number n of nodes in the input graph. It is open whether these problems admit truly subcubic algorithms, i.e., algorithms with running time Õ (n3-δ ) for some constant δ > 0.1 We relate the complexity of the mentioned centrality problems to two classical problems for which no truly subcubic algorithm is known, namely All Pairs Shortest Paths (APSP) and Diameter. We show that Radius, Median, and Betweenness Centrality are equivalent under subcubic reductions to APSP, i.e., that a truly subcubic algorithm for any of these problems implies a truly subcubic algorithm for all of them. We then show that Reach Centrality is equivalent to Diameter under subcubic reductions. The same holds for the problem of approximating Betweenness Centrality within any finite factor. Thus, the latter two centrality problems could potentially be solved in truly subcubic time, even if APSP required essentially cubic time. On the positive side, our reductions for Reach Centrality imply an improved Õ (Mnω )-time algorithm for this problem in case of non-negative integer weights upper bounded by M, where ω is a fast matrix multiplication exponent. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",APSP; Betweenness Centrality; Diameter; Fine-grained complexity; Median; Radius; reach centrality; subcubic reductions,Graph theory; % reductions; All pairs shortest paths; Betweenness centrality; Diameter; Fine grained; Fine-grained complexity; Median; Radius; Reach centrality; Subcubic reduction; Complex networks
"Approximation Schemes for Capacitated Vehicle Routing on Graphs of Bounded Treewidth, Bounded Doubling, or Highway Dimension",2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159189001&doi=10.1145%2f3582500&partnerID=40&md5=60af31378fe3a64ef35b0f6420517b8b,"In this article, we present Approximation Schemes for Capacitated Vehicle Routing Problem (CVRP) on several classes of graphs. In CVRP, introduced by Dantzig and Ramser in 1959 [14], we are given a graph G=(V,E) with metric edges costs, a depot r ϵ V, and a vehicle of bounded capacity Q. The goal is to find a minimum cost collection of tours for the vehicle that returns to the depot, each visiting at most Q nodes, such that they cover all the nodes. This generalizes classic TSP and has been studied extensively. In the more general setting, each node v has a demand dv and the total demand of each tour must be no more than Q. Either the demand of each node must be served by one tour (unsplittable) or can be served by multiple tours (splittable). The best-known approximation algorithm for general graphs has ratio α +2(1-ϵ) (for the unsplittable) and α +1-ϵ (for the splittable) for some fixed ϵ > 1/3000, where α is the best approximation for TSP. Even for the case of trees, the best approximation ratio is 4/3 [5] and it has been an open question if there is an approximation scheme for this simple class of graphs. Das and Mathieu [15] presented an approximation scheme with time nlogO(1/ϵ)n for Euclidean plane ℝ2. No other approximation scheme is known for any other class of metrics (without further restrictions on Q). In this article, we make significant progress on this classic problem by presenting Quasi-Polynomial Time Approximation Schemes (QPTAS) for graphs of bounded treewidth, graphs of bounded highway dimensions, and graphs of bounded doubling dimensions. For comparison, our result implies an approximation scheme for the Euclidean plane with run time nO(log6n/ϵ5).  © 2023 Copyright held by the owner/author(s).",Approximation scheme; bounded doubling dimension; bounded treewidth; capacitated vehicle routing,Approximation algorithms; Graphic methods; Network routing; Polynomial approximation; Trees (mathematics); Vehicles; Approximation scheme; Best approximations; Bounded doubling dimension; Bounded treewidth; Capacitated vehicle routing; Capacitated vehicle routing problem; Capacitated vehicles; Doubling dimensions; Doublings; Euclidean planes; Vehicle routing
Ranked Document Retrieval in External Memory,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176735018&doi=10.1145%2f3559763&partnerID=40&md5=d60fc3ea5195ed26349ed653b163926e,"The ranked (or top-k) document retrieval problem is defined as follows: preprocess a collection {T1,T2,... ,Td} of d strings (called documents) of total length n into a data structure, such that for any given query (P,k), where P is a string (called pattern) of length p ≥ 1 and k ϵ [1,d] is an integer, the identifiers of those k documents that are most relevant to P can be reported, ideally in the sorted order of their relevance. The seminal work by Hon et al. [FOCS 2009 and Journal of the ACM 2014] presented an O(n)-space (in words) data structure with O(p+k log k) query time. The query time was later improved to O(p+k) [SODA 2012] and further to O(p/ log σn+k) [SIAM Journal on Computing 2017] by Navarro and Nekrich, where σ is the alphabet size. We revisit this problem in the external memory model and present three data structures. The first one takes O(n)-space and answer queries in O(p/B + log B n + k/B+ log ∗ (n/B)) I/Os, where B is the block size. The second one takes O(n log ∗ (n/B)) space and answer queries in optimal O(p/B + log B n + k/B) I/Os. In both cases, the answers are reported in the unsorted order of relevance. To handle sorted top-k document retrieval, we present an O(n log (d/B)) space data structure with optimal query cost.  © 2023 Association for Computing Machinery.",Data structures; external memory; text indexing,Information retrieval; Query processing; Structural optimization; Alphabet size; Block sizes; Document Retrieval; External memory; External memory models; Log space; Preprocess; Query time; Text-indexing; Total length; Data structures
A PTAS for Capacitated Vehicle Routing on Trees,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159188017&doi=10.1145%2f3575799&partnerID=40&md5=1136fa8650433fa2314e2347a48e9780,"We give a polynomial time approximation scheme (PTAS) for the unit demand capacitated vehicle routing problem (CVRP) on trees, for the entire range of the tour capacity. The result extends to the splittable CVRP.  © 2023 Association for Computing Machinery.",Approximation algorithms; capacitated vehicle routing; combinatorial optimization; graph algorithms,Approximation algorithms; Polynomial approximation; Routing algorithms; Trees (mathematics); Vehicle routing; Vehicles; Capacitated vehicle routing; Capacitated vehicle routing problem; Capacitated vehicles; Graph algorithms; Polynomial time approximation schemes; Combinatorial optimization
PTAS for Sparse General-valued CSPs,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159219911&doi=10.1145%2f3569956&partnerID=40&md5=08bed942e608bc13e49d5b7150009295,"We study polynomial-time approximation schemes (PTASes) for constraint satisfaction problems (CSPs) such as Maximum Independent Set or Minimum Vertex Cover on sparse graph classes. Baker's approach gives a PTAS on planar graphs, excluded-minor classes, and beyond. For Max-CSPs, and even more generally, maximisation finite-valued CSPs (where constraints are arbitrary non-negative functions), Romero, Wrochna, and Živný [SODA'21] showed that the Sherali-Adams LP relaxation gives a simple PTAS for all fractionally-treewidth-fragile classes, which is the most general ""sparsity""condition for which a PTAS is known. We extend these results to general-valued CSPs, which include ""crisp""(or ""strict"") constraints that have to be satisfied by every feasible assignment. The only condition on the crisp constraints is that their domain contains an element that is at least as feasible as all the others (but possibly less valuable).For minimisation general-valued CSPs with crisp constraints, we present a PTAS for all Baker graph classes - a definition by Dvořák [SODA'20] that encompasses all classes where Baker's technique is known to work, except for fractionally-treewidth-fragile classes. While this is standard for problems satisfying a certain monotonicity condition on crisp constraints, we show this can be relaxed to diagonalisability - a property of relational structures connected to logics, statistical physics, and random CSPs.  © 2023 Association for Computing Machinery.",Baker classes; Constraint satisfaction; diagonalisability; sparsity,Graph theory; Polynomial approximation; Statistical Physics; Baker class; Condition; Constraint Satisfaction; Constraint-satisfaction problems; Diagonalisability; Graph class; Polynomial time approximation schemes; Sparsity; Tree-width; Valued constraint satisfaction problems; Constraint satisfaction problems
Universal Algorithms for Clustering Problems,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159171670&doi=10.1145%2f3572840&partnerID=40&md5=50106f3085ff178413a8e6fa8c9650b4,"This article presents universal algorithms for clustering problems, including the widely studied k-median, k-means, and k-center objectives. The input is a metric space containing all potential client locations. The algorithm must select k cluster centers such that they are a good solution for any subset of clients that actually realize. Specifically, we aim for low regret, defined as the maximum over all subsets of the difference between the cost of the algorithm's solution and that of an optimal solution. A universal algorithm's solution Sol for a clustering problem is said to be an α, β-approximation if for all subsets of clients C′, it satisfies sol (C′) ≤ α · OPT (C′) + β · MR, where opt (C′ is the cost of the optimal solution for clients (C′) and mr is the minimum regret achievable by any solution. Our main results are universal algorithms for the standard clustering objectives of k-median, k-means, and k-center that achieve (O(1), O(1))-approximations. These results are obtained via a novel framework for universal algorithms using linear programming (LP) relaxations. These results generalize to other ĝ.,""p-objectives and the setting where some subset of the clients are fixed. We also give hardness results showing that (α, β)-approximation is NP-hard if α or β is at most a certain constant, even for the widely studied special case of Euclidean metric spaces. This shows that in some sense, (O(1), O(1))-approximation is the strongest type of guarantee obtainable for universal clustering.  © 2023 Copyright held by the owner/author(s).",clustering; Universal algorithms,Approximation algorithms; K-means clustering; Linear programming; Optimal systems; Set theory; Topology; Algorithm solution; Clustering problems; Clusterings; K cluster; K-center; K-means; K-median; Metric spaces; Optimal solutions; Universal algorithm; Sols
The Minimum Principle of SINR: A Useful Discretization Tool for Wireless Communication,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176787710&doi=10.1145%2f3477144&partnerID=40&md5=54dad669e276a49ed4d5798579a686d7,"Theoretical study of optimization problems in wireless communication often deals with tasks that concern a single point. For example, the power control problem requires computing a power assignment guaranteeing that each transmitting station si is successfully received at a single receiver point ri. This paper aims at addressing communication applications that require handling two-dimensional tasks (e.g., guaranteeing successful transmission in entire regions rather than at specific points).The natural approach to two-dimensional optimization tasks is to discretize the optimization domain, e.g., by sampling points within the domain. The straightforward implementation of the discretization approach, however, might incur high time and memory requirements, and moreover, it cannot guarantee exact solutions.The alternative proposed and explored in this paper is based on establishing the minimum principle1 for the signal to interference and noise ratio (SINR) function with free space path loss (i.e., when the signal decays in proportion to the square of the distance between the transmitter and receiver). Essentially, the minimum principle allows us to reduce the dimension of the optimization domain without losing anything in the accuracy or quality of the solution. More specifically, when the two-dimensional optimization domain is bounded and free from any interfering station, the minimum principle implies that it is sufficient to optimize the SINR function over the boundary of the domain, as the ""hardest""points to be satisfied reside on the boundary and not in the interior.We then utilize the minimum principle as the basis for an improved discretization technique for solving two-dimensional problems in the SINR model. This approach is shown to be useful for handling optimization problems over two dimensions (e.g., power control, energy minimization); in providing tight bounds on the number of null cells in the reception map; and in approximating geometric and topological properties of the wireless reception map (e.g., maximum inscribed sphere).The minimum principle, as well as the interplay between continuous and discrete analysis presented in this paper, are expected to pave the way to future study of algorithmic SINR in higher dimensions. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSINR; communication; power control,Optimization; Signal interference; Signal receivers; Signal to noise ratio; Topology; Additional key word and phrasessinr; Discretizations; Key words; Minimum Principles; Optimization domain; Optimization problems; Power-control; Signal-to-interference-and-noise-ratios; Two dimensional optimizations; Wireless communications; Power control
Reliable Spanners for Metric Spaces,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168622655&doi=10.1145%2f3563356&partnerID=40&md5=361fa46627042be0856300cdbab97522,"A spanner is reliable if it can withstand large, catastrophic failures in the network. More precisely, any failure of some nodes can only cause a small damage in the remaining graph in terms of the dilation. In other words, the spanner property is maintained for almost all nodes in the residual graph. Constructions of reliable spanners of near linear size are known in the low-dimensional Euclidean settings. Here, we present new constructions of reliable spanners for planar graphs, trees, and (general) metric spaces. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesComputational geometry; reliable; spanners,Set theory; Additional key word and phrasescomputational geometry; Catastrophic failures; Euclidean; Key words; Low dimensional; Metric spaces; Property; Reliable; Small damage; Spanner; Trees (mathematics)
Approximating Pathwidth for Graphs of Small Treewidth,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159174361&doi=10.1145%2f3576044&partnerID=40&md5=92cf8615723beb96d5dfc51daf7b6e11,"We describe a polynomial-time algorithm which, given a graph G with treewidth t, approximates the pathwidth of G to within a ratio of (t√log t) This is the first algorithm to achieve an f(t)-approximation for some function f. Our approach builds on the following key insight: every graph with large pathwidth has large treewidth or contains a subdivision of a large complete binary tree. Specifically, we show that every graph with pathwidth at least th+2 has treewidth at least t or contains a subdivision of a complete binary tree of height h+1. The bound th+2 is best possible up to a multiplicative constant. This result was motivated by, and implies (with c=2), the following conjecture of Kawarabayashi and Rossman (SODA'18): there exists a universal constant c such that every graph with pathwidth ω(kc) has treewidth at least k or contains a subdivision of a complete binary tree of height k. Our main technical algorithm takes a graph G and some (not necessarily optimal) tree decomposition of G of width t′ in the input, and it computes in polynomial time an integer h, a certificate that G has pathwidth at least h, and a path decomposition of G of width at most (t′+1)h+1. The certificate is closely related to (and implies) the existence of a subdivision of a complete binary tree of height h. The approximation algorithm for pathwidth is then obtained by combining this algorithm with the approximation algorithm of Feige, Hajiaghayi, and Lee (STOC'05) for treewidth.  © 2023 Copyright held by the owner/author(s).",pathwidth; Treewidth,Binary trees; Graph theory; Polynomial approximation; Complete binary tree; Graph G; Multiplicative constants; Path-decompositions; Pathwidth; Polynomial-time; Polynomial-time algorithms; Tree decomposition; Tree-width; Universal constants; Approximation algorithms
"Approximating (k,l)-Median Clustering for Polygonal Curves",2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176724263&doi=10.1145%2f3559764&partnerID=40&md5=0f88e9c19bc7bec4044b2a952e7f0580,"In 2015, Driemel, Krivošija, and Sohler introduced the k,ĝ.,""-median clustering problem for polygonal curves under the Fréchet distance. Given a set of input curves, the problem asks to find k median curves of at most ĝ.,""vertices each that minimize the sum of Fréchet distances over all input curves to their closest median curve. A major shortcoming of their algorithm is that the input curves are restricted to lie on the real line. In this article, we present a randomized bicriteria-approximation algorithm that works for polygonal curves in ĝ., d and achieves approximation factor (1+I ) with respect to the clustering costs. The algorithm has worst-case running time linear in the number of curves, polynomial in the maximum number of vertices per curve (i.e., their complexity), and exponential in d, ĝ.,"", 1/I and 1/δ(i.e., the failure probability). We achieve this result through a shortcutting lemma, which guarantees the existence of a polygonal curve with similar cost as an optimal median curve of complexity ĝ.,"", but of complexity at most 2ĝ.,""-2, and whose vertices can be computed efficiently. We combine this lemma with the superset sampling technique by Kumar et al. to derive our clustering result. In doing so, we describe and analyze a generalization of the algorithm by Ackermann et al., which may be of independent interest. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesClustering; approximation algorithms; median; polygonal curves,Clustering algorithms; Polynomial approximation; Additional key word and phrasesclustering; Clustering problems; Clusterings; Frechet distance; Input curve; K-median; Key words; Median; Polygonal curve; Real line; Approximation algorithms
Online Throughput Maximization on Unrelated Machines: Commitment is No Burden,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171980898&doi=10.1145%2f3569582&partnerID=40&md5=ed79eaae365bbadcc6c79210f049a69c,"We consider a fundamental online scheduling problem in which jobs with processing times and deadlines arrive online over time at their release dates. The task is to determine a feasible preemptive schedule on a single or multiple possibly unrelated machines that maximizes the number of jobs that complete before their deadline. Due to strong impossibility results for competitive analysis on a single machine, we require that jobs contain some slack ϵ > 0, which means that the feasible time window for scheduling a job is at least 1+ ϵ times its processing time on each eligible machine. Our contribution is two-fold: (i) We give the first non-trivial online algorithms for throughput maximization on unrelated machines, and (ii), this is the main focus of our paper, we answer the question on how to handle commitment requirements which enforce that a scheduler has to guarantee at a certain point in time the completion of admitted jobs. This is very relevant, e.g., in providing cloud-computing services, and disallows last-minute rejections of critical tasks. We present an algorithm for unrelated machines that is -competitive when the scheduler must commit upon starting a job. Somewhat surprisingly, this is the same optimal performance bound (up to constants) as for scheduling without commitment on a single machine. If commitment decisions must be made before a job's slack becomes less than a δ-fraction of its size, we prove a competitive ratio of for 0 < δ< ϵ. This result nicely interpolates between commitment upon starting a job and commitment upon arrival. For the latter commitment model, it is known that no (randomized) online algorithm admits any bounded competitive ratio. While we mainly focus on scheduling without migration, our results also hold when comparing against a migratory optimal solution in case of identical machines.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",competitive analysis; Deadline scheduling; migration; online algorithms; throughput; unrelated machines,Competitive analysis; Competitive ratio; Deadline scheduling; Migration; On-line algorithms; Online scheduling; Processing time; Single- machines; Throughput maximization; Unrelated machines; Scheduling algorithms
Monotone Edge Flips to an Orientation of Maximum Edge-Connectivity à la Nash-Williams,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160825683&doi=10.1145%2f3561302&partnerID=40&md5=4ccff24fca3555613d8e9fb25db325f0,"We initiate the study of k-edge-connected orientations of undirected graphs through edge flips for k ≥ 2. We prove that in every orientation of an undirected 2k-edge-connected graph, there exists a sequence of edges such that flipping their directions one by one does not decrease the edge connectivity, and the final orientation is k-edge connected. This yields an ""edge-flip based""new proof of Nash-Williams' theorem: A undirected graph G has a k-edge-connected orientation if and only if G is 2k-edge connected. As another consequence of the theorem, we prove that the edge-flip graph of k-edge-connected orientations of an undirected graph G is connected if G is (2k+2)-edge connected. This has been known to be true only when k=1. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesGraph orientation; edge connectivity; edge-flip graph; Nash-Williams' theorem,Additional key word and phrasesgraph orientation; Connected graph; Edge connectivity; Edge flips; Edge-flip graph; Graph G; Key words; Nash-william' theorem; Undirected graph; Williams; Undirected graphs
Competitive Algorithms for Generalized k-Server in Uniform Metrics,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176756530&doi=10.1145%2f3568677&partnerID=40&md5=e529fe33ae936e85baa745434d4ff6d8,"The generalized k-server problem is a far-reaching extension of the k-server problem with several applications. Here, each server si lies in its own metric space Mi. A request is a k-tuple r = (r1,r2,⋯ ,rk, which is served by moving some server si to the point ri Mi, and the goal is to minimize the total distance traveled by the servers. Despite much work, no f(k)-competitive algorithm is known for the problem for k > 2 servers, even for special cases such as uniform metrics and lines.Here, we consider the problem in uniform metrics and give the first f(k)-competitive algorithms for general k. In particular, we obtain deterministic and randomized algorithms with competitive ratio k · 2k and O(k3 log k), respectively. Our deterministic bound is based on a novel application of the polynomial method to online algorithms, and essentially matches the long-known lower bound of 2k-1. We also give a 22O(k)-competitive deterministic algorithm for weighted uniform metrics, which also essentially matches the recent doubly exponential lower bound for the problem.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",competitive analysis; k-server problem; online algorithms,Competitive algorithms; Competitive analysis; Deterministic algorithms; K-server; K-server problem; Low bound; Metric spaces; On-line algorithms; Total distances; Uniform metric
A Linear-Time n0.4-Approximation for Longest Common Subsequence,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176766068&doi=10.1145%2f3568398&partnerID=40&md5=1283733c00ebc4bf5999e6a08b83ad65,"We consider the classic problem of computing the Longest Common Subsequence (LCS) of two strings of length n. The 40-year-old quadratic-time dynamic programming algorithm has recently been shown to be near-optimal by Abboud, Backurs, and Vassilevska Williams [FOCS'15] and Bringmann and Künnemann [FOCS'15] assuming the Strong Exponential Time Hypothesis. This has led the community to look for subquadratic approximation algorithms for the problem.Yet, unlike the edit distance problem for which a constant-factor approximation in almost-linear time is known, very little progress has been made on LCS, making it a notoriously difficult problem also in the realm of approximation. For the general setting, only a naive O(nI/2-approximation algorithm with running time OŠ(n2-I has been known, for any constant 0 < I ≤ 1. Recently, a breakthrough result by Hajiaghayi, Seddighin, Seddighin, and Sun [SODA'19] provided a linear-time algorithm that yields a O(n0.497956-approximation in expectation; improving upon the naive -approximation for the first time.In this paper, we provide an algorithm that in time O(n2-I) computes an OŠ(n2/5-approximation with high probability, for any 0 < I ≤ 1. Our result (1) gives an OŠ(n0.4-approximation in linear time, improving upon the bound of Hajiaghayi, Seddighin, Seddighin, and Sun, (2) provides an algorithm whose approximation scales with any subquadratic running time O(n2-I), improving upon the naive bound of O(nI/2) for any I, and (3) instead of only in expectation, succeeds with high probability.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximation algorithms; Longest common subsequence; string algorithms,Clustering algorithms; Dynamic programming; Dynamic programming algorithm; High probability; Linear time; Longest common subsequences; Ni 2; Quadratic time; Running time; String algorithms; Time dynamic; Approximation algorithms
Counting Homomorphic Cycles in Degenerate Graphs,2023,ACM Transactions on Algorithms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176766610&doi=10.1145%2f3560820&partnerID=40&md5=7e38510a99eb159ec5d44ca357fb2ab9,"Since counting subgraphs in general graphs is, by and large, a computationally demanding problem, it is natural to try and design fast algorithms for restricted families of graphs. One such family that has been extensively studied is that of graphs of bounded degeneracy (e.g., planar graphs). This line of work, which started in the early 80's, culminated in a recent work of Gishboliner et al., which highlighted the importance of the task of counting homomorphic copies of cycles (i.e., cyclic walks) in graphs of bounded degeneracy.Our main result in this paper is a surprisingly tight relation between the above task and the well-studied problem of detecting (standard) copies of directed cycles in general directed graphs. More precisely, we prove the following:One can compute the number of homomorphic copies of C2k and C2k+1 in n-vertex graphs of bounded degeneracy in time Õ(nd), where the fastest known algorithm for detecting directed copies of Ck in general m-edge digraphs runs in time Õ(md).Conversely, one can transform any O(nb) algorithm for computing the number of homomorphic copies of C2k or of C2k+1 in n-vertex graphs of bounded degeneracy, into an Õ(mb) time algorithm for detecting directed copies of Ck in general m-edge digraphs.We emphasize that our first result does not use a black-box reduction (as opposed to the second result which does). Instead, we design an algorithm for computing the number of Ck-homomorphisms in degenerate graphs and show that one part of its analysis can be reduced to the analysis of the fastest known algorithm for detecting directed cycles in general digraphs, which was carried out in a recent breakthrough of Dalirrooyfard, Vuong and Vassilevska Williams. As a by-product of our algorithm, we obtain a new algorithm for detecting k-cycles in directed and undirected graphs of bounded degeneracy that is faster than all previously known algorithms for 7 ≤ k ≤ 11, and faster for all k ≥ 7 if the matrix multiplication exponent is 2.  © 2023 Association for Computing Machinery.",cycle detection; cycle homomorphisms; degeneracy; Subgraph counting,Graphic methods; Undirected graphs; Cycle detection; Cycle homomorphism; Degeneracy; Degenerate graphs; Fast algorithms; General graph; N-vertex graph; Planar graph; Subgraph counting; Subgraphs; Directed graphs
