Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Using architecture to reason about information security,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954454987&doi=10.1145%2f2829949&partnerID=40&md5=939295c3a35be23507781c0253c2f7c6,"We demonstrate, by a number of examples, that information flow security properties can be proved from abstract architectural descriptions, which describe only the causal structure of a system and local properties of trusted components. We specify these architectural descriptions of systems by generalizing intransitive noninterference policies to admit the ability to filter information passed between communicating domains. A notion of refinement of such system architectures is developed that supports top-down development of architectural specifications and proofs by abstraction of information security properties. We also show that, in a concrete setting where the causal structure is enforced by access control, a static check of the access control setting plus local verification of the trusted components is sufficient to prove that a generalized intransitive noninterference policy is satisfied. © 2015 ACM.",Epistemic logic; Information flow security; Intransitive noninterference; System architecture,Access control; Network security; Security of data; Architectural descriptions; Architectural specifications; Control settings; Epistemic logic; Information flow security; Intransitive non-interference; System architectures; Trusted components; Computer architecture
Randomization-based intrusion detection system for advanced metering infrastructure,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954461221&doi=10.1145%2f2814936&partnerID=40&md5=0ab7b334aaaadbc29f8d6a8d141f6734,"Smart grid deployment initiatives have been witnessed in recent years. Smart grids provide bidirectional communication between meters and head-end systems through Advanced Metering Infrastructure (AMI). Recent studies highlight the threats targeting AMI. Despite the need for tailored Intrusion Detection Systems (IDSs) for smart grids, very limited progress has been made in this area. Unlike traditional networks, smart grids have their own unique challenges, such as limited computational power devices and potentially high deployment cost, that restrict the deployment options of intrusion detectors. We show that smart grids exhibit deterministic and predictable behavior that can be accurately modeled to detect intrusion. However, it can also be leveraged by the attackers to launch evasion attacks. To this end, in this article, we present a robust mutation-based intrusion detection system that makes the behavior unpredictable for the attacker while keeping it deterministic for the system. We model the AMI behavior using event logs collected at smart collectors, which in turn can be verified using the invariant specifications generated from the AMI behavior and mutable configuration. Event logs are modeled using fourth-order Markov chain and specifications are written in Linear Temporal Logic (LTL). To counter evasion and mimicry attacks, we propose a configuration randomization module. The approach provides robustness against evasion and mimicry attacks; however, we discuss that it still can be evaded to a certain extent. We validate our approach on a real-world dataset of thousands of meters collected at the AMI of a leading utility provider. © 2015 ACM.",Advanced metering infrastructure; Intrusion detection systems; Smart grid,Advanced metering infrastructures; Computation theory; Computer crime; Electric power transmission networks; Markov processes; Mercury (metal); Random processes; Smart power grids; Specifications; Temporal logic; Bi-directional communication; Computational power; Deployment costs; Fourth order; Intrusion Detection Systems; Intrusion detectors; Linear temporal logic; Smart grid; Intrusion detection
Model checking distributed mandatory access control policies,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942056163&doi=10.1145%2f2785966&partnerID=40&md5=f1c8721620febed7d759610e6621007e,"This work examines the use of model checking techniques to verify system-level security properties of a collection of interacting virtual machines. Specifically, we examine how local access control policies implemented in individual virtual machines and a hypervisor can be shown to satisfy global access control constraints. The SAL model checker is used to model and verify a collection of stateful domains with protected resources and local MAC policies attempting to access needed resources from other domains. The model is described along with verification conditions. The need to control state-space explosion is motivated and techniques for writing theorems and limiting domains explored. Finally, analysis results are examined along with analysis complexity. © 2015 ACM 1094-9224/2015/07-ART6 $15.00.",Access control; Model checking; Virtualization,Access control; Network security; Virtual machine; Virtualization; Access control policies; Control constraint; Control state spaces; Mandatory access control; Model-checking techniques; Security properties; System levels; Verification condition; Model checking
Integrity attacks on real-time pricing in electric power grids,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942038774&doi=10.1145%2f2790298&partnerID=40&md5=3881a3a415de2ae39588c1b0277a73fb,"Modern information and communication technologies used by electric power grids are subject to cybersecurity threats. This article studies the impact of integrity attacks on real-time pricing (RTP), an emerging feature of advanced power grids that can improve system efficiency. Recent studies have shown that RTP creates a closed loop formed by the mutually dependent real-time price signals and price-taking demand. Such a closed loop can be exploited by an adversary whose objective is to destabilize the pricing system. Specifically, small malicious modifications to the price signals can be iteratively amplified by the closed loop, causing highly volatile prices, fluctuating power demand, and increased system operating cost. This article adopts a control-theoretic approach to deriving the fundamental conditions of RTP stability under basic demand, supply, and RTP models that characterize the essential behaviors of consumers, suppliers, and system operators, as well as two broad classes of integrity attacks, namely, the scaling and delay attacks. We show that, under an approximated linear time-invariant formulation, the RTP system is at risk of being destabilized only if the adversary can compromise the price signals advertised to consumers, by either reducing their values in the scaling attack or providing old prices to over half of all consumers in the delay attack. The results provide useful guidelines for system operators to analyze the impact of various attack parameters on system stability so that they may take adequate measures to secure RTP systems. © 2015 ACM 1094-9224/2015/07-ART5 $15.00.",Cyber security; Demand response; Electricity market; Power grid; Real-time pricing; Smart grid; Stability,Consumer behavior; Convergence of numerical methods; Costs; Electric power system security; Electric power transmission networks; Power markets; Security of data; Smart power grids; Cyber security; Demand response; Power grids; Real time pricing; Smart grid; Electric power system stability
V Vote: A verifiable voting system,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934989092&doi=10.1145%2f2746338&partnerID=40&md5=1d7a2a5c0b470c06f7e04fb0161cd509,"The Prêt à Voter cryptographic voting system was designed to be flexible and to offer voters a familiar and easy voting experience. In this article, we present our development of the Prêt à Voter design to a practical implementation used in a real state election in November 2014, called vVote. As well as solving practical engineering challenges, we have also had to tailor the system to the idiosyncrasies of elections in the Australian state of Victoria and the requirements of the Victorian Electoral Commission. This article includes general background, user experience, and details of the cryptographic protocols and human processes. We explain the problems, present solutions, then analyze their security properties and explain how they tie in to other design decisions. © 2015 ACM.",Verifiable electronic voting systems; Voting protocols,Cryptography; Cryptographic protocols; Cryptographic voting; Design decisions; Electronic voting systems; Practical engineering; Security properties; User experience; Voting protocols; Voting machines
CacheAudit: A tool for the static analysis of cache side channels,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934969322&doi=10.1145%2f2756550&partnerID=40&md5=2e303c5152e633a5497fb6f456375d0f,"We present CacheAudit, a versatile framework for the automatic, static analysis of cache side channels. CacheAudit takes as input a program binary and a cache configuration and derives formal, quantitative security guarantees for a comprehensive set of side-channel adversaries, namely, those based on observing cache states, traces of hits and misses, and execution times. Our technical contributions include novel abstractions to efficiently compute precise overapproximations of the possible side-channel observations for each of these adversaries. These approximations then yield upper bounds on the amount of information that is revealed. In case studies, we apply CacheAudit to binary executables of algorithms for sorting and encryption, including the AES implementation from the PolarSSL library, and the reference implementations of the finalists of the eSTREAM stream cipher competition. The results we obtain exhibit the influence of cache size, line size, associativity, replacement policy, and coding style on the security of the executables and include the first formal proofs of security for implementations with countermeasures such as preloading and data-independent memory access patterns. © 2015 ACM.",Caches; Side-channel attacks,Bins; Cryptography; Mobile security; Side channel attack; Amount of information; Binary executables; Cache configurations; Caches; Memory access patterns; Reference implementation; Replacement policy; Technical contribution; Static analysis
Misbehavior in Bitcoin: A study of double-spending and accountability,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930850900&doi=10.1145%2f2732196&partnerID=40&md5=55157f2fb4a2ccb57f7e63fbe28bb5d6,"Bitcoin is a decentralized payment system that relies on Proof-of-Work (PoW) to resist double-spending through a distributed timestamping service. To ensure the operation and security of Bitcoin, it is essential that all transactions and their order of execution are available to all Bitcoin users. Unavoidably, in such a setting, the security of transactions comes at odds with transaction privacy. Motivated by the fact that transaction confirmation in Bitcoin requires tens of minutes, we analyze the conditions for performing successful double-spending attacks against fast payments in Bitcoin, where the time between the exchange of currency and goods is short (in the order of a minute). We show that unless new detection techniques are integrated in the Bitcoin implementation, double-spending attacks on fast payments succeed with considerable probability and can be mounted at low cost. We propose a new and lightweight countermeasure that enables the detection of double-spending attacks in fast transactions. In light of such misbehavior, accountability becomes crucial. We show that in the specific case of Bitcoin, accountability complements privacy. To illustrate this tension, we provide accountability and privacy definition for Bitcoin, and we investigate analytically and empirically the privacy and accountability provisions in Bitcoin. © 2015 Association for Computing Machinery.","Bitcoin; Design; Distributed computing; Double-spending; Economics; Experimentation; K.4.1 [computers and society]: public policy issues - privacy; K.4.4 [computers and society]: electronic commerce - payment schemes, security; Monetization; Privacy; Security; Security",Costs; Data privacy; Design; Distributed computer systems; Economics; Public policy; Computers and societies; Double-spending; Experimentation; Monetization; Payment schemes; Security; Bitcoin
A large-scale evaluation of high-impact password strength meters,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930843938&doi=10.1145%2f2739044&partnerID=40&md5=510a2b8815626d88c3e517b1a9b414f0,"Passwords are ubiquitous in our daily digital lives. They protect various types of assets ranging from a simple account on an online newspaper website to our health information on government websites. However, due to the inherent value they protect, attackers have developed insights into cracking/guessing passwords both offline and online. In many cases, users are forced to choose stronger passwords to comply with password policies; such policies are known to alienate users and do not significantly improve password quality. Another solution is to put in place proactive password-strength meters/checkers to give feedback to users while they create new passwords. Millions of users are now exposed to these meters on highly popular web services that use user-chosen passwords for authentication. More recently, these meters are also being built into popular password managers, which protect several user secrets including passwords. Recent studies have found evidence that some meters actually guide users to choose better passwords - which is a rare bit of good news in password research. However, these meters are mostly based on ad hoc design. At least, as we found, most vendors do not provide any explanation for their design choices, sometimes making them appear as a black box. We analyze password meters deployed in selected popular websites and password managers. We document obfuscated source-available meters, infer the algorithm behind the closed-source ones, and measure the strength labels assigned to common passwords from several password dictionaries. From this empirical analysis with millions of passwords, we shed light on how the server end of some web service meters functions and provide examples of highly inconsistent strength outcomes for the same password in different meters, along with examples of many weak passwords being labeled as strong or even excellent. These weaknesses and inconsistencies may confuse users in choosing a stronger password, and thus may weaken the purpose of these meters. On the other hand, we believe these findings may help improve existing meters and possibly make them an effective tool in the long run. © 2015 Association for Computing Machinery.",H.1.2 [models and principles]: user/machine systems-human factors; Human factors; K.6.5 [management of computing and information systems]: security and protection - authentication; Password manager; Password strength; Security; Strength meter,Human engineering; Information management; Managers; Web services; Websites; H.1.2 [models and principles]: user/machine systems - human factors; Password managers; Password strength; Security; Security and protection; Authentication
Dynamic provable data possession,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929164643&doi=10.1145%2f2699909&partnerID=40&md5=a09cc1700d320cc7fd753227f9b52ba2,"As storage-outsourcing services and resource-sharing networks have become popular, the problem of efficiently proving the integrity of data stored at untrusted servers has received increased attention. In the Provable Data Possession (PDP)model, the client preprocesses the data and then sends them to an untrusted server for storage while keeping a small amount of meta-data. The client later asks the server to prove that the stored data have not been tampered with or deleted (without downloading the actual data). However, existing PDP schemes apply only to static (or append-only) files. We present a definitional framework and efficient constructions for Dynamic Provable Data Possession (DPDP), which extends the PDP model to support provable updates to stored data. We use a new version of authenticated dictionaries based on rank information. The price of dynamic updates is a performance change from O(1) to O(log n) (or O(nεlog n)) for a file consisting of n blocks while maintaining the same (or better, respectively) probability of misbehavior detection. Our experiments show that this slowdown is very low in practice (e.g., 415KB proof size and 30ms computational overhead for a 1GB file). We also show how to apply our DPDP scheme to outsourced file systems and version control systems (e.g., CVS). © 2015 ACM.",Cloud security; Coud storage; Outsourced storage; Proof of retrievability; Provable data possession; Secure storage,Mobile security; Security of data; Cloud securities; Computational overheads; Efficient construction; Misbehavior detection; Proof of retrievability; Provable data possessions; Secure storage; Version control system; Digital storage
Attacking the internet using broadcast digital television,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928475123&doi=10.1145%2f2723159&partnerID=40&md5=9c688b9ea5e4ae5a7581a78021d9f214,"In the attempt to bring modern broadband Internet features to traditional broadcast television, the Digital Video Broadcasting (DVB) consortium introduced a specification called Hybrid Broadcast-Broadband Television (HbbTV), which allows broadcast streams to include embedded HTML content that is rendered by the television. This system is already in very wide deployment in Europe and has recently been adopted as part of the American digital television standard. Our analyses of the specifications, and of real systems implementing them, show that the broadband and broadcast systems are combined insecurely. This enables a large-scale exploitation technique with a localized geographical footprint based on Radio Frequency (RF) injection, which requires a minimal budget and infrastructure and is remarkably difficult to detect. In this article, we present the attack methodology and a number of follow-on exploitation techniques that provide significant flexibility to attackers. Furthermore, we demonstrate that the technical complexity and required budget are low, making this attack practical and realistic, especially in areas with high population density: In a dense urban area, an attacker with a budget of about $450 can target more than 20,000 devices in a single attack. A unique aspect of this attack is that, in contrast to most Internet of Things/Cyber-Physical System threat scenarios, where the attack comes from the data network side and affects the physical world, our attack uses the physical broadcast network to attack the data network. © 2015 ACM.",Radio-frequency attacks; Relay attacks; Smart TV,Budget control; Computer crime; Computer graphics; Digital television; Multimedia systems; Population statistics; Radio broadcasting; Radio waves; Specifications; Television standards; Broadband televisions; Broadcast television; Exploitation techniques; High population density; Radio frequencies; Relay attack; Smart-TV; Technical complexity; Television broadcasting
Gatling: Automatic performance attack discovery in large-scale distributed systems,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929165830&doi=10.1145%2f2714565&partnerID=40&md5=b6dec3f9698ab4b29e65e278b23fd53b,"In this article, we propose Gatling, a framework that automatically finds performance attacks caused by insider attackers in large-scale message-passing distributed systems. In performance attacks, malicious nodes deviate from the protocol when sending or creatingmessages, with the goal of degrading system performance. We identify a representative set of basic malicious message delivery and lying actions and design a greedy search algorithm that finds effective attacks consisting of a subset of these actions. Although lying malicious actions are protocol dependent, requiring the format and meaning of messages, Gatling captures them without needing to modify the target system by using a type-aware compiler. We have implemented and used Gatling on nine systems, a virtual coordinate system, a distributed hash table lookup service and application, two multicast systems and one file sharing application, and three secure systems designed specifically to tolerate insiders, two based on virtual coordinates and one using Outlier Detection, one invariant derived from physical laws, and the last one a Byzantine resilient replication system. We found a total of 48 attacks, with the time needed to find each attack ranging from a few minutes to a few hours. © 2015 ACM.",Automatic attack discovery; Distributed systems,Distributed computer systems; Message passing; Automatic attack discovery; Distributed Hash Table; Distributed systems; File sharing application; Greedy search algorithms; Large-scale distributed system; Virtual coordinate systems; Virtual coordinates; Table lookup
"Picture gesture authentication: Empirical analysis, automated attacks, and scheme evaluation",2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928468420&doi=10.1145%2f2701423&partnerID=40&md5=6ddff9806119c5c9731abe625c023e2c,"Picture gesture authentication has been recently introduced as an alternative login experience to text-based password on touch-screen devices. In particular, the newly on market Microsoft Windows 8TM operating system adopts such an alternative authentication to complement its traditional text-based authentication. We present an empirical analysis of picture gesture authentication on more than 10,000 picture passwords collected from more than 800 subjects through online user studies. Based on the findings of our user studies, we propose a novel attack framework that is capable of cracking passwords on previously unseen pictures in a picture gesture authentication system. Our approach is based on the concept of selection function that models users' thought processes in selecting picture passwords. Our evaluation results show the proposed approach could crack a considerable portion of picture passwords under different settings. Based on the empirical analysis and attack results, we comparatively evaluate picture gesture authentication using a set of criteria for a better understanding of its advantages and limitations.",Automated attacks; Empirical analysis; Picture gesture authentication; Scheme evaluation,Authentication; Touch screens; Windows operating system; Authentication systems; Automated attacks; Empirical analysis; Evaluation results; Microsoft windows; Scheme evaluation; Selection function; Thought process; Image analysis
Cryptographic theory meets practice: Efficient and privacy-preserving payments for public transport,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929144617&doi=10.1145%2f2699904&partnerID=40&md5=960e81d588f9a19305baa6b8372cc955,"We propose a new lightweight cryptographic payment scheme for transit systems, called P4R (Privacy- Preserving Pre-Payments with Refunds), which is suitable for low-cost user devices with limited capabilities. Using P4R, users deposit money to obtain one-show credentials, where each credential allows the user to make an arbitrary ride on the system. The trip fare is determined on-the-fly at the end of the trip. If the deposit for the credential exceeds this fare, the user obtains a refund. Refund values collected over several trips are aggregated in a single token, thereby saving memory and increasing privacy. Our solution builds on Brands's e-cash scheme to realize the prepayment system and on Boneh-Lynn-Shacham (BLS) signatures to implement the refund capabilities. Compared to a Brands-only solution for transportation payment systems, P4R allows us to minimize the number of coins a user needs to pay for his rides and thus minimizes the number of expensive withdrawal transactions, as well as storage requirements for the fairly large coins. Moreover, P4R enables flexible pricing because it allows for exact payments of arbitrary amounts (within a certain range) using a single fast paying (and refund) transaction. Fortunately, the mechanisms enabling these features require very little computational overhead. Choosing contemporary security parameters, we implemented P4R on a prototyping payment device and show its suitability for future transit payment systems. Estimation results demonstrate that the data required for 20 rides consume less than 10KB of memory, and the payment and refund transactions during a ride take less than half a second. We show that malicious users are not able to cheat the system by receiving a refund that exceeds the overall deposit minus the overall fare and can be identified during double-spending checks. At the same time, the system protects the privacy of honest users in that transactions are anonymous (except for deposits) and trips are unlinkable. © 2015 ACM.",E-cash; Lightweight payments; Privacy; Refunds; Transportation systems,Computation theory; Cryptography; Deposits; Digital storage; Economics; Electronic money; Mass transportation; Network security; Computational overheads; E-cash; Lightweight payments; Pre-payment systems; Refunds; Security parameters; Storage requirements; Transportation system; Data privacy
Pareto-optimal adversarial defense of enterprise systems,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924809380&doi=10.1145%2f2699907&partnerID=40&md5=e67efcd4f22e28a355b506a5b1958a2c,"The National Vulnerability Database (NVD) maintained by the US National Institute of Standards and Technology provides valuable information about vulnerabilities in popular software, as well as any patches available to address these vulnerabilities. Most enterprise security managers today simply patch the most dangerous vulnerabilities-an adversary can thus easily compromise an enterprise by using less important vulnerabilities to penetrate an enterprise. In this article, we capture the vulnerabilities in an enterprise as a Vulnerability Dependency Graph (VDG) and show that attacks graphs can be expressed in them. We first ask the question: What set of vulnerabilities should an attacker exploit in order to maximize his expected impact? We show that this problem can be solved as an integer linear program. The defender would obviously like to minimize the impact of the worst-case attack mounted by the attacker-but the defender also has an obligation to ensure a high productivity within his enterprise. We propose an algorithm that finds a Pareto-optimal solution for the defender that allows him to simultaneously maximize productivity and minimize the cost of patching products on the enterprise network. We have implemented this framework and show that runtimes of our computations are all within acceptable time bounds even for large VDGs containing 30K edges and that the balance between productivity and impact of attacks is also acceptable. © 2015 ACM 1094-9224/2015/03-ART11 $15.00.",Adversarial models; Computer security; Enterprise systems,Integer programming; Productivity; Security of data; Security systems; Enterprise networks; Enterprise security; Enterprise system; High productivity; Integer linear programs; National Institute of Standards and Technology; National vulnerability database; Pareto optimal solutions; Pareto principle
A visualizable evidence-driven approach for Authorship attribution,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924768546&doi=10.1145%2f2699910&partnerID=40&md5=71a78c844b60f5d83bc679e6ebe099c6,"The Internet provides an ideal anonymous channel for concealing computer-mediated malicious activities, as the network-based origins of critical electronic textual evidence (e.g., emails, blogs, forum posts, chat logs, etc.) can be easily repudiated. Authorship attribution is the study of identifying the actual author of the given anonymous documents based on the text itself, and for decades, many linguistic stylometry and computational techniques have been extensively studied for this purpose. However, most of the previous research emphasizes promoting the authorship attribution accuracy, and few works have been done for the purpose of constructing and visualizing the evidential traits. In addition, these sophisticated techniques are difficult for cyber investigators or linguistic experts to interpret. In this article, based on the End-to-End Digital Investigation (EEDI) framework, we propose a visualizable evidence-driven approach, namely VEA, which aims at facilitating the work of cyber investigation. Our comprehensive controlled experiment and the stratified experiment on the real-life Enron email dataset demonstrate that our approach can achieve even higher accuracy than traditional methods; meanwhile, its output can be easily visualized and interpreted as evidential traits. In addition to identifying the most plausible author of a given text, our approach also estimates the confidence for the predicted result based on a given identification context and presents visualizable linguistic evidence for each candidate. © 2015 ACM 1094-9224/2015/03-ART12 $15.00.",Authorship attribution; Computational linguistics; Cyber forensics; Text mining,Computational linguistics; Electronic mail; Linguistics; Text mining; Authorship attribution; Computational technique; Controlled experiment; Cyber forensics; Digital investigation; End to end; Malicious activities; Network-based; Computer forensics
Silence is golden: Exploiting jamming and radio Silence to communicate,2015,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924755405&doi=10.1145%2f2699906&partnerID=40&md5=2ededeee6430c50aab2ddf1db7beaf5a,"Jamming techniques require only moderate resources to be deployed, while their effectiveness in disrupting communications is unprecedented. In this article, we introduce several contributions to jamming mitigation. In particular, we introduce a novel adversarymodel that has both (unlimited) jamming reactive capabilities as well as powerful (but limited) proactive jamming capabilities. Under this adversary model, to the best of our knowledge more powerful than any other adversary model addressed in the literature, the communication bandwidth provided by current anti-jamming solutions drops to zero. We then present Silence is Golden (SiG): a novel anti-jamming protocol that, introducing a tunable, asymmetric communication channel, is able to mitigate the adversary capabilities, enabling the parties to communicate. For instance, withSiGit is possible to deliver a 128-bits-long message with a probability greater than 99% in 4096 time slots despite the presence of a jammer that jams all on-the-fly communications and 74% of the silent radio spectrum-while competing proposals simply fail. Moreover, whenSiGis used in a scenario in which the adversary can jam only a subset of all the available frequencies, performance experiences a boost: a 128-bits-long message is delivered within just 17 time slots for an adversary able to jam 90% of the available frequencies. We present a thorough theoretical analysis for the solution, which is supported by extensive simulation results, showing the viability of our proposal. © 2015 ACM 1094-9224/2015/03-ART9 $15.00.",Anti-jamming protocols,Security systems; Silicon; Adversary modeling; Anti-jamming; Communication bandwidth; Extensive simulations; Jamming mitigation; Jamming technique; On the flies; Radio spectra; Jamming
Spartan RPC: Remote procedure call authorization in wireless sensor networks,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911899707&doi=10.1145%2f2644809&partnerID=40&md5=96be712aeb847b0aa84b0c2e0f7e702a,"We describe SpartanRPC, a secure middleware technology that supports cooperation between distinct security domains in wireless sensor networks. SpartanRPC extends nesC to provide a link-layer remote procedure call (RPC) mechanism, along with an enhancement of configuration wirings that allow specification of remote, dynamic endpoints. RPC invocation is secured via an authorization logic that enables servers to specify access policies and requires clients to prove authorization. This mechanism is implemented using a combination of symmetric and public key cryptography. We report on benchmark testing of a prototype implementation and on an application of the framework that supports secure collaborative use and administration of an existing WSN data-gathering system. © 2014 ACM.",Remote procedure call; Sensor networks; Trust management,Benchmarking; Middleware; Network security; Public key cryptography; Sensor networks; Benchmark testing; Data gathering systems; Middleware technology; Prototype implementations; Remote Procedure Call; Remote procedure calls; Security domains; Trust management; Wireless sensor networks
Know your enemy: Compromising adversaries in protocol analysis,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911908058&doi=10.1145%2f2658996&partnerID=40&md5=693b2c6905c4029610906a8af91f7c21,"We present a symbolic framework, based on a modular operational semantics, for formalizing different notions of compromise relevant for the design and analysis of cryptographic protocols. The framework's rules can be combined to specify different adversary capabilities, capturing different practically-relevant notions of key and state compromise. The resulting adversary models generalize the models currently used in different domains, such as security models for authenticated key exchange. We extend an existing securityprotocol analysis tool, Scyther, with our adversary models. This extension systematically supports notions such as weak perfect forward secrecy, key compromise impersonation, and adversaries capable of statereveal queries. Furthermore, we introduce the concept of a protocol-security hierarchy, which classifies the relative strength of protocols against different adversaries. In case studies, we use Scyther to analyse protocols and automatically construct protocol-security hierarchies in the context of our adversary models. Our analysis confirms known results and uncovers new attacks. Additionally, our hierarchies refine and correct relationships between protocols previously reported in the cryptographic literature. © 2014 ACM.",Adversary models; Automated analysis; Security protocols; Threat models,Cryptography; Semantics; Adversary models; Authenticated key exchange; Automated analysis; Cryptographic protocols; Key-compromise impersonation; Perfect forward secrecy; Security protocols; Threat models; Network security
Stop watch: A cloud architecture for timing channel mitigation,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911923046&doi=10.1145%2f2670940&partnerID=40&md5=f92fd3e61e37b8666506730216648cdb,"This article presents StopWatch, a system that defends against timing-based side-channel attacks that arise from coresidency of victims and attackers in infrastructure-as-a-service clouds. StopWatch triplicates each cloud-resident guest virtual machine (VM) and places replicas so that the three replicas of a guest VM are coresident with nonoverlapping sets of (replicas of) other VMs. StopWatch uses the timing of I/O events at a VM's replicas collectively to determine the timings observed by each one or by an external observer, so that observable timing behaviors are similarly likely in the absence of any other individual, coresident VMs. We detail the design and implementation of StopWatch in Xen, evaluate the factors that influence its performance, demonstrate its advantages relative to alternative defenses against timing side channels with commodity hardware, and address the problem of placing VM replicas in a cloud under the constraints of StopWatch so as to still enable adequate cloud utilization. © 2014 ACM.",Clouds; Replication; Side channels; Timing channels; Virtualization,Clouds; Infrastructure as a service (IaaS); Stop watches; Timing circuits; Virtual machine; Virtualization; Cloud architectures; Commodity hardware; Design and implementations; External observer; Replication; Side-channel; Timing channels; Timing side channels; Side channel attack
Mutual authentication and trust bootstrapping towards secure disk encryption,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911926769&doi=10.1145%2f2663348&partnerID=40&md5=6ffed8d89b936a3c10101fbdb219103f,"The weakest link in software-based full disk encryption is the authentication procedure. Since the master boot record must be present unencrypted in order to launch the decryption of remaining system parts, it can easily be manipulated and infiltrated by bootkits that perform keystroke logging; consequently, passwordbased authentication schemes become attackable. The current technological response, as enforced by Bit- Locker, verifies the integrity of the boot process by use of the trusted platform module. But, as we show, this countermeasure is insufficient in practice. We present STARK, the first tamperproof authentication scheme that mutually authenticates the computer and the user in order to resist keylogging during boot. To achieve this, STARK implements trust bootstrapping from a secure token to the whole PC. The secure token is an active USB drive that verifies the integrity of the PC and indicates the verification status by an LED to the user. This way, users can ensure the authenticity of the PC before entering their passwords. © 2014 ACM.",Full disk encryption; Mutual authentication; Trust bootstrapping,Authentication; Authentication scheme; Full disk encryptions; Mutual authentication; Password-based authentication; Tamperproof; Trust bootstrapping; USB drives; Weakest links; Cryptography
Rumpole: An introspective break-glass access control language,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906253527&doi=10.1145%2f2629502&partnerID=40&md5=146c141998ffbe188401017e88c3b7a2,"Access control policies define what resources can be accessed by which subjects and under which conditions. It is, however, often not possible to anticipate all subjects that should be permitted access and the conditions under which they should be permitted. For example, predicting and correctly encoding all emergency and exceptional situations is impractical. Traditional access control models simply deny all requests that are not permitted, and in doing so may cause unpredictable and unacceptable consequences. To overcome this issue, break-glass access control models permit a subject to override an access control denial if he accepts a set of obligatory actions and certain override conditions are met. Existing break-glass models are limited in how the override decision is specified. They either grant overrides for a predefined set of exceptional situations, or they grant unlimited overrides to selected subjects, and as such, they suffer from the difficulty of correctly encoding and predicting all override situations and permissions. To address this, we develop Rumpole, a novel break-glass language that explicitly represents and infers knowledge gaps and knowledge conflicts about the subject's attributes and the contextual conditions, such as emergencies. For example, a Rumpole policy can distinguish whether or not it is known that an emergency holds. This leads to a more informed decision for an override request, whereas current break-glass languages simply assume that there is no emergency if the evidence for it is missing. To formally define Rumpole, we construct a novel many-valued logic programming language called Beagle. It has a simple syntax similar to that of Datalog, and its semantics is an extension of Fitting's bilattice-based semantics for logic programs. Beagle is a knowledge non-monotonic langauge, and as such, is strictly more expressive than current many-valued logic programming languages. © 2014 ACM.",Access control; Break-glass access control; Logic programming; Many-valued logics; Security,Encoding (symbols); Glass; Logic programming; Many valued logics; Semantics; Access control language; Access control models; Access control policies; Datalog; Informed decision; Knowledge gaps; Logic programs; Security; Access control
Security analysis of accountable anonymity in dissent,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906269209&doi=10.1145%2f2629621&partnerID=40&md5=5e1436556bb8194ef9779e28954732b3,"Users often wish to communicate anonymously on the Internet, for example, in group discussion or instant messaging forums. Existing solutions are vulnerable to misbehaving users, however, who may abuse their anonymity to disrupt communication. Dining Cryptographers Networks (DC-nets) leave groups vulnerable to denial-of-service and Sybil attacks; mix networks are difficult to protect against traffic analysis; and accountable voting schemes are unsuited to general anonymous messaging. DISSENT is the first general protocol offering provable anonymity and accountability for moderate-size groups, while efficiently handling unbalanced communication demands among users. We present an improved and hardened DISSENT protocol, define its precise security properties, and offer rigorous proofs of these properties. The improved protocol systematically addresses the delicate balance between provably hiding the identities of well-behaved users, while provably revealing the identities of disruptive users, a challenging task because many forms of misbehavior are inherently undetectable. The new protocol also addresses several nontrivial attacks on the original DISSENT protocol stemming from subtle design flaws. © 2014 ACM.",Accountable anonymity; Anonymous communication; Provable security,Computer science; Safety engineering; Accountable anonymity; Anonymous communication; Denial of Service; Group discussions; Instant messaging; Provable security; Security analysis; Security properties; Telecommunication services
Comparing vulnerability severity and exploits using case-control studies,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906251599&doi=10.1145%2f2630069&partnerID=40&md5=524ea203ee2ad52f0d39d57b6daea6ac,"(U.S.) Rule-based policies for mitigating software risk suggest using the CVSS score to measure the risk of an individual vulnerability and act accordingly. A key issue is whether the 'danger' score does actually match the risk of exploitation in the wild, and if and how such a score could be improved. To address this question, we propose using a case-control studymethodology similar to the procedure used to link lung cancer and smoking in the 1950s. A case-control study allows the researcher to draw conclusions on the relation between some risk factor (e.g., smoking) and an effect (e.g., cancer) by looking backward at the cases (e.g., patients) and comparing them with controls (e.g., randomly selected patients with similar characteristics). Themethodology allows us to quantify the risk reduction achievable by acting on the risk factor.We illustrate the methodology by using publicly available data on vulnerabilities, exploits, and exploits in the wild to (1) evaluate the performances of the current risk factor in the industry, the CVSS base score; (2) determine whether it can be improved by considering additional factors such the existence of a proof-of-concept exploit, or of an exploit in the black markets. Our analysis reveals that (a) fixing a vulnerability just because it was assigned a high CVSS score is equivalent to randomly picking vulnerabilities to fix; (b) the existence of proof-of-concept exploits is a significantly better risk factor; (c) fixing in response to exploit presence in black markets yields the largest risk reduction. © 2014 ACM.",Compliance; CVSS; Exploitation; Patching; Software vulnerability,Commerce; Diseases; Compliance; CVSS; Exploitation; Patching; Software vulnerabilities; Risk assessment
A framework for expressing and enforcing purpose-based privacy policies,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906258852&doi=10.1145%2f2629689&partnerID=40&md5=b2d8c8e9f0547899944aea002a85d9c7,"Purpose is a key concept in privacy policies. Although some models have been proposed for enforcing purposebased privacy policies, little has been done in defining formal semantics for purpose, and therefore an effective enforcement mechanism for such policies has remained a challenge. We have developed a framework for expressing and enforcing such policies by giving a formal definition of purpose and proposing a modal-logic language for formally expressing purpose constraints. The semantics of this language are defined over an abstract model of workflows. Based on this formal framework, we discuss some properties of purpose, show how common forms of purpose constraints can be formalized, how purpose-based constraints can be connected to more general access control policies, and how they can be enforced in a workflow-based information system by extending common access control technologies. © 2014 ACM.",Modal logic; Petri net; Privacy; Purpose; Purpose-based policies; Semantics; Workflow,Data privacy; Petri nets; Semantics; Abstract modeling; Access control policies; Access control technologies; Enforcement mechanisms; Formal definition; Modal logic; Purpose; Workflow; Access control
Sophisticated access control via SMT and logical frameworks,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900323684&doi=10.1145%2f2595222&partnerID=40&md5=7c6a1a510807ebc37b622d7e08aab379,"We introduce a new methodology for formulating, analyzing, and applying access-control policies. Policies are expressed as formal theories in the SMT (satisfiability-modulo-theories) subset of typed first-order logic, and represented in a programmable logical framework, with each theory extending a core ontology of access control. We reduce both request evaluation and policy analysis to SMT solving, and provide experimental results demonstrating the practicality of these reductions. We also introduce a class of canonical requests and prove that such requests can be evaluated in linear time. In many application domains, access requests are either naturally canonical or can easily be put into canonical form. The resulting policy framework is more expressive than XACML and languages in the Datalog family, without compromising efficiency. Using the computational logic facilities of the framework, a wide range of sophisticated policy analyses (including consistency, coverage, observational equivalence, and change impact) receive succinct formulations whose correctness can be straightforwardly verified. The use of SMT solving allows us to efficiently analyze policies with complicated numeric (integer and real) constraints, a weak point of previous policy analysis systems. Further, by leveraging the programmability of the underlying logical framework, our system provides exceptionally flexible ways of resolving conflicts and composing policies. Specifically, we show that our system subsumes FIA (Fine-grained Integration Algebra), an algebra recently developed for the purpose of integrating complex policies. © 2014 ACM.",Access control; Access request; Authentication; Canonical requests; Policies; Policy analysis; SMT; XACML,Algebra; Authentication; Formal logic; Logic programming; Public policy; Surface mount technology; Access request; Canonical requests; Computational logic; First order logic; Logical frameworks; Observational equivalences; Policy analysis; XACML; Access control
An anti-phishing system employing diffused information,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900297126&doi=10.1145%2f2584680&partnerID=40&md5=022c792dc1a8d4c8c83487c05aae3ae5,"The phishing scam and its variants are estimated to cost victims billions of dollars per year. Researchers have responded with a number of anti-phishing systems, based either on blacklists or on heuristics. The former cannot cope with the churn of phishing sites, while the latter usually employ decision rules that are not congruent to human perception. We propose a novel heuristic anti-phishing system that explicitly employs gestalt and decision theory concepts to model perceptual similarity. Our system is evaluated on three corpora contrasting legitimate Web sites with real-world phishing scams. The proposed system's performance was equal or superior to current best-of-breed systems. We further analyze current anti-phishing warnings from the perspective of warning theory, and propose a new warning design employing our Gestalt approach. © 2014 ACM.",Compression-based learning; Phishing,Computer science; Safety engineering; Anti-phishing; Compression-based learning; Decision rules; Human perception; Perceptual similarity; Phishing; Real-world; System's performance; Computer crime
Cross-domain password-based authenticated key exchange revisited,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900304781&doi=10.1145%2f2584681&partnerID=40&md5=402de44b6a04a811c4518234641f9619,"We revisit the problem of secure cross-domain communication between two users belonging to different security domains within an open and distributed environment. Existing approaches presuppose that either the users are in possession of public key certificates issued by a trusted certificate authority (CA), or the associated domain authentication servers share a long-term secret key. In this article, we propose a generic framework for designing four-party password-based authenticated key exchange (4PAKE) protocols. Our framework takes a different approach from previous work. The users are not required to have public key certificates, but they simply reuse their login passwords, which they share with their respective domain authentication servers. On the other hand, the authentication servers, assumed to be part of a standard PKI, act as ephemeral CAs that certify some key materials that the users can subsequently use to exchange and agree on as a session key. Moreover, we adopt a compositional approach. That is, by treating any secure two-party password-based key exchange (2PAKE) protocol and two-party asymmetric-key/symmetric-key-based key exchange (2A/SAKE) protocol as black boxes, we combine them to obtain generic and provably secure 4PAKE protocols. © 2014 ACM.",Client-to-client; Cross-domain; Key exchange; Password-based protocol,Computer science; Safety engineering; Authenticated key exchange; Client-to-client; Cross-domain; Cross-domain communication; Distributed environments; Key exchange; Password-based key exchange; Public key certificates; Authentication
Off-path TCP injection attacks,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900295815&doi=10.1145%2f2597173&partnerID=40&md5=11b5362ebd1cc39dd5ae3e85e6e67bd1,"We present practical off-path TCP injection attacks for connections between current, nonbuggy browsers and Web servers. The attacks allow Web-cache poisoning with malicious objects such as spoofed Web pages and scripts; these objects can be cached for a long period of time, exposing any user of that cache to cross-site scripting, cross-site request forgery, and phishing attacks. In contrast to previous TCP injection attacks, we do not require MitM capabilities or malware running on the client machine. Instead, our attacks rely on a weaker assumption, that the user only enters a malicious Web site, but does not download or install any application. Our attacks exploit subtle details of the TCP and HTTP specifications, and features of legitimate (and very common) browser implementations. An empirical evaluation of our techniques with current versions of browsers shows that connections with most popular Web sites are vulnerable. We conclude this work with practical client- and server-end defenses against our attacks. © 2014 ACM.",Browser security; Web and network security,Computer crime; HTTP; Websites; Browser security; Client machine; Cross site scripting; Empirical evaluations; Malwares; Phishing attacks; TCP and HTTP; Web servers; Network security
EXPOSURE: A passive DNS analysis service to detect and report malicious domains,2014,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900328882&doi=10.1145%2f2584679&partnerID=40&md5=ee68f1e8044a4e1c19b8171153f81686,"A wide range of malicious activities rely on the domain name service (DNS) to manage their large, distributed networks of infected machines. As a consequence, the monitoring and analysis of DNS queries has recently been proposed as one of the most promising techniques to detect and blacklist domains involved in malicious activities (e.g., phishing, spam, botnets command-and-control, etc.). EXPOSURE is a system we designed to detect such domains in real time, by applying 15 unique features grouped in four categories. We conducted a controlled experiment with a large, real-world dataset consisting of billions of DNS requests. The extremely positive results obtained in the tests convinced us to implement our techniques and deploy it as a free, online service. In this article, we present the EXPOSURE system and describe the results and lessons learned from 17 months of its operation. Over this amount of time, the service detected over 100K malicious domains. The statistics about the time of usage, number of queries, and target IP addresses of each domain are also published on a daily basis on the service Web page. © 2014 ACM.",Domain name system; Machine learning; Malicious domains,Computer science; Learning systems; Safety engineering; Command-and-control; Controlled experiment; Distributed networks; Domain name service; Domain name system; Malicious activities; Monitoring and analysis; On-line service; Internet protocols
An experimental security analysis of two satphone standards,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890377437&doi=10.1145%2f2535522&partnerID=40&md5=82ab2c258201e21bc490cc223d0809ad,"General-purpose communication systems such as GSM and UMTS have been in the focus of security researchers for over a decade now. Recently also technologies that are only used under more specific circumstances have come into the spotlight of academic research and the hacker scene alike. A striking example of this is recent work [Driessen et al. 2012] that analyzed the security of the over-the-air encryption in the two existing ETSI satphone standards GMR-1 and GMR-2. The firmware of handheld devices was reverseengineered and the previously unknown stream ciphers A5-GMR-1 and A5-GMR-2 were recovered. In a second step, both ciphers were cryptanalized, resulting in a ciphertext-only attack on A5-GMR-1 and a known-plaintext attack on A5-GMR-2. © 2013 ACM.",,Cryptography; Firmware; Personal computing; Academic research; Ciphertext-only attacks; Gsm and umts; Hand held device; Known-plaintext attacks; Security analysis; Stream Ciphers; Galvanomagnetic effects
Access privacy and correctness on untrusted storage,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890332626&doi=10.1145%2f2535524&partnerID=40&md5=0d2cb331c658161a3d3bdf6c8e2d0412,"We introduce a new practical mechanism for remote data storage with access pattern privacy and correctness. A storage client can deploy this mechanism to issue encrypted reads, writes, and inserts to a potentially curious and malicious storage service provider, without revealing information or access patterns. The provider is unable to establish any correlation between successive accesses, or even to distinguish between a read and a write. Moreover, the client is provided with strong correctness assurances for its operations-illicit provider behavior does not go undetected.We describe a practical system that can execute an unprecedented several queries per second on terabyte-plus databases while maintaining full computational privacy and correctness. © 2013 ACM.",,Safety engineering; Access patterns; Practical systems; Remote data; Storage service providers; Untrusted storages; Computer science
Bringing Java's wild native world under control,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890392640&doi=10.1145%2f2535505&partnerID=40&md5=4fed530ae17755443062b3c221598869,"For performance and for incorporating legacy libraries, many Java applications contain native-code components written in unsafe languages such as C and C++. Native-code components interoperate with Java components through the Java Native Interface (JNI). As native code is not regulated by Java's security model, it poses serious security threats to the managed Java world. We introduce a security framework that extends Java's security model and brings native code under control. Leveraging software-based fault isolation, the framework puts native code in a separate sandbox and allows the interaction between the native world and the Java world only through a carefully designed pathway. Two different implementations were built. In one implementation, the security framework is integrated into a Java Virtual Machine (JVM). In the second implementation, the framework is built outside of the JVM and takes advantage of JVM-independent interfaces. The second implementation provides JVM portability, at the expense of some performance degradation. Evaluation of our framework demonstrates that it incurs modest runtime overhead while significantly enhancing the security of Java applications. © 2013 ACM.",,Computer science; Safety engineering; Java applications; Java components; Java Native Interfaces; Java virtual machines; Performance degradation; Runtime overheads; Security frameworks; Security threats; Java programming language
Secure and verifiable outsourcing of large-scale biometric computations,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890386591&doi=10.1145%2f2535523&partnerID=40&md5=e14cd9999ccb7f3b42e90ce93bcaa6f7,"Cloud computing services are becoming more prevalent and readily available today, bringing to us economies of scale and making large-scale computation feasible. Security and privacy considerations, however, stand in the way of fully utilizing the benefits of such services and architectures. In this work we address the problem of secure outsourcing of large-scale biometric experiments to a cloud or grid in a way that the client can verify that with very high probability the task was computed correctly.We conduct thorough theoretical analysis of the proposed techniques and provide implementation results that indicate that our solution imposes modest overhead. © 2013 ACM.",,Biometrics; Cloud computing services; High probability; Large-Scale Computations; Security and privacy; US economy; Outsourcing
Examining a large keystroke biometrics dataset for statistical-attack openings,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885006760&doi=10.1145%2f2516960&partnerID=40&md5=a665c00492574c0282af5ed127c6e81c,"Research on keystroke-based authentication has traditionally assumed human impostors who generate forgeries by physically typing on the keyboard. With bots now well understood to have the capacity to originate precisely timed keystroke sequences, this model of attack is likely to underestimate the threat facing a keystroke-based system in practice. In this work, we investigate how a keystroke-based authentication system would perform if it were subjected to synthetic attacks designed to mimic the typical user. To implement the attacks, we perform a rigorous statistical analysis on keystroke biometrics data collected over a 2-year period from more than 3000 users, and then use the observed statistical traits to design and launch algorithmic attacks against three state-of-the-art password-based keystroke verification systems. Relative to the zero-effort attacks typically used to test the performance of keystroke biometric systems, we show that our algorithmic attack increases the mean Equal Error Rates (EERs) of three high performance keystroke verifiers by between 28.6% and 84.4%. We also find that the impact of the attack is more pronounced when the keystroke profiles subjected to the attack are based on shorter strings, and that some users see considerably greater performance degradation under the attack than others. This article calls for a shift from the traditional zero-effort approach of testing the performance of password-based keystroke verifiers, to a more rigorous algorithmic approach that captures the threat posed by today's bots. © 2013 ACM.",Biometrics; Keystroke dynamics; Spoofing attacks,Algorithms; Authentication; Algorithmic approach; Algorithmic attacks; Authentication systems; Biometric systems; Keystroke dynamics; Performance degradation; Spoofing attacks; Verification systems; Biometrics
Modelling access propagation in dynamic systems,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885007282&doi=10.1145%2f2516951.2516952&partnerID=40&md5=85f2ba7279ec01a05d88435ba1e59e45,"Access control is a critical feature of many systems, including networks of services, processes within a computer, and objects within a running process. The security consequences of a particular architecture or access control policy are often difficult to determine, especially where some components are not under our control, where components are created dynamically, or where access policies are updated dynamically. The SERSCIS Access Modeller (SAM) takes a model of a system and explores how access can propagate through it. It can both prove defined safety properties and discover unwanted properties. By defining expected behaviours, recording the results as a baseline, and then introducing untrusted actors, SAM can discover a wide variety of design flaws. SAM is designed to handle dynamic systems (i.e., at runtime, new objects are created and access policies modified) and systems where some objects are not trusted. It extends previous approaches such as Scollar and Authodox to provide a programmer-friendly syntax for specifying behaviour, and allows modelling of services with mutually suspicious clients. Taking the Confused Deputy example from Authodox we show that SAM detects the attack automatically; using a web-based backup service, we show how to model RBAC systems, detecting a missing validation check; and using a proxy certificate system, we show how to extend it to model new access mechanisms. On discovering that a library fails to follow an RFC precisely, we re-evaluate our existing models under the new assumption and discover that the proxy certificate design is not safe with this library. © 2013 ACM.",Datalog; Object-capabilities; Proxy certificates,Computer science; Safety engineering; Access control policies; Access mechanism; Access policies; Critical features; Datalog; Object-capabilities; Proxy certificate; Validation checks; Access control
DriverGuard: Virtualization-based fine-grained protection on i/o flows,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885012508&doi=10.1145%2f2505123&partnerID=40&md5=343744a64807cbe8ebdddd74322cf80d,"Most commodity peripheral devices and their drivers are geared to achieve high performance with security functions being opted out. The absence of strong security measures invites attacks on the I/O data and consequently posts threats to those services feeding on them, such as fingerprint-based biometric authentication. In this article, we present a generic solution called DriverGuard, which dynamically protects the secrecy of I/O flows such that the I/O data are not exposed to the malicious kernel. Our design leverages a composite of cryptographic and virtualization techniques to achieve fine-grained protection without using any extra devices and modifications on user applications. We implement the DriverGuard prototype on Xen by adding around 1.7K SLOC. DriverGuard is lightweight as it only needs to protect around 2% of the driver code's execution. We measure the performance and evaluate the security of DriverGuard with three input devices (keyboard, fingerprint reader and camera) and three output devices (printer, graphic card, and sound card). The experiment results show that Driver Guard induces negligible overhead to the applications. © 2013 ACM.",Hypervisor; I/O data protection; Trusted path; Untrusted OS; Virtualization,Biometrics; Security of data; Smart cards; Data protection; Fingerprint reader; Fingerprint-based biometrics; Hypervisor; Security functions; Trusted path; Virtualization Techniques; Virtualizations; Virtual reality
Bridging the semantic gap in virtual machine introspection via online kernel data redirection,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884996603&doi=10.1145%2f2505124&partnerID=40&md5=ee3e82381b63c3b8b12a38b1eba4ae02,"It is generally believed to be a tedious, time-consuming, and error-prone process to develop a virtual machine introspection (VMI) tool because of the semantic gap. Recent advance shows that the semantic-gap can be largely narrowed by reusing the executed code from a trusted OS kernel. However, the limitation for such an approach is that it only reuses the exercised code through a training process, which suffers the code coverage issues. Thus, in this article, we present VMST, a new technique that can seamlessly bridge the semantic gap and automatically generate the VMI tools. The key idea is that, through system wide instruction monitoring, VMST automatically identifies the introspection related data from a secure-VM and online redirects these data accesses to the kernel memory of a product-VM, without any training. VMST offers a number of new features and capabilities. Particularly, it enables an in-VM inspection program (e.g., ps) to automatically become an out-of-VM introspection program. We have tested VMST with over 25 commonly used utilities on top of a number of different OS kernels including Linux and Microsoft Windows. The experimental results show that our technique is general (largely OS-independent), and it introduces 9.3X overhead for Linux utilities and 19.6X overhead for Windows utilities on average for the introspected program compared to the native in-VM execution without data redirection. © 2013 ACM.",,Computer operating systems; Computer simulation; Tools; Code coverage; Error-prone process; Inspection programs; Kernel memory; Microsoft windows; Semantic gap; Training process; Virtual machine introspection; Semantics
CPM: Masking code pointers to prevent code injection attacks,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880210740&doi=10.1145%2f2487222.2487223&partnerID=40&md5=561ff0a326b013ccdf0c94422c71e8af,"Code Pointer Masking (CPM) is a novel countermeasure against code injection attacks on native code. By enforcing the correct semantics of code pointers, CPM thwarts attacks that modify code pointers to divert the application's control flow. It does not rely on secret values such as stack canaries and protects against attacks that are not addressed by state-of-the-art countermeasures of similar performance. This article reports on two prototype implementations on very distinct processor architectures, showing that the idea behind CPM is portable. The evaluation also shows that the overhead of using our countermeasure is very small and the security benefits are substantial. © 2013 ACM.",Code injection; Code pointer; Countermeasure; Masking,Semantics; Speech intelligibility; Code injection; Code injection attacks; Code pointers; Control flows; Countermeasure; Processor architectures; Prototype implementations; Secret value; Computer architecture
On the parameterized complexity and kernelization of the workflow satisfiability problem,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880215215&doi=10.1145%2f2487222.2487226&partnerID=40&md5=cb8f0f51cbf8b4746749623062f95f56,"A workflow specification defines a set of steps and the order in which these steps must be executed. Security requirements may impose constraints on which groups of users are permitted to perform subsets of these steps. A workflow specification is said to be satisfiable if there exists an assignment of users to workflow steps that satisfies all the constraints. An algorithm for determining whether such an assignment exists is important, both as a static analysis tool for workflow specifications and for the construction of runtime reference monitors for workflow management systems. Finding such an assignment is a hard problem in general, but work by Wang and Li [2010] using the theory of parameterized complexity suggests that efficient algorithms exist under reasonable assumptions about workflow specifications. In this article, we improve the complexity bounds for the workflow satisfiability problem. We also generalize and extend the types of constraints that may be defined in a workflow specification and prove that the satisfiability problem remains fixed-parameter tractable for such constraints. Finally, we consider preprocessing for the problem and prove that in an important special case, in polynomial time, we can reduce the given input into an equivalent one where the number of users is at most the number of steps. We also show that no such reduction exists for two natural extensions of this case, which bounds the number of users by a polynomial in the number of steps, provided a widely accepted complexity-theoretical assumption holds. © 2013 ACM.",Authorization constraints; Kernelization; Parameterized complexity; Workflow satisfiability,Algorithms; Polynomial approximation; Specifications; Static analysis; Work simplification; Authorization constraints; Kernelization; Parameterized complexity; Satisfiability; Satisfiability problems; Security requirements; Workflow management systems; Workflow specification; Formal logic
Leakage mapping: A systematic methodology for assessing the side-channel information leakage of cryptographic implementations,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880212057&doi=10.1145%2f2487222.2487224&partnerID=40&md5=59f7d2b9330237ae523c7682b3b75b7f,"We propose a generalized framework to evaluate the side-channel information leakage of symmetric block ciphers. The leakage mapping methodology enables the systematic and efficient identification and mitigation of problematic information leakages by exhaustively considering relevant leakage models. The evaluation procedure bounds the anticipated resistance of an implementation to the general class of univariate differential side-channel analysis techniques. Typical applications are demonstrated using the well-known Hamming weight and Hamming distance leakage models, with recommendations for the incorporation of more accurate models. The evaluation results are empirically validated against correlation-based differential side-channel analysis attacks on two typical unprotected implementations of the Advanced Encryption Standard. © 2013 ACM.",Advanced encryption standard; Block cipher; Cryptanalysis; Cryptography; Differential power analysis; Encryption; Hardware security; Information leakage; Physical-layer security; Side-channel analysis,Data privacy; Hamming distance; Advanced Encryption Standard; Block ciphers; Cryptanalysis; Differential power Analysis; Hardware security; Information leakage; Physical-layer securities; Side-channel analysis; Cryptography
Enforceable security policies revisited,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880235899&doi=10.1145%2f2487222.2487225&partnerID=40&md5=46b1fa4715d6361ba70fab5f0e2d1cd7,"We revisit Schneider's work on policy enforcement by execution monitoring. We overcome limitations of Schneider's setting by distinguishing between system actions that are controllable by an enforcement mechanism and those actions that are only observable, that is, the enforcement mechanism sees them but cannot prevent their execution. For this refined setting, we give necessary and sufficient conditions on when a security policy is enforceable. To state these conditions, we generalize the standard notion of safety properties. Our classification of system actions also allows one, for example, to reason about the enforceability of policies that involve timing constraints. Furthermore, for different specification languages, we investigate the decision problem of whether a given policy is enforceable. We provide complexity results and show how to synthesize an enforcement mechanism from an enforceable policy. © 2013 ACM.",Automata; Monitoring; Safety properties; Security policies; Temporallogic,Access control; Automata theory; Monitoring; Specification languages; Temporal logic; Automata; Classification of systems; Enforcement mechanisms; Execution monitoring; Safety property; Security policy; Sufficient conditions; Timing constraints; Security systems
Fragmentation considered vulnerable,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878528223&doi=10.1145%2f2445566.2445568&partnerID=40&md5=0c56499cde6816b306347d27bd01331d,"We show that fragmented IPv4 and IPv6 traffic is vulnerable to effective interception and denial-of-service (DoS) attacks by an off -path attacker. Specifically, we demonstrate a weak attacker intercepting more than 80% of the data between peers and causing over 94% loss rate. We show that our attacks are practical through experimental validation on popular industrial and opensource products, with realistic network setups that involve NAT or tunneling and include concurrent legitimate traffic as well as packet losses. The interception attack requires a zombie agent behind the same NAT or tunnel-gateway as the victim destination; the DoS attack only requires a puppet agent, that is, a sandboxed applet or script running in web-browser context. The complexity of our attacks depends on the predictability of the IP Identification (ID) field which is typically implemented as one or multiple counters, as allowed and recommended by the IP specifications. The attacks are much simpler and more efficient for implementations, such as Windows, which use one ID counter for all destinations. Therefore, much of our focus is on presenting effective attacks for implementations, such as Linux, which use per-destination ID counters. We present practical defenses for the attacks presented in this article, the defenses can be deployed on network firewalls without changes to hosts or operating system kernel.© 2013 ACM.",Denial of service; IP fragmentation,Complex networks; Computer operating systems; Computer system firewalls; Gateways (computer networks); Internet protocols; Denial of Service; Denial of service attacks; Experimental validations; Interception attack; IP fragmentation; Network firewalls; Network setup; Operating system kernel; Security of data
Automated anomaly detector adaptation using adaptive threshold tuning,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878558678&doi=10.1145%2f2445566.2445569&partnerID=40&md5=b1c362ebb41c03bc5c35390efb7e4372,"Real-time network- and host-based Anomaly Detection Systems (ADSs) transform a continuous stream of input data into meaningful and quantifiable anomaly scores. These scores are subsequently compared to a fixed detection threshold and classified as either benign or malicious. We argue that a real-time ADS' input changes considerably over time and a fixed threshold value cannot guarantee good anomaly detection accuracy for such a time-varying input. In this article, we propose a simple and generic technique to adaptively tune the detection threshold of any ADS that works on threshold method. To this end, we first perform statistical and information-theoretic analysis of network- and host-based ADSs' anomaly scores to reveal a consistent time correlation structure during benign activity periods. We model the observed correlation structure using Markov chains, which are in turn used in a stochastic target tracking framework to adapt an ADS' detection threshold in accordance with real-time measurements. We also use statistical techniques to make the proposed algorithm resilient to sporadic changes and evasion attacks. In order to evaluate the proposed approach, we incorporate the proposed adaptive thresholding module into multiple ADSs and evaluate those ADSs over comprehensive and independently collected network and host attack datasets. We show that, while reducing the need of human threshold configuration, the proposed technique provides considerable and consistent accuracy improvements for all evaluated ADSs.© 2013 ACM.",Adaptive thresholding; Anomaly detection; Anomaly scores; Intrusion detection,Algorithms; Data processing; Information theory; Intrusion detection; Markov processes; Stochastic models; Target tracking; Adaptive thresholding; Anomaly detection; Anomaly detection systems; Anomaly scores; Correlation structure; Information-theoretic analysis; Real time measurements; Statistical techniques; Computer crime
Role mining with probabilistic models,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878619204&doi=10.1145%2f2445566.2445567&partnerID=40&md5=317ef9e881889f7d0e1f5c6535fcb8a0,"Role mining tackles the problem of finding a role-based access control (RBAC) configuration, given an accesscontrol matrix assigning users to access permissions as input. Most role-mining approaches work by constructing a large set of candidate roles and use a greedy selection strategy to iteratively pick a small subset such that the differences between the resulting RBAC configuration and the access control matrix are minimized. In this article, we advocate an alternative approach that recasts role mining as an inference problem rather than a lossy compression problem. Instead of using combinatorial algorithms to minimize the number of roles needed to represent the access-control matrix, we derive probabilistic models to learn the RBAC configuration that most likely underlies the given matrix. Our models are generative in that they reflect the way that permissions are assigned to users in a given RBAC configuration. We additionally model how user-permission assignments that conflict with an RBAC configuration emerge and we investigate the influence of constraints on role hierarchies and on the number of assignments. In experiments with access-control matrices from real-world enterprises, we compare our proposed models with other role-mining methods. Our results show that our probabilistic models infer roles that generalize well to new system users for a wide variety of data, while other models' generalization abilities depend on the dataset given.© 2013 ACM.",Access control; Clustering; Matrix factorization; RBAC; Role mining,Iterative methods; Mining; Alternative approach; Clustering; Combinatorial algorithm; Generalization ability; Matrix factorizations; RBAC; Role minings; Role-based Access Control; Access control
MOHAWK: Abstraction-refinement and bound-estimation for verifying access control policies,2013,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878578416&doi=10.1145%2f2445566.2445570&partnerID=40&md5=44de11dcc86ff81a04b13c2aa2929ab9,"Verifying that access-control systems maintain desired security properties is recognized as an important problem in security. Enterprise access-control systems have grown to protect tens of thousands of resources, and there is a need for verification to scale commensurately. We present techniques for abstractionrefinement and bound-estimation for bounded model checkers to automatically find errors in Administrative Role-Based Access Control (ARBAC) security policies. ARBAC is the first and most comprehensive administrative scheme for Role-Based Access Control (RBAC) systems. In the abstraction- refinement portion of our approach, we identify and discard roles that are unlikely to be relevant to the verification question (the abstraction step). We then restore such abstracted roles incrementally (the refinement steps). In the boundestimation portion of our approach, we lower the estimate of the diameter of the reachability graph from the worst-case by recognizing relationships between roles and state-change rules. Our techniques complement one another, and are used with conventional bounded model checking. Our approach is sound and complete: an error is found if and only if it exists.We have implemented our technique in an access-control policy analysis tool called MOHAWK. We show empirically that MOHAWK scales well to realistic policies, and provide a comparison with prior tools.© 2013 ACM.",,Abstracting; Distributed computer systems; Estimation; Model checking; Abstraction-refinement; Access control policies; Bounded model checkers; Bounded model checking; Reachability graphs; Role-based Access Control; Security properties; Sound and complete; Access control
Adversarial stylometry: Circumventing authorship recognition to preserve privacy and anonymity,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872026771&doi=10.1145%2f2382448.2382450&partnerID=40&md5=deb455919350b37e23ae762a12873fac,"The use of stylometry, authorship recognition through purely linguistic means, has contributed to literary, historical, and criminal investigation breakthroughs. Existing stylometry research assumes that authors have not attempted to disguise their linguistic writing style. We challenge this basic assumption of existing stylometry methodologies and present a new area of research: adversarial stylometry. Adversaries have a devastating effect on the robustness of existing classification methods. Our work presents a framework for creating adversarial passages including obfuscation, where a subject attempts to hide her identity, and imitation, where a subject attempts to frame another subject by imitating his writing style, and translation where original passages are obfuscated with machine translation services. This research demonstrates that manual circumvention methods work very well while automated translation methods are not effective. The obfuscation method reduces the techniques' effectiveness to the level of random guessing and the imitation attempts succeed up to 67% of the time depending on the stylometry technique used. These results are more significant given the fact that experimental subjects were unfamiliar with stylometry, were not professional writers, and spent little time on the attacks. This article also contributes to the field by using human subjects to empirically validate the claim of high accuracy for four current techniques (without adversaries).We have also compiled and released two corpora of adversarial stylometry texts to promote research in this field with a total of 57 unique authors. We argue that this field is important to a multidisciplinary approach to privacy, security, and anonymity. © 2012 ACM.",Algorithms; Experimentation Additional,Algorithms; Linguistics; Automated translation; Classification methods; Criminal investigation; Devastating effects; Experimentation Additional; Human subjects; Machine translations; Multi-disciplinary approach; Stylometry; Writing style; Research
Dynamic enforcement of abstract separation of duty constraints,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872051400&doi=10.1145%2f2382448.2382451&partnerID=40&md5=ec9f8cfef4f95f0b768109a7438243cd,"Separation of Duties (SoD) aims at preventing fraud and errors by distributing tasks and associated authorizations among multiple users. Li and Wang [2008] proposed an algebra (SoDA) for specifying SoD requirements, which is both expressive in the requirements it formalizes and abstract in that it is not bound to a workflow model. In this article, we bridge the gap between the specification of SoD constraints modeled in SoDA and their enforcement in a dynamic, service-oriented enterprise environment. We proceed by generalizing SoDA's semantics to traces, modeling workflow executions that satisfy the respective SoDA terms. We then refine the set of traces induced by a SoDA term to also account for a workflow's control-flow and role-based authorizations. Our formalization, which is based on the process algebra CSP, supports the enforcement of SoD on general workflows and handles changing role assignments during workflow execution, addressing a well-known source of fraud. The resulting CSP model serves as blueprint for a distributed and loosely coupled architecture where SoD enforcement is provisioned as a service. This concept, which we call SoD as a Service, facilitates a separation of concerns between business experts and security professionals. As a result, integration and configuration efforts are minimized and enterprises can quickly adapt to organizational, regulatory, and technological changes. We describe an implementation of SoD as a Service, which combines commercial components such as a workflow engine with newly developed components such as an SoD enforcement monitor. To evaluate our design decisions and to demonstrate the feasibility of our approach, we present a case study of a drug dispensation workflow deployed in a hospital. © 2012 ACM.",Design; Performance; Security; Theory Additional,Algebra; Computer crime; Crime; Design; Industry; Semantics; Commercial components; Control-flow; CSP model; Design decisions; Loosely coupled architectures; Multiple user; Performance; Process algebras; Role assignment; Role-based; Security; Security professionals; Separation of concerns; Separation of duty; Service-oriented enterprise; Technological change; Theory Additional; Work-flows; Workflow engines; Workflow execution; Workflow models; Abstracting
Effectiveness and detection of denial-of-service attacks in tor,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872008887&doi=10.1145%2f2382448.2382449&partnerID=40&md5=f0c4cb726e9718d1aa1bfc1d55a820b4,"Amherst Tor is one of the more popular systems for anonymizing near-real-time communications on the Internet. Borisov et al. [2007] proposed a denial-of-service-based attack on Tor (and related systems) that significantly increases the probability of compromising the anonymity provided. In this article, we analyze the effectiveness of the attack using both an analytic model and simulation.We also describe two algorithms for detecting such attacks, one deterministic and proved correct, the other probabilistic and verified in simulation. © 2012 ACM.",Algorithms; Measurement; Security Additional,Algorithms; Measurements; Analytic models; Denial of service attacks; Near-real time; Related systems; Security Additional; Network security
Probabilistic analysis of onion routing in a black-box model,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872023509&doi=10.1145%2f2382448.2382452&partnerID=40&md5=56a034581a8e5615fedd78ade9816b14,"We perform a probabilistic analysis of onion routing. The analysis is presented in a black-box model of anonymous communication in the Universally Composable (UC) framework that abstracts the essential properties of onion routing in the presence of an active adversary who controls a portion of the network and knows all a priori distributions on user choices of destination. Our results quantify how much the adversary can gain in identifying users by exploiting knowledge of their probabilistic behavior. In particular, we show that, in the limit as the network gets large, a user u's anonymity is worst either when the other users always choose the destination u is least likely to visit or when the other users always choose the destination u chooses. This worst-case anonymity with an adversary that controls a fraction b of the routers is shown to be comparable to the best-case anonymity against an adversary that controls a fraction √b. © 2012 ACM.",Security; Theory Additional,Computer science; Safety engineering; Active adversary; Anonymous communication; Black-box model; Onion routing; Priori distribution; Probabilistic analysis; Probabilistic behavior; Security; Theory Additional; Universally composable; Network security
LOT: A defense against IP spoofing and flooding attacks,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878546532&doi=10.1145%2f2240276.2240277&partnerID=40&md5=7d0993c2fc5d82cec44dc93fd328f279,"We present LOT, a lightweight plug and play secure tunneling protocol deployed at network gateways. Two communicating gateways, A and B, running LOT would automatically detect each other and establish an efficient tunnel, securing communication between them. LOT tunnels allow A to discard spoofed packets that specify source addresses in B's network and vice versa. This helps to mitigate many attacks, including DNS poisoning, network scans, and most notably (Distributed) Denial of Service (DoS). LOT tunnels provide several additional defenses against DoS attacks. Specifically, since packets received from LOT-protected networks cannot be spoofed, LOT gateways implement quotas, identifying and blocking packet floods from specific networks. Furthermore, a receiving LOT gateway (e.g., B) can send the quota assigned to each tunnel to the peer gateway (A), which can then enforce near-source quotas, reducing waste and congestion by filtering excessive traffic before it leaves the source network. Similarly, LOT tunnels facilitate near-source filtering, where the sending gateway discards packets based on filtering rules defined by the destination gateway. LOT gateways also implement an intergateway congestion detection mechanism, allowing sending gateways to detect when their packets get dropped before reaching the destination gateway and to perform appropriate near-source filtering to block the congesting traffic; this helps against DoS attacks on the backbone connecting the two gateways. LOT is practical: it is easy to manage (plug and play, requires no coordination between gateways), deployed incrementally at edge gateways (not at hosts and core routers), and has negligible overhead in terms of bandwidth and processing, as we validate experimentally. LOT storage requirements are also modest. © 2012 ACM.",Denial of service; IP spoofing,Computer crime; Floods; Network security; Peer to peer networks; Telecommunication systems; Transmission control protocol; Congestion detection; Denial of Service; Filtering rules; Flooding attacks; IP spoofing; Source address; Storage requirements; Tunneling protocols; Gateways (computer networks)
Towards practical identification of HF RFID devices,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872114018&doi=10.1145%2f2240276.2240278&partnerID=40&md5=4d0eb2dcc8bb49372f89d2c8093b1131,"The deployment of RFID poses a number of security and privacy threats such as cloning, unauthorized tracking, etc. Although the literature contains many investigations of these issues on the logical level, few works have explored the security implications of the physical communication layer. Recently, related studies have shown the feasibility of identifying RFID-enabled devices based on physical-layer fingerprints. In this work, we leverage on these findings and demonstrate that physical-layer identification of HF RFID devices is also practical, that is, can achieve high accuracy and stability. We propose an improved hardware setup and enhanced techniques for fingerprint extraction and matching. Our new system enables device identification with an Equal Error Rate as low as 0.005 (0.5%) on a set 50 HF RFID smart cards of the same manufacturer and type. We further investigate the fingerprint stability over an extended period of time and across different acquisition setups. In the latter case, we propose a solution based on channel equalization that preserves the fingerprint quality across setups. Our results strengthen the practical use of physical-layer identification of RFID devices in product and document anti-counterfeiting solutions. © 2012 ACM.",Identification; Physical layer; RFID; Security,Genetic engineering; Identification (control systems); Network layers; Smart cards; Channel equalization; Fingerprint extractions; Fingerprint qualities; Physical communications; Physical layers; Security; Security and privacy; Security implications; Radio frequency identification (RFID)
Corrective enforcement: A new paradigm of security policy enforcement by monitors,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878587195&doi=10.1145%2f2240276.2240281&partnerID=40&md5=4d00edf2ed36a84478c02dc4952a2537,"Runtime monitoring is an increasingly popular method to ensure the safe execution of untrusted codes. Monitors observe and transform the execution of these codes, responding when needed to correct or prevent a violation of a user-defined security policy. Prior research has shown that the set of properties monitors can enforce correlates with the latitude they are given to transform and alter the target execution. But for enforcement to be meaningful this capacity must be constrained, otherwise the monitor can enforce any property, but not necessarily in a manner that is useful or desirable. However, such constraints have not been significantly addressed in prior work. In this article, we develop a new paradigm of security policy enforcement in which the behavior of the enforcement mechanism is restricted to ensure that valid aspects present in the execution are preserved notwithstanding any transformation it may perform. These restrictions capture the desired behavior of valid executions of the program, and are stated by way of a preorder over sequences. The resulting model is closer than previous ones to what would be expected of a real-life monitor, from which we demand a minimal footprint on both valid and invalid executions. We illustrate this framework with examples of real-life security properties. Since several different enforcement alternatives of the same property are made possible by the flexibility of this type of enforcement, our study also provides metrics that allow the user to compare monitors objectively and choose the best enforcement paradigm for a given situation. © 2012 ACM.",Dynamic analysis; Monitoring; Program transformation; Runtime monitors; Security policies enforcement,Dynamic analysis; Monitoring; Security systems; Enforcement mechanisms; Program transformations; Runtime Monitoring; Runtime monitors; Security policy; Security policy enforcement; Security properties; Untrusted code; Access control
BAF and FI-BAF: Efficient and publicly verifiable cryptographic schemes for secure logging in resource-constrained systems,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878574973&doi=10.1145%2f2240276.2240280&partnerID=40&md5=8038735d91d41d607996c9155a9400d7,"Audit logs are an integral part of modern computer systems due to their forensic value. Protecting audit logs on a physically unprotected machine in hostile environments is a challenging task, especially in the presence of active adversaries. It is critical for such a systemto have forward security and append-only properties such that when an adversary compromises a logging machine, she cannot forge or selectively delete the log entries accumulated before the compromise. Existing public-key-based secure logging schemes are computationally costly. Existing symmetric secure logging schemes are not publicly verifiable and open to certain attacks. In this article, we develop a new forward-secure and aggregate signature scheme called Blind-Aggregate- Forward (BAF), which is suitable for secure logging in resource-constrained systems. BAF is the only cryptographic secure logging scheme that can produce publicly verifiable, forward-secure and aggregate signatures with low computation, key/signature storage, and signature communication overheads for the loggers, without requiring any online trusted third party support. A simple variant of BAF also allows a fine-grained verification of log entries without compromising the security or computational efficiency of BAF. We prove that our schemes are secure in Random Oracle Model (ROM). We also show that they are significantly more efficient than all the previous publicly verifiable cryptographic secure logging schemes. © 2012 ACM.",Applied cryptography; Digital signature; Forward security; Secure audit logging; Signature aggregation,Authentication; Barium compounds; Cryptography; Electronic document identification systems; Management; Network security; Aggregate signature; Applied cryptography; Communication overheads; Cryptographic schemes; Forward security; Hostile environments; Modern computer systems; Trusted third parties; Aggregates
On protection by layout randomization,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865972351&doi=10.1145%2f2240276.2240279&partnerID=40&md5=07cd0ae5a762649b88a730ea9799ee17,"Layout randomization is a powerful, popular technique for software protection. We present it and study it in programming-language terms. More specifically, we consider layout randomization as part of an implementation for a high-level programming language; the implementation translates this language to a lower-level language in which memory addresses are numbers. We analyze this implementation, by relating low-level attacks against the implementation to contexts in the high-level programming language, and by establishing full abstraction results. © 2012 ACM.",Protection; Randomization,Computer science; Safety engineering; Full abstraction; High-level programming language; Memory address; Protection; Randomization; Software protection; Random processes
Verified cryptographic implementations for TLS,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859467774&doi=10.1145%2f2133375.2133378&partnerID=40&md5=aafbf05b33e51a1e0466a705c0e3b24c,We narrow the gap between concrete implementations of cryptographic protocols and their verified models. We develop and verify a small functional implementation of the Transport Layer Security protocol (TLS 1.0). We make use of the same executable code for interoperability testing against mainstream implementations for automated symbolic cryptographic verification and automated computational cryptographic verification. We rely on a combination of recent tools and also develop a new tool for extracting computational models from executable code. We obtain strong security guarantees for TLS as used in typical deployments. © 2012 ACM 1094-9224/2012/03-ART3 $10.00.,Security; Verification,Seebeck effect; Verification; Computational model; Cryptographic implementation; Cryptographic protocols; Executable codes; Interoperability testing; Security; Transport layer security protocols; Cryptography
Efficient attributes for anonymous credentials,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859475923&doi=10.1145%2f2133375.2133379&partnerID=40&md5=53855003389c5919f96acf6da9d873c8,"We extend the Camenisch-Lysyanskaya anonymous credential system such that selective disclosure of attributes becomes highly efficient. The resulting system significantly improves upon existing approaches, which suffer from a linear number of modular exponentiations in the total number of attributes. This limitation makes them unfit for many practical applications, such as electronic identity cards. Our novel approach can incorporate a large number of binary and finite-set attributes without significant performance impact. It compresses all such attributes into a single attribute base and, thus, boosts the efficiency of all proofs of possession. The core idea is to encode discrete binary and finite-set values as prime numbers. We then use the divisibility property for efficient proofs of their presence or absence. In addition, we contribute efficient methods for conjunctions and disjunctions. The system builds on the strong RSA assumption. We demonstrate the aptness of our method in realistic application scenarios, notably electronic identity cards, and show its advantages for small devices, such as smartcards and cell phones. © 2012 ACM 1094-9224/2012/03-ART4 $10.00.",Algorithms; Performance; Security,Algorithms; Computer science; Safety engineering; Anonymous credential; Anonymous credential systems; Cell phone; Discrete binary; Electronic identity; Modular exponentiations; Performance; Performance impact; Prime number; Realistic applications; Security; Small devices; Strong RSA assumption; Smart cards
"Return-oriented programming: Systems, languages, and applications",2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859457954&doi=10.1145%2f2133375.2133377&partnerID=40&md5=cc4fc35f46e5986c06452717255b974f,"We introduce return-oriented programming, a technique by which an attacker can induce arbitrary behavior in a program whose control flow he has diverted, without injecting any code. A return-oriented program chains together short instruction sequences already present in a program's address space, each of which ends in a ""return"" instruction. Return-oriented programming defeats the WX protections recently deployed by Microsoft, Intel, and AMD; in this context, it can be seen as a generalization of traditional return-into-libc attacks. But the threat is more general. Return-oriented programming is readily exploitable on multiple architectures and systems. It also bypasses an entire category of security measures-those that seek to prevent malicious computation by preventing the execution of malicious code. To demonstrate the wide applicability of return-oriented programming, we construct a Turing-complete set of building blocks called gadgets using the standard C libraries of two very different architectures: Linux/x86 and Solaris/SPARC. To demonstrate the power of return-oriented programming, we present a high-level, general-purpose language for describing return-oriented exploits and a compiler that translates it to gadgets. © 2012 ACM 1094-9224/2012/03-ART2 $10.00.",Algorithms; Security,Algorithms; Computer systems programming; Address space; Building blockes; Control flows; Malicious codes; MicroSoft; Security; Solaris; C (programming language)
Information leaks in structured peer-to-peer anonymous communication systems,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859463301&doi=10.1145%2f2133375.2133380&partnerID=40&md5=9975c30f365992fe3c668073691483a7,"We analyze information leaks in the lookup mechanisms of structured peer-to-peer (P2P) anonymous communication systems and how these leaks can be used to compromise anonymity. We show that the techniques used to combat active attacks on the lookup mechanism dramatically increase information leaks and the efficacy of passive attacks, resulting in a tradeoff between robustness to active and passive attacks. We study this tradeoff in two P2P anonymous systems: Salsa and AP3. In both cases, we find that, by combining both passive and active attacks, anonymity can be compromised much more effectively than previously thought, rendering these systems insecure for most proposed uses. Our results hold even if security parameters are changed or other improvements to the systems are considered. Our study, therefore, shows the importance of considering these attacks in P2P anonymous communication. © 2012 ACM 1094-9224/2012/03-ART5 $10.00.",Security,Commerce; Communication systems; Active attack; Anonymous communication; Anonymous communication systems; Anonymous systems; Lookups; Passive and active attacks; Passive attacks; Security; Security parameters; Structured peer-to-peer; Techniques used; Peer to peer networks
Guest editorial: Special issue on computer and communications security,2012,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859465084&doi=10.1145%2f2133375.2133376&partnerID=40&md5=751cfea1e0f62df383dd6514d9d398a6,[No abstract available],,
Combining discretionary policy with mandatory information flow in operating systems,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855251460&doi=10.1145%2f2043621.2043624&partnerID=40&md5=4e6ea72d3d294bdfb5428ba2e90ed276,"Discretionary Access Control (DAC) is the primary access control mechanism in today's major operating systems. It is, however, vulnerable to Trojan Horse attacks and attacks exploiting buggy software. We propose to combine the discretionary policy in DAC with the dynamic information flow techniques in MAC, therefore achieving the best of both worlds, that is, the DAC's easy-to-use discretionary policy specification and MAC's defense against threats caused by Trojan Horses and buggy programs. We propose the Information Flow Enhanced Discretionary Access Control (IFEDAC) model that implements this design philosophy. We describe our design of IFEDAC, and discuss its relationship with the Usable Mandatory Integrity Protection (UMIP) model proposed earlier by us. In addition, we analyze their security property and their relationships with other protection systems. We also describe our implementations of IFEDAC in Linux and the evaluation results and deployment experiences of the systems. © 2011 ACM.",Discretionary access control; Information flow; Mandatory access control; Operating system,Computer operating systems; Access control mechanism; Design philosophy; Discretionary access control; Dynamic information; Evaluation results; Information flows; Integrity protection; Mandatory access control; Policy specification; Protection systems; Security properties; Trojan horse; Trojan Horse attacks; Access control
Group-centric secure information-sharing models for isolated groups,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855243142&doi=10.1145%2f2043621.2043623&partnerID=40&md5=e3c1eada589e585cdbcdff4f26625ac2,"Group-Centric Secure Information Sharing (g-SIS) envisions bringing users and objects together in a group to facilitate agile sharing of information brought in from external sources as well as creation of new information within the group. We expect g-SIS to be orthogonal and complementary to authorization systems deployed within participating organizations. The metaphors ""secure meeting room"" and ""subscription service"" characterize the g-SIS approach. The focus of this article is on developing the foundations of isolated g-SIS models. Groups are isolated in the sense that membership of a user or an object in a group does not affect their authorizations in other groups. Present contributions include the following: formal specification of core properties that at once help to characterize the family of g-SIS models and provide a ""sanity check"" for full policy specifications; informal discussion of policy design decisions that differentiate g-SIS policies from one another with respect to the authorization semantics of group operations; formalization and verification of a specific member of the family of g-SIS models; demonstration that the core properties are logically consistent and mutually independent; and identification of several directions for future extensions. The formalized specification is highly abstract. Besides certain well-formedness requirements that specify, for instance, a user cannot leave a group unless she is a member, it constrains only whether user-level read and write operations are authorized and it does so solely in terms of the history of group operations; join and leave for users and add, create, and remove for objects. This makes temporal logic one of the few formalisms in which the specification can be clearly and concisely expressed. The specification serves as a reference point that is the first step in deriving authorization-system component specifications from which a programmer with little security expertise could implement a high-assurance enforcement system for the specified policy. © 2011 ACM.",Access control; Groups; Information sharing; Linear temporal logic; Security properties,Access control; Information management; Information retrieval; Semantics; Specifications; Temporal logic; Authorization systems; Component specification; External sources; Formal Specification; Group operations; Groups; Information sharing; Information-sharing model; Linear temporal logic; Mutually independents; Policy design; Policy specification; Reference points; Sanity check; Security properties; Subscription services; Write operations; Information dissemination
The Frog-Boiling attack: Limitations of secure network coordinate systems,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855230119&doi=10.1145%2f2043621.2043627&partnerID=40&md5=906b80b489e9e56443213e039a7a9821,"A network coordinate system assigns Euclidean ""virtual"" coordinates to every node in a network to allow easy estimation of network latency between pairs of nodes that have never contacted each other. These systems have been implemented in a variety of applications, most notably the popular Vuze BitTorrent client. Zage and Nita-Rotaru (at CCS 2007) and independently, Kaafar et al. (at SIGCOMM 2007), demonstrated that several widely-cited network coordinate systems are prone to simple attacks, and proposed mechanisms to defeat these attacks using outlier detection to filter out adversarial inputs. Kaafar et al. goes a step further and requires that a fraction of the network is trusted. More recently, Sherr et al. (at USENIX ATC 2009) proposed Veracity, a distributed reputation system to secure network coordinate systems. We describe a new attack on network coordinate systems, Frog-Boiling, that defeats all of these defenses. Thus, even a system with trusted entities is still vulnerable to attacks. Moreover, having witnesses vouch for your coordinates as in Veracity does not prevent our attack. Finally, we demonstrate empirically that the Frog-Boiling attack is more disruptive than the previously known attacks: systems that attempt to reject ""bad"" inputs by statistical means or reputation cannot be used to secure a network coordinate system. © 2011 ACM.",Attack; Network coordinate; Secure,Phase transitions; Attack; BitTorrent; Distributed reputation systems; Euclidean; Network coordinate; Network coordinates; Network latencies; Outlier Detection; Secure; Secure networks; Statistical mean; Computer crime
"Access control policy translation, verification, and minimization within heterogeneous data federations",2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855252905&doi=10.1145%2f2043621.2043625&partnerID=40&md5=3f5a65c29dd8d1624fa0dbc89465243d,"Data federations provide seamless access to multiple heterogeneous and autonomous data sources pertaining to a large organization. As each source database defines its own access control policies for a set of local identities, enforcing such policies across the federation becomes a challenge. In this article, we first consider the problem of translating existing access control policies defined over source databases in a manner that allows the original semantics to be observed while becoming applicable across the entire data federation. We show that such a translation is always possible, and provide an algorithm for automating the translation. We show that verifying whether a translated policy obeys the semantics of the original access control policy defined over a source database is intractable, even under restrictive scenarios. We then describe a practical algorithmic framework for translating relational access control policies into their XML equivalent, expressed in the eXtensible Access Control Markup Language. Finally, we examine the difficulty of minimizing translated policies, and contribute a minimization algorithm applicable to nonrecursive translated policies. © 2011 ACM.",Access control; Relational databases; XACML; XML,Algorithmic languages; Algorithms; Database systems; Hypertext systems; Semantics; XML; Access control policies; Algorithmic framework; Autonomous data sources; Data federation; Extensible access control markup languages; Heterogeneous data; Large organizations; Minimization algorithms; Relational Database; XACML; Access control
Private and continual release of statistics,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855245645&doi=10.1145%2f2043621.2043626&partnerID=40&md5=5de7f0f626b93e1aa7c69b4007aa8be7,"We ask the question: how can Web sites and data aggregators continually release updated statistics, and meanwhile preserve each individual user's privacy? Suppose we are given a stream of 0's and 1's. We propose a differentially private continual counter that outputs at every time step the approximate number of 1's seen thus far. Our counter construction has error that is only poly-log in the number of time steps. We can extend the basic counter construction to allow Web sites to continually give top-k and hot items suggestions while preserving users' privacy. © 2011 ACM.",Continual mechanism; Differential privacy; Streaming algorithm,Computer science; Differential privacies; Streaming algorithm; Time step; Safety engineering
Guest editorial SACMAT 2009 and 2010,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855235604&doi=10.1145%2f2043621.2043622&partnerID=40&md5=e74269d5f643c0b41038380846b37f47,[No abstract available],,
"Empowering end users to confine their own applications: The results of a usability study comparing SELinux, AppArmor, and FBAC-LSM",2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855169730&doi=10.1145%2f2019599.2019604&partnerID=40&md5=7f65357224e7ce2de32a82b7f5f8aba0,"Protecting end users from security threats is an extremely difficult, but increasingly critical, problem. Traditional security models that focused on separating users from each other have proven ineffective in an environment of widespread software vulnerabilities and rampant malware. However, alternative approaches that provide more finely grained security generally require greater expertise than typical end users can reasonably be expected to have, and consequently have had limited success. The functionality-based application confinement (FBAC) model is designed to allow end users with limited expertise to assign applications hierarchical and parameterised policy abstractions based upon the functionalities each program is intended to perform. To validate the feasibility of this approach and assess the usability of existing mechanisms, a usability study was conducted comparing an implementation of the FBAC model with the widely used Linux-based SELinux and AppArmor security schemes. The results showed that the functionality-based mechanism enabled end users to effectively control the privileges of their applications with far greater success than widely used alternatives. In particular, policies created using FBAC were more likely to be enforced and exhibited significantly lower risk exposure, while not interfering with the ability of the application to perform its intended task. In addition to the success of the functionality-based approach, the usability study also highlighted a number of limitations and problems with existing mechanisms. These results indicate that a functionality-based approach has significant potential in terms of enabling end users with limited expertise to defend themselves against insecure and malicious software. © 2011 ACM.",AppArmor; Application-oriented access controls; FBAC-LSM; Functionality-based application confinement; HCISec; POLA; Sandboxing; SELinux; Usability,Access control; Computer crime; Computer operating systems; Risk management; AppArmor; Application-oriented access control; FBAC-LSM; Functionality-based application confinements; HCISec; POLA; Sandboxing; SELinux; Usability; Usability engineering
Formal reasoning about physical properties of security protocols,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855183282&doi=10.1145%2f2019599.2019601&partnerID=40&md5=57ff1b6874ddfb893e248aca0bdc0c1b,"Traditional security protocols are mainly concerned with authentication and key establishment and rely on predistributed keys and properties of cryptographic operators. In contrast, new application areas are emerging that establish and rely on properties of the physical world. Examples include protocols for secure localization, distance bounding, and secure time synchronization. We present a formal model for modeling and reasoning about such physical security protocols. Our model extends standard, inductive, trace-based, symbolic approaches with a formalization of physical properties of the environment, namely communication, location, and time. In particular, communication is subject to physical constraints, for example, message transmission takes time determined by the communication medium used and the distance between nodes. All agents, including intruders, are subject to these constraints and this results in a distributed intruder with restricted, but more realistic, communication capabilities than those of the standard Dolev-Yao intruder. We have formalized our model in Isabelle/HOL and have used it to verify protocols for authenticated ranging, distance bounding, broadcast authentication based on delayed key disclosure, and time synchronization. © 2011 ACM.",Formal models; Interactive theorem proving; Protocol verification; Wireless networks,Authentication; Communication; Models; Network protocols; Physical properties; Theorem proving; Wireless networks; Broadcast authentication; Communication capabilities; Communication medium; Distance bounding; Dolev-Yao intruders; Formal model; Formal reasoning; Interactive theorem proving; Isabelle/HOl; Key establishments; Message transmissions; New applications; Physical constraints; Physical security; Physical world; Protocol verification; Secure localization; Security protocols; Time synchronization; Network security
Checksum-aware fuzzing combined with dynamic taint analysis and symbolic execution,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855183288&doi=10.1145%2f2019599.2019600&partnerID=40&md5=6f21816177f63128ff00c60a7978bfef,"Fuzz testing has proven successful in finding security vulnerabilities in large programs. However, traditional fuzz testing tools have a well-known common drawback: they are ineffective if most generated inputs are rejected at the early stage of program running, especially when target programs employ checksum mechanisms to verify the integrity of inputs. This article presents TaintScope, an automatic fuzzing system using dynamic taint analysis and symbolic execution techniques, to tackle the above problem. TaintScope has several novel features: (1) TaintScope is a checksum-aware fuzzing tool. It can identify checksum fields in inputs, accurately locate checksum-based integrity checks by using branch profiling techniques, and bypass such checks via control flow alteration. Furthermore, it can fix checksum values in generated inputs using combined concrete and symbolic execution techniques. (2) TaintScope is a taint-based fuzzing tool working at the x86 binary level. Based on fine-grained dynamic taint tracing, TaintScope identifies the ""hot bytes"" in a well-formed input that are used in security-sensitive operations (e.g., invoking system/library calls), and then focuses on modifying such bytes with random or boundary values. (3) TaintScope is also a symbolicexecution-based fuzzing tool. It can symbolically evaluate a trace, reason about all possible values that can execute the trace, and then detect potential vulnerabilities on the trace. We evaluate TaintScope on a number of large real-world applications. Experimental results show that TaintScope can accurately locate the checksum checks in programs and dramatically improve the effectiveness of fuzz testing. TaintScope has already found 30 previously unknown vulnerabilities in several widely used applications, including Adobe Acrobat, Flash Player, Google Picasa, and Microsoft Paint. Most of these severe vulnerabilities have been confirmed by Secunia and oCERT, and assigned CVE identifiers (such as CVE-2009-1882, CVE-2009-2688). Vendor patches have been released or are in preparation based on our reports. © 2011 ACM.",Checksum-aware fuzzing; Symbolic execution; Taint analysis; Vulnerability detection,Software testing; Adobe Acrobat; Boundary values; Checksum; Checksum-aware fuzzing; Control flows; Fuzz Testing; Integrity check; Large programs; MicroSoft; Real-world application; Security vulnerabilities; Symbolic execution; Taint analysis; Vulnerability detection; Dynamic analysis
CANTINA+: A feature-rich machine learning framework for detecting phishing web sites,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855218311&doi=10.1145%2f2019599.2019606&partnerID=40&md5=6894d1524b575bcd7373503a01380e4e,"Phishing is a plague in cyberspace. Typically, phish detection methods either use human-verified URL blacklists or exploit Web page features via machine learning techniques. However, the former is frail in terms of new phish, and the latter suffers from the scarcity of effective features and the high false positive rate (FP). To alleviate those problems, we propose a layered anti-phishing solution that aims at (1) exploiting the expressiveness of a rich set of features with machine learning to achieve a high true positive rate (TP) on novel phish, and (2) limiting the FP to a low level via filtering algorithms. Specifically, we proposed CANTINA+, the most comprehensive feature-based approach in the literature including eight novel features, which exploits the HTML Document Object Model (DOM), search engines and third party services with machine learning techniques to detect phish. Moreover, we designed two filters to help reduce FP and achieve runtime speedup. The first is a near-duplicate phish detector that uses hashing to catch highly similar phish. The second is a login form filter, which directly classifies Web pages with no identified login form as legitimate. We extensively evaluated CANTINA+ with two methods on a diverse spectrum of corpora with 8118 phish and 4883 legitimate Web pages. In the randomized evaluation, CANTINA+ achieved over 92% TP on unique testing phish and over 99% TP on near-duplicate testing phish, and about 0.4% FP with 10% training phish. In the time-based evaluation, CANTINA+ also achieved over 92% TP on unique testing phish, over 99% TP on near-duplicate testing phish, and about 1.4% FP under 20% training phish with a two-week sliding window. Capable of achieving 0.4% FP and over 92% TP, our CANTINA+ has been demonstrated to be a competitive anti-phishing solution. © 2011 ACM.",Anti-phishing; Information retrieval; Machine learning,Computer crime; Filtration; Learning algorithms; Learning systems; Search engines; Websites; Anti-phishing; Cyberspaces; Detection methods; False positive rates; Feature-based; Filtering algorithm; HTML documents; Low level; Machine learning techniques; Machine-learning; Phishing; Runtimes; Sliding Window; Third party services; True positive rates; Web page; Information retrieval
Authenticated dictionaries: Real-world costs and trade-offs,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855227245&doi=10.1145%2f2019599.2019602&partnerID=40&md5=d60b8f61525a41f084a40d150d560756,"Authenticated dictionaries are a widely discussed paradigm to enable verifiable integrity for data storage on untrusted servers, such as today's widely used ""cloud computing"" resources, allowing a server to provide a ""proof,"" typically in the form of a slice through a cryptographic data structure, that the results of any given query are the correct answer, including that the absence of a query result is correct. Persistent authenticated dictionaries (PADs) further allow queries against older versions of the structure. This research presents implementations of a variety of different PAD algorithms, some based on Merkle tree-style data structures and others based on individually signed ""tuple"" statements (with and without RSA accumulators). We present system throughput benchmarks, indicating costs in terms of time, storage, and bandwidth as well as considering how much money would be required given standard cloud computing costs. We conclude that Merkle tree PADs are preferable in cases with frequent updates, while tuple-based PADs are preferable with higher query rates. For Merkle tree PADs, red-black trees outperform treaps and skiplists. Applying Sarnak-Tarjan's versioned node strategy, with a cache of old hashes at every node, to red-black trees yields the fastest Merkle tree PAD implementation, notably using half the memory of the more commonly used mutation-free path copying strategy. For tuple PADs, although we designed and implemented an algorithm using RSA accumulators that offers constant update size, constant storage per update, constant proof size, and sublinear computation per update, we found that RSA accumulators are so expensive that they are never worthwhile. We find that other optimizations in the literature for tuple PADs are more cost-effective. © 2011 ACM.",Authenticated dictionaries; Persistent data structures; Tamper evident data structures,Algorithms; Computation; Costs; Forestry; Mathematics; Trees; Algorithms; Cloud computing; Costs; Data structures; Forestry; Query processing; Data storage; Merkle trees; Query results; Red black tree; Sublinear; System throughput; Tamper-evident; Untrusted server; Trees (mathematics)
Practical Oblivious outsourced storage,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855227236&doi=10.1145%2f2019599.2019605&partnerID=40&md5=880d83578386117512d0264ff9623e91,"In this article we introduce a technique, guaranteeing access pattern privacy against a computationally bounded adversary, in outsourced data storage, with communication and computation overheads orders of magnitude better than existing approaches. In the presence of a small amount of temporary storage (enough to store O(√n log n) items and IDs, where n is the number of items in the database), we can achieve access pattern privacy with computational complexity of less than O(log2 n) per query (as compared to, for instance, O(log4 n) for existing approaches). We achieve these novel results by applying new insights based on probabilistic analyses of data shuffling algorithms to Oblivious RAM, allowing us to significantly improve its asymptotic complexity. This results in a protocol crossing the boundary between theory and practice and becoming generally applicable for access pattern privacy. We show that on off-the-shelf hardware, large data sets can be queried obliviously orders of magnitude faster than in existing work. © 2011 ACM.",Access privacy; Integrity; Oblivious RAM; Private information retrieval; Protection,Computational complexity; Information dissemination; Information retrieval; Access patterns; Asymptotic complexity; Computation overheads; Data storage; Integrity; Large datasets; Off-the-shelf hardwares; Orders of magnitude; Private information retrieval; Probabilistic analysis; Protection; Shuffling algorithm; Temporary storage; Theory and practice; Random access storage
Security seals on voting machines: A case study,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855227239&doi=10.1145%2f2019599.2019603&partnerID=40&md5=f0d19d90e0d710ddd9fe6dfba735c6c7,"Tamper-evident seals are used by many states' election officials on voting machines and ballot boxes, either to protect the computer and software from fraudulent modification or to protect paper ballots from fraudulent substitution or stuffing. Physical tamper-indicating seals can usually be easily defeated, given they way they are typically made and used; and the effectiveness of seals depends on the protocol for their application and inspection. The legitimacy of our elections may therefore depend on whether a particular state's use of seals is effective to prevent, deter, or detect election fraud. This paper is a case study of the use of seals on voting machines by the State of New Jersey. I conclude that New Jersey's protocols for the use of tamper-evident seals have been not at all effective. I conclude with a discussion of the more general problem of seals in democratic elections. © 2011 ACM.",,Voting machines; New Jersey; Security seals; Tamper-evident seals; Computer crime
Relations among privacy notions,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959853254&doi=10.1145%2f1952982.1952986&partnerID=40&md5=154672c28c07e452a20dd9136a5ccd2b,"This article presents a hierarchy of privacy notions that covers multiple anonymity and unlinkability variants. The underlying definitions, which are based on the idea of indistinguishability between two worlds, provide new insights into the relation between, and the fundamental structure of, different privacy notions. We furthermore place previous privacy definitions concerning group signature, anonymous communication, and secret voting systems in the context of our hierarchy; this renders these traditionally disconnected notions comparable. © 2011.",Adversarial model; Anonymity; Privacy notions; Unlinkability,Adversarial model; Anonymity; Anonymous communication; Fundamental structures; Group signatures; Indistinguishability; Unlinkability; Voting systems; Voting machines
"Access control via belnap logic: Intuitive, expressive, and analyzable policy composition",2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959831193&doi=10.1145%2f1952982.1952991&partnerID=40&md5=5c5ae88079382153313e09de5c40d54d,"Access control to IT systems increasingly relies on the ability to compose policies. Hence there is benefit in any framework for policy composition that is intuitive, formal (and so ""analyzable"" and ""implementable"") , expressive, independent of specific application domains, and yet able to be extended to create domain-specific instances. Here we develop such a framework based on Belnap logic. An access-control policy is interpreted as a four-valued predicate that maps access requests to either grant, deny, conflict, or unspecified - the four values of the Belnap bilattice. We define an expressive access-control policy language PBel, having composition operators based on the operators of Belnap logic. Natural orderings on policies are obtained by lifting the truth and information orderings of the Belnap bilattice. These orderings lead to a query language in which policy analyses, for example, conflict freedom, can be specified. Policy analysis is supported through a reduction of the validity of policy queries to the validity of propositional formulas on predicates over access requests. We evaluate our approach through firewall policy and RBAC policy examples, and discuss domain-specific and generic extensions of our policy language. © 2011.",Access-control policy languages; Bilattices; Multivalued logic,Query languages; Security systems; Transistor transistor logic circuits; Application domains; Bilattices; Composition operators; Conflict-freedom; Domain specific; Firewall policies; Generic extensions; IT system; Multivalued logic; Policy analysis; Policy language; Propositional formulas; RBAC policy; Access control
Practical defenses against pollution attacks in wireless network coding,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959817037&doi=10.1145%2f1952982.1952989&partnerID=40&md5=bf7ccc2343d915eafe4798357536ad64,"Recent studies have shown that network coding can provide significant benefits to network protocols, such as increased throughput, reduced network congestion, higher reliability, and lower power consumption. The core principle of network coding is that intermediate nodes actively mix input packets to produce output packets. This mixing subjects network coding systems to a severe security threat, known as a pollution attack, where attacker nodes inject corrupted packets into the network. Corrupted packets propagate in an epidemic manner, depleting network resources and significantly decreasing throughput. Pollution attacks are particularly dangerous in wireless networks, where attackers can easily inject packets or compromise devices due to the increased network vulnerability. In this article, we address pollution attacks against network coding systems in wireless mesh networks. We demonstrate that previous solutions are impractical in wireless networks, incurring an unacceptable high degradation of throughput. We propose a lightweight scheme, DART, that uses time-based authentication in combination with random linear transformations to defend against pollution attacks. We further improve system performance and propose EDART, which enhances DART with an optimistic forwarding scheme. We also propose efficient attacker identification schemes for both DART and EDART that enable quick attacker isolation and the selection of attacker-free paths, achieving additional performance improvement. A detailed security analysis shows that the probability of a polluted packet passing our verification procedure is very low (less than 0.002% in typical settings). Performance results using the well-known MORE protocol and realistic link quality measurements from the Roofnet experimental testbed show that our schemes improve system performance over 20 times compared with previous solutions. © 2011.",Network coding; Network coding security; Pollution attacks; Security; Wireless network security,Computer crime; Information theory; Linear transformations; MESH networking; Network protocols; Network security; Pollution; Security systems; Telecommunication links; Throughput; Wireless networks; Attacker identification; Experimental testbed; Intermediate node; Link quality; Lower-power consumption; Network congestions; Network resource; Network vulnerability; Performance improvements; Pollution attack; Pollution attacks; Security; Security analysis; Security threats; Network coding
Robust and efficient authentication of video stream broadcasting,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959819616&doi=10.1145%2f1952982.1952987&partnerID=40&md5=842dfc2dd4f6fb627cf90fbf0adf12c2,"We present a novel video stream authentication scheme which combines signature amortization by means of hash chains and an advanced watermarking technique. We propose a new hash chain construction, the Duplex Hash Chain, which allows us to achieve bit-by-bit authentication that is robust to low bit error rates. This construction is well suited for wireless broadcast communications characterized by low packet losses such as in satellite networks. Moreover, neither hardware upgrades nor specific end-user equipment are needed to enjoy the authentication services. The computation overhead experienced on the receiver only sums to two hashes per block of pictures and one digital signature verification for the whole received stream. This overhead introduces a provably negligible decrease in video quality. A thorough analysis of the proposed solution is provided in conjunction with extensive simulations. © 2011.",Video streaming; Watermarking,Depreciation; Multicasting; Video streaming; Videotex; Authentication services; Computation overheads; Digital Signature; Efficient authentication; End-user equipments; Extensive simulations; Hash chain construction; Hash chains; Satellite network; Signature amortization; Video quality; Video streams; Watermarking techniques; Wireless broadcast; Authentication
Detecting and resolving policy misconfigurations in access-control systems,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959832940&doi=10.1145%2f1952982.1952984&partnerID=40&md5=d25572f1606389703ec97912a562858d,"Access-control policy misconfigurations that cause requests to be erroneously denied can result in wasted time, user frustration, and, in the context of particular applications (e.g., health care), very severe consequences. In this article we apply association rule mining to the history of accesses to predict changes to access-control policies that are likely to be consistent with users' intentions, so that these changes can be instituted in advance of misconfigurations interfering with legitimate accesses. Instituting these changes requires the consent of the appropriate administrator, of course, and so a primary contribution of our work is how to automatically determine from whom to seek consent and how to minimize the costs of doing so. We show using data from a deployed access-control system that our methods can reduce the number of accesses that would have incurred costly time-of-access delays by 43%, and can correctly predict 58% of the intended policy. These gains are achieved without impacting the total amount of time users spend interacting with the system. © 2011.",Access control; Machine learning; Policy inference,Control systems; Health care; Association rule mining; Machine-learning; Misconfigurations; Primary contribution; Access control
Nexus authorization logic (NAL): Design rationale and applications,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959832939&doi=10.1145%2f1952982.1952990&partnerID=40&md5=c02fec3f31b255999a50c05c4e8ab879,"Nexus Authorization Logic (NAL) provides a principled basis for specifying and reasoning about credentials and authorization policies. It extends prior access control logics that are based on ""says"" and ""speaks for"" operators. NAL enables authorization of access requests to depend on (i) the source or pedigree of the requester, (ii) the outcome of any mechanized analysis of the requester, or (iii) the use of trusted software to encapsulate or modify the requester. To illustrate the convenience and expressive power of this approach to authorization, a suite of document-viewer applications was implemented to run on the Nexus operating system. One of the viewers enforces policies that concern the integrity of excerpts that a document contains; another viewer enforces confidentiality policies specified by labels tagging blocks of text. © 2011.",Authorization logic; CDD; Credentials-based authorization,Access control logic; Authorization logic; Authorization policy; CDD; Credentials-based authorization; Design rationale; Expressive power; Access control
Introduction to special section SACMAT'08,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959812490&doi=10.1145%2f1952982.1952983&partnerID=40&md5=03d2aeb3129921e13a7531fc4556ed74,[No abstract available],,
False data injection attacks against state estimation in electric power grids,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959856672&doi=10.1145%2f1952982.1952995&partnerID=40&md5=ad414b85e3cbaea2e87d42fa740d7342,"A power grid is a complex system connecting electric power generators to consumers through power transmission and distribution networks across a large geographical area. System monitoring is necessary to ensure the reliable operation of power grids, and state estimation is used in system monitoring to best estimate the power grid state through analysis of meter measurements and power system models. Various techniques have been developed to detect and identify bad measurements, including interacting bad measurements introduced by arbitrary, nonrandom causes. At first glance, it seems that these techniques can also defeat malicious measurements injected by attackers. In this article, we expose an unknown vulnerability of existing bad measurement detection algorithms by presenting and analyzing a new class of attacks, called false data injection attacks, against state estimation in electric power grids. Under the assumption that the attacker can access the current power system configuration information and manipulate the measurements of meters at physically protected locations such as substations, such attacks can introduce arbitrary errors into certain state variables without being detected by existing algorithms. Moreover, we look at two scenarios, where the attacker is either constrained to specific meters or limited in the resources required to compromise meters. We show that the attacker can systematically and efficiently construct attack vectors in both scenarios to change the results of state estimation in arbitrary ways. We also extend these attacks to generalized false data injection attacks, which can further increase the impact by exploiting measurement errors typically tolerated in state estimation. We demonstrate the success of these attacks through simulation using IEEE test systems, and also discuss the practicality of these attacks and the real-world constraints that limit their effectiveness. © 2011.",Attack; Power grids; State estimation,Algorithms; Electric power distribution; Electric power transmission networks; Electricity; Estimation; Measurement errors; Monitoring; Power transmission; State estimation; Attack; Attack vector; Best estimates; Current power; Detection algorithm; Electric power generators; Electric power grids; False data injection attacks; Geographical area; IEEE test systems; Power grids; Power system model; Power transmission and distributions; Reliable operation; State variables; System monitoring; Security of data
Access controls for oblivious and anonymous systems,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959823641&doi=10.1145%2f1952982.1952992&partnerID=40&md5=408f7e65a0612eab26734932964fe8f6,"The use of privacy-enhancing cryptographic protocols, such as anonymous credentials and oblivious transfer, could have a detrimental effect on the ability of providers to effectively implement access controls on their content. In this article, we propose a stateful anonymous credential system that allows the provider to implement nontrivial, real-world access controls on oblivious protocols conducted with anonymous users. Our system models the behavior of users as a state machine and embeds that state within an anonymous credential to restrict access to resources based on the state information. The use of state machine models of user behavior allows the provider to restrict the users' actions according to a wide variety of access control models without learning anything about the users' identities or actions. Our system is secure in the standard model under basic assumptions and, after an initial setup phase, each transaction requires only constant time. As a concrete example, we show how to implement the Brewer-Nash (Chinese Wall) and Bell-La Padula (Multilevel Security) access control models within our credential system. Furthermore, we combine our credential system with an adaptive oblivious transfer scheme to create a privacy-friendly oblivious database with strong access controls. © 2011.",Access controls; Anonymous credentials; Oblivious transfer,Adaptive control systems; Behavioral research; Mathematical models; Security systems; Access control models; Adaptive oblivious transfer; Anonymous credential; Anonymous credential systems; Anonymous systems; Constant time; Credential systems; Cryptographic protocols; Detrimental effects; Multi-level security; Oblivious transfer; Set-up phase; State information; State machine; State machine models; System models; The standard model; User behaviors; Access control
Remote data checking using provable data possession,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959823910&doi=10.1145%2f1952982.1952994&partnerID=40&md5=121fcbdc2975315dcf786ae9c32a64f1,"We introduce a model for provable data possession (PDP) that can be used for remote data checking: A client that has stored data at an untrusted server can verify that the server possesses the original data without retrieving it. The model generates probabilistic proofs of possession by sampling random sets of blocks from the server, which drastically reduces I/O costs. The client maintains a constant amount of metadata to verify the proof. The challenge/response protocol transmits a small, constant amount of data, which minimizes network communication. Thus, the PDP model for remote data checking is lightweight and supports large data sets in distributed storage systems. The model is also robust in that it incorporates mechanisms for mitigating arbitrary amounts of data corruption. We present two provably-secure PDP schemes that aremore efficient than previous solutions. In particular, the overhead at the server is low (or even constant), as opposed to linear in the size of the data. We then propose a generic transformation that adds robustness to any remote data checking scheme based on spot checking. Experiments using our implementation verify the practicality of PDP and reveal that the performance of PDP is bounded by disk I/O and not by cryptographic computation. Finally, we conduct an in-depth experimental evaluation to study the tradeoffs in performance, security, and space overheads when adding robustness to a remote data checking scheme. © 2011.",Archival storage; Cloud storage security; Erasure coding; Homomorphic verifiable tags; PDP; Provable data possession; Remote data checking; Robust auditing,Model checking; Network security; Archival storage; Erasure coding; Homomorphic verifiable tags; PDP; Provable data possession; Remote data; Robust auditing; Metadata
Lightweight RFID authentication with forward and backward security,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959820353&doi=10.1145%2f1952982.1952993&partnerID=40&md5=4a59f99d327051354661cbe14fc21118,"We propose a lightweight RFID authentication protocol that supports forward and backward security. The only cryptographic mechanism that this protocol uses is a pseudorandom number generator (PRNG) that is shared with the backend Server. Authentication is achieved by exchanging a few numbers (3 or 5) drawn from the PRNG. The lookup time is constant, and the protocol can be easily adapted to prevent online man-in-the-middle relay attacks. Security is proven in the UC security framework. © 2011.",Authentication; Backward security; EPCGen2; Forward security; RFID; Universal composability,Authentication; Cryptography; Number theory; Random number generation; Back-end servers; Backward security; EPCGen2; Forward security; Lookup time; Pseudo random number generators; Relay attack; RFID authentication; UC security; Universal Composability; Radio frequency identification (RFID)
Cross-application data provenance and policy enforcement,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959838282&doi=10.1145%2f1952982.1952988&partnerID=40&md5=cbe0b4cc5f3ea1c7f1d4c622d4b59f01,"We present a new technique that can trace data provenance and enforce data access policies across multiple applications and machines. We have developed Garm, a tool that uses binary rewriting to implement this technique on arbitrary binaries. Users can use Garm to attach access policies to data and Garm enforces the policy on all accesses to the data (and any derived data) across all applications and executions. Garm uses static analysis to generate optimized instrumentation that traces the provenance of an application's state and the policies that apply to this state. Garm monitors the interactions of the application with the underlying operating system to enforce policies. Conceptually, Garm combines trusted computing support from the underlying operating system with a stream cipher to ensure that data protected by an access policy cannot be accessed outside of Garm's policy enforcement mechanisms. We have evaluated Garm with several common Linux applications.We found that Garm can successfully trace the provenance of data across executions of multiple applications and enforce data access policies on the application's executions. © 2011.",Binary rewriting; Policy enforcement; Provenance,Access policies; Binary rewriting; Data access; Data provenance; Derived data; Multiple applications; Policy enforcement; Provenance; Stream Ciphers; Trace data; Trusted computing; Computer operating systems
Practical and efficient cryptographic enforcement of interval-based access control policies,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959820119&doi=10.1145%2f1952982.1952996&partnerID=40&md5=5be2815713e04473d5a2fc1c7c4ae315,"The enforcement of access control policies using cryptography has received considerable attention in recent years and the security of such enforcement schemes is increasingly well understood. Recent work in the area has considered the efficient enforcement of temporal and geo-spatial access control policies, and asymptotic results for the time and space complexity of efficient enforcement schemes have been obtained. However, for practical purposes, it is useful to have explicit bounds for the complexity of enforcement schemes. In this article we consider interval-based access control policies, of which temporal and geo-spatial access control policies are special cases.We define enforcement schemes for interval-based access control policies for which it is possible, in almost all cases, to obtain exact values for the schemes' complexity, thereby subsuming a substantial body of work in the literature. Moreover, our enforcement schemes are more practical than existing schemes, in the sense that they operate in the same way as standard cryptographic enforcement schemes, unlike other efficient schemes in the literature. The main difference between our approach and earlier work is that we develop techniques that are specific to the cryptographic enforcement of intervalbased access control policies, rather than applying generic techniques that give rise to complex constructions and asymptotic bounds. © 2011.",Cryptographic enforcement; Geo-spatial access control; Interval-based access control; Temporal access control,Asymptotic analysis; Cryptography; Security systems; Access control policies; Asymptotic bounds; Complex construction; Cryptographic enforcement; Explicit bounds; Geo-spatial; Interval-based access control; Space complexity; Temporal access control; Access control
Authorization recycling in hierarchical RBAC systems,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959816765&doi=10.1145%2f1952982.1952985&partnerID=40&md5=27d85cc14bef1892e9b93b6808fa277d,"As distributed applications increase in size and complexity, traditional authorization architectures based on a dedicated authorization server become increasingly fragile because this decision point represents a single point of failure and a performance bottleneck. Authorization caching, which enables the reuse of previous authorization decisions, is one technique that has been used to address these challenges. This article introduces and evaluates the mechanisms for authorization ""recycling"" in RBAC enterprise systems. The algorithms that support these mechanisms allow making precise and approximate authorization decisions, thereby masking possible failures of the authorization server and reducing its load. We evaluate these algorithms analytically as well as using simulation and a prototype implementation. Our evaluation results demonstrate that authorization recycling can improve the performance of distributed-access control mechanisms. © 2011.",Access control; Authorization recycling; RBAC; SAAM,Access control; Algorithms; Hierarchical systems; Security systems; Authorization architecture; Authorization decision; Authorization recycling; Control mechanism; Decision points; Distributed applications; Enterprise system; Evaluation results; Performance bottlenecks; Prototype implementations; RBAC; SAAM; Single point; Recycling
Modeling Key Compromise Impersonation Attacks on Group Key Exchange Protocols,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864819691&doi=10.1145%2f2043628.2043629&partnerID=40&md5=866578c9620d7e1766cee9cdeb2c18b4,"Two-party key exchange (2PKE) protocols have been rigorously analyzed under various models considering different adversarial actions. However, the analysis of group key exchange (GKE) protocols has not been as extensive as that of 2PKE protocols. Particularly, an important security attribute called key compromise impersonation (KCI) resilience has been completely ignored for the case of GKE protocols. Informally, a protocol is said to provideKCI resilience if the compromise of the long-term secret key of a protocol participant A does not allow the adversary to impersonate an honest participant B to A. In this paper, we argue that KCI resilience for GKE protocols is at least as important as it is for 2PKE protocols. Our first contribution is revised definitions of security for GKE protocols considering KCI attacks by both outsider and insider adversaries. We also give a new proof of security for an existing two-round GKE protocol under the revised security definitions assuming random oracles. We then show how to achieve insider KCIR in a generic way using a known compiler in the literature. As one may expect, this additional security assurance comes at the cost of an extra round of communication. Finally, we show that a few existing protocols are not secure against outsider KCI attacks. The attacks on these protocols illustrate the necessity of considering KCI resilience for GKE protocols. © 2011, ACM. All rights reserved.",Algorithms; Design; Group key exchange; insider attacks; key compromise impersonation; Security; Theory,
PEREA: Practical TTP-free revocation of repeatedly misbehaving anonymous users,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865584730&doi=10.1145%2f2043628.2043630&partnerID=40&md5=cf37130e2eeeeb008f81344f7091a4f5,"Several anonymous authentication schemes allow servers to revoke a misbehaving user's future accesses. Traditionally, these schemes have relied on powerful Trusted Third Parties (TTPs) capable of deanonymizing (or linking) users' connections. Such TTPs are undesirable because users' anonymity is not guaranteed, and users must trust them to judge 'misbehavior' fairly. Recent schemes such as Blacklistable Anonymous Credentials (BLAC) and Enhanced Privacy ID (EPID) support ""privacy-enhanced revocation"" - servers can revoke misbehaving users without a TTP's involvement, and without learning the revoked users' identities. In BLAC and EPID, however, the computation required for authentication at the server is linear in the size (L) of the revocation list, which is impractical as the size approaches thousands of entries. We propose PEREA, a new anonymous authentication scheme for which this bottleneck of computation is independent of the size of the revocation list. Instead, the time complexity of authentication is linear in the size of a revocation window K 蠐 L, the number of subsequent authentications before which a user's misbehavior must be recognized if the user is to be revoked. We extend PEREA to support more complex revocation policies that take the severity of misbehaviors into account. Users can authenticate anonymously if their naughtiness, i.e., the sum of the severities of their blacklisted misbehaviors, is below a certain naughtiness threshold. We call our extension PEREA-Naughtiness. We prove the security of our constructions, and validate their efficiency as compared to BLAC both analytically and quantitatively. © ACM 2011.",,Anonymous authentication; Anonymous credential; Time complexity; Trusted third parties; Authentication
On Two Rfid Privacy Notions and Their Relations,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883377411&doi=10.1145%2f2043628.2043631&partnerID=40&md5=e3f484f24b8bb894f4fd3acdcd5bb6c2,"Privacy of RFID systems is receiving increasing attention in the RFID community. Basically, there are two kinds of RFID privacy notions in the literature: one based on the indistinguishability of two tags, denoted as ind-privacy, and the other based on the unpredictability of the output of an RFID protocol, denoted as unpprivacy. In this article, we first revisit the existing unpredictability-based RFID privacy models and point out their limitations. We then propose a new RFID privacy model, denoted as unpublic-privacy, based on the indistinguishability of a real tag and a virtual tag. We formally clarify its relationship with the ind-privacy model. It is proven that ind-privacy is weaker than unp*-privacy. Moreover, the minimal (necessary and sufficient) condition on RFID tags to achieve unp*-privacy is determined. It is shown that if an RFID system is unp*-private, then the computational power of an RFID tag can be used to construct a pseudorandom function family provided that the RFID system is complete and sound. On the other hand, if each tag is able to compute a pseudorandom function, then the tags can be used to construct an RFID system with unp*-privacy. In this sense, a pseudorandom function family is the minimal requirement on an RFID tag's computational power for enforcing RFID system privacy. Finally, a new RFID mutual authentication protocol is proposed to satisfy the minimal requirement. © 2011, ACM. All rights reserved.",Design; privacy; pseudorandom function; RFID; Security,
Privacy-Preserving Distributed Network Troubleshooting—Bridging the Gap Between Theory and Practice,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893345636&doi=10.1145%2f2043628.2043632&partnerID=40&md5=10d2f5a6fbab4dee7ef641f03993e900,"Today, there is a fundamental imbalance in cybersecurity. While attackers act more andmore globally and coordinated, network defense is limited to examine local information only due to privacy concerns. To overcome this privacy barrier, we use secure multiparty computation (MPC) for the problem of aggregating network data from multiple domains. We first optimize MPC comparison operations for processing high volume data in near real-time by not enforcing protocols to run in a constant number of synchronization rounds. We then implement a complete set of basic MPC primitives in the SEPIA library. For parallel invocations, SEPIA's basic operations are between 35 and several hundred times faster than those of comparable MPC frameworks. Using these operations, we develop four protocols tailored for distributed network monitoring and security applications: the entropy, distinct count, event correlation, and top-k protocols. Extensive evaluation shows that the protocols are suitable for near real-time data aggregation. For example, our top-k protocol PPTKS accurately aggregates counts for 180,000 distributed IP addresses in only a few minutes. Finally, we use SEPIA with real traffic data from 17 customers of a backbone network to collaboratively detect, analyze, and mitigate distributed anomalies. Our work follows a path starting from theory, going to system design, performance evaluation, and ending with measurement. Along this way, it makes a first effort to bridge two very disparate worlds: MPC theory and network monitoring and security practices. © 2011, ACM. All rights reserved.",aggregation; Algorithms; anomaly detection; Applied cryptography; collaborative network security; Design; Experimentation; Measurement; network management; root-cause analysis; secure multiparty computation; Security,
Server-Side Verification Of Client Behavior In Online Games,2011,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024250317&doi=10.1145%2f2043628.2043633&partnerID=40&md5=936cb0f1b92eac833fbb7cc1d73469af,"Online gaming is a lucrative and growing industry but one that is slowed by cheating that compromises the gaming experience and hence drives away players (and revenue). In this paper we develop a technique by which game developers can enable game operators to validate the behavior of game clients as being consistent with valid execution of the sanctioned client software. Our technique employs symbolic execution of the client software to extract constraints on client-side state implied by each client-to-server message, and then uses constraint solving to determine whether the sequence of client-to-server messages can be “explained” by any possible user inputs, in light of the server-to-client messages already received. The requisite constraints and solving components can be developed either simultaneously with the game or retroactively for existing games. We demonstrate our approach in three case studies on the open-source game XPilot, a game similar to Pac-Man of our own design, and an open-source multiplayer version of Tetris. © 2011, ACM. All rights reserved.",cheat detection; Computer games; Security; Verification,
Uncovering spoken phrases in encrypted Voice over IP conversations,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651398360&doi=10.1145%2f1880022.1880029&partnerID=40&md5=a3caa6c414b05efd450e9327c2ef390e,"Although Voice over IP (VoIP) is rapidly being adopted, its security implications are not yet fully understood. Since VoIP calls may traverse untrusted networks, packets should be encrypted to ensure confidentiality. However, we show that it is possible to identify the phrases spoken within encrypted VoIP calls when the audio is encoded using variable bit rate codecs. To do so, we train a hidden Markov model using only knowledge of the phonetic pronunciations of words, such as those provided by a dictionary, and search packet sequences for instances of specified phrases. Our approach does not require examples of the speaker's voice, or even example recordings of the words that make up the target phrase. We evaluate our techniques on a standard speech recognition corpus containing over 2,000 phonetically rich phrases spoken by 630 distinct speakers from across the continental United States. Our results indicate that we can identify phrases within encrypted calls with an average accuracy of 50%, and with accuracy greater than 90% for some phrases. Clearly, such an attack calls into question the efficacy of current VoIP encryption standards. In addition, we examine the impact of various features of the underlying audio on our performance and discuss methods for mitigation. © 2010 ACM.",Network security; Traffic analysis; Voice over IP,Cryptography; Hidden Markov models; Internet telephony; Network security; Voice/data communication systems; Packet sequence; Security implications; Traffic analysis; Untrusted network; Variable bit rate; Voice over IP; Speech recognition
Pairing-based onion routing with improved forward secrecy,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651394431&doi=10.1145%2f1880022.1880023&partnerID=40&md5=ce4becb68dca9119543a61d1fb59df7d,"This article presents new protocols for onion routing anonymity networks. We define a provably secure privacy-preserving key agreement scheme in an identity-based infrastructure setting, and use it to design new onion routing circuit constructions. These constructions, based on a user's selection, offer immediate or eventual forward secrecy at each node in a circuit and require significantly less computation and communication than the telescoping mechanism used by the Tor project. Further, the use of an identity-based infrastructure also leads to a reduction in the required amount of authenticated directory information. Therefore, our constructions provide practical ways to allow onion routing anonymity networks to scale gracefully. © 2010 ACM.",Anonymous key agreement; Forward secrecy; Onion routing; Pairing-based cryptography; Tor,Cryptography; Network protocols; Forward secrecy; Key agreement; Onion routing; Pairing-based cryptography; Tor; Network security
MPSS: Mobile proactive secret sharing,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651405496&doi=10.1145%2f1880022.1880028&partnerID=40&md5=43a64c3059f979939126e9297eb95ee5,"This article describes MPSS, a new way to do proactive secret sharing. MPSS provides mobility: The group of nodes holding the shares of the secret can change at each resharing, which is essential in a long-lived system. MPSS additionally allows the number of tolerated faulty shareholders to change when the secret is moved so that the system can tolerate more (or fewer) corruptions; this allows reconfiguration on-the-fly to accommodate changes in the environment. MPSS includes an efficient protocol that is intended to be used in practice. The protocol is optimized for the common case of no or few failures, but degradation when there are more failures is modest. MPSS contains a step in which nodes accuse proposals made by other nodes; we show a novel way to handle these accusations when their verity cannot be known. We also present a way to produce accusations that can be verified without releasing keys of other nodes; verifiable accusations improve the performance of MPSS, and are a useful primitive independent of MPSS. © 2010 ACM.",Security,Long-lived systems; On-the-fly; Proactive secret sharing; Security
Attribute-Based Messaging: Access control and confidentiality,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651407156&doi=10.1145%2f1880022.1880025&partnerID=40&md5=83eb24c5fc7aecdb069e5cd058b96f8a,"Attribute-Based Messaging (ABM) enables messages to be addressed using attributes of recipients rather than an explicit list of recipients. Such messaging offers benefits of efficiency, exclusiveness, and intensionality, but faces challenges in access control and confidentiality. In this article we explore an approach to intraenterprise ABM based on providing access control and confidentiality using information from the same attribute database exploited by the addressing scheme. We show how to address three key challenges. First, we demonstrate a manageable access control system based on attributes. Second, we demonstrate use of attribute-based encryption to provide end-to-end confidentiality. Third, we show that such a system can be efficient enough to support ABM for mid-size enterprises. Our implementation can dispatch confidential ABM messages approved by XACML policy review for an enterprise of at least 60,000 users with only seconds of latency. © 2010 ACM.",Attribute-based encryption; Attributes; Messaging,Cryptography; Security systems; Addressing scheme; Attribute database; Attribute-based encryption; Attributes; Intensionality; Messaging; XACML policies; Access control
Mining roles with multiple objectives,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651401363&doi=10.1145%2f1880022.1880030&partnerID=40&md5=074a88b394b98d725ad32d65fb4e207f,"With the growing adoption of Role-Based Access Control (RBAC) in commercial security and identity management products, how to facilitate the process of migrating a non-RBAC system to an RBAC system has become a problem with significant business impact. Researchers have proposed to use data mining techniques to discover roles to complement the costly top-down approaches for RBAC system construction. An important problem is how to construct RBAC systems with low complexity. In this article, we define the notion of weighted structural complexity measure and propose a role mining algorithm that mines RBAC systems with low structural complexity. Another key problem that has not been adequately addressed by existing role mining approaches is how to discover roles with semantic meanings. In this article, we study the problem in two primary settings with different information availability. When the only information is user-permission relation, we propose to discover roles whose semantic meaning is based on formal concept lattices. We argue that the theory of formal concept analysis provides a solid theoretical foundation formining roles from a user-permission relation. When user-attribute information is also available, we propose to create roles that can be explained by expressions of user-attributes. Since an expression of attributes describes a real-world concept, the corresponding role represents a real-world concept as well. Furthermore, the algorithms we propose balance the semantic guarantee of roles with system complexity. Finally, we indicate how to create a hybrid approach combining top-down candidate roles. Our experimental results demonstrate the effectiveness of our approaches. © 2010 ACM.",RBAC; Role engineering; Role mining,Algorithms; Data mining; Formal logic; Semantics; Attribute information; Business impact; Data mining techniques; Formal Concept Analysis; Formal concept lattices; Hybrid approach; Identity management; Information availability; Key problems; Low complexity; Multiple objectives; RBAC; Real-world; Role engineering; Role minings; Role-based Access Control; Structural complexity; System complexity; System construction; Theoretical foundations; Top-down approach; Topdown; Access control
Key evolution systems in untrusted update environments,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651396152&doi=10.1145%2f1880022.1880031&partnerID=40&md5=fdc95dc7c8aa633b16741062f6ec6680,"Forward-Secure Signatures (FSS) prevent forgeries for past time periods when an attacker obtains full access to the signer's storage by evolving the private key in a one-way fashion. To simplify the integration of these primitives into standard security architectures, Boyen et al. [2006] recently introduced the concept of forward-secure signatures with untrusted updates where private keys are additionally protected by a second factor (derived from a password). Key updates can be made on encrypted version of signing keys so that passwords only come into play for signing messages and not at update time (since update is not user-driven). The scheme put forth by Boyen et al. relies on bilinear maps and does not require the random oracle. They also suggest the integration of untrusted updates in the Bellare-Miner forward-secure signature. Their work left open the problem of endowing other existing FSS systems with the same second factor protection, and a natural second question is whether the method can apply to other key-evolving paradigms. This article solves the first problem by showing an efficient generic construction that does not require to set a bound on the number of time periods at key generation. The article then extends the unprotected update model to other key-evolving primitives such as forward-secure public key encryption and key-insulated cryptosystems. © 2010 ACM.",Digital signatures; Forward security; Generic constructions; Key exposures; Second factor,Authentication; Electronic document identification systems; Public key cryptography; Digital Signature; Forward security; Generic construction; Key exposure; Second factor; Network security
A simple and generic construction of Authenticated Encryption with Associated Data,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651389846&doi=10.1145%2f1880022.1880027&partnerID=40&md5=c1a7aa7054753f5c27e137c875a6c55b,We revisit the problem of constructing a protocol for performing Authenticated Encryption with Associated Data (AEAD). A technique is described which combines a collision-resistant hash function with a protocol for Authenticated Encryption (AE). The technique is both simple and generic and does not require any additional key material beyond that of the AE protocol. Concrete instantiations are shown where a 256-bit hash function is combined with some known single-pass AE protocols employing either 128-bit or 256-bit block ciphers. This results in possible efficiency improvement in the processing of the header. © 2010 ACM.,Authenticated Encryption with Associated Data; Collision resistant hash function; Generic construction,Hash functions; Authenticated encryption; Block ciphers; Collision resistant hash function; Collision-resistant hash functions; Efficiency improvement; Generic construction; Key materials; Authentication
BLAC: Revoking repeatedly misbehaving anonymous users without relying on TTPs,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651408573&doi=10.1145%2f1880022.1880033&partnerID=40&md5=4e16db5ebb49aa5548e65855e1e7a315,"Several credential systems have been proposed in which users can authenticate to service providers anonymously. Since anonymity can give users the license to misbehave, some variants allow the selective deanonymization (or linking) of misbehaving users upon a complaint to a Trusted Third Party (TTP). The ability of the TTP to revoke a user's privacy at any time, however, is too strong a punishment for misbehavior. To limit the scope of deanonymization, some systems have been proposed in which users can be deanonymized only if they authenticate ""too many times,"" such as ""double spending"" with electronic cash. While useful in some applications, such techniques cannot be generalized to more subjective definitions of misbehavior, for example, using such schemes it is not possible to block anonymous users who ""deface too many Web pages"" on a Web site. We present BLAC, the first anonymous credential system in which service providers can revoke the credentials of misbehaving users without relying on a TTP. Since revoked users remain anonymous, misbehaviors can be judged subjectively without users fearing arbitrary deanonymization by a TTP. Additionally, our construction supports a d-strikes-out revocation policy, whereby users who have been subjectively judged to have repeatedly misbehaved at least d times are revoked from the system. Thus, for the first time, it is indeed possible to block anonymous users who have ""defaced too many Web pages"" using our scheme. © 2010 ACM.",Anonymous authentication; Anonymous blacklisting; Privacy; Privacy-enhanced revocation; User misbehavior,Websites; Anonymous authentication; Anonymous blacklisting; Privacy; Privacy-enhanced revocation; User misbehavior; Authentication
Storage-based intrusion detection,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651392217&doi=10.1145%2f1880022.1880024&partnerID=40&md5=fe0dc609f8bbca5d721f6f297852071c,"Storage-based intrusion detection consists of storage systems watching for and identifying data access patterns characteristic of system intrusions. Storage systems can spot several common intruder actions, such as adding backdoors, inserting Trojan horses, and tampering with audit logs. For example, examination of 18 real intrusion tools reveals that most (15) can be detected based on their changes to stored files. Further, an Intrusion Detection System (IDS) embedded in a storage device continues to operate even after client operating systems are compromised. We describe and evaluate a prototype storage IDS, built into a disk emulator, to demonstrate both feasibility and efficiency of storage-based intrusion detection. In particular, both the performance overhead (< 1%) and memory required (1.62MB for 13995 rules) are minimal. © 2010 ACM.",Intrusion detection; Storage,Computer crime; Computer operating systems; Embedded systems; Audit logs; Backdoors; Data access patterns; Intrusion detection systems; Operating systems; Storage; Storage devices; Storage systems; Storage-based intrusion detection; Trojan horse; Intrusion detection
Authenticated index structures for aggregation queries,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651408225&doi=10.1145%2f1880022.1880026&partnerID=40&md5=f98c13e05b79cf7c54288083f881e6f4,"Query authentication is an essential component in Outsourced DataBase (ODB) systems. This article introduces efficient index structures for authenticating aggregation queries over large datasets. First, we design an index that features good performance characteristics for static environments. Then, we propose more involved structures for the dynamic case. Our structures feature excellent performance for authenticating queries with multiple aggregate attributes and multiple selection predicates. Furthermore, our techniques cover a large number of aggregate types, including distributive aggregates (such as SUM, COUNT, MIN, and MAX), algebraic aggregates (such as the AVG), and holistic aggregates (such asMEDIAN and QUANTILE). We have also addressed the issue of authenticating aggregation queries efficiently when the database is encrypted to protect data confidentiality. Finally, we implemented a working prototype of the proposed techniques and experimentally validated the effectiveness and efficiency of our methods. © 2010 ACM.",Aggregation; Authentication; Indexing; Outsourced databases,Aggregates; Authentication; Feature extraction; Indexing (of information); Aggregate type; Aggregation; Aggregation queries; Data confidentiality; Essential component; Excellent performance; Index structure; Indexing; Large datasets; Multiple selection; Outsourced databases; Performance characteristics; Static environment; Query languages
Identity escrow protocol and anonymity analysis in the applied pi-calculus,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651384169&doi=10.1145%2f1880022.1880035&partnerID=40&md5=0001599e57fd647bc9df98ad4377e27e,"Anonymity with identity escrow attempts to allow users of an online service to remain anonymous, while providing the possibility that the service owner can break the anonymity in exceptional circumstances, such as to assist in a criminal investigation. In the article, we propose an identity escrow protocol that distributes user identity among several escrow agents. The main feature of our scheme is it is based on standard encryption algorithms and it provides user anonymity even if all but one escrow holders are dishonest acting in a coalition. We also present analysis of the anonymity property of our protocol in the applied pi-calculus. We review a related scheme by Marshall and Molina-Jiminez [2003] that aimed to achieve goals similar to ours, and show that their scheme suffers from serious weaknesses. © 2010 ACM.",Accountability; Anonymity; Applied π-calculus; Identity escrow protocol,Cryptography; Accountability; Anonymity; Anonymity analysis; Anonymity property; Criminal investigation; Encryption algorithms; Identity escrow protocol; On-line service; Pi calculus; User anonymity; User identity; Calculations
Satisfiability and resiliency in workflow authorization systems,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651384017&doi=10.1145%2f1880022.1880034&partnerID=40&md5=3013d02e1447b4927454510c61b044b6,"We propose the role-and-relation-based access control (R2BAC) model for workflow authorization systems. In R2BAC, in addition to a user's role memberships, the user's relationships with other users help determine whether the user is allowed to perform a certain step in a workflow. For example, a constraint may require that two steps must not be performed by users who have conflicts of interests. We study computational complexity of the workflow satisfiability problem, which asks whether a set of users can complete a workflow. In particular, we apply tools from parameterized complexity theory to better understand the complexities of this problem. Furthermore, we reduce the workflow satisfiability problem to SAT and apply SAT solvers to address the problem. Experiments show that our algorithm can solve instances of reasonable size efficiently. Finally, it is sometimes not enough to ensure that a workflow can be completed in normal situations. We study the resiliency problem in workflow authorization systems, which asks whether a workflow can be completed even if a number of users may be absent. We formally define three levels of resiliency in workflow systems and study computational problems related to these notions of resiliency. © 2010 ACM.",Access control; Fault tolerant; Policy design,Computational complexity; Formal logic; Management; Security systems; Authorization systems; Computational problem; Conflicts of interest; Fault tolerant; Parameterized complexity; Policy design; SAT solvers; Satisfiability; Satisfiability problems; Work-flow systems; Access control
Robust decentralized virtual coordinate systems in adversarial environments,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651386029&doi=10.1145%2f1880022.1880032&partnerID=40&md5=d4a01567ea5062c29f40f9b38d1df408,"Virtual coordinate systems provide an accurate and efficient service that allows hosts on the Internet to determine the latency to arbitrary hosts without actively monitoring all of the nodes in the network. Many of the proposed systems were designed with the assumption that all of the nodes are altruistic. However, this assumption may be violated by compromised nodes acting maliciously to degrade the accuracy of the coordinate system. As numerous peer-to-peer applications come to rely on virtual coordinate systems to achieve good performance, it is critical to address the security of such systems. In this work, we demonstrate the vulnerability of decentralized virtual coordinate systems to insider (or Byzantine) attacks. We propose techniques to make the coordinate assignment robust to malicious attackers without increasing the communication cost. We use both spatial and temporal correlations to perform context-sensitive outlier analysis to reject malicious updates and prevent unnecessary and erroneous adaptations. We demonstrate the attacks and mitigation techniques in the context of a well-known virtual coordinate system using simulations based on three representative, real-life Internet topologies of hosts and corresponding Round Trip Times (RTT). We show the effects of the attacks and the utility of the mitigation techniques on the virtual coordinate system as seen by higher-level applications, elucidating the utility of deploying robust virtual coordinate systems as network services. © 2010 ACM.",Attack mitigation; Network coordinates; Reliability; Security; Virtual coordinate systems,Internet; Adversarial environments; Attack mitigation; Co-ordinate system; Communication cost; Compromised nodes; Context-sensitive; Internet topologies; Mitigation techniques; Network coordinates; Network services; Outlier analysis; Peer-to-peer application; Round-trip time; Security; Spatial and temporal correlation; Virtual coordinate systems; Peer to peer networks
On the consistency of distributed proofs with hidden subtrees,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955218979&doi=10.1145%2f1805974.1805981&partnerID=40&md5=4ff27ca24d1385d833eac062e02fa40d,"Previous work has shown that distributed authorization systems that fail to sample a consistent snapshot of the underlying system during policy evaluation are vulnerable to a number of attacks. Unfortuantely, the consistency enforcement solutions presented in previous work were designed for systems in which only CA-certified evidence is used during the decision-making process, all of which is available to the decision-making node at runtime. In this article, we generalize previous results and present light-weight mechanisms through which consistency constraints can be enforced in proof systems in which the full details of a proof may be unavailable to the querier due to information release policies, and the existence of certificate authorities for certifying evidence is unlikely; these types of distributed proof systems are likely candidates for use in pervasive computing and sensor network environments. We present modifications to one such distributed proof system that enable three types of consistency constraints to be enforced while still respecting the same confidentiality and integrity policies as the original proof system. We then discuss how these techniques can be adapted and applied to other, less restrictive, distributed proof systems. Further, we detail a performance analysis that illustrates the modest overheads of our consistency enforcement schemes. © 2010 ACM.",Consistency; Distributed proving; Pervasive computing,Decision making; Service oriented architecture (SOA); Certificate authority; Consistency constraints; Consistency enforcement; Decision making process; Distributed authorization; Integrity Policy; Light weight; Performance analysis; Pervasive computing; Policy evaluation; Proof system; Runtimes; Sensor network environment; Subtrees; Underlying systems; Programmable logic controllers
The Role Mining Problem: A formal perspective,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955190756&doi=10.1145%2f1805974.1895983&partnerID=40&md5=dbbf117eb5accdc1d0c3eb9359ddb585,"Devising a complete and correct set of roles has been recognized as one of the most important and challenging tasks in implementing role-based access control. A key problem related to this is the notion of goodness/ interestingness-when is a role good/interesting? In this article, we define the Role Mining Problem (RMP) as the problem of discovering an optimal set of roles from existing user permissions. The main contribution of this article is to formally define RMP and analyze its theoretical bounds. In addition to the above basic RMP, we introduce two different variations of the RMP, called the d-Approx RMP and the minimal-noise RMP that have pragmatic implications. We reduce the known ""Set Basis Problem"" to RMP to show that RMP is an NP-complete problem. An important contribution of this article is also to show the relation of the RMP to several problems already identified in the data mining and data analysis literature. By showing that the RMP is in essence reducible to these known problems, we can directly borrow the existing implementation solutions and guide further research in this direction. We also develop a heuristic solution based n the previously proposed FastMiner algorithm, which is very accurate and efficient. © 2010 ACM.",RBAC; Role engineering; Role mining,Data reduction; Data analysis; Heuristic solutions; Interestingness; Key problems; Mining problems; NP complete problems; Optimal sets; RBAC; Role engineering; Role-based Access Control; Theoretical bounds; Access control
Combining fragmentation and encryption to protect privacy in data storage,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955210832&doi=10.1145%2f1805974.1805978&partnerID=40&md5=118976404c9a444a45d442eba2100ed3,"The impact of privacy requirements in the development of modern applications is increasing very quickly. Many commercial and legal regulations are driving the need to develop reliable solutions for protecting sensitive information whenever it is stored, processed, or communicated to external parties. To this purpose, encryption techniques are currently used in many scenarios where data protection is required since they provide a layer of protection against the disclosure of personal information, which safeguards companies from the costs that may arise from exposing their data to privacy breaches. However, dealing with encrypted data may make query processing more expensive. In this article, we address these issues by proposing a solution to enforce the privacy of data collections that combines data fragmentation with encryption. We model privacy requirements as confidentiality constraints expressing the sensitivity of attributes and their associations. We then use encryption as an underlying (conveniently available) measure for making data unintelligible while exploiting fragmentation as a way to break sensitive associations among attributes. We formalize the problem of minimizing the impact of fragmentation in terms of number of fragments and their affinity and present two heuristic algorithms for solving such problems. We also discuss experimental results, comparing the solutions returned by our heuristics with respect to optimal solutions, which show that the heuristics, while guaranteeing a polynomial-time computation cost are able to retrieve solutions close to optimum. © 2010 ACM.",Encryption; Fragmentation; Privacy,Cryptography; Data acquisition; Digital storage; Heuristic algorithms; Laws and legislation; Optimization; Polynomial approximation; Polynomials; Data fragmentation; Encryption technique; Fragmentation; Modern applications; Personal information; Polynomial-time computation; Privacy requirements; Sensitive informations; Data privacy
Privacy-aware Role-based access control,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955210831&doi=10.1145%2f1805974.1805980&partnerID=40&md5=4c017830681b4e4da0776c8f72a2267b,"In this article, we introduce a comprehensive framework supporting a privacy-aware access control mechanism, that is, a mechanism tailored to enforce access control to data containing personally identifiable information and, as such, privacy sensitive. The key component of the framework is a family of models (P-RBAC) that extend the well-known RBAC model in order to provide full support for expressing highly complex privacy-related policies, taking into account features like purposes and obligations. We formally define the notion of privacy-aware permissions and the notion of conflicting permission assignments in P-RBAC, together with efficient conflict-checking algorithms. The framework also includes a flexible authoring tool, based on the use of the SPARCLEsystem, supporting the high-level specification of P-RBAC permissions. SPARCLE supports the use of natural language for authoring policies and is able to automatically generate P-RBAC permissions from these natural language specifications. In the article, we also report performanceevaluation results and contrast our approach with other relevant access control and privacy policy frameworks such as P3P, EPAL, and XACML. © 2010 ACM.",Model; Privacy; Purpose; Role-based access control,Linguistics; Security systems; Specifications; Authoring tool; High level specification; Key component; Natural language specifications; Natural languages; Personally identifiable information; Privacy; Privacy policies; Privacy-Aware Access Control; RBAC model; Role-based Access Control; Access control
Editorial ESORICS 2007,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955186793&doi=10.1145%2f1805974.1805975&partnerID=40&md5=1bedebc9c44f74ab046c79abf9dd0266,[No abstract available],,
A framework to enforce access control over data streams,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955203039&doi=10.1145%2f1805974.1805984&partnerID=40&md5=5b85b12c48255f84ec22acff397aeaed,"Although access control is currently a key component of any computational system, it is only recently that mechanisms to guard against unauthorized access to streaming data have started to be investigated. To cope with this lack, in this article, we propose a general framework to protect streaming data, which is, as much as possible, independent from the target stream engine. Differently from RDBMSs, up to now a standard query language for data streams has not yet emerged and this makes the development of a general solution to access control enforcement more difficult. The framework we propose in this article is based on an expressive role-based access controlmodel proposed by us. It exploits a query rewriting mechanism, which rewrites user queries in such a way that they do not return tuples/attributes that should not be accessed according to the specified access control policies. Furthermore, the framework contains a deployment module able to translate the rewritten query in such a way that it can be executed by different stream engines, therefore, overcoming the lack of standardization. In the article, besides presenting all the components of our framework, we prove the correctness and completeness of the query rewriting algorithm, and we present some experiments that show the feasibility of the developed techniques. © 2010 ACM.",Access control; Data stream; Secure query rewriting,Computer hardware description languages; Data communication systems; Hydraulics; Query languages; Security systems; Standardization; Access control policies; Computational system; Data stream; General solutions; Key component; Query rewritings; Rewritten query; Role-based; Standard query languages; Streaming data; Target streams; User query; Access control
Editorial SACMAT 2007,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955179932&doi=10.1145%2f1805974.1805979&partnerID=40&md5=fffc2f52a07b41af9ae590167a212439,[No abstract available],,
A logic for state-modifying authorization policies,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955186221&doi=10.1145%2f1805974.1805976&partnerID=40&md5=c8725b79d92769025644980e9360a0e1,"Administering and maintaining access control systems is a challenging task, especially in environments with complex and changing authorization requirements. A number of authorization logics have been proposed that aim at simplifying access control by factoring the authorization policy out of the hard-coded resource guard. However, many policies require the authorization state to be updated after a granted access request, for example, to reflect the fact that a user has activated or deactivated a role. Current authorization languages cannot express such state modifications; these still have to be hard-coded into the resource guard. We present a logic for specifying policies where access requests can have effects on the authorization state. The logic is semantically defined by a mapping to Transaction Logic. Using this approach, updates to the state are factored out of the resource guard, thus enhancing maintainability and facilitating more expressive policies that take the history of access requests into account. We also present a sound and complete proof system for reasoning about sequences of access requests. This gives rise to a goal-oriented algorithm for finding minimal sequences that lead to a specified target authorization state. © 2010 ACM.",Access control; Authorization; Hoare logic; Policy,Maintainability; Security systems; Authorization; Authorization policy; Goal-oriented; Hoare Logic; Proof system; Access control
Security of multithreaded programs by compilation,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955226136&doi=10.1145%2f1805974.1895977&partnerID=40&md5=140d81f20b7425d93d8f8cfaf0b6e3cb,"End-to-End security of mobile code requires that the code neither intentionally nor accidentally propagates sensitive information to an adversary. Althoughmobile code is commonlymultithreaded low-level code, there lack enforcement mechanisms that ensure information security for such programs. The modularity is three-fold: we give modular extensions of sequential semantics, sequential security typing, and sequential security-type preserving compilation that allow us enforcing security for multithreaded programs. Thanks to the modularity, there are no more restrictions on multithreaded source programs than on sequential ones, and yet we guarantee that their compilations are provably secure for a wide class of schedulers. © 2010 ACM.",Compilers; Noninterference; Schedulers; Type systems,Java programming language; Program compilers; Scheduling; Security of data; End-to-end security; Enforcement mechanisms; Information security; Mobile codes; Modular extension; Multi-threaded programs; Multithreaded; Provably secure; Sensitive informations; Sequential semantics; Type preserving compilation; Type systems; Multitasking
A logical specification and analysis for SELinux MLS policy,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955220788&doi=10.1145%2f1805974.1805982&partnerID=40&md5=9294d4dd48d0019ea60ed88956cac8c8,"The SELinux mandatory access control (MAC) policy has recently added a multilevel security (MLS)model which is able to express a fine granularity of control over a subject's access rights. The problem is that the richness of the SELinux MLS model makes it impractical to manually evaluate that a given policy meets certain specific properties. To address this issue, we have modeled the SELinux MLS model, using a logical specification and implemented that specification in the Prolog language. Furthermore, we have developed some analyses for testing information flow properties of a given policy as well as an algorithm to determine whether one policy is compliant with another. We have implemented these analyses in Prolog and compiled our implementation into a tool for SELinux MLS policy analysis, called PALMS.Using PALMS, we verified some important properties of the SELinux MLS reference policy, namely that it satisfies the simple security condition and property defined by Bell and LaPadula. We also evaluated whether the policy associated to a given application is compliant with the policy of the SELinux system in which it would be deployed. © 2010 ACM.",Multilevel security; Policy analysis; Policy compliance; SELinux,Specifications; Logical analysis; Logical specifications; Mandatory access control; Multi-level security models; Multi-level security policies; Multilevel security; Policy analysis; Policy compliance; Property; SELinux; Access control
How much anonymity does network latency leak?,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949464624&doi=10.1145%2f1698750.1698753&partnerID=40&md5=cf34a864f65fcad8bf5a04025dcafc92,"Low-latency anonymity systems such as Tor, AN.ON, Crowds, and Anonymizer.com aim to provide anonymous connections that are both untraceable by local adversaries who control only a few machines and have low enough delay to support anonymous use of network services like Web browsing and remote login. One consequence of these goals is that these services leak some information about the network latency between the sender and one or more nodes in the system. We present two attacks on low-latency anonymity schemes using this information. The first attack allows a pair of colluding Web sites to predict, based on local timing information and with no additional resources, whether two connections from the same Tor exit node are using the same circuit with high confidence. The second attack requires more resources but allows a malicious Web site to gain several bits of information about a client each time he visits the site. We evaluate both attacks against two low-latency anonymity protocolsthe Tor network and the MultiProxy proxy aggregator serviceand conclude that both are highly vulnerable to these attacks. © 2010 ACM.",,Packet networks; Timing circuits; World Wide Web; High confidence; Low-latency; Network latencies; Network services; Remote login; Timing information; Web browsing; Network protocols
Split-ballot voting: Everlasting privacy with distributed trust,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949444906&doi=10.1145%2f1698750.1698756&partnerID=40&md5=843ab5dd87dadbe72d2b3a7dd6f88456,"In this article, we propose a new voting protocol with several desirable security properties. The voting stage of the protocol can be performed by humans without computers; it provides every voter with the means to verify that all the votes were counted correctly (universal verifiability) while preserving ballot secrecy. The protocol has everlasting privacy: Even a computationally unbounded adversary gains no information about specific votes from observing the protocol's output. Unlike previous protocols with these properties, this protocol distributes trust between two authorities: a single corrupt authority will not cause voter privacy to be breached. Finally, the protocol is receipt-free: A voter cannot prove how she voted even if she wants to do so. We formally prove the security of the protocol in the universal composability framework, based on number-theoretic assumptions. © 2010 ACM.",Everlasting privacy; Receipt-free; Universally composable; Voting protocol,Network protocols; Ballot secrecy; Receipt-free; Security properties; Universal Composability; Universal verifiability; Universally composable; Voting protocols; Network security
"Stealthy malware detection and monitoring through VMM-based ""out-of-the-box"" semantic view reconstruction",2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949441598&doi=10.1145%2f1698750.1698752&partnerID=40&md5=1155754ea1cbc52685e36777a8f59756,"An alarming trend in recent malware incidents is that they are armed with stealthy techniques to detect, evade, and subvert malware detection facilities of the victim. On the defensive side, a fundamental limitation of traditional host-based antimalware systems is that they run inside the very hosts they are protecting (in-the-box), making them vulnerable to counter detection and subversion by malware. To address this limitation, recent solutions based on virtual machine (VM) technologies advocate placing the malware detection facilities outside of the protected VM (out-of-the-box). However, they gain tamper resistance at the cost of losing the internal semantic view of the host, which is enjoyed by in-the-box approaches. This poses a technical challenge known as the semantic gap. In this article, we present the design, implementation, and evaluation of VMwatcheran out-of-the-box approach that overcomes the semantic gap challenge. A new technique called guest view casting is developed to reconstruct internal semantic views (e.g., files, processes, and kernel modules) of a VM nonintrusively from the outside. More specifically, the new technique casts semantic definitions of guest OS data structures and functions on virtual machine monitor (VMM)-level VM states, so that the semantic view can be reconstructed. Furthermore, we extend guest view casting to reconstruct details of system call events (e.g., the process that makes the system call as well as the system call number, parameters, and return value) in the VM, enriching the semantic view. With the semantic gap effectively narrowed, we identify three unique malware detection and monitoring capabilities: (i) view comparison-based malware detection and its demonstration in rootkit detection; (ii) out-of-the-box deployment of off-the-shelf anti malware software with improved detection accuracy and tamper-resistance; and (iii) nonintrusive system call monitoring for malware and intrusion behavior observation. We have implemented a proof-of-concept VMwatcher prototype on a number of VMM platforms. Our evaluation experiments with real-world malware, including elusive kernel-level rootkits, demonstrate VMwatcher's practicality and effectiveness. © 2010 ACM.",Malware detection; Rootkits; Virtual machines,Concentration (process); Data structures; Intrusion detection; Semantics; Anti-malware; Behavior observation; Detection accuracy; Evaluation experiments; Fundamental limitations; Host-based; Kernel modules; Malware detection; Malwares; Monitoring capabilities; Non-intrusive; Proof of concept; Real-world; Return value; Rootkits; Semantic gap; System call monitoring; System calls; Tamper resistance; Technical challenges; Virtual machine monitors; Virtual machine technology; Virtual machines; Computer crime
New payload attribution methods for network forensic investigations,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949456302&doi=10.1145%2f1698750.1698755&partnerID=40&md5=1eeb38072f4f2c410513fe0dd81a66b3,"Payload attribution can be an important element in network forensics. Given a history of packet transmissions and an excerpt of a possible packet payload, a payload attribution system (PAS) makes it feasible to identify the sources, destinations, and the times of appearance on a network of all the packets that contained the specified payload excerpt. A PAS, as one of the core components in a network forensics system, enables investigating cybercrimes on the Internet by, for example, tracing the spread of worms and viruses, identifying who has received a phishing e-mail in an enterprise, or discovering which insider allowed an unauthorized disclosure of sensitive information. Due to the increasing volume of network traffic in today's networks, it is infeasible to effectively store and query all the actual packets for extended periods of time in order to allow analysis of network events for investigative purposes; therefore, we focus on extremely compressed digests of the packet activity. We propose several new methods for payload attribution, which utilize Rabin fingerprinting, shingling, and winnowing. Our best methods allow building practical payload attribution systems, which provide data reduction ratios greater than 100:1 while supporting efficient queries with very low false positive rates. We demonstrate the properties of the proposed methods and specifically analyze their performance and practicality when used as modules of a network forensics system ForNet. Our experimental results outperform current state-of-the-art methods both in terms of false positives and data reduction ratio. Finally, these approaches directly allow the collected data to be stored and queried by an untrusted party without disclosing any payload information nor the contents of queries. © 2010 ACM.",Bloom filter; Network forensics; Payload attribution,Blooms (metal); Data reduction; Packet networks; Statistics; Viruses; Bloom filters; Core components; Cyber-crimes; False positive; False positive rates; In-network; Network forensics; Network forensics system; Network traffic; Packet payloads; Packet transmissions; Payload attribution; Payload information; Phishing; Sensitive informations; State-of-the-art methods; Computer viruses
Guest editorial: Special issue on computer and communications security,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949434563&doi=10.1145%2f1698750.1698751&partnerID=40&md5=8acb2e98f74592e14f2978a30649aa5c,[No abstract available],,
Authenticated error-correcting codes with applications to multicast authentication,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949483212&doi=10.1145%2f1698750.1698757&partnerID=40&md5=1b6a8b4ef7c34648f9253ec7eb3251fb,"We consider the problem of authenticating a stream of packets transmitted over a network controlled by an adversary who may perform arbitrary attacks on the stream: He may drop or modify chosen packets, rearrange the order of the packets in any way, and inject new, random, or specially crafted packets into the stream. In contrast, prior work on the multicast authentication problem has focused on a less powerful adversarial network model or has examined a considerably more restrictive setting with specific timing or structural assumptions about the network. We model the ability of the network to modify a stream of n packets with two parameters: the survival rate α (0 <α 1) denoting the fraction of the packets that are guaranteed to reach any particular receiver unmodified and the flood rate Β (Β 1) indicating the factor by which the size of the received stream at any particular receiver may exceed the size of the transmitted stream. Combining error-correcting codes with standard cryptographic primitives, our approach gives almost the same security guarantees as if each packet were individually signed, but requires only one signature operation for the entire stream and adds to each transmitted packet only a small amount of authentication information, proportional to Β/α2. We prove the security and correctness of our scheme and analyze its performance in terms of communication overhead and computational effort at the sender and the receiver. Our results demonstrate how list decoding can be transformed into unambiguous decoding in the public-key model and the bounded computational model for the underlying communication channel. Overall, our technique provides an authenticated error-correcting code of independent interest that may be useful in other settings. © 2010 ACM.",Authentication; Data stream; Digital signature schemes; Error-correcting codes; Information integrity; List decoding; Multicast security,Artificial intelligence; Data communication systems; Decoding; Electronic document identification systems; Hydraulics; Information theory; Multicasting; Authentication data; Digital signature schemes; Error correcting code; Information integrity; List decoding; Multicast security; Authentication
CANDID: Dynamic candidate evaluations for automatic prevention of SQL injection attacks,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949464016&doi=10.1145%2f1698750.1698754&partnerID=40&md5=5a6ecf1179c5dd8ae6eefb50f94b98fd,"SQL injection attacks are one of the top-most threats for applications written for the Web. These attacks are launched through specially crafted user inputs, on Web applications that use low-level string operations to construct SQL queries. In this work, we exhibit a novel and powerful scheme for automatically transforming Web applications to render them safe against all SQL injection attacks. A characteristic diagnostic feature of SQL injection attacks is that they change the intended structure of queries issued. Our technique for detecting SQL injection is to dynamically mine the programmer-intended query structure on any input, and detect attacks by comparing it against the structure of the actual query issued. We propose a simple and novel mechanism, called Candid, for mining programmer intended queries by dynamically evaluating runs over benign candidate inputs. This mechanism is theoretically well founded and is based on inferring intended queries by considering the symbolic query computed on a program run. Our approach has been implemented in a tool called Candid that retrofits Web applications written in Java to defend them against SQL injection attacks. We have also implemented Candid by modifying a Java Virtual Machine, which safeguards applications without requiring retrofitting. We report extensive experimental results that show that our approach performs remarkably well in practice. © 2010 ACM.",Dynamic monitoring; Retrofitting code; SQL injection attacks; Symbolic evaluation,Computer crime; Dynamics; Mining; Retrofitting; World Wide Web; Diagnostic features; Dynamic monitoring; Java virtual machines; Query structures; SQL injection; SQL query; Symbolic evaluation; User input; WEB application; Java programming language
Deterring voluntary trace disclosure in re-encryption mix-networks,2010,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949454440&doi=10.1145%2f1698750.1698758&partnerID=40&md5=1bf96a1da5736fb37ce99186c06149e2,"Mix-networks, a family of anonymous messaging protocols, have been engineered to withstand a wide range of theoretical internal and external adversaries. An undetectable insider threatvoluntary partial trace disclosures by server administratorsremains a troubling source of vulnerability. An administrator's cooperation could be the resulting coercion, bribery, or a simple change of interests. While eliminating this insider threat is impossible, it is feasible to deter such unauthorized disclosures by bundling them with additional penalties. We abstract these costs with collateral keys, which grant access to customizable resources. This article introduces the notion of trace-deterring mix-networks, which encode collateral keys for every server-node into every end-to-end message trace. The network reveals no keying material when the input-to-output transitions of individual servers remain secret. Two permutation strategies for encoding key information into traces, mix-and-flip and all-or-nothing, are presented. We analyze their trade-offs with respect to computational efficiency, anonymity sets, and colluding message senders. Our techniques have sufficiently low overhead for deployment in large-scale elections, thereby providing a sort of publicly verifiable privacy guarantee. © 2010 ACM.",Anonymous messaging; Electronic voting; Insider threat; Re-encryption mix-network; Zero-knowledge protocol,Computational efficiency; Cryptography; Servers; Voting machines; Electronic voting; Insider Threat; Mix networks; Re-encryption; Zero-knowledge protocols; Network protocols
Efficient and secure protocols for privacy-preserving set operations,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449139107&doi=10.1145%2f1609956.1609965&partnerID=40&md5=04cdd8663352136dd7d333d73a70eaa8,"Many applications require performing set operations without publishing individual datesets. In this article, we address this problem for five fundamental set operations including set intersection, cardinality of set intersection, element reduction, overthreshold set-union, and subset relation. Our protocols are obtained in the universally composable security framework, in the assumption of the probabilistic polynomial time bounded adversary, which actively controls a fixed set of t parties and the assumption of an authenticated broadcast channel. Our constructions utilize building blocks of nonmalleable NonInteractive Zero-Knowledge (NIZK) arguments, which are based on a (t + 1,N)-threshold version (N is the number of parties in the protocol) of the boneh-goh-nissim (BGN) cryptosystem whose underlying group supports bilinear maps, in the assumption that the public key and shares of the secret key have been generated by a trusted dealer. The previous studies were all based on the stand-alone model with the same assumptions on the adversary, broadcast channel, and key generation. For the first four operations, we propose protocols that improve the previously known results by an O(N) factor in the computation and communication complexities. For the subset relation, our protocol is the first one secure against the active adversary. Our constructions of NIZK have independent interest in that, though also mentioned as building blocks, the previous work did not illustrate how to construct them. We construct these NIZK with an additional nonmalleable property, the same complexity as claimed in the previous work, and also an improvement on the communication complexity. © 2009 ACM.",Bilinear groups; Cryptographic protocol; Noninteractive zeroknowledge argument; Privacy preservation; Set operations,Broadcasting; Fluorine containing polymers; Network protocols; Network security; Polynomial approximation; Cryptographic protocols; Non-interactive; Privacy preservation; Set operation; Zero knowledge; Cryptography
Introduction to ACM TISSEC special issue on CCS 2005,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449208046&doi=10.1145%2f1609956.1609957&partnerID=40&md5=7c424edf0d8250ef2ac2f3ff51eb3aa5,[No abstract available],,
Enforcing access control in Web-based social networks,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350406398&doi=10.1145%2f1609956.1609962&partnerID=40&md5=9ddccbc31fae02af83dd8b77c2ec5e69,"In this article, we propose an access control mechanism for Web-based social networks, which adopts a rule-based approach for specifying access policies on the resources owned by network participants, and where authorized users are denoted in terms of the type, depth, and trust level of the relationships existing between nodes in the network. Different from traditional access control systems, our mechanism makes use of a semidecentralized architecture, where access control enforcement is carried out client-side. Access to a resource is granted when the requestor is able to demonstrate being authorized to do that by providing a proof. In the article, besides illustrating the main notions on which our access control model relies, we present all the protocols underlying our system and a performance study of the implemented prototype. © 2009 ACM.",Access control; Semantic Web; Social networks,Behavioral research; Internet protocols; Security systems; Semantic Web; Semantics; Access control mechanism; Access control models; Access policies; Authorized users; Performance study; Rule-based approach; Social Networks; Trust level; Access control
Defining strong privacy for RFID,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449181354&doi=10.1145%2f1609956.1609963&partnerID=40&md5=2ea7656e4251c666b7ce716177ba2193,"In this work, we consider privacy in Radio Frequency IDentification (RFID) systems. Our contribution is twofold: (i) We propose a simple, formal definition of strong privacy useful for basic analysis of RFID systems, as well as a different (weaker) definition applicable to multiverifier systems; (ii) We apply our definition to reveal vulnerabilities in several proposed privacy-enhancing RFID protocols; and (iii) We formally analyze and suggest improvements to hash-locks, one of the first privacy-enhancing RFID protocols in the literature. © 2009 ACM.",EPC; Privacy; Proximity cards; RFID; Security,Radio frequency identification (RFID); Radio systems; Formal definition; Proximity cards; Radio frequency identification systems; RFID protocols; RFID security; RFID systems; Network security
Maintaining control while delegating trust: Integrity constraints in trust management,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449135752&doi=10.1145%2f1609956.1609961&partnerID=40&md5=df82d488aedc736a02b83d54a78bc3ed,"We introduce the use, monitoring, and enforcement of integrity constraints in trust management-style authorization systems. We consider what portions of the policy state must be monitored to detect violations of integrity constraints. Then, we address the fact that not all participants in a trust-management system can be trusted to assist in such monitoring, and show how many integrity constraints can be monitored in a conservative manner so that trusted participants detect and report if the system enters a policy state from which evolution in unmonitored portions of the policy could lead to a constraint violation. © 2009 ACM.",Access control; Distributed system security; Integrity; Trust management,Network security; Quality assurance; Security systems; Authorization systems; Constraint violation; Distributed system security; Integrity; Integrity constraints; Management systems; Trust management; Access control
"Control-flow integrity principles, implementations, and applications",2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449145808&doi=10.1145%2f1609956.1609960&partnerID=40&md5=eda28ab3d44f308cfb6eabb1d44573d1,"Current software attacks often build on exploits that subvert machine-code execution. The enforcement of a basic safety property, control-flow integrity (CFI), can prevent such attacks from arbitrarily controlling program behavior. CFI enforcement is simple and its guarantees can be established formally, even with respect to powerful adversaries. Moreover, CFI enforcement is practical: It is compatible with existing software and can be done efficiently using software rewriting in commodity systems. Finally, CFI provides a useful foundation for enforcing further security policies, as we demonstrate with efficient software implementations of a protected shadow call stack and of access control for memory regions. © 2009 ACM.",Binary rewriting; Control-flow graph; Inlined reference monitors; Vulnerabilities,Computer software; Security systems; Binary rewriting; Code execution; Commodity systems; Control-flow; Control-flow graphs; Memory region; Program behavior; Reference monitors; Safety property; Security policy; Software implementation; Access control
Compromising anonymous communication systems using blind source separation,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449188189&doi=10.1145%2f1609956.1609964&partnerID=40&md5=d978c38b8eae11702d4faa3dc86b0ea0,"We propose a class of anonymity attacks to both wired and wireless anonymity networks. These attacks are based on the blind source separation algorithms widely used to recover individual signals from mixtures of signals in statistical signal processing. Since the philosophy behind the design of current anonymity networks is to mix traffic or to hide in crowds, the proposed anonymity attacks are very effective. The flow separation attack proposed for wired anonymity networks can separate the traffic in a mix network. Our experiments show that this attack is effective and scalable. By combining the flow separation method with frequency spectrum matching, a passive attacker can derive the traffic map of the mix network. We use a nontrivial network to show that the combined attack works. The proposed anonymity attacks for wireless networks can identify nodes in fully anonymized wireless networks using collections of very simple sensors. Based on a time series of counts of anonymous packets provided by the sensors, we estimate the number of nodes with the use of principal component analysis. We then proceed to separate the collected packet data into traffic flows that, with help of the spatial diversity in the available sensors, can be used to estimate the location of the wireless nodes. Our simulation experiments indicate that the estimators show high accuracy and high confidence for anonymized TCP traffic. Additional experiments indicate that the estimators perform very well in anonymous wireless networks that use traffic padding. © 2009 ACM.",Anonymous communication; Blind source separation; Location privacy,Apartment houses; Communication systems; Experiments; Flow separation; Philosophical aspects; Principal component analysis; Sensor networks; Sensors; Separation; Signal analysis; Signal processing; Spectroscopy; Time series; Time series analysis; Traffic signals; Wireless networks; Wireless sensor networks; Anonymity networks; Anonymous communication; Anonymous communication systems; Frequency spectra; High confidence; Location privacy; Mix networks; Mix traffic; Packet data; Simulation experiments; Spatial diversity; Statistical signal processing; TCP traffic; Traffic flow; Wired and wireless; Wireless nodes; Blind source separation
Cryptanalysis of the random number generator of the Windows operating system,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449134804&doi=10.1145%2f1609956.1609966&partnerID=40&md5=ccf1ded10c9330df50c031568d0d3e07,"The PseudoRandom Number Generator (PRNG) used by the Windows operating system is the most commonly used PRNG. The pseudorandomness of the output of this generator is crucial for the security of almost any application running in Windows. Nevertheless, its exact algorithm was never published. We examined the binary code of a distribution of Windows 2000. This investigation was done without any help from Microsoft. We reconstructed the algorithm used by the pseudorandom number generator (namely, the function CryptGenRandom). We analyzed the security of the algorithm and found a nontrivial attack: Given the internal state of the generator, the previous state can be computed in 2 23 steps. This attack on forward security demonstrates that the design of the generator is flawed, since it is well known how to prevent such attacks. After our analysis was published, Microsoft acknowledged that Windows XP is vulnerable to the same attack. We also analyzed the way in which the generator is used by the operating system and found that it amplifies the effect of the attack: The generator is run in user mode rather than in kernel mode; therefore, it is easy to access its state even without administrator privileges. The initial values of part of the state of the generator are not set explicitly, but rather are defined by whatever values are present on the stack when the generator is called. Furthermore, each process runs a different copy of the generator, and the state of the generator is refreshed with system-generated entropy only after generating 128KB of output for the process running it. The result of combining this observation with our attack is that learning a single state may reveal 128KB of the past and future output of the generator. The implication of these findings is that a buffer overflow attack or a similar attack can be used to learn a single state of the generator, which can then be used to predict all random values, such as SSL keys, used by a process in all its past and future operations. This attack is more severe and more efficient than known attacks in which an attacker can only learn SSL keys if it is controlling the attacked machine at the time the keys are used. Copy; 2009 ACM.",Cryptanalysis; Pseudorandom number generator (PRNG); Windows operating system,Binary codes; Cryptography; Keys (for locks); Network security; Number theory; Random number generation; Windows; Buffer overflow attacks; Cryptanalysis; Exact algorithms; Forward security; Initial values; Internal state; Kernel mode; MicroSoft; Operating systems; Pseudo random number generators; Pseudorandomness; Random number generators; Random values; Running-in; Single state; User mode; Windows 2000; Windows XP; Windows operating system
Keyboard acoustic emanations revisited,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449149692&doi=10.1145%2f1609956.1609959&partnerID=40&md5=b25ccc89e81e406c0c2573f2c7d382d7,"We examine the problem of keyboard acoustic emanations. We present a novel attack taking as input a 10-minute sound recording of a user typing English text using a keyboard and recovering up to 96% of typed characters. There is no need for training recordings labeled with the corresponding clear text. A recognizer bootstrapped from a 10-minute sound recording can even recognize random text such as passwords: In our experiments, 90% of 5-character random passwords using only letters can be generated in fewer than 20 attempts by an adversary; 80% of 10-character passwords can be generated in fewer than 75 attempts by an adversary. In the attack, we use the statistical constraints of the underlying content, English language, to reconstruct text from sound recordings without knowing the corresponding clear text. The attack incorporates a combination of standard machine learning and speech recognition techniques, including cepstrum features, Hidden Markov Models, linear classification, and feedback-based incremental learning. © 2009 ACM.",Acoustic manations; Cepstrum; Computer security; Electronic eavesdropping; Hidden markov models; HMM; Human factors; Keyboards; Learning theory; Privacy; Signal analysis,Acoustics; Character recognition; Computer privacy; Education; Human engineering; Learning systems; Network security; Security systems; Signal analysis; Sound recording; Speech recognition; Cepstrum; Computer security; HMM; Human factors; Learning Theory; Hidden Markov models
Automated trust negotiation using cryptographic credentials,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449173381&doi=10.1145%2f1609956.1609958&partnerID=40&md5=6802b5d7d9b78f28e7a0993f7dbeee29,"In automated trust negotiation (ATN), two parties exchange digitally signed credentials that contain attribute information to establish trust and make access control decisions. Because the information in question is often sensitive, credentials are protected according to access control policies. In traditional ATN, credentials are transmitted either in their entirety or not at all. This approach can at times fail unnecessarily, either because a cyclic dependency makes neither negotiator willing to reveal her credential before her opponent because the opponent must be authorized for all attributes packaged together in a credential to receive any of them, or because it is necessary to disclose the precise attribute values, rather than merely proving they satisfy some predicate (such as being over 21 years of age). Recently, several cryptographic credential schemes and associated protocols have been developed to address these and other problems. However, they can be used only as fragments of an ATN process. This article introduces a framework for ATN in which the diverse credential schemes and protocols can be combined, integrated, and used as needed. A policy language is introduced that enables negotiators to specify authorization requirements that must be met by an opponent to receive various amounts of information about certified attributes and the credentials that contain it. The language also supports the use of uncertified attributes, allowing them to be required as part of policy satisfaction, and to place their (automatic) disclosure under policy control. © 2009 ACM.",Access control; Automated trust negotiation; Digital credentials; Privacy,Automation; Cryptography; Linguistics; Security systems; Access control decisions; Access control policies; Attribute information; Attribute values; Cyclic dependencies; Digital credentials; Policy control; Policy language; Trust negotiations; Access control
Universally composable RFID identification and authentication protocols,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849106311&doi=10.1145%2f1513601.1513603&partnerID=40&md5=2611106a6e90650ec104a68692e5bc47,"As the number of RFID applications grows, concerns about their security and privacy become greatly amplified. At the same time, the acutely restricted and cost-sensitive nature of RFID tags rules out simple reuse of traditional security/privacy solutions and calls for a new generation of extremely lightweight identification and authentication protocols. This article describes a universally composable security framework designed especially for RFID applications. We adopt RFID-specific setup, communication, and concurrency assumptions in a model that guarantees strong security, privacy, and availability properties. In particular, the framework supports modular deployment, which is most appropriate for ubiquitous applications. We also describe a set of simple, efficient, secure, and anonymous (untraceable) RFID identification and authentication protocols that instantiate the proposed framework. These protocols involve minimal interaction between tags and readers and place only a small computational load on the tag, and a light computational burden on the back-end server. We show that our protocols are provably secure within the proposed framework. © 2009 ACM.",Authentication and key-exchange protocols; RFID security; Universal composability,Authentication; Network protocols; Radio navigation; Authentication protocols; Back-end servers; Computational burden; Computational loads; Cost-sensitive; Key-exchange protocol; Minimal interactions; Provably secure; RF-ID tags; RFID applications; RFID security; Security and privacy; Ubiquitous application; Universal Composability; Universally composable; Universally Composable Security; Network security
IP covert channel detection,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849086598&doi=10.1145%2f1513601.1513604&partnerID=40&md5=b0bfbdf17e16d96c267d9609d950f8ea,"A covert channel can occur when an attacker finds and exploits a shared resource that is not designed to be a communication mechanism. A network covert channel operates by altering the timing of otherwise legitimate network traffic so that the arrival times of packets encode confidential data that an attacker wants to exfiltrate from a secure area from which she has no other means of communication. In this article, we present the first public implementation of an IP covert channel, discuss the subtle issues that arose in its design, and present a discussion on its efficacy. We then show that an IP covert channel can be differentiated from legitimate channels and present new detection measures that provide detection rates over 95%. We next take the simple step an attacker would of adding noise to the channel to attempt to conceal the covert communication. For these noisy IP covert timing channels, we show that our online detection measures can fail to identify the covert channel for noise levels higher than 10%. We then provide effective offline search mechanisms that identify the noisy channels. © 2009 ACM.",Channel detection; Information hiding; Network covert channels,Arrival time; Channel detection; Communication mechanisms; Confidential data; Covert channels; Covert communications; Covert timing channels; Detection rates; Information hiding; Network covert channel; Network traffic; Noise levels; Noisy channel; Offline; On-line detection; Search mechanism; Shared resources; Time measurement
Resiliency policies in access control,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849085284&doi=10.1145%2f1513601.1513602&partnerID=40&md5=f79abaddd04e7fc699cda38fbb1d50a8,"We introduce the notion of resiliency policies in the context of access control systems. Such policies require an access control system to be resilient to the absence of users. An example resiliency policy requires that upon removal of any s users, there should still exist d disjoint sets of users such that the users in each set together possess certain permissions of interest. Such a policy ensures that even when emergency situations cause some users to be absent, there still exist independent teams of users that have the permissions necessary for carrying out critical tasks. The Resiliency Checking Problem determines whether an access control state satisfies a given resiliency policy. We show that the general case of the problem and several subcases are intractable (NP-hard), and identify two subcases that are solvable in linear time. For the intractable cases, we also identify the complexity class in the polynomial hierarchy to which these problems belong. We discuss the design and evaluation of an algorithm that can efficiently solve instances of nontrivial sizes that belong to the intractable cases of the problem. Furthermore, we study the consistency problem between resiliency policies and static separation of duty policies. Finally, we combine the notions of resiliency and separation of duty to introduce the resilient separation of duty policy, which is useful in situations where both fault-tolerance and fraud-prevention are desired. © 2009 ACM.",Access control; Fault-tolerant; Policy design,Computational complexity; Fault tolerance; Quality assurance; Security systems; Separation; Complexity class; Consistency problems; Critical tasks; Disjoint sets; Emergency situation; Fault-tolerant; Linear time; NP-hard; Policy design; Polynomial hierarchies; Separation of duty; Static separation of duty; Access control
New techniques for private stream searching,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349098175&doi=10.1145%2f1455526.1455529&partnerID=40&md5=bc3f223404bdcb4e8a33bb29afdfbf05,"A system for private stream searching, introduced by Ostrovsky and Skeith, allows a client to provide an untrusted server with an encrypted search query. The server uses the query on a stream of documents and returns the matching documents to the client while learning nothing about the nature of the query. We present a new scheme for conducting private keyword search on streaming data which requires O(m) server to client communication complexity to return the content of the matching documents, where m is an upper bound on the size of the documents. The required storage on the server conducting the search is also O(m). The previous best scheme for private stream searching was shown to have O(m logm) communication and storage complexity. Our solution employs a novel construction in which the user reconstructs the matching files by solving a system of linear equations. This allows the matching documents to be stored in a compact buffer rather than relying on redundancies to avoid collisions in the storage buffer as in previous work. This technique requires a small amount of metadata to be returned in addition to the documents; for this the original scheme of Ostrovsky and Skeith may be employed with O(m logm) communication and storage complexity. We also present an alternative method for returning the necessary metadata based on a unique encrypted Bloom filter construction. This method requires O(m log(t/m)) communication and storage complexity, where t is the number of documents in the stream. In this article we describe our scheme, prove it secure, analyze its asymptotic performance, and describe a number of extensions. We also provide an experimental analysis of its scalability in practice. Specifically, we consider its performance in the demanding scenario of providing a privacy preserving version of the Google News Alerts service. © 2009 ACM.",Bloom filter; Private information retrieval; Private stream searching; Public key program obfuscation,Blooms (metal); Communication; Information dissemination; Information retrieval; Information services; Metadata; Alternative methods; Asymptotic performance; Bloom filter; Client communications; Experimental analysis; Keyword searches; Novel constructions; Privacy preserving; Private information retrieval; Private stream searching; Public key program obfuscation; Search queries; Storage complexity; Streaming datum; System of linear equations; Untrusted servers; Upper bounds; Buffer storage
Opportunities and Limits of Remote Timing Attacks,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349110770&doi=10.1145%2f1455526.1455530&partnerID=40&md5=2a38a2fdfbd0448a95ffefc6723b90d2,"Many algorithms can take a variable amount of time to complete depending on the data being processed. These timing differences can sometimes disclose confidential information. Indeed, researchers have been able to reconstruct an RSA private key purely by querying an SSL Web server and timing the results. Our work analyzes the limits of attacks based on accurately measuring network response times and jitter over a local network and across the Internet. We present the design of filters to significantly reduce the effects of jitter, allowing an attacker to measure events with 15-100μs accuracy across the Internet, and as good as 100ns over a local network. Notably, security-related algorithms on Web servers and other network servers need to be carefully engineered to avoid timing channel leaks at the accuracy demonstrated in this article. © 2009 ACM.",Information leakage; Jitter; Timing attacks,Internet; Servers; Time measurement; Web services; World Wide Web; Confidential informations; Information leakage; Local networks; Network response; Network servers; Rsa private keys; Timing attacks; Timing channels; Web servers; Jitter
Run-time enforcement of nonsafety policies,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349103393&doi=10.1145%2f1455526.1455532&partnerID=40&md5=7dfec55927dce5008e081150f281b1de,"A common mechanism for ensuring that software behaves securely is to monitor programs at run time and check that they dynamically adhere to constraints specified by a security policy. Whenever a program monitor detects that untrusted software is attempting to execute a dangerous action, it takes remedial steps to ensure that only safe code actually gets executed. This article improves our understanding of the space of policies enforceable by monitoring the run-time behaviors of programs. We begin by building a formal framework for analyzing policy enforcement: we precisely define policies, monitors, and enforcement. This framework allows us to prove that monitors enforce an interesting set of policies that we call the infinite renewal properties. We show how to construct a program monitor that provably enforces any reasonable infinite renewal property. We also show that the set of infinite renewal properties includes some nonsafety policies, that is, that monitors can enforce some nonsafety (including some purely liveness) policies. Finally, we demonstrate concrete examples of nonsafety policies enforceable by practical run-time monitors. © 2009 ACM.",Liveness; Monitoring; Policy enforcement; Safety; Security automata; Security policies,Translation (languages); Liveness; Monitoring; Policy enforcement; Safety; Security automata; Security policies; Security systems
Dynamic and efficient key management for access hierarchies,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349128962&doi=10.1145%2f1455526.1455531&partnerID=40&md5=3a69b3c72c8ae98e0c8c7ed3e38ffbff,"Hierarchies arise in the context of access control whenever the user population can be modeled as a set of partially ordered classes (represented as a directed graph). A user with access privileges for a class obtains access to objects stored at that class and all descendant classes in the hierarchy. The problem of key management for such hierarchies then consists of assigning a key to each class in the hierarchy so that keys for descendant classes can be obtained via efficient key derivation. We propose a solution to this problem with the following properties: (1) the space complexity of the public information is the same as that of storing the hierarchy; (2) the private information at a class consists of a single key associated with that class; (3) updates (i.e., revocations and additions) are handled locally in the hierarchy; (4) the scheme is provably secure against collusion; and (5) each node can derive the key of any of its descendant with a number of symmetric-key operations bounded by the length of the path between the nodes. Whereas many previous schemes had some of these properties, ours is the first that satisfies all of them. The security of our scheme is based on pseudorandom functions, without reliance on the Random Oracle Model. © 2009 ACM.",Efficient key derivation; Hierarchical access control; Key management,Graph theory; Security systems; Directed graphs; Efficient key derivation; Hierarchical access control; Key management; Private informations; Provably secure; Pseudo-random functions; Public informations; Random oracle models; Space complexity; Symmetric keys; Access control
Alcatraz: An isolated environment for experimenting with untrusted software,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349104471&doi=10.1145%2f1455526.1455527&partnerID=40&md5=1cbeb3abf4c5f883cc576ab7f9a2f9db,"In this article, we present an approach for realizing a safe execution environment (SEE) that enables users to try out new software (or configuration changes to existing software) without the fear of damaging the system in any manner. A key property of our SEE is that it faithfully reproduces the behavior of applications, as if they were running natively on the underlying (host) operating system. This is accomplished via one-way isolation: processes running within the SEE are given read-access to the environment provided by the host OS, but their write operations are prevented from escaping outside the SEE. As a result, SEE processes cannot impact the behavior of host OS processes, or the integrity of data on the host OS. SEEs support a wide range of tasks, including: study of malicious code, controlled execution of untrusted software, experimentation with software configuration changes, testing of software patches, and so on. It provides a convenient way for users to inspect system changes made within the SEE. If these changes are not accepted, they can be rolled back at the click of a button. Otherwise, the changes can be committed so as to become visible outside the SEE. We provide consistency criteria that ensure semantic consistency of the committed results. We develop two different implementation approaches, one in user-land and the other in the OS kernel, for realizing a safe-execution environment. Our implementation results show that most software, including fairly complex server and client applications, can run successfully within our SEEs. It introduces low performance overheads, typically below 10 percent. © 2009 ACM.",Isolation; One-way isolation,Codes (symbols); Computer operating systems; Information theory; Secondary emission; Software testing; Client applications; Consistency criterion; Execution environments; Implementation approaches; Isolation; Malicious codes; One-way isolation; Operating systems; Semantic consistencies; Software configurations; Software patches; System changes; Write operations; Computer software
Compact and anonymous role-based authorization chain,2009,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349114116&doi=10.1145%2f1455526.1455528&partnerID=40&md5=0d3e3cf9aaab2eb55835bb1da0ff6792,"We introduce a decentralized delegation model called anonymous role-based cascaded delegation. In this model, a delegator can issue authorizations on behalf of her role without revealing her identity. This type of delegation protects the sensitive membership information of a delegator and hides the internal structure of an organization. To provide an efficient storage and transmission mechanism for credentials used in anonymous role-based cascaded delegation, we present a new digital signature scheme that supports both signer anonymity and signature aggregation. Our scheme has compact role signatures that make it especially suitable for ubiquitous computing environments, where users may have mobile computing devices with narrow communication bandwidth and small storage units. © 2009 ACM.",Aggregate signature; Anonymity; Delegation,Aggregates; Electronic document identification systems; Aggregate signature; Anonymity; Communication bandwidths; Compact roles; Delegation; Delegator; Digital signature schemes; Internal structures; Membership informations; Mobile computing devices; Role-based; Storage units; Transmission mechanisms; Ubiquitous computing environments; Ubiquitous computing
Thwarting E-mail spam laundering,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57949090418&doi=10.1145%2f1455518.1455525&partnerID=40&md5=0ab4d038b37490e5b291e428fda8b6e4,"Laundering e-mail spam through open-proxies or compromised PCs is a widely-used trick to conceal real spam sources and reduce spamming cost in the underground e-mail spam industry. Spammers have plagued the Internet by exploiting a large number of spam proxies. The facility of breaking spam laundering and deterring spamming activities close to their sources, which would greatly benefit not only e-mail users but also victim ISPs, is in great demand but still missing. In this article, we reveal one salient characteristic of proxy-based spamming activities, namely packet symmetry, by analyzing protocol semantics and timing causality. Based on the packet symmetry exhibited in spam laundering, we propose a simple and effective technique, DBSpam, to online detect and break spam laundering activities inside a customer network. Monitoring the bidirectional traffic passing through a network gateway, DBSpam utilizes a simple statistical method, Sequential Probability Ratio Test, to detect the occurrence of spam laundering in a timely manner. To balance the goals of promptness and accuracy, we introduce a noise-reduction technique in DBSpam, after which the laundering path can be identified more accurately. Then DBSpam activates its spam suppressing mechanism to break the spam laundering. We implement a prototype of DBSpam based on libpcap, and validate its efficacy on spam detection and suppression through both theoretical analyses and trace-based experiments. © ACM 2008.",Proxy; Spam; SPRT,Computer crime; Electric network analysis; Electronic mail; Gateways (computer networks); Information theory; Internet protocols; Internet service providers; Laundering; Probability; Spamming; Statistical methods; Bidirectional traffics; Network gateways; Online detect; Proxy; Reduction techniques; Sequential probability ratio tests; Spam; Spam detections; Spam sources; Spammers; SPRT; Still missing; Theoretical analyses; Internet
Puppetnets: Misusing web browsers as a distributed attack infrastructure,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57949111985&doi=10.1145%2f1455518.1455524.&partnerID=40&md5=3e00f04b17971515fc607e615017a929,"Most of the recent work on Web security focuses on preventing attacks that directly harm the browsers host machine and user. In this paper we attempt to quantify the threat of browsers being indirectly misused for attacking third parties. Specifically, we look at how the existing Web infrastructure (e.g., the languages, protocols, and security policies) can be exploited by malicious or subverted Web sites to remotely instruct browsers to orchestrate actions including denial of service attacks, worm propagation, and reconnaissance scans. We show that attackers are able to create powerful botnet-like infrastructures that can cause significant damage. We explore the effectiveness of countermeasures including anomaly detection and more fine-grained browser security policies. © ACM 2008.",Distributed attacks; Malicious software; Web security,Computer crime; Internet protocols; Security of data; Security systems; World Wide Web; Anomaly detections; Denial of Service attacks; Distributed attacks; Effectiveness of countermeasures; Infra structures; Malicious software; Security policies; Third parties; Web infrastructures; Web security; Web sites; Worm propagations; Web browsers
Guest Editorial: Special issue on computer and communications security,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849083563&doi=10.1145%2f1455518.1455519&partnerID=40&md5=e6c77f49cca7f2996a0be8298ec4aa9a,[No abstract available],,
EXE: Automatically generating inputs of death,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849087817&doi=10.1145%2f1455518.1455522&partnerID=40&md5=d27746f881d6d18af3c4e39a1990f0f4,"This article presents EXE, an effective bug-finding tool that automatically generates inputs that crash real code. Instead of running code on manually or randomly constructed input, EXE runs it on symbolic input initially allowed to be anything. As checked code runs, EXE tracks the constraints on each symbolic (i.e., input-derived) memory location. If a statement uses a symbolic value, EXE does not run it, but instead adds it as an input-constraint; all other statements run as usual. If code conditionally checks a symbolic expression, EXE forks execution, constraining the expression to be true on the true branch and false on the other. Because EXE reasons about all possible values on a path, it has much more power than a traditional runtime tool: (1) it can force execution down any feasible program path and (2) at dangerous operations (e.g., a pointer dereference), it detects if the current path constraints allow any value that causes a bug. When a path terminates or hits a bug, EXE automatically generates a test case by solving the current path constraints to find concrete values using its own co-designed constraint solver, STP. Because EXEs constraints have no approximations, feeding this concrete input to an uninstrumented version of the checked code will cause it to follow the same path and hit the same bug (assuming deterministic code). EXE works well on real code, finding bugs along with inputs that trigger them in: the BSD and Linux packet filter implementations, the dhcpd DHCP server, the pcre regular expression library, and three Linux file systems. © ACM 2008.",Attack generation; Bug finding; Constraint solving; Dynamic analysis; Symbolic execution; Test case generation,Dynamic analysis; Servers; Attack generation; Bug finding; Constraint solving; Symbolic execution; Test case generation; Program debugging
Enforcing safety and consistency constraints in policy-based authorization systems,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57949107810&doi=10.1145%2f1455518.1455520&partnerID=40&md5=1851f9d328e35403ca8e44b98c62e306,"In trust negotiation and other forms of distributed proving, networked entities cooperate to form proofs of authorization that are justified by collections of certified attribute credentials. These attributes may be obtained through interactions with any number of external entities and are collected and validated over an extended period of time. Although these collections of credentials in some ways resemble partial system snapshots, current trust negotiation and distributed proving systems lack the notion of a consistent global state in which the satisfaction of authorization policies should be checked. In this article, we argue that unlike the notions of consistency studied in other areas of distributed computing, the level of consistency required during policy evaluation is predicated solely upon the security requirements of the policy evaluator. As such, there is little incentive for entities to participate in complicated consistency preservation schemes like those used in distributed computing, distributed databases, and distributed shared memory. We go on to show that the most intuitive notion of consistency fails to provide basic safety guarantees under certain circumstances and then propose several more refined notions of consistency that provide stronger safety guarantees. We provide algorithms that allow each of these refined notions of consistency to be attained in practice with minimal overheads and formally prove several security and privacy properties of these algorithms. Lastly, we explore the notion of strategic design trade-offs in the consistency enforcement algorithm space and propose several modifications to the core algorithms presented in this article. These modifications enhance the privacy-preservation or completeness properties of these algorithms without altering the consistency constraints that they enforce. © ACM 2008.",Consistency; Credentials; Distributed proving; Trust negotiation,Electronic document identification systems; Multimedia services; Refining; Authorization policies; Authorization systems; Consistency; Consistency constraints; Consistency enforcements; Consistency preservations; Consistent global states; Core algorithms; Credentials; Distributed computing; Distributed data basis; Distributed proving; Distributed Shared memories; External entities; Partial systems; Policy evaluations; Safety guarantees; Security and privacies; Security requirements; Strategic designs; Trust negotiation; Computational methods
Data collection with self-enforcing privacy,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849106781&doi=10.1145%2f1455518.1477940&partnerID=40&md5=2424fd1927405c21ca744259eb51c48b,"Consider a pollster who wishes to collect private, sensitive data from a number of distrustful individuals. How might the pollster convince the respondents that it is trustworthy? Alternately, what mechanism could the respondents insist upon to ensure that mismanagement of their data is detectable and publicly demonstrable? We detail this problem, and provide simple data submission protocols with the properties that a) leakage of private data by the pollster results in evidence of the transgression and b) the evidence cannot be fabricated without breaking cryptographic assumptions. With such guarantees, a responsible pollster could post a privacy-bond, forfeited to anyone who can provide evidence of leakage. The respondents are assured that appropriate penalties are applied to a leaky pollster, while the protection from spurious indictment ensures that any honest pollster has no disincentive to participate in such a scheme. © ACM 2008.",Data collection; Privacy,Data acquisition; Breakings; Cryptographic assumptions; Data collection; Data submission; Privacy; Private data; Property; Sensitive datas; Simple++; Sensitive data
Fast and black-box exploit detection and signature generation for commodity software,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849129485&doi=10.1145%2f1455518.1455523&partnerID=40&md5=f7b22d0d5a4af6df1be2e5e9bc9e493a,"In biology, a vaccine is a weakened strain of a virus or bacterium that is intentionally injected into the body for the purpose of stimulating antibody production. Inspired by this idea, we propose a packet vaccine mechanism that randomizes address-like strings in packet payloads to carry out fast exploit detection and signature generation. An exploit with a randomized jump address behaves like a vaccine: it will likely cause an exception in a vulnerable programs process when attempting to hijack the control flow, and thereby expose itself. Taking that exploit as a template, our signature generator creates a set of new vaccines to probe the program in an attempt to uncover the necessary conditions for the exploit to happen. A signature is built upon these conditions to shield the underlying vulnerability from further attacks. In this way, packet vaccine detects exploits and generates signatures in a black-box fashion, that is, not relying on the knowledge of a vulnerable programs source and binary code. Therefore, it even works on the commodity software obfuscated for the purpose of copyright protection. In addition, since our approach avoids the expense of tracking the programs execution flow, it performs almost as fast as a normal run of the program and is capable of generating a signature of high quality within seconds or even subseconds. We present the design of the packet vaccine mechanism and an example of its application. We also describe our proof-of-concept implementation and the evaluation of our technique using real exploits. © ACM 2008.",Black-box defense; Exploit detection; Signature generation; Vaccine injection; Worm,Binary codes; Concurrency control; Copyrights; Electric equipment protection; Machine design; Packet switching; Vaccines; Black-box defense; Exploit detection; Signature generation; Vaccine injection; Worm; Computer viruses
Private information: To reveal or not to reveal,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349121668&doi=10.1145%2f1410234.1410240&partnerID=40&md5=dfb08049a1501abe3780f67c0a7e91ae,"This article studies the notion of quantitative policies for trust management and gives protocols for realizing them in a disclosure-minimizing fashion. Specifically, Bob values each credential with a certain number of points, and requires a minimum total threshold of points before granting Alice access to a resource. In turn, Alice values each of her credentials with a privacy score that indicates her degree of reluctance to reveal that credential. Bob's valuation of credentials and his threshold are private. Alice's privacy-valuation of her credentials is also private. Alice wants to find a subset of her credentials that achieves Bob's required threshold for access, yet is of as small a value to her as possible. We give protocols for computing such a subset of Alice's credentials without revealing any of the two parties' above-mentioned private information. Furthermore, we develop a fingerprint method that allows Alice to independently and easily recover the optimal knapsack solution, once the computed optimal value is given, but also enables verification of the integrity of the optimal value. The fingerprint method is useful beyond the specific authorization problem studied, and can be applied to any integer knapsack dynamic programming in a private setting. © 2008 ACM.",Authorization; Policies; Secure multi-party computation,Integer programming; Systems engineering; Authorization; Fingerprint methods; Optimal values; Policies; Private informations; Secure multi-party computation; Trust managements; Optimal systems
A graph based approach toward network forensics analysis,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349109092&doi=10.1145%2f1410234.1410238&partnerID=40&md5=2ca430b83a8056b51b3a8080372e8b4c,"In this article we develop a novel graph-based approach toward network forensics analysis. Central to our approach is the evidence graph model that facilitates evidence presentation and automated reasoning. Based on the evidence graph, we propose a hierarchical reasoning framework that consists of two levels. Local reasoning aims to infer the functional states of network entities from local observations. Global reasoning aims to identify important entities from the graph structure and extract groups of densely correlated participants in the attack scenario. This article also presents a framework for interactive hypothesis testing, which helps to identify the attacker's nonexplicit attack activities from secondary evidence. We developed a prototype system that implements the techniques discussed. Experimental results on various attack datasets demonstrate that our analysis mechanism achieves good coverage and accuracy in attack group and scenario extraction with less dependence on hard-coded expert knowledge. © 2008 ACM.",Evidence graph; Hierarchical reasoning; Network forensics,Automata theory; Attack scenarios; Automated reasonings; Data sets; Evidence graph; Expert knowledges; Functional states; Graph based; Graph models; Graph structures; Hierarchical reasoning; Hierarchical reasonings; Hypothesis testing; Local reasonings; Network forensics; Prototype systems; Various attacks; Graph theory
Status-based access control,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349114982&doi=10.1145%2f1410234.1410235&partnerID=40&md5=27592fbd156aaf229c6f3070b5929bfe,"Despite their widespread adoption, Role-based Access Control (RBAC) models exhibit certain shortcomings that make them less than ideal for deployment in, for example, distributed access control. In the distributed case, standard RBAC assumptions (e.g., of relatively static access policies, managed by human users, with complete information available about users and job functions) do not necessarily apply. Moreover, RBAC is restricted in the sense that it is based on one type of ascribed status, an assignment of a user to a role. In this article, we introduce the status-based access control (SBAC) model for distributed access control. The SBAC model (or family of models) is based on the notion of users having an action status as well as an ascribed status. A user's action status is established, in part, from a history of events that relate to the user; this history enables changing access policy requirements to be naturally accommodated. The approach can be implemented as an autonomous agent that reasons about the events, actions, and a history (of events and actions), which relates to a requester for access to resources, in order to decide whether the requester is permitted the access sought. We define a number of algebras for composing SBAC policies, algebras that exploit the language that we introduce for SBAC policy representation: identification-based logic programs. The SBAC model is richer than RBAC models and the policies that can be represented in our approach are more expressive than the policies admitted by a number of monotonic languages that have been hitherto described for representing distributed access control requirements. Our algebras generalize existing algebras that have been defined for access policy composition. We also describe an approach for the efficient implementation of SBAC policies. © 2008 ACM.",Algebras; Distributed security; Logic; Status-based access control,Algebra; Autonomous agents; Linguistics; Logic programming; Query languages; Security systems; Complete informations; Distributed access controls; Distributed security; Efficient implementations; Human users; Job functions; Logic; Logic programs; Policy compositions; Rbac models; Role-based access controls; Short-comings; Status-based access control; Access control
Distributed and secure bootstrapping of mobile Ad Hoc networks: Framework and constructions,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349109552&doi=10.1145%2f1410234.1410236&partnerID=40&md5=c6959298fedc85e602d05862591d2f2a,"Secure bootstrapping of mobile ad hoc networks (MANETs) is a challenging problem in scenarios in which network users (or nodes) do not share trust relationships prior to the network deployment. In recent years, a number of schemes have been proposed to solve this problem, assuming either no or limited trust between the nodes prior to their deployment. Despite numerous proposals, there is no common understanding of the proposed schemes and of the trade-offs that they provide. This has consequences for both researchers and practitioners, who do not have a clear idea how to compare the schemes and how to select a scheme for a given application. In this article, we present a framework that helps in understanding and comparing schemes for secure bootstrapping of MANETs. The framework is general because it is policy-neutral and can accommodate many existing bootstrapping schemes. The proposed framework can equally serve as a good basis for the development of new MANET bootstrapping schemes; we show how the development of the framework leads to two new (classes of) distributed bootstrapping schemes. Within the framework, we not only investigate and characterize the properties of the relevant bootstrapping schemes, but also give methods for practitioners to select the relevant system parameters in the Random Walk and the (Restricted) Random Waypoint mobility models. © 2008 ACM.",MANETs; Secure communication; Security bootstrapping,Economic and social effects; Mobile security; Network security; Secure communication; Bootstrapping scheme; MANETs; Mobile ad-hoc network (MANETs); Network deployment; Network users; Random Waypoint mobility model; Security bootstrapping; Trust relationship; Mobile ad hoc networks
Secrecy in multiagent systems,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349155009&doi=10.1145%2f1410234.1410239&partnerID=40&md5=1b293b1a3473f00e6bb7c5dbf67134ae,"We introduce a general framework for reasoning about secrecy requirements in multiagent systems. Our definitions extend earlier definitions of secrecy and nondeducibility given by Shannon and Sutherland. Roughly speaking, one agent maintains secrecy with respect to another if the second agent cannot rule out any possibilities for the behavior or state of the first agent. We show that the framework can handle probability and nondeterminism in a clean way, is useful for reasoning about asynchronous systems as well as synchronous systems, and suggests generalizations of secrecy that may be useful for dealing with issues such as resource-bounded reasoning. We also show that a number of well-known attempts to characterize the absence of information flow are special cases of our definitions of secrecy. © 2008 ACM.",Information flow; Secrecy,Computer science; Safety engineering; Asynchronous system; Information flows; Non-determinism; Resource-bounded; Secrecy; Shannon; Synchronous system; Multi agent systems
New multiparty signature schemes for network routing applications,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349146197&doi=10.1145%2f1410234.1410237&partnerID=40&md5=68d490948b2d580b0edd33f69cb14ba7,"We construct two new multiparty digital signature schemes that allow multiple signers to sequentially and non-interactively produce a compact, fixed-length signature. First, we introduce a new primitive that we call ordered multisignature (OMS) scheme, which allows signers to attest to a common message as well as the order in which they signed. Our OMS construction substantially improves computational efficiency and scalability over any existing scheme with suitable functionality. Second, we design a new identity-based sequential aggregate signature scheme, where signers can attest to different messages and signature verification does not require knowledge of traditional public keys. The latter property permits savings on bandwidth and storage as compared to public-key solutions. In contrast to the only prior scheme to provide this functionality, ours offers improved security that does not rely on synchronized clocks or a trusted first signer. We provide formal security definitions and support the proposed schemes with security proofs under appropriate computational assumptions. We focus on applications of our schemes to secure network routing, but we believe that they will find other applications as well. © 2008 ACM.",Aggregate signatures; Digital signatures; Identity-based signatures; Multisignatures; Network security; Pairings,Access control; Agglomeration; Aggregates; Applications; Authentication; Computer networks; Diffractive optical elements; Digital arithmetic; Internet; Network routing; Network security; Aggregate signatures; Digital signatures; Identity-based signatures; Multisignatures; Pairings; Electronic document identification systems
Using first-order logic to reason about policies,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449088504&doi=10.1145%2f1380564.1380569&partnerID=40&md5=412c7c4a90fa3b51a961829b60fba782,"A policy describes the conditions under which an action is permitted or forbidden. We show that a fragment of (multi-sorted) first-order logic can be used to represent and reason about policies. Because we use first-order logic, policies have a clear syntax and semantics. We show that further restricting the fragment results in a language that is still quite expressive yet is also tractable. More precisely, questions about entailment, such as May Alice access the file?, can be answered in time that is a low-order polynomial (indeed, almost linear in some cases), as can questions about the consistency of policy sets. © 2008 ACM.",Digital rights management,Information theory; Digital rights management; First-order logic; Formal logic
Secure time synchronization in sensor networks,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449093129&doi=10.1145%2f1380564.1380571&partnerID=40&md5=ff8944268637d52d0233ed89310b0e8f,"Time synchronization is critical in sensor networks at many layers of their design. It enables better duty-cycling of the radio, accurate and secure localization, beamforming, and other collaborative signal processing tasks. These benefits make time-synchronization protocols a prime target of malicious adversaries who want to disrupt the normal operation of a sensor network. In this article, we analyze attacks on existing time synchronization protocols for wireless sensor networks and we propose a secure time synchronization toolbox to counter these attacks. This toolbox includes protocols for secure pairwise and group synchronization of nodes that either lie in the neighborhood of each other or are separated by multiple hops. We provide an in-depth analysis of the security and the energy overhead of the proposed protocols. The efficiency of these protocols has been tested through an experimental study on Mica2 motes. © 2008 ACM.",Delay; Message authentication code; Sensor networks; Time synchronization,Laws and legislation; Network protocols; Sensors; Signal processing; Synchronization; Wireless sensor networks; Wireless telecommunication systems; Collaborative signal processing; Delay; Experimental studies; In-depth analysis; Message authentication code; Multiple hops; Normal operation; Secure localization; Synchronization protocols; Time synchronization; Sensor networks
Characterization and improvement of time-memory trade-off based on perfect tables,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449086865&doi=10.1145%2f1380564.1380565&partnerID=40&md5=6180890456813d32bd4c2ee2c653e00c,"Cryptanalytic time-memory trade-offs have been studied for 25 years and have benefited from several improvements since the original work of Hellman. The ensuing variants definitely improve the original trade-off but their real impact has never been evaluated in practice. We fill this lack by analyzing the perfect form of classic tables, distinguished point-based tables, and rainbow tables. We especially provide a thorough analysis of the latter variant, whose performances have never been formally calculated yet. Our analysis leads to the concept of a characteristic that enables to measure the intrinsic quality of a trade-off. We finally introduce a new technique based on checkpoints that still reduces the cryptanalysis time by ruling out false alarms probabilistically. Our analysis yields the exact gain of this approach and establishes its efficiency when applied on rainbow tables. © 2008 ACM.",Cryptography; Hellman's time-memory trade-off; Password cracking; Rainbow tables,Alarm systems; Computer hardware; Cryptography; Security of data; False alarms; Hellman's time-memory trade-off; Password cracking; Point-based; Rainbow tables; Time-Memory Trade-Off; Commerce
An analytic framework for modeling and detecting access layer misbehavior in wireless networks,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449096549&doi=10.1145%2f1380564.1380567&partnerID=40&md5=22288d4754e8378a41cdf633a0db767e,"The widespread deployment of wireless networks and hot spots that employ the IEEE 802.11 technology has forced network designers to put emphasis on the importance of ensuring efficient and fair use of network resources. In this work we propose a novel framework for detection of intelligent adaptive adversaries in the IEEE 802.11 MAC by addressing the problem of detection of the worst-case scenario attacks. Utilizing the nature of this protocol we employ sequential detection methods for detecting greedy behavior and illustrate their performance for detection of least favorable attacks. By using robust statistics in our problem formulation, we attempt to utilize the precision given by parametric tests, while avoiding the specification of the adversarial distribution. This approach establishes the lowest performance bound of a given Intrusion Detection System (IDS) in terms of detection delay and is applicable in online detection systems where users who pay for their services want to obtain the information about the best and the worst case scenarios and performance bounds of the system. This framework is meaningful for studying misbehavior due to the fact that it does not focus on specific adversarial strategies and therefore is applicable to a wide class of adversarial strategies. © 2008 ACM.",MAC layer; Min-max robust detection; Protocol misbehavior; Wireless networks,Computer crime; Online systems; Sensors; Signal detection; Standards; Adaptive adversaries; Detection delay; Fair use; Hot spotting; IEEE 802.11 MAC; IEEE 802.11 technology; Intrusion-detection system; MAC layer; Min-max robust detection; Network designers; Network resources; On-line detection; Parametric testing; Performance bounds; Problem formulation; Protocol misbehavior; Robust statistics; Sequential detection; Wireless networks; Worst case; Worst-case scenario; Intrusion detection
Evaluation of intrusion detection systems under a resource constraint,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449088913&doi=10.1145%2f1380564.1380566&partnerID=40&md5=c13b0530b70560e21373e284f8d33700,"An intrusion detection system plays an important role in a firm's overall security protection. Its main purpose is to identify potentially intrusive events and alert the security personnel to the danger. A typical intrusion detection system, however, is known to be imperfect in detection of intrusive events, resulting in high false-alarm rates. Nevertheless, current intrusion detection models unreasonably assume that upon alerts raised by a system, an information security officer responds to all alarms without any delay and avoids damages of hostile activities. This assumption of responding to all alarms with no time lag is often impracticable. As a result, the benefit of an intrusion detection system can be overestimated by current intrusion detection models. In this article, we extend previous models by including an information security officer's alarm inspection under a constraint as a part of the process in determining the optimal intrusion detection policy. Given a potentially hostile environment for a firm, in which the intrusion rates and costs associated with intrusion and security officers' inspection can be estimated, we outline a framework to establish the optimal operating points for intrusion detection systems under security officers' inspection constraint. The optimal solution to the model will provide not only a basis of better evaluation of intrusion detection systems but also useful insights into operations of intrusion detection systems. The firm can estimate expected benefits for running intrusion detection systems and establish a basis for increase in security personnel to relax security officers' inspection constraint. © 2008 ACM.",Computer security; Intrusion detection; Optimal inspection rates; Optimal operating points,Alarm systems; Computer crime; Damage detection; Estimation; Industrial economics; Information services; Inspection; Operating costs; Personnel; Security of data; Sensors; Signal detection; Computer security; False-alarm rates; Hostile environments; Information security; Intrusion detection models; Intrusion detection system; Intrusion Detection Systems; Optimal inspection rates; Optimal operating points; Optimal solutions; Resource constraints; Security officers; Security personnel; Security protection; Time lags; Intrusion detection
Attack-resistant location estimation in wireless sensor networks,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449086367&doi=10.1145%2f1380564.1380570&partnerID=40&md5=b52316ebed03fb74f40f3831c3b297f7,"Many sensor network applications require sensors' locations to function correctly. Despite the recent advances, location discovery for sensor networks in hostile environments has been mostly overlooked. Most of the existing localization protocols for sensor networks are vulnerable in hostile environments. The security of location discovery can certainly be enhanced by authentication. However, the possible node compromises and the fact that location determination uses certain physical features (e.g., received signal strength) of radio signals make authentication not as effective as in traditional security applications. This article presents two methods to tolerate malicious attacks against range-based location discovery in sensor networks. The first method filters out malicious beacon signals on the basis of the consistency among multiple beacon signals, while the second method tolerates malicious beacon signals by adopting an iteratively refined voting scheme. Both methods can survive malicious attacks even if the attacks bypass authentication, provided that the benign beacon signals constitute the majority of the beacon signals. This article also presents the implementation and experimental evaluation (through both field experiments and simulation) of all the secure and resilient location estimation schemes that can be used on the current generation of sensor platforms (e.g., MICA series of motes), including the techniques proposed in this article, in a network of MICAz motes. The experimental results demonstrate the effectiveness of the proposed methods, and also give the secure and resilient location estimation scheme most suitable for the current generation of sensor networks. © 2008 ACM.",Localization; Security; Sensor networks,Access control; Authentication; Estimation; Iterative methods; Location; Mica; Sensors; Silicate minerals; Wireless sensor networks; Wireless telecommunication systems; Beacon signals; Current generation; Experimental evaluations; Field experiments; Hostile environments; Localization; Localization protocols; Location determination; Location discovery; Location estimation; Malicious attacks; Physical features; Radio signals; Received Signal Strength; Security; Security applications; Sensor Network applications; Sensor platforms; Voting schemes; Sensor networks
SDAP: A secure hop-by-hop data aggregation protocol for sensor networks,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449109253&doi=10.1145%2f1380564.1380568&partnerID=40&md5=21e5cc99c1d0a0dec995aba580b3ed63,"Hop-by-hop data aggregation is a very important technique for reducing the communication overhead and energy expenditure of sensor nodes during the process of data collection in a sensor network. However, because individual sensor readings are lost in the per-hop aggregation process, compromised nodes in the network may forge false values as the aggregation results of other nodes, tricking the base station into accepting spurious aggregation results. Here a fundamental challenge is how can the base station obtain a good approximation of the fusion result when a fraction of sensor nodes are compromised? To answer this challenge, we propose SDAP, a Secure Hop-by-hop Data Aggregation Protocol for sensor networks. SDAP is a general-purpose secure data aggregation protocol applicable to multiple aggregation functions. The design of SDAP is based on the principles of divide-and-conquer and commit-and-attest. First, SDAP uses a novel probabilistic grouping technique to dynamically partition the nodes in a tree topology into multiple logical groups (subtrees) of similar sizes. A commitment-based hop-by-hop aggregation is performed in each group to generate a group aggregate. The base station then identifies the suspicious groups based on the set of group aggregates. Finally, each group under suspect participates in an attestation process to prove the correctness of its group aggregate. The aggregate by the base station is calculated over all the group aggregates that are either normal or have passed the attestation procedure. Extensive analysis and simulations show that SDAP can achieve the level of efficiency close to an ordinary hop-by-hop aggregation protocol while providing high assurance on the trustworthiness of the aggregation result. Last, prototype implementation on top of TinyOS shows that our scheme is practical on current generation sensor nodes such as Mica2 motes. © 2008 ACM.",Commit-and-attest; Data aggregation; Hop-by-hop; Probabilistic grouping; Sensor network security,Agglomeration; Aggregates; Base stations; Laws and legislation; Network protocols; Sensor data fusion; Sensors; Telecommunication equipment; Wireless sensor networks; Aggregation functions; Aggregation processing; Commit-and-attest; Communication overheads; Compromised nodes; Data aggregation; Data aggregation protocol; Data collections; Divide-and-conquer; Energy expenditure; Grouping technique; High-assurance; Hop-by-hop; ON currents; Probabilistic grouping; Prototype implementations; Secure data aggregation; Sensor network security; Sensor nodes; Sensor readings; Subtrees; Tree topology; Sensor networks
Verifying completeness of relational query answers from online servers,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049111546&doi=10.1145%2f1330332.1330337&partnerID=40&md5=6aa8fbe3b7be7d723e64544ae25a1e0f,"The number of successful attacks on the Internet shows that it is very difficult to guarantee the security of online servers over extended periods of time. A breached server that is not detected in time may return incorrect query answers to users. In this article, we introduce authentication schemes for users to verify that their query answers from an online server are complete (i.e., no qualifying tuples are omitted) and authentic (i.e., all the result values are legitimate). We introduce a scheme that supports range selection, projection as well as primary key-foreign key join queries on relational databases. We also present authentication schemes for single-and multi-attribute range aggregate queries. The schemes complement access control mechanisms that rewrite queries dynamically, and are computationally secure. We have implemented the proposed schemes, and experiment results showed that they are practical and feasible schemes with low overheads. © 2008 ACM 1094-9224/2008/05-ART9.",Query answer verification; Secure database systems,Access control; Online systems; Overhead lines; Relational database systems; User interfaces; Verification; Access control mechanisms; Query answer verification; Secure database systems; Queueing networks
RIPPS: Rogue Identifying Packet Payload Slicer detecting unauthorized wireless hosts through network traffic conditioning,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049098504&doi=10.1145%2f1330332.1330334&partnerID=40&md5=aa133c7dc2252155727dc688282c8997,"Wireless network access has become an integral part of computing both at home and at the workplace. The convenience of wireless network access at work may be extremely beneficial to employees, but can be a burden to network security personnel. This burden is magnified by the threat of inexpensive wireless access points being installed in a network without the knowledge of network administrators. These devices, termed Rogue Wireless Access Points, may allow a malicious outsider to access valuable network resources, including confidential communication and other stored data. For this reason, wireless connectivity detection is an essential capability, but remains a difficult problem. We present a method of detecting wireless hosts using a local RTT metric and a novel packet payload slicing technique. The local RTT metric provides the means to identify physical transmission media while packet payload slicing conditions network traffic to enhance the accuracy of the detections. Most importantly, the packet payload slicing method is transparent to both clients and servers and does not require direct communication between the monitoring system and monitored hosts. © 2008 ACM.",Network security; Rogue systems; Traffic conditioning,Carrier communication; Intrusion detection; Network security; Telecommunication traffic; Wireless networks; Rogue systems; Traffic conditioning; Wireless hosts; Wireless network access; Packet networks
On the existence of unconditionally privacy-preserving auction protocols,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049106703&doi=10.1145%2f1330332.1330338&partnerID=40&md5=04034d8a2976ec6b1c19dd2a7537b103,"We investigate whether it is possible to preserve privacy in sealed-bid auctions to a maximal extent. In particular, this paper focuses on unconditional full privacy, i.e., privacy that relies neither on trusted third parties (like auctioneers), nor on computational intractability assumptions (like the hardness of factoring). These constraints imply a scenario in which bidders exchange messages according to some predefined protocol in order to jointly determine the auction outcome without revealing any additional information. It turns out that the first-price sealed-bid auction can be emulated by an unconditionally fully private protocol. However, the protocol's round complexity is exponential in the bid size, and there is no more efficient protocol. On the other hand, we prove the impossibility of privately emulating the second-price sealed-bid auction for more than two bidders. This impossibility holds even when relaxing various privacy constraints such as allowing the revelation of all but one losing bid (while maintaining anonymity) or allowing the revelation of the second highest bidder's identity. © 2008 ACM.",Auctions; Multiparty computation,Constraint theory; Electronic commerce; Intrusion detection; Message passing; Network protocols; Auctions; Exchange messages; Multiparty computation; Predefined protocols; Network security
Passive-logging attacks against anonymous communications systems,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049087740&doi=10.1145%2f1330332.1330335&partnerID=40&md5=219b7496484fcb6ab835da9562c7850e,"Using analysis, simulation, and experimentation, we examine the threat against anonymous communications posed by passive-logging attacks. In previous work, we analyzed the success of such attacks under various assumptions. Here, we evaluate the effects of these assumptions more closely. First, we analyze the Onion Routing-based model used in prior work in which a fixed set of nodes remains in the system indefinitely. We show that for this model, by removing the assumption of uniformly random selection of nodes for placement in the path, initiators can greatly improve their anonymity. Second, we show by simulation that attack times are significantly lower in practice than bounds given by analytical results from prior work. Third, we analyze the effects of a dynamic membership model, in which nodes are allowed to join and leave the system; we show that all known defenses fail more quickly when the assumption of a static node set is relaxed. Fourth, intersection attacks against peer-to-peer systems are shown to be an additional danger, either on their own or in conjunction with the predecessor attack. Finally, we address the question of whether the regular communication patterns required by the attacks exist in real traffic. We collected and analyzed the Web requests of users to determine the extent to which basic patterns can be found. We show that, for our study, frequent and repeated communication to the same Web site is common. © 2008 ACM.",Anonymity; Anonymous communication; Intersection attack; Predecessor attack; Privacy,Communication systems; Distributed computer systems; Model checking; Network security; Routing protocols; Static analysis; Anonymous communication; Intersection attacks; Passive-logging attacks; Predecessor attacks; Intrusion detection
Provably secure timed-release public key encryption,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049107631&doi=10.1145%2f1330332.1330336&partnerID=40&md5=dba2498061601ff22f3daef3dd8419cc,"A timed-release cryptosystem allows a sender to encrypt a message so that only the intended recipient can read it only after a specified time. We formalize the concept of a secure timed-release public-key cryptosystem and show that, if a third party is relied upon to guarantee decryption after the specified date, this concept is equivalent to identity-based encryption; this explains the observation that all known constructions use identity-based encryption to achieve timed-release security. We then give several provably-secure constructions of timed-release encryption: a generic scheme based on any identity-based encryption scheme, and two more efficient schemes based on the existence of cryptographically admissible bilinear mappings. The first of these is essentially as efficient as the Boneh-Franklin Identity-Based encryption scheme, and is provably secure and authenticated in the random oracle model; the final scheme is not authenticated but is provably secure in the standard model (i.e., without random oracles). © 2008 ACM.",Authenticated encryption; Key-insulated encryption; Timed-release,Equivalent circuits; Message passing; Model checking; Public key cryptography; Random processes; Authenticated encryption; Bilinear mappings; Key-insulated encryption; Timed-release cryptosystems; Network security
On the construction of practical key predistribution schemes for distributed sensor networks using combinatorial designs,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049091352&doi=10.1145%2f1330332.1330333&partnerID=40&md5=adc1d9520c8231cc5f701c05f9b7ffcd,"In this paper, we discuss the use of combinatorial set systems (combinatorial designs) in the design of key predistribution schemes (KPSs) for sensor networks. We show that the performance of a KPS can be improved by carefully choosing a certain class of set systems as ""key ring spaces"". Especially, we analyze KPSs based on a type of combinatorial design known as a transversal design. We employ two types of transversal designs, which are represented by the set of all linear polynomials and the set of quadratic polynomials (over some finite field), respectively. These KPSs turn out to have significant efficiency in a shared-key discovery phase without degrading connectivity and resiliency. © 2008 ACM.",Key predistribution; Security; Wireless sensor networks,Combinatorial circuits; Distributed computer systems; Linear systems; Phase modulation; Polynomial approximation; combinatorial designs; Key predistribution schemes (KPS); Quadratic polynomials; Shared-key discovery; Wireless sensor networks
ACM Transactions on Information and Systems Security: Editorial,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38949088209&doi=10.1145%2f1330295.1330296&partnerID=40&md5=c31580659d6ce8ea1f06f45e99038bdc,[No abstract available],,
Message dropping attacks in overlay networks: Attack detection and attacker identification,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549123255&doi=10.1145%2f1341731.1341736&partnerID=40&md5=e9c820c3e84007a05d0d011999a5d57f,"Overlay multicast networks are used by service providers to distribute contents such as Web pages, static and streaming multimedia data, or security updates to a large number of users. However, such networks are extremely vulnerable to message-dropping attacks by malicious or selfish nodes that intentionally drop the packets they are required to forward to others. It is difficult to detect such attacks both efficiently and effectively and to further identify the attackers, especially when members in the overlay switch between online/offline statuses frequently. In this article, we consider various attacking strategies of an attacker and propose an optimal sampling-based scheme to detect such attacks in the overlay network. We analyze the detection problem from a game-theoretical viewpoint and show that our scheme outperforms a random sampling-based scheme in terms of detection rate. In addition, based on a reputation system, we propose a sampling-based path-resolving scheme to identify compromised or selfish nodes. Unlike other existing approaches, our schemes do not assume global knowledge of the overlay hierarchy and work for dynamic overlay networks as well. Extensive analysis and simulation results show that besides being band width efficient, our schemes have high detection and identification rates and low false-positive rates. © 2008 ACM.",Attack detection; Attacker identification; Message dropping attacks; Overlay networks,Game theory; Hierarchical systems; Multimedia services; Video streaming; Websites; Wireless sensor networks; Attack detection; Attacker identification; Message dropping attacks; Overlay networks; Computer crime
XACML policy integration algorithms,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38949106870&doi=10.1145%2f1330295.1330299&partnerID=40&md5=0a0754b22ecfc5d6eff1e69f90f143e5,"XACML is the OASIS standard language specifically aimed at the specification of authorization policies. While XACML fits well with the security requirements of a single enterprise (even if large and composed by multiple departments), it does not address the requirements of virtual enterprises in which several autonomous subjects collaborate by sharing their resources to provide better services to customers. In this article we highlight such limitation, and we propose an XACML extension, the policy integration algorithms, to address them. In the article we also present the implementation of a system that makes use of the policy integration algorithms to securely replicate information in a P2P-like environment. In our solution, the data replication process considers the policies specified by both the owners of the data shared and the peers sharing data storage. © 2008 ACM.",Content distributed networks; Distributed systems; Security policies integration; SOA; Web services; XACML,Algorithms; Customer satisfaction; Data storage equipment; Distributed computer systems; Virtual corporation; Web services; Content distributed networks; Data replication process; Standard language; Virtual enterprises; Computer programming languages
ACM Transactions on Information and Systems Security: Editorial,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549166839&doi=10.1145%2f1341731.1341732&partnerID=40&md5=73cfeff4481494fbee5aeb741bb84005,[No abstract available],,
Distributed authentication of program integrity verification in wireless sensor networks,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549139189&doi=10.1145%2f1341731.1341735&partnerID=40&md5=e1915095b59e29f862b01cdec2a13d41,"Security in wireless sensor networks has become important as they are being developed and deployed for an increasing number of applications. The severe resource constraints in each sensor make it very challenging to secure sensor networks. Moreover, sensors are usually deployed in hostile and unattended environments and hence are susceptible to various attacks, including node capture, physical tampering, and manipulation of the sensor program. Park and Shin [2005] proposed a soft tamper-proofing scheme that verifies the integrity of the program in each sensor device, called the program integrity verification (PIV), in which sensors authenticate PIV servers (PIVSs) using centralized and trusted third-party entities, such as authentication servers (ASs). This article presents a distributed authentication protocol of PIVSs (DAPP) without requiring the commonly used ASs. DAPP uses the Blundo scheme [Blundo et al. 1992] for sensors and PIVSs to establish pairwise keys and for PIVSs to authenticate one another. We also present a protocol for PIVSs to cooperatively detect and revoke malicious PIVSs in the network. We implement and evaluate both DAPP and PIV on Mica2 Motes and laptops, showing that DAPP reduces the sensors' communication traffic in the network by more than 90% and the energy consumption on each sensor by up to 85%, as compared to the case of using a centralized AS for authenticating PIVSs. We also analyze the security of DAPP under various attack models, demonstrating its capability in dealing with diverse types of attacks. © 2008 ACM.",Distributed authentication; Node revocation; Program integrity verification; Wireless sensor networks,Authentication; Constraint theory; Mathematical models; Security of data; Servers; Telecommunication traffic; Verification; Distributed authentication; Node revocation; Program integrity verification; Wireless sensor networks
A framework for identifying compromised nodes in wireless sensor networks,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549167232&doi=10.1145%2f1341731.1341733&partnerID=40&md5=182bb528ec6a23f1a07eca4d8c6ce93d,"Sensor networks are often subject to physical attacks. Once a node's cryptographic key is compromised, an attacker may completely impersonate it and introduce arbitrary false information into the network. Basic cryptographic mechanisms are often not effective in this situation. Most techniques to address this problem focus on detecting and tolerating false information introduced by compromised nodes. They cannot pinpoint exactly where the false information is introduced and who is responsible for it. In this article, we propose an application-independent framework for accurately identifying compromised sensor nodes. The framework provides an appropriate abstraction of application-specific detection mechanisms and models the unique properties of sensor networks. Based on the framework, we develop alert reasoning algorithms to identify compromised nodes. The algorithm assumes that compromised nodes may collude at will. We show that our algorithm is optimal in the sense that it identifies the largest number of compromised nodes without introducing false positives. We evaluate the effectiveness of the designed algorithm through comprehensive experiments. © 2008 ACM.",Intrusion detection; Sensor networks,Algorithms; Information analysis; Intrusion detection; Problem solving; Public key cryptography; Application-independent framework; False information; Reasoning algorithms; Wireless sensor networks
The traust authorization Service,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38949188899&doi=10.1145%2f1330295.1330297&partnerID=40&md5=6501f8b9404f14b74984c515af2a9057,"In recent years, trust negotiation has been proposed as a novel authorization solution for use in open-system environments, in which resources are shared across organizational boundaries. Researchers have shown that trust negotiation is indeed a viable solution for these environments by developing a number of policy languages and strategies for trust negotiation that have desirable theoretical properties. Further, existing protocols, such as TLS, have been altered to interact with prototype trust negotiation systems, thereby illustrating the utility of trust negotiation. Unfortunately, modifying existing protocols is often a time-consuming and bureaucratic process that can hinder the adoption of this promising technology. In this paper, we present Traust, a third-party authorization service that leverages the strengths of existing prototype trust negotiation systems. Traust acts as an authorization broker that issues access tokens for resources in an open system after entities use trust negotiation to satisfy the appropriate resource access policies. The Traust architecture was designed to allow Traust to be integrated either directly with newer trust-aware applications or indirectly with existing legacy applications; this flexibility paves the way for the incremental adoption of trust negotiation technologies without requiring widespread software or protocol upgrades. We discuss the design and implementation of Traust, the communication protocol used by the Traust system, and its performance. We also discuss our experiences using Traust to broker access to legacy resources, our proposal for a Traust-aware version of the GridFTP protocol, and Traust's resilience to attack. © 2008 ACM.",Attribute-based access control; Credentials; Trust negotiation,Access control; Computer software; Network protocols; Open systems; Strategic planning; Policy languages; Trust negotiation systems; Information services
Redoubtable sensor networks,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549165648&doi=10.1145%2f1341731.1341734&partnerID=40&md5=9da38c06f7d19f88bc54b4a83e840a58,"We give, for the first time, a precise mathematical analysis of the connectivity and security properties of sensor networks that make use of the random predistribution of keys. We also show how to set the parameters - -pool and key ring size - -in such a way that the network is not only connected with high probability via secure links but also provably resilient, in the following sense: We formally show that any adversary that captures sensors at random with the aim of compromising a constant fraction of the secure links must capture at least a constant fraction of the nodes of the network. In the context of wireless sensor networks where random predistribution of keys is employed, we are the first to provide a mathematically precise proof, with a clear indication of parameter choice, that two crucial properties - -connectivity via secure links and resilience against malicious attacks - -can be obtained simultaneously. We also show in a mathematically rigorous way that the network enjoys another strong security property. The adversary cannot partition the network into two linear size components, compromising all the links between them, unless it captures linearly many nodes. This implies that the network is also fault tolerant with respect to node failures. Our theoretical results are complemented by extensive simulations that reinforce our main conclusions. © 2008 ACM.",Connectivity; Probabilistic key sharing; Random graphs; Wireless sensor network,Computer crime; Computer simulation; Computer system recovery; Parameter estimation; Security of data; Connectivity; Probabilistic key sharing; Random graphs; Wireless sensor networks
Noninvasive methods for host certification,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549140066&doi=10.1145%2f1341731.1341737&partnerID=40&md5=f404164c6435ee17b54ebd7e37323811,"Determining whether a user or system is exercising appropriate security practices is difficult in any context. Such difficulties are particularly pronounced when uncontrolled or unknown platforms join public networks. Commonly practiced techniques used to vet these hosts, such as system scans, have the potential to infringe on the privacy of users. In this article, we show that it is possible for clients to prove both the presence and proper functioning of security infrastructure without allowing unrestricted access to their system. We demonstrate this approach, specifically applied to antivirus security, by requiring clients seeking admission to a network to positively identify the presence or absence of malcode in a series of puzzles. The implementation of this mechanism and its application to real networks are also explored. In so doing, we demonstrate that it is not necessary for an administrator to be invasive to determine whether a client implements required security practices. © 2008 ACM.",Assurance; Certification; Malware; Network security,Client server computer systems; Computer crime; Host certification; Public networks; System scans; Security of data
Toward a usage-based security framework for collaborative computing systems,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38949184514&doi=10.1145%2f1330295.1330298&partnerID=40&md5=35dc3f2e402cb4d8e25e2e6ef35d8f25,"Collaborative systems such as Grids provide efficient and scalable access to distributed computing capabilities and enable seamless resource sharing between users and platforms. This heterogeneous distribution of resources and the various modes of collaborations that exist between users, virtual organizations, and resource providers require scalable, flexible, and fine-grained access control to protect both individual and shared computing resources. In this article we propose a usage control (UCON) based security framework for collaborative applications, by following a layered approach with policy, enforcement, and implementation models, called the PEI framework. In the policy model layer, UCON policies are specified with predicates on subject and object attributes, along with system attributes as conditional constraints and user actions as obligations. General attributes include not only persistent attributes such as role and group memberships but also mutable usage attributes of subjects and objects. Conditions in UCON can be used to support context-based authorizations in ad hoc collaborations. In the enforcement model layer, our novel framework uses a hybrid approach for subject attribute acquisition with both push and pull modes. By leveraging attribute propagations between a centralized attribute repository and distributed policy decision points, our architecture supports decision continuity and attribute mutability of the UCON policy model, as well as obligation evaluations during policy enforcement. As a proof-of-concept, we implement a prototype system based on our proposed architecture and conduct experimental studies to demonstrate the feasibility and performance of our approach. © 2008 ACM.",Access control; Authorization; Collaborative computing; Security architecture; UCON; Usage control,Access control; Computer supported cooperative work; Resource allocation; Security systems; User interfaces; Collaborative systems; Security architecture; Usage control; Virtual organizations; Distributed computer systems
Controlled physical random functions and applications,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149145168&doi=10.1145%2f1284680.1284683&partnerID=40&md5=5aab11c81800e04d1797190b2d7c97d7,"The cryptographic protocols that we use in everyday life rely on the secure storage of keys in consumer devices. Protecting these keys from invasive attackers, who open a device to steal its key, is a challenging problem. We propose controlled physical random functions (CPUFs) as an alternative to storing keys and describe the core protocols that are needed to use CPUFs. A physical random functions (PUF) is a physical system with an input and output. The functional relationship between input and output looks like that of a random function. The particular relationship is unique to a specific instance of a PUF, hence, one needs access to a particular PUF instance to evaluate the function it embodies. The cryptographic applications of a PUF are quite limited unless the PUF is combined with an algorithm that limits the ways in which the PUF can be evaluated; this is a CPUF. A major difficulty in using CPUFs is that you can only know a small set of outputs of the PUF - -the unknown outputs being unrelated to the known ones. We present protocols that get around this difficulty and allow a chain of trust to be established between the CPUF manufacturer and a party that wishes to interact securely with the PUF device. We also present some elementary applications, such as certified execution. © 2008 ACM.",Certified execution; Physical random function; Physical security; Physical unclonable function; Trusted computing,Access control; Consumer products; Cryptography; Network protocols; Problem solving; Certified execution; Consumer devices; Controlled physical random functions (CPUFs); Physical security; Physical unclonable function; Trusted computing; Control equipment
Dynamic access-control policies on XML encrypted data,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149103087&doi=10.1145%2f1284680.1284684&partnerID=40&md5=d447ac244cbe113b32f05d53b2e9ceb7,"The erosion of trust put in traditional database servers and in Database Service Providers and the growing interest for different forms of selective data dissemination are different factors that lead to move the access-control from servers to clients. Different data encryption and key dissemination schemes have been proposed to serve this purpose. By compiling the access-control rules into the encryption process, all these methods suffer from a static way of sharing data. With the emergence of hardware security elements on client devices, more dynamic client-based access-control schemes can be devised. This paper proposes a tamper-resistant client-based XML access-right controller supporting flexible and dynamic access-control policies. The access-control engine is embedded in a hardware-secure device and, therefore, must cope with specific hardware resources. This engine benefits from a dedicated index to quickly converge toward the authorized parts of a potentially streaming XML document. Pending situations (i.e., where data delivery is conditioned by predicates, which apply to values encountered afterward in the document stream) are handled gracefully, skipping, whenever possible the pending elements and reassembling relevant parts when the pending situation is solved. Additional security mechanisms guarantee that (1) the input document is protected from any form of tampering and (2) no forbidden information can be gained by replay attacks on different versions of the XML document and of the access-control rules. Performance measurements on synthetic and real datasets demonstrate the effectiveness of the approach. Finally, the paper reports on two experiments conducted with a prototype running on a secured hardware platform. © 2008 ACM.",Access-control; Data confidentiality; Smartcard; Ubiquitous data management,Data privacy; Information management; Problem oriented languages; Safety devices; Servers; XML; Access-control policies; Data confidentiality; Smartcards; Ubiquitous data management; Access control
ODSBR: An on-demand secure Byzantine resilient routing protocol for wireless ad hoc networks,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149117709&doi=10.1145%2f1284680.1341892&partnerID=40&md5=5c99f8903c29aa5d99ffed02fd7dd188,"Ah hoc networks offer increased coverage by using multihop communication. This architecture makes services more vulnerable to internal attacks coming from compromised nodes that behave arbitrarily to disrupt the network, also referred to as Byzantine attacks. In this work, we examine the impact of several Byzantine attacks performed by individual or colluding attackers. We propose ODSBR, the first on-demand routing protocol for ad hoc wireless networks that provides resilience to Byzantine attacks caused by individual or colluding nodes. The protocol uses an adaptive probing technique that detects a malicious link after log n faults have occurred, where n is the length of the path. Problematic links are avoided by using a route discovery mechanism that relies on a new metric that captures adversarial behavior. Our protocol never partitions the network and bounds the amount of damage caused by attackers. We demonstrate through simulations ODSBR's effectiveness in mitigating Byzantine attacks. Our analysis of the impact of these attacks versus the adversary's effort gives insights into their relative strengths, their interaction, and their importance when designing multihop wireless routing protocols. © 2008 ACM.",Ad hoc wireless networks; Byzantine failures; On-demand routing; Security,Computer architecture; Computer crime; Computer system recovery; Routing protocols; Security of data; Adaptive probing techniques; Byzantine failures; Byzantine resilient routing protocols; On-demand routing; Wireless ad hoc networks; Ad hoc networks
On predictive models and user-drawn graphical passwords,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149106769&doi=10.1145%2f1284680.1284685&partnerID=40&md5=6c5fff37d13ab5a70892aeacd3ff1065,"In commonplace text-based password schemes, users typically choose passwords that are easy to recall, exhibit patterns, and are thus vulnerable to brute-force dictionary attacks. This leads us to ask whether other types of passwords (e.g., graphical) are also vulnerable to dictionary attack because of users tending to choose memorable passwords. We suggest a method to predict and model a number of such classes for systems where passwords are created solely from a user's memory. We hypothesize that these classes define weak password subspaces suitable for an attack dictionary. For user-drawn graphical passwords, we apply this method with cognitive studies on visual recall. These cognitive studies motivate us to define a set of password complexity factors (e.g., reflective symmetry and stroke count), which define a set of classes. To better understand the size of these classes and, thus, how weak the password subspaces they define might be, we use the ""Draw-A-Secret"" (DAS) graphical password scheme of Jermyn et al. [1999] as an example. We analyze the size of these classes for DAS under convenient parameter choices and show that they can be combined to define apparently popular subspaces that have bit sizes ranging from 31 to 41-a surprisingly small proportion of the full password space (58 bits). Our results quantitatively support suggestions that user-drawn graphical password systems employ measures, such as graphical password rules or guidelines and proactive password checking. © 2008 ACM.",Dictionary attack; Draw-a-Secret; Graphical dictionary; Graphical passwords; Memorable passwords; Modeling user choice; Password complexity factors,Cognitive systems; Computer crime; Computer privacy; Computer simulation; Glossaries; Graphical user interfaces; Dictionary attacks; Draw-a-Secret; Graphical passwords; Memorable passwords; Modeling user choice; Password complexity factors; Visual recalls; Codes (symbols)
Formal foundations for hybrid hierarchies in GTRBAC,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149138668&doi=10.1145%2f1284680.1284682&partnerID=40&md5=bd3c92d955ca908919124c210ad3ddf5,"A role hierarchy defines permission acquisition and role-activation semantics through role - role relationships. It can be utilized for efficiently and effectively structuring functional roles of an organization having related access-control needs. The focus of this paper is the analysis of hybrid role hierarchies in the context of the generalized temporal role-based access control (GTRBAC) model that allows specification of a comprehensive set of temporal constraints on role, user-role, and role-permission assignments. We introduce the notion of uniquely activable set (UAS) associated with a role hierarchy that indicates the access capabilities of a user resulting from his membership to a role in the hierarchy. Identifying such a role set is essential, while making an authorization decision about whether or not a user should be allowed to activate a particular combination of roles in a single session. We formally show how UAS can be determined for a hybrid hierarchy. Furthermore, within a hybrid hierarchy, various hierarchical relations may be derived between an arbitrary pair of roles. We present a set of inference rules that can be used to generate all the possible derived relations that can be inferred from a specified set of hierarchical relations and show that it is sound and complete. We also present an analysis of hierarchy transformations with respect to role addition, deletion, and partitioning, and show how various cases of these transformations allow the original permission acquisition and role-activation semantics to be managed. The formal results presented here provide a basis for developing efficient security administration and management tools. © 2008 ACM.",Derived relation; Role hierarchy,Access control; General purpose computers; Hybrid computers; Semantics; User interfaces; Derived relations; GTRBAC; Hybrid hierarchies; Role hierarchy; Uniquely activable set; Hierarchical systems
Epidemic thresholds in real networks,2008,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39349087037&doi=10.1145%2f1284680.1284681&partnerID=40&md5=66923cad633829921fba2723533162c6,"How will a virus propagate in a real network How long does it take to disinfect a network given particular values of infection rate and virus death rate What is the single best node to immunize Answering these questions is essential for devising network-wide strategies to counter viruses. In addition, viral propagation is very similar in principle to the spread of rumors, information, and fads, implying that the solutions for viral propagation would also offer insights into these other problem settings. We answer these questions by developing a nonlinear dynamical system (NLDS) that accurately models viral propagation in any arbitrary network, including real and synthesized network graphs. We propose a general epidemic threshold condition for the NLDS system: we prove that the epidemic threshold for a network is exactly the inverse of the largest eigenvalue of its adjacency matrix. Finally, we show that below the epidemic threshold, infections die out at an exponential rate. Our epidemic threshold model subsumes many known thresholds for special-case graphs (e.g., Erdös - Rényi, BA powerlaw, homogeneous). We demonstrate the predictive power of our model with extensive experiments on real and synthesized graphs, and show that our threshold condition holds for arbitrary graphs. Finally, we show how to utilize our threshold condition for practical uses: It can dictate which nodes to immunize; it can assess the effects of a throttling policy; it can help us design network topologies so that they are more resistant to viruses. © 2008 ACM.",Eigenvalue; Epidemic threshold; Viral propagation,Computer viruses; Dynamical systems; Graph theory; Information retrieval; Nonlinear dynamical system (NLDS); Real network; Threshold logic
Just fast keying in the pi calculus,2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547517142&doi=10.1145%2f1266977.1266978&partnerID=40&md5=e9194ce790a7c7bed5b2010d9414182b,"JFK is a recent, attractive protocol for fast key establishment as part of securing IP communication. In this paper, we formally analyze this protocol in the applied pi calculus (partly in terms of observational equivalences and partly with the assistance of an automatic protocol verifier). We treat JFK's core security properties and also other properties that are rarely articulated and rigorously studied, such as plausible deniability and resistance to denial-of-service attacks. In the course of this analysis, we found some ambiguities and minor problems, such as limitations in identity protection, but we mostly obtain positive results about JFK. For this purpose, we develop ideas and techniques that should be more generally useful in the specification and verification of security protocols. © 2007 ACM.",IP security; Key exchange; Process calculus,Automation; Computer crime; Internet protocols; Problem solving; IP security; Key exchange; Process calculus; Security of data
PP-trust-X: A system for privacy preserving trust negotiations,2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547505017&doi=10.1145%2f1266977.1266981&partnerID=40&md5=35d69d80fb36f06f6b62b74bf797d8d6,"Trust negotiation is a promising approach for establishing trust in open systems, in which sensitive interactions may often occur between entities with no prior knowledge of each other. Although, to date several trust negotiation systems have been proposed, none of them fully address the problem of privacy preservation. Today, privacy is one of the major concerns of users when exchanging information through the Web and thus we believe that trust negotiation systems must effectively address privacy issues in order to be widely applicable. For these reasons, in this paper, we investigate privacy in the context of trust negotiations. We propose a set of privacy-preserving features for inclusion in any trust negotiation system, such as the support for the P3P standard, as well as a number of innovative features, such as a novel format for encoding digital credentials specifically designed for preserving privacy. Further, we present a variety of interoperable strategies to carry on the negotiation with the aim of improving both privacy and efficiency. © 2007 ACM.",Access control; Attribute-based access control; Automated trust negotiation; Credentials; Privacy; Strategy,Access control; Computational efficiency; Data privacy; Innovation; Problem solving; Standards; Attribute-based access control; Automated trust negotiation; Credentials; Strategy; Information systems
Provably secure authenticated group Diffie-Hellman key exchange,2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547502275&doi=10.1145%2f1266977.1266979&partnerID=40&md5=8a877024fbebf3d577f6a1629bae82c9,"Authenticated key-exchange protocols allow two participants A and B, communicating over a public network and each holding an authentication means to exchange a shared secret value. Methods designed to deal with this cryptographic problem ensure A (resp. B) that no other participants aside from B (resp. A) can learn any information about the agreed value and often also ensure A and B that their respective partner has actually computed this value. A natural extension to this cryptographic method is to consider a pool of participants exchanging a shared secret value and to provide a formal treatment for it. Starting from the famous two-party Diffie - Hellman (DH) key-exchange protocol and from its authenticated variants, security experts have extended it to the multiparty setting for over a decade and, in the past few years, completed a formal analysis in the framework of modern cryptography. The present paper synthesizes this body of work on the provably-secure authenticated group DH key exchange. © 2007 ACM.",Cryptography; Diffie-Hellman; Group key exchange,Authentication; Cryptography; Network protocols; Problem solving; Group key exchange; Natural extension; Secret values; Information systems
On interdomain routing security and pretty secure BGP (psBGP),2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547504417&doi=10.1145%2f1266977.1266980&partnerID=40&md5=c0d6818aa99e7287670ff549b3e5dfff,"It is well known that the Border Gateway Protocol (BGP), the IETF standard interdomain routing protocol, is vulnerable to a variety of attacks, and that a single misconfigured or malicious BGP speaker could result in large-scale service disruption. In this paper, we present Pretty Secure BGP (psBGP) - -a proposal for securing BGP, including an architectural overview, design details for significant aspects, and preliminary security and operational analysis. psBGP differs from other security proposals (e.g., S-BGP and soBGP) in that it makes use of a single-level PKI for AS number authentication, a decentralized trust model for verifying the propriety of IP prefix origin, and a rating-based stepwise approach for AS_PATH (integrity) verification. psBGP trades off the strong security guarantees of S-BGP for presumed-simpler operation, e.g., using a PKI with a simple structure, with a small number of certificate types, and of manageable size. psBGP is designed to successfully defend against various (nonmalicious and malicious) threats from uncoordinated BGP speakers, and to be incrementally deployed with incremental benefits. © 2007 ACM.",Authentication; BGP; Certificates; Interdomain routing; Public-key infrastructure; Secure routing protocols; Trust,Authentication; Computer crime; Mathematical models; Network architecture; Certificates; Interdomain routing; Public-key infrastructure; Secure routing protocols; Routing protocols
Batch zero-knowledge proof and verification and its applications,2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249734850&doi=10.1145%2f1237500.1237502&partnerID=40&md5=886d9bd9c0c432aa52cd1f8891a3b328,"The batch verification technique of Bellare et al. is extended to verification of several frequently employed zero-knowledge proofs. The new techniques are correct, sound, efficient, and can be widely applied. Specific applications are discussed in detail, including batch ZK proof and verification of validity of encryption (or reencryption) and batch ZK proof and verification of validity of decryption. Considerable efficiency improvements are gained in these two applications without compromising security. As a result, efficiency of the practical cryptographic systems (such as mix networks) based on these two applications is dramatically improved. © 2007 ACM.",Batch proof and verification of decryption; Batch proof and verification of reencryption; Mix network,Cryptography; Security systems; Verification; Batch proof; Mix network; Verification of decryption; Verification of reencryption; Security of data
Specification and verification of security requirements in a programming model for decentralized CSCW systems,2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249723796&doi=10.1145%2f1237500.1237503&partnerID=40&md5=9e8ebe9a91b6504997ccdc8aea88efee,"We present, in this paper, a role-based model for programming distributed CSCW systems. This model supports specification of dynamic security and coordination requirements in such systems. We also present here a model-checking methodology for verifying the security properties of a design expressed in this model. The verification methodology presented here is used to ensure correctness and consistency of a design specification. It is also used to ensure that sensitive security requirements cannot be violated when policy enforcement functions are distributed among the participants. Several aspect-specific verification models are developed to check security properties, such as task-flow constraints, information flow, confidentiality, and assignment of administrative privileges. © 2007 ACM.",Finite state-based model checking; Methodology for access-control policy design; Role-based access control; Security policy specification,Access control; Data flow analysis; Mathematical models; Model checking; Public policy; Access-control policy design; Dynamic security; Role based access control; Security policy specification; Task flow constraints; Computer programming
On mutually exclusive roles and separation-of-duty,2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249745790&doi=10.1145%2f1237500.1237501&partnerID=40&md5=8259d17e0bbb21bce9a38ffa87659a07,"Separation-of-duty (SoD) is widely considered to be a fundamental principle in computer security. A static SoD (SSoD) policy states that in order to have all permissions necessary to complete a sensitive task, the cooperation of at least a certain number of users is required. Role-based access control (RBAC) is today's dominant access-control model. It is widely believed that one of RBAC's main strengths is that it enables the use of constraints to support policies, such as separation-of-duty. In the literature on RBAC, statically mutually exclusive roles (SMER) constraints are used to enforce SSoD policies. In this paper, we formulate and study fundamental computational problems related to the use of SMER constraints to enforce SSoD policies. We show that directly enforcing SSoD policies is intractable (coNP-complete), while checking whether an RBAC state satisfies a set of SMER constraints is efficient; however, verifying whether a given set of SMER constraints enforces an SSoD policy is also intractable (coNP-complete). We discuss the implications of these results. We show also how to generate SMER constraints that are as accurate as possible for enforcing an SSoD policy. © 2007 ACM.",Computational complexity; constraints; Role-based access control; separation-of-duty; verification,Computational complexity; Constraint theory; Mathematical models; Verification; Role-based access control; Separation-of-duty; Statically mutually exclusive roles (SMER); Security of data
Secure sessions for Web services,2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249658491&doi=10.1145%2f1237500.1237504&partnerID=40&md5=7732a7e8b59ccce5b1d0ba80f81ef4cc,"We address the problem of securing sequences of SOAP messages exchanged between web services and their clients. The WS-Security standard defines basic mechanisms to secure SOAP traffic, one message at a time. For typical web services, however, using WS-Security independently for each message is rather inefficient; moreover, it is often important to secure the integrity of a whole session, as well as each message. To these ends, recent specifications provide further SOAP-level mechanisms. WS-SecureConversation defines security contexts, which can be used to secure sessions between two parties. WS-Trust specifies how security contexts are issued and obtained. We develop a semantics for the main mechanisms of WS-Trust and WS-SecureConversation, expressed as a library for TulaFale, a formal scripting language for security protocols. We model typical protocols relying on these mechanisms and automatically prove their main security properties. We also informally discuss some pitfalls and limitations of these specifications. © 2007 ACM.",Web services; XML security,Formal languages; Network protocols; Security of data; Semantics; XML; Formal scripting language; Security protocols; WS-SecureConversation; XML security; Web services
Relevancy-based access control and its evaluation on versioned XML documents,2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847109290&doi=10.1145%2f1210263.1210266&partnerID=40&md5=b5ec76252677b50d6e58e41ffd3cad90,"Integration of version and access control of XML documents has the benefit of regulating access to rapidly growing archives of XML documents. Versioned XML documents provide us with valuable information on dependencies between document nodes, but, at the same time, presenting the risk of undesirable data disclosure. In this article, we introduce the notion of relevancy-based access control, which realizes protection of versioned XML documents by various types of relevancy, such as version dependencies, schema similarities, and temporal proximity. We define a new path query language XVerPath over XML document versions, which can be utilized for specifying relevancy-based access-control policies. We also introduce the notion of relevancy class, for collectively and compactly specifying relevancy-based policies. Regarding efficient processing of access requests, we propose the packed version model, which realizes space-efficient difference-based archives of versioned XML documents and, at the same time, providing efficient evaluation of XVerPath queries. Experimental results show reasonable performance superiority over conventional methods, which do not utilize version differences. © 2007 ACM.",Access control; Query language; Security; Version control; XML; XPath,Computational methods; Data privacy; Information dissemination; Query languages; XML; Access control; Documents; Nodes; Security of data
Guest editorial: Special issue on access control models and technologies,2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847171378&doi=10.1145%2f1210263.1216576&partnerID=40&md5=33d3f7c7da94b5fa39ad90fd69c4cddf,[No abstract available],,
GEO-RBAC: A spatially aware RBAC,2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847092871&doi=10.1145%2f1210263.1210265&partnerID=40&md5=e8ebd35b1fcb4591e990aeb07bab8262,"Securing access to data in location-based services and mobile applications requires the definition of spatially aware access-control systems. Even if some approaches have already been proposed either in the context of geographic database systems or context-aware applications, a comprehensive framework, general and flexible enough to deal with spatial aspects in real mobile applications, is still missing. In this paper, we make one step toward this direction and present GEO-RBAC, an extension of the RBAC model enhanced with spatial-and location-based information. In GEORBAC, spatial entities are used to model objects, user positions, and geographically bounded roles. Roles are activated based on the position of the user. Besides a physical position, obtained from a given mobile terminal or a cellular phone, users are also assigned a logical and device-independent position, representing the feature (the road, the town, the region) in which they are located. To enhance flexibility and reusability, we also introduce the concept of role schema, specifying the name of the role, as well as the type of the role spatial boundary and the granularity of the logical position. We then extend GEO-RBAC to support hierarchies, modeling permission, user, and activation inheritance, and separation of duty constraints. The proposed classes of constraints extend the conventional ones to deal with different granularities (schema/instance level) and spatial information. We conclude the paper with an analysis of several properties concerning the resulting model. © 2007 ACM.",Access-control model; GIS; Location-based services,Cellular telephone systems; Control systems; Database systems; Geographic information systems; Mathematical models; Telecommunication services; Access control model; Geographic database systems; Mobile applications; Security of data
Modeling network intrusion detection alerts for correlation,2007,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847137650&doi=10.1145%2f1210263.1210267&partnerID=40&md5=ddb154058f74acfb1821945d42a5b1e5,"Signature-based network intrusion-detection systems (NIDSs) often report a massive number of simple alerts of low-level security-related events. Many of these alerts are logically involved in a single multi-stage intrusion incident and a security officer often wants to analyze the complete incident instead of each individual simple alert. This paper proposes a well-structured model that abstracts the logical relation between the alerts in order to support automatic correlation of those alerts involved in the same intrusion. The basic building block of the model is a logical formula called a capability. We use capability to abstract consistently and precisely all levels of accesses obtained by the attacker in each step of a multistage intrusion. We then derive inference rules to define logical relations between different capabilities. Based on the model and the inference rules, we have developed several novel alert correlation algorithms and implemented a prototype alert correlator. The experimental results of the correlator using several intrusion datasets demonstrate that the approach is effective in both alert fusion and alert correlation and has the ability to correlate alerts of complex multistage intrusions. In several instances, the alert correlator successfully correlated more than two thousand Snort alerts involved in massive scanning incidents. It also helped us find two multistage intrusions that were missed in auditing by the security officers. © 2007 ACM.",Alert correlation; Alert fusion; Capability; Intrusion detection,Computer system firewalls; Correlation methods; Inference engines; Mathematical models; Security of data; Datasets; Inference rules; Network intrusion detection systems (NIDS); Computer crime
Controlled and cooperative updates of XML documents in byzantine and failure-prone distributed systems,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845524696&doi=10.1145%2f1187441.1187443&partnerID=40&md5=0dbbb4c67cbfb000b99d4c8e081dd7ac,"This paper proposes an infrastructure and related algorithms for the controlled and cooperative updates of XML documents. Key components of the proposed system are a set of XML-based languages for specifying access-control policies and the path that the document must follow during its update. Such path can be fully specified before the update process begins or can be dynamically modified by properly authorized subjects while being transmitted. Our approach is fully distributed in that each party involved in the process can verify the correctness of the operations performed until that point on the document without relying on a central authority. More importantly, the recovery procedure also does not need the participation of a central authority. Our approach is based on the use of some special control information that is transmitted together with the document and a suite of protocols. We formally specify the structure of such control information and the protocols. We also analyze security and complexity of the proposed protocols. © 2006 ACM.",Byzantine and distributed systems; Policy languages; Updates; XML documents,Algorithms; Computational complexity; Computer system recovery; Network protocols; Program documentation; XML; Byzantine and distributed systems; Cooperative updates; Policy languages; XML documents; Distributed computer systems
Improved efficiency for revocation schemes via Newton interpolation,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845538766&doi=10.1145%2f1187441.1187444&partnerID=40&md5=aca2d8063d03cdb0c1c562f441e5b608,"We present a novel way to implement the secret-sharing-based family of revocation schemes of Naor and Pinkas [2003]. The basic scheme of [Naor and Pinkas 2000] uses Shamir's polynomial secret-sharing to revoke up to r users, where r is the degree of the secret-sharing polynomial, and it is information theoretically secure against coalitions of up to r collaborators. The nonrevoked users use Lagrange interpolation in order to compute the new key. Our basic scheme uses a novel modification of Shamir's polynomial secret-sharing: The secret equals the leading coefficient of the polynomial (as opposed to the free coefficient as in the original scheme) and the polynomial is reconstructed by Newton interpolation (rather than Lagrange interpolation). Comparing our scheme to one variant of the Naor - Pinkas scheme, we offer revocation messages that are shorter by a factor of almost 2, while the computation cost at the user end is smaller by a constant factor of approximately 13/2. Comparing to a second variant of the Naor - Pinkas scheme, our scheme offers a reduction of O(r) in the computation cost at the user end, without affecting any of the other performance parameters. We then extend our basic scheme to perform multiround revocation for stateless and stateful receivers, along the lines offered by Naor and Pinkas [2000] and Kogan et al. [2003]. We show that using Newton rather than Lagrange interpolants enables a significantly more efficient transmission of the new revocation message and shorter response time for each round. Pay TV systems that implement broadcast encryption techniques can benefit significantly from the improved efficiency offered by our revocation schemes. © 2006 ACM.",Broadcast encryption; Newton interpolation; Secret sharing; User revocation,Broadcasting; Computational methods; Cryptography; Interpolation; Parameter estimation; Polynomials; Broadcast encryption; Newton interpolation; Secret sharing; User revocation; Security of data
Security analysis in role-based access control,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845517298&doi=10.1145%2f1187441.1187442&partnerID=40&md5=ea8a7f5a0c1d215bee4012aeb61ad795,"The administration of large role-based access control (RBAC) systems is a challenging problem. In order to administer such systems, decentralization of administration tasks by the use of delegation is an effective approach. While the use of delegation greatly enhances flexibility and scalability, it may reduce the control that an organization has over its resources, thereby diminishing a major advantage RBAC has over discretionary access control (DAC). We propose to use security analysis techniques to maintain desirable security properties while delegating administrative privileges. We give a precise definition of a family of security analysis problems in RBAC, which is more general than safety analysis that is studied in the literature. We show that two classes of problems in the family can be reduced to similar analysis in the RT[↞∩] role-based trust-management language, thereby establishing an interesting relationship between RBAC and the RT framework. The reduction gives efficient algorithms for answering most kinds of queries in these two classes and establishes the complexity bounds for the intractable cases. © 2006 ACM.",Delegation; Role-based access control; Role-based administration; Trust management,Algorithms; Boundary value problems; Computational complexity; Problem solving; Query languages; Security of data; Discretionary access control (DAC); Role-based access control (RBAC); Role-based administration; Trust-management languages; Information retrieval systems
An effective role administration model using organization structure,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748319570&doi=10.1145%2f1151414.1151415&partnerID=40&md5=4609b4f9c811059d3ac51dfaba47c1c0,"Role-based access control (RBAC) is a well-accepted model for access control in an enterprise environment. When we apply RBAC model to large enterprises, effective role administration is a major issue. ARBAC97 is a well-known solution for decentralized RBAC administration. ARBAC97 authorizes administrative roles by means of role ranges and prerequisite conditions, where prerequisite conditions effectively work as a restricted pool for administrative roles to pick users or permissions. Although attractive and elegant in their own right, these mechanisms have significant shortcomings. In this paper, we propose an improved role administration model named ARBAC02 to overcome the weaknesses of ARBAC97. ARBAC02 introduces the concept of organization structure for defining user and permission pools independent of roles and role hierarchies, with a refined prerequisite condition specification. In addition, we present a bottom-up approach of permission-role administration in contrast to the top-down approach in ARBAC97. As a general solution, we illustrate the applications of organization structured-based security administration with other access control models, such as access control list model and lattice-based access control model. © 2006 ACM.",Access control; RBAC; Role administration; Role-based access control,Decentralized control; Industrial management; Information analysis; Security systems; Role-based access control (RBAC); Enterprise resource planning
A framework for password-based authenticated key exchange,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748310753&doi=10.1145%2f1151414.1151418&partnerID=40&md5=174bd2d7f130f09b3f33023ee5d4dda4,"In this paper, we present a general framework for password-based authenticated key exchange protocols, in the common reference string model. Our protocol is actually an abstraction of the key exchange protocol of Katz et al. and is based on the recently introduced notion of smooth projective hashing by Cramer and Shoup. We gain a number of benefits from this abstraction. First, we obtain a modular protocol that can be described using just three high-level cryptographic tools. This allows a simple and intuitive understanding of its security. Second, our proof of security is significantly simpler and more modular. Third, we are able to derive analogs to the Katz et al. protocol under additional cryptographic assumptions. Specifically, in addition to the DDH assumption used by Katz et al., we obtain protocols under both the quadratic and N-reaiduosity assumptions. In order to achieve this, we construct new smooth projective hash functions. © 2006 ACM.",Authentication; Dictionary attack; Passwords; Projective hash functions,Cryptography; Functions; Network protocols; Security of data; Authentication; Dictionary attack; Passwords; Projective hash functions; Security systems
Accountability protocols: Formalized and verified,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748318017&doi=10.1145%2f1151414.1151416&partnerID=40&md5=ac1d710eb00ffa3ef64f4d7a003fc512,"Classical security protocols aim to achieve authentication and confidentiality under the assumption that the peers behave honestly. Some recent protocols are required to achieve their goals even if the peer misbehaves. Accountability is a protocol design strategy that may help. It delivers to peers sufficient evidence of each other's participation in the protocol. Accountability underlies the nonrepudiation protocol of Zhou and Gollmann and the certified email protocol of Abadi et al. This paper provides a comparative, formal analysis of the two protocols, and confirms that they reach their goals under realistic conditions. The treatment, which is conducted with mechanized support from the proof assistant Isabelle, requires various extensions to the existing analysis method. A byproduct is an account of the concept of higher-level protocol. © 2006 ACM.",Certified email; Inductive method; Isabelle; Nonrepudiation; Proof tools,Data privacy; Electronic mail; Security of data; Security systems; Certified email; Inductive method; Isabelle; Nonrepudiation; Proof tools; Network protocols
Battery power-aware encryption,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748300805&doi=10.1145%2f1151414.1151417&partnerID=40&md5=2a40506ec832e3920a5b87f8c7b9c891,"Minimizing power consumption is crucial in battery power-limited secure wireless mobile networks. In this paper, we (a) introduce a hardware/software set-up to measure the battery power consumption of encryption algorithms through real-life experimentation, (b) based on the profiled data, propose mathematical models to capture the relationships between power consumption and security, and (c) formulate and solve security maximization subject to power constraints. Numerical results are presented to illustrate the gains that can be achieved in using solutions of the proposed security maximization problems subject to power constraints. © 2006 ACM.",Low-power encryption; Optimization; Profiling,Algorithms; Electric power utilization; Mathematical models; Mobile telecommunication systems; Numerical analysis; Optimization; Security of data; Low-power encryption; Power constraints; Profiling; Wireless mobile networks; Cryptography
On countering online dictionary attacks with login histories and humans-in-the-loop,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750906057&doi=10.1145%2f1178618.1178619&partnerID=40&md5=6595d8df3a78dc8117df445c522adb88,"Automated Turing Tests (ATTs), also known as human-in-the-loop techniques, were recently employed in a login protocol by Pinkas and Sander (2002) to protect against online password-guessing attacks. We present modifications providing a new history-based login protocol with ATTs, which uses failed-login counts. Analysis indicates that the new protocol offers opportunities for improved security and user friendliness (fewer ATTs to legitimate users) and greater flexibility (e.g., allowing protocol parameter customization for particular situations and users). We also note that the Pinkas-Sander and other protocols involving ATTs are susceptible to minor variations of well-known middle-person attacks. We discuss complementary techniques to address such attacks, and to augment the security of the original protocol. © 2006 ACM.",Mandatory human participation schemes; Online dictionary attacks; Password protocols; Relay attack; Usable security,Glossaries; Network protocols; Online systems; Security of data; User interfaces; Mandatory human participation schemes; Online dictionary attacks; Password protocols; Relay attack; Usable security; Computer viruses
Safety in automated trust negotiation,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750915444&doi=10.1145%2f1178618.1178623&partnerID=40&md5=c1ddd6b77fc94943f1bf29385927e113,"Exchange of attribute credentials is a means to establish mutual trust between strangers wishing to share resources or conduct business transactions. Automated Trust Negotiation (ATN) is an approach to regulate the exchange of sensitive information during this process. It treats credentials as potentially sensitive resources, access to which is under policy control. Negotiations that correctly enforce policies have been called ""safe"" in the literature. Prior work on ATN lacks an adequate definition of this safety notion. In large part, this is because fundamental questions such as ""what needs to be protected in ATN?"" and ""what are the security requirements?"" are not adequately answered. As a result, many prior methods of ATN have serious security holes. We introduce a formal framework for ATN in which we give precise, usable, and intuitive definitions of correct enforcement of policies in ATN. We argue that our chief safety notion captures intuitive security goals. We give precise comparisons of this notion with two alternative safety notions that may seem intuitive, but that are seen to be inadequate under closer inspection. We prove that an approach to ATN from the literature meets the requirements set forth in the preferred safety definition, thus validating the safety of that approach, as well as the usability of the definition. © 2006 ACM.",Access control; Attribute-based access control; Automated trust negotiation; Credentials; Safety; Strategy,Information analysis; Public policy; Resource allocation; Security systems; Strategic planning; Access control; Attribute based access control; Automated Trust Negotiation (ATN); Credentials; Electronic commerce
A practical revocation scheme for broadcast encryption using smartcards,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750913340&doi=10.1145%2f1178618.1178622&partnerID=40&md5=2e2fc971fe8e48244e073f6797a1e252,"We present an anti-pirate revocation scheme for broadcast encryption systems (e.g., pay TV), in which the data is encrypted to ensure payment by users. In the systems we consider, decryption of keys is done on smartcards and key management is done in-band. Our starting point is a scheme of Naor and Pinkas. Their basic scheme uses secret sharing to remove up to t parties, is information-theoretic secure against coalitions of size t, and is capable of creating a new group key. However, with current smartcard technology, this scheme is only feasible for small system parameters, allowing up to about 100 pirates to be revoked before all the smartcards need to be replaced. We first present a novel implementation method of their basic scheme that distributes the work among the smartcard, set-top terminal, and center. Based on this, we construct several improved schemes for many revocation rounds that scale to realistic system sizes. We allow up to about 10,000 pirates to be revoked using current smartcard technology before recarding is needed. The transmission lengths of our constructions are on par with those of the best tree-based schemes. However, our constructions have much lower smartcard CPU complexity: only O(1) smartcard operations per revocation round (a single 10-byte field multiplication and addition), as opposed to the complexity of the best tree-based schemes, which is polylogarithmic in the number of users. We evaluate the system behavior via an exhaustive simulation study coupled with a queueing theory analysis. Our simulations show that with mild assumptions on the piracy discovery rate, our constructions can perform effective pirate revocation for realistic broadcast encryption scenarios. © 2006 ACM.",Broadcast encryption; Smart cards,Computer simulation; Cryptography; Information retrieval; Security of data; Smart cards; Trees (mathematics); Broadcast encryption systems; Polylogarithmic; Revocation scheme; Users; Broadcasting
Methods and limitations of security policy reconciliation,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750907119&doi=10.1145%2f1178618.1178620&partnerID=40&md5=0f73abb5a968e90c89cecc4f3a0bf4ef,"A security policy specifies session participant requirements. However, existing frameworks provide limited facilities for the automated reconciliation of participant policies. This paper considers the limits and methods of reconciliation in a general-purpose policy model. We identify an algorithm for efficient two-policy reconciliation and show that, in the worst-case, reconciliation of three or more policies is intractable. Further, we suggest efficient heuristics for the detection and resolution of intractable reconciliation. Based upon the policy model, we describe the design and implementation of the Ismene policy language. The expressiveness of Ismene, and indirectly of our model, is demonstrated through the representation and exposition of policies supported by existing policy languages. We conclude with brief notes on the integration and enforcement of Ismene policy within the Antigone communication system. © 2006 ACM.",Security policy,Algorithms; Communication systems; Heuristic methods; Optical resolving power; Security systems; Antigone communication system; Automated reconciliation; Policy languages; Security policy; Public policy
XML access control using static analysis,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750924800&doi=10.1145%2f1178618.1178621&partnerID=40&md5=7f94a3bb645d7eecc3027efc26c8558d,"Access control policies for XML typically use regular path expressions such as XPath for specifying the objects for access-control policies. However such access-control policies are burdens to the query engines for XML documents. To relieve this burden, we introduce static analysis for XML access-control. Given an access-control policy, query expression, and an optional schema, static analysis determines if this query expression is guaranteed not to access elements or attributes that are hidden by the access-control policy but permitted by the schema. Static analysis can be performed without evaluating any query expression against actual XML documents. Run-time checking is required only when static analysis is unable to determine whether to grant or deny access requests. A side effect of static analysis is query optimization: access-denied expressions in queries can be evaluated to empty lists at compile time. We further extend static analysis for handling value-based access-control policies and introduce view schemas. © 2006 ACM.",Access control; Automaton; Query optimization; Schema; Static analysis; Value-based access control; View schema; XML; XPath; XQuery,Finite automata; Optimization; Public policy; Query languages; Access control policies; Query optimization; Static analysis; Value based access control; XPath; XQuery; XML
Improved proxy re-encryption schemes with applications to secure distributed storage,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745218758&doi=10.1145%2f1127345.1127346&partnerID=40&md5=fb4c6d0696870dfb584b2b83a5441e3f,"In 1998, Blaze, Bleumer, and Strauss (BBS) proposed an application called atomic proxy re-encryption, in which a semitrusted proxy converts a ciphertext for Alice into a ciphertext for Bob without seeing the underlying plaintext. We predict that fast and secure re-encryption will become increasingly popular as a method for managing encrypted file systems. Although efficiently computable, the wide-spread adoption of BBS re-encryption has been hindered by considerable security risks. Following recent work of Dodis and Ivan, we present new re-encryption schemes that realize a stronger notion of security and demonstrate the usefulness of proxy re-encryption as a method of adding access control to a secure file system. Performance measurements of our experimental file system demonstrate that proxy re-encryption can work effectively in practice. © 2006 ACM.",Bilinear maps; Double decryption; Key translation; Proxy re-encryption,Control systems; Risk assessment; Security of data; Bilinear maps; Double decryption; Key translation; Proxy re-encryption; Cryptography
X-GTRBAC admin: A decentralized administration model for enterprise-wide access control,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745472982&doi=10.1145%2f1108906.1108909&partnerID=40&md5=e03c33614a6b2839ad894185010c9f42,"The modern enterprise spans several functional units or administrative domains with diverse authorization requirements. Access control policies in an enterprise environment typically express these requirements as authorization constraints. While desirable for access control, constraints can lead to conflicts in the overall policy in a multidomain environment. The administration problem for enterprise-wide access control, therefore, not only includes authorization management for users and resources within a single domain but also conflict resolution among heterogeneous access control policies of multiple domains to allow secure interoperation within the enterprise. This work presents design and implementation of X-GTRBAC Admin, an administration model that aims at enabling administration of role-based access control (RBAC) policies in the presence of constraints with support for conflict resolution in a multidomain environment. A key feature of the model is that it allows decentralization of policy administration tasks through the abstraction of administrative domains, which not only simplifies authorization management, but is also fundamental to the concept of decentralized conflict resolution presented. The paper also illustrates the applicability of the outlined administrative concepts in a realistic enterprise environment using an implementation prototype that facilitates policy administration in large enterprises. © 2005 ACM.",Policy administration; Role-based access control; Secure interoperation; XML,Constraint theory; Identification (control systems); Problem solving; Security of data; XML; Policy administration; Realistic enterprise environment; Role-based access control; Secure interoperation; Telecommunication networks
Auditing sum-queries to make a statistical database secure,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745188712&doi=10.1145%2f1127345.1127347&partnerID=40&md5=13c658a78a2ea445f35f0f6d5bb03214,"In response to queries asked to a statistical database, the query system should avoid releasing summary statistics that could lead to the disclosure of confidential individual data. Attacks to the security of a statistical database may be direct or indirect and, in order to repel them, the query system should audit queries by controlling the amount of information released by their responses. This paper focuses on sum-queries with a response variable of nonnegative real type and proposes a compact representation of answered sum-queries, called an information model in ""normal form,"" which allows the query system to decide whether the value of a new sum-query can or cannot be safely answered. If it cannot, then the query system will issue the range of feasible values of the new sum-query consistent with previously answered sum-queries. Both the management of the information model and the answering procedure require solving linear-programming problems and, since standard linear-programming algorithms are not polynomially bounded (despite their good performances in practice), effective procedures that make a parsimonious use of them are stated for the general case. Moreover, in the special case that the information model is ""graphical."" It is shown that the answering procedure can be implemented in polynomial time. © 2006 ACM.",Additive data; Sensitive information,Database systems; Linear programming; Mathematical models; Polynomials; Query languages; Security of data; Additive data; Information model; Sensitive information; Statistical methods
Foundations and applications for secure triggers,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745191654&doi=10.1145%2f1127345.1127349&partnerID=40&md5=39f67b2d280847ee3f63acf7692b6983,"Imagine there is certain content we want to maintain private until some particular event occurs, when we want to have it automatically disclosed. Suppose, furthermore, that we want this done in a (possibly) malicious host. Say the confidential content is a piece of code belonging to a computer program that should remain ciphered and then ""be triggered"" (i.e., deciphered and executed) when the underlying system satisfies a preselected condition, which must remain secret after code inspection. In this work we present different solutions for problems of this sort, using different ""declassification"" criteria, based on a primitive we call secure triggers. We establish the notion of secure triggers in the universally composable security framework of Canetti [2001] and introduce several examples. Our examples demonstrate that a new sort of obfuscation is possible. Finally, we motivate its use with applications in realistic scenarios. © 2006 ACM.",Malicious host problem; Mobile code security; Obfuscation; Secure triggers; Universally composable security,Codes (symbols); Computer programming; Problem solving; Malicious host problem; Mobile code security; Obfuscation; System Security; Universally composable security; Security of data
Anomalous system call detection,2006,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745201000&doi=10.1145%2f1127345.1127348&partnerID=40&md5=02b04c4f8027f7c7b2277b98cdab0ecf,"Intrusion detection systems (IDSs) are used to detect traces of malicious activities targeted against the network and its resources. Anomaly-based IDSs build models of the expected behavior of applications by analyzing events that are generated during the applications' normal operation. Once these models have been established, subsequent events are analyzed to identify deviations, on the assumption that anomalies represent evidence of an attack. Host-based anomaly detection systems often rely on system call sequences to characterize the normal behavior of applications. Recently, it has been shown how these systems can be evaded by launching attacks that execute legitimate system call sequences. The evasion is possible because existing techniques do not take into account all available features of system calls. In particular, system call arguments are not considered. We propose two primary improvements upon existing host-based anomaly detectors. First, we apply multiple detection models to system call arguments. Multiple models allow the arguments of each system call invocation to be evaluated from several different perspectives. Second, we introduce a sophisticated method of combining the anomaly scores from each model into an overall aggregate score. The combined anomaly score determines whether an event is part of an attack. Individual anomaly scores are often contradicting and, therefore, a simple weighted sum cannot deliver reliable results. To address this problem, we propose a technique that uses Bayesian networks to perform system call classification. We show that the analysis of system call arguments and the use of Bayesian classification improves detection accuracy and resilience against evasion attempts. In addition, the paper describes a tool based on our approach and provides a quantitative evaluation of its performance in terms of both detection effectiveness and overhead. A comparison with four related approaches is also presented. © 2006 ACM.",Anomaly detection; Bayesian network; Computer security; Intrusion detection,Computer aided software engineering; Computer networks; Mathematical models; Neural networks; Problem solving; Anomaly detection; Bayesian network; Intrusion detection systems (IDS); Security of data
Formal model and policy specification of usage control,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745450500&doi=10.1145%2f1108906.1108908&partnerID=40&md5=bf2b73318cf29d0a62297be7e2bea495,"The recent usage control model (UCON) is a foundation for next-generation access control models with distinguishing properties of decision continuity and attribute mutability. A usage control decision is determined by combining authorizations, obligations, and conditions, presented as UCON ABC core models by Park and Sandhu. Based on these core aspects, we develop a formal model and logical specification of UCON with an extension of Lamport's temporal logic of actions (TLA). The building blocks of this model include: (1) a set of sequences of system states based on the attributes of subjects, objects, and the system, (2) authorization predicates based on subject and object attributes, (3) usage control actions to update attributes and accessing status of a usage process, (4) obligation actions, and (5) condition predicates based on system attributes. A usage control policy is defined as a set of temporal logic formulas that are satisfied as the system state changes. A fixed set of scheme rules is defined to specify general UCON policies with the properties of soundness and completeness. We show the flexibility and expressive capability of this formal model by specifying the core models of UCON and some applications. © 2005 ACM.",Access control; Formal specification; Security policy; Usage control,Identification (control systems); Security of data; Security systems; Specifications; Access control; Formal specification; Security policy; Usage control; Control systems
Guest editorial: Special issue on access control models and technologies,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745441326&doi=10.1145%2f1108906.1108907&partnerID=40&md5=d9abbe3c754679f7e260513eed8f1181,[No abstract available],,
Access control to people location information,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745432342&doi=10.1145%2f1108906.1108910&partnerID=40&md5=13661439508f2db8e159112829ca866a,"Ubiquitous computing uses a variety of information for which access needs to be controlled. For instance, a person's current location is a sensitive piece of information that only authorized entities should be able to learn. Several challenges arise in the specification and implementation of policies controlling access to location information. For example, there can be multiple sources of location information. The sources can be within different administrative domains, which might allow different entities to specify policies, and policies need to be flexible. We address these issues in our design of a distributed access control mechanism for a people location system. Our design encodes policies as digital certificates, which enables decentralized storage of policies. We also present an algorithm for the discovery of distributed certificates. Furthermore, we discuss several privacy issues and show how our design addresses them. To show feasibility of our design, we built an example implementation based on SPKI/SDSI certificates. Using measurements, we quantify the influence of access control on query processing time. We also discuss trade-offs between RSA-based and DSA-based signature schemes for digital certificates. © 2005 ACM.",Certificates; Credential discovery; Delegation; DSA; Location; Privacy; RSA; SPKI/SDSI; Trust,Algorithms; Data privacy; Information retrieval; Location; Security of data; Specifications; Certificates; Credential discovery; Delegation; DSA; RSA; SPKI/SDSI; Trust; Computer networks
Keystroke analysis of free text,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745215614&doi=10.1145%2f1085126.1085129&partnerID=40&md5=6c2261f33599b89ea9853c6b637216d3,"Keystroke dynamics can be useful to ascertain personal identity even after an authentication phase has been passed, provided that we are able to deal with the typing rhythms of free text, chosen and entered by users without any specific constraint. In this paper we present a method to compare typing samples of free text that can be used to verify personal identity. We have tested our technique with a wide set of experiments on 205 individuals, obtaining a False Alarm Rate of less than 5% and an Impostor Pass Rate of less than 0.005%. Different trade-offs are, however, possible. Our approach can rely on what is typed by people because of their normal job, and a few lines of text, even collected in different working sessions, are sufficient to reach a high level of accuracy, which improves proportionally to the amount of available information: As a consequence, we argue that our method can be useful in computer security as a complementary or alternative way to user authentication and as an aid to intrusion detection. © 2005 ACM.",Biometric techniques; Identity verification; Keystroke analysis of free text,Constraint theory; Information technology; Security of data; Security systems; Biometric techniques; Identity verification; Keystroke analysis of free text; Electronic document identification systems
The concept of layered proving trees and its application to the automation of security protocol verification,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30144436140&doi=10.1145%2f1085126.1085128&partnerID=40&md5=598d43c21e4ad79950290c977165d05c,"Security protocols are one of the most critical elements in enabling the secure communication and processing of information. The presence of flaws in published protocols highlights the complexity of security protocol design. Only formal verification can provide strong confidence in the correctness of security protocols and is considered an imperative step in their design. This paper presents a new theoretical concept, called Layered Proving Trees, for automatically applying logical postulates in logic-based security protocol verification. An algorithm for the new concept is introduced and the soundness and completeness of the technique is proved. Empirical results on the performance of the algorithm are presented. The presented proofs and empirical results demonstrate the feasibility and effectiveness of the Layered Proving Tree approach. © 2005 ACM.",Automated protocol verification; Cryptographic protocols; Cryptography; Logic-based verification of security protocols; Security protocols,Algorithms; Cryptography; Data processing; Network protocols; Security of data; Trees (mathematics); Automated protocol verification; Cryptographic protocols; Logic-based verification of security protocols; Security protocols; Information technology
APSS: Proactive secret sharing in asynchronous systems,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27644508954&doi=10.1145%2f1085126.1085127&partnerID=40&md5=5d486904f8b429b4cef0019ca9c18c67,"APSS, a proactive secret sharing (PSS) protocol for asynchronous systems, is explained and proved correct. The protocol enables a set of secret shares to be periodically refreshed with a new, independent set, thereby thwarting mobile-adversary attacks. Protocols for asynchronous systems are inherently less vulnerable to denial-of-service attacks, which slow processor execution or delay message delivery. So APSS tolerates certain attacks that PSS protocols for synchronous systems cannot. © 2005 ACM.",Asynchronous system; Denial of service; Proactive secret sharing; Threshold cryptography,Cryptography; Data processing; Information technology; Network protocols; Program processors; Security of data; Asyncronous systems; Denial of service; Proactive secret sharing; Threshold cryptography; Asynchronous transfer mode
Trusted paths for browsers,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23244459906&doi=10.1145%2f1065545.1065546&partnerID=40&md5=b2a25dfbf9d140c24a356764840d039d,"Computer security protocols usually terminate in a computer; however, the human-based services which they support usually terminate in a human. The gap between the human and the computer creates potential for security problems. We examine this gap, as it is manifested in secure Web servers. Felten et al. demonstrated the potential, in 1996, for malicious servers to impersonate honest servers. In this paper, we show how malicious servers can still do this - and can also forge the existence of an SSL session and the contents of the alleged server certificate. We then consider how to systematically defend against Web spoofing, by creating a trusted path from the browser to the human user. We present potential designs, propose a new one, prototype it in open-source Mozilla, and demonstrate its effectiveness via user studies. © 2005 ACM.",HCISEC; Trust path; Web browser security,Electronic commerce; Human engineering; Network protocols; Problem solving; User interfaces; Web browsers; HCISEC; Trust path; Web browser security; Web spoofing; Security of data
A pairwise key predistribution scheme for wireless sensor networks,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23244467182&doi=10.1145%2f1065545.1065548&partnerID=40&md5=c8398059328ed1effa492b9478463af2,"To achieve security in wireless sensor networks, it is important to be able to encrypt and authenticate messages sent between sensor nodes. Before doing so, keys for performing encryption and authentication must be agreed upon by the communicating parties. Due to resource constraints, however, achieving key agreement in wireless sensor networks is nontrivial. Many key agreement schemes used in general networks, such as Diffie-Hellman and other public-key based schemes, are not suitable for wireless sensor networks due to the limited computational abilities of the sensor nodes. Predistribution of secret keys for all pairs of nodes is not viable due to the large amount of memory this requires when the network size is large. In this paper, we provide a framework in which to study the security of key predistribution schemes, propose a new key predistribution scheme which substantially improves the resilience of the network compared to previous schemes, and give an in-depth analysis of our scheme in terms of network resilience and associated overhead. Our scheme exhibits a nice threshold property: when the number of compromised nodes is less than the threshold, the probability that communications between any additional nodes are compromised is close to zero. This desirable property lowers the initial payoff of smaller-scale network breaches to an adversary, and makes it necessary for the adversary to attack a large fraction of the network before it can achieve any significant gain. © 2005 ACM.",Key predistribution; Security; Wireless sensor networks,Graph theory; Probability; Problem solving; Public key cryptography; Servers; Telecommunication networks; Wireless telecommunication systems; Key agreement; Key predistribution; Related key attacks; Wireless sensor network (WSN); Security of data
X-GTRBAC: An XML-based policy specification framework and architecture for enterprise-wide access control,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23244445557&doi=10.1145%2f1065545.1065547&partnerID=40&md5=6a8737ae031b3b09b77779fd905448ba,"Modern day enterprises exhibit a growing trend toward adoption of enterprise computing services for efficient resource utilization, scalability, and flexibility. These environments are characterized by heterogeneous, distributed computing systems exchanging enormous volumes of time-critical data with varying levels of access control in a dynamic business environment. The enterprises are thus faced with significant challenges as they endeavor to achieve their primary goals, and simultaneously ensure enterprise-wide secure interoperation among the various collaborating entities. Key among these challenges are providing effective mechanism for enforcement of enterprise policy across distributed domains, ensuring secure content-based access to enterprise resources at all user levels, and allowing the specification of temporal and nontemporal context conditions to support fine-grained dynamic access control. In this paper, we investigate these challenges, and present X-GTRBAC, an XML-based GTRBAC policy specification language and its implementation for enforcing enterprise-wide access control. Our specification language is based on the GTRBAC model that incorporates the content- and context-aware dynamic access control requirements of an enterprise. An X-GTRBAC system has been implemented as a Java application. We discuss the salient features of the specification language, and present the software architecture of our system. A comprehensive example is included to discuss and motivate the applicability of the X-GTRBAC framework to a generic enterprise environment. An application level interface for implementing the policy in the X-GTRBAC system is also provided to consolidate the ideas presented in the paper. © 2005 ACM.",Role-based access control; Secure enterprises; XML,Computer hardware description languages; Computer software; Distributed computer systems; Java programming language; Mathematical models; XML; Role-based access control; Secure enterprises; Software architecture; User-to-role assignment (UA); Security of data
Establishing pairwise keys in distributed sensor networks,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16644374371&doi=10.1145%2f1053283.1053287&partnerID=40&md5=fe16754f3b51805cbf4a174fdf0e2fe1,"Pairwise key establishment is a fundamental security service in sensor networks; it enables sensor nodes to communicate securely with each other using cryptographic techniques. However, due to the resource constraints on sensor nodes, it is not feasible to use traditional key management techniques such as public key cryptography and key distribution center (KDC). A number of key predistribution techniques have been proposed for pairwise key establishment in sensor networks recently. To facilitate the study of novel pairwise key predistribution techniques, this paper develops a general framework for establishing pairwise keys between sensor nodes using bivariate polynomials. This paper then proposes two efficient instantiations of the general framework: a random subset assignment key predistribution scheme, and a hypercube-based key predistribution scheme. The analysis shows that both schemes have a number of nice properties, including high probability, or guarantee to establish pairwise keys, tolerance of node captures, and low storage, communication, and computation overhead. To further reduce the computation at sensor nodes, this paper presents an optimization technique for polynomial evaluation, which is used to compute pairwise keys. This paper also reports the implementation and the performance of the proposed schemes on MICA2 motes running TinyOS, an operating system for networked sensors. The results indicate that the proposed techniques can be applied efficiently in resource-constrained sensor networks. © 2005 ACM.",Key management; Key predistribution; Pairwise key; Sensor networks,Computer operating systems; Cryptography; Data reduction; Distributed computer systems; Network protocols; Optimization; Polynomials; Sensors; Key management; Key predistribution; Pairwise key; Sensor networks; Computer networks
Randomized instruction set emulation,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16644362894&doi=10.1145%2f1053283.1053286&partnerID=40&md5=e4f5aeebf907ce8de516208e67a8dd4b,"Injecting binary code into a running program is a common form of attack. Most defenses employ a ""guard the doors"" approach, blocking known mechanisms of code injection. Randomized instruction set emulation (RISE) is a complementary method of defense, one that performs a hidden randomization of an application's machine code. If foreign binary code is injected into a program running under RISE, it will not be executable because it will not know the proper randomization. The paper describes and analyzes RISE, describing a proof-of-concept implementation built on the open-source Valgrind IA32-to-IA32 translator, The prototype effectively disrupts binary code injection attacks, without requiring recompilation, linking, or access to application source code. Under RISE, injected code (attacks) essentially executes random code sequences. Empirical studies and a theoretical model are reported which treat the effects of executing random code on two different architectures (IA32 and PowerPC). The paper discusses possible extensions and applications of the RISE technique in other contexts. © 2005 ACM.",Automated diversity; Randomized instruction sets; Software diversity,Codes (symbols); Computer programming; Computer programming languages; Cryptography; Distributed computer systems; Interfaces (computer); Microprocessor chips; Probability; Automated diversity; Binary code; Randomized instruction sets; Software diversity; Computer software
ACM Transactions on Information and System Security: Preface,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16644377108&doi=10.1145%2f1053283.1053285&partnerID=40&md5=20a4456e9dee5f9ee5ada0680190468e,[No abstract available],,
ACM Transactions on Information and System Security: Editorial,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16644388525&doi=10.1145%2f1053283.1053284&partnerID=40&md5=2eb9645a2781e009481c5fb40102721e,[No abstract available],,
"Incentive-based modeling and inference of attacker intent, objectives, and strategies",2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16644384919&doi=10.1145%2f1053283.1053288&partnerID=40&md5=82f03f5bb83152b270a06d604297a605,"Although the ability to model and infer attacker intent, objectives, and strategies (AIOS) may dramatically advance the literature of risk assessment, harm prediction, and predictive or proactive cyber defense, existing AIOS inference techniques are ad hoc and system or application specific. In this paper, we present a general incentive-based method to model AIOS and a game-theoretic approach to inferring AIOS. On one hand, we found that the concept of incentives can unify a large variety of attacker intents; the concept of utilities can integrate incentives and costs in such a way that attacker objectives can be practically modeled. On the other hand, we developed a game-theoretic AIOS formalization which can capture the inherent interdependency between AIOS and defender objectives and strategies in such a way that AIOS can be automatically inferred. Finally, we use a specific case study to show how attack strategies can be inferred in real-world attack-defense scenarios. © 2005 ACM.",Attack strategy inference; Attacker intent and strategy modeling; Game theory,Cryptography; Mathematical models; Probability; Random processes; Risk assessment; Security of data; Attack strategy inference; Attacker intent and strategy modeling; Cyber defense; Real-world attack; Game theory
Modeling and assessing inference exposure in encrypted databases,2005,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16644381926&doi=10.1145%2f1053283.1053289&partnerID=40&md5=c6f752c0e6db81378263dd38fee05d47,"The scope and character of today's computing environments are progressively shifting from traditional, one-on-one client-server interaction to the new cooperative paradigm. It then becomes of primary importance to provide means of protecting the secrecy of the information, while guaranteeing its availability to legitimate clients. Operating online querying services securely on open networks is very difficult; therefore many enterprises outsource their data center operations to external application service providers. A promising direction toward prevention of unauthorized access to outsourced data is represented by encryption. However, data encryption is often supported for the sole purpose of protecting the data in storage while allowing access to plaintext values by the server, which decrypts data for query execution. In this paper, we present a simple yet robust single-server solution for remote querying of encrypted databases on external servers. Our approach is based on the use of indexing information attached to the encrypted database, which can be used by the server to select the data to be returned in response to a query without the need of accessing the plaintext database content. Our indexes balance the trade-off between efficiency requirements in query execution and protection requirements due to possible inference attacks exploiting indexing information. We investigate quantitative measures to model inference exposure and provide some related experimental results. © 2005 ACM.",Cryptography; Database service; Indexing; Inference,Client server computer systems; Cryptography; Indexing (of information); Online systems; Query languages; Security of data; Data integrity; Database services; Inference; Service providers; Database systems
Client-side caching for TLS,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14544299579&doi=10.1145%2f1042031.1042034&partnerID=40&md5=a53c653aaf90e57c1dbdc6741b9f7e14,"We propose two new mechanisms for caching handshake information on TLS clients. The ""fast-track"" mechanism provides a client-side cache of a server's public parameters and negotiated parameters in the course of an initial, enabling handshake. These parameters need not be resent on subsequent handshakes. Fast-track reduces both network traffic and the number of round trips, and requires no additional server state. These savings are most useful in high-latency environments such as wireless networks. The second mechanism, ""client-side session caching,"" allows the server to store an encrypted version of the session information on a client, allowing a server to maintain a much larger number of active sessions in a given memory footprint. Our design is fully backward-compatible with TLS: extended clients can interoperate with servers unaware of our extensions and vice versa. We have implemented our fast-track proposal to demonstrate the resulting efficiency improvements.",Bloom filters; Session cache; TLS; Wireless networks,Bandwidth; Client server computer systems; Network protocols; Parameter estimation; Product design; Public policy; Telecommunication traffic; Wireless telecommunication systems; Bloom filters; Payload transfers; Security; Session cache; TLS; Cache memory
Traducement: A model for record security,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14544273217&doi=10.1145%2f1042031.1042035&partnerID=40&md5=2ab7eff2bba68c15d9e310ecc45a2131,"Security models generally incorporate elements of both confidentiality and integrity. We examine a case where confidentiality is irrelevant to the process being modeled. In this case, integrity includes not only the authentication of origin and the lack of unauthorized changes to a document, but also the acceptance of all parties that the document is complete, signed by all parties, and cannot be modified further. This is especially critical when the document is recorded, so that it is legally the agreement or statement of record, and any copies of the document have no legal force. We show that current security models do not capture the details of this process. We then present a new security model for this process. This model captures the recordation process, and augments, rather than supplants, existing models. Hence it can also be used with existing security models to describe other situations.",Integrity; Recordation; Security policy; Traducement,Cryptography; Data processing; Database systems; Management information systems; Product design; Program documentation; Public policy; Records management; Integrity; Recordation; Security policy; Traducement; Security systems
A key-chain-based keying scheme for many-to-many secure group communication,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14544302192&doi=10.1145%2f1042031.1042033&partnerID=40&md5=d57b83a92eaebca8bad926224192dcf0,"We propose a novel secure group keying scheme using hash chain for many-to-many secure group communication. This scheme requires a key predistribution center to generate multiple hash chains and allocates exactly one hash value from each chain to a group member. A group member can use its allocated hash values (secrets) to generate group and subgroup keys. Key distribution can be offline or online via the key distribution protocol. Once keys are distributed, this scheme enables a group member to communicate with any possible subgroups without the help of the key distribution center, and without having to leave the overall group, thus avoiding any setup delay. Our scheme is suitable for applications where the population of a system is stable, group size is moderate, subgroup formation is frequent, and the application is delay sensitive. Through analysis, we present effectiveness of our approach.",Hash chain; Key chain; Many-to-many secure group communication; Secure group communication,Data privacy; Delay control systems; Linear control systems; Network protocols; Real time systems; Teleconferencing; Wireless telecommunication systems; Hash chains; Key chain; Many-to-many secure group communication; Secure group communication; Communication systems
The predecessor attack: An analysis of a threat to anonymous communications systems,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14544273666&doi=10.1145%2f1042031.1042032&partnerID=40&md5=27e7af19c8e10d26dda41c4fa78028b8,"There have been a number of protocols proposed for anonymous network communication. In this paper, we investigate attacks by corrupt group members that degrade the anonymity of each protocol over time. We prove that when a particular initiator continues communication with a particular responder across path reformations, existing protocols are subject to the attack. We use this result to place an upper bound on how long existing protocols, including Crowds, Onion Routing, Hordes, Web Mixes, and DC-Net, can maintain anonymity in the face of the attacks described. This provides a basis for comparing these protocols against each other. Our results show that fully connected DC-Net is the most resilient to these attacks, but it suffers from scalability issues that keep anonymity group sizes small. We also show through simulation that the underlying topography of the DC-Net affects the resilience of the protocol: as the number of neighbors a node has increases the strength of the protocol increases, at the cost of higher communication overhead.",Anonymity; Anonymous communication; Predecessor attack; Privacy,Accident prevention; Computer simulation; Costs; Data privacy; Distributed computer systems; Network protocols; Problem solving; Routers; Time domain analysis; Anonymity; Anonymous communication; Predecessor attack; Privacy; Communication systems
Hypothesizing and reasoning about attacks missed by intrusion detection systems,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14544283296&doi=10.1145%2f1042031.1042036&partnerID=40&md5=1619fb52f170d430646cfec9cdb04fc0,"Several alert correlation methods have been proposed over the past several years to construct high-level attack scenarios from low-level intrusion alerts reported by intrusion detection systems (IDSs). However, all of these methods depend heavily on the underlying IDSs, and cannot deal with attacks missed by IDSs. In order to improve the performance of intrusion alert correlation and reduce the impact of missed attacks, this paper presents a series of techniques to hypothesize and reason about attacks possibly missed by the IDSs. In addition, this paper also discusses techniques to infer attribute values for hypothesized attacks, to validate hypothesized attacks through raw audit data, and to consolidate hypothesized attacks to generate concise attack scenarios. The experimental results in this paper demonstrate the potential of these techniques in building high-level attack scenarios.",Intrusion alert correlation; Intrusion detection; Missed attacks,Automation; Communication systems; Computer operating procedures; Correlation methods; Data processing; Identification (control systems); Internet; Problem solving; Intrusion alert correlation; Intrusion detection; Missed attacks; Security; Management information systems
On the performance of group key agreement protocols,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444225744&doi=10.1145%2f1015040.1015045&partnerID=40&md5=8e5b82dd95afc366c6335f434a37c23e,"Group key agreement is a fundamental building block for secure peer group communication systems. Several group key management techniques were proposed in the last decade, all assuming the existence of an underlying group communication infrastructure to provide reliable and ordered message delivery as well as group membership information. Despite analysis, implementation, and deployment of some of these techniques, the actual costs associated with group key management have been poorly understood so far. This resulted in an undesirable tendency: on the one hand, adopting suboptimal security for reliable group communication, while, on the other hand, constructing excessively costly group key management protocols. This paper presents a thorough performance evaluation of five notable distributed key management techniques (for collaborative peer groups) integrated with a reliable group communication system. An in-depth comparison and analysis of the five techniques is presented based on experimental results obtained in actual local- and wide-area networks. The extensive performance measurement experiments conducted for all methods offer insights into their scalability and practicality. Furthermore, our analysis of the experimental results highlights several observations that are not obvious from the theoretical analysis.",Group Communication; Group Key Management; Peer Groups; Secure Communication,Abstracting; Cost accounting; Data processing; Network protocols; Performance; Reliability; Security of data; Wide area networks; Building block; Group communication system; Group key agreement; Scalability; Communication systems
Modular authorization and administration,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444250230&doi=10.1145%2f1015040.1015042&partnerID=40&md5=37572d4659f5543822ea6d56348bba72,"In large organizations the administration of access privileges (such as the assignment of access rights to a user in a particular role) is handled cooperatively through distributed administrators in various different capacities. A quorum may be necessary, or a veto may be possible for such a decision. In this paper, we present two major contributions: We develop a role-based access control (RBAC) approach for specifying distributed administration requirements, and procedures between administrators, or administration teams, extending earlier work on distributed (modular) authorization. While a comprehensive specification in such a language is conceivable it would be quite tedious to evaluate, or analyze, their operational aspects and properties in practice. For this reason we create a new class of extended Petri Nets called Administration Nets (Adm-Nets) such that any RBAC specification of (cooperative) administration requirements (given in terms of predicate logic formulas) can be embedded into an Adm-Net. This net behaves within the constraints specified by the logical formulas, and at the same time, it explicitly exhibits all needed operational details such as allowing for an efficient and comprehensive formal analysis of administrative behavior. We introduce the new concepts and illustrate their use in several examples. While Adm-Nets are much more refined and (behaviorally) explicit than workflow systems our work provides for a constructive step towards novel workflow management tools as well. We demonstrate the usefulness of Adm-Nets by modeling typical examples of administration processes concerned with sets of distributed authorization rules.",Composability; Modularity; Petri-Nets; Work-flow,Computer software; Constraint theory; Mathematical models; Petri nets; Reliability; Security of data; Composability; Modularity; Security and protection; Work-flow; Information technology
An integrated approach to engineer and enforce context constraints in RBAC environments,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444246080&doi=10.1145%2f1015040.1015043&partnerID=40&md5=c9aba7fa595b3f7d4dfea4c7f91edc59,"We present an approach that uses special purpose role-based access control (RBAC) constraints to base certain access control decisions on context information. In our approach a context constraint is defined as a dynamic RBAC constraint that checks the actual values of one or more contextual attributes for predefined conditions. If these conditions are satisfied, the corresponding access request can be permitted. Accordingly, a conditional permission is an RBAC permission that is constrained by one or more context constraints. We present an engineering process for context constraints that is based on goal-oriented requirements engineering techniques, and describe how we extended the design and implementation of an existing RBAC service to enable the enforcement of context constraints. With our approach we aim to preserve the advantages of RBAC and offer an additional means for the definition and enforcement of fine-grained context-dependent access control policies.",Constraints engineering; Context constraints; Context-dependent access control; Role-based access control,Computer operating systems; Computer systems; Constraint theory; Control; Information analysis; Security of data; Constraint engineering; Context constraints; Context-dependent access control; Role-based access control; Software engineering
Content-triggered trust negotiation,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444341241&doi=10.1145%2f1015040.1015044&partnerID=40&md5=1d0352c2d3c399eb61e8625e2420b26c,"The focus of access control in client/server environments is on protecting sensitive server resources by determining whether or not a client is authorized to access those resources. The set of resources is usually static, and an access control policy associated with each resource specifies who is authorized to access the resource. In this article, we turn the traditional client/server access control model on its head and address how to protect the sensitive content that clients disclose to and receive from servers. Since client content is often dynamically generated at run-time, the usual approach of associating a policy with the resource (content) a priori does not work. We propose a general-purpose access control model designed to detect whenever sensitive information is being transmitted, and determine whether the sender or receiver is authorized. The model identifies sensitive content, maps the sensitive content to an access control policy, and establishes the trustworthiness of the sender or receiver before the sensitive content is disclosed or received. We have implemented the model within TrustBuilder, an architecture for negotiating trust between strangers based on properties other than identity. The implementation targets open systems, where clients and servers do not have preexisting trust relationships. The implementation is the first example of content-triggered trust negotiation. It currently supports access control for sensitive content disclosed by web and email clients.",Access control; Authentication; Credentials; Trust negotiation,Computer operating systems; Control; Information analysis; Resource allocation; Security of data; Sensitivity analysis; Servers; Access control; Authentication; Credentials; Trust negotiation; Client server computer systems
The session token protocol for forensics and traceback,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444372495&doi=10.1145%2f1015040.1015041&partnerID=40&md5=8674378d289a6cddb7341fd72a8711b4,"In this paper we present the Session Token Protocol (STOP), a new protocol that can assist in the forensic analysis of a computer involved in malicious network activity. It has been designed to help automate the process of tracing attackers who log on to a series of hosts to hide their identity. STOP utilizes the Identification Protocol infrastructure, improving both its capabilities and user privacy. On request, the STOP protocol saves user-level and application-level data associated with a particular TCP connection and returns a random token specifically related to that session. The saved data are not revealed to the requester unless the token is returned to the local administrator, who verifies the legitimacy of the need for the release of information. The protocol supports recursive traceback requests to gather information about the entire path of a connection. This allows an incident investigator to trace attackers to their home systems, but does not violate the privacy of normal users. This paper details the new protocol and presents implementation and performance results.",Auditing and intrusion detection; Digital forensics; Digital investigations; Privacy; TCP traceback,Automation; Data reduction; Design; Network protocols; Performance; Servers; Auditing; Digital forensics; Digital investigations; Infrastructure; Traceback; Security of data
Breaking and provably repairing the SSH authenticated encryption scheme: A case study of the encode-then-encrypt-and-MAC paradigm,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142623034&doi=10.1145%2f996943.996945&partnerID=40&md5=95da27c3775d9f70c7730444ffc526e2,"The secure shell (SSH) protocol is one of the most popular cryptographic protocols on the Internet. Unfortunately, the current SSH authenticated encryption mechanism is insecure. In this paper, we propose several fixes to the SSH protocol and, using techniques from modern cryptography, we prove that our modified versions of SSH meet strong new chosen-ciphertext privacy and integrity requirements. Furthermore, our proposed fixes will require relatively little modification to the SSH protocol and to SSH implementations. We believe that our new notions of privacy and integrity for encryption schemes with stateful decryption algorithms will be of independent interest.",Authenticated encryption; Secure shell; Security proofs; Stateful decryption,Algorithms; Computational complexity; Computer operating systems; Cryptography; Data privacy; Network protocols; Authentication encryption; Secure shells; Security proofs; Stateful decryption; Security of data
Consistency analysis of authorization hook placement in the Linux security modules framework,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142518085&doi=10.1145%2f996943.996944&partnerID=40&md5=9dedf6968b58759c9e49f85cd89e99da,"We present a consistency analysis approach to assist the Linux community in verifying the correctness of authorization hook placement in the Linux Security Modules (LSM) framework. The LSM framework consists of a set of authorization hooks inserted into the Linux kernel to enable additional authorizations to be performed (e.g., for mandatory access control). When compared to system call interposition, authorization within the kernel has both security and performance advantages, but it is more difficult to verify that placement of the LSM hooks ensures that all the kernel's security-sensitive operations are authorized. Static analysis has been used previously to verified mediation (i.e., that some hook mediates access to a security-sensitive operation), but that work did not determine whether the necessary set of authorizations were checked. In this paper, we develop an approach to test the consistency of the relationships between security-sensitive operations and LSM hooks. The idea is that whenever a security-sensitive operation is performed as part of specifiable event, a particular set of LSM hooks must have mediated that operation. This work demonstrates that the number of events that impact consistency is manageable and that the notion of consistency is useful for verifying correctness. We describe our consistency approach for performing verification, the implementation of run-time tools that implement this approach, the anomalous situations found in an LSM-patched Linux 2.4.16 kernel, and an implementation of a static analysis version of this approach.",Access control models; Authorization mechanisms; Role-based access control,Data handling; Data privacy; Object oriented programming; Security of data; Software engineering; Statistical methods; Linux security modules; Software configuration management; Unauthorized access; Computer operating systems
Just fast keying: Key agreement in a hostile internet,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142632089&doi=10.1145%2f996943.996946&partnerID=40&md5=c5ea22bfb09040fa979113f2113da3aa,"We describe Just Fast Keying (JFK), a new key-exchange protocol, primarily designed for use in the IP security architecture. It is simple, efficient, and secure; we sketch a proof of the latter property. JFK also has a number of novel engineering parameters that permit a variety of tradeoffs, most notably the ability to balance the need for perfect forward secrecy against susceptibility to denial-of-service attacks.",Cryptography; Denial-of-service attacks,Data privacy; Internet; Network protocols; Public key cryptography; Software engineering; Denial of service attacks; Internet protocol (IP); Key exchange protocols; Security of data
Techniques and tools for analyzing intrusion alerts,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142632087&doi=10.1145%2f996943.996947&partnerID=40&md5=2b54fc24bfb97cbc506735e5e0d3c6d3,"Traditional intrusion detection systems (IDSs) focus on low-level attacks or anomalies, and raise alerts independently, though there may be logical connections between them. In situations where there are intensive attacks, not only will actual alerts be mixed with false alerts, but the amount of alerts will also become unmanageable. As a result, it is difficult for human users or intrusion response systems to understand the alerts and take appropriate actions. This paper presents a sequence of techniques to address this issue. The first technique constructs attack scenarios by correlating alerts on the basis of prerequisites and consequences of attacks. Intuitively, the prerequisite of an attack is the necessary condition for the attack to be successful, while the consequence of an attack is the possible outcome of the attack. Based on the prerequisites and consequences of different types of attacks, the proposed method correlates alerts by (partially) matching the consequences of some prior alerts with the prerequisites of some later ones. Moreover, to handle large collections of alerts, this paper presents a set of interactive analysis utilities aimed at facilitating the investigation of large sets of intrusion alerts. This paper also presents the development of a toolkit named TIAA, which provides system support for interactive intrusion analysis. This paper finally reports the experiments conducted to validate the proposed techniques with the 2000 DARPA intrusion detection scenario-specific datasets, and the data collected at the DEFCON 8 Capture the Flag event.",Alert correlation; Intrusion detection; Security management,Computer operating systems; Computer software; Data mining; Data privacy; Interactive computer systems; Alert correlation; Intrusion detection; Security management; Security of data
A key recovery attack on the 802.11b wired equivalent privacy protocol (WEP),2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142552643&doi=10.1145%2f996943.996948&partnerID=40&md5=a26292f5b21f5e4cb70bc5475345f48d,"In this paper, we present a practical key recovery attack on WEP, the link-layer security protocol for 802.11b wireless networks. The attack is based on a partial key exposure vulnerability in the RC4 stream cipher discovered by Fluhrer, Mantin, and Shamir. This paper describes how to apply this flaw to breaking WEP, our implementation of the attack, and optimizations that can be used to reduce the number of packets required for the attack. We conclude that the 802.11b WEP standard is completely insecure, and we provide recommendations on how this vulnerability could be mitigated and repaired.",Wired equivalent privacy; Wireless security,Data privacy; Internet; Network protocols; Public key cryptography; Wireless telecommunication systems; Wired equivalent privacy; Wireless networking; Wireless security; Security of data
Verifiable encryption of digital signatures and applications,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142585398&doi=10.1145%2f984334.984335&partnerID=40&md5=ced37d3a0c163aefa5b7a0b1b65183fd,"This paper presents a new simple schemes for verifiable encryption of digital signatures. We make use of a trusted third party (TTP) but in an optimistic sense, that is, the TTP takes part in the protocol only if one user cheats or simply crashes. Our schemes can be used as primitives to build efficient fair exchange and certified e-mail protocols.",Certified e-mail; Contract signing; Digital signatures; Fair exchange; Proof of knowledge; Public-key cryptography,Computer networks; Contracts; Electronic document identification systems; Electronic mail; Internet; Network protocols; Postal services; Probabilistic logics; Certified e-mail; Contract signing; Fair exchange; Proof of knowledge; Trusted third party (TTP); Public key cryptography
Crypto-based identifiers (CBIDs): Concepts and applications,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142596446&doi=10.1145%2f984334.984338&partnerID=40&md5=35db758141dedd861f50accc0f61fac1,"This paper addresses the identifier ownership problem. It does so by using characteristics of Statistical Uniqueness and Cryptographic Verifiability (SUCV) of certain entities which this document calls SUCV Identifiers and Addresses, or, alternatively, Crypto-based Identifiers. Their characteristics allow them to severely limit certain classes of denial-of-service attacks and hijacking attacks. SUCV addresses are particularly applicable to solve the address ownership problem that hinders mechanisms like Binding Updates in Mobile IPv6.",Address ownership; Authorization; Group management; Mobile IPv6; Opportunistic encryption; Security,Computer systems; Data privacy; Electronic document identification systems; Internet; Mobile computing; Mobile telecommunication systems; Network protocols; Problem solving; Routers; Security of data; Address ownership; Authorization; Crypto-Based identifiers (CBID); Group management; Mobile IPv6; Opportunistic encryption; Security; Public key cryptography
Tree-based group key agreement,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142512728&doi=10.1145%2f984334.984337&partnerID=40&md5=0da633ad4f9a0f0e06e86e29ad19c257,"Secure and reliable group communication is an active area of research. Its popularity is fueled by the growing importance of group-oriented and collaborative applications. The central research challenge is secure and efficient group key management. While centralized methods are often appropriate for key distribution in large multicast-style groups, many collaborative group settings require distributed key agreement techniques. This work investigates a novel group key agreement approach which blends key trees with Diffie-Hellman key exchange. It yields a secure protocol suite called Tree-based Group Diffie-Hellman (TGDH) that is both simple and fault-tolerant. Moreover, the efficiency of TGDH appreciably surpasses that of prior art.",Communication complexity; Cryptographic protocols; Group communication; Group key agreement; Security,Computational complexity; Contracts; Data communication systems; Distributed computer systems; Energy efficiency; Fault tolerant computer systems; Network protocols; Public key cryptography; Communication complexity; Cryptographic protocols; Group communication; Group key agreements; Security; Tree-based Group Diffle-Hellman (TGDH); Computer supported cooperative work
The UCON ABC usage control model,2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142523603&doi=10.1145%2f984334.984339&partnerID=40&md5=3d30bdcd2b0a5d9689d5ea0f4f229b82,"The family of UCON ABC models for Usage Control (UCON), which integrates authorizations, obligations, and conditions is discussed. The obligations are found to be the requirements that has to be fulfilled by obligation subjects for allowing access. The UCON ABC model integrated various diverse concepts in a unified framework. It is analyzed that the UCON ABC model enriched and refined the access control discipline in its definition and scope.",Access control; Digital rights management; Privacy; Trust; Usage control,Client server computer systems; Computer aided network analysis; Computer monitors; Computer operating systems; Decision making; Internet; Mathematical models; Mobile computing; Mobile telecommunication systems; Security of data; Access control; Core models; Digital rights management (DRM); Privacy; Trust; Usage control; Digital computers
"Use of nested certificates for efficient, dynamic, and trust preserving public key infrastructure",2004,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142558066&doi=10.1145%2f984334.984336&partnerID=40&md5=9029592e229aac9376bf4d4d9feb0d85,The nested certification and the corresponding subject certificate verification methods were proposed to improve certificate path verification times. It was found that the Nested public key infrastructure (NPKI) construction model was the transition from existing public key infrastructure (PKI) and the method to realize the transition was called the nested certificate propagation method. The verification time of a nested certificate path was found to be less than the classical certificate path. It was also found that the certificate revocation advantage of nested certificate paths helped the end users. It was suggested that the nested certificates would help to have hierarchical PKI for Wireless Application Protocol (WAP).,Digital certificates; Key management; Nested certificates; Public key infrastructure,Algorithms; Bandwidth; Computation theory; Cost effectiveness; Electric network topology; Electronic document identification systems; Electronic mail; Mobile computing; Postal services; Security of data; Servers; Websites; Digital certificates; Key management; Nested certificates; Privacy Enhanced Mail (PEM); Public key infrastructure (PKI); Public key cryptography
Flexible access control policy specification with constraint logic programming,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042584122&doi=10.1145%2f950191.950194&partnerID=40&md5=5efd35871da7a7ed444d73d78a13f75d,"We show how a range of role-based access control (RBAC) models may be usefully represented as constraint logic programs, executable logical specifications. The RBAC models that we define extend the ""standardμ RBAC models that are described by Sandhu et al., and enable security administrators to define a range of access policies that may include features, like denials of access and temporal authorizations, that are often useful in practice, but which are not widely supported in existing access control models. Representing access policies as constraint logic programs makes it possible to support certain policy options, constraint checks, and administrator queries that cannot be represented by using related methods (like logic programs). Representing an access control policy as a constraint logic program also enables access requests and constraint checks to be efficiently evaluated.",Constraint logic programming; Role-based access control,Computer programming languages; Constraint theory; Mathematical models; Problem solving; Public policy; Relational database systems; Authorization; Constraint logic programming; Control policy; Role-based access control; Logic programming
Clustering intrusion detection alarms to support root cause analysis,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142623031&doi=10.1145%2f950191.950192&partnerID=40&md5=ce6727bd3f3a38f59c11c17be01a390e,"It is a well-known problem that intrusion detection systems overload their human operators by triggering thousands of alarms per day. This paper presents a new approach for handling intrusion detection alarms more efficiently. Central to this approach is the notion that each alarm occurs for a reason, which is referred to as the alarm's root causes. This paper observes that a few dozens of rather persistent root causes generally account for over 90% of the alarms that an intrusion detection system triggers. Therefore, we argue that alarms should be handled by identifying and removing the most predominant and persistent root causes. To make this paradigm practicable, we propose a novel alarm-clustering method that supports the human analyst in identifying root causes. We present experiments with real-world intrusion detection alarms to show how alarm clustering helped us identify root causes. Moreover, we show that the alarm load decreases quite substantially if the identified root causes are eliminated so that they can no longer trigger alarms in the future.",Cluster analysis; Data mining; False positives; Intrusion detection; Root cause analysis,Alarm systems; Computer software; Data mining; Information management; Risk management; Security of data; Cluster analysis; False positives; Intrusion detection; Root cause analysis; Computer networks
Public-key support for group collaboration,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142521758&doi=10.1145%2f950191.950195&partnerID=40&md5=e0f691e7e6ff307abaa232bb50665c4d,"This paper characterizes the security of group collaboration as being a product not merely of cryptographic algorithms and coding practices, but also of the man-machine process of group creation. We show that traditional security mechanisms do not properly address the needs of a secured collaboration and present a research prototype, called NGC (next generation collaboration), that was designed to meet those needs. NGC distinguishes itself in the care with which the man-machine process was analyzed and shaped to improve the security of the whole process. We include a detailed analysis of the problem of binding a name to a key, traditionally thought to be the province of PKI, but we show that the SDSI local name concept produces a result with superior security to that produced by standard PKI.",Human-computer interface; IPsec; PGP; PKI; S/MIME; SDSI; SPKI; SSH,Algorithms; Human computer interaction; Internet; Mapping; Network protocols; Problem solving; Process control; Public key cryptography; Security of data; Human-computer interface; IPsec; PGP; PKI; S/MIME; SDSI; SPKI; SSH; Groupware
A Rule-Based Framework for Role-Based Delegation and Revocation,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042684910&doi=10.1145%2f937527.937530&partnerID=40&md5=1ac3b8b9b030fa0b8dfacfb5de59a352,"Delegation is the process whereby an active entity in a distributed environment authorizes another entity to access resources. In today's distributed systems, a user often needs to act on another user's behalf with some subset of his/her rights. Most systems have attempted to resolve such delegation requirements with ad-hoc mechanisms by compromising existing disorganized policies or simply attaching additional components to their applications. Still, there is a strong need in the large, distributed systems for a mechanism that provides effective privilege delegation and revocation management. This paper describes a rule-based framework for role-based delegation and revocation. The basic idea behind a role-based delegation is that users themselves may delegate role authorities to others to carry out some functions authorized to the former. We present a role-based delegation model called RDM2000 (role-based delegation model 2000) supporting hierarchical roles and multistep delegation. Different approaches for delegation and revocation are explored. A rulebased language for specifying and enforcing policies on RDM2000 is proposed.We describe a proof of-concept prototype implementation of RDM2000 to demonstrate the feasibility of the proposed framework and provide secure protocols for managing delegations. The prototype is a web-based application for law enforcement agencies allowing reliable delegation and revocation. The future directions are also discussed. © 2003, ACM. All rights reserved.",access control; delegation; Management; revocation; Role; rule-based; Security,
Policy Management Using Access Control Spaces,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968352943&doi=10.1145%2f937527.937528&partnerID=40&md5=591494273a38970368d2b17bfac583f6,"We present the concept of an access control space and investigate how it may be useful in managing access control policies. An access control space represents the permission assignment state of a subject or role. For example, the set of permissions explicitly assigned to a role defines its specified subspace, and the set of constraints precluding assignment to that role defines its prohibited subspace. In analyzing these subspaces, we identify two problems: (1) often a significant portion of an access control space has unknown assignment semantics, which indicates that the policy is underspecified; and (2) often high-level assignments and constraints that are easily understood result in conflicts, where resolution often leads to significantly more complex specifications. We have developed a prototype system, called Gokyo, that computes access control spaces. Gokyo identifies the unknown subspace to assist system administrators in developing more complete policy specifications. Also, Gokyo identifies conflicting subspaces and enables system administrators to resolve conflicts in a variety of ways in order to preserve the simplicity of constraint specification. We demonstrate Gokyo by analyzing aWeb server policy example and examine its utility by applying it to the SELinux example policy. Even for the extensive SELinux example policy, we find that only eight additional expressions are necessary to resolve Apache administrator policy conflicts. © 2003, ACM. All rights reserved.",Access control models; authorization mechanisms; Design; Management; role-based access control; Security,
A secure and private system for subscription-based remote services,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142585395&doi=10.1145%2f950191.950193&partnerID=40&md5=4c3299ef8ea696f4ac5bf7805b8204cd,"In this paper we study privacy issues regarding the use of the SSL/TLS protocol and X.509 certificates. Our main attention is placed on subscription-based remote services (e.g., subscription to newspapers and databases) where the service manager charges a flat fee for a period of time independent of the actual number of times the service is requested. We start by pointing out that restricting the access to such services by using X.509 certificates and the SSL/TLS protocol, while preserving the interests of the service managers, neglects the right to privacy of the users. We then propose the concept of a crypto certificate and the Secure and Private Socket Layer protocol (SPSL protocol, in short) and show how they can be used to preserve user privacy and, at the same time, protecting the interests of the service managers. The SPSL protocol only requires the user to have a standard X.509 certificate (with an RSA key) and does not require the user to get any special ad hoc certificate. Finally, we show the viability of the proposed solution by describing a system based on SPSL for secure and private access to subscription-based web services. Our implementation includes an SPSL proxy for a TLS-enabled web client and a module for the Apache web server along with administrative tools for the server side. The system has been developed starting from the implementation of an API for the SPSL protocol that we describe in the paper.",Access control; Anonymity; Cryptographic algorithms and protocols; Privacy; World-wide web,Algorithms; Cryptography; Data privacy; Database systems; Information management; Network protocols; World Wide Web; Access control; Anonymity; Cryptographic algorithms and protocols; Privacy; Security of data
Certificate-based authorization policy in a pki environment,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942654406&doi=10.1145%2f950191.950196&partnerID=40&md5=c59cf3e80d3eff481d02a5c8f4b93f1a,"The major emphasis of public key infrastructure has been to provide a cryptographically secure means of authenticating identities. However, procedures for authorizing the holders of these identities to perform specific actions still need additional research and development. While there are a number of proposed standards for authorization structures and protocols such as KeyNote, SPKI, and SAML based on X.509 or other key-based identities, none have been widely adopted. As part of an effort to use X.509 identities to provide authorization in highly distributed environments, we have developed and deployed an authorization service based on X.509 identified users and access policy contained in certificates signed by X.509 identified stakeholders. The major goal of this system, called Akenti, is to produce a usable authorization system for an environment consisting of distributed resources used by geographically and administratively distributed users. Akenti assumes communication between users and resources over a secure protocol such as transport layer security (TLS) to provide mutual authentication with X.509 certificates. This paper explains the authorization model and policy language used by Akenti, and how we have implemented an Apache authorization module to provide Akenti authorization.",Digital certificates; Public key infrastructure; XML,Computer operating systems; Interfaces (computer); Mathematical models; Network protocols; Public key cryptography; Public policy; Security of data; World Wide Web; XML; Authentication; Authorization; Digital certificates; Public key infrastructure; Software engineering
OCB: A Block-Cipher Mode of Operation for Efficient Authenticated Encryption,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-26444532494&doi=10.1145%2f937527.937529&partnerID=40&md5=96fe4b0f27a1d6944cd92de63daa585f,"We describe a parallelizable block-cipher mode of operation that simultaneously provides privacy and authenticity. OCB encrypts-and-authenticates a nonempty string Mϵ{0,1}* using [|M|/n]+2 block-cipher invocations, where n is the block length of the underlying block cipher. Additional overhead is small. OCB refines a scheme, IAPM, suggested by Charanjit Jutla. Desirable properties of OCB include the ability to encrypt a bit string of arbitrary length into a ciphertext of minimal length, cheap offset calculations, cheap key setup, a single underlying cryptographic key, no extended-precision addition, a nearly optimal number of block-cipher calls, and no requirement for a random IV. We prove OCB secure, quantifying the adversary's ability to violate the mode's privacy or authenticity in terms of the quality of its block cipher as a pseudorandom permutation (PRP) or as a strong PRP, respectively. © 2003, ACM. All rights reserved.",AES; authenticity; block-cipher usage; cryptography; encryption; integrity; modes of operation; Performance; provable security; Security; standards; Theory,
"Bluebox: A policy-driven, host-based intrusion detection system",2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942749266&doi=10.1145%2f762476.762477&partnerID=40&md5=e379f942522ffab5e810ae7f79cf9064,"Detecting attacks against systems has, in practice, largely been delegated to sensors, such as network intrustion detection systems. However, due to the inherent limitations of these systems and the increasing use of encryption in communication, intrusion detection and prevention have once again moved back to the host systems themselves. In this paper, we describe our experiences with building BlueBox, a host-based intrusion detection system. Our approach, based on the technique of system call introspection, can be viewed as creating an infrastructure for defining and enforcing very fine-grained process capabilities in the kernel. These capabilities are specified as a set of rules (policies) for regulating access to system resources on a per executable basis. The language for expressing the rules is intuitive and sufficiently expressive to effectively capture security boundaries. We have prototyped our approach on Linux operating system kernel and have built rule templates for popular daemons such as Apache and wu-ftpd. Our design has been validated by testing against a comprehensive database of known attacks. Our system has been designed to minimize the kernel changes and performance impact and thus can be ported easily to new kernels. We describe the motivation and rationale behind BlueBox, its design, implementation on Linux, and how it relates to prior work on detecting and preventing intrusions on host systems.",Intrusion detection; Policy; Sandboxing; System call introspection,Computer networks; Computer operating systems; Computer system firewalls; Cryptography; Network protocols; Servers; Communication flow; Intrusion detection; Sandboxing; System call introspection; Information technology
Administrative scope: A foundation for role-based administrative models,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142510989&doi=10.1145%2f762476.762478&partnerID=40&md5=2738e794ea304511aa1e02401f3b47fe,"We introduce the concept of administrative scope in a role hierarchy and demonstrate that it can be used as a basis for role-based administration. We then develop a family of models for role hierarchy administration (RHA) employing administrative scope as the central concept. We then extend RHA 4, the most complex model in the family, to a complete, decentralized model for role-based administration. We show that SARBAC, the resulting role-based administrative model, has significant practical and theoretical advantages over ARBAC97. We also discuss how administrative scope might be applied to the administration of general hierarchical structures, how our model can be used to reduce inheritance in the role hierarchy, and how it can be configured to support discretionary access control features.",Administrative scope; Encapsulated range; Role hierarchy operation; Role-based access control; Role-based administration,Algorithms; Computer operating systems; Constraint theory; Distributed computer systems; Graph theory; Hierarchical systems; Information management; Mathematical models; Security of data; Administrative scope; Encapsulated range; Role hierarchy operation; Role-based access control; Role-based administration; Information technology
Access control with IBM Tivoli Access Manager,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942624556&doi=10.1145%2f762476.762479&partnerID=40&md5=beed4fa93b22d330ba922761b3b39130,"Web presence has become a key consideration for the majority of companies and other organizations. Besides being an essential information delivery tool, the Web is increasingly being regarded as an extension of the organization itself, directly integrated with its operating processes. As this transformation takes place, security grows in importance. IBM Tivoli Access Manager offers a shared infrastructure for authentication and access management, technologies that have begun to emerge in the commercial marketplace. This paper describes the Authorization Service provided by IBM Tivoli Access Manager for e-business (AM) and its use by AM family members as well as third-party applications. Policies are defined over a protected object namespace and stored in a database, which is managed via a management console and accessed through an Authorization API. The protected object namespace abstracts from heterogeneous systems and thus enables the definition of consistent policies and their centralized management. ACL inheritance and delegated management allow these policies to be managed efficiently. The Authorization API allows applications with their own access control requirements to decouple authorization logic from application logic. Policy checking can be externalized by using either a proxy that sits in front of the Web servers and application servers or a plug-in that examines the request. Thus, AM familiy members establish a single entry point to enforce enterprise policies that regulate access to corporate data.",Access control; Authorization management; Web servers; WWW security,Computer operating systems; Data processing; Information management; Public policy; Security of data; Servers; Access control; Authorization management; Web servers; WWW security; World Wide Web
Efficient multicast stream authentication using erasure codes,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542625909&doi=10.1145%2f762476.762480&partnerID=40&md5=8111b2171d26f083c89c4fb3a3aec9d5,"We describe a novel method for authenticating multicast packets that is robust against packet loss. Our focus is to minimize the size of the communication overhead required to authenticate the packets. Our approach is to encode the hash values and the signatures with Rabin's Information Dispersal Algorithm (IDA) to construct an authentication scheme that amortizes a single signature operation over multiple packets. This strategy is especially efficient in terms of space overhead, because just the essential elements needed for authentication (i.e., one hash per packet and one signature per group of packets) are used in conjunction with an erasure code that is space optimal. Using asymptotic techniques, we derive the authentication probability of our scheme using two different bursty loss models. A lower bound of the authentication probability is also derived for one of the loss models. To evaluate the performance of our scheme, we compare our technique with four other previously proposed schemes using empirical results.",Digital signatures; Erasure codes; Information Dispersal Algorithm (IDA); Multicast,Algorithms; Codes (symbols); Electronic document identification systems; Groupware; Mathematical models; Packet networks; Probability distributions; Erasure codes; Group communication; Information Dispersal Algorithm (IDA); Multicast; Information technology
A propositional policy algebra for access control,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142523600&doi=10.1145%2f762476.762481&partnerID=40&md5=f59b954a611fee854a41cc86dae3f714,"Security-sensitive environments protect their information resources against unauthorized use by enforcing access control mechanisms driven by access control policies. Due to the need to compare, contrast, and compose such protected information resources, access control policies regulating their manipulation need to be compared, contrasted, and composed. An algebra for manipulating such access control policies at a higher (propositional) level, where the operations of the algebra are abstracted from their specification details, is the subject of this paper. This algebra is applicable to policies that have controlled nondeterminism and all or nothing assignments of access privileges in their specification. These requirements reflect current practices in discretionary and role-based access control models. Therefore, the proposed algebra can be used to reason about role-based access control policies combined with other forms of discretionary policies. We show how to use algebraic identities to reason about consistency, completeness, and determinacy of composed policies using similar properties of their constituents.",Access control; Policy algebra; Policy composition; Security policy,Algebra; Database systems; Information management; Mathematical models; Public policy; Security of data; Set theory; Access control; Information systems; Policy algebra; Policy composition; Security policy; Information technology
A logical framework for reasoning about access control models,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041524664&doi=10.1145%2f605434.605437&partnerID=40&md5=84b19ca165aac4920d41554af64bf2d5,"The increased awareness of the importance of data protection has made access control a relevant component of current data management systems. Moreover, emerging applications and data models call for flexible and expressive access control models. This has led to an extensive research activity that has resulted in the definition of a variety of access control models that differ greatly with respect to the access control policies they support. Thus, the need arises for developing tools for reasoning about the characteristics of these models. These tools should support users in the tasks of model specification, analysis of model properties, and authorization management. For example, they must be able to identify inconsistencies in the model specification and must support the administrator in comparing the expressive power of different models. In this paper, we make a first step in this direction by proposing a formal framework for reasoning about access control models. The framework we propose is based on a logical formalism and is general enough to model discretionary, mandatory, and role-based access control models. Each instance of the proposed framework corresponds to a C-Datalog program, interpreted according to a stable model semantics. In the paper, besides giving the syntax and the formal semantics of our framework, we show some examples of its application. Additionally, we present a number of dimensions along which access control models can be analyzed and compared. For each dimension, we show decidability results and we present some examples of its application.",Access control framework; Access control models analysis; Logic programming,C (programming language); Computer software; Data privacy; Database systems; Semantics; Access control models; Database administration; Model specification; Security of data
Delegation logic: A logic-based approach to distributed authorization,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041524661&doi=10.1145%2f605434.605438&partnerID=40&md5=1ab53449b2d2e046ef868702361f1e9c,"We address the problem of authorization in large-scale, open, distributed systems. Authorization decisions are needed in electronic commerce, mobile-code execution, remote resource sharing, privacy protection, and many other applications. We adopt the trust-management approach, in which ""authorization"" is viewed as a ""proof-of-compliance"" problem: Does a set of credentials prove that a request complies with a policy? We develop a logic-based language, called Delegation Logic (DL), to represent policies, credentials, and requests in distributed authorization. In this paper, we describe D1LP, the monotonic version of DL. D1LP extends the logic-programming (LP) language Datalog with expressive delegation constructs that feature delegation depth and a wide variety of complex principals (including, but not limited to, k-out-of-n thresholds). Our approach to defining and implementing D1LP is based on tractably compiling D1LP programs into ordinary logic programs (OLPs). This compilation approach enables D1LP to be implemented modularly on top of existing technologies for OLP, for example, Prolog. As a trust-management language, D1LP provides a concept of proof-of-compliance that is founded on well-understood principles of logic programming and knowledge representation. D1LP also provides a logical framework for studying delegation.",Access control; Delegation Logic; Distributed system security; Logic programs; Trust management,Data privacy; Distributed computer systems; Formal logic; Knowledge representation; Logic programming; Access control; Distributed system security; Trust management; Security of data
On the relationship between strand spaces and multi-agent systems,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142594319&doi=10.1145%2f605434.605436&partnerID=40&md5=c9e123d1c4be0b18224ae1bb4d05a9c5,"Strand spaces are a popular framework for the analysis of security protocols. Strand spaces have some similarities to a formalism used successfully to model protocols for distributed systems, namely multi-agent systems. We explore the exact relationship between these two frameworks here. It turns out that a key difference is the handling of agents, which are unspecified in strand spaces and explicit in multi-agent systems. We provide a family of translations from strand spaces to multi-agent systems parameterized by the choice of agents in the strand space. We also show that not every multi-agent system of interest can be expressed as a strand space. This reveals a lack of expressiveness in the strand-space framework that can be characterized by our translation. To highlight this lack of expressiveness, we show one simple way in which strand spaces can be extended to model more systems.",Agents; Expressiveness; Multi-agent systems; Security protocols; Strand spaces,Computer operating systems; Computer programming languages; Data privacy; Network protocols; Security of data; Expressiveness; Security protocols; Strand spaces; Multi agent systems
Supporting structured credentials and sensitive policies through interoperable strategies for automated trust negotiation,2003,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1642327001&doi=10.1145%2f605434.605435&partnerID=40&md5=5f4fd066f86918574104179e05fe808f,"Business and military partners, companies and their customers, and other closely cooperating parties may have a compelling need to conduct sensitive interactions on line, such as accessing each other's local services and other local resources. Automated trust negotiation is an approach to establishing trust between parties so that such interactions can take place, through the use of access control policies that specify what combinations of digital credentials a stranger must disclose to gain access to a local resource. A party can use many different strategies to negotiate trust, offering tradeoffs between the length of the negotiation, the amount of extraneous information disclosed, and the computational effort expended. To preserve parties' autonomy, each party should ideally be able to choose its negotiation strategy independently, while still being guaranteed that negotiations will succeed whenever possible - that the two parties' strategies will interoperate. In this paper we provide the formal underpinnings for that goal, by formalizing the concepts of negotiation protocols, strategies, and interoperation. We show how to model the information flow of a negotiation for use in analyzing strategy interoperation. We also present two large sets of strategies whose members all interoperate with one another, and show that these sets contain many practical strategies. We develop the theory for black-box propositional credentials as well as credentials with internal structure, and for access control policies whose contents are (respectively are not) sensitive. We also discuss how these results fit into TrustBuilder, our prototype system for trust negotiation.",Access control; Automated trust negotiation; Digital credentials; Interoperable strategies,Computer software; Computer supported cooperative work; Data privacy; Internet; Interoperability; Network protocols; Automated trust negotiations; Interoperable strategies; TrustBuilder; Security of data
The Economics of Information Security Investment,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990029315&doi=10.1145%2f581271.581274&partnerID=40&md5=885b41edbef418440f12951f4c9ac3ae,"This article presents an economic model that determines the optimal amount to invest to protect a given set of information. The model takes into account the vulnerability of the information to a security breach and the potential loss should such a breach occur. It is shown that for a given potential loss, a firm should not necessarily focus its investments on information sets with the highest vulnerability. Since extremely vulnerable information sets may be inordinately expensive to protect, a firm may be better off concentrating its efforts on information sets with midrange vulnerabilities. The analysis further suggests that to maximize the expected benefit from investment to protect information, a firm should spend only a small fraction of the expected loss due to a security breach. © 2002, ACM. All rights reserved.",Economics; Optimal security investment; Security,
User Authentication through Keystroke Dynamics,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650165017&doi=10.1145%2f581271.581272&partnerID=40&md5=1617ea20e8d516ae3d1d1f11be02ddca,"Unlike other access control systems based on biometric features, keystroke analysis has not led to techniques providing an acceptable level of accuracy. The reason is probably the intrinsic variability of typing dynamics, versus other-very stable-biometric characteristics, such as face or fingerprint patterns. In this paper we present an original measure for keystroke dynamics that limits the instability of this biometric feature. We have tested our approach on 154 individuals, achieving a False Alarm Rate of about 4% and an Impostor Pass Rate of less than 0.01%. This performance is reached using the same sampling text for all the individuals, allowing typing errors, without any specific tailoring of the authentication system with respect to the available set of typing samples and users, and collecting the samples over a 28.8-Kbaud remote modem connection. © 2002, ACM. All rights reserved.",Biometric techniques; Experimentation; keystroke analysis; Security,
Improving the Granularity of Access Control for Windows 2000,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961645820&doi=10.1145%2f581271.581273&partnerID=40&md5=396d75fa0c38a2d425692fe6492265a8,"This article presents the mechanisms in Windows 2000 that enable fine-grained and centrally managed access control for both operating system components and applications. These features were added during the transition from Windows NT 4.0 to support the Active Directory, a new feature in Windows 2000, and to protect computers connected to the Internet. While the access control mechanisms in Windows NT are suitable for file systems and applications with simple requirements, they fall short of the needs of applications with complex data objects. Our goal was to use operating system access control mechanisms to protect a large object hierarchy with many types of objects, each with many data properties. We also wanted to reduce the exposure of users to untrustworthy or exploited programs. We introduced three extensions to support these goals. First, we extended the entries in access control lists to provide an unlimited number of access rights for a single object and to allow grouping those rights for efficiency. Second, we extended the entries to specify precisely how access control lists are assigned to each distinct type of object, instead of treating all types identically. Finally, we extended the data structure identifying users' identity to the operating system to allow users to restrict the set of objects a program may access. These changes allow a single access control mechanism to be used to protect both system and application resources, as well as protect users from each other and users from their programs, simplifying both program development and system management. © 2002, ACM. All rights reserved.",Access control lists; active directory; Design; Microsoft Windows 2000; Performance; Security; Windows NT,
A Methodology for Analyzing the Performance of Authentication Protocols,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12844277408&doi=10.1145%2f581271.581275&partnerID=40&md5=963a8d5a54e4867fd6b1e47d1f526c31,"Performance, in terms of user response time and the consumption of processing and communications resources, is an important factor to be considered when designing authentication protocols. The mix of public key and secret key encryption algorithms typically included in these protocols makes it difficult to model performance using conventional analytical methods. In this article, we develop a validated modeling methodology to be used for analyzing authentication protocol features, and we use two examples to illustrate the methodology. In the first example, we analyze the environmental parameters that favor one proposed public-key-enabled Kerberos variant over another in the context of a large, multiple-realm network. In the second example, we propose a Kerberos variant for a mobile computing environment and analyze the performance benefits realized by introducing a proxy to offload processing and communications workload. © 2002, ACM. All rights reserved.",Authentication; Design; Kerberos; Measurement; mobile computing; Performance; performance modeling; proxy servers; public key cryptography; Security,
A Model of OASIS Role-Based Access Control and Its Support for Active Security,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865788031&doi=10.1145%2f581271.581276&partnerID=40&md5=46c87cbdf0095d801548a4b52775afa6,"OASIS is a role-based access control architecture for achieving secure interoperation of services in an open, distributed environment. The aim of OASIS is to allow autonomous management domains to specify their own access control policies and to interoperate subject to service level agreements (SLAs). Services define roles and implement formally specified policy to control role activation and service use; users must present the required credentials, in an appropriate context, in order to activate a role or invoke a service. All privileges are derived from roles, which are activated for the duration of a session only. In addition, a role is deactivated immediately if any of the conditions of the membership rule associated with its activation becomes false. These conditions can test the context, thus ensuring active monitoring of security. To support the management of privileges, OASIS introduces appointment. Users in certain roles are authorized to issue other users with appointment certificates, whichmay be a prerequisite for activating one or more roles. The conditions for activating a role at a service may include appointment certificates as well as prerequisite roles and constraints on the context. An appointment certificate does not therefore convey privileges directly but can be used as a credential for role activation. The lifetime of appointment certificates is not restricted to the issuing session, so they can be used as long-lived credentials to represent academic and professional qualification, or membership of an organization. Role-based access control (RBAC), in associating privileges with roles, provides a means of expressing access control that is scalable to large numbers of principals. However, pure RBAC associates privileges only with roles, whereas applications often require more fine-grained access control. Parametrized roles extend the functionality to meet this need. We motivate our approach and formalise OASIS. We first present the overall architecture through a basic model, followed by an extended model that includes parametrization. © 2002, ACM. All rights reserved.",Certificates; Design; distributed systems; Management; OASIS; policy; RBAC; role-based access control; Security; service-level agreements; Theory,
A Fine-Grained Access Control System for XML Documents,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001697848&doi=10.1145%2f505586.505590&partnerID=40&md5=6cf9b23705b4c468ccf5951fcd0443e8,"Web-based applications greatly increase information availability and ease of access, which is optimal for public information. The distribution and sharing of information via the Web that must be accessed in a selective way, such as electronic commerce transactions, require the definition and enforcement of security controls, ensuring that information will be accessible only to authorized entities. Different approaches have been proposed that address the problem of protecting information in a Web system. However, these approaches typically operate at the file-system level, independently of the data that have to be protected from unauthorized accesses. Part of this problem is due to the limitations of HTML, historically used to design Web documents. The extensible markup language (XML), a markup language promoted by the World Wide Web Consortium (W3C), is de facto the standard language for the exchange of information on the Internet and represents an important opportunity to provide fine-grained access control. We present an access control model to protect information distributed on the Web that, by exploiting XML's own capabilities, allows the definition and enforcement of access restrictions directly on the structure and content of the documents. We present a language for the specification of access restrictions, which uses standard notations and concepts, together with a description of a system architecture for access control enforcement based on existing technology. The result is a flexible and powerful security system offering a simple integration with current solutions. © 2002, ACM. All rights reserved.",Design; Hard Faults; Performance; Phase Change Memory; Reliability; Salvaging; Wear-Leveling,
A Graph-Based Formalism for RBAC,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008430937&doi=10.1145%2f545186.545191&partnerID=40&md5=7565d762d7e19aef15f62f985d797f06,"Role-Based Access Control (RBAC) is supported directly or in a closely related form, by a number of products. This article presents a formalization of RBAC using graph transformations that is a graphical specification technique based on a generalization of classical string grammars to nonlinear structures. The proposed formalization provides an intuitive description for the manipulation of graph structures as they occur in information systems access control and a precise specification of static and dynamic consistency conditions on graphs and graph transformations. The formalism captures the RBAC models published in the literature, and also allows a uniform treatment of user roles and administrative roles, and a detailed analysis of the decentralization of administrative roles. © 2002, ACM. All rights reserved.",Access control in information systems; Administration; Correctness; Decentralized; Design; Graph transformations; Management; Permission management; Role-Based access control; Security,
Termination in Language-Based Systems,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015594409&doi=10.1145%2f505586.505589&partnerID=40&md5=5abc6a3ab83cb51c9faaa7f3b330375a,"Language run-time systems are increasingly being embedded in systems to support run-time extensibility via mobile code. Such systems raise a number of concerns when the code running in such systems is potentially buggy or untrusted. Although sophisticated access controls have been designed for mobile code and are shipping as part of commercial systems such as Java, there is no support for terminating mobile code short of terminating the entire language run-time. This article presents a concept called “soft termination” that can be applied to virtually any mobile code system. Soft termination allows mobile code threads to be safely terminated while preserving the stability of the language run-time. In addition, function bodies can be permanently disabled, thwarting attacks predicated on system threads eventually calling untrusted functions. Soft termination guarantees termination by breaking any potential infinite loops in mobile code. We present a formal design for soft termination and an implementation of it for Java, built using Java bytecode rewriting, which demonstrates reasonable performance (3 to 25% slowdowns on benchmarks). © 2002, ACM. All rights reserved.",Applets; Internet; Java; Languages; Resource Control; Security; Soft Termination; Termination,
REMUS: A Security-Enhanced Operating System,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867854789&doi=10.1145%2f504909.504911&partnerID=40&md5=e7868fcd6949c114bd40a94f2639f8d7,"We present a detailed analysis of the UNIX system calls and classify them according to their level of threat with respect to system penetration. Based on these results, an effective mechanism is proposed to control the invocation of critical, from the security viewpoint, system calls. The integration into existing UNIX operating systems is carried out by instrumenting the code of the system calls in such a way that the execution is granted only in the case where the invoking process and the value of the arguments comply with the rules held in an access control database. This method does not require changes in the kernel data structures and algorithms. All kernel modifications are transparent to the application processes that continue to work correctly with no need of source code changes or recompilation. A working prototype has been implemented as a loadable kernel module for the Linux operating system. The prototype is able to detect and block any attacks by which an intruder tries to gain direct access to the system as a privileged user. © 2002, ACM. All rights reserved.",Access Control; Design; Linux; Privileged Tasks; Security; System Calls Interception; System Penetration,
An Algebra for Composing Access Control Policies,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025424440&doi=10.1145%2f504909.504910&partnerID=40&md5=b4492b300ae93e681b9f8d2f7d98469e,"Despite considerable advancements in the area of access control and authorization languages, current approaches to enforcing access control are all based on monolithic and complete specifications. This assumption is too restrictive when access control restrictions to be enforced come from the combination of different policy specifications, each possibly under the control of independent authorities, and where the specifics of some component policies may not even be known a priori. Turning individual specifications into a coherent policy to be fed into the access control system requires a nontrivial combination and translation process. This article addresses the problem of combining authorization specifications that may be independently stated, possibly in different languages and according to different policies. We propose an algebra of security policies together with its formal semantics and illustrate how to formulate complex policies in the algebra and reason about them. A translation of policy expressions into equivalent logic programs is illustrated, which provides the basis for the implementation of the algebra. The algebra's expressiveness is analyzed through a comparison with first-order logic. © 2002, ACM. All rights reserved.",Access Control; Algebra; Design; Logic Programs; Policy Composition; Security,
Information Leakage from Optical Emanations,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3543094762&doi=10.1145%2f545186.545189&partnerID=40&md5=1fdeb10ab3d38bb4eb1b9a8ddb190eba,"A previously unknown form of compromising emanations has been discovered. LED status indicators on data communication equipment, under certain conditions, are shown to carry a modulated optical signal that is significantly correlated with information being processed by the device. Physical access is not required; the attacker gains access to all data going through the device, including plaintext in the case of data encryption systems. Experiments show that it is possible to intercept data under realistic conditions at a considerable distance. Many different sorts of devices, including modems and Internet Protocol routers, were found to be vulnerable. A taxonomy of compromising optical emanations is developed, and design changes are described that will successfully block this kind of “Optical TEMPEST” attack. © 2002, ACM. All rights reserved.",Comint; Communication; Compromising emanations; COMSEC; Covert channel; EMSEC; Encryption; Experimentation; Fiber optics; Information displays; Light emitting diode (LED); Security; SIGINT; TEMPEST,
Token-Based Scanning of Source Code for Security Problems,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016660264&doi=10.1145%2f545186.545188&partnerID=40&md5=1298b96512c46854f83a92e4c0895be8,"We describe ITS4, a tool for statically scanning C and CCC source code for security vulnerabilities. Compared to other approaches, our scanning technique stakes out a new middle ground between accuracy and efficiency. This method is efficient enough to offer real-time feedback to developers during coding while producing few false negatives. Unlike other techniques, our method is also simple enough to scan C++ code despite the complexities inherent in the language. Using ITS4, we found new remotely exploitable vulnerabilities in a widely distributed software package as well as in a major piece of e-commerce software. We also describe functionality in more recent tools modeled after ITS4, and discuss algorithms that could easily be used to augment these kinds of tools. Particularly, we describe a solution we have prototyped that allows for more rigorous analysis of C and C++ source code, without failing to analyze parts of the program due to preprocessor conditionals. © 2002, ACM. All rights reserved.",Buffer overflows; Languages; Race conditions; Security; Security analysis,
"Simple, State-Based Approaches to Program-Based Anomaly Detection",2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882800335&doi=10.1145%2f545186.545187&partnerID=40&md5=ddf7720eaf77a6e107cad04a97914e94,"This article describes variants of two state-based intrusion detection algorithms from Michael and Ghosh [2000] and Ghosh et al. [2000], and gives experimental results on their performance. The algorithms detect anomalies in execution audit data. One is a simply constructed finite-state machine, and the other two monitor statistical deviations from normal program behavior. The performance of these algorithms is evaluated as a function of the amount of available training data, and they are compared to the well-known intrusion detection technique of looking for novel n-grams in computer audit data. © 2002, ACM. All rights reserved.",Anomaly Detection; Experimentation; Finite Automata; Information System Security; Intrusion Detection; Machine Learning; Performance; Security,
Trust Management for IPsec,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958235938&doi=10.1145%2f505586.505587&partnerID=40&md5=bcb9cee5693fcd526c7e8fd833f15ce0,"IPsec is the standard suite of protocols for network-layer confidentiality and authentication of Internet traffic. The IPsec protocols, however, do not address the policies for how protected traffic should be handled at security endpoints. This article introduces an efficient policy management scheme for IPsec, based on the principles of trust management. A compliance check is added to the IPsec architecture that tests packet filters proposed when new security associations are created for conformance with the local security policy, based on credentials presented by the peer host. Security policies and credentials can be quite sophisticated (and specified in the trust-management language), while still allowing very efficient packet-filtering for the actual IPsec traffic.We present a practical portable implementation of this design, based on the KeyNote trust-management language, that works with a variety of UNIX-based IPsec implementations. Finally, we discuss some applications of the enhanced IPsec architecture. © 2002, ACM. All rights reserved.",Credentials; Ipsec; Keynote; Languages; Network Security; Policy; Security; Trust Management,
Secure and Selective Dissemination of XML Documents,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001786260&doi=10.1145%2f545186.545190&partnerID=40&md5=cdd7211b46d91e3ddcbfbca4af1c40dc,"XML (eXtensible Markup Language) has emerged as a prevalent standard for document representation and exchange on the Web. It is often the case that XML documents contain information of different sensitivity degrees that must be selectively shared by (possibly large) user communities. There is thus the need for models and mechanisms enabling the specification and enforcement of access control policies for XML documents. Mechanisms are also required enabling a secure and selective dissemination of documents to users, according to the authorizations that these users have. In this article, we make several contributions to the problem of secure and selective dissemination of XML documents. First, we define a formal model of access control policies for XML documents. Policies that can be defined in our model take into account both user profiles, and document contents and structures. We also propose an approach, based on an extension of the CryptolopeTM approach [Gladney and Lotspiech 1997], which essentially allows one to send the same document to all users, and yet to enforce the stated access control policies. Our approach consists of encrypting different portions of the same document according to different encryption keys, and selectively distributing these keys to the various users according to the access control policies. We show that the number of encryption keys that have to be generated under our approach is minimal and we present an architecture to support document distribution. © 2002, ACM. All rights reserved.",Access control; Secure distribution; Security; XML,
An Algebraic Approach to IP Traceback,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025403958&doi=10.1145%2f505586.505588&partnerID=40&md5=c4bef027e3dffa880dcbe455c81d3475,"We present a new solution to the problem of determining the path a packet traversed over the Internet (called the traceback problem) during a denial-of-service attack. This article reframes the traceback problem as a polynomial reconstruction problem and uses algebraic techniques from coding theory and learning theory to provide robust methods of transmission and reconstruction. © 2002, ACM. All rights reserved.",Algorithms; Design; Internet protocol; Security; Traceback,
An Authorization Model for Temporal and Derived Data: Securing Information Portals,2002,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008724838&doi=10.1145%2f504909.504912&partnerID=40&md5=41655350a8314790aa14f3b3cec34b0a,"The term information portals refers to Web sites that serve as main providers of focused information, gathered from distributed data sources. Gathering and disseminating information through information portals introduce new security challenges. In particular, the authorization specifications, as well as the granting process, are temporal by nature. Also, more often than not, the information provided by the portal is in fact derived from more than one backend data source. Therefore, any authorization model for information portals should support access control based on temporal characteristics of the data, and also should provide tools to prevent indirect unauthorized access through the use of derived data. In this article we focus our attention on devising such an authorization model. The distinguishing features of this model include: (1) the specification of authorizations based on temporal characteristics of data, and (2) a formal framework to derive authorizations in a consistent and safe manner, based on relationships among data. © 2002, ACM. All rights reserved.",Access Control; Authorization Administration; Derived Data; Security; Temporal Data,
A Nested Transaction Model for Multilevel Secure Database Management Systems,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995455083&doi=10.1145%2f503339.503340&partnerID=40&md5=0e21ae4a333eedb11e7ac8c2a0f4e7b6,"This article presents an approach to concurrency control for transactions in a Multilevel Secure Database Management System (MLS/DBMS). The major problem is that concurrency control mechanisms used in traditional DBMSs are not adequate in a MLS/DBMS, since they may be exploited to establish covert channels. The approach presented in this article, which uses single-version data items, is based on the use of nested transactions, application-level recovery, and notification-based locking protocols. All these features allow us to develop a concurrency control mechanism that is free of timing channels and avoids many of the shortcomings of the concurrency control mechanisms so far developed for conventional (i.e., flat) transactions, such as transaction starvation and resource wastage. © 2001, ACM. All rights reserved.",Concurrency control; Covert channels; Multilevel secure database management systems; Nested transactions; Security,
Role-Based Access Control on the Web,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994391858&doi=10.1145%2f383775.383777&partnerID=40&md5=0c66fef30c185635d97c57572629e64a,"Current approaches to access control on Web servers do not scale to enterprise-wide systems because they are mostly based on individual user identities. Hence we were motivated by the need to manage and enforce the strong and efficient RBAC access control technology in large-scale Web environments. To satisfy this requirement, we identify two different architectures for RBAC on the Web, called user-pull and server-pull. To demonstrate feasibility, we implement each architecture by integrating and extending well-known technologies such as cookies, X.509, SSL, and LDAP, providing compatibility with current Web technologies. We describe the technologies we use to implement RBAC on the Web in different architectures. Based on our experience, we also compare the tradeoffs of the different approaches. © 2001, ACM. All rights reserved.",Cookies; Design; Digital certificates; Experimentation; Role-Based access control; Security; WWW security,
An Unknown Key-Share Attack on the MQV Key Agreement Protocol,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649746889&doi=10.1145%2f501978.501981&partnerID=40&md5=d150dc806a82a731f59ab6ab7014130a,"The MQV key agreement protocol, a technique included in recent standards, is shown in its basic form to be vulnerable to an unknown key-share attack. Although the attack's practical impact on security is minimal—a key confirmation step easily prevents it—the attack is noteworthy in the principles it illustrates about protocol design. First, minor “efficiency improvements” can significantly alter the security properties of a protocol. Second, protocol analysis must consider potential interactions with all parties, not just those that are normally online. Finally, attacks must be assessed in terms of system requirements, not just in isolation. © 2001, ACM. All rights reserved.",Algorithms; Key agreement; MQV; Protocol design; Security; Unknown key-share attack,
The Architecture and Performance of Security Protocols in the Ensemble Group Communication System: Using Diamonds to Guard the Castle,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010853294&doi=10.1145%2f501978.501982&partnerID=40&md5=933e379608f0166e2ef8c56b365970e2,"Ensemble is a Group Communication System built at Cornell and the Hebrew universities. It allows processes to create process groups within which scalable reliable fifo-ordered multicast and point-to-point communication are supported. The system also supports other communication properties, such as causal and total multicast ordering, flow control, and the like. This article describes the security protocols and infrastructure of Ensemble. Applications using Ensemble with the extensions described here benefit from strong security properties. Under the assumption that trusted processes will not be corrupted, all communication is secured from tampering by outsiders. Our work extends previous work performed in the Horus system (Ensemble's predecessor) by adding support for multiple partitions, efficient rekeying, and application-defined security policies. Unlike Horus, which used its own security infrastructure with nonstandard key distribution and timing services, Ensemble's security mechanism is based on off-the shelf authentication systems, such as PGP and Kerberos.We extend previous results on group rekeying, with a novel protocol that makes use of diamondlike data structures. Our Diamond protocol allows the removal of untrusted members within milliseconds. In this work we are considering configurations of hundreds of members, and further assume that member trust policies are symmetric and transitive. These assumptions dictate some of our design decisions. © 2001, ACM. All rights reserved.",Group communication; Reliability; Security; Security,
Practical Safety in Flexible Access Control Models,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006937063&doi=10.1145%2f501963.501966&partnerID=40&md5=6caeba9bcf05ca400960933644c2a44e,"Assurance that an access control configuration will not result in the leakage of a right to an unauthorized principal, called safety, is fundamental to ensuring that the most basic of access control policies can be enforced. It has been proven that the safety of an access control configuration cannot be decided for a general access control model, such as Lampson's access matrix, so safety is achieved either through the use of limited access control models or the verification of safety via constraints. Currently, almost all safety critical systems use limited access control models, such as Bell—LaPadula or Domain and Type Enforcement, because constraint expression languages are far too complex for typical administrators to use properly. However, researchers have identified that most constraints belong to one of a few basic types, so our goal is to develop a constraint expression model in which these constraints can be expressed in a straightforward way and extensions can be made to add other constraints, if desired. Our approach to expressing constraints has the following properties: (1) an access control policy is expressed using a graphical model in which the nodes represent sets (e.g., of subjects, objects, etc.) and the edges represent binary relationships on those sets and (2) constraints are expressed using a few, simple set operators on graph nodes. The basic graphical model is very simple, and we extend this model only as necessary to satisfy the identified constraint types. Since the basic graphical model is also general, further extension to support other constraints is possible, but such extensions should be made with caution as each increases the complexity of the model. Our hope is that by keeping the complexity of constraint expression in check, flexible access control models, such as role-based access control, may also be used for expressing access control policy for safety-critical systems. © 2001, ACM. All rights reserved.",Access control; Access control models; Authorization mechanisms; Design; Management; Role-Based; Security,
Abstraction-Based Intrusion Detection in Distributed Environments,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883386223&doi=10.1145%2f503339.503342&partnerID=40&md5=aa7a94d2d1da0beadd43c5e41bd3c1d4,"Abstraction is an important issue in intrusion detection, since it not only hides the difference between heterogeneous systems, but also allows generic intrusion-detection models. However, abstraction is an error-prone process and is not well supported in current intrusion-detection systems (IDSs). This article presents a hierarchical model to support attack specification and event abstraction in distributed intrusion detection. The model involves three concepts: system view, signature, and view definition. A system view provides an abstract interface of a particular type of information; defined on the instances of system views, a signature specifies certain distributed attacks or events to be monitored; a view definition is then used to derive information from the matches of a signature and presents it through a system view. With the three elements, the model provides a hierarchical framework for maintaining signatures, system views, as well as event abstraction. As a benefit, the model allows generic signatures that can accommodate unknown variants of known attacks. Moreover, abstraction represented by a system view can be updated without changing either its specification or the signatures specified on its basis. This article then presents a decentralized method for autonomous but cooperative component systems to detect distributed attacks specified by signatures. Specifically, a signature is decomposed into finer units, called detection tasks, each of which represents the activity to be monitored on a component system. The component systems (involved in a signature) then perform the detection tasks cooperatively according to the “dependency” relationships among these tasks. An experimental system called CARDS has been implemented to test the feasibility of the proposed approach. © 2001, ACM. All rights reserved.",Cooperative information systems; Heterogeneous systems; Intrusion detection; Misuse detection; Security,
TRBAC: A Temporal Role-Based Access Control Model,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944792110&doi=10.1145%2f501978.501979&partnerID=40&md5=a39afad9c64e18b827cd3a403c02b406,"Role-based access control (RBAC) models are receiving increasing attention as a generalized approach to access control. Roles may be available to users at certain time periods, and unavailable at others. Moreover, there can be temporal dependencies among roles. To tackle such dynamic aspects, we introduce Temporal-RBAC (TRBAC), an extension of the RBAC model. TRBAC supports periodic role enabling and disabling—possibly with individual exceptions for particularusers— and temporal dependencies among such actions, expressed by means of role triggers. Role trigger actions may be either immediately executed, or deferred by an explicitly specified amount of time. Enabling and disabling actions may be given a priority, which is used to solve conflicting actions. A formal semantics for the specification language is provided, and a polynomial safeness check is introduced to reject ambiguous or inconsistent specifications. Finally, a system implementing TRBAC on top of a conventional DBMS is presented. © 2001, ACM. All rights reserved.",Role triggers; Role-Based access control; Security; Temporal constraints,
Real-Time Protocol Analysis for Detecting Link-State Routing Protocol Attacks,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017067350&doi=10.1145%2f383775.383776&partnerID=40&md5=16cc04765e3df073d2477705997ca12b,"A real-time knowledge-based network intrusion-detection model for a link-state routing protocol is presented for the OSPF protocol. This model includes three layers: a data process layer to parse packets and dispatch data; an event abstractor to abstract predefined real-time events for the link-state routing protocol; and an extended timed finite state machine to express the real-time behavior of the protocol engine and to detect intrusions by pattern matching. The timed FSM, called the JiNao Finite State Machine (JFSM) is extended from the conventional FSM with timed states, multiple timers, and time constraints on state transitions. The JFSM is implemented as a generator that can create any FSM by constructing the configuration file only. The results show that this approach is very effective for detecting real-time intrusions. Our approach can be extended for use in other network protocol intrusion-detection systems, especially for those with known attacks. © 2001, ACM. All rights reserved.",Design; Event correlation; Knowledge-Based IDS; Link-State routing protocol security; OSPF attacks; Real-Time misuse intrusion detection; Real-Time network protocol analysis; Security; Theory; Timed finite state machine,
Secure Password-Based Cipher Suite for TLS,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016441190&doi=10.1145%2f501963.501965&partnerID=40&md5=15713bd22f7247f86c7056921a87c54e,"SSL is the de facto standard today for securing end-to-end transport on the Internet. While the protocol itself seems rather secure, there are a number of risks that lurk in its use, for example, in web banking. However, the adoption of password-based key-exchange protocols can overcome some of these problems. We propose the integration of such a protocol (DH-EKE) in the TLS protocol, the standardization of SSL by IETF. The resulting protocol provides secure mutual authentication and key establishment over an insecure channel. It does not have to resort to a PKI or keys and certificates stored on the users computer. Additionally, its integration in TLS is as minimal and non-intrusive as possible. © 2001, ACM. All rights reserved.",Algorithms; Authenticated key exchange; Dictionary attack; Human factors; Key agreement; Password; Perfect forward secrecy; Secure channel; Security; Transport layer security; Weak secret,
The Securering Group Communication System,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542488415&doi=10.1145%2f503339.503341&partnerID=40&md5=4146db59a8a647ee4c8b18f8507c5366,"Secure reliable group communication protocols can facilitate the development of survivable distributed systems that are able to remain correct and reliable despite intrusions that cause some nodes to behave in an arbitrary or malicious manner. However, the development of such protocols is itself difficult, and prior systems have exhibited high overheads, primarily due to the cost of digital signatures. The SecureRing group communication system provides secure, reliable, totallyordered message delivery and group membership services despite the malicious corruption of a constant fraction of the processors within the system. The network is assumed not to partition, and persistent communication faults are handled as processor faults. The SecureRing message delivery protocol makes use of message digests in a signed token to allow a single digital signature to cover multiple messages, and to avoid the need for multiple rounds of message exchange in normal operation. While these techniques mean that messages are not authenticated in real time, they enable the SecureRing protocols to achieve high throughput and reasonable latency. © 2001, ACM. All rights reserved.",Byzantine faults; Group communication; Intrusion; Partial synchrony; Reliability; Security; State machine replication; Survivability,
An Authorization Model for a Public Key Management Service,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2342606346&doi=10.1145%2f503339.503343&partnerID=40&md5=14bb4a46e5b5ba803d9b8a7c7857ef18,"Public key management has received considerable attention from both the research and commercial communities as a useful primitive for secure electronic commerce and secure communication. While the mechanics of certifying and revoking public keys and escrowing and recovering private keys have been widely explored, less attention has been paid to access control frameworks for regulating access to stored keys by different parties. In this article we propose such a framework for a key management service that supports public key registration, lookup, and revocation, and private key escrow, protected use (e.g., to decrypt selected messages), and recovery.We propose an access control model using a policy based on principal, ownership, and authority relationships on keys. The model allows owners to grant to others (and revoke) privileges to execute various actions on their keys. The simple authorization language is very expressive, enabling the specification of authorizations for composite subjects that can be fully specified (ground) or partially specified, thus making the authorizations applicable to all subjects satisfying some conditions. We illustrate how the access control policy and the authorizations can easily be expressed through a simple and restricted, hence efficiently computable, form of logic language. © 2001, ACM. All rights reserved.",Access control; Authorizations specification and enforcement; Public key infrastructure; Security,
"Cost Profile of a Highly Assured, Secure Operating System",2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344286486&doi=10.1145%2f383775.383778&partnerID=40&md5=4c536ffed092ef1b50d8c565cb013f82,"The Logical Coprocessing Kernel (LOCK) began as a research project to stretch the state of the art in secure computing by trying to meet or even exceed the “A1” requirements of the Trusted Computer System Evaluation Criteria (TCSEC). Over the span of seven years, the project was transformed into an effort to develop and deploy a product: the Standard Mail Guard (SMG). Since the project took place under a US government contract, the development team needed to maintain detailed records of the time spent on the project. The records from 1987 to 1992 have been combined with information about software code size and error detection. This information has been used to examine the practical impacts of high assurance techniques on a large-scale software development program. Tasks associated with the A1 formal assurance requirements added approximately 58% to the development cost of security-critical software. In exchange for these costs, the formal assurance tasks (formal specifications, proofs, and specification to code correspondence) uncovered 68% of the security flaws detected in LOCK's critical security mechanisms. However, a study of flaw detection during the SMG program found that only 14% of all flaws detected were of the type that could be detected using formal assurance, and that the work of the formal assurance team only accounted for 19% of all flaws detected. While formal assurance is clearly effective at detecting flaws, its practicality hinges on the degree to which the formally modeled system properties represent all of a system's essential properties. © 2001, ACM. All rights reserved.",Economics; LOCK (LOgical Coprocessing Kernel); Security; Security kernels; Verification,
Secure Virtual Enclaves: Supporting Coalition Use of Distributed Application Technologies,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21644476470&doi=10.1145%2f501963.501964&partnerID=40&md5=75a74bd98aa101999dd2b9ff529b9785,"The Secure Virtual Enclaves (SVE) collaboration infrastructure allows multiple organizations to share their distributed application objects, while respecting organizational autonomy over local resources. The infrastructure is transparent to applications, which may be accessed via a web server, or may be based on Java or Microsoft's DCOM. The SVE infrastructure is implemented in middleware, with no modifications to COTS operating systems or network protocols. The system enables dynamic updates to security policies to support changes in both coalition membership and participants' perception of risks. While the prototype demonstrates fine-grained access control for secure collaborative computing, we have identified significant issues that remain to be addressed, particularly in the area of policy development, before such collaboration will be convenient. The SVE infrastructure offers a platform and conceptual basis for further exploration of these issues and experimentation with new solutions. © 2001, ACM. All rights reserved.",Access Control; Coalition; Collaborative system; Group communication; Middleware; Security; Security policy,
Proposed NIST Standard for Role-Based Access Control,2001,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992227458&doi=10.1145%2f501978.501980&partnerID=40&md5=2502a90b4f0e8565bef0fca1bb8939ef,"In this article we propose a standard for role-based access control (RBAC). Although RBAC models have received broad support as a generalized approach to access control, and are well recognized for their many advantages in performing large-scale authorization management, no single authoritative definition of RBAC exists today. This lack of a widely accepted model results in uncertainty and confusion about RBAC's utility and meaning. The standard proposed here seeks to resolve this situation by unifying ideas from a base of frequently referenced RBAC models, commercial products, and research prototypes. It is intended to serve as a foundation for product development, evaluation, and procurement specification. Although RBAC continues to evolve as users, researchers, and vendors gain experience with its application, we feel the features and components proposed in this standard represent a fundamental and stable set of mechanisms that may be enhanced by developers in further meeting the needs of their customers. As such, this document does not attempt to standardize RBAC features beyond those that have achieved acceptance in the commercial marketplace and research community, but instead focuses on defining a fundamental and stable set of RBAC components. This standard is organized into the RBAC Reference Model and the RBAC System and Administrative Functional Specification. The reference model defines the scope of features that comprise the standard and provides a consistent vocabulary in support of the specification. The RBAC System and Administrative Functional Specification defines functional requirements for administrative operations and queries for the creation, maintenance, and review of RBAC sets and relations, as well as for specifying system level functionality in support of session attribute management and an access control decision process. © 2001, ACM. All rights reserved.",Access control; Authorization management; Role-Based access control; Security; Security; Standardization; Standards,
Role-Based Authorization Constraints Specification,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956993736&doi=10.1145%2f382912.382913&partnerID=40&md5=804a46600497224de0695e5efdec98eb,"Constraints are an important aspect of role-based access control (RBAC) and are often regarded as one of the principal motivations behind RBAC. Although the importance of constraints in RBAC has been recognized for a long time, they have not received much attention. In this article, we introduce an intuitive formal language for specifying role-based authorization constraints named RCL 2000 including its basic elements, syntax, and semantics. We give soundness and completeness proofs for RCL 2000 relative to a restricted form of first-order predicate logic. Also, we show how previously identified role-based authorization constraints such as separation of duty (SOD) can be expressed in our language. Moreover, we show there are other significant SOD properties that have not been previously identified in the literature. Our work shows that there are many alternate formulations of even the simplest SOD properties, with varying degree of flexibility and assurance. Our language provides us a rigorous foundation for systematic study of role-based authorization constraints. © 2000, ACM. All rights reserved.",Access control models; authorization constraints; constraints specification; Languages; role-based access control; Security,
The Base-Rate Fallacy and the Difficulty of Intrusion Detection,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928016636&doi=10.1145%2f357830.357849&partnerID=40&md5=4b06541086e82bf32b5e8890b51b0269,"Many different demands can be made of intrusion detection systems. An important requirement is that an intrusion detection system be effective; that is, it should detect a substantial percentage of intrusions into the supervised system, while still keeping the false alarm rate at an acceptable level. This article demonstrates that, for a reasonable set of assumptions, the false alarm rate is the limiting factor for the performance of an intrusion detection system. This is due to the base-rate fallacy phenomenon, that in order to achieve substantial values of the Bayesian detection rate P(Intrusion | Alarm), we have to achieve a (perhaps in some cases unattainably) low false alarm rate. A selection of reports of intrusion detection performance are reviewed, and the conclusion is reached that there are indications that at least some types of intrusion detection have far to go before they can attain such low false alarm rates. © 2000, ACM. All rights reserved.",Base-rate fallacy; detection rate; false alarm rate; intrusion detection; Performance; Security; Theory,
Scalable Multicast Security with Dynamic Recipient Groups,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995289060&doi=10.1145%2f357830.357834&partnerID=40&md5=6b1e804ed7ebbf065175d29a24035d74,"In this article we propose a new framework for multicast security based on distributed computation of security transforms by intermediate nodes. The involvement of intermediate nodes in the security process causes a new type of dependency between group membership and the topology of the multicast network. Thanks to this dependency, the containment of security exposures in large multicast groups is ensured. The framework also ensures both the scalability for large dynamic groups and the security of individual members. Two different key distribution protocols complying with the framework are introduced: an extension of the ElGamal encryption scheme, and one based on a multiexponent version of RSA. © 2000, ACM. All rights reserved.",Confidentiality; Diffie-Hellman; group communications; key distribution; Multicast; RSA; scalability; Security,
Testing Intrusion Detection Systems: A Critique of the 1998 and 1999 DARPA Intrusion Detection System Evaluations as Performed by Lincoln Laboratory,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019691440&doi=10.1145%2f382912.382923&partnerID=40&md5=7ebc479425001dd9dbb836b31a4ebb83,"In 1998 and again in 1999, the Lincoln Laboratory of MIT conducted a comparative evaluation of intrusion detection systems (IDSs) developed under DARPA funding. While this evaluation represents a significant and monumental undertaking, there are a number of issues associated with its design and execution that remain unsettled. Some methodologies used in the evaluation are questionable and may have biased its results. One problem is that the evaluators have published relatively little concerning some of the more critical aspects of their work, such as validation of their test data. The appropriateness of the evaluation techniques used needs further investigation. The purpose of this article is to attempt to identify the shortcomings of the Lincoln Lab effort in the hope that future efforts of this kind will be placed on a sounder footing. Some of the problems that the article points out might well be resolved if the evaluators were to publish a detailed description of their procedures and the rationale that led to their adoption, but other problems would clearly remain. © 2000, ACM. All rights reserved.",Computer security; intrusion detection; receiver operating curves (ROC); Security; software evaluation,
A Framework for Constructing Features and Models for Intrusion Detection Systems,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885774862&doi=10.1145%2f382912.382914&partnerID=40&md5=da3bc85c11686bceefa70f973801d848,"Intrusion detection (ID) is an important component of infrastructure protection mechanisms. Intrusion detection systems (IDSs) need to be accurate, adaptive, and extensible. Given these requirements and the complexities of today's network environments, we need a more systematic and automated IDS development process rather than the pure knowledge encoding and engineering approaches. This article describes a novel framework, MADAM ID, for Mining Audit Data for Automated Models for Intrusion Detection. This framework uses data mining algorithms to compute activity patterns from system audit data and extracts predictive features from the patterns. It then applies machine learning algorithms to the audit records that are processed according to the feature definitions to generate intrusion detection rules. Results from the 1998 DARPA Intrusion Detection Evaluation showed that our ID model was one of the best performing of all the participating systems. We also briefly discuss our experience in converting the detection models produced by off-line data mining programs to real-time modules of existing IDSs. © 2000, ACM. All rights reserved.",Data mining; Design; Experimentation; feature construction; intrusion detection; Security,
Signature Schemes Based on the Strong RSA Assumption,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024261619&doi=10.1145%2f357830.357847&partnerID=40&md5=a6c4d5acaaeef97f48d36bbd0ceb7c62,"We describe and analyze a new digital signature scheme. The new scheme is quite efficient, does not require the the signer to maintain any state, and can be proven secure against adaptive chosen message attack under a reasonable intractability assumption, the so-called strong RSA assumption. Moreover, a hash function can be incorporated into the scheme in such a way that it is also secure in the random oracle model under the standard RSA assumption. © 2000, ACM. All rights reserved.",Algorithms; digital signatures; provable security; RSA; Security; Theory,
Key Management for Encrypted Broadcast,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975035144&doi=10.1145%2f354876.354879&partnerID=40&md5=c0a5240fa2532ccfd8446e5fa85b69e8,"We consider broadcast applications where the transmissions need to be encrypted, such as direct broadcast digital TV networks or Internet multicasts. In these applications the number of encrypted TV programs may be very large, but the secure memory capacity at the set-top terminals (STT) is severely limited due to the need to withstand pirate attacks and hardware tampering. Despite this, we would like to allow the service provider to offer different packages of programs to the users. A user who buys a package should be able to view every program belonging to that package, but nothing else. A flexible scheme should allow for packages of various sizes to be offered, from a single program up to all the programs. We suggest two novel schemes to manage the encryption keys for these applications. The schemes are highly flexible, and understandable to users, yet require very few keys to be stored in the STTs' secure memory. The computational power required of the STTs is very low. The security of these schemes is as good or better than that offered by current technology. © 2000, ACM. All rights reserved.",Conditional access; pay-per-view; Security,
Enforceable Security Policies,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881225236&doi=10.1145%2f353323.353382&partnerID=40&md5=2455ebb694f770af966b40fcd03415c4,"A precise characterization is given for the class of security policies enforceable with mechanisms that work by monitoring system execution, and automata are introduced for specifying exactly that class of security policies. Techniques to enforce security policies specified by such automata are also discussed. © 2000, ACM. All rights reserved.",EM security policies; inlined reference monitors; proof carrying code; safety properties; SASI; Security; security automata; security policies,
Balancing Cooperation and Risk in Intrusion Detection,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007724451&doi=10.1145%2f353323.353324&partnerID=40&md5=95ee0b1b623ef8c0c1f4353524378c25,"Early systems for networked intrusion detection (or, more generally, intrusion or misuse management) required either a centralized architecture or a centralized decision-making point, even when the data gathering was distributed. More recently, researchers have developed far more decentralized intrusion detection systems using a variety of techniques. Such systems often rely upon data sharing between sites which do not have a common administrator and therefore cooperation will be required in order to detect and respond to security incidents. It has therefore become important to address cooperation and data sharing in a formal manner. In this paper, we discuss the detection of distributed attacks across cooperating enterprises. We begin by defining relationships between cooperative hosts, then use the take-grant model to identify both when a host could identify a widespread attack and when that host is at increased risk due to data sharing. We further refine our definition of potential identification using access, integrity, and cooperation policies which limit sharing. Finally, we include a brief description of both a simple Prolog model encorporating data sharing policies and a prototype cooperative intrusion detection system. © 2000, ACM. All rights reserved.",Access control models; authorization mechanisms; collaborative systems; Management; Security,
Xor-Trees for Efficient Anonymous Multicast and Reception,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011586610&doi=10.1145%2f354876.354877&partnerID=40&md5=35c8d49877514b6dec087c14d4a28704,"We examine the problem of efficient anonymous multicast and reception in general communication networks. We present algorithms that achieve anonymous communication, are protected against traffic analysis, and require O(1) amortized communication complexity on each link and low computational complexity. The algorithms support sender anonymity, receiver(s) anonymity, or sender-receiver anonymity. © 2000, ACM. All rights reserved.",Algorithms; Anonymous communication; anonymous multicast; Security,
Configuring Role-Based Access Control to Enforce Mandatory and Discretionary Access Control Policies,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884311106&doi=10.1145%2f354876.354878&partnerID=40&md5=01a5f0fc90f6835a006ce6ef4043e7d0,"Access control models have traditionally included mandatory access control (or lattice-based access control) and discretionary access control. Subsequently, role-based access control has been introduced, along with claims that its mechanisms are general enough to simulate the traditional methods. In this paper we provide systematic constructions for various common forms of both of the traditional access control paradigms using the role-based access control (RBAC) models of Sandhu et al., commonly called RBAC96. We see that all of the features of the RBAC96 model are required, and that although for the mandatory access control simulation, only one administrative role needs to be assumed, for the discretionary access control simulations, a complex set of administrative roles is required. © 2000, ACM. All rights reserved.",discretionary access control; lattice-based access control; Management; mandatory access control; Role-based access control; Security,
Reflection as a Mechanism for Software Integrity Verification,2000,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002390908&doi=10.1145%2f353323.353383&partnerID=40&md5=57d8f16b6da81751fd59f1d6bf1bcb2e,"The integrity verification of a device's controlling software is an important aspect of many emerging information appliances. We propose the use of reflection, whereby the software is able to examine its own operation, in conjunction with cryptographic hashes as a basis for developing a suitable software verification protocol. For more demanding applications metareflective techniques can be used to thwart attacks based on device emulation strategies. We demonstrate how our approach can be used to increase the security of mobile phones, devices for the delivery of digital content, and smartcards. © 2000, ACM. All rights reserved.",Authentication; cryptographic hash function; embedded device; message digest,
Flexible Control of Downloaded Executable Content,1999,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013593271&doi=10.1145%2f317087.317091&partnerID=40&md5=2727c3532453ab325e2f73dc000c57f9,"We present a security architecture that enables system and application access control requirements to be enforced on applications composed from downloaded executable content. Downloaded executable content consists of messages downloaded from remote hosts that contain executables that run, upon receipt, on the downloading principal's machine. Unless restricted, this content can perform malicious actions, including accessing its downloading principal's private data and sending messages on this principal's behalf. Current security architectures for controlling downloaded executable content (e.g., JDK 1.2) enable specification of access control requirements for content-based on its provider and identity. Since these access control requirements must cover every legal use of the class, they may include rights that are not necessary for a particular application of content. Therefore, using these systems, an application composed from downloaded executable content cannot enforce its access control requirements without the addition of application-specific security mechanisms. In this paper, we define an access control model with the following properties: (1) system administrators can define system access control requirements on applications and (2) application developers can use the same model to enforce application access control requirements without the need for ad hoc security mechanisms. This access control model uses features of role-based access control models to enable (1) specification of a single role that applies to multiple application instances; (2) selection of a content's access rights based on the content's application and role in the application; (3) consistency maintained between application state and content access rights; and (4) control of role administration. We detail a system architecture that uses this access control model to implement secure collaborative applications. Lastly, we describe an implementation of this architecture, called the Lava security architecture. © 1999, ACM. All rights reserved.",Access control models; authentication; authorization mechanisms; collaborative systems; Design; Management; role-based access control; Security,
Secure Audit Logs to Support Computer Forensics,1999,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021950441&doi=10.1145%2f317087.317089&partnerID=40&md5=55c3952f190fd1aed771a090b4752cc3,"In many real-world applications, sensitive information must be kept in log files on an untrusted machine. In the event that an attacker captures this machine, we would like to guarantee that he will gain little or no information from the log files and to limit his ability to corrupt the log files. We describe a computationally cheap method for making all log entries generated prior to the logging machine's compromise impossible for the attacker to read, and also impossible to modify or destroy undetectably. © 1999, ACM. All rights reserved.",audit Logs; Auditing; authentication; computer forensics; hash chains; intrusion detection; Security,
Unlinkable Serial Transactions: Protocols and Applications,1999,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005072366&doi=10.1145%2f330382.330384&partnerID=40&md5=de66fad968ef04405bbf75571a1c8c12,"We present a protocol for unlinkable serial transactions suitable for a variety of networkbased subscription services. It is the first protocol to use cryptographic blinding to enable subscription services. The protocol prevents the service from tracking the behavior of its customers, while protecting the service vendor from abuse due to simultaneous or “cloned” use by a single subscriber. Our basic protocol structure and recovery protocol are robust against failure in protocol termination. We evaluate the security of the basic protocol and extend the basic protocol to include auditing, which further deters subscription sharing. We describe other applications of unlinkable serial transactions for pay-per-use transactions within a subscription, third-party subscription management, multivendor coupons, proof of group membership, and voting. © 1999, ACM. All rights reserved.",Anonymity; blinding; cryptographic protocols; Design; Security; unlinkable serial transactions; Verification,
Inductive Analysis of the Internet Protocol TLS,1999,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024252307&doi=10.1145%2f322510.322530&partnerID=40&md5=40299c58e92c06e79e261ddeb510a2bb,"Internet browsers use security protocols to protect sensitive messages. An inductive analysis of TLS (a descendant of SSL 3.0) has been performed using the theorem prover Isabelle. Proofs are based on higher-order logic and make no assumptions concerning beliefs or finiteness. All the obvious security goals can be proved; session resumption appears to be secure even if old session keys are compromised. The proofs suggest minor changes to simplify the analysis. TLS, even at an abstract level, is much more complicated than most protocols verified by researchers. Session keys are negotiated rather than distributed, and the protocol has many optional parts. Nevertheless, the resources needed to verify TLS are modest: six man-weeks of effort and three minutes of processor time. © 1999, ACM. All rights reserved.",authentication; inductive method; Isabelle; proof tools; Security; TLS; Verification,
Public-Key Cryptography and Password Protocols,1999,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71849087704&doi=10.1145%2f322510.322514&partnerID=40&md5=34b600aa613d40235ca0304501edca9d,"We study protocols for strong authentication and key exchange in asymmetric scenarios where the authentication server possesses a pair of private and public keys while the client has only a weak human-memorizable password as its authentication key. We present and analyze several simple password authentication protocols in this scenario, and show that the security of these protocols can be formally proven based on standard cryptographic assumptions. Remarkably, our analysis shows optimal resistance to off-line password guessing attacks under the choice of suitable public key encryption functions. In addition to user authentication, we describe ways to enhance these protocols to provide two-way authentication, authenticated key exchange, defense against server's compromise, and user anonymity. We complement these results with a proof that strongly indicates that public key techniques are unavoidable for password protocols that resist off-line guessing attacks. As a further contribution, we introduce the notion of public passwords that enables the use of the above protocols in situations where the client's machine does not have the means to validate the server's public key. Public passwords serve as “hand-held certificates” that the user can carry without the need for special computing devices. © 1999, ACM. All rights reserved.",Dictionary attacks; hand-held certificates; key exchange; passwords; public passwords; public-key protocols; Security; Theory,
Authentication Metric Analysis and Design,1999,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13244288768&doi=10.1145%2f317087.317088&partnerID=40&md5=088ca7c735c70d888b319ed17a1cb6d2,"Authentication using a path of trusted intermediaries, each able to authenticate the next in the path, is a well-known technique for authenticating entities in a large-scale system. Recent work has extended this technique to include multiple paths in an effort to bolster authentication, but the success of this approach may be unclear in the face of intersecting paths, ambiguities in the meaning of certificates, and interdependencies in the use of different keys. Thus, several authors have proposed metrics to evaluate the confidence afforded by a set of paths. In this paper we develop a set of guiding principles for the design of such metrics. We motivate our principles by showing how previous approaches failed with respect to these priniciples and what the consequences to authentication might be. We then propose a new metric that appears to meet our principles, and so to be a satisfactory metric of authentication. © 1999, ACM. All rights reserved.",Measurement; metrics of authentication; Public key infrastructure; Security,
Temporal Sequence Learning and Data Reduction for Anomaly Detection,1999,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949731575&doi=10.1145%2f322510.322526&partnerID=40&md5=ca08602397ad1a782036747d0b34395c,"The anomaly-detection problem can be formulated as one of learning to characterize the behaviors of an individual, system, or network in terms of temporal sequences of discrete data. We present an approach on the basis of instance-based learning (IBL) techniques. To cast the anomaly-detection task in an IBL framework, we employ an approach that transforms temporal sequences of discrete, unordered observations into a metric space via a similarity measure that encodes intra-attribute dependencies. Classification boundaries are selected from an a posteriori characterization of valid user behaviors, coupled with a domain heuristic. An empirical evaluation of the approach on user command data demonstrates that we can accurately differentiate the profiled user from alternative users when the available features encode sufficient information. Furthermore, we demonstrate that the system detects anomalous conditions quickly - an important quality for reducing potential damage by a malicious user. We present several techniques for reducing data storage requirements of the user profile, including instance-selection methods and clustering. An empirical evaluation shows that a new greedy clustering algorithm reduces the size of the user model by 70%, with only a small loss in accuracy. © 1999, ACM. All rights reserved.",Anomaly detection; clustering; data reduction; empirical evaluation; Experimentation; instance based learning; machine learning; Security; user profiling,
On Secure and Pseudonymous Client-Relationships with Multiple Servers,1999,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949287358&doi=10.1145%2f330382.330386&partnerID=40&md5=adb948d957953eb60f143fd4ad2fda58,"This paper introduces a cryptographic engine, Janus, which assists clients in establishing and maintaining secure and pseudonymous relationships with multiple servers. The setting is such that clients reside on a particular subnet (e.g., corporate intranet, ISP) and the servers reside anywhere on the Internet. The Janus engine allows each client-server relationship to use either weak or strong authentication on each interaction. At the same time, each interaction preserves privacy by neither revealing a client's true identity (except for the subnet) nor the set of servers with which a particular client interacts. Furthermore, clients do not need any secure long-term memory, enabling scalability and mobility. The interaction model extends to allow servers to send data back to clients via e-mail at a later date. Hence, our results complement the functionality of current network anonymity tools and remailers. The paper also describes the design and implementation of the Lucent Personalized Web Assistant (LPWA), which is a practical system that provides secure and pseudonymous relations with multiple servers on the Internet. LPWA employs the Janus function to generate site-specific persone, which consist of alias usernames, passwords, and e-mail addresses. © 1999, ACM. All rights reserved.",Algorithms; Anonymity; Experimentation; Janus function; mailbox; persistent relationship; privacy; pseudonym; Security,
Design of a High-Performance ATM Firewall,1999,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349181586&doi=10.1145%2f322510.322520&partnerID=40&md5=23700fb12530a33aebee375d20b24334,"A router-based packet-filtering firewall is an effective way of protecting an enterprise network from unauthorized access. However, it will not work efficiently in an ATM network because it requires the termination of end-to-end ATM connections at a packet-filtering router, which incurs huge overhead of SAR (Segmentation and Reassembly). Very few approaches to this problem have been proposed in the literature, and none is completely satisfactory. In this paper we present the hardware design of a high-speed ATM firewall that does not require the termination of an end-to-end connection in the middle. We propose a novel firewall design philosophy, called Quality of Firewalling (QoF), that applies security measures of different strength to traffic with different risk levels and show how it can be implemented in our firewall. Compared with the traditional firewalls, this ATM firewall performs exactly the same packet-level filtering without compromising the performance and has the same ‘look and feel’ by sitting at the chokepoint between the trusted ATM LAN and untrusted ATM WAN. It is also easy to manage and flexible to use. © 1999, ACM. All rights reserved.",Asynchronous Transfer Mode (ATM); Design; Firewall; Packet filtering; Performance; Security; Switch architecture; TCP/IP; Theory,
Strength of Two Data Encryption Standard Implementations Under Timing Attacks,1999,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017093014&doi=10.1145%2f330382.330390&partnerID=40&md5=465d17e3d1bc60ad82e241bf3ce69e57,"We study the vulnerability of two implementations of the Data Encryption Standard (DES) cryptosystem under a timing attack. A timing attack is a method, recently proposed by Paul Kocher, that is designed to break cryptographic systems. It exploits the engineering aspects involved in the implementation of cryptosystems and might succeed even against cryptosystems that remain impervious to sophisticated cryptanalytic techniques. A timing attack is, essentially, a way of obtaining some user's private information by carefully measuring the time it takes the user to carry out cryptographic operations. In this work, we analyze two implementations of DES. We show that a timing attack yields the Hamming weight of the key used by both DES implementations. Moreover, the attack is computationally inexpensive. We also show that all the design characteristics of the target system, necessary to carry out the timing attack, can be inferred from timing measurements. © 1999, ACM. All rights reserved.",Cryptanalysis; cryptography; Data Encryption Standard; Security; timing attack,
Designing password policies for strength and usability,2016,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970028821&doi=10.1145%2f2891411&partnerID=40&md5=8e3760eba5ecb2352bca3e5fe23cd225,"Password-composition policies are the result of service providers becoming increasingly concerned about the security of online accounts. These policies restrict the space of user-created passwords to preclude easily guessed passwords and thus make passwords more difficult for attackers to guess. However, many users struggle to create and recall their passwords under strict password-composition policies, for example, ones that require passwords to have at least eight characters with multiple character classes and a dictionary check. Recent research showed that a promising alternative was to focus policy requirements on password length instead of on complexity. In this work, we examine 15 password policies, many focusing on length requirements. In doing so, we contribute the first thorough examination of policies requiring longer passwords. We conducted two online studies with over 20,000 participants, and collected both usability and password-strength data. Our findings indicate that password strength and password usability are not necessarily inversely correlated: policies that lead to stronger passwords do not always reduce usability. We identify policies that are both more usable and more secure than commonly used policies that emphasize complexity rather than length requirements. We also provide practical recommendations for service providers who want their users to have strong yet usable passwords.",Authentication; Password-composition policy; Passwords; Usable security,Usability engineering; Online studies; Password composition policies; Password strength; Passwords; Practical recommendation; Recent researches; Service provider; Usable security; Authentication
General graph data de-anonymization: From mobility traces to social networks,2016,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969909616&doi=10.1145%2f2894760&partnerID=40&md5=b29e6072cc9837a87e303f6ce84f9031,"When people utilize social applications and services, their privacy suffers a potential serious threat. In this article, we present a novel, robust, and effective de-anonymization attack to mobility trace data and social data. First, we design a Unified Similarity (US) measurement, which takes account of local and global structural characteristics of data, information obtained from auxiliary data, and knowledge inherited from ongoing de-anonymization results. By analyzing the measurement on real datasets, we find that some data can potentially be de-anonymized accurately and the other can be de-anonymized in a coarse granularity. Utilizing this property, we present a US-based De-Anonymization (DA) framework, which iteratively de-anonymizes data with accuracy guarantee. Then, to de-anonymize large-scale data without knowledge of the overlap size between the anonymized data and the auxiliary data, we generalize DA to an Adaptive De-Anonymization (ADA) framework. By smartly working on two core matching subgraphs, ADA achieves high de-anonymization accuracy and reduces computational overhead. Finally, we examine the presented de-anonymization attack on three well-known mobility traces: St.Andrews, Infocom06, and Smallblue, and three social datasets: ArnetMiner, Google+, and Facebook. The experimental results demonstrate that the presented de-anonymization framework is very effective and robust to noise. The source code and employed datasets are now publicly available at SecGraph [2015]. © 2016 ACM.",Graph de-anonymization; Mobility traces; Social networks,Ada (programming language); Salinity measurement; Anonymization; Auxiliary data; Computational overheads; Large scale data; Mobility traces; Real data sets; Social applications; Structural characteristics; Social networking (online)
Toward robotic robbery on the touch screen,2016,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969915653&doi=10.1145%2f2898353&partnerID=40&md5=df73ca4120764dd3f7697d91913d8a37,"Despite the tremendous amount of research fronting the use of touch gestures as a mechanism of continuous authentication on smart phones, very little research has been conducted to evaluate how these systems could behave if attacked by sophisticated adversaries. In this article, we present two Lego-driven robotic attacks on touch-based authentication: a population statistics-driven attack and a user-tailored attack. The population statistics-driven attack is based on patterns gleaned from a large population of users, whereas the user-tailored attack is launched based on samples stolen from the victim. Both attacks are launched by a Lego robot that is trained on how to swipe on the touch screen. Using seven verification algorithms and a large dataset of users, we show that the attacks cause the system's mean false acceptance rate (FAR) to increase by up to fivefold relative to the mean FAR seen under the standard zero-effort impostor attack. The article demonstrates the threat that robots pose to touch-based authentication and provides compelling evidence as to why the zero-effort attack should cease to be used as the benchmark for touch-based authentication systems. © 2016 ACM.",Behavioral biometrics; Robotic attacks; Smartphone security; Touch gestures,Authentication; Population statistics; Robotics; Smartphones; Authentication systems; Behavioral biometrics; Continuous authentications; False acceptance rate; Large population; Smartphone securities; Touch gestures; Verification algorithms; Touch screens
An efficient user verification system using angle-based mouse movement biometrics,2016,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974627763&doi=10.1145%2f2893185&partnerID=40&md5=7b235e4cd4cedd8bec733160020ac457,"Biometric authentication verifies a user based on its inherent, unique characteristics - who you are. In addition to physiological biometrics, behavioral biometrics has proven very useful in authenticating a user. Mouse dynamics, with their unique patterns of mouse movements, is one such behavioral biometric. In this article, we present a user verification system using mouse dynamics, which is transparent to users and can be naturally applied for continuous reauthentication. The key feature of our system lies in using much more fine-grained (point-by-point) angle-based metrics of mouse movements for user verification. These new metrics are relatively unique from person to person and independent of a computing platform. Moreover, we utilize support vector machines (SVMs) for quick and accurate classification. Our technique is robust across different operating platforms, and no specialized hardware is required. The efficacy of our approach is validated through a series of experiments, which are based on three sets of user mouse movement data collected in controllable environments and in the field. Our experimental results show that the proposed system can verify a user in an accurate and timely manner, with minor induced system overhead. © 2016 ACM.",Angle-based metrics; Mouse dynamics; User verification,Biometrics; Dynamics; Mammals; Support vector machines; Angle-based metrics; Behavioral biometrics; Biometric authentication; Mouse dynamics; Physiological biometrics; Specialized hardware; Support vector machine (SVMs); User verification; Authentication
Behavioral study of users when interacting with active honeytokens,2016,ACM Transactions on Information and System Security,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958540227&doi=10.1145%2f2854152&partnerID=40&md5=78075a2615790ab87ee67bb010bb10e6,"Active honeytokens are fake digital data objects planted among real data objects and used in an attempt to detect data misuse by insiders. In this article, we are interested in understanding how users (e.g., employees) behave when interacting with honeytokens, specifically addressing the following questions: Can users distinguish genuine data objects from honeytokens? And, how does the user's behavior and tendency to misuse data change when he or she is aware of the use of honeytokens? First, we present an automated and generic method for generating the honeytokens that are used in the subsequent behavioral studies. The results of the first study indicate that it is possible to automatically generate honeytokens that are difficult for users to distinguish from real tokens. The results of the second study unexpectedly show that users did not behave differently when informed in advance that honeytokens were planted in the database and that these honeytokens would be monitored to detect illegitimate behavior. These results can inform security system designers about the type of environmental variables that affect people's data misuse behavior and how to generate honeytokens that evade detection. © 2016 ACM.",Data misuse; Honeypots; Honeytokens; Insider threat,Computer science; Safety engineering; Behavioral studies; Data misuse; Environmental variables; Generic method; Honeypots; Honeytokens; Insider Threat; System designers; Behavioral research
