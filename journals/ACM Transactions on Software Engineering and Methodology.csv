Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Intelligent code completion with Bayesian networks,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952062892&doi=10.1145%2f2744200&partnerID=40&md5=ada4177429a51ead03113192ec772a22,"Code completion is an integral part of modern Integrated Development Environments (IDEs). Developers often use it to explore Application Programming Interfaces (APIs). It is also useful to reduce the required amount of typing and to help avoid typos. Traditional code completion systems propose all type-correct methods to the developer. Such a list is often very long with many irrelevant items. More intelligent code completion systems have been proposed in prior work to reduce the list of proposed methods to relevant items. This work extends one of these existing approaches, the Best Matching Neighbor (BMN) algorithm. We introduce Bayesian networks as an alternative underlying model, use additional context information for more precise recommendations, and apply clustering techniques to improve model sizes. We compare our new approach, Pattern-based Bayesian Networks (PBN), to the existing BMN algorithm. We extend previously used evaluation methodologies and, in addition to prediction quality, we also evaluate model size and inference speed. Our results show that the additional context information we collect improves prediction quality, especially for queries that do not contain method calls. We also show that PBN can obtain comparable prediction quality to BMN, while model size and inference speed scale better with large input sizes. © 2015 ACM.",Code completion; Code recommender; Content assist; Evaluation; Integrated Development Environments; Machine learning; Productivity,Application programming interfaces (API); Artificial intelligence; Codes (symbols); Forecasting; Learning systems; Productivity; Quality control; Semantics; Code completions; Code recommender; Content assist; Evaluation; Integrated development environment; Bayesian networks
Combining genetic algorithms and constraint programming to support stress testing of task deadlines,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952064666&doi=10.1145%2f2818640&partnerID=40&md5=867389abeae5177ed44c29c2bae4b40c,"Tasks in real-time embedded systems (RTES) are often subject to hard deadlines that constrain how quickly the system must react to external inputs. These inputs and their timing vary in a large domain depending on the environment state and can never be fully predicted prior to system execution. Therefore, approaches for stress testing must be developed to uncover possible deadline misses of tasks for different input arrival times. In this article, we describe stress-test case generation as a search problem over the space of task arrival times. Specifically, we search for worst-case scenarios maximizing deadline misses, where each scenario characterizes a test case. In order to scale our search to large industrial-size problems, we combine two state-of-the-art search strategies, namely, genetic algorithms (GA) and constraint programming (CP). Our experimental results show that, in comparison with GA and CP in isolation, GA+CP achieves nearly the same effectiveness as CP and the same efficiency and solution diversity as GA, thus combining the advantages of the two strategies. In light of these results, we conclude that a combined GA+CP approach to stress testing is more likely to scale to large and complex systems. 2015 Copyright is held by the owner/author(s).",Constraint programming; Genetic algorithms; Real-time systems; Search-based software testing; Stress testing; Task deadline,Algorithms; Computer programming; Constraint theory; Embedded systems; Interactive computer systems; Real time systems; Software testing; Constraint programming; Environment state; Real-time embedded systems; Search strategies; Search-based software testing; Stress Testing; Task deadline; Worst case scenario; Genetic algorithms
Type-based call graph construction algorithms for scala,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951945373&doi=10.1145%2f2824234&partnerID=40&md5=808e656bd50b2dc69cd5003d318d465d,"Call graphs have many applications in software engineering. For example, they serve as the basis for code navigation features in integrated development environments and are at the foundation of static analyses performed in verification tools. While many call graph construction algorithms have been presented in the literature, we are not aware of any that handle Scala features such as traits and abstract type members. Applying existing algorithms to the JVM bytecodes generated by the Scala compiler produces very imprecise results because type information is lost during compilation. We adapt existing type-based call graph construction algorithms to Scala and present a formalization based on Featherweight Scala. An experimental evaluation shows that our most precise algorithm generates call graphs with 1.1-3.7 times fewer nodes and 1.5-17.3 times fewer edges than a bytecode-based RTA analysis. © 2015 ACM.",Call graphs; Scala; Static analysis,Application programs; Program compilers; Software engineering; Static analysis; XML; Call graph construction; Call graphs; Code navigation; Experimental evaluation; Integrated development environment; Scala; Type information; Verification tools; Algorithms
Test case prioritization using extended digraphs,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952036405&doi=10.1145%2f2789209&partnerID=40&md5=79da5fa0a834430c0829a8bea1d19faf,"Although many test case prioritization techniques exist, their performance is far from perfect. Hence, we propose a new fault-based test case prioritization technique to promote fault-revealing test cases in modelbased testing (MBT) procedures. We seek to improve the fault detection rate-a measure of how fast a test suite is able to detect faults during testing-in scenarios such as regression testing. We propose an extended digraph model as the basis of this new technique. The model is realized using a novel reinforcement-learning (RL)- and hidden-Markov-model (HMM)-based technique which is able to prioritize test cases for regression testing objectives. We present a method to initialize and train an HMM based upon RL concepts applied to an application's digraph model. The model prioritizes test cases based upon forward probabilities, a new test case prioritization approach. In addition, we also propose an alternative approach to prioritizing test cases according to the amount of change they cause in applications. To evaluate the effectiveness of the proposed techniques, we perform experiments on graphical user interface (GUI)-based applications and compare the results with state-of-the-art test case prioritization approaches. The experimental results show that the proposed technique is able to detect faults early within test runs. © 2015 ACM.",Additional statement coverage; Fault-based test case prioritization; Gui testing; HMM; Model-based testing (MBT); Random prioritization; Rein-forcement learning,Directed graphs; Fault detection; Graph theory; Graphical user interfaces; Hidden Markov models; Interface states; Markov processes; Model checking; Reinforcement learning; Software testing; User interfaces; GUI testing; HMM; Model based testing; Prioritization; Statement coverage; Test case prioritization; Testing
Editorial: Journal-first publication for the software engineering community,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952009164&doi=10.1145%2f2837717&partnerID=40&md5=3008808d3d990411a22f23972acaccde,[No abstract available],,
The effectiveness of test coverage criteria for relational database schema integrity constraints,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951980271&doi=10.1145%2f2818639&partnerID=40&md5=be854c5e7da81248618b3b81da2a4a2c,"Despite industry advice to the contrary, there has been little work that has sought to test that a relational database's schema has correctly specified integrity constraints. These critically important constraints ensure the coherence of data in a database, defending it from manipulations that could violate requirements such as ""usernames must be unique"" or ""the host name cannot be missing or unknown."" This article is the first to propose coverage criteria, derived from logic coverage criteria, that establish different levels of testing for the formulation of integrity constraints in a database schema. These range from simple criteria that mandate the testing of successful and unsuccessful INSERT statements into tables to more advanced criteria that test the formulation of complex integrity constraints such as multi-column PRIMARY KEYs and arbitrary CHECK constraints. Due to different vendor interpretations of the structured query language (SQL) specification with regard to how integrity constraints should actually function in practice, our criteria crucially account for the underlying semantics of the database management system (DBMS). After formally defining these coverage criteria and relating them in a subsumption hierarchy, we present two approaches for automatically generating tests that satisfy the criteria. We then describe the results of an empirical study that uses mutation analysis to investigate the fault-finding capability of data generated when our coverage criteria are applied to a wide variety of relational schemas hosted by three well-known and representative DBMSs-HyperSQL, PostgreSQL, and SQLite. In addition to revealing the complementary fault-finding capabilities of the presented criteria, the results show that mutation scores range from as low as just 12% of mutants being killed with the simplest of criteria to 96% with the most advanced. © 2015 ACM.",Automatic test data generation; Coverage criteria; Integrity constraints; Mutation analysis; Relational database schemas; Schema testing; Search-based software engineering; Software testing,Automatic test pattern generation; Query languages; Query processing; Search engines; Semantics; Software engineering; Automatic test data generation; Coverage criteria; Integrity constraints; Mutation analysis; Relational Database; Search-based software engineering; Software testing
Boa: Ultra-large-scale software repository and source-code mining,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949680341&doi=10.1145%2f2803171&partnerID=40&md5=e53e078a197072bcda595d9cf6ed0169,"In today's software-centric world, ultra-large-scale software repositories, such as SourceForge, GitHub, and Google Code, are the new library of Alexandria. They contain an enormous corpus of software and related information. Scientists and engineers alike are interested in analyzing this wealth of information. However, systematic extraction and analysis of relevant data from these repositories for testing hypotheses is hard, and best left for mining software repository (MSR) experts! Specifically, mining source code yields significant insights into software development artifacts and processes. Unfortunately, mining source code at a large scale remains a difficult task. Previous approaches had to either limit the scope of the projects studied, limit the scope of the mining task to be more coarse grained, or sacrifice studying the history of the code. In this article we address mining source code: (a) at a very large scale; (b) at a fine-grained level of detail; and (c) with full history information. To address these challenges, we present domain-specific language features for source-code mining in our language and infrastructure called Boa. The goal of Boa is to ease testing MSR-related hypotheses. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also show drastic improvements in scalability. © 2015 ACM.",Boa; Domain-specific language; Ease of use; Lower barrier to entry; Mining software repositories; Scalable,Codes (symbols); Computational linguistics; Computer programming languages; Data mining; Problem oriented languages; Software design; Software testing; Boa; Domain specific languages; Ease-of-use; Lower barrier to entry; Mining software repositories; Scalable; C (programming language)
Do automatically generated test cases make debugging easier? An experimental assessment of debugging effectiveness and efficiency,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951925728&doi=10.1145%2f2768829&partnerID=40&md5=6590ce44c7fb83fb14e7b7bb9829e6ee,"Several techniques and tools have been proposed for the automatic generation of test cases. Usually, these tools are evaluated in terms of fault-revealing or coverage capability, but their impact on the manual debugging activity is not considered. The question is whether automatically generated test cases are equally effective in supporting debugging as manually written tests. We conducted a family of three experiments (five replications) with humans (in total, 55 subjects) to assess whether the features of automatically generated test cases, which make them less readable and understandable (e.g., unclear test scenarios, meaningless identifiers), have an impact on the effectiveness and efficiency of debugging. The first two experiments compare different test case generation tools (Randoop vs. EvoSuite). The third experiment investigates the role of code identifiers in test cases (obfuscated vs. original identifiers), since a major difference between manual and automatically generated test cases is that the latter contain meaningless (obfuscated) identifiers. We show that automatically generated test cases are as useful for debugging as manual test cases. Furthermore, we find that, for less experienced developers, automatic tests are more useful on average due to their lower static and dynamic complexity. © 2015 ACM.",Automatic test case generation; Debugging; Empirical software engineering,Computer debugging; Efficiency; Program debugging; Software engineering; Software testing; Automatic Generation; Automatic test-case generations; Automatically generated; Effectiveness and efficiencies; Empirical Software Engineering; Experimental assessment; Techniques and tools; Test case generation; Automatic test pattern generation
Estimating semantic relatedness in source code,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952042007&doi=10.1145%2f2824251&partnerID=40&md5=01ed9ca61f1b3b684cb0a5727d34a6f1,"Contemporary software engineering tools exploit semantic relations between individual code terms to aid in code analysis and retrieval tasks. Such tools employ word similarity methods, often used in natural language processing (NLP), to analyze the textual content of source code. However, the notion of similarity in source code is different from natural language. Source code often includes unnatural domain-specific terms (e.g., abbreviations and acronyms), and such terms might be related due to their structural relations rather than linguistic aspects. Therefore, applying natural language similarity methods to source code without adjustment can produce low-quality and error-prone results. Motivated by these observations, we systematically investigate the performance of several semantic-relatedness methods in the context of software. Our main objective is to identify the most effective semantic schemes in capturing association relations between source code terms. To provide an unbiased comparison, different methods are compared against human-generated relatedness information using terms from three software systems. Results show that corpus-based methods tend to outperform methods that exploit external sources of semantic knowledge. However, due to inherent code limitations, the performance of such methods is still suboptimal. To address these limitations, we propose Normalized Software Distance (NSD), an information-theoretic method that captures semantic relatedness in source code by exploiting the distributional cues of code terms across the system. NSD overcomes data sparsity and lack of context problems often associated with source code, achieving higher levels of resemblance to the human perception of relatedness at the term and the text levels of code. © 2015 ACM.",Clustering; Information retrieval; Information theory; Latent semantics; Semantic relatedness,Computational linguistics; Computer programming languages; Information retrieval; Information theory; Natural language processing systems; Semantics; Software engineering; Clustering; Corpus-based methods; Information-theoretic methods; Latent semantics; NAtural language processing; Semantic knowledge; Semantic relatedness; Software engineering tools; Codes (symbols)
Understanding integer overflow in C/C++,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951938210&doi=10.1145%2f2743019&partnerID=40&md5=4b6c39b5affd2459c13051c48f67ea38,"Integer overflow bugs in C and C++ programs are difficult to track down and may lead to fatal errors or exploitable vulnerabilities. Although a number of tools for finding these bugs exist, the situation is complicated because not all overflows are bugs. Better tools need to be constructed, but a thorough understanding of the issues behind these errors does not yet exist. We developed IOC, a dynamic checking tool for integer overflows, and used it to conduct the first detailed empirical study of the prevalence and patterns of occurrence of integer overflows in C and C++ code. Our results show that intentional uses of wraparound behaviors are more common than is widely believed; for example, there are over 200 distinct locations in the SPEC CINT2000 benchmarks where overflow occurs. Although many overflows are intentional, a large number of accidental overflows also occur. Orthogonal to programmers' intent, overflows are found in both well-defined and undefined flavors. Applications executing undefined operations can be, and have been, broken by improvements in compiler optimizations. Looking beyond SPEC, we found and reported undefined integer overflows in SQLite, PostgreSQL, SafeInt, GNU MPC and GMP, Firefox, LLVM, Python, BIND, and OpenSSL; many of these have since been fixed. © ACM 2015.",Integer overflow; Integer wraparound; Undefined behavior,C++ (programming language); Computer software; Program compilers; Program debugging; C++ codes; Compiler optimizations; Dynamic checking; Empirical studies; Integer overflow; Integer wraparound; PostgreSQL; Undefined behavior; Integer programming
"Data model property inference, verification, and repair for web applications",2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941569283&doi=10.1145%2f2699691&partnerID=40&md5=510063a606daf6edf7e0b6515a973aad,"Most software systems nowadays are Web-based applications that are deployed over compute clouds using a three-tier architecture, where the persistent data for the application is stored in a backend datastore and is accessed and modified by the server-side code based on the user interactions at the client-side. The data model forms the foundation of these three tiers, and identifies the sets of objects (object classes) and the relations among them (associations among object classes) stored by the application. In this article, we present a set of property patterns to specify properties of a data model, as well as several heuristics for automatically inferring them. We show that the specified or inferred data model properties can be automatically verified using bounded and unbounded verification techniques. For the properties that fail, we present techniques that generate fixes to the data model that establish the failing properties. We implemented this approach for Web applications built using the Ruby on Rails framework and applied it to ten open source applications. Our experimental results demonstrate that our approach is effective in automatically identifying and fixing errors in data models of real-world web applications. © 2015 ACM.",Automated repair; Automated verification; Datamodels; Web applications,Client server computer systems; Open source software; Web services; Automated verification; Datamodels; Open source application; Property patterns; Three-tier architecture; Verification techniques; WEB application; Web-based applications; Application programs
Introduction to the special issue on ISSTA 2013,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941554217&doi=10.1145%2f2809789&partnerID=40&md5=16cd88a3d4f9f8642133facd4a4ab56e,[No abstract available],,
Does automated unit test generation really help software testers? A controlled empirical study,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941555928&doi=10.1145%2f2699688&partnerID=40&md5=d57774da2f0237628fb52200fd5adee2,"Work on automated test generation has produced several tools capable of generating test data which achieves high structural coverage over a program. In the absence of a specification, developers are expected to manually construct or verify the test oracle for each test input. Nevertheless, it is assumed that these generated tests ease the task of testing for the developer, as testing is reduced to checking the results of tests. While this assumption has persisted for decades, there has been no conclusive evidence to date confirming it. However, the limited adoption in industry indicates this assumption may not be correct, and calls into question the practical value of test generation tools. To investigate this issue, we performed two controlled experiments comparing a total of 97 subjects split between writing tests manually and writing tests with the aid of an automated unit test generation tool, EVOSUITE. We found that, on one hand, tool support leads to clear improvements in commonly applied quality metrics such as code coverage (up to 300% increase). However, on the other hand, there was no measurable improvement in the number of bugs actually found by developers. Our results not only cast some doubt on how the research community evaluates test generation tools, but also point to improvements and future work necessary before automated test generation tools will be widely adopted by practitioners. © 2015 ACM.",Automated test generation; Branch coverage; Empirical software engineering; Unit testing,Automation; Engineering research; Software testing; Automated test generations; Branch coverage; Controlled experiment; Empirical Software Engineering; Empirical studies; Research communities; Test generations; Unit testing; Automatic test pattern generation
Guidelines for coverage-based comparisons of non-adequate test suites,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941551208&doi=10.1145%2f2660767&partnerID=40&md5=5e9a9f133eaa7e881e3a2ca9bca558d5,"A fundamental question in software testing research is how to compare test suites, often as a means for comparing test-generation techniques that produce those test suites. Researchers frequently compare test suites by measuring their coverage. A coverage criterion C provides a set of test requirements and measures how many requirements a given suite satisfies. A suite that satisfies 100% of the feasible requirements is called C-adequate. Previous rigorous evaluations of coverage criteria mostly focused on such adequate test suites: given two criteria C and C, are C-adequate suites on average more effective than C-adequate suites? However, in many realistic cases, producing adequate suites is impractical or even impossible. This article presents the first extensive study that evaluates coverage criteria for the common case of nonadequate test suites: given two criteria C and C, which one is better to use to compare test suites? Namely, if suites T1, T2, . . . , Tn have coverage values c1, c2, . . . , cn for C and c 1, c2, . . . , c n for C, is it better to compare suites based on c1, c2, . . . , cn or based on c 1, c 2, . . . , c n? We evaluate a large set of plausible criteria, including basic criteria such as statement and branch coverage, as well as stronger criteria used in recent studies, including criteria based on program paths, equivalence classes of covered statements, and predicate states. The criteria are evaluated on a set of Java and C programs with both manually written and automatically generated test suites. The evaluation uses three correlation measures. Based on these experiments, two criteria perform best: branch coverage and an intraprocedural acyclic path coverage. We provide guidelines for testing researchers aiming to evaluate test suites using coverage criteria as well as for other researchers evaluating coverage criteria for research use. © 2015 ACM.",Coverage criteria; Non-adequate test suites,Automatic test pattern generation; Equivalence classes; Software testing; Well testing; Automatically generated; Branch coverage; Correlation measures; Coverage criteria; Non-adequate test suites; Rigorous evaluation; Test generations; Test requirements; C (programming language)
Automated support for reproducing and debugging field failures,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941569759&doi=10.1145%2f2774218&partnerID=40&md5=5f94abdd69bd78323b39e69d396ef3ab,"As confirmed by a recent survey conducted among developers of the Apache, Eclipse, and Mozilla projects, two extremely challenging tasks during maintenance are reproducing and debugging field failures-failures that occur on user machines after release. To help developers with these tasks, in this article we present an overall approach that comprises two different techniques: BUGREDUX and F3. BUGREDUX is a general technique for reproducing field failures that collects dynamic data about failing executions in the field and uses this data to synthesize executions that mimic the observed field failures. F3 leverages the executions generated by BUGREDUX to perform automated debugging using a set of suitably optimized fault-localization techniques. To assess the usefulness of our approach, we performed an empirical evaluation of the approach on a set of real-world programs and field failures. The results of our evaluation are promising in that, for all the failures considered, our approach was able to (1) synthesize failing executions that mimicked the observed field failures, (2) synthesize passing executions similar to the failing ones, and (3) use the synthesized executions to successfully perform fault localization with accurate results. © 2015 ACM.",Debugging; Fault localization; Field failures,Computer debugging; Computer software; Automated debugging; Automated support; Dynamic data; Empirical evaluations; Fault localization; Field failure; Mozilla; Real world projects; Software engineering
Effective techniques for static race detection in Java parallel loops,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941565255&doi=10.1145%2f2729975&partnerID=40&md5=d3e45e3b7846b232878515235aacb3fe,"Despite significant progress in recent years, the important problem of static race detection remains open. Previous techniques took a general approach and looked for races by analyzing the effects induced by lowlevel concurrency constructs (e.g., Java.lang.Thread). But constructs and libraries for expressing parallelism at a higher level (e.g., fork-join, futures, parallel loops) are becoming available in all major programming languages. We claim that specializing an analysis to take advantage of the extra semantic information provided by the use of these constructs and libraries improves precision and scalability. We present ITERACE, a set of techniques that are specialized to use the intrinsic thread, safety, and dataflow structure of collections and of the new loop parallelism mechanism introduced in Java 8. Our evaluation shows that ITERACE is fast and precise enough to be practical. It scales to programs of hundreds of thousands of lines of code and reports very few race warnings, thus avoiding a common pitfall of static analyses. In five out of the seven case studies, ITERACE reported no false warnings. Also, it revealed six bugs in real-world applications. We reported four of them: one had already been fixed, and three were new and the developers confirmed and fixed them. Furthermore, we evaluate the effect of each specialization technique on the running time and precision of the analysis. For each application, we run the analysis under 32 different configurations. This allows to analyze each technique's effect both alone and in all possible combinations with other techniques. © 2015 ACM.",Java; Static analysis; Static race detection; Synchronization,Libraries; Program debugging; Semantics; Static analysis; Synchronization; Case-studies; Dataflow structures; Java; Lines of code; Parallel loops; Running time; Semantic information; Static race detection; Java programming language
Documenting design-pattern instances: A family of experiments on source-code comprehensibility,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930177029&doi=10.1145%2f2699696&partnerID=40&md5=8bfd96b9f97ea77a7f731b311a7e19c1,"Design patterns are recognized as a means to improve software maintenance by furnishing an explicit specification of class and object interactions and their underlying intent [Gamma et al. 1995]. Only a few empirical investigations have been conducted to assess whether the kind of documentation for design patterns implemented in source code affects its comprehensibility. To investigate this aspect, we conducted a family of four controlled experiments with 88 participants having different experience (i.e., professionals and Bachelor, Master, and PhD students). In each experiment, the participants were divided into three groups and asked to comprehend a nontrivial chunk of an open-source software system. Depending on the group, each participant was, or was not, provided with graphical or textual representations of the design patterns implemented within the source code. We graphically documented design-pattern instances with UML class diagrams. Textually documented instances are directly reported source code as comments. Our results indicate that documenting design-pattern instances yields an improvement in correctness of understanding source code for those participants with an adequate level of experience. © 2015 ACM.",Controlled experiment; Design patterns; Maintenance; Replications; Software models; Source-code comprehension,Computer programming languages; Computer software maintenance; Maintenance; Open systems; Controlled experiment; Design Patterns; Replications; Software model; Source code comprehensions; Open source software
Software change contracts,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930161410&doi=10.1145%2f2729973&partnerID=40&md5=d02498b5ba6beb7683c525b3ebc27f33,"Software errors often originate from incorrect changes, including incorrect program fixes, incorrect feature updates, and so on. Capturing the intended program behavior explicitly via contracts is thus an attractive proposition. In our recent work, we had espoused the notion of ""change contracts"" to express the intended program behavior changes across program versions. Change contracts differ from program contracts in that they do not require the programmer to describe the intended behavior of those program features which are unchanged across program versions. In this work, we present the formal semantics of our change contract language built on top of the Java modeling language (JML). Our change contract language can describe behavioral as well as structural changes. We evaluate the expressivity of the change contract language via a survey given to final-year undergraduate students. The survey results enable to understand the usability of our change contract language for purposes of writing contracts, comprehending written contracts, and modifying programs according to given change contracts. Finally, we develop both dynamic and static checkers for change contracts, and show how they can be used in maintaining software changes. We use our dynamic checker to automatically suggest tests that manifest violations of change contracts. Meanwhile, we use our static checker to verify that a program is changed as specified in its change contract. Apart from verification, our static checker also performs various other software engineering tasks, such as localizing the buggy method, detecting/debugging regression errors, and classifying the cause for a test failure as either error in production code or error in test code. © 2015 ACM.",Dynamic checking; Software changes; Static checking,Computer simulation languages; Errors; Formal methods; Modeling languages; Semantics; Students; Surveys; Verification; Contract languages; Dynamic checking; Java Modeling Language; Program behavior; Regression errors; Software change; Static checking; Undergraduate students; Software testing
A baseline model for software effort estimation,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930149397&doi=10.1145%2f2738037&partnerID=40&md5=d886e9e1c047501b7fada72677922719,"Software effort estimation (SEE) is a core activity in all software processes and development lifecycles. A range of increasingly complex methods has been considered in the past 30 years for the prediction of effort, often with mixed and contradictory results. The comparative assessment of effort prediction methods has therefore become a common approach when considering how best to predict effort over a range of project types. Unfortunately, these assessments use a variety of sampling methods and error measurements, making comparison with other work difficult. This article proposes an automatically transformed linear model (ATLM) as a suitable baseline model for comparison against SEE methods. ATLM is simple yet performs well over a range of different project types. In addition, ATLM may be used with mixed numeric and categorical data and requires no parameter tuning. It is also deterministic, meaning that results obtained are amenable to replication. These and other arguments for using ATLM as a baseline model are presented, and a reference implementation described and made available. We suggest that ATLM should be used as a baseline of effort prediction quality for all future model comparisons in SEE. © 2015 ACM.",Baseline model; Transformed linear model,Life cycle; Baseline models; Comparative assessment; Effort prediction; Error measurements; Linear modeling; Parameter-tuning; Reference implementation; Software effort estimation; Forecasting
aToucan: An automated framework to derive UML analysis models from use case models,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930158786&doi=10.1145%2f2699697&partnerID=40&md5=4c730f9444b950899fda5e9a1f0a4ea8,"The transition from an informal requirements specification in natural language to a structured, precise specification is an important challenge in practice. It is particularly so for object-oriented methods, defined in the context of the OMG's Model Driven Architecture (MDA), where a key step is to transition from a use case model to an analysis model. However, providing automated support for this transition is challenging, mostly because, in practice, requirements are expressed in natural language and are much less structured than other kinds of development artifacts. Such an automated transformation would enable at least the generation of an initial, likely incomplete, analysis model and enable automated traceability from requirements to code, through various intermediate models. In this article, we propose a method and a tool called aToucan, building on existing work, to automatically generate a UML analysis model comprising class, sequence and activity diagrams from a use case model and to automatically establish traceability links between model elements of the use case model and the generated analysis model. Note that our goal is to save effort through automated support, not to replace human abstraction and decision making. Seven (six) case studies were performed to compare class (sequence) diagrams generated by aToucan to the ones created by experts, Masters students, and trained, fourth-year undergraduate students. Results show that aToucan performs well regarding consistency (e.g., 88% class diagram consistency) and completeness (e.g., 80% class completeness) when comparing generated class diagrams with reference class diagrams created by experts and Masters students. Similarly, sequence diagrams automatically generated by aToucan are highly consistent with the ones devised by experts and are also rather complete, for instance, 91% and 97% message consistency and completeness, respectively. Further, statistical tests show that aToucan significantly outperforms fourth-year engineering students in this respect, thus demonstrating the value of automation. We also conducted two industrial case studies demonstrating the applicability of aToucan in two different industrial domains. Results showed that the vast majority of model elements generated by aToucan are correct and that therefore, in practice, such models would be good initial models to refine and augment so as to converge towards to correct and complete analysis models. A performance analysis shows that the execution time of aToucan (when generating class and sequence diagrams) is dependent on the number of simple sentences contained in the use case model and remains within a range of a few minutes. Five different software system descriptions (18 use cases altogether) were performed to evaluate the generation of activity diagrams. Results show that aToucan can generate 100% complete and correct control flow information of activity diagrams and on average 85% data flAow information completeness. Moreover, we show that aToucan outperforms three commercial tools in terms of activity diagram generation. © 2015 ACM.",Activity diagram; Analysis model; Automation; Class diagram; Sequence diagram; Traceability; Transformation; UML; Use case modeling,Automation; Decision making; Encoding (symbols); Graphic methods; Object oriented programming; Software architecture; Software design; Specifications; Activity diagram; Analysis models; Class diagrams; Sequence diagrams; Traceability; Transformation; Use case model; Students
Platys: An active learning framework for place-aware application development and its evaluation,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930154899&doi=10.1145%2f2729976&partnerID=40&md5=8057adc6b7cc5ab5bab3c98ac607aacf,"We introduce a high-level abstraction of location called place. A place derives its meaning from a user's physical space, activities, or social context. In this manner, place can facilitate improved user experience compared to the traditional representation of location, which is spatial coordinates. We propose the Platys framework as a way to address the special challenges of place-aware application development. The core of Platys is a middleware that (1) learns a model of places specific to each user via active learning, a machine learning paradigm that seeks to reduce the user-effort required for training the middleware, and (2) exposes the learned user-specific model of places to applications at run time, insulating application developers from dealing with both low-level sensors and user idiosyncrasies in perceiving places. We evaluated Platys via two studies. First, we collected place labels and Android phone sensor readings from 10 users. We applied Platys' active learning approach to learn each user's places and found that Platys (1) requires fewer place labels to learn a user's places with a desired accuracy than do two traditional supervised approaches, and (2) learns places with higher accuracy than two unsupervised approaches. Second, we conducted a developer study to evaluate Platys' efficiency in assisting developers and its effectiveness in enabling usable applications. In this study, 46 developers employed either Platys or the Android location API to develop a place-aware application. Our results indicate that application developers employing Platys, when compared to those employing the Android API, (1) develop a place-aware application faster and perceive reduced difficulty and (2) produce applications that are easier to understand (for developers) and potentially more usable and privacy preserving (for application users). © 2015 ACM.",Active learning; Contextaware; Location-aware; Middleware; Mobile application development; Place recognition; Place-aware; Privacy; Semi-supervised learning; Usability,Android (operating system); Data privacy; Location; Middleware; Mobile computing; Semi-supervised learning; User experience; Active Learning; Context-Aware; Location-aware; Mobile application development; Place recognition; Place-aware; Usability; Application programming interfaces (API)
Automatic workarounds: Exploiting the intrinsic redundancy of web applications,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930152100&doi=10.1145%2f2755970&partnerID=40&md5=7bffbc2848cd5ea5cc9323906d84a91d,"Despite the best intentions, the competence, and the rigorous methods of designers and developers, software is often delivered and deployed with faults. To cope with imperfect software, researchers have proposed the concept of self-healing for software systems. The ambitious goal is to create software systems capable of detecting and responding ""autonomically"" to functional failures, or perhaps even preempting such failures, to maintain a correct functionality, possibly with acceptable degradation. We believe that self-healing can only be an expression of some form of redundancy, meaning that, to automatically fix a faulty behavior, the correct behavior must be already present somewhere, in some form, within the software system either explicitly or implicitly. One approach is to deliberately design and develop redundant systems, and in fact this kind of deliberate redundancy is the essential ingredient of many fault tolerance techniques. However, this type of redundancy is also generally expensive and does not always satisfy the time and cost constraints of many software projects. With this article we take a different approach. We observe that modern software systems naturally acquire another type of redundancy that is not introduced deliberately but rather arises intrinsically as a by-product of modern modular software design. We formulate this notion of intrinsic redundancy and we propose a technique to exploit it to achieve some level of self-healing. We first demonstrate that software systems are indeed intrinsically redundant. Then we develop a way to express and exploit this redundancy to tolerate faults with automatic workarounds. In essence, a workaround amounts to replacing some failing operations with alternative operations that are semantically equivalent in their intended effect, but that execute different code and ultimately avoid the failure. The technique we propose finds such workarounds automatically. We develop this technique in the context of Web applications. In particular, we implement this technique within a browser extension, which we then use in an evaluation with several known faults and failures of three popular Web libraries. The evaluation demonstrates that automatic workarounds are effective: out of the nearly 150 real faults we analyzed, 100 could be overcome with automatic workarounds, and half of these workarounds found automatically were not publicly known before. © 2015 ACM.",Automatic workarounds; Web API; Web applications,Computer software; Fault tolerance; Product design; Redundancy; Self-healing materials; Automatic workarounds; Fault tolerance techniques; Functional failure; Modular software designs; Software project; Software systems; Web API; WEB application; Software design
Many-objective software remodularization using NSGA-III,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930174017&doi=10.1145%2f2729974&partnerID=40&md5=5876e6aef6e63d3e8ad483016cc2ae8d,"Software systems nowadays are complex and difficult to maintain due to continuous changes and bad design choices. To handle the complexity of systems, software products are, in general, decomposed in terms of packages/modules containing classes that are dependent. However, it is challenging to automatically remodularize systems to improve their maintainability. The majority of existing remodularization work mainly satisfy one objective which is improving the structure of packages by optimizing coupling and cohesion. In addition, most of existing studies are limited to only few operation types such as move class and split packages. Many other objectives, such as the design semantics, reducing the number of changes and maximizing the consistency with development change history, are important to improve the quality of the software by remodularizing it. In this article, we propose a novel many-objective search-based approach using NSGA-III. The process aims at finding the optimal remodularization solutions that improve the structure of packages, minimize the number of changes, preserve semantics coherence, and reuse the history of changes. We evaluate the efficiency of our approach using four different open-source systems and one automotive industry project, provided by our industrial partner, through a quantitative and qualitative study conducted with software engineers. © 2015 ACM.",Remodularization; Search-based software engineering; Software maintenance; Software quality,Automotive industry; Computer software maintenance; Computer software selection and evaluation; Open source software; Semantics; Software engineering; Industrial partners; Open source system; Qualitative study; Remodularization; Search-based software engineering; Software products; Software Quality; Software systems; Open systems
Deciding type-based partial-order constraints for path-sensitive analysis,2015,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930151677&doi=10.1145%2f2755971&partnerID=40&md5=9c669d1ecff7b8ff422a2183f0ee7fa6,"The precision and scalability of path-sensitive program analyses depend on their ability to distinguish feasible and infeasible program paths. Analyses express path feasibility as the satisfiability of conjoined branch conditions, which is then decided by cooperating decision procedures such as those in satisfiability modulo theory (SMT) solvers. Consequently, efficient underlying decision procedures are key to precise, scalable program analyses. When we investigate the branch conditions accumulated by inter-procedural path-sensitive analyses of object-oriented programs, we find that many relate to an object's dynamic type. These conditions arise from explicit type tests and the branching implicit in dynamic dispatch and type casting. These conditions share a common form that comprises a fragment of the theory of partial orders, which we refer to as type-based partial orders (TPO). State-of-the-art SMT solvers can heuristically instantiate the quantified formulae that axiomatize partial orders, and thereby support TPO constraints. We present two custom decision procedures with significantly better performance. On benchmarks that reflect inter-procedural path-sensitive analyses applied to significant Java systems, the custom procedures run three orders of magnitude faster. The performance of the two decision procedures varies across benchmarks, which suggests that a portfolio approach may be beneficial for solving constraints generated by program analyses. © 2015 ACM.",Decision procedure; Path-sensitive analysis; SMT; Subtyping; Type hierarchy,Benchmarking; Decision theory; Formal logic; Surface mount technology; Decision procedure; Object-oriented program; Path-sensitive analysis; Program analysis; Satisfiability modulo Theories; Subtypings; Three orders of magnitude; Type hierarchies; Object oriented programming
When and how to use multilevel modelling,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920188632&doi=10.1145%2f2685615&partnerID=40&md5=bc79fd0d213251defa1e986b9f160818,"Model-Driven Engineering (MDE) promotes models as the primary artefacts in the software development process, from which code for the final application is derived. Standard approaches to MDE (like those based on MOF or EMF) advocate a two-level metamodelling setting where Domain-Specific Modelling Languages (DSMLs) are defined through a metamodel that is instantiated to build models at the metalevel below. Multilevel modelling (also called deep metamodelling) extends the standard approach to metamodelling by enabling modelling at an arbitrary number of metalevels, not necessarily two. Proposers of multilevel modelling claim this leads to simpler model descriptions in some situations, although its applicability has been scarcely evaluated. Thus, practitioners may find it difficult to discern when to use it and how to implement multilevel solutions in practice. In this article, we discuss those situations where the use of multilevel modelling is beneficial, and identify recurring patterns and idioms. Moreover, in order to assess how often the identified patterns arise in practice, we have analysed a wide range of existing two-level DSMLs from different sources and domains, to detect when their elements could be rearranged in more than two metalevels. The results show this scenario is not uncommon, while in some application domains (like software architecture and enterprise/process modelling) pervasive, with a high average number of pattern occurrences per metamodel. © 2014 ACM.",Domain-Specific Modelling Languages; Metamodelling; Metamodelling patterns; Model-Driven Engineering; Multilevel modelling,Application programs; Software design; Arbitrary number; Average numbers; Domain-Specific Modelling Languages; Meta-modelling; Model description; Model-driven Engineering; Multilevel solutions; Software development process; Modeling languages
Conditional commitments: Reasoning and model checking,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920161666&doi=10.1145%2f2685613&partnerID=40&md5=6db8b093db09489c479b97b83e5e9489,"While modeling interactions using social commitments provides a fundamental basis for capturing flexible and declarative interactions and helps in addressing the challenge of ensuring compliance with specifications, the designers of the system cannot guarantee that an agent complies with its commitments as it is supposed to, or at least an agent doesn't want to violate its commitments. They may still wish to develop efficient and scalable algorithms by which model checking conditional commitments, a natural and universal frame of social commitments, is feasible at design time. However, distinguishing between different but related types of conditional commitments, and developing dedicated algorithms to tackle the problem of model checking conditional commitments, is still an active research topic. In this article, we develop the temporal logic CTLcc that extends Computation Tree Logic (CTL) with new modalities which allow representing and reasoning about two types of communicating conditional commitments and their fulfillments using the formalism of interpreted systems. We introduce a set of rules to reason about conditional commitments and their fulfillments. The verification technique is based on developing a new symbolic model checking algorithm to address this verification problem. We analyze the computational complexity and present the full implementation of the developed algorithm on top of the MCMAS model checker. We also evaluate the algorithm's effectiveness and scalability by verifying the compliance of the NetBill protocol, taken from the business domain, and the process of breast cancer diagnosis and treatment, taken from the health-care domain, with specifications expressed in CTLcc. We finally compare the experimental results with existing proposals. © 2014 ACM.",Reasoning rules; Strong and weak conditional commitments,Specifications; Temporal logic; Breast cancer diagnosis; Computation tree logic; Interpreted systems; Reasoning rules; Strong and weak conditional commitments; Symbolic model checking; Verification problems; Verification techniques; Model checking
A large-scale evaluation of automated unit test generation using EvoSuite,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920201232&doi=10.1145%2f2685612&partnerID=40&md5=c56133de2bee1209ec9235dc020fca4d,"Research on software testing produces many innovative automated techniques, but because software testing is by necessity incomplete and approximate, any new technique faces the challenge of an empirical assessment. In the past, we have demonstrated scientific advance in automated unit test generation with the EvoSuite tool by evaluating it on manually selected open-source projects or examples that represent a particular problem addressed by the underlying technique. However, demonstrating scientific advance is not necessarily the same as demonstrating practical value; even if EvoSuite worked well on the software projects we selected for evaluation, it might not scale up to the complexity of real systems. Ideally, one would use large ""real-world"" software systems to minimize the threats to external validity when evaluating research tools. However, neither choosing such software systems nor applying research prototypes to them are trivial tasks. In this article we present the results of a large experiment in unit test generation using the EvoSuite tool on 100 randomly chosen open-source projects, the 10 most popular open-source projects according to the SourceForge Web site, seven industrial projects, and 11 automatically generated software projects. The study confirms that EvoSuite can achieve good levels of branch coverage (on average, 71% per class) in practice. However, the study also exemplifies how the choice of software systems for an empirical study can influence the results of the experiments, which can serve to inform researchers to make more conscious choices in the selection of software system subjects. Furthermore, our experiments demonstrate how practical limitations interfere with scientific advances, branch coverage on an unbiased sample is affected by predominant environmental dependencies. The surprisingly large effect of such practical engineering problems in unit testing will hopefully lead to a larger appreciation of work in this area, thus supporting transfer of knowledge from software testing research to practice. © 2014 ACM.",Automated test generation; Benchmark; Branch coverage; Empirical software engineering; Java; JUnit; Unit testing,Automatic test pattern generation; Automation; Benchmarking; Engineering research; Knowledge management; Open source software; Open systems; Petroleum reservoir evaluation; Software prototyping; Automated test generations; Branch coverage; Empirical Software Engineering; Java; JUnit; Unit testing; Software testing
Formal verification of software countermeasures against side-channel attacks,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920131533&doi=10.1145%2f2685616&partnerID=40&md5=873b24d24e29b2964f1ef7ee1d720642,"A common strategy for designing countermeasures against power-analysis-based side-channel attacks is using random masking techniques to remove the statistical dependency between sensitive data and sidechannel emissions. However, this process is both labor intensive and error prone and, currently, there is a lack of automated tools to formally assess how secure a countermeasure really is. We propose the first SMT-solver-based method for formally verifying the security of a masking countermeasure against such attacks. In addition to checking whether the sensitive data are masked by random variables, we also check whether they are perfectly masked, that is, whether the intermediate computation results in the implementation of a cryptographic algorithm are independent of the secret key. We encode this verification problem using a series of quantifier-free first-order logic formulas, whose satisfiability can be decided by an off-the-shelf SMT solver.Wehave implemented the proposed method in a software verification tool based on the LLVM compiler frontend and the Yices SMT solver. Our experiments on a set of recently proposed masking countermeasures for cryptographic algorithms such as AES and MAC-Keccak show the method is both effective in detecting power side-channel leaks and scalable for practical use. © 2014 ACM.",AES; Countermeasure; Cryptographic software; Differential power analysis; MAC-Keccak; Perfect masking; Satisfiability modulo theory (SMT); Side-channel attack,Formal logic; Formal verification; Countermeasure; Cryptographic software; Differential power Analysis; MAC-Keccak; Satisfiability modulo Theories; Side channel attack
Residual investigation: Predictive and precise Bug detection,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920135597&doi=10.1145%2f2656201&partnerID=40&md5=da2e0c230473989923f1c73fa0fc4cdc,"We introduce the concept of residual investigation for program analysis. A residual investigation is a dynamic check installed as a result of running a static analysis that reports a possible program error. The purpose is to observe conditions that indicate whether the statically predicted program fault is likely to be realizable and relevant. The key feature of a residual investigation is that it has to be much more precise (i.e., with fewer false warnings) than the static analysis alone, yet significantly more general (i.e., reporting more errors) than the dynamic tests in the program's test suite that are pertinent to the statically reported error. That is, good residual investigations encode dynamic conditions that, when considered in conjunction with the static error report, increase confidence in the existence or severity of an error without needing to directly observe a fault resulting from the error. We enhance the static analyzer FindBugs with several residual investigations appropriately tuned to the static error patterns in FindBugs, and apply it to nine large open-source systems and their native test suites. The result is an analysis with a low occurrence of false warnings (false positives) while reporting several actual errors that would not have been detected by mere execution of a program's test suite. © 2014 ACM.",Existing test cases; False warnings; RFBI,Errors; Open systems; Static analysis; Testing; Dynamic condition; False positive; False warnings; Open source system; Program analysis; RFBI; Static analyzers; Test case; Software testing
A unified test case prioritization approach,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920185224&doi=10.1145%2f2685614&partnerID=40&md5=abbdc8bed76d839a2620a96bcc14ca38,"Test case prioritization techniques attempt to reorder test cases in a manner that increases the rate at which faults are detected during regression testing. Coverage-based test case prioritization techniques typically use one of two overall strategies: a total strategy or an additional strategy. These strategies prioritize test cases based on the total number of code (or code-related) elements covered per test case and the number of additional (not yet covered) code (or code-related) elements covered per test case, respectively. In this article, we present a unified test case prioritization approach that encompasses both the total and additional strategies. Our unified test case prioritization approach includes two models (basic and extended) by which a spectrum of test case prioritization techniques ranging from a purely total to a purely additional technique can be defined by specifying the value of a parameter referred to as the fp value. To evaluate our approach, we performed an empirical study on 28 Java objects and 40 C objects, considering the impact of three internal factors (model type, choice of fp value, and coverage type) and three external factors (coverage granularity, test case granularity, and programming/testing paradigm), all of which can be manipulated by our approach. Our results demonstrate that a wide range of techniques derived from our basic and extended models with uniform fp values can outperform purely total techniques and are competitive with purely additional techniques. Considering the influence of each internal and external factor studied, the results demonstrate that various values of each factor have nontrivial influence on test case prioritization techniques. © 2014 ACM.",Additional strategy; Software testing; Test case prioritization; Total strategy,C (programming language); Additional strategy; Empirical studies; External factors; Internal and external factors; Internal factors; Regression testing; Test case prioritization; Total strategy; Software testing
Directed incremental symbolic execution,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907991946&doi=10.1145%2f2629536&partnerID=40&md5=a3ebefa1757f3d031ac329528f28cf74,"The last few years have seen a resurgence of interest in the use of symbolic execution - a program analysis technique developed more than three decades ago to analyze program execution paths. Scaling symbolic execution to real systems remains challenging despite recent algorithmic and technological advances. An effective approach to address scalability is to reduce the scope of the analysis. For example, in regression analysis, differences between two related program versions are used to guide the analysis. While such an approach is intuitive, finding efficient and precise ways to identify program differences, and characterize their impact on how the program executes has proved challenging in practice. In this article, we present Directed Incremental Symbolic Execution (DiSE), a novel technique for detecting and characterizing the impact of program changes to scale symbolic execution. The novelty of DiSE is to combine the efficiencies of static analysis techniques to compute program difference information with the precision of symbolic execution to explore program execution paths and generate path conditions affected by the differences. DiSE complements other reduction and bounding techniques for improving symbolic execution. Furthermore, DiSE does not require analysis results to be carried forward as the software evolves - only the source code for two related program versions is required. An experimental evaluation using our implementation of DiSE illustrates its effectiveness at detecting and characterizing the effects of program changes. © 2014 ACM.",Program differencing; software evolution; symbolic execution,Model checking; Regression analysis; Analysis techniques; Bounding techniques; Effective approaches; Experimental evaluation; Program differencing; Software Evolution; Symbolic execution; Technological advances; Static analysis
Code-smell detection as a bilevel problem,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908128129&doi=10.1145%2f2675067&partnerID=40&md5=a9a9d2da5d48ea783278d6041591144b,"Code smells represent design situations that can affect the maintenance and evolution of software. They make the system difficult to evolve. Code smells are detected, in general, using quality metrics that represent some symptoms. However, the selection of suitable quality metrics is challenging due to the absence of consensus in identifying some code smells based on a set of symptoms and also the high calibration effort in determining manually the threshold value for each metric. In this article, we propose treating the generation of code smell detection rules as a bilevel optimization problem. Bilevel optimization problems represent a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, only the optimal solutions to the lower-level problem become possible feasible candidates to the upper-level problem. In this sense, the code-smell detection problem can be treated as a bilevel optimization problem, but due to lack of suitable solution techniques, it has been attempted to be solved as a single-level optimization problem in the past. In our adaptation here, the upper-level problem generates a set of detection rules, a combination of quality metrics, which maximizes the coverage of the base of code-smell examples and artificial code smells generated by the lower level. The lower level maximizes the number of generated artificial code smells that cannot be detected by the rules produced by the upper level. The main advantage of our bilevel formulation is that the generation of detection rules is not limited to some code-smell examples identified manually by developers that are difficult to collect, but it allows the prediction of new code-smell behavior that is different from those of the base of examples. The statistical analysis of our experiments over 31 runs on nine open source systems and one industrial project shows that seven types of code smells were detected with an average of more than 86% in terms of precision and recall. The results confirm the out performance of our bilevel proposal compared to state-of-art code-smell detection techniques. The evaluation performed by software engineers also confirms the relevance of detected code smells to improve the quality of software systems. © 2014 ACM.",,Odors; Open source software; Optimization; Quality control; Bilevel optimization problems; Detection problems; Industrial projects; Open source system; Optimization problems; Precision and recall; Quality of softwares; Suitable solutions; Open systems
Mining unit tests for discovery and migration of math APIs,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907998636&doi=10.1145%2f2629506&partnerID=40&md5=01fd266ca673d085a80a47ae8ca58a11,"Today's programming languages are supported by powerful third-party APIs. For a given application domain, it is common to have many competing APIs that provide similar functionality. Programmer productivity therefore depends heavily on the programmer's ability to discover suitable APIs both during an initial coding phase, as well as during software maintenance. The aim of this work is to support the discovery and migration of math APIs. Math APIs are at the heart of many application domains ranging from machine learning to scientific computations. Our approach, called MathFinder, combines executable specifications of mathematical computations with unit tests (operational specifications) of API methods. Given a math expression, MathFinder synthesizes pseudo-code comprised of API methods to compute the expression by mining unit tests of the API methods. We present a sequential version of our unit test mining algorithm and also design a more scalable data-parallel version. We perform extensive evaluation of MathFinder (1) for API discovery, where math algorithms are to be implemented from scratch and (2) for API migration, where client programs utilizing a math API are to be migrated to another API. We evaluated the precision and recall of MathFinder on a diverse collection of math expressions, culled from algorithms used in a wide range of application areas such as control systems and structural dynamics. In a user study to evaluate the productivity gains obtained by using MathFinder for API discovery, the programmers who used MathFinder finished their programming tasks twice as fast as their counterparts who used the usual techniques like web and code search, IDE code completion, and manual inspection of library documentation. For the problem of API migration, as a case study, we used MathFinder to migrate Weka, a popular machine learning library. Overall, our evaluation shows that MathFinder is easy to use, provides highly precise results across several math APIs and application domains even with a small number of unit tests per method, and scales to large collections of unit tests. © 2014 ACM.",API discovery; API migration; mathematical computation; mining; unit tests,Codes (symbols); Digital libraries; Machine learning; Mining; Object oriented programming; Productivity; Specifications; Structural dynamics; Testing; Api discoveries; API migration; Executable specifications; Mathematical computation; Operational specifications; Programmer productivity; Scientific computation; Unit tests; Application programming interfaces (API)
Combining centralised and distributed testing,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907971358&doi=10.1145%2f2661296&partnerID=40&md5=316d5956305c93e19dbc895ce031a2c8,"Many systems interact with their environment at distributed interfaces (ports) and sometimes it is not possible to place synchronised local testers at the ports of the system under test (SUT). There are then two main approaches to testing: having independent local testers or a single centralised tester that interacts asynchronously with the SUT. The power of using independent testers has been captured using implementation relation dioco. In this article, we define implementation relation diococ for the centralised approach and prove that dioco and diococ are incomparable. This shows that the frameworks detect different types of faults and so we devise a hybrid framework and define an implementation relation diocos for this. We prove that the hybrid framework is more powerful than the distributed and centralised approaches. We then prove that the Oracle problem is NP-complete for diococ and diocos but can be solved in polynomial time if we place an upper bound on the number of ports. Finally, we consider the problem of deciding whether there is a test case that is guaranteed to force a finite state model into a particular state or to distinguish two states, proving that both problems are undecidable for the centralised and hybrid frameworks. © 2014 ACM.",Centralised testing; distributed testing; model-based testing,Polynomial approximation; Centralised; Distributed interfaces; Distributed testing; Finite-state models; Hybrid framework; Implementation relation; Model based testing; System under test; Model checking
Assessing the effect of screen mockups on the comprehension of functional requirements,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907980696&doi=10.1145%2f2629457&partnerID=40&md5=3685952260932704985e61ad24d78d9d,"Over the last few years, the software engineering community has proposed a number of modeling methods to represent functional requirements. Among them, use cases are recognized as an easy to use and intuitive way to capture and define such requirements. Screen mockups (also called user-interface sketches or user interface-mockups) have been proposed as a complement to use cases for improving the comprehension of functional requirements. In this article, we aim at quantifying the benefits achievable by augmenting use cases with screen mockups in the comprehension of functional requirements with respect to effectiveness, effort, and efficiency. For this purpose, we conducted a family of four controlled experiments, involving 139 participants having different profiles. The experiments involved comprehension tasks performed on the requirements documents of two desktop applications. Independently from the participants' profile, we found a statistically significant large effect of the presence of screen mockups on both comprehension effectiveness and comprehension task efficiency. No significant effect was observed on the effort to complete tasks. The main pragmatic lesson is that the screen mockups addition to use cases is able to almost double the efficiency of comprehension tasks. © 2014 ACM.",analysis models; controlled experiment; family of experiments; replicated experiments; Screen mockups; use cases,Efficiency; Software engineering; User interfaces; Analysis models; Comprehension tasks; Controlled experiment; Desktop applications; Engineering community; Functional requirement; Replicated experiment; Requirements document; Mockups
A continuous ASM modelling approach to pacemaker sensing,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908005760&doi=10.1145%2f2610375&partnerID=40&md5=ed5a09ade0548825292e195f5e0a47a6,"The cardiac pacemaker system, proposed as a problem topic in the Verification Grand Challenge, offers a range of difficulties to address for formal specification, development, and verification technologies. We focus on the sensing problem, the question of whether the heart has produced a spontaneous heartbeat or not. This question is plagued by uncertainties arising from the often unpredictable environment that a real pacemaker finds itself in. We develop a time domain tracking approach to this problem, as a complement to the usual frequency domain approach most frequently used. We develop our case study in the continuous ASM (Abstract State Machine) formalism, which is briefly summarised, through a series of refinement and retrenchment steps, each adding new levels of complexity to the model. © 2014 ACM.",cardiac pacemakers; Continuous ASM; rigorous design and development; sensing,Pacemakers; Abstract state machines; Cardiac pacemakers; Continuous ASM; Design and Development; Frequency domain approaches; sensing; Unpredictable environments; Verification grand challenge; Frequency domain analysis
Amplifying tests to validate exception handling code: An extended study in the mobile application domain,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907021168&doi=10.1145%2f2652483&partnerID=40&md5=7f7ccb269685dc56fea625b490cdbb2e,"Validating code handling exceptional behavior is difficult, particularly when dealing with external resources that may be noisy and unreliable, as it requires (1) systematic exploration of the space of exceptions thatmay be thrown by the external resources, and (2) setup of the context to trigger specific patterns of exceptions. In this work, we first present a study quantifying the magnitude of the problem by inspecting the bug repositories of a set of popular applications in the increasingly relevant domain of Android mobile applications. The study revealed that 22% of the confirmed and fixed bugs have to do with poor exceptional handling code, and half of those correspond to interactions with external resources. We then present an approach that addresses this challenge by performing an systematic amplification of the program space explored by a test by manipulating the behavior of external resources. Each amplification attempts to expose a program's exception handling constructs to new behavior by mocking an external resource so that it returns normally or throws an exception following a predefined set of patterns. Our assessment of the approach indicates that it can be fully automated, is powerful enough to detect 67% of the faults reported in the bug reports of this kind, and is precise enough that 78% of the detected anomalies are fixed, and it has a great potential to assist developers. © 2014 ACM.",Exception handling; Mobile applications; Test amplification; Test case generation; Test transformation,Mobile computing; Program debugging; Testing; Exception handling; Mobile applications; Test amplifications; Test case generation; Test transformation; Software testing
DIG: A dynamic invariant generator for polynomial and array invariants,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907021166&doi=10.1145%2f2556782&partnerID=40&md5=1b697b1513ed04482d24cc952fa81e69,"This article describes and evaluates DIG, a dynamic invariant generator that infers invariants from observed program traces, focusing on numerical and array variables. For numerical invariants, DIG supports both nonlinear equalities and inequalities of arbitrary degree defined over numerical program variables. For array invariants, DIG generates nested relations among multidimensional array variables. These properties are nontrivial and challenging for current static and dynamic invariant analysis methods. The key difference between DIG and existing dynamic methods is its generative technique, which infers invariants directly from traces, instead of using traces to filter out predefined templates. To generate accurate invariants, DIG employs ideas and tools from the mathematical and formal methods domains, including equation solving, polyhedra construction, and theorem proving; for example, DIG represents and reasons about polynomial invariants using geometric shapes. Experimental results on 27 mathematical algorithms and an implementation of AES encryption provide evidence that DIG is effective at generating invariants for these programs. © 2014 ACM.",Array invariants; Dynamic analysis; Geometric invariant inference; Invariant generation; Nonlinear invariants; Program analysis; Theorem proving,Dynamic analysis; Formal methods; Geometry; Theorem proving; Array invariants; Geometric invariant; Invariant generations; Nonlinear invariants; Program analysis; Cryptography
"Peer review on open-source software projects: Parameters, statistical models, and theory",2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907033452&doi=10.1145%2f2594458&partnerID=40&md5=d9a999e0a4f45aa00dd36f91300a0078,"Peer review is seen as an important quality-assurance mechanism in both industrial development and the open-source software (OSS) community. The techniques for performing inspections have been well studied in industry; in OSS development, software peer reviews are not as well understood.; To develop an empirical understanding of OSS peer review, we examine the review policies of 25 OSS projects and study the archival records of six large, mature, successful OSS projects. We extract a series of measures based on those used in traditional inspection experiments.We measure the frequency of review, the size of the contribution under review, the level of participation during review, the experience and expertise of the individuals involved in the review, the review interval, and the number of issues discussed during review. We create statistical models of the review efficiency, review interval, and effectiveness, the issues discussed during review, to determine which measures have the largest impact on review efficacy.; We find that OSS peer reviews are conducted asynchronously by empowered experts who focus on changes that are in their area of expertise. Reviewers provide timely, regular feedback on small changes. The descriptive statistics clearly show that OSS review is drastically different from traditional inspection. 2014 Copyright held by the Owner/Author.",Inspection; Mining software repositories; Open-source software; Peer review,Information dissemination; Inspection; Open systems; Quality assurance; Descriptive statistics; Industrial development; Mining software repositories; Open source software projects; Peer review; Open source software
Some code smells have a significant but small effect on faults,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907027182&doi=10.1145%2f2629648&partnerID=40&md5=d32739cff113c108bb4130fa1f5a48f0,"We investigate the relationship between faults and five of Fowler et al.'s least-studied smells in code: Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man. We developed a tool to detect these five smells in three open-source systems: Eclipse, ArgoUML, and Apache Commons. We collected fault data from the change and fault repositories of each system. We built Negative Binomial regression models to analyse the relationships between smells and faults and report the McFadden effect size of those relationships. Our results suggest that Switch Statements had no effect on faults in any of the three systems; Message Chains increased faults in two systems; Message Chains which occurred in larger files reduced faults; Data Clumps reduced faults in Apache and Eclipse but increased faults in ArgoUML; Middle Man reduced faults only in ArgoUML, and Speculative Generality reduced faults only in Eclipse. File size alone affects faults in some systems but not in all systems. Where smells did significantly affect faults, the size of that effect was small (always under 10 percent). Our findings suggest that some smells do indicate fault-prone code in some circumstances but that the effect that these smells have on faults is small. Our findings also show that smells have different effects on different systems. We conclude that arbitrary refactoring is unlikely to significantly reduce fault-proneness and in some cases may increase fault-proneness. © 2014 ACM.",Defects; Software code smells,Chains; Codes (symbols); Defects; Odors; Open source software; Open systems; Regression analysis; Different effects; Effect size; Fault proneness; Fault-prone codes; Negative binomial regression model; Open source system; Refactorings; Software codes; Data reduction
Editorial,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907029128&doi=10.1145%2f2656368&partnerID=40&md5=f5e2854245abccab194343f1abccb1ba,[No abstract available],,
Scaling up symbolic analysis by removing z-equivalent states,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907021009&doi=10.1145%2f2652484&partnerID=40&md5=b2efe6e3105ca82c8b5afa320f5e9b1f,"Path explosion is a major issue in applying path-sensitive symbolic analysis to large programs. We observe that many symbolic states generated by the symbolic analysis of a procedure are indistinguishable to its callers. It is, therefore, possible to keep only one state from each set of equivalent symbolic states without affecting the analysis result. Based on this observation, we propose an equivalence relation called z-equivalence, which is weaker than logical equivalence, to relate a large number of z-equivalent states. We prove that z-equivalence is strong enough to guarantee that paths to be traversed by the symbolic analysis of two z-equivalent states are identical, giving the same solutions to satisfiability and validity queries. We propose a sound linear algorithm to detect z-equivalence. Our experiments show that the symbolic analysis that leverages z-equivalence is able to achieve more than ten orders of magnitude reduction in terms of search space. The reduction significantly alleviates the path explosion problem, enabling us to apply symbolic analysis in large programs such as Hadoop and Linux Kernel. © 2014 ACM.",Path explosion; State equivalence detection; Symbolic analysis,Computer operating systems; Software engineering; Equivalence relations; Equivalent state; Linear algorithms; Logical equivalence; Orders of magnitude; Path explosions; State equivalences; Symbolic analysis; Equivalence classes
Introduction to the Special Issue International Conference on Software Engineering (ICSE 2012),2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907033519&doi=10.1145%2f2658849&partnerID=40&md5=c16f944e3adbbae6a59b8c523e958ce0,[No abstract available],,
On the comprehension of program comprehension,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907033652&doi=10.1145%2f2622669&partnerID=40&md5=0405f62141022f546c194282bb18db5a,"Research in program comprehension has evolved considerably over the past decades. However, only little is known about how developers practice program comprehension in their daily work. This article reports on qualitative and quantitative research to comprehend the strategies, tools, and knowledge used for program comprehension.We observed 28 professional developers, focusing on their comprehension behavior, strategies followed, and tools used. In an online survey with 1,477 respondents, we analyzed the importance of certain types of knowledge for comprehension and where developers typically access and share this knowledge.; We found that developers follow pragmatic comprehension strategies depending on context. They try to avoid comprehension whenever possible and often put themselves in the role of users by inspecting graphical interfaces. Participants confirmed that standards, experience, and personal communication facilitate comprehension. The team size, its distribution, and open-source experience influence their knowledge sharing and access behavior. While face-to-face communication is preferred for accessing knowledge, knowledge is frequently shared in informal comments.; Our results reveal a gap between research and practice, as we did not observe any use of comprehension tools and developers seem to be unaware of them. Overall, our findings call for reconsidering the research agendas towards context-aware tool support. © 2014 ACM.",Context-aware software engineering; Empirical software engineering; Information needs; Knowledge sharing; Program comprehension,Computer programming; Information management; Knowledge management; Open source software; Surveys; Context-Aware; Empirical Software Engineering; Information needs; Knowledge-sharing; Program comprehension; Software engineering
Automated detection of client-state manipulation vulnerabilities,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907033664&doi=10.1145%2f2531921&partnerID=40&md5=dfcb9899590be9b5d570107dc9d5b9df,"Web application programmers must be aware of a wide range of potential security risks. Although the most common pitfalls are well described and categorized in the literature, it remains a challenging task to ensure that all guidelines are followed. For this reason, it is desirable to construct automated tools that can assist the programmers in the application development process by detecting weaknesses. Many vulnerabilities are related to Web application code that stores references to application state in the generated HTML documents to work around the statelessness of the HTTP protocol. In this article, we show that such clientstate manipulation vulnerabilities are amenable to tool-supported detection.; We present a static analysis for the widely used frameworks Java Servlets, JSP, and Struts. Given a Web application archive as input, the analysis identifies occurrences of client state and infers the information flow between the client state and the shared application state on the server. This makes it possible to check how client-state manipulation performed by malicious users may affect the shared application state and cause leakage or modifications of sensitive information. The warnings produced by the tool help the application programmer identify vulnerabilities before deployment. The inferred information can also be applied to configure a security filter that automatically guards against attacks at runtime. Experiments on a collection of open-source Web applications indicate that the static analysis is able to effectively help the programmer prevent client-state manipulation vulnerabilities. The analysis detects a total of 4,802 client-state parameters in ten applications, whereof 4,437 are classified as safe and 241 reveal exploitable vulnerabilities. © 2014 ACM.",Information flow analysis; Static analysis; Web application security,HTTP; Hypertext systems; Network security; Open source software; Application development process; Application programmers; Automated detection; Information flow analysis; Information flows; Sensitive informations; Shared applications; Web application security; Static analysis
Automated cookie collection testing,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894378998&doi=10.1145%2f2559936&partnerID=40&md5=6acf2ac5d043ed204afc76e073fa301b,"Cookies are used by over 80% of Web applications utilizing dynamic Web application frameworks. Applications deploying cookies must be rigorously verified to ensure that the application is robust and secure. Given the intense time-to-market pressures faced by modern Web applications, testing strategies that are low cost and automatable are required. The HTTP state management mechanism, colloquially referred to as the cookie, is used by two-thirds of Web sites and by over 80% of Web applications utilizing dynamic Web application frameworks. Cookies are a staple of modern Web applications providing a framework upon which state-based sessions are maintained across the Internet. There are a wide range of automated test generation strategies established within the literature, each possessing strengths and weaknesses. Recently, a number of novel strategies termed Adaptive Random Testing (ART) have been demonstrated to significantly increase the effectiveness of random testing.",Adaptive random testing; Automated testing; Cookies; Software testing; Test generation; Test strategies; Web application testing,
Improving software modularization via automated analysis of latent topics and dependencies,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894384748&doi=10.1145%2f2559935&partnerID=40&md5=1e3e66d09047b3a04977619470cbaf5f,"The article examines how to improve software modularization via automated analysis of latent topics and dependencies. To alleviate this issue, we propose an automated approach to help developers improve the quality of software modularization. Our approach analyzes underlying latent topics in source code as well as structural dependencies to recommend (and explain) refactoring operations aiming at moving a class to a more suitable package. The topics are acquired via Relational Topic Models (RTM), a probabilistic topic modeling technique. The resulting tool, coined as R3 (Rational Refactoring via RTM), has been evaluated in two empirical studies. The proposed approach analyzes underlying latent topics in classes and packages and uses structural dependencies to recommend refactoring operations aiming at moving classes to more suitable packages.",Empirical Studies; Recommendation system; Refactoring; Relational topic modeling; Software modularization,
Join point interfaces for safe and flexible decoupling of aspects,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894340809&doi=10.1145%2f2559933&partnerID=40&md5=2b35f58590c29961855d6580f9093f12,"The introduces a novel abstraction called Join Point Interface, which, by design, aids modular reasoning and independent evolution by decoupling aspects from base code and by providing a modular type-checking algorithm. Join point interfaces can be used both with implicit announcement through pointcuts, and with explicit announcement, using closure join points. Join point interfaces further offer polymorphic dispatch on join points, with an advice-dispatch semantics akin to multi-methods. Aspect-Oriented Programming (AOP) is a paradigm that aims at enhancing modularity of software by locally handling crosscutting concerns that would otherwise be scattered in different parts of a given system. The emblematic language mechanism of aspect-oriented programming languages is pointcuts and advice, where pointcuts are predicates that denote join points in the execution of a program where advice is executed.",Advice dispatch; Aspect-oriented programming; Explicit announcement; Implicit Announcement; Interfaces; Join point polymorphism; Modularity; Typing,
Model-based synthesis of control software from system-level formal specifications,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894302917&doi=10.1145%2f2559934&partnerID=40&md5=02552a631413410afcbc39ee91e142c9,"Many embedded systems are indeed software-based control systems, that is, control systems whose controller consists of control software running on a microcontroller device. This motivates investigation on formal model-based design approaches for automatic synthesis of embedded systems control software. To stabilize an underactuated inverted pendulum from the hanging position to the upright position, a controller needs to find a nonobvious strategy that consists of swinging the pendulum once or more times to gain enough momentum. The column labeled Quantization denotes that the row supplies the quantization process. The group of columns labeled K lists the output controller characteristics researchers are interested in. Finally, the group of columns labeled Impl considers implementation issues, namely if a method is fully automatic or semi-automatic, and if there exists a tool available implementing the presented method.",Correct-by-construction control software synthesis; Hybrid systems; Model-based design of control software,
Do we need to handle every temporal violation in scientific workflow systems?,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894381581&doi=10.1145%2f2559938&partnerID=40&md5=ab9b83f68a4faeb45d498713b915d697,"Scientific processes are usually time constrained with overall deadlines and local milestones. In scientific workflow systems, due to the dynamic nature of the underlying computing infrastructures such as grid and cloud, execution delays often take place and result in a large number of temporal violations. Temporal violation handling is to execute violation handling strategies which can compensate for the occurring time deficit but would impose some additional cost. Generally speaking, the two fundamental requirements for delivering satisfactory temporal QoS in scientific workflow systems are temporal conformance and cost effectiveness. Every task for workflow temporal management incurs some cost. Take a single temporal violation handling as an example, its cost can be primarily referred to monetary costs and time overheads of violation handling strategies which are normally nontrivial in scientific workflow systems.",Quality of service; Scientific workflows; Temporal constraints; Temporal verification; Violation handling point selection,
Editorial,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894366908&doi=10.1145%2f2559939&partnerID=40&md5=e263af502760fb86d4a841823a3057c1,[No abstract available],,
Prevalence of coincidental correctness and mitigation of its impact on fault localization,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894309597&doi=10.1145%2f2559932&partnerID=40&md5=6b2de00a7bc297360de672b00d031ce2,"The article examines the prevalence of coincidental correctness and mitigation of its impact on fault localization. Coverage-Based Fault Localization (CBFL) techniques seek to identify failure correlated program elements using test suites in which tests are tagged as failing or passing, that is, elements that are induced by all (or most) failing runs and not induced by any (or most) passing runs; and locate the faulty code using some examination strategy. To evaluate the effectiveness of research techniques, researchers empirically quantified their accuracy in identifying CC tests in test suites involving programs with seeded faults as well as real faults. The results were promising, for example, the better performing technique, using 105 test suites and statement coverage, exhibited 9% false negatives, 30% false positives, and no false negatives nor false positives in 14.3% of the test suites.",Cluster analysis; Coincidental correctness; Coverage-based fault localization; Fuzzy sets; Strong coincidental Correctness; Weak coincidental correctness,
An in-depth study of the potentially confounding effect of class size in fault prediction,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894338320&doi=10.1145%2f2556777&partnerID=40&md5=e5a5346a3b9242058bd01cbb7e28d2a2,"The article offers an in-depth understanding of the effect of class size on the true associations between object-oriented metrics and fault-proneness. The third variable that distorts the true association between the independent and dependent variables is usually called a confounding variable or a confounder. The distortion that results from confounding may lead to overestimation or underestimation of an association, depending on the direction and magnitude of the relations that the confounding variable has with the independent and dependent variables. The reasons are twofold. First, logistic regression is the most commonly used and well-understood modeling technique in the context of fault prediction. Second, a recent systematic review suggests that, compared to other complex modeling techniques, logistic regression performs well when predicting fault-proneness.",Class size; Confounding effect; Fault; Metrics; Prediction,
Architecture-centric support for adaptive service collaborations,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894343614&doi=10.1145%2f2559937&partnerID=40&md5=33740c28d68ca2bae672414e8b68a553,"Collaboration between information systems, both within and across company borders, has become essential to success. An efficient supply chain, for example, requires the collaboration of distributed and heterogeneous systems of multiple companies. Developing such collaborative applications and building the supporting information systems poses several engineering challenges. A key challenge is to manage the ever-growing design complexity. State-of-the-art solutions, however, often lack proper abstractions for modeling collaborations at architectural level or do not reify these abstractions at detailed design and implementation level. Developers, on the other hand, rely on middleware, business process management, and Web services, techniques that mainly focus on low-level infrastructure.",Agent organizations; Architecturalviews; Collaborative systems; Empirical evaluation; Middleware; Role-based modeling; Service-oriented architecture (SOA); Softwarearchitecture; Web services,
Traceability and sysml design slices to support safety inspections: A controlled experiment,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894310312&doi=10.1145%2f2559978&partnerID=40&md5=98934b27d0c9060e0b92aec8b0f3e3af,"Certifying safety-critical software and ensuring its safety requires checking the conformance between safety requirements and design. Increasingly, the development of safety-critical software relies on modeling, and the System Modeling Language (SysML) is now commonly used in many industry sectors. Inspecting safety conformance by comparing design models against safety requirements requires safety inspectors to browse through large models and is consequently time consuming and error-prone. An important concern in relation to traceability is cost effectiveness. Making traceability cost effective requires a careful analysis of the trade-offs between the costs incurred over establishing and maintaining traceability links and the benefits that traceability offers. Traceability is considered worthwhile if it presents a significant advantage for achieving certain goals. In our case, the goals pursued from traceability are to increase the correctness and decrease the effort associated with design safety inspections.",Design; Empirical software engineering; Requirements Specification; Software and system safety; Software/program verification,
Generating test cases for programs that are coded against interfaces and annotations,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901589368&doi=10.1145%2f2544135&partnerID=40&md5=a06e28719ed5b210dde21b98cd5e59bd,"Automatic test case generation for software programs is very powerful but suffers from a key limitation. That is, most current test case generation techniques fail to cover testee code when covering that code requires additional pieces of code not yet part of the program under test. To address some of these cases, the Pex state-of-The-art test case generator can generate basic mock code. However, current test case generators cannot handle cases in which the code under test uses multiple interfaces, annotations, or reflection. To cover such code in an object-oriented setting, we describe a novel technique for generating test cases and mock classes. The technique consists of collecting constraints on interfaces, annotations, and reflection, combining them with program constraints collected during dynamic symbolic execution, encoding them in a constraint system, solving them with an off-The-shelf constraint solver, and mapping constraint solutions to test cases and custom mock classes.We demonstrate the value of this technique on open-source applications. Our approach covered such third-party code with generated mock classes, while competing approaches failed to cover the code and sometimes produced unintended side-effects such as filling the screen with dialog boxes and writing into the file system. © 2014 ACM.",Dynamic symbolic execution; Mock classes; Stubs; Test case generation,Application programs; Codes (symbols); Open source software; Automatic test-case generations; Dynamic symbolic executions; Mock classes; Multiple interfaces; Program constraints; Stubs; Test case generation; Test case generators; Software testing
Architecture-level configuration of large-scale embedded software systems,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901610601&doi=10.1145%2f2581376&partnerID=40&md5=04c73c9c97ae7d754bab9ba61a8bec92,"Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions. © 2014 ACM.",Consistent configuration; Constraint satisfaction techniques; Formal specification; Model-based product-line engineering; Product configuration; UML/OCL,Specifications; Consistent configuration; Constraint satisfaction techniques; Formal Specification; Product configuration; Product line engineering; Uml/ocl; Intelligent control
"GreASE: A tool for efficient ""Nonequivalence"" checking",2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901634483&doi=10.1145%2f2560563&partnerID=40&md5=359800edeae0b11bd9d8736fa6ce0ae5,"Equivalence checking plays a crucial role in formal verification to ensure the correctness of concurrent systems. However, this method cannot be scaled as easily with the increasing complexity of systems due to the state explosion problem. This article presents an efficient procedure, based on heuristic search, for checking Milner's strong and weak equivalence; to achieve higher efficiency, we actually search for a difference between two processes to be discovered as soon as possible, thus the heuristics aims to find a counterexample, even if not the minimum one, to prove nonequivalence. The presented algorithm builds the system state graph on-The-fly, during the checking, and the heuristics promotes the construction of the more promising subgraph. The heuristic function is syntax based, but the approach can be applied to different specification languages such as CCS, LOTOS, and CSP, provided that the language semantics is based on the concept of transition. The algorithm to explore the search space of the problem is based on a greedy technique; GreASE (Greedy Algorithm for System Equivalence), the tool supporting the approach, is used to evaluate the achieved reduction of both state-space size and time with respect to other verification environments. © 2014 ACM.",Equivalence checking; Formal methods; Heuristic searches,Formal methods; Heuristic algorithms; Semantics; Specification languages; Equivalence checking; Formal verifications; Heuristic functions; Heuristic search; Language semantics; State explosion problems; System equivalence; Verification environment; Equivalence classes
Solving the search for source code,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901636097&doi=10.1145%2f2581377&partnerID=40&md5=ff534667cb6865694255fc7807d724a5,"Programmers frequently search for source code to reuse using keyword searches. The search effectiveness in facilitating reuse, however, depends on the programmer's ability to specify a query that captures how the desired code may have been implemented. Further, the results often include many irrelevant matches that must be filtered manually. More semantic search approaches could address these limitations, yet existing approaches are either not flexible enough to find approximate matches or require the programmer to define complex specifications as queries. We propose a novel approach to semantic code search that addresses several of these limitations and is designed for queries that can be described using a concrete input/output example. In this approach, programmers write lightweight specifications as inputs and expected output examples. Unlike existing approaches to semantic search, we use an SMT solver to identify programs or program fragments in a repository, which have been automatically transformed into constraints using symbolic analysis, that match the programmer-provided specification. We instantiated and evaluated this approach in subsets of three languages, the Java String library, Yahoo! Pipes mashup language, and SQL select statements, exploring its generality, utility, and trade-offs. The results indicate that this approach is effective at finding relevant code, can be used on its own or to filter results from keyword searches to increase search precision, and is adaptable to find approximate matches and then guide modifications to match the user specifications when exact matches do not already exist. These gains in precision and flexibility come at the cost of performance, for which underlying factors and mitigation strategies are identified. © 2014 ACM.",Lightweight specification; Semantic code search; SMT solvers; Symbolic analysis,Codes (symbols); Computer programming; Semantics; Specifications; Approximate matches; Mitigation strategy; Program fragments; Semantic codes; Semantic search; Smt solvers; Symbolic analysis; Underlying factors; Java programming language
Scalable runtime bloat detection using abstract dynamic slicing,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901596638&doi=10.1145%2f2560047&partnerID=40&md5=1b5bb7ba2f9812e0bcc3e4f4e82d9833,"Many large-scale Java applications suffer from runtime bloat. They execute large volumes of methods and create many temporary objects, all to execute relatively simple operations. There are large opportunities for performance optimizations in these applications, but most are being missed by existing optimization and tooling technology. While JIT optimizations struggle for a few percent improvement, performance experts analyze deployed applications and regularly find gains of 2×or more. Finding such big gains is difficult, for both humans and compilers, because of the diffuse nature of runtime bloat. Time is spread thinly across calling contexts, making it difficult to judge how to improve performance. Our experience shows that, in order to identify large performance bottlenecks in a program, it is more important to understand its dynamic dataflow than traditional performance metrics, such as running time. This article presents a general framework for designing and implementing scalable analysis algorithms to find causes of bloat in Java programs. At the heart of this framework is a generalized form of runtime dependence graph computed by abstract dynamic slicing, a semantics-aware technique that achieves high scalability by performing dynamic slicing over bounded abstract domains. The framework is instantiated to create two independent dynamic analyses, copy profiling and cost-benefit analysis, that help programmers identify performance bottlenecks by identifying, respectively, high-volume copy activities and data structures that have high construction cost but low benefit for the forward execution. We have successfully applied these analyses to large-scale and long-running Java applications. We show that both analyses are effective at detecting inefficient operations that can be optimized for better performance. We also demonstrate that the general framework is flexible enough to be instantiated for dynamic analyses in a variety of application domains. © 2014 ACM.",Abstract dynamic slicing; Copy profiling; Cost-benefit analysis; Dynamic analysis; Runtime bloat,Abstracting; Computer software; Cost benefit analysis; Dynamic analysis; Semantics; Better performance; Deployed applications; Dynamic slicing; Improve performance; Performance bottlenecks; Performance metrics; Performance optimizations; Runtimes; Java programming language
A methodology for exposing risk in achieving emergent system properties,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901590811&doi=10.1145%2f2560048&partnerID=40&md5=43fa173fc1137f27b937829ea16eecbd,"Determining whether systems achieve desired emergent properties, such as safety or reliability, requires an analysis of the system as a whole, often in later development stages when changes are difficult and costly to implement. In this article we propose the Process Risk Indicator (PRI) methodology for analyzing and evaluating emergent properties early in the development cycle. A fundamental assumption of system engineering is that risk mitigation processes reduce system risks, yet these processes may also be a source of risk: (1) processes may not be appropriate for achieving the desired emergent property; or (2) processes may not be followed appropriately. PRI analyzes development process artifacts (e.g., designs pertaining to reliability or safety analysis reports) to quantify process risks that may lead to higher system risk.We applied PRI to the hazard analysis processes of a network-centric, Department of Defense system-of-systems and two NASA spaceflight projects to assess the risk of not achieving one such emergent property, software safety, during the early stages of the development lifecycle. The PRI methodology was used to create measurement baselines for process indicators of software safety risk, to identify risks in the hazard analysis process, and to provide feedback to projects for reducing these risks. © 2014 ACM.",Process risk; Risk measurement; Software safety,Computer software selection and evaluation; Hazards; NASA; Reliability analysis; Risk perception; Department of Defense; Development process; Development stages; Process indicators; Process risks; Risk measurement; Safety analysis reports; Software safety; Risk assessment
Required behavior of sequence diagrams: Semantics and conformance,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898001358&doi=10.1145%2f2523108&partnerID=40&md5=79cabe5e39efc50b4e74eb863e505a1f,"Many reusable software artifacts such as design patterns and design aspects make use of UML sequence diagrams to describe interaction behaviors. When a pattern or an aspect is reused in an application, it is important to ensure that the sequence diagrams for the application conform to the corresponding sequence diagrams for the pattern or aspect. Reasoning about conformance relationship between sequence diagrams has not been addressed adequately in literature. In this article, we focus on required behaviors specified by a UML sequence diagram and provide a semantic-based formalization of conformance relationships between sequence diagrams. A novel trace semantics is first given that captures precisely required behaviors. A refinement relation between sequence diagrams is then defined based on the semantics. The refinement relation allows a sequence diagram to be refined by changing its structure so long as its required behaviors are preserved. A conformance relation between sequence diagrams is finally given that includes the refinement relation as a special case. It allows one to introduce and rename lifelines, messages, and system variables when reusing sequence diagrams. Properties of the semantics, refinement, and conformance relations are studied. Two case studies are provided to illustrate the efficacy of semantic-based conformance reasoning. © 2014 ACM.",Conformance; Design aspect; Design pattern; Refinement; Required behavior; Semantics; Sequence diagram; Trace semantics,Computer software reusability; Semantics; Conformance; Design Patterns; Refinement; Required behavior; Sequence diagrams; Trace semantics; Graphic methods
ADAM: External dependency-driven architecture discovery and analysis of quality attributes,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897988189&doi=10.1145%2f2529998&partnerID=40&md5=beb014b796c4eaf226fab4958cd6d66e,"This article introduces the Architecture Discovery and Analysis Method (ADAM). ADAMsupports the discovery ofmodule and runtime views as well as the analysis of quality attributes, such as testability, performance, andmaintainability, of software systems. The premise of ADAM is that the implementation constructs, architecture constructs, concerns, and quality attributes are all influenced by the external entities (e.g., libraries, frameworks, COTS software) used by the system under analysis. The analysis uses such external dependencies to identify, classify, and review a minimal set of key source-code files supported by a knowledge base of the external entities. Given the benefits of analyzing external dependencies as a way to discover architectures and potential risks, it is demonstrated that dependencies to external entities are useful not only for architecture discovery but also for analysis of quality attributes. ADAM is evaluated using the NASA's Space Network Access System (SNAS). The results show that this method offers systematic guidelines for discovering the architecture and locating potential risks (e.g., low testability and decreased performance) that are hidden deep inside the system implementation. Some generally applicable lessons for developers and analysts, as well as threats to validity are also discussed. © 2014 ACM.",Concerns; External entities; Knowledge base; Maintainability; Module and runtime views; Quality; Reverse engineering; Software architecture; Testability,Image quality; Knowledge based systems; Maintainability; NASA; Network architecture; Reverse engineering; Risk assessment; Software architecture; Concerns; External entities; Knowledge base; Runtimes; Testability; Quality control
On the impact of UML analysis models on source-code comprehensibility and modifiability,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897988356&doi=10.1145%2f2491912&partnerID=40&md5=9420803c0b797396523f364fb36f4767,We carried out a family of experiments to investigate whether the use of UML models produced in the requirements analysis process helps in the comprehensibility and modifiability of source code. The family consists of a controlled experiment and 3 external replications carried out with students and professionals from Italy and Spain. 86 participants with different abilities and levels of experience with UML took part. The results of the experiments were integrated through the use of meta-analysis. The results of both the individual experiments and meta-analysis indicate that UML models produced in the requirements analysis process influence neither the comprehensibility of source code nor its modifiability. © 2014 ACM.,Analysis models; Comprehensibility; Controlled experiment; Family of experiments; Maintenance; Modifiability; Replicated experiments; UML,Maintenance; Requirements engineering; Unified Modeling Language; Analysis models; Comprehensibility; Controlled experiment; Modifiability; Replicated experiment; UML; Experiments
Guided test generation for database applications via synthesized database interactions,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897995944&doi=10.1145%2f2491529&partnerID=40&md5=a9b1b213fbcb3f5e653f624e63f43e7e,"Testing database applications typically requires the generation of tests consisting of both program inputs and database states. Recently, a testing technique called Dynamic Symbolic Execution (DSE) has been proposed to reduce manual effort in test generation for software applications. However, applying DSE to generate tests for database applications faces various technical challenges. For example, the database application under test needs to physically connect to the associated database, which may not be available for various reasons. The program inputs whose values are used to form the executed queries are not treated symbolically, posing difficulties for generating valid database states or appropriate database states for achieving high coverage of query-result-manipulation code. To address these challenges, in this article, we propose an approach called SynDB that synthesizes new database interactions to replace the original ones from the database application under test. In this way, we bridge various constraints within a database application: query-construction constraints, query constraints, database schema constraints, and query-result-manipulation constraints.We then apply a state-of-the-art DSE engine called Pex for .NET from Microsoft Research to generate both program inputs and database states. The evaluation results show that tests generated by our approach can achieve higher code coverage than existing test generation approaches for database applications. © 2014 ACM.",Automatic test generation; Database application testing; Dynamic symbolic execution; Synthesized database interactions,Application programs; Bridges; Query languages; Software testing; Testing; Automatic test generation; Database applications; Database interactions; Dynamic symbolic executions; Evaluation results; Microsoft researches; Software applications; Technical challenges; Query processing
Editorial,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897985272&doi=10.1145%2f2581373&partnerID=40&md5=81a31e16b1922c156c6106d285ec7668,[No abstract available],,
Dynamite: A tool for the verification of alloy models based on PVS,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897969341&doi=10.1145%2f2544136&partnerID=40&md5=25e5a61fff0014e732eca43bf1207e59,"Automatic analysis of Alloy models is supported by the Alloy Analyzer, a tool that translates an Alloy model to a propositional formula that is then analyzed using off-the-shelf SAT solvers. The translation requires user-provided bounds on the sizes of data domains. The analysis is limited by the bounds and is therefore partial. Thus, the Alloy Analyzer may not be appropriate for the analysis of critical applications where more conclusive results are necessary. Dynamite is an extension of PVS that embeds a complete calculus for Alloy. It also includes extensions to PVS that allow one to improve the proof effort by, for instance, automatically analyzing new hypotheses with the aid of the Alloy Analyzer. Since PVS sequents may get cluttered with unnecessary formulas, we use the Alloy unsat-core extraction feature in order to refine proof sequents. An internalization of Alloy's syntax as an Alloy specification allows us to use the Alloy Analyzer for producing witnesses for proving existentially quantified formulas. Dynamite complements the partial automatic analysis offered by the Alloy Analyzer with semi-automatic verification through theorem proving. It also improves the theorem proving experience by using the Alloy Analyzer for early error detection, sequent refinement, and witness generation. © 2014 ACM.",Alloy; Alloy calculus; PVS; Unsat-cores,Alloying; Calculations; Formal logic; Theorem proving; Tools; Alloy analyzers; Automatic analysis; Critical applications; Data domains; Propositional formulas; PVS; Semi-automatics; Unsat-cores; Alloys
Degree-of-knowledge: Modeling a developer's knowledge of code,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897976176&doi=10.1145%2f2512207&partnerID=40&md5=b06900829a5be17a677d777fc6190a05,"As a software system evolves, the system's codebase constantly changes, making it difficult for developers to answer such questions as who is knowledgeable about particular parts of the code or who needs to know about changes made. In this article, we show that an externalized model of a developer's individual knowledge of code can make it easier for developers to answer such questions. We introduce a degree-of-knowledge model that computes automatically, for each source-code element in a codebase, a real value that represents a developer's knowledge of that element based on a developer's authorship and interaction data. We present evidence that shows that both authorship and interaction data of the code are important in characterizing a developer's knowledge of code. We report on the usage of our model in case studies on expert finding, knowledge transfer, and identifying changes of interest. We show that our model improves upon an existing expertise-finding approach and can accurately identify changes for which a developer should likely be aware. We discuss how our model may provide a starting point for knowledge transfer but that more refinement is needed. Finally, we discuss the robustness of the model across multiple development sites. © 2014 ACM.",Authorship; Degree-of-interest; Degree-of-knowledge; Development environment; Expertise; Onboarding; Recommendation,Knowledge management; Authorship; Degree of interests; Degree-of-knowledge; Development environment; Expertise; Onboarding; Recommendation; Codes (symbols)
A taxonomy for requirements engineering and software test alignment,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897975506&doi=10.1145%2f2523088&partnerID=40&md5=c1114efca676af9895074fd77bceb0ea,"Requirements Engineering and Software Testing are mature areas and have seen a lot of research. Nevertheless, their interactions have been sparsely explored beyond the concept of traceability. To fill this gap, we propose a definition of requirements engineering and software test (REST) alignment, a taxonomy that characterizes the methods linking the respective areas, and a process to assess alignment. The taxonomy can support researchers to identify new opportunities for investigation, as well as practitioners to compare alignment methods and evaluate alignment, or lack thereof. We constructed the REST taxonomy by analyzing alignment methods published in literature, iteratively validating the emerging dimensions. The resulting concept of an information dyad characterizes the exchange of information required for any alignment to take place. We demonstrate use of the taxonomy by applying it on five in-depth cases and illustrate angles of analysis on a set of thirteen alignment methods. In addition, we developed an assessment framework (REST-bench), applied it in an industrial assessment, and showed that it, with a low effort, can identify opportunities to improve REST alignment. Although we expect that the taxonomy can be further refined, we believe that the information dyad is a valid and useful construct to understand alignment. © 2014 ACM.",Alignment; Software process assessment; Software testing; Taxonomy,Alignment; Iterative methods; Requirements engineering; Software testing; Alignment methods; Exchange of information; Software process assessment; Taxonomies
Key factors for adopting Inner Source,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897993676&doi=10.1145%2f2533685&partnerID=40&md5=8c758a0b1877cf28f0c0395c91472e2a,"A number of organizations have adopted Open Source Software (OSS) development practices to support or augment their software development processes, a phenomenon frequently referred to as Inner Source. However the adoption of Inner Source is not a straightforward issue. Many organizations are struggling with the question of whether Inner Source is an appropriate approach to software development for them in the first place. This article presents a framework derived from the literature on Inner Source, which identifies nine important factors that need to be considered when implementing Inner Source. The framework can be used as a probing instrument to assess an organization on these nine factors so as to gain an understanding of whether or not Inner Source is suitable.We applied the framework in three case studies at Philips Healthcare, Neopost Technologies, and Rolls-Royce, which are all large organizations that have either adopted Inner Source or were planning to do so. Based on the results presented in this article, we outline directions for future research. © 2014 ACM.",Case study; Framework; Inner source; Open-source development practices,Research; Software engineering; Framework; Inner source; Large organizations; Open source development; Open source software development; Philips; Rolls-royce; Software development process; Societies and institutions
Exact scalable sensitivity analysis for the next release problem,2014,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898015922&doi=10.1145%2f2537853&partnerID=40&md5=f8d752888459dd6a72d3c5744d48884e,"The nature of the requirements analysis problem, based as it is on uncertain and often inaccurate estimates of costs and effort, makes sensitivity analysis important. Sensitivity analysis allows the decision maker to identify those requirements and budgets that are particularly sensitive to misestimation. However, finding scalable sensitivity analysis techniques is not easy because the underlying optimization problem is NP-hard. This article introduces an approach to sensitivity analysis based on exact optimization.We implemented this approach as a tool, OATSAC, which allowed us to experimentally evaluate the scalability and applicability of Requirements Sensitivity Analysis (RSA). Our results show that OATSAC scales sufficiently well for practical applications in Requirements Sensitivity Analysis. We also show how the sensitivity analysis can yield insights into difficult and otherwise obscure interactions between budgets, requirements costs, and estimate inaccuracies using a real-world case study. © 2014 ACM.",Next release problem; Requirement engineering; Sensitivity analysis,Budget control; Cost benefit analysis; Cost estimating; Decision makers; Next release problems; NP-hard; Optimization problems; Real-world; Requirement engineering; Requirements analysis; Sensitivity analysis techniques; Sensitivity analysis
Exception handlers for healing component-based systems,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969790792&doi=10.1145%2f2522920.2522923&partnerID=40&md5=bf70b7b47e707c604fdc4b9e653b1a6a,"To design effective exception handlers, developers must predict at design time the exceptional events that may occur at runtime, and must implement the corresponding handlers on the basis of their predictions. Designing exception handlers for component-based software systems is particularly difficult because the information required to build handlers is distributed between component and application developers. Component developers know the internal details of the components but ignore the applications, while application developers own the applications but cannot access the details required to implement handlers in components. This article addresses the problem of automatically healing the infield failures that are caused by faulty integration of OTS components. In the article, we propose a technique and a methodology to decouple the tasks of component and application developers, who will be able to share information asynchronously and independently, and communicate implicitly by developing and deploying what we call healing connectors. Component developers implement healing connectors on the basis of information about the integration problems frequently experienced by application developers. Application developers easily and safely install healing connectors in their applications without knowing the internal details of the connectors. Healing connectors heal failures activated by exceptions raised in the OTS components actually deployed in the system. The article defines healing connectors, introduces amethodology to develop and deploy healing connectors, and presents several case studies that indicate that healing connectors are effective, reusable and efficient. © 2013 ACM.",Component-based software engineering; Cots components; Exception handling in component-based software systems; Healing connectors; Healing patterns; Self-healing,Computer software; Computer software reusability; Software engineering; Software packages; Component-based software engineering; Component-based software systems; COTS component; Healing patterns; Self-healing; Application programs
A Methodology for testing CPU emulators,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886477286&doi=10.1145%2f2522920.2522922&partnerID=40&md5=a07c153c5a8bdd1a45896f0e39007b17,"A CPU emulator is a software system that simulates a hardware CPU. Emulators are widely used by computer scientists for various kind of activities (e.g., debugging, profiling, and malware analysis). Although no theoretical limitation prevents developing an emulator that faithfully emulates a physical CPU, writing a fully featured emulator is a very challenging and error prone task. Modern CISC architectures have a very rich instruction set, some instructions lack proper specifications, and others may have undefined effects in corner cases. This article presents a testing methodology specific for CPU emulators, based on fuzzing. The emulator is ""stressed"" with specially crafted test cases, to verify whether the CPU is properly emulated or not. Improper behaviors of the emulator are detected by running the same test case concurrently on the emulated and on the physical CPUs and by comparing the state of the two after the execution. Differences in the final state testify defects in the code of the emulator. We implemented this methodology in a prototype (named as EmuFuzzer), analyzed five state-of-the-art IA-32 emulators (QEMU, Valgrind, Pin, BOCHS, and JPC), and found several defects in each of them, some of which can prevent proper execution of programs. © 2013 ACM.",Automatic test generation; Emulation; Fuzzing; Software testing,Program processors; Software testing; Automatic test generation; Computer scientists; Emulation; Error prone tasks; Fuzzing; Malware analysis; Software systems; Testing methodology; Defects
Software effort estimation as a multiobjective learning problem,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884946446&doi=10.1145%2f2522920.2522928&partnerID=40&md5=798b2a715c8c74332266007ab4bb7c95,"Ensembles of learning machines are promising for software effort estimation (SEE), but need to be tailored for this task to have their potential exploited. A key issue when creating ensembles is to produce diverse and accurate base models. Depending on how differently different performance measures behave for SEE, they could be used as a natural way of creating SEE ensembles. We propose to view SEE model creation as a multiobjective learning problem. A multiobjective evolutionary algorithm (MOEA) is used to better understand the tradeoff among different performancemeasures by creating SEE models through the simultaneous optimisation of these measures.We show that the performance measures behave very differently, presenting sometimes even opposite trends. They are then used as a source of diversity for creating SEE ensembles. A good tradeoff among different measures can be obtained by using an ensemble of MOEA solutions. This ensemble performs similarly or better than a model that does not consider these measures explicitly. Besides, MOEA is also flexible, allowing emphasis of a particular measure if desired. In conclusion, MOEA can be used to better understand the relationship among performance measures and has shown to be very effective in creating SEE models. © 2013 ACM.",Ensembles of learning machines; Multiobjective evolutionary algorithms; Software effort estimation,Learning systems; Base models; Ensembles of learning machines; Model creation; Multi objective evolutionary algorithms; Multi-objective learning; Optimisations; Performance measure; Software effort estimation; Evolutionary algorithms
Evaluating A query framework for software evolution data,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886570561&doi=10.1145%2f2522920.2522931&partnerID=40&md5=5ab4f65d10b650cb318750874211e3ab,"With the steady advances in tooling to support software engineering, mastering all the features of modern IDEs, version control systems, and project trackers is becoming increasingly difficult. Answering even the most common developer questions can be surprisingly tedious and difficult. In this article we present a user study with 35 subjects to evaluate our quasi-natural language interface that provides access to various facets of the evolution of a software system but requires almost zero learning effort. Our approach is tightly woven into the Eclipse IDE and allows developers to answer questions related to source code, development history, or bug and issue management. The results of our evaluation show that our query interface can outperform classical software engineering tools in terms of correctness, while yielding significant time savings to its users and greatly advancing the state of the art in terms of usability and learnability. © 2013 ACM.",Conceptual queries; Natural language; Semantic web; Software evolution; Softwaremaintenance; Source-code analysis; Tool support,Semantic Web; Software engineering; Tools; Conceptual queries; Natural languages; Software Evolution; Softwaremaintenance; Source-code analysis; Tool support; Natural language processing systems
Path-and index-sensitive string analysis based on monadic second-order logic,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886461926&doi=10.1145%2f2522920.2522926&partnerID=40&md5=fd9dadba415b0122a26a61df9da6d60d,"We propose a novel technique for statically verifying the strings generated by a program. The verification is conducted by encoding the program in Monadic Second-order Logic (M2L). We use M2L to describe constraints among program variables and to abstract built-in string operations. Once we encode a program in M2L, a theorem prover for M2L, such as MONA, can automatically check if a string generated by the program satisfies a given specification, and if not, exhibit a counterexample. With this approach, we can naturally encode relationships among strings, accounting also for cases in which a program manipulates strings using indices. In addition, our string analysis is path sensitive in that it accounts for the effects of string and Boolean comparisons, as well as regular-expression matches. We have implemented our string analysis algorithm, and used it to augment an industrial security analysis for Web applications by automatically detecting and verifying sanitizers-methods that eliminate malicious patterns from untrusted strings, making these strings safe to use in security-sensitive operations. On the 8 benchmarks we analyzed, our string analyzer discovered 128 previously unknown sanitizers, compared to 71 sanitizers detected by a previously presented string analysis. © 2013 ACM.",Static program analysis; String analysis; Web security,Security systems; Theorem proving; Monadic second-order logic; Novel techniques; Path sensitives; Program variables; Security analysis; Static program analysis; String analysis; WEB security; Encoding (symbols)
A theoretical analysis of the risk evaluation formulas for spectrum-based fault localization,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886517134&doi=10.1145%2f2522920.2522924&partnerID=40&md5=90d77a466c48df9cf34810e8ab606d6d,"An important research area of Spectrum-Based Fault Localization (SBFL) is the effectiveness of risk evaluation formulas.Most previous studies have adopted an empirical approach, which can hardly be considered as sufficiently comprehensive because of the huge number of combinations of various factors in SBFL. Though some studies aimed at overcoming the limitations of the empirical approach, none of them has provided a completely satisfactory solution. Therefore, we provide a theoretical investigation on the effectiveness of risk evaluation formulas. We define two types of relations between formulas, namely, equivalent and better. To identify the relations between formulas, we develop an innovative framework for the theoretical investigation. Our framework is based on the concept that the determinant for the effectiveness of a formula is the number of statements with risk values higher than the risk value of the faulty statement. We group all program statements into three disjoint sets with risk values higher than, equal to, and lower than the risk value of the faulty statement, respectively. For different formulas, the sizes of their sets are compared using the notion of subset. We use this framework to identify the maximal formulas which should be the only formulas to be used in SBFL. © 2013 ACM.",Debugging; Risk evaluation formulas; Spectrum-based fault localization; Testing,Computer debugging; Computer software; Testing; Disjoint sets; Empirical approach; Fault localization; Program statements; Risk evaluation; Satisfactory solutions; Theoretical investigations; Types of relations; Software engineering
Portfolio: Searching for relevant functions and their usages in millions of lines of code,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886566807&doi=10.1145%2f2522920.2522930&partnerID=40&md5=37935ceeaa808fe80eda553092d9c711,"Different studies show that programmers are more interested in finding definitions of functions and their uses than variables, statements, or ordinary code fragments. Therefore, developers require support in finding relevant functions and determining how these functions are used.Unfortunately, existing code search engines do not provide enough of this support to developers, thus reducing the effectiveness of code reuse.We provide this support to programmers in a code search system called Portfolio that retrieves and visualizes relevant functions and their usages. We have built Portfolio using a combination of models that address surfing behavior of programmers and sharing related concepts among functions.We conducted two experiments: first, an experiment with 49 C/C++ programmers to compare Portfolio to Google Code Search and Koders using a standard methodology for evaluating information-retrieval-based engines; and second, an experiment with 19 Java programmers to compare Portfolio to Koders. The results show with strong statistical significance that users find more relevant functions with higher precision with Portfolio than with Google Code Search and Koders.We also show that by using PageRank, Portfolio is able to rank returned relevant functions more efficiently. © 2013 ACM.",Information retrieval; Natural language processing; Pagerank; Source-code search; User studies,Experiments; Information retrieval; Natural language processing systems; Code fragments; Code search engine; Java programmers; NAtural language processing; PageRank; Source-code search; Statistical significance; User study; Search engines
A web-centred approach to end-user software engineering,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886482728&doi=10.1145%2f2522920.2522929&partnerID=40&md5=fd391e431a2ceee69e32b5fbc4bdf18a,"This article addresses one of the major end-user software engineering (EUSE) challenges, namely, how to motivate end users to apply unfamiliar software engineering techniques and activities to achieve their goal: translate requirements into software that meets their needs. EUSE activities are secondary to the goal that the program is helping to achieve and end-user programming is opportunistic. The challenge is then to find ways to incorporate EUSE activities into the existing workflow without users having to make substantial changes to the type of work they do or their priorities. In this article, we set out an approach to EUSE for webbased applications.We also propose a software lifecycle that is consistent with the conditions and priorities of end users without programming skills and is well-aligned with EUSE's characteristic informality, ambiguity and opportunisticness.Users applying this lifecyclemanage to find solutions tha theywould otherwise be unable to identify. They also develop quality products. Users of this approach will not have to be acquainted with software engineering, as a framework will take them through the web-centred EUSE lifecycle step-by-step. We also report a statistical experiment in which users develop web software with and without a framework to guide them through the lifecycle. Its aim is to validate the applicability of our framework-driven lifecycle. © 2013 ACM.",End-user development; End-user programming; Human-computer interaction; Visual programming,Computer programming; Human computer interaction; Life cycle; Software engineering; End user development; End user programming; End-user software engineering; Engineering techniques; Programming skills; Software life cycles; Statistical experiments; Visual programming; World Wide Web
On software component co-installability,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886473954&doi=10.1145%2f2522920.2522927&partnerID=40&md5=07d5e013a21a74bacd38a8328eeea9c9,"Modern software systems are built by composing components drawn from large repositories, whose size and complexity is increasing at a very fast pace. A fundamental challenge for the maintainability and the scalability of such software systems is the ability to quickly identify the components that can or cannot be installed together: this is the co-installability problem, which is related to boolean satisfiability and is known to be algorithmically hard. This article develops a novel theoretical framework, based on formally certified semantic preserving graph-theoretic transformations, that allows us to associate to each concrete component repository a much smaller one with a simpler structure, that we call strongly flat, with equivalent co-installability properties. This flat repository can be displayed in a way that provides a concise view of the co-installability issues in the original repository, or used as a basis for various algorithms related to co-installability, like the efficient computation of strong conflicts between components. The proofs contained in this work have been machine checked using the Coq proof assistant. © 2013 ACM.",Co-installability; Component; Conflicts; Dependencies; Open source; Package management,Graph theory; Semantics; Theorem proving; Co-installability; Component; Conflicts; Dependencies; Open sources; Package managements; Computer software
Path exploration based on symbolic output,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886536649&doi=10.1145%2f2522920.2522925&partnerID=40&md5=a999ba542b33122587081eeea14aad42,"Efficient program path exploration is important for many software engineering activities such as testing, debugging, and verification. However, enumerating all paths of a program is prohibitively expensive. In this article, we develop a partitioning of program paths based on the program output. Two program paths are placed in the same partition if they derive the output similarly, that is, the symbolic expression connecting the output with the inputs is the same in both paths. Our grouping of paths is gradually created by a smart path exploration. Our experiments show the benefits of the proposed path exploration in test-suite construction. Our path partitioning produces a semantic signature of a program-describing all the different symbolic expressions that the output can assume along different program paths. To reason about changes between program versions, we can therefore analyze their semantic signatures. In particular, we demonstrate the applications of our path partitioning in testing and debugging of software regressions. © 2013 ACM.",Software evolution; Software testing; Symbolic execution,Application programs; Program debugging; Semantics; Software engineering; Engineering activities; Path exploration; Semantic signatures; Software Evolution; Symbolic execution; Symbolic expression; Testing and debugging; Software testing
Test-and-Adapt: An approach for improving service interchangeability,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886561770&doi=10.1145%2f2522920.2522921&partnerID=40&md5=a026e3202725af730e30202b234ffaec,"Service-oriented applications do not fully benefit from standard APIs yet, and many applications fail to use interchangeably all the services that implement a standard service API. This article presents an approach to develop adaptation strategies that improve service interchangeability for service-oriented applications based on standard APIs. In our approach, an adaptation strategy consists of sets of parametric adaptation plans (called test-and-adapt plans), which execute test cases to reveal the occurrence of interchangeability problems, and activate runtime adaptors according to the test results. Throughout this article, we formalize the structure of the parametric test-and-adapt plans and of their execution semantics, present an algorithm for identifying correct execution orders through sets of test-and-adapt plans, provide empirical evidence of the occurrence of interchangeability problems for sample applications and services, and discuss the effectiveness of the approach in terms of avoided failures, runtime overheads and development costs. © 2013 ACM.",Dynamic adaption of serviceoriented applications based on standard apis; Reliability of service-oriented architectures,Information services; Semantics; Service oriented architecture (SOA); Adaptation strategies; Development costs; Execution semantics; Runtime overheads; Sample applications; Service-oriented applications; Standard services; Test case; Testing
Using a functional size measurement procedure to evaluate the quality of models in MDD environments,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894586312&doi=10.1145%2f2491509.2491520&partnerID=40&md5=2f4bc0c472492ea4bada4de829b145e0,"Models are key artifacts in Model-Driven Development (MDD) methods. To produce high-quality software by using MDD methods, quality assurance of models is of paramount importance. To evaluate the quality of models, defect detection is considered a suitable approach and is usually applied using reading techniques. However, these reading techniques have limitations and constraints, and new techniques are required to improve the efficiency at finding as many defects as possible. This article presents a case study that has been carried out to evaluate the use of a Functional Size Measurement (FSM) procedure in the detection of defects in models of anMDDenvironment. To do this, we compare the defects and the defect types found by an inspection group with the defects and the defect types found by the FSM procedure. The results indicate that the FSM is useful since it finds all the defects related to a specific defect type, it finds different defect types than an inspection group, and it finds defects related to the correctness and the consistency of the models. © 2013 ACM.",Case study; Defect detection; Functional size; Model-driven development,
An algebra of design patterns,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894587693&doi=10.1145%2f2491509.2491517&partnerID=40&md5=73dfc83468811c98ebaf8c6093dafc77,"In a pattern-oriented software design process, design decisions are made by selecting and instantiating appropriate patterns, and composing them together. In our previous work, we enabled these decisions to be formalized by defining a set of operators on patterns with which instantiations and compositions can be represented. In this article, we investigate the algebraic properties of these operators.We provide and prove a complete set of algebraic laws so that equivalence between pattern expressions can be proven. Furthermore, we define an always-terminating normalization of pattern expression to a canonical form which is unique modulo equivalence in first-order logic. By a case study, the pattern-oriented design of an extensible request-handling framework, we demonstrate two practical applications of the algebraic framework. First, we can prove the correctness of a finished design with respect to the design decisions made and the formal specification of the patterns. Second, we can even derive the design from these components. © 2013 ACM.",Algebra; Design patterns; Equational reasoning; Formal method; Pattern composition; Software design methodology,
Personalized reliability prediction of web services,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876119948&doi=10.1145%2f2430545.2430548&partnerID=40&md5=e1b38319e3061293372c265de0732e76,"Service Oriented Architecture (SOA) is a business-centric IT architectural approach for building distributed systems. Reliability of service-oriented systems heavily depends on the remote Web services as well as the unpredictable Internet connections. Designing efficient and effective reliability prediction approaches ofWeb services has become an important research issue. In this article, we propose two personalized reliability prediction approaches of Web services, that is, neighborhood-based approach and model-based approach. The neighborhood-based approach employs past failure data of similar neighbors (either service users or Web services) to predict the Web service reliability. On the other hand, the model-based approach fits a factor model based on the available Web service failure data and use this factor model to make further reliability prediction. Extensive experiments are conducted with our real-world Web service datasets, which include about 23 millions invocation results on more than 3,000 real-world Web services. The experimental results show that our proposed reliability prediction approaches obtain better reliability prediction accuracy than other competing approaches. © 2013 ACM.",Reliability prediction; User-collaboration; Web service,Forecasting; Information services; Service oriented architecture (SOA); Web services; Websites; Architectural approach; Distributed systems; Internet connection; Model based approach; Reliability prediction; Service Oriented Systems; Similar neighbors; User collaborations; Reliability
Automated comparison of state-based software models in terms of their language and structure,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876138196&doi=10.1145%2f2430545.2430549&partnerID=40&md5=6cb69656b0b55633d01d237427d8fa9b,"State machines capture the sequential behavior of software systems. Their intuitive visual notation, along with a range of powerful verification and testing techniques render them an important part of the modeldriven software engineering process. There are several situations that require the ability to identify and quantify the differences between two state machines (e.g. to evaluate the accuracy of state machine inference techniques is measured by the similarity of a reverse-engineered model to its reference model). State machines can be compared from two complementary perspectives: (1) In terms of their language - the externally observable sequences of events that are permitted or not, and (2) in terms of their structure - the actual states and transitions that govern the behavior. This article describes two techniques to compare models in terms of these two perspectives. It shows how the difference can be quantified and measured by adapting existing binary classification performance measures for the purpose. The approaches have been implemented by the authors, and the implementation is openly available. Feasibility is demonstrated via a case study to compare two real state machine inference approaches. Scalability and accuracy are assessed experimentally with respect to a large collection of randomly synthesized models. © 2013 ACM.",Accuracy; Comparison; Labeled transition systems,Testing; Accuracy; Binary classification; Comparison; Inference techniques; Labeled transition systems; Model-driven software engineerings; States and transitions; Verification and testing; Software engineering
"An information foraging theory perspective on tools for debugging, refactoring, and reuse tasks",2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876106539&doi=10.1145%2f2430545.2430551&partnerID=40&md5=c290c92ef9d3ccaf6060099f1e3b9b3a,"Theories of human behavior are an important but largely untapped resource for software engineering research. They facilitate understanding of human developers' needs and activities, and thus can serve as a valuable resource to researchers designing software engineering tools. Furthermore, theories abstract beyond specific methods and tools to fundamental principles that can be applied to new situations. Toward filling this gap, we investigate the applicability and utility of Information Foraging Theory (IFT) for understanding information-intensive software engineering tasks, drawing upon literature in three areas: debugging, refactoring, and reuse. In particular, we focus on software engineering tools that aim to support information-intensive activities, that is, activities in which developers spend time seeking information. Regarding applicability, we consider whether and how the mathematical equations within IFT can be used to explain why certain existing tools have proven empirically successful at helping software engineers. Regarding utility, we applied an IFT perspective to identify recurring design patterns in these successful tools, and consider what opportunities for future research are revealed by our IFT perspective. © 2013 ACM.",Information foraging; Software maintenance,Computer software maintenance; Research; Designing softwares; Fundamental principles; Human behaviors; Information foraging; Mathematical equations; Software engineering tools; Software engineers; Utility of information; Software engineering
ConMem: Detecting crash-triggering concurrency bugs through an effect-oriented approach,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876113775&doi=10.1145%2f2430545.2430546&partnerID=40&md5=467e243cb11d4729680e8f80df5fbaf3,"Multicore technology is making concurrent programs increasingly pervasive. Unfortunately, it is difficult to deliver reliable concurrent programs, because of the huge and nondeterministic interleaving space. In reality, without the resources to thoroughly check the interleaving space, critical concurrency bugs can slip into production versions and cause failures in the field. Approaches to making the best use of the limited resources and exposing severe concurrency bugs before software release would be desirable. Unlike previous work that focuses on bugs caused by specific interleavings (e.g., races and atomicity violations), this article targets concurrency bugs that result in one type of severe effect: program crashes. Our study of the error-propagation process of real-world concurrency bugs reveals a common pattern (50% in our nondeadlock concurrency bug set) that is highly correlated with program crashes. We call this pattern concurrency-memory bugs: buggy interleavings directly cause memory bugs (NULL-pointer-dereferences, dangling-pointers, buffer-overflows, uninitialized-reads) on shared memory objects. Guided by this study, we built ConMem to monitor program execution, analyze memory accesses and synchronizations, and predictively detect these common and severe concurrency-memory bugs. We also built a validator,ConMem-v, to automatically prune false positives by enforcing potential bug-triggering interleavings. We evaluated ConMem using 7 open-source programs with 10 real-world concurrency bugs. ConMem detects more tested bugs (9 out of 10 bugs) than a lock-set-based race detector and an unserializableinterleaving detector, which detect 4 and 6 bugs, respectively, with a false-positive rate about one tenth of the compared tools. ConMem-v further prunes out all the false positives. ConMem has reasonable overhead suitable for development usage. © 2013 ACM.",Concurrency bugs; Software testing,Multicore programming; Software testing; Atomicity violations; Concurrency bugs; Concurrent program; Error propagation; Highly-correlated; Multicore technology; Open-source program; Program execution; Program debugging
Business process model merging: An approach to business process consolidation,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876124384&doi=10.1145%2f2430545.2430547&partnerID=40&md5=234a4966ce4392181d5a7a99b7951dfa,"This article addresses the problem of constructing consolidated business process models out of collections of process models that share common fragments. The article considers the construction of unions of multiple models (called merged models) as well as intersections (called digests). Merged models are intended for analysts who wish to create a model that subsumes a collection of process models - typically representing variants of the same underlying process - with the aim of replacing the variants with the merged model. Digests, on the other hand, are intended for analysts who wish to identify the most recurring fragments across a collection of process models, so that they can focus their efforts on optimizing these fragments. The article presents an algorithm for computing merged models and an algorithm for extracting digests from a merged model. The merging and digest extraction algorithms have been implemented and tested against collections of process models taken from multiple application domains. The tests show that the merging algorithm produces compact models and scales up to process models containing hundreds of nodes. Furthermore, a case study conducted in a large insurance company has demonstrated the usefulness of the merging and digest extraction operators in a practical setting. © 2013 ACM.",Business process model; Graph matching; Model merging; Variability,Algorithms; Extraction; Insurance; Mergers and acquisitions; Merging; Pattern matching; Business process model; Extraction algorithms; Graph matchings; Insurance companies; Merging algorithms; Model merging; Multiple applications; Variability; Mathematical models
Detecting missing method calls as violations of the majority rule,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874898771&doi=10.1145%2f2430536.2430541&partnerID=40&md5=61023d19b4e10af962c69a6f3278eca3,"When using object-oriented frameworks it is easy to overlook certain important method calls that are required at particular places in code. In this article, we provide a comprehensive set of empirical facts on this problem, starting from traces of missing method calls in a bug repository. We propose a new system that searches for missing method calls in software based on the other method calls that are observable. Our key insight is that the voting theory concept of majority rule holds for method calls: a call is likely to be missing if there is a majority of similar pieces of code where this call is present. The evaluation shows that the system predictions go further missing method calls and often reveal different kinds of code smells (e.g., violations of API best practices). © 2013 ACM.",Verification,Computer software; Verification; Best practices; Code smell; Empirical facts; Majority rule; Object-oriented frameworks; System prediction; Voting theory; Software engineering
Achieving scalable model-based testing through test case diversity,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874821350&doi=10.1145%2f2430536.2430540&partnerID=40&md5=38c884557a76e4df8dcd1480a8e070ee,"The increase in size and complexity of modern software systems requires scalable, systematic, and automated testing approaches. Model-based testing (MBT), as a systematic and automated test case generation technique, is being successfully applied to verify industrial-scale systems and is supported by commercial tools. However, scalability is still an open issue for large systems, as in practice there are limits to the amount of testing that can be performed in industrial contexts. Even with standard coverage criteria, the resulting test suites generated by MBT techniques can be very large and expensive to execute, especially for system level testing on real deployment platforms and network facilities. Therefore, a scalable MBT technique should be flexible regarding the size of the generated test suites and should be easily accommodated to fit resource and time constraints. Our approach is to select a subset of the generated test suite in such a way that it can be realistically executed and analyzed within the time and resource constraints, while preserving the fault revealing power of the original test suite to a maximum extent. In this article, to address this problem, we introduce a family of similarity-based test case selection techniques for test suites generated from state machines. We evaluate 320 different similarity-based selection techniques and then compare the effectiveness of the best similarity-based selection technique with other common selection techniques in the literature. The results based on two industrial case studies, in the domain of embedded systems, show significant benefits and a large improvement in performance when using a similarity-based approach. We complement these analyses with further studies on the scalability of the technique and the effects of failure rate on its effectiveness. We also propose a method to identify optimal tradeoffs between the number of test cases to run and fault detection. © 2013 ACM.",Verification,Complex networks; Fault detection; Industry; Scalability; Verification; Automated test case generation; Industrial case study; Industrial context; Model based testing; Resource Constraint; Selection techniques; System level testing; Test case selection; Testing
Modeling and verifying hierarchical real-time systems using stateful timed CSP,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874914130&doi=10.1145%2f2430536.2430537&partnerID=40&md5=c7666d1d998c9b35a61d1fa103fabe36,"Modeling and verifying complex real-time systems are challenging research problems. The de facto approach is based on Timed Automata, which are finite state automata equipped with clock variables. Timed Automata are deficient in modeling hierarchical complex systems. In this work, we propose a language called Stateful Timed CSP and an automated approach for verifying Stateful Timed CSP models. Stateful Timed CSP is based on Timed CSP and is capable of specifying hierarchical real-time systems. Through dynamic zone abstraction, finite-state zone graphs can be generated automatically from Stateful Timed CSP models, which are subject to model checking. Like Timed Automata, Stateful Timed CSP models suffer from Zeno runs, that is, system runs that take infinitely many steps within finite time. Unlike Timed Automata, model checking with non-Zenoness in Stateful Timed CSP can be achieved based on the zone graphs. We extend the PAT model checker to support system modeling and verification using Stateful Timed CSP and show its usability/scalability via verification of real-world systems. © 2013 ACM.",Algorithms; Languages; Verification,Algorithms; Model checking; Query languages; Real time systems; Verification; Automated approach; Complex real-time systems; Finite-state; Model checker; Real-world system; Research problems; Support systems; Timed Automata; Automata theory
Editorial-Looking forward,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874919651&doi=10.1145%2f2430536.2431202&partnerID=40&md5=820f336fd23036c2ead8413d30b8b821,[No abstract available],,
Synthesizing nonanomalous event-based controllers for liveness goals,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874836417&doi=10.1145%2f2430536.2430543&partnerID=40&md5=59e2bf973e8b2412c84783a9a5e6b0a9,"We present SGR(1), a novel synthesis technique and methodological guidelines for automatically constructing event-based behavior models. Our approach works for an expressive subset of liveness properties, distinguishes between controlled and monitored actions, and differentiates system goals from environment assumptions. We show that assumptions must be modeled carefully in order to avoid synthesizing anomalous behavior models. We characterize nonanomalous models and propose assumption compatibility, a sufficient condition, as a methodological guideline. © 2013 ACM.",Algorithms; Design,Algorithms; Computer software; Design; Anomalous behavior; Behavior model; Event-based; Liveness; Liveness properties; Methodological guidelines; Sufficient conditions; Synthesis techniques; Software engineering
Facilitating the transition from use case models to analysis models: Approach and experiments,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874828402&doi=10.1145%2f2430536.2430539&partnerID=40&md5=ec2247a87bee234abf38911dc2a42b0e,"Use case modeling, including use case diagrams and use case specifications (UCSs), is commonly applied to structure and document requirements. UCSs are usually structured but unrestricted textual documents complying with a certain use case template. However, because Use Case Models (UCMods) remain essentially textual, ambiguity is inevitably introduced. In this article, we propose a use case modeling approach, called Restricted Use Case Modeling (RUCM), which is composed of a set of well-defined restriction rules and a modified use case template. The goal is two-fold: (1) restrict the way users can document UCSs in order to reduce ambiguity and (2) facilitate the manual derivation of initial analysis models which, when using the Unified Modeling Language (UML), are typically composed of class diagrams, sequence diagrams, and possibly other types of diagrams. Though the proposed restriction rules and template are based on a clear rationale, two main questions need to be investigated. First, do users find them too restrictive or impractical in certain situations? In other words, can users express the same requirements with RUCM as with unrestricted use cases? Second, do the rules and template have a positive, significant impact on the quality of the constructed analysis models? To investigate these questions, we performed and report on two controlled experiments, which evaluate the restriction rules and use case template in terms of (1) whether they are easy to apply while developing UCMods and facilitate the understanding of UCSs, and (2) whether they help users manually derive higher quality analysis models than what can be generated when they are not used, in terms of correctness, completeness, and redundancy. This article reports on the first controlled experiments that evaluate the applicability of restriction rules on use case modeling and their impact on the quality of analysis models. The measures we have defined to characterize restriction rules and the quality of analysis class and sequence diagrams can be reused to perform similar experiments in the future, either with RUCM or other approaches. Results show that the restriction rules are overall easy to apply and that RUCM results into significant improvements over traditional approaches (i.e., with standard templates, without restrictions) in terms of class correctness and class diagram completeness, message correctness and sequence diagram completeness, and understandability of UCSs. © 2013 ACM.",Design; Documentation; Experimentation; Measurement,Design; Encoding (symbols); Experiments; Graphic methods; Measurements; System program documentation; Unified Modeling Language; Controlled experiment; Experimentation; Sequence diagrams; Textual documents; Traditional approaches; Understandability; Use case specifications; Use case template; Quality control
Editorial-Looking back,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874829431&doi=10.1145%2f2430536.2431201&partnerID=40&md5=df035a7f472b15c88e3fd2d13c33058d,[No abstract available],,
Scaling predictive analysis of concurrent programs by removing trace redundancy,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874897828&doi=10.1145%2f2430536.2430542&partnerID=40&md5=4690d52e646c36abddfd22e7d2cb6ef8,"Predictive trace analysis (PTA) of concurrent programs is powerful in finding concurrency bugs unseen in past program executions. Unfortunately, existing PTA solutions face considerable challenges in scaling to large traces. In this article, we identify that a large percentage of events in the trace are redundant for presenting useful analysis results to the end user. Removing them from the trace can significantly improve the scalability of PTA without affecting the quality of the results. We present a trace redundancy theorem that specifies a redundancy criterion and the soundness guarantee that the PTA results are preserved after removing the redundancy. Based on this criterion, we design and implement TraceFilter, an efficient algorithm that automatically removes redundant events from a trace for the PTA of general concurrency access anomalies. We evaluated TraceFilter on a set of popular concurrent benchmarks as well as real world large server programs. Our experimental results show that TraceFilter is able to significantly improve the scalability of PTA by orders of magnitude, without impairing the analysis result. © 2013 ACM.",Algorithms; Performance; Theory,Algorithms; Human computer interaction; Predictive control systems; Redundancy; Scalability; Concurrent program; Design and implements; Orders of magnitude; Performance; Predictive analysis; Predictive traces; Program execution; Theory; Trace analysis
Verification across Intellectual Property Boundaries,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009752195&doi=10.1145%2f2430545.2430550&partnerID=40&md5=3dd85c49120466e347e2756eaaa3cb8f,"In many industries, the importance of software components provided by third-party suppliers is steadily increasing. As the suppliers seek to secure their intellectual property (IP) rights, the customer usually has no direct access to the suppliers' source code, and is able to enforce the use of verification tools only by legal requirements. In turn, the supplier has no means to convince the customer about successful verification without revealing the source code. This article presents an approach to resolve the conflict between the IP interests of the supplier and the quality interests of the customer. We introduce a protocol in which a dedicated server (called the “amanat”) is controlled by both parties: the customer controls the verification task performed by the amanat, while the supplier controls the communication channels of the amanat to ensure that the amanat does not leak information about the source code. We argue that the protocol is both practically useful and mathematically sound. As the protocol is based on well-known (and relatively lightweight) cryptographic primitives, it allows a straightforward implementation on top of existing verification tool chains. To substantiate our security claims, we establish the correctness of the protocol by cryptographic reduction proofs. © 2013, ACM. All rights reserved.",Management; Security Intellectual Property; Supply Chain; Verification,
Views: Synthesizing fine-grained concurrency control,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874862465&doi=10.1145%2f2430536.2430538&partnerID=40&md5=4de81c4c41f06619611739d42eb673df,"Fine-grained locking is often necessary to increase concurrency. Correctly implementing fine-grained locking with today's concurrency primitives can be challenging-race conditions often plague programs with sophisticated locking schemes. We present views, a new approach to concurrency control. Views ease the task of implementing sophisticated locking schemes and provide static checks to automatically detect many data races. A view of an object declares a partial interface, consisting of fields and methods, to the object that the view protects. A view also contains an incompatibility declaration, which lists views that may not be simultaneously held by other threads. A set of view annotations specify which code regions hold a view of an object. Our view compiler performs simple static checks that identify many data races. We pair the basic approach with an inference algorithm that can infer view incompatibility specifications for many applications. We have ported four benchmark applications to use views: portions of Vuze, a BitTorrent client; Mailpuccino, a graphical email client; jphonelite, a VoIP softphone implementation; and TupleSoup, a database. Our experience indicates that views are easy to use, make implementing sophisticated locking schemes simple, and can help eliminate concurrency bugs. We have evaluated the performance of a view implementation of a red-black tree and found that views can significantly improve performance over that of the lock-based implementation. © 2013 ACM.",Design; Languages; Reliability,Design; Inference engines; Internet telephony; Query languages; Reliability; Benchmark applications; Concurrency bugs; Fine-grained concurrency; Improve performance; Inference algorithm; Locking schemes; New approaches; Red black tree; Concurrency control
Use case and task models: An integrated development methodology and its formal foundation,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894596998&doi=10.1145%2f2491509.2491521&partnerID=40&md5=b7ad861e806e63e18454577cdc042ab7,"User Interface (UI) development methods are poorly integrated with standard software engineering practice. The differences in terms of artifacts involved, development philosophies, and lifecycles can often result in inconsistent system and UI specifications leading to duplication of effort and increased maintenance costs. To address such shortcomings, we propose an integrated development methodology for use case and task models. Use cases are generally used to capture functional requirements whereas task models specify the detailed user interactions with the UI. Our methodology can assist practitioners in developing software processes which allow these two kinds of artifacts to be developed in a codependent and integrated manner.We present our methodology, describe its semantic foundations along with a set of formal conformance relations, and introduce an automated verification tool. © 2013 ACM.",Conformance; Integrated development methodology; Task models; Use case models; Verification,Life cycle; Philosophical aspects; Semantics; Verification; Automated verification; Conformance; Functional requirement; Inconsistent systems; Integrated development; Semantic foundation; Task models; Use case model; User interfaces
Precise memory leak detection for java software using container profiling,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894586409&doi=10.1145%2f2491509.2491511&partnerID=40&md5=82dbb918b81d0ddc2d9006de8d5b142d,"A memory leak in a Java program occurs when object references that are no longer needed are unnecessarily maintained. Such leaks are difficult to detect because static analysis typically cannot precisely identify these redundant references, and existing dynamic leak detection tools track and report fine-grained information about individual objects, producing results that are usually hard to interpret and lack precision. In this article we introduce a novel container-based heap-tracking technique, based on the fact that many memory leaks in Java programs occur due to incorrect uses of containers, leading to containers that keep references to unused data entries. The novelty of the described work is twofold: (1) instead of tracking arbitrary objects and finding leaks by analyzing references to unused objects, the technique tracks only containers and directly identifies the source of the leak, and (2) the technique computes a confidence value for each container based on a combination of its memory consumption and its elements' staleness (time since last retrieval), while previous approaches do not consider such combined metrics. Our experimental results show that the reports generated by the proposed technique can be very precise: for two bugs reported by Sun, a known bug in SPECjbb 2000, and an example bug from IBM developerWorks, the top containers in the reports include the containers that leak memory. © 2013 ACM.",Container profiling; Leaking confidence; Memory leaks,Computer software; Containers; Leak detection; Object tracking; Static analysis; Arbitrary objects; Individual objects; Leak detection tool; Leaking confidence; Memory consumption; Memory leak detections; Memory leaks; Tracking techniques; Java programming language
Bounded satisfiability checking of metric temporal logic specifications,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894515204&doi=10.1145%2f2491509.2491514&partnerID=40&md5=d2c80bf187e2e9f02ab02de80221b71a,"We introduce bounded satisfiability checking, a verification technique that extends bounded model checking by allowing also the analysis of a descriptive model, consisting of temporal logic formulae, instead of the more customary operational model, consisting of a state transition system.We define techniques for encoding temporal logic formulae into Boolean logic that support the use of bi-infinite time domain and of metric time operators. In the framework of bounded satisfiability checking, we show how a descriptive model can be refined into an operational one, and how the correctness of such a refinement can be verified for the bounded case, setting the stage for a stepwise system development method based on a bounded model refinement. Finally, we show how the adoption of a modular approach can make the bounded refinement process more manageable and efficient. All introduced concepts are extensively applied to a set of case studies, and thoroughly experimented through Zot, our SAT solver-based verification toolset. © 2013 ACM.",Bi-infinite time; Bounded model checking; Formal methods; Refinement; Temporal logic,Boolean algebra; Computer circuits; Formal methods; Formal specification; Formal verification; Temporal logic; Time domain analysis; Bi-infinite time; Bounded model checking; Metric temporal logic; Refinement; Refinement process; Satisfiability checking; System development; Verification techniques; Model checking
Trading obliviousness for modularity with cooperative aspect-oriented programming,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894525191&doi=10.1145%2f2491509.2491516&partnerID=40&md5=8f7c6576bf249acd3151912cdda05be0,"The potential of aspect-oriented programming to adequately capture crosscutting concerns has yet to be fully realized. For example, authors have detailed significant challenges in creating reusable aspect component libraries. One proposed solution is to introduce Explicit Join Points (EJPs) to increase modularity by reducing obliviousness, enabling a Cooperative Aspect-Oriented Programming (Co-AOP) methodology where base code and aspects synergistically collaborate. This article explores the trade-offs between obliviousness and modularity. We briefly introduce EJPs and Co-AOP, and hypothesize how to balance obliviousness and modularity using Co-AOP. We build upon a prior empirical study to refactor three real-life Java applications to implement the exception handling concern using three distinct strategies: (1) using fully oblivious aspects in AspectJ, (2) using EJPs in a fully explicit fashion, and (3) using EJPs while following the Co-AOP methodology. We study other crosscutting concerns by refactoring a fourth application, JHotDraw. The differences in terms of common code metrics are analyzed, and the impact on modularity is assessed using design structure matrices. Results indicate that the Co-AOP methodology can in many cases significantly improve code quality attributes versus fully oblivious or fully explicit approaches. We conclude with guiding principles on the proper use of EJPs within the Co-AOP methodology. © 2013 ACM.",Aspect-oriented programming; Design structure matrix; Explicit join points; Modularity; Obliviousness,Commerce; Computer software reusability; Economic and social effects; Component libraries; Cooperative aspects; Cross-cutting concerns; Design Structure Matrices; Guiding principles; Join point; Modularity; Obliviousness; Aspect oriented programming
Marple: Detecting faults in path segments using automatically generated analyses,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894588881&doi=10.1145%2f2491509.2491512&partnerID=40&md5=b87f9be0c11266affe77f49f887d5458,"Generally, a fault is a property violation at a program point along some execution path. To obtain the path where a fault occurs, we can either run the program or manually identify the execution paths through code inspection. In both of the cases, only a very limited number of execution paths can be examined for a program. This article presents a static framework, Marple, that automatically detects path segments where a fault occurs at a whole program scale. An important contribution of the work is the design of a demand-driven analysis that effectively addresses scalability challenges faced by traditional path-sensitive fault detection. The techniques are made general via a specification language and an algorithm that automatically generates path-based analyses from specifications. The generality is achieved in handling both data- and controlcentric faults as well as both liveness and safety properties, enabling the exploitation of fault interactions for diagnosis and efficiency. Our experimental results demonstrate the effectiveness of our techniques in detecting path segments of buffer overflows, integer violations, null-pointer dereferences, and memory leaks. Because we applied an interprocedural, path-sensitive analysis, our static fault detectors generally report better precision than the tools available for comparison. Our demand-driven analyses are shown scalable to deployed applications such as apache, putty, and ffmpeg. © 2013 ACM.",Demand-driven; Faults; Path segments; Specification,Data handling; Faulting; Specification languages; Specifications; Automatically generated; Demand-driven; Demand-driven analysis; Deployed applications; Fault interactions; Inter-procedural; Path segments; Path-sensitive analysis; Fault detection
Enabledness-based program abstractions for behavior validation,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894559631&doi=10.1145%2f2491509.2491519&partnerID=40&md5=ca8c1107941ffeeb304511edc90c3db9,"Code artifacts that have nontrivial requirements with respect to the ordering in which their methods or procedures ought to be called are common and appear, for instance, in the form of API implementations and objects. This work addresses the problem of validating if API implementations provide their intended behavior when descriptions of this behavior are informal, partial, or nonexistent. The proposed approach addresses this problem by generating abstract behavior models which resemble typestates. These models are statically computed and encode all admissible sequences of method calls. The level of abstraction at which such models are constructed has shown to be useful for validating code artifacts and identifying findings which led to the discovery of bugs, adjustment of the requirements expected by the engineer to the requirements implicit in the code, and the improvement of available documentation. © 2013 ACM.",Enabledness abstractions; Source-code validation,Codes (symbols); Admissible sequences; Behavior model; Enabledness abstractions; Level of abstraction; Source codes; Abstracting
The value of design rationale information,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894548893&doi=10.1145%2f2491509.2491515&partnerID=40&md5=b4c64e2bf4b27971802c21a558bb4f78,"A complete and detailed (full) Design Rationale Documentation (DRD) could support many software development activities, such as an impact analysis or a major redesign. However, this is typically too onerous for systematic industrial use as it is not cost effective to write, maintain, or read. The key idea investigated in this article is that DRD should be developed only to the extent required to support activities particularly difficult to execute or in need of significant improvement in a particular context. The aim of this article is to empirically investigate the customization of the DRD by documenting only the information items that will probably be required for executing an activity. This customization strategy relies on the hypothesis that the value of a specific DRD information item depends on its category (e.g., assumptions, related requirements, etc.) and on the activity it is meant to support. We investigate this hypothesis through two controlled experiments involving a total of 75 master students as experimental subjects. Results show that the value of a DRD information item significantly depends on its category and, within a given category, on the activity it supports. Furthermore, on average among activities, documenting only the information items that have been required at least half of the time (i.e., the information that will probably be required in the future) leads to a customized DRD containing about half the information items of a full documentation. We expect that such a significant reduction in DRD information should mitigate the effects of some inhibitors that currently prevent practitioners from documenting design decision rationale. © 2013 ACM.",Design decisions; Empirical software engineering; Software architecture; Software maintenance; Value-based software engineering,Computer software maintenance; Cost effectiveness; Design; Software architecture; Controlled experiment; Customization strategies; Design decisions; Development activity; Empirical Software Engineering; Experimental subjects; Information items; Value based software engineering; Software design
Finite satisfiability of UML class diagrams with constrained class hierarchy,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894600913&doi=10.1145%2f2491509.2491518&partnerID=40&md5=7f8d0c9f22cd8721c4fdfa61cc2d2424,"Models lie at the heart of the emerging model-driven engineering approach. In order to guarantee precise, consistent, and correct models, there is a need for efficient powerful methods for verifying model correctness. Class diagram is the central language within UML. Its correctness problems involve issues of contradiction, namely the consistency problem, and issues of finite instantiation, namely the finite satisfiability problem. This article analyzes the problem of finite satisfiability of class diagrams with class hierarchy constraints and generalization-set constraints. The article introduces the FiniteSat algorithm for efficient detection of finite satisfiability in such class diagrams, and analyzes its limitations in terms of complex hierarchy structures. FiniteSat is strengthened in two directions. First, an algorithm for identification of the cause for a finite satisfiability problem is introduced. Second, a method for propagation of generalization-set constraints in a class diagram is introduced. The propagation method serves as a preprocessing step that improves FiniteSat performance, and helps developers in clarifying intended constraints. These algorithms are implemented in the FiniteSatUSE tool [BGU Modeling Group 2011b], as part of our ongoing effort for constructing a model-level integrated development environment [BGU Modeling Group 2010a]. © 2013 ACM.",Class hierarchy constraints; Class hierarchy structure; Detection and cause identification; Finite satisfiability; Generalization set constraints; Identification graph; Multiplicity constraints; Solvability of linear inequality system; UML class diagram,Formal logic; Class hierarchies; Finite satisfiability; Linear inequality systems; Multiplicity constraints; Set constraints; UML class diagrams; Encoding (symbols)
Fault localization prioritization: Comparing information-theoretic and coverage-based approaches,2013,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893320937&doi=10.1145%2f2491509.2491513&partnerID=40&md5=50b91be2c6a9a1426e0704853fbf2318,"Test case prioritization techniques seek to maximize early fault detection. Fault localization seeks to use test cases already executed to help find the fault location. There is a natural interplay between the two techniques; once a fault is detected, we often switch focus to fault fixing, for which localization may be a first step. In this article we introduce the Fault Localization Prioritization (FLP) problem, which combines prioritization and localization. We evaluate three techniques: a novel FLP technique based on information theory, FLINT (Fault Localization using INformation Theory), that we introduce in this article, a standard Test Case Prioritization (TCP) technique, and a ""test similarity technique"" used in previous work. Our evaluation uses five different releases of four software systems. The results indicate that FLP and TCP can statistically significantly reduce fault localization costs for 73% and 76% of cases, respectively, and that FLINT significantly outperforms similarity-based localization techniques in 52% of the cases considered in the study. © 2013 ACM.",Fault localization; Information theory; Test case prioritization,Fault detection; Testing; Transmission control protocol; Evaluation use; Fault localization; Localization technique; Prioritization; Software systems; Standard tests; Test case; Test case prioritization; Information theory
Validating software metrics: A spectrum of philosophies,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873637463&doi=10.1145%2f2377656.2377661&partnerID=40&md5=ee19674188b09a98303e857c7941c224,"Context. Researchers proposing a new metric have the burden of proof to demonstrate to the research community that the metric is acceptable in its intended use. This burden of proof is provided through the multi-faceted, scientific, and objective process of software metrics validation. Over the last 40 years, however, researchers have debated what constitutes a ""valid"" metric. Aim. the debate over what constitutes a valid metric centers on software metrics validation criteria. the objective of this article is to guide researchers in making sound contributions to the field of software engineering metrics by providing a practical summary of the metrics validation criteria found in the academic literature. Method.We conducted a systematic literature review that began with 2,288 papers and ultimately focused on 20 papers. After extracting 47 unique validation criteria from these 20 papers, we performed a comparative analysis to explore the relationships amongst the criteria. Results. Our 47 validation criteria represent a diverse view of what constitutes a valid metric.We present an analysis of the criteria's categorization, conflicts, common themes, and philosophical motivations behind the validation criteria. Conclusions. Although the 47 validation criteria are not conflict-free, the diversity of motivations and philosophies behind the validation criteria indicates that metrics validation is complex. Researchers proposing new metrics should consider the applicability of the validation criteria in terms of our categorization and analysis. Rather than arbitrarily choosing validation criteria for each metric, researchers should choose criteria that can confirm that the metric is appropriate for its intended use. We conclude that metrics validation criteria provide answers to questions that researchers have about the merits and limitations of a metric. © 2012 ACM.",Software metrics; Systematic literature review; Validation criterion,Motivation; Philosophical aspects; Software engineering; Academic literature; Burden of proof; Comparative analysis; Conflict free; Metrics validations; Research communities; Software engineering metrics; Software metrics; Sound contributions; Systematic literature review; Validation criteria; Research
A formal model for automated software modularity and evolvability analysis,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873660335&doi=10.1145%2f2377656.2377658&partnerID=40&md5=3669141f485da924f40be86986068de5,"Neither the nature of modularity in software design, characterized as a property of the structure of dependencies among design decisions, or its economic value are adequately well understood. one basic problem is that we do not even have a sufficiently clear definition of what it means for one design decision to depend on another. the main contribution of this work is one possible mathematically precise definition of dependency based on an augmented constraint network model. the model provides an end-to-end account of the connection between modularity and its value in terms of options to make adaptive changes in uncertain and changing design spaces. We demonstrate the validity and theoretical utility of the model, showing that it is consistent with, and provides new insights into, several previously published results in design theory. © 2012 ACM.",Augmented constraint network; Design analysis; Design modeling; Design structure matrix; Evolution; Modularity; Software economics,Economics; Constraint networks; Design Analysis; Design modeling; Design Structure Matrices; Evolution; Modularity; Software economics; Design
Validation of requirements for hybrid systems: A formal approach,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873684136&doi=10.1145%2f2377656.2377659&partnerID=40&md5=b8a360f275ec0f4f05d4241fb78451a8,"Flaws in requirements may have unacceptable consequences in the development of safety-critical applications. Formal approaches may help with a deep analysis that takes care of the precise semantics of the requirements. However, the proposed solutions often disregard the problem of integrating the formalization with the analysis, and the underlying logical framework lacks either expressive power, or automation. We propose a new, comprehensive approach for the validation of functional requirements of hybrid systems, where discrete components and continuous components are tightly intertwined. the proposed solution allows to tackle problems of conversion from informal to formal, traceability, automation, user acceptance, and scalability. We build on a new language, OTHELLO which is expressive enough to represent various domains of interest, yet allowing efficient procedures for checking the satisfiability. Around this, we propose a structured methodology where: informal requirements are fragmented and categorized according to their role; each fragment is formalized based on its category; specialized formal analysis techniques, optimized for requirements analysis, are finally applied. the approach was the basis of an industrial project aiming at the validation of the European Train Control System (ETCS) requirements specification. During the project a realistic subset of the ETCS specification was formalized and analyzed. the approach was positively assessed by domain experts. © 2012 ACM.",European train control system; Formal languages; Methodology; Requirements validation; Safety-critical applications,Formal languages; Requirements engineering; Safety engineering; Semantics; Specifications; Discrete components; Domain experts; European Train Control Systems; Expressive power; Formal analysis; Formal approach; Functional requirement; Industrial projects; Informal requirements; Logical frameworks; Methodology; Othello; Requirements analysis; Requirements specifications; Requirements validation; Safety critical applications; Satisfiability; User acceptance; Hybrid systems
"HAMPI: A solver for word equations over strings, regular expressions, and context-free grammars",2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873660420&doi=10.1145%2f2377656.2377662&partnerID=40&md5=964c386714377fe4bacf3c89143d0d4c,"Many automatic testing, analysis, and verification techniques for programs can be effectively reduced to a constraint-generation phase followed by a constraint-solving phase. This separation of concerns often leads to more effective and maintainable software reliability tools. the increasing efficiency of off-the-shelf constraint solvers makes this approach even more compelling. However, there are few effective and sufficiently expressive off-the-shelf solvers for string constraints generated by analysis of string-manipulating programs, so researchers end up implementing their own ad-hoc solvers. To fulfill this need, we designed and implementedHAMPI, a solver for string constraints over bounded string variables. Users of HAMPI specify constraints using regular expressions, context-free grammars, equality between string terms, and typical string operations such as concatenation and substring extraction. HAMPI then finds a string that satisfies all the constraints or reports that the constraints are unsatisfiable. We demonstrate HAMPI's expressiveness and efficiency by applying it to program analysis and automated testing. We used HAMPI in static and dynamic analyses for finding SQL injection vulnerabilities in Web applications with hundreds of thousands of lines of code. We also used HAMPI in the context of automated bug finding in C programs using dynamic systematic testing (also known as concolic testing). We then compared HAMPI with another string solver, CFGAnalyzer, and show that HAMPI is several times faster. HAMPI's source code, documentation, and experimental data are available at http://people.csail.mit. edu/akiezun/hampi1. © 2012 ACM.",Concolic testing; Context-free languages; Program analysis; Regular languages; String constraints; Word equations,Automatic testing; Context free grammars; Context free languages; Formal languages; Software reliability; Automated testing; Bug finding; C programs; Concolic testing; Constraint solvers; Constraint Solving; Experimental datum; Lines of code; Program analysis; Regular expressions; Separation of concerns; Source codes; SQL injection; Static and dynamic analysis; String constraints; Substring; Systematic testing; Verification techniques; WEB application; Word equations; Pattern matching
Systematizing pragmatic software reuse,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873620965&doi=10.1145%2f2377656.2377657&partnerID=40&md5=ded113d943202c4692ef3bb86d09a45a,"Many software reuse tasks involve reusing source code that was not designed in a manner conducive to those tasks, requiring that ad hoc modifications be applied. Such pragmatic reuse tasks are a reality in disciplined industrial practice; they arise for a variety of organizational and technical reasons. To investigate a pragmatic reuse task, a developer must navigate through, and reason about, source code dependencies in order to identify program elements that are relevant to the task and to decide how those elements should be reused. the developer must then convert his mental model of the task into a set of actions that he can perform. these steps are poorly supported by modern development tools and practices. We provide a model for the process involved in performing a pragmatic reuse task, including the need to capture (mentally or otherwise) the developer's decisions about how each program element should be treated: this is a pragmatic-reuse plan. We provide partial support for this model via a tool suite, called Gilligan; other parts of the model are supported via standard IDE tools. Using a pragmatic-reuse plan, Gilligan can semiautomatically transform the selected source code from its originating system and integrate it into the developer's system. We have evaluated Gilligan through a series of case studies and experiments (each involving industrial developers) using a variety of source systems and tasks; we report in particular on a previously unpublished, formal experiment. the results show that pragmatic-reuse plans are a robust metaphor for capturing pragmatic reuse intent and that, relative to standard IDE tools, Gilligan can (1) significantly decrease the time that developers require to perform pragmatic reuse tasks, (2) increase the likelihood that developers will successfully complete pragmatic reuse tasks, (3) decrease the time required by developers to identify infeasible reuse tasks, and (4) improve developers' sense of their ability to manage the risk in such tasks. © 2012 ACM.",Gilligan; Lightweight process and tooling; Low commitment; Planning; Pragmatic software reuse; Pragmatic-reuse plans; Pragmaticreuse plan enactment; Software reuse; Source code investigation; White box reuse,Computer programming languages; Experiments; Integrodifferential equations; Planning; Gilligan; Lightweight process and tooling; Low commitment; Pragmatic-reuse plans; Pragmaticreuse plan enactment; Source codes; White box; Computer software reusability
Concept location using formal concept analysis and information retrieval,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863193536&doi=10.1145%2f2377656.2377660&partnerID=40&md5=da21c2341dbc1e454ba81415aab17d0a,"the article addresses the problem of concept location in source code by proposing an approach that combines Formal Concept Analysis and Information Retrieval. In the proposed approach, Latent Semantic Indexing, an advanced Information Retrieval approach, is used to map textual descriptions of software features or bug reports to relevant parts of the source code, presented as a ranked list of source code elements. Given the ranked list, the approach selects the most relevant attributes from the best ranked documents, clusters the results, and presents them as a concept lattice, generated using Formal Concept Analysis. the approach is evaluated through a large case study on concept location in the source code on six open-source systems, using several hundred features and bugs. the empirical study focuses on the analysis of various configurations of the generated concept lattices and the results indicate that our approach is effective in organizing different concepts and their relationships present in the subset of the search results. In consequence, the proposed concept location method has been shown to outperform a standalone Information Retrieval based concept location technique by reducing the number of irrelevant search results across all the systems and lattice configurations evaluated, potentially reducing the programmers' effort during software maintenance tasks involving concept location. © 2012 ACM.",Concept location; Feature identification; Formal Concept Analysis; Information Retrieval; Program comprehension; Software evolution and maintenance,Computer programming languages; Information retrieval; Maintenance; Program debugging; Advanced informations; Analysis of various; Bug reports; Concept Lattices; Concept locations; Empirical studies; Feature identification; Latent Semantic Indexing; Lattice configuration; Open source system; Program comprehension; Search results; Software evolution and maintenances; Software features; Software-maintenance tasks; Source codes; Textual description; Formal concept analysis
A logical verification methodology for service-oriented computing,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863934994&doi=10.1145%2f2211616.2211619&partnerID=40&md5=f3d4580eb1edfa7483acd021c11445c4,"We introduce a logical verification methodology for checking behavioral properties of service-oriented computing systems. Service properties are described by means of SocL, a branching-Time temporal logic that we have specifically designed for expressing in an effective way distinctive aspects of services, such as, acceptance of a request, provision of a response, correlation among service requests and responses, etc. Our approach allows service properties to be expressed in such a way that they can be independent of service domains and specifications. We show an instantiation of our general methodology that uses the formal language COWS to conveniently specify services and the expressly developed software tool CMC to assist the user in the task of verifying SocL formulas over service specifications. We demonstrate the feasibility and effectiveness of our methodology by means of the specification and analysis of a case study in the automotive domain. © 2012 ACM.",,Computer aided software engineering; Formal languages; Automotive domains; Behavioral properties; General methodologies; Service domain; Service oriented computing; Service properties; Service requests; Service specifications; Verification methodology; Specifications
DARWIN: An approach to debugging evolving programs,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863954406&doi=10.1145%2f2211616.2211622&partnerID=40&md5=48555689c6759994e17fbee4dce35914,"Bugs in programs are often introduced when programs evolve from a stable version to a new version. In this article, we propose a new approach called DARWIN for automatically finding potential root causes of such bugs. Given two programs-A reference program and a modified program-And an input that fails on the modified program, our approach uses symbolic execution to automatically synthesize a new input that (a) is very similar to the failing input and (b) does not fail. We find the potential cause(s) of failure by comparing control-flow behavior of the passing and failing inputs and identifying code fragments where the control flows diverge. A notable feature of our approach is that it handles hard-To-explain bugs, like code missing errors, by pointing to code in the reference program. We have implemented this approach and conducted experiments using several real-world applications, such as the Apache Web server, libPNG (a library for manipulating PNG images), and TCPflow (a program for displaying data sent through TCP connections). In each of these applications, DARWIN was able to localize bugs with high accuracy. Even though these applications contain several thousands of lines of code, DARWIN could usually narrow down the potential root cause(s) to less than ten lines. In addition, we find that the inputs synthesized by DARWIN provide additional value by revealing other undiscovered errors. © 2012 ACM.",,Errors; Transmission control protocol; Apache web server; Control flows; Control-flow; Identifying code; Lines of code; Real-world application; Root cause; Symbolic execution; TCP connections; Program debugging
A framework for the checking and refactoring of crosscutting concepts,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863980151&doi=10.1145%2f2211616.2211618&partnerID=40&md5=108422cd08478015971e2da949767350,"Programmers employ crosscutting concepts, such as design patterns and other programming idioms, when their design ideas cannot be efficiently or effectively modularized in the underlying programming language. As a result, implementations of these crosscutting concepts can be hard to change even when the code is well structured. In this article, we describe Arcum, a system that supports the modular maintenance of crosscutting concepts. Arcum can be used to both check essential constraints of crosscutting concepts and to substitute crosscutting concept implementations with alternative implementations. Arcum is complementary to existing refactoring systems that focus on meaning-preserving program transformations at the programminglanguage- semantics level, because Arcum focuses on transformations at the conceptual level. We present the underpinnings of the Arcum approach and show how Arcum can be used to address several classical software engineering problems. © 2012 ACM.",,Software engineering; Conceptual levels; Design ideas; Design Patterns; Modularized; Program transformations; Programming idioms; Refactorings; Semantics
Accounting for defect characteristics in evaluations of testing techniques,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863973654&doi=10.1145%2f2211616.2211620&partnerID=40&md5=93f33fe75a32f073fdeb79bf94b656b4,"As new software-Testing techniques are developed, before they can achieve widespread acceptance, their effectiveness at detecting defects must be evaluated. The most common way of evaluating testing techniques is with empirical studies, in which one or more techniques are tried out on software with known defects. However, the defects used can affect the performance of the techniques. To complicate matters, it is not even clear how to effectively describe or characterize defects. To address these problems, this article describes an experiment architecture for empirically evaluating testing techniques which takes both defect and test-suite characteristics into account. As proof of concept, an experiment on GUI-Testing techniques is conducted. It provides evidence that the defect characteristics proposed do help explain defect detection, at least for GUI testing, and it explores the relationship between the coverage of defective code and the detection of defects. © 2012 ACM.",,Experiments; Testing; Defect detection; Detection of defects; Empirical studies; GUI testing; Proof of concept; Testing technique; Defects
A two-phase approximation for model checking probabilistic unbounded until properties of probabilistic systems,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863912638&doi=10.1145%2f2211616.2211621&partnerID=40&md5=d2d89fd4809b6f344cec4db3f93636c7,"We have developed a new approximate probabilistic model-checking method for untimed properties in probabilistic systems, expressed in a probabilistic temporal logic (PCTL, CSL). This method, in contrast to the existing ones, does not require the untimed until properties to be bounded a priori, where the bound refers to the number of discrete steps in the system required to verify the until property. The method consists of two phases. In the first phase, a suitable system- and property-dependent bound k0 is obtained automatically. In the second phase, the probability of satisfying the k0-bounded until property is computed as the estimate of the probability of satisfying the original unbounded until property. Both phases require only verification of bounded until properties, which can be effectively performed by simulation-based methods. We prove the correctness of the proposed two-phase method and present its optimized implementation in the widely used PRISM model-checking engine. We compare this implementation with sampling-based model-checking techniques implemented in two tools: PRISM and MRMC. We show that for several models these existing tools fail to compute the result, while the two-phase method successfully computes the result efficiently with respect to time and space. © 2012 ACM.",,Prisms; Discrete step; Model-checking techniques; Optimized implementation; Probabilistic model-checking; Probabilistic systems; Probabilistic temporal logic; Sampling-based; Second phase; Simulation-based method; Two phase method; Model checking
Type checking annotation-based product lines,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863940098&doi=10.1145%2f2211616.2211617&partnerID=40&md5=a9f8df82ed971ca83234d24fb7a66b6f,"Software product line engineering is an efficient means of generating a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test them all and ensure properties like type safety for the entire product line. We present a product-line-Aware type system that can type check an entire software product line without generating each variant in isolation. Specifically, we extend the Featherweight Java calculus with feature annotations for product-line development and prove formally that all program variants generated from a well typed product line are well typed. Furthermore, we present a solution to the problem of typing mutually exclusive features. We discuss how results from our formalization helped implement our own product-line tool CIDE for full Java and report of our experience with detecting type errors in four existing software product line implementations. © 2012 ACM.",,Java programming language; Software design; Featherweight Java; Feature annotation; Product-lines; Software Product Line; Software product line engineerings; Type errors; Type safety; Type systems; Typechecking; Software testing
Editorial,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859409082&doi=10.1145%2f2089116.2089117&partnerID=40&md5=3389ed9cdcbd8622a0e29f9bf487d395,[No abstract available],,
Symbolic message sequence charts,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859418056&doi=10.1145%2f2089116.2089122&partnerID=40&md5=9a9d62c681f1902835bb73f814175381,"Message sequence charts (MSCs) are a widely used visual formalism for scenario-based specifications of distributed reactive systems. In its conventional usage, an MSC captures an interaction snippet between concrete objects in the system. This leads to voluminous specifications when the system contains several objects that are behaviorally similar. MSCs also play an important role in the model-based testing of reactive systems, where they may be used for specifying (partial) system behaviors, describing test generation criteria, or representing test cases. However, since the number of processes in a MSC specification are fixed, model-based testing of systems consisting of process classes may involve a significant amount of rework: for example, reconstructing system models, or regenerating test cases for systems differing only in the number of processes of various types. In this article we propose a scenario-based notation, called symbolic message sequence charts (SMSCs), for modeling, simulation, and testing of process classes. SMSCs are a lightweight syntactic and semantic extension of MSCs where, unlike MSCs, a SMSC lifeline can denote some/all objects from a collection. Our extensions give us substantially more modeling power. Moreover, we present an abstract execution semantics for (structured collections of) SMSCs. This allows us to validate MSC-based system models capturing interactions between large, or even unbounded, number of objects. Finally, we describe a SMSC-based testing methodology for process classes, which allows generation of test cases for new object configurations with minimal rework. Since our SMSC extensions are only concerned with MSC lifelines, we believe that they can be integrated into existing standards such as UML 2.0. We illustrate our SMSC-based framework for modeling, simulation, and testing of process classes using a weather-update controller case-study from NASA. © 2012 ACM.",Message sequence charts; Symbolic execution; Test generation; Unified modeling language (UML),NASA; Semantics; Specifications; Unified Modeling Language; Concrete objects; Distributed reactive systems; Execution semantics; Message Sequence Charts; Model based testing; Modeling power; Process class; Reactive system; Scenario-based specifications; Semantic extensions; Symbolic execution; System behaviors; System models; Test case; Test generations; Testing methodology; UML 2.0; Unified modeling language (UML); Visual formalism; Flowcharting
A precise method-method interaction-based cohesion metric for object-oriented classes,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859402061&doi=10.1145%2f2089116.2089118&partnerID=40&md5=37755063322b43c427d978e9a7d90418,"The building of highly cohesive classes is an important objective in object-oriented design. Class cohesion refers to the relatedness of the class members, and it indicates one important aspect of the class design quality. A meaningful class cohesion metric helps object-oriented software developers detect class design weaknesses and refactor classes accordingly. Several class cohesion metrics have been proposed in the literature. Most of these metrics are applicable based on low-level design information such as attribute references in methods. Some of these metrics capture class cohesion by counting the number of method pairs that share common attributes. A few metrics measure cohesion more precisely by considering the degree of interaction, through attribute references, between each pair of methods. However, the formulas applied by these metrics to measure the degree of interaction cause the metrics to violate important mathematical properties, thus undermining their construct validity and leading to misleading cohesion measurement. In this paper, we propose a formula that precisely measures the degree of interaction between each pair of methods, and we use it as a basis to introduce a low-level design class cohesion metric (LSCC). We verify that the proposed formula does not cause the metric to violate important mathematical properties. In addition, we provide a mechanism to use this metric as a useful indicator for refactoring weakly cohesive classes, thus showing its usefulness in improving class cohesion. Finally, we empirically validate LSCC. Using four open source software systems and eleven cohesion metrics, we investigate the relationship between LSCC, other cohesion metrics, and fault occurrences in classes. Our results show that LSCC is one of three metrics that explains more accurately the presence of faults in classes. LSCC is the only one among the three metrics to comply with important mathematical properties, and statistical analysis shows it captures a measurement dimension of its own. This suggests that LSCC is a better alternative, when taking into account both theoretical and empirical results, as a measure to guide the refactoring of classes. From a more general standpoint, the results suggest that class quality, as measured in terms of fault occurrences, can be more accurately explained by cohesion metrics that account for the degree of interaction between each pair of methods. © 2012 ACM.",Attribute; Class cohesion; Low-level design; Method; Method-method interaction; Object-oriented software quality; Refactoring,Computer software selection and evaluation; Information dissemination; Object oriented programming; Product design; Attribute; Class cohesion; Method; Method-method interaction; Object oriented software; Refactorings; Adhesion
A generative programming framework for context-aware CSCW applications,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859394497&doi=10.1145%2f2089116.2089121&partnerID=40&md5=c7099f78088929ba29de89f78b54555e,"We present a programming framework based on the paradigm of generative application development for building context-aware collaborative applications. In this approach, context-aware applications are implemented using a domain-specific designmodel, and their execution environment is generated and maintained by the middleware. The key features of this design model include support for context-based service discovery and binding, context-based access control, context-based multiuser coordination, and context-triggered automated task executions. The middleware uses the technique of policy-based specialization for generating application-specific middleware components from the generic middleware components. Through a case-study example, we demonstrate this approach and present the evaluations of the design model and the middleware. © 2012 ACM.",Context-aware computing; Generative middleware; Pervasive computing,Access control; Ubiquitous computing; Application development; Automated tasks; Collaborative application; Context aware applications; Context-Aware; Context-aware computing; Context-based; Design models; Domain specific; Execution environments; Generative programming; Key feature; Middleware components; Multi-user; Programming framework; Service discovery; Middleware
Weak alphabet merging of partial behavior models,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856292378&doi=10.1145%2f2089116.2089119&partnerID=40&md5=c8606ef9d892eada3ec4146db35b7187,"Constructing comprehensive operational models of intended system behavior is a complex and costly task, which can be mitigated by the construction of partial behavior models, providing early feedback and subsequently elaborating them iteratively. However, how should partial behavior models with different viewpoints covering different aspects of behavior be composed? How should partial models of component instances of the same type be put together? In this article, we propose model merging of modal transition systems (MTSs) as a solution to these questions. MTS models are a natural extension of labelled transition systems that support explicit modeling of what is currently unknown about system behavior. We formally define model merging based on weak alphabet refinement, which guarantees property preservation, and show that merging consistent models is a process that should result in a minimal common weak alphabet refinement (MCR). In this article, we provide theoretical results and algorithms that support such a process. Finally, because in practice MTS merging is likely to be combined with other operations over MTSs such as parallel composition, we also study the algebraic properties of merging and apply these, together with the algorithms that support MTS merging, in a case study. © 2012 ACM.",Merge; MTS; Partial behavior models,Algorithms; Algebraic properties; Component instances; Explicit modeling; Labelled transition systems; Merge; Modal Transition Systems; Model merging; MTS; Natural extension; Operational model; Parallel composition; Partial behavior models; Property preservation; System behaviors; Theoretical result; Merging
Verification and validation of UML conceptual schemas with OCL constraints,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859400920&doi=10.1145%2f2089116.2089123&partnerID=40&md5=f6e4ae304c06e0db2e83ee0afaa397ee,"To ensure the quality of an information system, it is essential that the conceptual schema that represents the knowledge about its domain is semantically correct. The semantic correctness of a conceptual schema can be seen from two different perspectives. On the one hand, from the point of view of its definition, a conceptual schema must be right. This is ensured by means of verification techniques that check whether the schema satisfies several correctness properties. On the other hand, from the point of view of the requirements that the information system should satisfy, a schema must also be the right one. This is ensured by means of validation techniques, which help the designer understand the exact meaning of a schema and to see whether it corresponds to the requirements. In this article we propose an approach to verify and validate UML conceptual schemas, with arbitrary constraints formalized in OCL. We have also implemented our approach to show its feasibility. © 2012 ACM.",Conceptual modeling; Constraints; OCL,Semantics; Arbitrary constraints; Conceptual modeling; Conceptual schemas; Constraints; Correctness properties; OCL; Verification and validation; Verification techniques; Information systems
An approach for modeling architectural design rules in UML and its application to embedded software,2012,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859416909&doi=10.1145%2f2089116.2089120&partnerID=40&md5=1f809e18985668ab8c57c550cabe20a6,"Current techniques for modeling software architecture do not provide sufficient support for modeling architectural design rules. This is a problem in the context of model-driven development in which it is assumed that major design artifacts are represented as formal or semi-formal models. This article addresses this problem by presenting an approach to modeling architectural design rules in UML at the abstraction level of the meaning of the rules. The high abstraction level and the use of UML makes the rules both amenable to automation and easy to understand for both architects and developers, which is crucial to deployment in an organization. To provide a proof-of-concept, a tool was developed that validates a system model against the architectural rules in a separate UML model. To demonstrate the feasibility of the approach, the architectural design rules of an existing live industrial-strength system were modeled according to the approach. © 2012 ACM.",Embedded software development; Model-driven development (MDD); Model-driven engineering (MDE),Abstracting; Application programs; Architectural design; Embedded software; Unified Modeling Language; Abstraction level; Architectural rules; Design artifacts; Industrial strength; ITS applications; Model-driven development; Model-driven Engineering; Semi-formal models; Software design
Lattice-based sampling for path property monitoring,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856932987&doi=10.1145%2f2063239.2063244&partnerID=40&md5=21c8f34cbec37e595707c2dcb5a86b0d,"Runtime monitoring can provide important insights about a program's behavior and, for simple properties, it can be done efficiently. Monitoring properties describing sequences of program states and events, however, can result in significant runtime overhead. This is particularly critical when monitoring programs deployed at user sites that have low tolerance for overhead. In this paper we present a novel approach to reducing the cost of runtime monitoring of path properties. A set of original properties are composed to form a single integrated property that is then systematically decomposed into a set of properties that encode necessary conditions for property violations. The resulting set of properties forms a lattice whose structure is exploited to select a sample of properties that can lower monitoring cost, while preserving violation detection power relative to the original properties. The lattice is then complemented with a weighting scheme that assigns each property a different priority that can be adjusted continuously to better drive the property sampling process. Our evaluation using the Hibernate API reveals that our approach produces a rich, structured set of properties that enables control of monitoring overhead, while detecting more violations more quickly than alternative techniques. © 2011 ACM.",Deployed analysis; Runtime monitoring; Sequencing and path properties,Network components; Deployed analysis; Monitoring costs; Monitoring programs; Path properties; Program state; Runtime Monitoring; Runtime overheads; Sampling process; Weighting scheme; Importance sampling
Expressive and extensible parameter passing for distributed object systems,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856823303&doi=10.1145%2f2063239.2063242&partnerID=40&md5=4e729db0d566d72c8b29bab767227457,"In modern distributed object systems, reference parameters to a remote method are passed according to their runtime type. This design choice limits the expressiveness, readability, and maintainability of distributed applications. Further, to extend the built-in set of parameter passing semantics of a distributed object system, the programmer has to understand and modify the underlying middleware implementation. To address these design shortcomings, this article presents (i) a declarative and extensible approach to remote parameter passing that decouples parameter passing semantics from parameter types, and (ii) a plugin-based framework, DeXteR, which enables the programmer to extend the built-in set of remote parameter passing semantics, without having to understand or modify the underlying middleware implementation. DeXteR treats remote parameter passing as a distributed cross-cutting concern and uses aspect-oriented and generative techniques. DeXteR enables the implementation of different parameter passing semantics as reusable application-level plugins, applicable to application, system, and third-party library classes. The expressiveness, flexibility, and extensibility of the approach is validated by adding several nontrivial remote parameter passing semantics (i.e., copy-restore, lazy, streaming) to Java Remote Method Invocation (RMI) as DeXteR plugins. © 2011 ACM.",Aspect-oriented programming (AOP); Declarative programming; Extensiblemiddleware; Generative programming; Metadata,Aspect oriented programming; Computer aided software engineering; Computer software reusability; Maintainability; Metadata; Middleware; Semantics; Aspect-oriented; Aspect-oriented programming (AOP); Cross-cutting; Declarative Programming; Distributed applications; Distributed object system; Extensiblemiddleware; Generative programming; Java remote method invocations; Plug-ins; Reference parameters; Run-time types; Distributed computer systems
QVM: An efficient runtime for detecting defects in deployed systems,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856837812&doi=10.1145%2f2063239.2063241&partnerID=40&md5=aa36b8f4babb3e69ecfb0c3252c2dbe7,"Coping with software defects that occur in the post-deployment stage is a challenging problem: bugs may occur only when the system uses a specific configuration and only under certain usage scenarios. Nevertheless, halting production systems until the bug is tracked and fixed is often impossible. Thus, developers have to try to reproduce the bug in laboratory conditions. Often, the reproduction of the bug takes most of the debugging effort. In this paper we suggest an approach to address this problem by using a specialized runtime environment called Quality Virtual Machine (QVM). QVM efficiently detects defects by continuously monitoring the execution of the application in a production setting. QVM enables the efficient checking of violations of user-specified correctness properties, that is, typestate safety properties, Java assertions, and heap properties pertaining to ownership. QVM is markedly different from existing techniques for continuous monitoring by using a novel overhead manager which enforces a user-specified overhead budget for quality checks. Existing tools for error detection in the field usually disrupt the operation of the deployed system. QVM, on the other hand, provides a balanced trade-off between the cost of the monitoring process and the maintenance of sufficient accuracy for detecting defects. Specifically, the overhead cost of using QVM instead of a standard JVM, is low enough to be acceptable in production environments. We implemented QVM on top of IBM's J9 Java Virtual Machine and used it to detect and fix various errors in real-world applications. © 2011 ACM.",Debugging; Diagnosis; Heap assertions; Typestate; Virtual machines,Computer debugging; Computer simulation; Defects; Diagnosis; Managers; Continuous monitoring; Correctness properties; Debugging efforts; Deployed systems; Heap assertions; Java virtual machines; Laboratory conditions; Monitoring process; Overhead costs; Production environments; Production system; Quality checks; Real-world application; Runtime environments; Runtimes; Safety property; Software defects; System use; Typestate; Usage scenarios; Virtual machines; Program debugging
Feasibility of stepwise design of multitolerant programs,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856851511&doi=10.1145%2f2063239.2063240&partnerID=40&md5=e9bca7951e72a2cbce8467304cd10be7,"The complexity of designing programs that simultaneously tolerate multiple classes of faults, called multitolerant programs, is in part due to the conflicting nature of the fault tolerance requirements that must be met by a multitolerant program when different types of faults occur. To facilitate the design of multitolerant programs, we present sound and (deterministically) complete algorithms for stepwise design of two families of multitolerant programs in a high atomicity program model, where a process can read and write all program variables in an atomic step. We illustrate that if one needs to design failsafe (respectively, nonmasking) fault tolerance for one class of faults and masking fault tolerance for another class of faults, then a multitolerant program can be designed in separate polynomial-time (in the state space of the faultintolerant program) steps regardless of the order of addition. This result has a significant methodological implication in that designers need not be concerned about unknown fault tolerance requirements that may arise due to unanticipated types of faults. Further, we illustrate that if one needs to design failsafe fault tolerance for one class of faults and nonmasking fault tolerance for a different class of faults, then the resulting problem is NP-complete in program state space. This is a counterintuitive result in that designing failsafe and nonmasking fault tolerance for the same class of faults can be done in polynomial time. We also present sufficient conditions for polynomial-time design of failsafe-nonmasking multitolerance. Finally, we demonstrate the stepwise design of multitolerance for a stable disk storage system, a token ring network protocol and a repetitive agreement protocol that tolerates Byzantine and transient faults. Our automatic approach decreases the design time from days to a few hours for the token ring program that is our largest example with 200 million reachable states and 8 processes. © 2011 ACM.",Automatic addition of fault tolerance; Multitolerance,Data storage equipment; Fault tolerance; Internet protocols; Network protocols; Polynomial approximation; Teaching; Atomic step; Automatic addition of fault-tolerance; Design time; Disk storage systems; Failsafe; Multiple class; Multitolerance; NP Complete; Polynomial-time; Program models; Program state; Program variables; State space; Sufficient conditions; Token ring networks; Token ring programs; Tolerance requirement; Transient faults; Design
The choice calculus: A representation for software variation,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052687942&doi=10.1145%2f2063239.2063245&partnerID=40&md5=484025cd39cdea1a9662bc9ca21c04c6,"Many areas of computer science are concerned with some form of variation in software-from managing changes to software over time to supporting families of related artifacts. We present the choice calculus, a fundamental representation for software variation that can serve as a common language of discourse for variation research, filling a role similar to the lambda calculus in programming language research. We also develop an associated theory of software variation, including sound transformations of variation artifacts, the definition of strategic normal forms, and a design theory for variation structures, which will support the development of better algorithms and tools. © 2011 ACM.",Representation; Variation,Differentiation (calculus); Common languages; Design theory; Lambda calculus; Normal form; Programming language; Representation; Variation; Calculations
How well do search engines support code retrieval on the web?,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960577309&doi=10.1145%2f2063239.2063243&partnerID=40&md5=b1173d8671907e9c72f9de59bdead3b4,"Software developers search theWeb for various kinds of source code for diverse reasons. In a previous study, we found that searches varied along two dimensions: the size of the search target (e.g., block, subsystem, or system) and the motivation for the search (e.g., reference example or as-is reuse). Would each of these kinds of searches require different search technologies? To answer this question, we conducted an experiment with 36 participants to evaluate three diverse approaches (general purpose information retrieval, source code search, and component reuse), as represented by fiveWeb sites (Google, Koders, Krugle, Google Code Search, and SourceForge). The independent variables were search engine, size of search target, and motivation for search. The dependent variable was the participants judgement of the relevance of the first ten hits. We found that it was easier to find reference examples than components for as-is reuse and that participants obtained the best results using a general-purpose information retrieval site. However, we also found an interaction effect: code-specific search engines worked better in searches for subsystems, but Google worked better on searches for blocks. These results can be used to guide the creation of new tools for retrieving source code from the Web. © 2011 ACM.",Empirical study; Open source; Opportunistic development; Search archetypes,Codes (symbols); Computer programming languages; Motivation; Open systems; Search engines; Code retrievals; Code search; Component reuse; Dependent variables; Empirical studies; General purpose; Independent variables; Interaction effect; Open sources; Opportunistic development; Search archetypes; Search technology; Software developer; Source code searches; Source codes; SourceForge; Two-dimension; Information retrieval
Gaia-PL: A product line engineering approach for efficiently designing multiagent systems,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053490209&doi=10.1145%2f2000799.2000803&partnerID=40&md5=8a463e459237d148382eb023b59e8a15,"Agent-oriented software engineering (AOSE) has provided powerful and natural, high-level abstractions in which software developers can understand, model and develop complex, distributed systems. Yet, the realization of AOSE partially depends on whether agent-based software systems can achieve reductions in development time and cost similar to other reuse-conscious development methods. Specifically, AOSE does not adequately address requirements specifications as reusable assets. Software product line engineering is a reuse technology that supports the systematic development of a set of similar software systems through understanding, controlling, and managing their common, core characteristics and their differing variation points. In this article, we present an extension to the Gaia AOSE methodology, named Gaia-PL (Gaia-Product Line), for agent-based distributed software systems that enables requirements specifications to be easily reused. We show how ourmethodology uses a product line perspective to promote reuse in agent-based software systems early in the development life cycle so that software assets can be reused throughout system development and evolution. We also present results from an application to show how Gaia-PL provided reuse that reduced the design and development effort for a large, multiagent system. © 2011 ACM.",Agent-oriented software engineering; Software product line engineering,Electric ship equipment; Multi agent systems; Production engineering; Software agents; Software design; Specifications; Agent based; Agent Oriented Software Engineering; Agent-based softwares; Design and Development; Development method; Development time; Distributed software system; Distributed systems; High-level abstraction; Product line engineering; Product-lines; Requirements specifications; Reusable assets; Reuse technology; Software assets; Software developer; Software product line engineering; Software product line engineerings; Software systems; System development; Computer software reusability
Recommending adaptive changes for framework evolution,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053491252&doi=10.1145%2f2000799.2000805&partnerID=40&md5=46905aeb73a957027a7b2c94bcbaa35b,"In the course of a framework's evolution, changes ranging from a simple refactoring to a complete rearchitecture can break client programs. Finding suitable replacements for framework elements that were accessed by a client program and deleted as part of the framework's evolution can be a challenging task. We present a recommendation system, SemDiff, that suggests adaptations to client programs by analyzing how a framework was adapted to its own changes. In a study of the evolution of one open source framework and three client programs, our approach recommended relevant adaptive changes with a high level of precision. In a second study of the evolution of two frameworks, we found that related change detection approaches were better at discovering systematic changes and that SemDiff was complementary to these approaches by detecting non-trivial changes such as when a functionality is imported from an external library. © 2011 ACM.",Adaptive changes; Framework; Legacy study; Mining software repositories; Origin analysis; Partial program analysis; Recommendation system; Software evolution,Adaptive changes; Framework; Legacy study; Mining software repositories; Origin analysis; Program analysis; Software evolution; Recommender systems
Runtime verification for LTL and TLTL,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960344619&doi=10.1145%2f2000799.2000800&partnerID=40&md5=5af8a38ca96ab715cad5f284947ddfc6,"This article studies runtime verification of properties expressed either in lineartime temporal logic (LTL) or timed lineartime temporal logic (TLTL). It classifies runtime verification in identifying its distinguishing features to model checking and testing, respectively. It introduces a three-valued semantics (with truth values true, false, inconclusive) as an adequate interpretation as to whether a partial observation of a running system meets an LTL or TLTL property. For LTL, a conceptually simple monitor generation procedure is given, which is optimal in two respects: First, the size of the generated deterministic monitor is minimal, and, second, the monitor identifies a continuously monitored trace as either satisfying or falsifying a property as early as possible. The feasibility of the developed methodology is demontrated using a collection of real-world temporal logic specifications. Moreover, the presented approach is related to the properties monitorable in general and is compared to existing concepts in the literature. It is shown that the set of monitorable properties does not only encompass the safety and cosafety properties but is strictly larger. For TLTL, the same road map is followed by first defining a three-valued semantics. The corresponding construction of a timed monitor is more involved, yet, as shown, possible. © 2011 ACM.",Assertion checkers; Monitors; Runtime verification,Display devices; Model checking; Semantics; Assertion checkers; Linear time temporal logic; Partial observation; Road-maps; Running systems; Runtime verification; Temporal logic specifications; Three-valued; Truth values; Temporal logic
A compiler for multimodal scenarios: Transforming LSCs into aspectJ,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053470521&doi=10.1145%2f2000799.2000804&partnerID=40&md5=7238f0f50f8db05c387e1a2efe5f4897,"We exploit the main similarity between the aspect-oriented programming paradigm and the inter-object, scenario-based approach to specification, in order to construct a new way of executing systems based on the latter. Specifically, we transform multimodal scenario-based specifications, given in the visual language of live sequence charts (LSC), into what we call scenario aspects, implemented in AspectJ. Unlike synthesis approaches, which attempt to take the inter-object scenarios and construct intra-object state-based per-object specifications or a single controller automaton, we follow the ideas behind the LSC play-out algorithm to coordinate the simultaneous monitoring and direct execution of the specified scenarios. Thus, the structure of the specification is reflected in the structure of the generated code; the high-level inter-object requirements and their structure are not lost in the translation. The transformation/compilation scheme is fully implemented in a UML2-compliant tool we term the S2A compiler (for Scenarios to Aspects), which provides full code generation of reactive behavior from inter-object multimodal scenarios. S2A supports advanced scenario-based programming features, such as multiple instances and exact and symbolic parameters. We demonstrate our work with an application whose interobject behaviors are specified using LSCs. We discuss advantages and challenges of the compilation scheme in the context of the more general vision of scenario-based programming. © 2011 ACM.",Aspect oriented programming; Code generation; Inter-object approach; Live sequence charts; Scenario-based programming; Scenarios; UML sequence diagrams; Visual formalisms,Aspect oriented programming; Computer systems programming; Flowcharting; Network components; Object oriented programming; Specifications; Code Generation; Inter-object approach; Live sequence chart; Scenarios; UML sequence diagrams; Visual formalism; Program compilers
Discovering multidimensional correlations among regulatory requirements to understand risk,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053500161&doi=10.1145%2f2000799.2000802&partnerID=40&md5=eb4b2412dc3688e6ce02fadb05add6fd,"Security breaches most often occur due to a cascading effect of failure among security constraints that collectively contribute to overall secure system behavior in a socio-technical environment. Therefore, during security certification activities, analysts must systematically take into account the nexus of causal chains that exist among security constraints imposed by regulatory requirements. Numerous regulatory requirements specified in natural language documents or listed in spreadsheets/databases do not facilitate such analysis. The work presented in this article outlines a stepwise methodology to discover and understand the multidimensional correlations among regulatory requirements for the purpose of understanding the potential for risk due to noncompliance during system operation. Our lattice algebraic computational model helps estimate the collective adequacy of diverse security constraints imposed by regulatory requirements and their interdependencies with each other in a bounded scenario of investigation. Abstractions and visual metaphors combine human intuition with metrics available from the methodology to improve the understanding of risk based on the level of compliance with regulatory requirements. In addition, a problem domain ontology that classifies and categorizes regulatory requirements from multiple dimensions of a socio-technical environment promotes a common understanding among stakeholders during certification and accreditation activities. A preliminary empirical investigation of our theoretical propositions has been conducted in the domain of The United States Department of Defense Information Technology Security Certification and Accreditation Process (DITSCAP). This work contributes a novel approach to understand the level of compliance with regulatory requirements in terms of the potential for risk during system operation. © 2011 ACM.",Certification and accreditation; Knowledge engineering; Ontology-based domain modeling; Requirements visualization; Risk; Software requirements engineering,Accreditation; Information technology; Knowledge engineering; Ontology; Requirements engineering; Visualization; Accreditation process; Cascading effects; Causal chains; Certification and accreditation; Computational model; Department of Defense; Empirical investigation; Multiple dimensions; Natural languages; Ontology-based domain modeling; Problem domain; Regulatory requirements; Requirements visualization; Risk-based; Secure system; Security breaches; Security certification; Security constraint; Sociotechnical; System operation; Visual metaphor; Regulatory compliance
The minimal failure-causing schema of combinatorial testing,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051932481&doi=10.1145%2f2000799.2000801&partnerID=40&md5=ae53137264472516ad77b649a8216276,"Combinatorial Testing (CT) involves the design of a small test suite to cover the parameter value combinations so as to detect failures triggered by the interactions among these parameters. To make full use of CT and to extend its advantages, this article first gives a model of CT and then presents a theory of the Minimal Failure-causing Schema (MFS), including the concept of the MFS, proof of its existence, some of its properties, and a method of finding the MFS. Then we propose a methodology for CT based on this MFS theory and the existing research. Our MFS-based methodology emphasizes that CT should work on accurate testing requirements, and has the following advantages: 1) Detect failure to the greatest degree with the least cost. 2) Effectiveness is improved by emphasizing mining of the information in software and making full use of the information gained from test design and execution. 3) Determine the root causes of failures and reveal related faults near the exposed ones. 4) Provide a foundation and model for regression testing and software quality evaluation of CT. A case study is presented to illustrate the MFS-based CT methodology, and an empirical study on a real software developed by us shows that the MFS really exists and the methodology based on MFS can considerably improve CT. © 2011 ACM.",Combinatorial testing (CT); Failure diagnosis; Minimal failure-causing schema (MFS); Test case generation,Computer software selection and evaluation; Accurate testing; Combinatorial testing; Empirical studies; Failure diagnosis; Least cost; Minimal failure-causing schema (MFS); Parameter values; Real softwares; Regression testing; Root cause; Software quality evaluation; Test case generation; Test designs; Software testing
Prime: A methodology for developing provenance-aware applications,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052319925&doi=10.1145%2f2000791.2000792&partnerID=40&md5=255f1f4d2f5b5ee5f9211b045d3772ce,"Provenance refers to the past processes that brought about a given (version of an) object, item or entity. By knowing the provenance of data, users can often better understand, trust, reproduce, and validate it. A provenance-aware application has the functionality to answer questions regarding the provenance of the data it produces, by using documentation of past processes. Prime is a software engineering technique for adapting application designs to enable them to interact with a provenance middleware layer, thereby making them provenance-aware. In this article, we specify the steps involved in applying Prime, analyze its effectiveness, and illustrate its use with two case studies, in bioinformatics and medicine. © 2011 ACM.",Methodology; Provenance,Bioinformatics; Middleware; Application design; Engineering techniques; Methodology; Middleware layer; Provenance; Software engineering
A revisit of fault class hierarchies in general Boolean specifications,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051645994&doi=10.1145%2f2000791.2000797&partnerID=40&md5=6eebac8efc7b5ec3610877e34f4f41bb,"Recently, Kapoor and Bowen [2007] have extended the works by Kuhn [1999], Tsuchiya and Kikuno [2002], and Lau and Yu [2005]. However, their proofs overlook the possibility that a mutant of the Boolean specifications under test may be equivalent. Hence, each of their fault relationships is either incorrect or has an incorrect proof. In this article, we give counterexamples to the incorrect fault relationships and provide new proofs for the valid fault relationships. Furthermore, a co-stronger fault relation is introduced to establish a new fault class hierarchy for general Boolean specifications. © 2011 ACM.",Boolean specifications; Fault class; Fault-based testing,Boolean specifications; Fault class; Fault-based testing; Specifications
FlagRemover: A testability transformation for transforming loop-assigned flags,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052326313&doi=10.1145%2f2000791.2000796&partnerID=40&md5=b597ee510f44477bbcff6311e1407d04,"Search-Based Testing is a widely studied technique for automatically generating test inputs, with the aim of reducing the cost of software engineering activities that rely upon testing. However, search-based approaches degenerate to random testing in the presence of flag variables, because flags create spikes and plateaux in the fitness landscape. Both these features are known to denote hard optimization problems for all search-based optimization techniques. Several authors have studied flag removal transformations and fitness function refinements to address the issue of flags, but the problem of loop-assigned flags remains unsolved. This article introduces a testability transformation along with a tool that transforms programs with loop-assigned flags into flag-free equivalents, so that existing search-based test data generation approaches can successfully be applied. The article presents the results of an empirical study that demonstrates the effectiveness and efficiency of the testability transformation on programs including those made up of open source and industrial production code, as well as test data generation problems specifically created to denote hard optimization problems. © 2011 ACM.",Empirical evaluation; Evolutionary testing; Flags; Testability transformation,Automatic test pattern generation; Data communication systems; Open systems; Optimization; Software engineering; Software testing; Empirical evaluations; Empirical studies; Engineering activities; Evolutionary testing; Fitness functions; Fitness landscape; Flags; Industrial production; Open sources; Optimization problems; Optimization techniques; Random testing; Search-based; Test data generation; Test inputs; Testability transformations; Metadata
A model for spectra-based software diagnosis,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052308307&doi=10.1145%2f2000791.2000795&partnerID=40&md5=bc127e1652a4fecfd0e5fbde845381b2,"This article presents an improved approach to assist diagnosis of failures in software (fault localisation) by ranking program statements or blocks in accordance with to how likely they are to be buggy. We present a very simple single-bug program to model the problem. By examining different possible execution paths through this model program over a number of test cases, the effectiveness of different proposed spectral ranking methods can be evaluated in idealised conditions. The results are remarkably consistent to those arrived at empirically using the Siemens test suite and Space benchmarks. The model also helps identify groups of metrics that are equivalent for ranking. Due to the simplicity of the model, an optimal ranking method can be devised. This new method out-performs previously proposed methods for the model program, the Siemens test suite and Space. It also helps provide insight into other ranking methods. © 2011 ACM.",Fault localization; Program spectra; Statistical debugging,Program diagnostics; Software testing; Execution paths; Fault localization; Localisation; Model programs; Program spectra; Program statements; Ranking methods; Siemens; Statistical debugging; Test case; Program debugging
Reducing the effort of bug report triage: Recommenders for development-oriented decisions,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052315877&doi=10.1145%2f2000791.2000794&partnerID=40&md5=24333d70603e86a21286b6bdc7a1ca80,"A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process. To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70% and 98% over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15% precision of hand-tuned developer recommenders. © 2011 ACM.",Bug report triage; Configuration assistance; Machine learning; Recommendation; Task assignment,Learning systems; Bug reports; Configuration assistance; Development process; Machine-learning; Open source projects; Recommendation; Software development process; Software development projects; Task assignment; Software design
Temporal dependency-based checkpoint selection for dynamic verification of temporal constraints in scientific workflow systems,2011,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052315881&doi=10.1145%2f2000791.2000793&partnerID=40&md5=b08432bfdaaee282892ede53f8362cdd,"In a scientific workflow system, a checkpoint selection strategy is used to select checkpoints along scientific workflow execution for verifying temporal constraints so that we can identify any temporal violations and handle them in time in order to ensure overall temporal correctness of the execution that is often essential for the usefulness of execution results. The problem of existing representative strategies is that they do not differentiate temporal constraints as, once a checkpoint is selected, they verify all temporal constraints. However, such a checkpoint does not need to be taken for those constraints whose consistency can be deduced from others. The corresponding verification of such constraints is consequently unnecessary and can severely impact overall temporal verification efficiency while the efficiency determines whether temporal violations can be identified quickly for handling in time. To address the problem, in this article, we develop a new temporal-dependency based checkpoint selection strategy which can select checkpoints in accordance with different temporal constraints.With our strategy, the corresponding unnecessary verification can be avoided. The comparison and experimental simulation further demonstrate that our new strategy can improve the efficiency of overall temporal verification significantly over the existing representative strategies. © 2011 ACM.",Checkpoint selection; Scientific workflows; Temporal constraints; Temporal verification,Checkpoint selection; Dynamic verifications; Experimental simulations; New strategy; Scientific workflows; Temporal constraints; Temporal correctness; Temporal verification; Temporal verification efficiency; Efficiency
Supporting dynamic aspect-oriented features,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956525273&doi=10.1145%2f1824760.1824764&partnerID=40&md5=f098d93a243592fc3900cb444771237b,"Dynamic aspect-oriented (AO) features have important software engineering benefits such as allowing unanticipated software evolution and maintenance. It is thus important to efficiently support these features in language implementations. Current implementations incur unnecessary design-time and runtime overhead due to the lack of support in underlying intermediate language (IL) models. To address this problem, we present a flexible and dynamic IL model that we call Nu. The Nu model provides a higher level of abstraction compared to traditional object-oriented ILs, making it easier to efficiently support dynamic AO features. We demonstrate these benefits by providing an industrial-strength VM implementation for Nu, by showing translation strategies from dynamic source-level constructs to Nu and by analyzing the performance of the resulting IL code. Nus VM extends the Sun Hotspot VM interpreter and uses a novel caching mechanism to significantly reduce the amortized costs of join point dispatch. Our evaluation using standard benchmarks shows that the overhead of supporting a dynamic deployment model can be reduced to as little as ̃1.5%. Nu provides an improved compilation target for dynamic deployment features, which makes it easier to support such features with corresponding software engineering benefits in software evolution and maintenance and in runtime verification. © 2010 ACM.",Aspect-oriented intermediate-languages; Aspect-oriented virtual machines; Invocation; Nu; Weaving,Computer simulation; Computer software maintenance; Cost reduction; Linguistics; Verification; Weaving; Aspect-oriented; Caching mechanism; Dynamic aspects; Dynamic deployment; Hot spot; Intermediate languages; Invocation; Join point; Language implementations; Level of abstraction; Nu; Object oriented; Run-time verification; Runtime overheads; Software evolution and maintenances; Translation strategies; Computer software selection and evaluation
The small-world effect: The influence of macro-level properties of developer collaboration networks on open-source project success,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956527513&doi=10.1145%2f1824760.1824763&partnerID=40&md5=f6313f36e340301afbf9655f643e5b69,"In this study we investigate the impact of community-level networksrelationships that exist among developers in an OSS communityon the productivity of member developers. Specifically, we argue that OSS community networks characterized by small-world properties would positively influence the productivity of the member developers by providing them with speedy and reliable access to more quantity and variety of information and knowledge resources. Specific hypotheses are developed and tested using longitudinal data on a large panel of 4,279 projects from 15 different OSS communities hosted at Sourceforge. Our results suggest that significant variation exists in small-world properties of OSS communities at Sourceforge. After accounting for project, foundry, and time-specific observed and unobserved effects, we found a statistically significant relationship between small-world properties of a community and the technical and commercial success of the software produced by its members. In contrast to the findings of prior research, we also found the lack of a significant relationship between closeness and betweenness centralities of the project teams and their success. These results were robust to a number of controls and model specifications. © 2010 ACM.",Collaborative software development; Online community; Open source software development; Productivity; Small world networks; Social networks; Team formation,Distributed computer systems; Groupware; Online systems; Open systems; Productivity; Social networking (online); Collaborative software development; Online communities; Open source software development; Small world networks; Social Networks; Team formation; Software design
Extracting and answering why and why not questions about Java program output,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956536785&doi=10.1145%2f1824760.1824761&partnerID=40&md5=1776435841e148320121b9fef7485389,"When software developers want to understand the reason for a programs behavior, they must translate their questions about the behavior into a series of questions about code, speculating about the causes in the process. The Whyline is a new kind of debugging tool that avoids such speculation by instead enabling developers to select a question about program output from a set of ""why did and why didnt"" questions extracted from the programs code and execution. The tool then finds one or more possible explanations for the output in question. These explanations are derived using a static and dynamic slicing, precise call graphs, reachability analyses, and new algorithms for determining potential sources of values. Evaluations of the tool on two debugging tasks showed that developers with the Whyline were three times more successful and twice as fast at debugging, compared to developers with traditional breakpoint debuggers. The tool has the potential to simplify debugging and program understanding in many software development contexts. © 2010 ACM.",Debugging; Questions; Whyline,Computer software; Java programming language; Software design; Breakpoint debuggers; Call graphs; Debugging; Debugging tools; Java program; Potential sources; Program understanding; Questions; Reachability analysis; Software developer; Software development; Static and dynamic; Whyline; Program debugging
Modular aspect-oriented design with XPIs,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956510050&doi=10.1145%2f1824760.1824762&partnerID=40&md5=222b634a65db39009e0e71b8377cbde4,"The emergence of aspect-oriented programming (AOP) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. What we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. The main contribution of this work is a new form of information-hiding interface for AOP that we call the crosscut programming interface, or XPI. XPIs abstract crosscutting behaviors and make these abstractions explicit. XPIs can be used, albeit with limited enforcement of interface rules, with existing AOP languages, such as AspectJ. To evaluate our notion of XPIs, we have applied our XPI-based design methodology. © 2010 ACM.",Aspect-oriented programming; Design rules; Options,Abstracting; Design; Linguistics; Modular construction; Aspect-J; Aspect-oriented designs; Aspect-Oriented Programming; Design Methodology; Design rules; Modularizations; New forms; New mechanisms; Options; Parallel development; Programming codes; Programming interface; Software designers; Computer systems programming
Method and developer characteristics for effective agile method tailoring: A study of xp expert opinion,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954368560&doi=10.1145%2f1767751.1767753&partnerID=40&md5=841b934ddb4ba68e4016b8b8bf0bbd69,"It has long been acknowledged that software methods should be tailored if they are to achieve optimum effect. However comparatively little research has been carried out to date on this topic in general, and more notably, on agile methods in particular. This dearth of evidence in the case of agile methods is especially significant in that it is reasonable to expect that such methods would particularly lend themselves to tailoring. In this research, we present a framework based on interviews with 20 senior software development researchers and a review of the extant literature. The framework is comprised of two sets of factors-characteristics of the method, and developer practices-that can improve method tailoring effectiveness. Drawing on the framework, we then interviewed 16 expert XP practitioners to examine the current state and effectiveness of XP tailoring efforts, and to shed light on issues the framework identified as being important. The article concludes with a set of recommendations for research and practice that would advance our understanding of the method tailoring area. © 2010 ACM.",Agile method; Contingency; Engineering; Expert opinion; Extreme programming; Software development; Tailoring; Xp,Computer software; Research; Agile method; Agile methods; Expert opinion; EXtreme Programming; Little research; Software development; Software methods; Software design
Types and modularity for implicit invocation with implicit announcement,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954340803&doi=10.1145%2f1767751.1767752&partnerID=40&md5=1ceb715e2c89c3324e72a03f71b91bfd,"Through implicit invocation, procedures are called without explicitly referencing them. Implicit announcement adds to this implicitness by not only keeping implicit which procedures are called, but also where or when-under implicit invocation with implicit announcement, the call site contains no signs of that, or what it calls. Recently, aspect-oriented programming has popularized implicit invocation with implicit announcement as a possibility to separate concerns that lead to interwoven code if conventional programming techniques are used. However, as has been noted elsewhere, as currently implemented it establishes strong implicit dependencies between components, hampering independent software development and evolution. To address this problem, we present a type-based modularization of implicit invocation with implicit announcement that is inspired by how interfaces and exceptions are realized in Java. By extending an existing compiler and by rewriting several programs to make use of our proposed language constructs, we found that the imposed declaration clutter tends to be moderate; in particular, we found that, for general applications of implicit invocation with implicit announcement, fears that programs utilizing our form of modularization become unreasonably verbose are unjustified. © 2010 ACM.",Aspect-oriented programming; Event-driven programming; Implicit invocation; Modularity; Publish/subscribe; Typing,Modular construction; Software design; Aspect-Oriented Programming; Event-driven programming; General applications; Implicit invocation; Language constructs; Modularizations; Programming technique; Publish/subscribe; Software development; Computer systems programming
Clone region descriptors: Representing and tracking duplication in source code,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954339263&doi=10.1145%2f1767751.1767754&partnerID=40&md5=e2e00cde5300b4f3c97a31e3ab294e7b,"Source code duplication, commonly known as code cloning, is considered an obstacle to software maintenance because changes to a cloned region often require consistent changes to other regions of the source code. Research has provided evidence that the elimination of clones may not always be practical, feasible, or cost-effective. We present a clone management approach that describes clone regions in a robust way that is independent from the exact text of clone regions or their location in a file, and that provides support for tracking clones in evolving software. Our technique relies on the concept of abstract clone region descriptors (CRDs), which describe clone regions using a combination of their syntactic, structural, and lexical information. We present our definition of CRDs, and describe a clone tracking system capable of producing CRDs from the output of different clone detection tools, notifying developers of modifications to clone regions, and supporting updates to the documented clone relationships. We evaluated the performance and usefulness of our approach across three clone detection tools and five subject systems, and the results indicate that CRDs are a practical and robust representation for tracking code clones in evolving software. © 2010 ACM.",Clone detection; Clone management; Code clones; Refactoring; Source code duplication,Codes (symbols); Computer software; Computer software maintenance; Inspection equipment; Clone detection; Clone management; Code clone; Refactorings; Source codes; Cloning
A verification system for interval-based specification languages,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951847231&doi=10.1145%2f1734229.1734232&partnerID=40&md5=65f2ef8b5f37d2cf764b6ce4a5584b21,"Interval-based specification languages have been used to formally model and rigorously reason about real-time computing systems. This usually involves logical reasoning and mathematical computation with respect to continuous or discrete time. When these systems are complex, analyzing their models by hand becomes error-prone and difficult. In this article, we develop a verification system to facilitate the formal analysis of interval-based specification languages with machine-assisted proof support. The verification system is developed using a generic theorem prover, Prototype Verification System (PVS). Our system elaborately encodes a highly expressive set-based notation, Timed Interval Calculus (TIC), and can rigorously carry out the verification of TIC models at an interval level. We validated all TIC reasoning rules and discovered subtle flaws in the original rules. We also apply TIC to model Duration Calculus (DC), which is a popular interval-based specification language, and thus expand the capacity of the verification system. We can check the correctness of DC axioms, and execute DC proofs in a manner similar to the corresponding pencil-and-paper DC arguments. © 2010 ACM.",Formal specification languages; Real-time systems; Theorem proving,Calculations; Linguistics; Problem solving; Query languages; Specification languages; Specifications; Technological forecasting; Temperature indicating cameras; Theorem proving; Based specification; Discrete time; Duration calculus; Error prones; Formal analysis; Formal specification language; Logical reasoning; Mathematical computation; Prototype verification systems; Real time computing systems; Reasoning rules; Theorem provers; Timed interval calculus; Verification systems; Real time systems
Analysis and applications of timed service protocols,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951783955&doi=10.1145%2f1734229.1734230&partnerID=40&md5=2b5230f8ce03a97ccd93f1db107ada4c,"Web services are increasingly gaining acceptance as a framework for facilitating application-to-application interactions within and across enterprises. It is commonly accepted that a service description should include not only the interface, but also the business protocol supported by the service. The present work focuses on the formalization of an important category of protocols that includes time-related constraints (called timed protocols), and the impact of time on compatibility and replaceability analysis. We formalized the following timing constraints: C-Invoke constraints define time windows within which a service operation can be invoked while M-Invoke constraints define expiration deadlines. We extended techniques for compatibility and replaceability analysis between timed protocols by using a semantic-preserving mapping between timed protocols and timed automata, leading to the identification of a novel class of timed automata, called protocol timed automata (PTA). PTA exhibit a particular kind of silent transition that strictly increase the expressiveness of the model, yet they are closed under complementation, making every type of compatibility or replaceability analysis decidable. Finally, we implemented our approach in the context of a larger project called ServiceMosaic, a model-driven framework for Web service life-cycle management. © 2010 ACM.",Compatiblity and replaceability analysis; Timed automata; Timed business protocols; Web services,Robots; Timing circuits; Translation (languages); Web services; Business protocols; Complementation; Model-driven; Service description; Service operations; Service protocols; Time windows; Timed automata; Timing constraints; Work Focus; Internet protocols
Semantic self-assessment of query results in dynamic environments,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951815669&doi=10.1145%2f1734229.1734231&partnerID=40&md5=05b45e8146d3c5f05e5b97177ea8dda2,"Queries are convenient abstractions for the discovery of information and services, as they offer content-based information access. In distributed settings, query semantics are well-defined, for example, queries are often designed to satisfy ACID transactional properties. When query processing is introduced in a dynamic network setting, achieving transactional semantics becomes complex due to the open and unpredictable environment. In this article, we propose a query processing model for mobile ad hoc and sensor networks that is suitable for expressing a wide range of query semantics; the semantics differ in the degree of consistency with which query results reflect the state of the environment during query execution. We introduce several distinct notions of consistency and formally express them in our model. A practical and significant contribution of this article is a protocol for query processing that automatically assesses and adaptively provides an achievable degree of consistency given the operational environment throughout its execution. The protocol attaches an assessment of the achieved guarantee to returned query results, allowing precise reasoning about a query with a range of possible semantics. We evaluate the performance of this protocol and demonstrate the benefits accrued to applications through examples drawn from an industrial application. © 2010 ACM.",Consistency; Mobile computing; Query processing,Ad hoc networks; Industrial applications; Information management; Query processing; Consistency; Content-based information access; Dynamic environments; Dynamic network; Mobile ad hoc; Operational environments; Query execution; Query results; Query semantics; Self-assessment; Transactional properties; Semantics
Partial constraint checking for context consistency in pervasive computing,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149141809&doi=10.1145%2f1656250.1656253&partnerID=40&md5=df7b2b5bc184f714d09eb3c90381e200,"Pervasive computing environments typically change frequently in terms of available resources and their properties. Applications in pervasive computing use contexts to capture these changes and adapt their behaviors accordingly. However, contexts available to these applications may be abnormal or imprecise due to environmental noises. This may result in context inconsistencies, which imply that contexts conflict with each other. The inconsistencies may set such an application into a wrong state or lead the application to misadjust its behavior. It is thus desirable to detect and resolve the context inconsistencies in a timely way. One popular approach is to detect context inconsistencies when contexts breach certain consistency constraints. Existing constraint checking techniques recheck the entire expression of each affected consistency constraint upon context changes. When a changed context affects only a constraint's subexpression, rechecking the entire expression can adversely delay the detection of other context inconsistencies. This article proposes a rigorous approach to identifying the parts of previous checking results that are reusable without entire rechecking. We evaluated our work on the Cabot middleware through both simulation experiments and a case study. The experimental results reported that our approach achieved over a fifteenfold performance improvement on context inconsistency detection than conventional approaches. © 2010 ACM.",Constraints; Performance; Pervasive computing; Validation,Gene expression; Middleware; Consistency constraints; Conventional approach; Environmental noise; Inconsistency detection; Partial constraints; Performance improvements; Pervasive computing; Pervasive computing environment; Rigorous approach; Simulation experiments; Subexpression; Use context; Computer software reusability
Editorial,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149100898&doi=10.1145%2f1656250.1656251&partnerID=40&md5=b04784016fa29aa2d5d1285803089001,[No abstract available],,
Design and implementation of SATOR: A web service aggregator,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149094516&doi=10.1145%2f1656250.1656254&partnerID=40&md5=851b8e16559e73fa7acc8df7ac2276dd,"Our long-term objective is to develop a general methodology for deploying (Web) service aggregation and adaptation middleware, capable of suitably overcoming syntactic and behavioral mismatches in view of application integration within and across organizational boundaries. This article focuses on describing the core aggregation process, which generates the workflow of a composite service from a set of service workflows to be aggregated and a data-flow mapping linking service parameters. © 2010 ACM.",BPEL; Service contracts; Web service aggregation; Workflows; YAWL,Agglomeration; Contracts; Maintenance; Middleware; Web services; Aggregation process; Application integration; BPEL; Composite services; Dataflow; Organizational boundaries; Service aggregation; Service contract; Service parameters; Work-flows; Management
Synthesizing hierarchical state machines from expressive scenario descriptions,2010,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149111133&doi=10.1145%2f1656250.1656252&partnerID=40&md5=9a9b4e7f7262e74568444e472737aa67,"There are many examples in the literature of algorithms for synthesizing state machines from scenario-based models. The motivation for these is to automate the transition from scenario-based requirements to early behavioral design models. To date, however, these synthesis algorithms have tended to generate flat state machines which can be difficult to understand or adapt for practical systems. One of the reasons for this is that relationships between scenarios are often not taken into account during synthesiseither because the relationships are not explicitly defined or because the synthesis algorithms are not sophisticated enough to cope with them. If relationships are not considered, it is impossible for a synthesis algorithm to know, for example, where one scenario stops and another continues. Furthermore, the lack of relationships makes it difficult to introduce structure into the generated state machines. With the introduction of interaction overview diagrams (IODs) in UML2.0, relationships such as continuation and concurrency can now be specified between scenarios in a way that conforms to the UML standard. But synthesis algorithms do not currently exist that take into account all of these relationships. This article presents a novel synthesis algorithm for an extended version of interaction overview diagram. This algorithm takes into account not only continuation and concurrency, but also preemption, suspension and the notion of a negative scenario. Furthermore, the synthesis algorithm generates well-structured state machines. These state machines are executable and can therefore be used to validate the scenarios. The hierarchy generated aids readability and so the state machines are more amenable to subsequent design steps. Our IOD extensions have a formal semantics and are supported by a synthesis and execution tool, UCSIM, which is currently provided as a plug-in to IBM Rational Software Modeler. © 2010 ACM.",Interaction overview diagrams; Scenario-based requirements; Software modeling; State machine synthesis,Algorithms; Computer software; Contour followers; Formal logic; Formal methods; Graphic methods; Machine design; Object oriented programming; Semantics; Behavioral design models; Design steps; Extended versions; Formal Semantics; Hierarchical state machines; Plug-ins; Practical systems; Scenario description; Software modeling; State machine; Synthesis algorithms; Synthesis (chemical)
Conceptual data model-based software size estimation for information systems,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350214668&doi=10.1145%2f1571629.1571630&partnerID=40&md5=77ddeda747f588eb6edc7c646044b79f,Size estimation plays a key role in effort estimation that has a crucial impact on software projects in the software industry. Some information required by existing software sizing methods is difficult to predict in the early stage of software development. A conceptual data model is widely used in the early stage of requirements analysis for information systems. Lines of code (LOC) is a commonly used software size measure. This article proposes a novel LOC estimation method for information systems from their conceptual data models through using a multiple linear regression model. We have validated the proposed method using samples from both the software industry and open-source systems. © 2009 ACM.,Conceptual data model; Line of code (LOC); Multiple linear regression model; Software sizing,Data warehouses; Estimation; Information systems; Linear regression; Models; Conceptual data model; Conceptual data models; Effort Estimation; Estimation methods; Line of code (LOC); Lines of code; Multiple linear regression model; Multiple linear regression models; Open source system; Requirements analysis; Size estimation; Software development; Software industry; Software project; Software size estimation; Software size measure; Software sizing; Computer software
Measuring the strength of information flows in programs,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350244718&doi=10.1145%2f1571629.1571631&partnerID=40&md5=44657dbaae79c76305ff243b31101177,"Dynamic information flow analysis (DIFA) was devised to enable the flow of information among variables in an executing program to be monitored and possibly regulated. It is related to techniques like dynamic slicing and dynamic impact analysis. To better understand the basis for DIFA, we conducted an empirical study in which we measured the strength of information flows identified by DIFA, using information theoretic and correlation-based methods. The results indicate that in most cases the occurrence of a chain of dynamic program dependences between two variables does not indicate a measurable information flow between them. We also explored the relationship between the strength of an information flow and the length of the corresponding dependence chain, and we obtained results indicating that no consistent relationship exists between the length of an information flow and its strength. Finally, we investigated whether data dependence and control dependence makes equal or unequal contributions to flow strength. The results indicate that flows due to data dependences alone are stronger, on average, than flows due to control dependences alone. We present the details of our study and consider the implications of the results for applications of DIFA and related techniques. © 2009 ACM.",Correlation; Dynamic information flow analysis; Dynamic slicing; Entropy; Information flow length; Information flow strength; Information leakage; Program dependence,Dynamic programming; Information theory; Pulsatile flow; Correlation; Dynamic information flow analysis; Dynamic slicing; Information flow length; Information flow strength; Information leakage; Program dependence; Dynamic analysis
Amoeba: A methodology for modeling and evolving cross-organizational business processes,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350223832&doi=10.1145%2f1571629.1571632&partnerID=40&md5=7101e83895f04dc8eda6fb771f5454bb,"Business service engagements involve processes that extend across two or more autonomous organizations. Because of regulatory and competitive reasons, requirements for cross-organizational business processes often evolve in subtle ways. The changes may concern the business transactions supported by a process, the organizational structure of the parties participating in the process, or the contextual policies that apply to the process. Current business process modeling approaches handle such changes in an ad hoc manner, and lack a principled means for determining what needs to be changed and where. Cross-organizational settings exacerbate the shortcomings of traditional approaches because changes in one organization can potentially affect the workings of another. This article describes Amoeba, a methodology for business processes that is based on business protocols. Protocols capture the business meaning of interactions among autonomous parties via commitments. Amoeba includes guidelines for (1) specifying cross-organizational processes using business protocols, and (2) handling the evolution of requirements via a novel application of protocol composition. This article evaluates Amoeba using enhancements of a real-life business scenario of auto-insurance claim processing, and an aerospace case study. © 2009 ACM.",Business process modeling; Business protocols; Requirements evolution,Business Process; Business process modeling; Business protocols; Business scenario; Business service; Business transaction; Insurance claims; Novel applications; Organizational process; Organizational setting; Organizational structures; Protocol composition; Requirements evolution; Insurance
J-Orchestra: Enhancing Java Programs with Distribution Capabilities,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015943777&doi=10.1145%2f1555392.1555394&partnerID=40&md5=18407bedba5dc9539263a2d85f170a92,"J-Orchestra is a system that enhances centralized Java programs with distribution capabilities. Operating at the bytecode level, J-Orchestra transforms a centralized Java program (i.e., running on a single Java Virtual Machine (JVM)) into a distributed one (i.e., running across multiple JVMs). This transformation effectively separates distribution concerns from the core functionality of a program. J-Orchestra follows a semiautomatic transformation process. Through a GUI, the user selects program elements (at class granularity) and assigns them to network locations. Based on the user's input, the J-Orchestra backend automatically partitions the program through compiler-level techniques, without changes to the JVM or to the Java Runtime Environment (JRE) classes. By means of bytecode engineering and code generation, J-Orchestra substitutes method calls with remote method calls, direct object references with proxy references, etc. It also translates Java language features (e.g., static methods and fields, inheritance, inner classes, new object construction, etc.) for efficient distributed execution. We detail the main technical issues that J-Orchestra addresses, including its mechanism for program transformation in the presence of unmodifiable code (e.g., in JRE classes) and the translation of concurrency and synchronization constructs to work correctly over the network. We further discuss a case study of transforming a large, commercial, third-party application for efficient execution in a client server environment and outline the architectural characteristics of centralized programs that are amenable to automated distribution with J-Orchestra. © 2009, ACM. All rights reserved.",bytecode engineering; distributed computing; Experimentation; Java; Languages; middleware; RMI; Separation of concerns,
From Business Process Models to Process-Oriented Software Systems,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900571578&doi=10.1145%2f1555392.1555395&partnerID=40&md5=de3e913d5a0b70f7fb5c4c2adb25e04d,"Several methods for enterprise systems analysis rely on flow-oriented representations of business operations, otherwise known as business process models. The Business Process Modeling Notation (BPMN) is a standard for capturing such models. BPMN models facilitate communication between domain experts and analysts and provide input to software development projects.Meanwhile, there is an emergence of methods for enterprise software development that rely on detailed process definitions that are executed by process engines. These process definitions refine their counterpart BPMN models by introducing data manipulation, application binding, and other implementation details. The de facto standard for defining executable processes is the Business Process Execution Language (BPEL). Accordingly, a standards-based method for developing process-oriented systems is to start with BPMN models and to translate these models into BPEL definitions for subsequent refinement. However, instrumenting this method is challenging because BPMN models and BPEL definitions are structurally very different. Existing techniques for translating BPMN to BPEL only work for limited classes of BPMN models. This article proposes a translation technique that does not impose structural restrictions on the source BPMN model. At the same time, the technique emphasizes the generation of readable (block-structured) BPEL code. An empirical evaluation conducted over a large collection of process models shows that the resulting BPEL definitions are largely block-structured. Beyond its direct relevance in the context of BPMN and BPEL, the technique presented in this article addresses issues that arise when translating from graph-oriented to block-structure flow definition languages. © 2009, ACM. All rights reserved.",BPEL; BPMN; Business process modeling; Design; Languages; Web services,
Unifying Aspect- and Object-Oriented Design,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992711252&doi=10.1145%2f1555392.1555396&partnerID=40&md5=d28ac9a77c6e61f8208cefe3b97dc78d,"The contribution of this work is the design and evaluation of a programming language model that unifies aspects and classes as they appear in AspectJ-like languages. We show that our model preserves the capabilities of AspectJ-like languages, while improving the conceptual integrity of the language model and the compositionality of modules. The improvement in conceptual integrity is manifested by the reduction of specialized constructs in favor of uniform orthogonal constructs. The enhancement in compositionality is demonstrated by better modularization of integration and higher-order crosscutting concerns. © 2009, ACM. All rights reserved.",aspectoriented programming; binding; Classpect; Design; Eos; first class aspect instances; Human Factors; instance-level advising; Languages; unified aspect language model,
Interacting Process Classes,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996480131&doi=10.1145%2f1538942.1538943&partnerID=40&md5=b8cec124673d5dcec7df7f8a9b8da863,"Many reactive control systems consist of classes of active objects involving both intraclass interactions (i.e., objects belonging to the same class interacting with each other) and interclass interactions. Such reactive control systems appear in domains such as telecommunication, transportation and avionics. In this article, we propose a modeling and simulation technique for interacting process classes. Our modeling style uses standard notations to capture behavior. In particular, the control flow of a process class is captured by a labeled transition system, unit interactions between process objects are described as transactions, and the structural relations are captured via class diagrams. The key feature of our approach is that our execution semantics leads to an abstract simulation technique which involves (i) grouping together active objects into equivalence classes according their potential futures, and (ii) keeping track of the number of objects in an equivalence class rather than their identities. Our simulation strategy is both time and memory efficient and we demonstrate this on well-studied nontrivial examples of reactive systems. We also present a case study involving a weather-update controller from NASA to demonstrate the use of our simulator for debugging realistic designs. © 2009, ACM. All rights reserved.",Abstract execution; active objects; Design; Languages; message sequence charts; Unified Modeling Language (UML); Verification,Abstracting; Control systems; Distributed computer systems; Equivalence classes; Modeling languages; NASA; Semantics; Unified Modeling Language; Abstract simulation; Active object; Execution; Interacting process; Labeled transition systems; Message Sequence Charts; Modeling and simulation techniques; Simulation strategies; Process control
Verdict Functions in Testing with a Fault Domain or Test Hypotheses,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996526120&doi=10.1145%2f1538942.1538944&partnerID=40&md5=6dd1e36734236390e52023c08211e051,"In state-based testing, it is common to include verdicts within test cases, the result of the test case being the verdict reached by the test run. In addition, approaches that reason about test effectiveness or produce tests that are guaranteed to find certain classes of faults are often based on either a fault domain or a set of test hypotheses. This article considers how the presence of a fault domain or test hypotheses affects our notion of a test verdict. The analysis reveals the need for new verdicts that provide more information than the current verdicts and for verdict functions that return a verdict based on a set of test runs rather than a single test run. The concepts are illustrated in the contexts of testing from a nondeterministic finite state machine and the testing of a datatype specified using an algebraic specification language but are potentially relevant whenever fault domains or test hypotheses are used. © 2009, ACM. All rights reserved.",fault domains; test hypotheses; Test verdicts; Verification,Specification languages; Algebraic specification languages; Data type; Fault domains; Non-deterministic finite state machine; State-based testing; Test case; Test effectiveness; Test runs; Testing
Programming Pervasive and Mobile Computing Applications: The TOTA Approach,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996602295&doi=10.1145%2f1538942.1538945&partnerID=40&md5=682014d9e21af4a32da891bbe0c29a1d,"Pervasive and mobile computing call for suitable middleware and programming models to support the activities of complex software systems in dynamic network environments. In this article we present TOTA (“Tuples On The Air”), a novel middleware and programming approach for supporting adaptive context-aware activities in pervasive and mobile computing scenarios. The key idea in TOTA is to rely on spatially distributed tuples, adaptively propagated across a network on the basis of application-specific rules, for both representing contextual information and supporting uncoupled interactions between application components. TOTA promotes a simple way of programming that facilitates access to distributed information, navigation in complex environments, and the achievement of complex coordination tasks in a fully distributed and adaptive way, mostly freeing programmers and system managers from the need to take care of low-level issues related to network dynamics. This article includes both application examples to clarify concepts and performance figures to show the feasibility of the approach. © 2009, ACM. All rights reserved.",coordination; Design; middleware; mobile computing; Performance; Pervasive computing; self-adaptation; self-organization; tuple spaces,Complex networks; Coordination reactions; Middleware; Mobile computing; Ubiquitous computing; Complex software systems; Contextual information; Coordination; Distributed information; Dynamic network environment; Self adaptation; Self organizations; Tuple space; Distributed computer systems
Empirical evaluation of a nesting testability transformation for evolutionary testing,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949176917&doi=10.1145%2f1525880.1525884&partnerID=40&md5=37573adc30aff3f568026e6902783806,"Evolutionary testing is an approach to automating test data generation that uses an evolutionary algorithm to search a test object's input domain for test data. Nested predicates can cause problems for evolutionary testing, because information needed for guiding the search only becomes available as each nested conditional is satisfied. This means that the search process can overfit to early information, making it harder, and sometimes near impossible, to satisfy constraints that only become apparent later in the search. The article presents a testability transformation that allows the evaluation of all nested conditionals at once. Two empirical studies are presented. The first study shows that the form of nesting handled is prevalent in practice. The second study shows how the approach improves evolutionary test data generation. © 2009 ACM.",Evolutionary testing; Search-based software engineering; Test data generation; Testability transformation,Computer software; Data communication systems; Evolutionary algorithms; Test facilities; Empirical evaluations; Empirical studies; Evolutionary testing; Search process; Search-based software engineering; Test data; Test data generation; Test object; Testability transformation; Testability transformations; Testing
ACM Transactions on Software Engineering and Methodology: Editorial,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649224456&doi=10.1145%2f1525880.1525881&partnerID=40&md5=2c4eaa2808e6da94c8d0ca9227c1f109,"David Notkin shares his views on the compensation given to the people who provide tutorials for a conference. He says that conferences always compensate people who give tutorials but not to the members of program committees. People who offer tutorials can get broad publicity for their own work, consulting opportunities, and other professional benefits. He states that they also get paid for travel expenses and often get a stipend, while program committee members who are doing a service to the community, are not benefited directly from this service and are expected to pay travel expenses to the conference and the program committee meeting. He on behalf of the editorial board of ACM Transactions on Software Engineering and Methodology, acknowledge efforts of members as a reviewer for ACM TOSEM for the 2007 and 2008 calendar years.",,Computer software; Editorial board; Program committee; Transactions on Software Engineering; Travel expense; While-programs; Helium
A systematic review of theory use in studies investigating the motivations of software engineers,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949125717&doi=10.1145%2f1525880.1525883&partnerID=40&md5=345cd3c745f02b0668e7ce3788c6c5f5,"Motivated software engineers make a critical contribution to delivering successful software systems. Understanding the motivations of software engineers and the impact of motivation on software engineering outcomes could significantly affect the industry's ability to deliver good quality software systems. Understanding the motivations of people generally in relation to their work is underpinned by eight classic motivation theories from the social sciences. We would expect these classic motivation theories to play an important role in developing a rigorous understanding of the specific motivations of software engineers. In this article we investigate how this theoretical basis has been exploited in previous studies of software engineering. We analyzed 92 studies of motivation in software engineering that were published in the literature between 1980 and 2006. Our main findings are that many studies of software engineers' motivations are not explicitly underpinned by reference to the classic motivation theories. Furthermore, the findings presented in these studies are often not explicitly interpreted in terms of those theories, despite the fact that in many cases there is a relationship between those findings and the theories. Our conclusion is that although there has been a great deal of previous work looking at motivation in software engineering, the lack of reference to classic theories of motivation means that the current body of work in the area is weakened and our understanding of motivation in software engineering is not as rigorous as it may at first appear. This weakness in the current state of knowledge highlights important areas for future researchers to contribute towards developing a rigorous and usable body of knowledge in motivating software engineers. © 2009 ACM.",Motivation; Software engineering,Engineering; Engineers; Motivation; Body of knowledge; Quality software; Software engineers; Software systems; Systematic Review; Theoretical basis; Computer software
Tools and experiments supporting a testing-based theory of component composition,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949161076&doi=10.1145%2f1525880.1525885&partnerID=40&md5=58ba5917fd881e689a570ab9f6a34c66,"Development of software using off-the-shelf components seems to offer a chance for improving product quality and developer productivity. This article reviews a foundational testing-based theory of component composition, describes tools that implement the theory, and presents experiments with functional and nonfunctional component/system properties that validate the theory and illuminate issues in component composition. The context for this work is an ideal form of Component-Based Software Development (CBSD) supported by tools. Component developers describe their components by measuring approximations to functional and nonfunctional behavior on a finite collection of subdomains. Systems designers describe an application-system structure by the component connections that form it. From measured component descriptions and a system structure, a CAD tool synthesizes the system properties, predicting how the system will behave. The system is not built, nor are any test executions performed. Neither the component sources nor executables are needed by systems designers. From CAD calculations a designer can learn (approximately) anything that could be learned by testing an actual system implementation. The CAD tool is often more efficient than it would be to assemble and execute an actual system. Using tools that support an ideal separation between component- and system development, experiments were conducted to investigate two related questions: (1) To what extent can unit (that is, component) testing replace system testing (2) What properties of software and subdomains influence the quality of subdomain testing?. © 2009 ACM.",CAD tool support for CBSD; Component-based software development (CBSD); Experiments with composition of software components; Synthesis of system properties,Computer software selection and evaluation; Experiments; Software design; Structure (composition); Actual system; CAD tool; CAD tool support for CBSD; Component composition; Component descriptions; Component-based software development; Component-based software development (CBSD); Executables; Ideal separation; Off-the-shelf components; Product quality; Sub-domains; Subdomain; Synthesis of system properties; System development; System property; System structures; System testing; Test execution; Computer aided design
Composing expressive runtime security policies,2009,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66849138438&doi=10.1145%2f1525880.1525882&partnerID=40&md5=18516ad20d66f6f12772d59f3ac2beb3,"Program monitors enforce security policies by interposing themselves into the control flow of untrusted software whenever that software attempts to execute security-relevant actions. At the point of interposition, a monitor has authority to permit or deny (perhaps conditionally) the untrusted software's attempted action. Program monitors are common security enforcement mechanisms and integral parts of operating systems, virtual machines, firewalls, network auditors, and antivirus and antispyware tools. Unfortunately, the runtime policies we require program monitors to enforce grow more complex, both as the monitored software is given new capabilities and as policies are refined in response to attacks and user feedback. We propose dealing with policy complexity by organizing policies in such a way as to make them composable, so that complex policies can be specified more simply as compositions of smaller subpolicy modules. We present a fully implemented language and system called Polymer that allows security engineers to specify and enforce composable policies on Java applications. We formalize the central workings of Polymer by defining an unambiguous semantics for our language. Using this formalization, we state and prove an uncircumventability theorem which guarantees that monitors will intercept all security-relevant actions of untrusted software. © 2009 ACM.",Policy composition; Policy enforcement; Policy-specification language,Chemical analysis; Computer operating systems; Computer viruses; Linguistics; Security systems; Specification languages; Specifications; Anti virus; Anti-spyware; Control flows; Integral part; Java applications; Operating systems; Policy composition; Policy enforcement; Policy-specification language; Runtime; Security enforcement mechanisms; Security engineers; Security policy; User feedback; Virtual machines; Java programming language
Semantic parameterization: A process for modeling domain descriptions,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149121201&doi=10.1145%2f1416563.1416565&partnerID=40&md5=fe518a1a90849bbbdf6be4de44d20994,"Software engineers must systematically account for the broad scope of environmental behavior, including nonfunctional requirements, intended to coordinate the actions of stakeholders and software systems. The Inquiry Cycle Model (ICM) provides engineers with a strategy to acquire and refine these requirements by having domain experts answer six questions: who, what, where, when, how, and why. Goal-based requirements engineering has led to the formalization of requirements to answer the ICM questions about when, how, and why goals are achieved, maintained, or avoided. In this article, we present a systematic process called Semantic Parameterization for expressing natural language domain descriptions of goals as specifications in description logic. The formalization of goals in description logic allows engineers to automate inquiries using who, what, and where questions, completing the formalization of the ICM questions. The contributions of this approach include new theory to conceptually compare and disambiguate goal specifications that enables querying goals and organizing goals into specialization hierarchies. The artifacts in the process include a dictionary that aligns the domain lexicon with unique concepts, distinguishing between synonyms and polysemes, and several natural language patterns that aid engineers in mapping common domain descriptions to formal specifications. Semantic Parameterization has been empirically validated in three case studies on policy and regulatory descriptions that govern information systems in the finance and health-care domains. © 2008 ACM.",Description logic; Domain knowledge; Formal specification; Natural language,Data structures; Engineering; Information theory; Linguistics; Semantics; Specifications; Case studies; Cycle models; Description logic; Description logics; Do-mains; Domain descriptions; Domain experts; Domain knowledge; Environmental behaviors; Formal specification; Formal specifications; Formalization of requirements; Natural language; Natural languages; New theories; Nonfunctional requirements; Software Engineers; Software systems; Systematic processes; Data description
Using a pilot study to derive a GUI model for automated testing,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149124049&doi=10.1145%2f1416563.1416567&partnerID=40&md5=576303e3a35735866c366a8fdfd233ce,"Graphical user interfaces (GUIs) are one of the most commonly used parts of today's software. Despite their ubiquity, testing GUIs for functional correctness remains an understudied area. A typical GUI gives many degrees of freedom to an end-user, leading to an enormous input event interaction space that needs to be tested. GUI test designers generate and execute test cases (modeled as sequences of user events) to traverse its parts; targeting a subspace in order to maximize fault detection is a nontrivial task. In this vein, in previous work, we used informal GUI code examination and personal intuition to develop an event-interaction graph (EIG). In this article we empirically derive the EIG model via a pilot study, and the resulting EIG validates our intuition used in previous work; the empirical derivation process also allows for model evolution as our understanding of GUI faults improves. Results of the pilot study show that events interact in complex ways; a GUI's response to an event may vary depending on the context established by preceding events and their execution order. The EIG model helps testers to understand the nature of interactions between GUI events when executed in test cases and why certain events detect faults, so that they can better traverse the event space. New test adequacy criteria are defined for the EIG; new algorithms use these criteria and EIG to systematically generate test cases that are shown to be effective on four fielded open-source applications. © 2008 ACM.",Graphical user interfaces; Model-based testing; Test minimization; Test suite management,Computer systems; Fault detection; Flow interactions; Testing; User interfaces; Applications.; Degrees of freedoms; Event spaces; Functional correctnesses; GUI tests; Input events; Interaction graphs; Interaction spaces; Model evolutions; Model-based testing; New algorithms; Nontrivial tasks; Pilot studies; Sub spaces; Test adequacy criterions; TEst cases; Test minimization; Test suite management; Graphical user interfaces
Domain-specific languages and program generation with meta-AspectJ,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149111605&doi=10.1145%2f1416563.1416566&partnerID=40&md5=b68e60e34d7ec186734f6d281319cb26,"Meta-AspectJ (MAJ) is a language for generating AspectJ programs using code templates. MAJ itself is an extension of Java, so users can interleave arbitrary Java code with AspectJ code templates. MAJ is a structured metaprogramming tool: a well-typed generator implies a syntactically correct generated program. MAJ promotes a methodology that combines aspect-oriented and generative programming. A valuable application is in implementing small domain-specific language extensions as generators using unobtrusive annotations for syntax extension and AspectJ as a back-end. The advantages of this approach are twofold. First, the generator integrates into an existing software application much as a regular API or library, instead of as a language extension. Second, a mature language implementation is easy to achieve with little effort since AspectJ takes care of the low-level issues of interfacing with the base Java language. In addition to its practical value, MAJ offers valuable insights to metaprogramming tool designers. It is a mature metaprogramming tool for AspectJ (and, by extension, Java): a lot of emphasis has been placed on context-sensitive parsing and error reporting. As a result, MAJ minimizes the number of metaprogramming (quote/unquote) operators and uses type inference to reduce the need to remember type names for syntactic entities. © 2008 ACM.",Domain-specific languages; Language extensions; Metaprogramming; Program synthesis; Program transformation; Program verification,Application programming interfaces (API); Codes (symbols); Computer programming; Computer programming languages; Computer software; Graphical user interfaces; Linguistics; Query languages; Syntactics; XML; Domain-specific languages; Language extensions; Metaprogramming; Program synthesis; Program transformation; Program verification; Java programming language
Automatically repairing event sequence-based GUI test suites for regression testing,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149089811&doi=10.1145%2f1416563.1416564&partnerID=40&md5=59e41f84e99508ebae31de113f36bfc4,"Although graphical user interfaces (GUIs) constitute a large part of the software being developed today and are typically created using rapid prototyping, there are no effective regression testing techniques for GUIs. The needs of GUI regression testing differ from those of traditional software. When the structure of a GUI is modified, test cases from the original GUI's suite are either reusable or unusable on the modified GUI. Because GUI test case generation is expensive, our goal is to make the unusable test cases usable, thereby helping to retain the suite's event coverage. The idea of reusing these unusable (obsolete) test cases has not been explored before. This article shows that a large number of test cases become unusable for GUIs. It presents a new GUI regression testing technique that first automatically determines the usable and unusable test cases from a test suite after a GUI modification, then determines the unusable test cases that can be repaired so that they can execute on the modified GUI, and finally uses repairing transformations to repair the test cases. This regression testing technique along with four repairing transformations has been implemented. An empirical study for four open-source applications demonstrates that (1) this approach is effective in that many of the test cases can be repaired, and is practical in terms of its time performance, (2) certain types of test cases are more prone to becoming unusable, and (3) certain types of ""dominator"" events, when modified, make a large number of test cases unusable. © 2008 ACM.",Graphical user interfaces; Regression testing; Repairing test cases; Test case management; Test maintenance,Computer software maintenance; Computer systems; Concurrent engineering; Job analysis; Maintenance; Rapid prototyping; Regression analysis; Repair; Software testing; Testing; User interfaces; Empirical studies; Event sequences; GUI regression testing; GUI tests; Large parts; Regression testing; Regression testing techniques; Repairing test cases; Test case management; TEst cases; Test suites; Time performances; Traditional softwares; Graphical user interfaces
Power laws in software,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53749089815&doi=10.1145%2f1391984.1391986&partnerID=40&md5=4204169e9c9420d3da589308e7e0987b,"A single statistical framework, comprising power law distributions and scale-free networks, seems to fit a wide variety of phenomena. There is evidence that power laws appear in software at the class and function level. We show that distributions with long, fat tails in software are much more pervasive than previously established, appearing at various levels of abstraction, in diverse systems and languages. The implications of this phenomenon cover various aspects of software engineering research and practice.",Power laws; Scale-free networks,Image segmentation; Network protocols; Sensor networks; Software engineering; Fat tails; Levels of abstraction; Power laws; Power-law distributions; Scale-free networks; Software engineering research; Statistical framework; Engineering research
Evaluating the benefits of context-sensitive points-to analysis using a BDD-based implementation,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53749104898&doi=10.1145%2f1391984.1391987&partnerID=40&md5=cc856401ec4d195f442640560665a8ce,"We present Paddle, a framework of BDD-based context-sensitive points-to and call graph analyses for Java, as well as client analyses that use their results. Paddle supports several variations of context-sensitive analyses, including call site strings and object sensitivity, and context-sensitively specializes both pointer variables and the heap abstraction. We empirically evaluate the precision of these context-sensitive analyses on significant Java programs. We find that that object-sensitive analyses are more precise than comparable variations of the other approaches, and that specializing the heap abstraction improves precision more than extending the length of context strings.",Binary decision diagrams; Call graph construction; Cast safety analysis; Context sensitivity; Interprocedural program analysis; Java; Points-to analysis,Abstracting; Computer programming languages; Java programming language; Binary decision diagrams; Call graph construction; Cast safety analysis; Context sensitivity; Interprocedural program analysis; Java; Points-to analysis; Sensitivity analysis
Unit-level test adequacy criteria for visual dataflow languages and a testing methodology,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53749094924&doi=10.1145%2f1391984.1391985&partnerID=40&md5=86088c50cfdb5de36da982428211c989,"Visual dataflow languages (VDFLs), which include commercial and research systems, have had a substantial impact on end-user programming. Like any other programming languages, whether visual or textual, VDFLs often contain faults. A desire to provide programmers of these languages with some of the benefits of traditional testing methodologies has been the driving force behind our effort in this work. In this article we introduce, in the context of prograph, a testing methodology for VDFLs based on structural test adequacy criteria and coverage. This article also reports on the results of two empirical studies. The first study was conducted to obtain meaningful information about, in particular, the effectiveness of our all-Dus criteria in detecting a reasonable percentage of faults in VDFLs. The second study was conducted to evaluate, under the same criterion, the effectiveness of our methodology in assisting users to visually localize faults by reducing their search space. Both studies were conducted using a testing system that we have implemented in Prograph's IDE.",Color; Fault detection; Fault localization; Software testing; Visual dataflow languages,Color; Fault detection; Software testing; Data flow language; Empirical studies; End user programming; Fault localization; Structural tests; Test adequacy criteria; Testing methodology; Testing systems; Visual languages
Topology analysis of software dependencies,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50649122720&doi=10.1145%2f13487689.13487691&partnerID=40&md5=555e59a6f42935c73c83796d3a7d17d2,"Before performing a modification task, a developer usually has to investigate the source code of a system to understand how to carry out the task. Discovering the code relevant to a change task is costly because it is a human activity whose success depends on a large number of unpredictable factors, such as intuition and luck. Although studies have shown that effective developers tend to explore a program by following structural dependencies, no methodology is available to guide their navigation through the thousands of dependency paths found in a nontrivial program. We describe a technique to automatically propose and rank program elements that are potentially interesting to a developer investigating source code. Our technique is based on an analysis of the topology of structural dependencies in a program. It takes as input a set of program elements of interest to a developer and produces a fuzzy set describing other elements of potential interest. Empirical evaluation of our technique indicates that it can help developers quickly select program elements worthy of investigation while avoiding less interesting ones. © 2008 ACM.",Feature location; Program understanding; Separation of concerns; Software change; Software evolution; Software navigation; Static analysis,Computer networks; Fuzzy sets; Ketones; Set theory; Feature location; Program understanding; Separation of concerns; Software change; Software evolution; Software navigation; Static analysis; Codes (symbols)
Post-release reliability growth in software products,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50649084510&doi=10.1145%2f13487689.13487690&partnerID=40&md5=68ab0c9ed3770aabab2d13b08426a53a,"Most software reliability growth models work under the assumption that reliability of software grows due to the removal of bugs that cause failures. However, another phenomenon has often been observedthe failure rate of a software product following its release decreases with time even if no bugs are corrected. In this article we present a simple model to represent this phenomenon. We introduce the concept of initial transient failure rate of the product and assume that it decays with a factor α per unit time thereby increasing the product reliability with time. When the transient failure rate decays away, the product displays a steady state failure rate. We discuss how the parameters in this modelinitial transient failure rate, decay factor, and steady state failure ratecan be determined from the failure and sales data of a product. We also describe how, using the model, we can determine the product stabilization timea product quality metric that describes how long it takes a product to reach close to its stable failure rate. We provide many examples where this model has been applied to data from released products. © 2008 ACM.",Post-release reliability growth; Product stabilization time,Computer software selection and evaluation; Program debugging; Quality assurance; Failure rates; Post-release reliability growth; Product stabilization time; Software reliability
The role of outcome feedback in improving the uncertainty assessment of software development effort estimates,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50649116352&doi=10.1145%2f13487689.13487693&partnerID=40&md5=9bb78d7c7805fcb2f074f7890d047dce,"Previous studies report that software developers are over-confident in the accuracy of their effort estimates. Aim: This study investigates the role of outcome feedback, that is, feedback about the discrepancy between the estimated and the actual effort, in improving the uncertainty assessments. Method: We conducted two in-depth empirical studies on uncertainty assessment learning. Study 1 included five student developers and Study 2, 10 software professionals. In each study the developers repeatedly assessed the uncertainty of their effort estimates of a programming task, solved the task, and received estimation accuracy outcome feedback. Results: We found that most, but not all, developers were initially over-confident in the accuracy of their effort estimates and remained over-confident in spite of repeated and timely outcome feedback. One important, but not sufficient, condition for improvement based on outcome feedback seems to be the use of explicitly formulated, instead of purely intuition-based, uncertainty assessment strategies. © 2008 ACM.",Cost estimation; Effort prediction intervals; Judgment-based uncertainty assessment; Overconfidence; Software cost estimation; Software development management,Cost estimation; Effort prediction intervals; Judgment-based uncertainty assessment; Overconfidence; Software cost estimation; Software development management; Uncertainty analysis
The impact of research on the development of middleware technology,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50649090010&doi=10.1145%2f13487689.13487692&partnerID=40&md5=73970383263d77d1753d8b3e862d5ecc,"The middleware market represents a sizable segment of the overall Information and Communication Technology market. In 2005, the annual middleware license revenue was reported by Gartner to be in the region of $8.5 billion. In this article we address the question whether research had any involvement in the creation of the technology that is being sold in this market We attempt a scholarly discourse. We present the research method that we have applied to answer this question. We then present a brief introduction into the key middleware concepts that provide the foundation for this market. It would not be feasible to investigate any possible impact that research might have had. Instead we select a few very successful technologies that are representative for the middleware market as a whole and show the existence of impact of research results in the creation of these technologies. We investigate the origins of Web services middleware, distributed transaction processing middleware, message-oriented middleware, distributed object middleware and remote procedure call systems. For each of these technologies we are able to show ample influence of research and conclude that without the research conducted by PhD students and researchers in university computer science labs at Brown, CMU, Cambridge, Newcastle, MIT, Vrije, and University of Washington as well as research in industrial labs at APM, AT&T Bell Labs, DEC Systems Research, HP Labs, IBM Research, and Xerox PARC we would not have middleware technology in its current form. We summarise the article by distilling lessons that can be learnt from this evidenced impact for future technology transfer undertakings. © 2008 ACM.",,Communication; Computer networks; Computer science; Computer software; Information services; Laboratories; Marketing; Technological forecasting; Technology transfer; Gartner; Middleware technologies; Research results; Web services; Technology
An upper bound on software testing effectiveness,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449086982&doi=10.1145%2f1363102.1363107&partnerID=40&md5=658dfab4683ae82796c169874d302c53,"Failure patterns describe typical ways in which inputs revealing program failure are distributed across the input domainin many cases, clustered together in contiguous regions. Based on these observations several debug testing methods have been developed. We examine the upper bound of debug testing effectiveness improvements possible through making assumptions about the shape, size and orientation of failure patterns. We consider the bounds for testing strategies with respect to minimizing the F-measure, maximizing the P-measure, and maximizing the E-measure. Surprisingly, we find that the empirically measured effectiveness of some existing methods that are not based on these assumptions is close to the theoretical upper bound of these strategies. The assumptions made to obtain the upper bound, and its further implications, are also examined. © 2008 ACM.",Adaptive random testing; Failure patterns; Failure-causing inputs; Random testing; Software testing; Testing effectiveness metrics,Computer software selection and evaluation; (1 1 1) orientation; Debug testing; F measure; Failure patterns; Testing strategies; Upper bounds; Software testing
Functional size measurement revisited,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449121855&doi=10.1145%2f1363102.1363106&partnerID=40&md5=878f1b13f4bd45bcae482ee112ab6952,"There are various approaches to software size measurement. Among these, the metrics and methods based on measuring the functionality attribute have become widely used since the original method was introduced in 1979. Although functional size measurement methods have gone a long way, they still provide challenges for software managers. This article identifies improvement opportunities based on empirical studies we performed on ongoing projects. We also compare our findings with the extended dataset provided by the International Software Benchmarking Standards Group (ISBSG). © 2008 ACM.",COSMIC FFP; Functional size measurement; MkII FPA; Software benchmarking; Software estimation,Standards; Data sets; Empirical studies; Functional size measurement; Functional size measurement (FSM) methods; International (CO); Metrics (CO); Software managers; Software sizes; Web services
An empirical investigation of software reuse benefits in a large telecom product,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449118849&doi=10.1145%2f1363102.1363104&partnerID=40&md5=9ed24211fb1d78a9bd9cd5550812a9c4,Background. This article describes a case study on the benefits of software reuse in a large telecom product. The reused components were developed in-house and shared in a product-family approach. Methods. Quantitative data mined from company repositories are combined with other quantitative data and qualitative observations. Results. We observed significantly lower fault density and less modified code between successive releases of the reused components. Reuse and standardization of software architecture and processes allowed easier transfer of development when organizational changes happened. Conclusions. The study adds to the evidence of quality benefits of large-scale reuse programs and explores organizational motivations and outcomes. © 2008 ACM.,Fault density; Product family; Risks; Software reuse; Standardization,Industrial engineering; Labeling; Labels; Software architecture; Standardization; case studies; Empirical investigation; Organizational changes; Qualitative observations; Quantitative data; Software Re-use; TeleCOM (CO); Computer software reusability
Developing and debugging algebraic specifications for Java classes,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449105771&doi=10.1145%2f1363102.1363105&partnerID=40&md5=faa247e41c310c3e1eed2fe885390406,"Modern programs make extensive use of reusable software libraries. For example, a study of a number of large Java applications shows that between 17% and 30% of the classes in those applications use container classes defined in the java.util package. Given this extensive code reuse in Java programs, it is important for the interfaces of reusable classes to be well documented. An interface is well documented if it satisfies the following requirements: (1) the documentation completely describes how to use the interface; (2) the documentation is clear; (3) the documentation is unambiguous; and (4) any deviation between the documentation and the code is machine detectable. Unfortunately, documentation in natural language, which is the norm, does not satisfy the above requirements. Formal specifications can satisfy them but they are difficult to develop, requiring significant effort on the part of programmers. To address the practical difficulties with formal specifications, we describe and evaluate a tool to help programmers write and debug algebraic specifications. Given an algebraic specification of a class, our interpreter generates a prototype that can be used within an application like a regular Java class. When running an application that uses the prototype, the interpreter prints error messages that tell the developer in which way the specification is incomplete or inconsistent with a hand-coded implementation of the class. We use case studies to demonstrate the usefulness of our system. © 2008 ACM.",Algebraic interpretation; Algebraic specifications; Specification discovery,Boolean algebra; Codes (standards); Codes (symbols); Computer programming languages; Computer software; Computer software reusability; Matrix algebra; Program interpreters; Software prototyping; Specifications; System program documentation; (SPM) classes; Algebraic specifications; Code Reuse (CR); Error messages; Formal specifications; JAVA applications; Java classes; JAVA programs; natural languages; Reusable softwares; Use cases; Java programming language
ACM Transactions on Software Engineering and Methodology: Editorial,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449084065&doi=10.1145%2f1363102.1363103&partnerID=40&md5=dda863ae73943db59e44b94c644c1d53,[No abstract available],,
Breaking up is hard to do: An evaluation of automated assume-guarantee reasoning,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43449095314&doi=10.1145%2f1348250.1348253&partnerID=40&md5=3eed8cdaf8ea11668c77c4e0bb6b9858,"Finite-state verification techniques are often hampered by the state-explosion problem. One proposed approach for addressing this problem is assume-guarantee reasoning, where a system under analysis is partitioned into subsystems and these subsystems are analyzed individually. By composing the results of these analyses, it can be determined whether or not the system satisfies a property. Because each subsystem is smaller than the whole system, analyzing each subsystem individually may reduce the overall cost of verification. Often the behavior of a subsystem is dependent on the subsystems with which it interacts, and thus it is usually necessary to provide assumptions about the environment in which a subsystem executes. Because developing assumptions has been a difficult manual task, the evaluation of assume-guarantee reasoning has been limited. Using recent advances for automatically generating assumptions, we undertook a study to determine if assume-guarantee reasoning provides an advantage over monolithic verification. In this study, we considered all two-way decompositions for a set of systems and properties, using two different verifiers, FLAVERS and LTSA. By increasing the number of repeated tasks in these systems, we evaluated the decompositions as they were scaled. We found that in only a few cases can assume-guarantee reasoning verify properties on larger systems than monolithic verification can, and in these cases the systems that can be analyzed are only a few sizes larger. Although these results are discouraging, they provide insight about research directions that should be pursued and highlight the importance of experimental evaluation in this area. © 2008 ACM.",Assume-guarantee reasoning,Finite automata; Problem solving; Systems analysis; Assume guarantee reasoning; Finite state verification; Verification
Combining symbolic execution with model checking to verify parallel numerical programs,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43449116062&doi=10.1145%2f1348250.1348256&partnerID=40&md5=5e91e05b6a821bc796f44c77928dd1a1,"We present a method to verify the correctness of parallel programs that perform complex numerical computations, including computations involving floating-point arithmetic. This method requires that a sequential version of the program be provided, to serve as the specification for the parallel one. The key idea is to use model checking, together with symbolic execution, to establish the equivalence of the two programs. In this approach the path condition from symbolic execution of the sequential program is used to constrain the search through the parallel program. To handle floating-point operations, three different types of equivalence are supported. Several examples are presented, demonstrating the approach and actual errors that were found. Limitations and directions for future research are also described. © 2008 ACM.",Concurrency; Finite-state verification; Floating-point; High performance computing; Message Passing Interface; Model checking; MPI; Numerical program; Parallel programming; Spin; Symbolic execution,Codes (symbols); Message passing; Parallel programming; Specifications; Finite state verification; High performance computing; Symbolic execution; Model checking
ACM Transactions on Software Engineering and Methodology: Editorial,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43449084353&doi=10.1145%2f1348250.1348251&partnerID=40&md5=7d18df90e1ea5882fae7bc4184c83fbb,[No abstract available],,
Effective typestate verification in the presence of aliasing,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43449124115&doi=10.1145%2f1348250.1348255&partnerID=40&md5=2a162d52daf769666f311de8047ec846,"This article addresses the challenge of sound typestate verification, with acceptable precision, for real-world Java programs. We present a novel framework for verification of typestate properties, including several new techniques to precisely treat aliases without undue performance costs. In particular, we present a flow-sensitive, context-sensitive, integrated verifier that utilizes a parametric abstract domain combining typestate and aliasing information. To scale to real programs without compromising precision, we present a staged verification system in which faster verifiers run as early stages which reduce the workload for later, more precise, stages. We have evaluated our framework on a number of real Java programs, checking correct API usage for various Java standard libraries. The results show that our approach scales to hundreds of thousands of lines of code, and verifies correctness for 93% of the potential points of failure. © 2008 ACM.",Alias analysis; Program verification; Typestate,Codes (symbols); Information analysis; Java programming language; Alias analysis; Program verification; Verification
DSD-Crasher: A hybrid analysis tool for bug finding,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43449101485&doi=10.1145%2f1348250.1348254&partnerID=40&md5=6e5a667796d42af1961f4f564ab46819,"DSD-Crasher is a bug finding tool that follows a three-step approach to program analysis: D. Capture the program's intended execution behavior with dynamic invariant detection. The derived invariants exclude many unwanted values from the program's input domain. S. Statically analyze the program within the restricted input domain to explore many paths. D. Automatically generate test cases that focus on reproducing the predictions of the static analysis. Thereby confirmed results are feasible. This three-step approach yields benefits compared to past two-step combinations in the literature. In our evaluation with third-party applications, we demonstrate higher precision over tools that lack a dynamic step and higher efficiency over tools that lack a static step. © 2008 ACM.",Automatic testing; Bug finding; Dynamic analysis; Dynamic invariant detection; Extended static checking; False positives; Static analysis; Test case generation; Usability,Static analysis; Usability engineering; Bug finding; Dynamic invariant detection; Extended static checking; Software testing
Introduction to the special section from the ACM international symposium on software testing and analysis (ISSTA 2006),2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43449092184&doi=10.1145%2f1348250.1348252&partnerID=40&md5=28aef165bf0f5a075aac29310c23f03f,[No abstract available],,
Impact of classes of development coordination tools on software development performance: A multinational empirical study,2008,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43449102487&doi=10.1145%2f1348250.1348257&partnerID=40&md5=2ab4188cdabf37f93836d5dcbbc4ff1f,"Although a diverse variety of software development coordination tools are widely used in practice, considerable debate surrounds their impact on software development performance. No large-scale field research has systematically examined their impact on software development performance. This paper reports the results of a multinational field study of software projects in 209 software development organizations to empirically examine the influence of six key classes of development coordination tools on the efficiency (reduction of development rework, budget compliance) and effectiveness (defect reduction) of software development performance. Based on an in-depth field study, the article conceptualizes six holistic classes of development coordination tools. The results provide nuanced insights - -some counter to prevailing beliefs - -into the relationships between the use of various classes of development coordination tools and software development performance. The overarching finding is that the performance benefits of development coordination tools are contingent on the salient types of novelty in a project. The dimension of development performance - -efficiency or effectiveness - -that each class of tools is associated with varies systematically with whether a project involves conceptual novelty, process novelty, multidimensional novelty (both process and conceptual novelty), or neither. Another noteworthy insight is that the use of some classes of tools introduces an efficiency-effectiveness tradeoff. Collectively, the findings are among the first to offer empirical support for the varied performance impacts of various classes of development coordination tools and have important implications for software development practice. The paper also identifies several promising areas for future research. © 2008 ACM.",Collaborative software engineering. Software outsourcing; Coordination; Development coordination tools; Development tools; Efficiency effectiveness tradeoff; Empirical study; Field study; Knowledge integration; Knowledge management; Software development,Knowledge management; Outsourcing; Project management; Efficiency effectiveness tradeoff; Empirical study; Field study; Software engineering
An empirical study of slice-based cohesion and coupling metrics,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349154910&doi=10.1145%2f1314493.1314495&partnerID=40&md5=9e810e802d39c400e25c3acd95a9bf32,"Software reengineering is a costly endeavor, due in part to the ambiguity of where to focus reengineering effort. Coupling and Cohesion metrics, particularly quantitative cohesion metrics, have the potential to aid in this identification and to measure progress. The most extensive work on such metrics is with slice-based cohesion metrics. While their use of semantic dependence information should make them an excellent choice for cohesion measurement, their wide spread use has been impeded in part by a lack of empirical study. Recent advances in software tools make, for the first time, a large-scale empirical study of slice-based cohesion and coupling metrics possible. Four results from such a study are presented. First, head-to-head qualitative and quantitative comparisons of the metrics identify which metrics provide similar views of a program and which provide unique views of a program. This study includes statistical analysis showing that slice-based metrics are not proxies for simple size-based metrics such as lines of code. Second, two longitudinal studies show that slice-based metrics quantify the deterioration of a program as it ages. This serves to validate the metrics: the metrics quantify the degradation that exists during development; turning this around, the metrics can be used to measure the progress of a reengineering effort. Third, baseline values for slice-based metrics are provided. These values act as targets for reengineering efforts with modules having values outside the expected range being the most in need of attention. Finally, slice-based coupling is correlated and compared with slice-based cohesion. © 2007 ACM.",Cohesion; Coupling; Reengineering; Slicing; Software intervention,Identification (control systems); Large scale systems; Reengineering; Statistical methods; Cohesion metrics; Software intervention; Computer aided software engineering
Editorial,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349140989&doi=10.1145%2f1314493.1314494&partnerID=40&md5=4c688b1d303a4057a31ec0f3d4479978,[No abstract available],,
Identifying crosscutting concerns using fan-in analysis,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349160454&doi=10.1145%2f1314493.1314496&partnerID=40&md5=c1b72050e3931b881b1581f3d41dd3fb,"Aspect mining is a reverse engineering process that aims at finding crosscutting concerns in existing systems. This article proposes an aspect mining approach based on determining methods that are called from many different places, and hence have a high fan-in, which can be seen as a symptom of crosscutting functionality. The approach is semiautomatic, and consists of three steps: metric calculation, method filtering, and call site analysis. Carrying out these steps is an interactive process supported by an Eclipse plug-in called FINT. Fan-in analysis has been applied to three open source Java systems, totaling around 200,000 lines of code. The most interesting concerns identified are discussed in detail, which includes several concerns not previously discussed in the aspect-oriented literature. The results show that a significant number of crosscutting concerns can be recognized using fan-in analysis, and each of the three steps can be supported by tools. © 2007 ACM.",Aspect-oriented programming; Crosscutting concerns; Fan-in metric; Reverse engineering,Data mining; Identification (control systems); Java programming language; Object oriented programming; Aspect-oriented literature; Crosscutting concerns; Fan-in metric; Reverse engineering
Efficient analysis of DynAlloy specifications,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349161653&doi=10.1145%2f1314493.1314497&partnerID=40&md5=523126f7173d0423a495c65aa43f6ef4,"DynAlloy is an extension of Alloy to support the definition of actions and the specification of assertions regarding execution traces. In this article we show how we can extend the Alloy tool so that DynAlloy specifications can be automatically analyzed in an efficient way. We also demonstrate that DynAlloy's semantics allows for a sound technique that we call program atomization, which improves the analyzability of properties regarding execution traces by considering certain programs as atomic steps in a trace. We present the foundations, case studies, and empirical results indicating that the analysis of DynAlloy specifications can be performed efficiently. © 2007 ACM.",Alloy; Dynamic logic; Software specification; Software validation,Atomization; Semantics; Software engineering; DynAlloy specifications; Dynamic logic; Alloys
Recovering traceability links in software artifact management systems using information retrieval methods,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34648836593&doi=10.1145%2f1276933.1276934&partnerID=40&md5=709030275fbbb179919d8906f861b3ac,"The main drawback of existing software artifact management systems is the lack of automatic or semi-automatic traceability link generation and maintenance. We have improved an artifact management system with a traceability recovery tool based on Latent Semantic Indexing (LSI), an information retrieval technique. We have assessed LSI to identify strengths and limitations of using information retrieval techniques for traceability recovery and devised the need for an incremental approach. The method and the tool have been evaluated during the development of seventeen software projects involving about 150 students. We observed that although tools based on information retrieval provide a useful support for the identification of traceability links during software development, they are still far to support a complete semi-automatic recovery of all links. The results of our experience have also shown that such tools can help to identify quality problems in the textual description of traced artifacts. © 2007 ACM.",Impact analysis; Latent semantic indexing; Software artifact management; Traceability management,Computer simulation; Indexing (of information); Information retrieval; Management information systems; Problem solving; Semantics; Impact analysis; Latent semantic indexing; Semi-automatic recovery; Software artifact management; Software engineering
Static checking of dynamically generated queries in database applications,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34648836594&doi=10.1145%2f1276933.1276935&partnerID=40&md5=1b48ce43088b3a7be04f32ec350e136a,"Many data-intensive applications dynamically construct queries in response to client requests and execute them. Java servlets, for example, can create strings that represent SQL queries and then send the queries, using JDBC, to a database server for execution. The servlet programmer enjoys static checking via Java's strong type system. However, the Java type system does little to check for possible errors in the dynamically generated SQL query strings. Thus, a type error in a generated selection query (e.g., comparing a string attribute with an integer) can result in an SQL runtime exception. Currently, such defects must be rooted out through careful testing, or (worse) might be found by customers at runtime. In this article, we present a sound, static program analysis technique to verify that dynamically generated query strings do not contain type errors. We describe our analysis technique and provide soundness results for our static analysis algorithm. We also describe the details of a prototype tool based on the algorithm and present several illustrative defects found in senior software-engineering student-team projects, online tutorial examples, and a real-world purchase order system written by one of the authors. © 2007 ACM.",Context-free language reachability; Database queries; JDBC; Static checking,Algorithms; Error analysis; Java programming language; Project management; Query processing; Software prototyping; Static analysis; Students; Data-intensive applications; Servlet programmer; Static checking; Student-team projects; Database systems
Three empirical studies on estimating the design effort of Web applications,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34648831105&doi=10.1145%2f1276933.1276936&partnerID=40&md5=5ba43abd05c55120e9b800f067a37ca6,"Our research focuses on the effort needed for designing modern Web applications. The design effort is an important part of the total development effort, since the implementation can be partially automated by tools. We carried out three empirical studies with students of advanced university classes enrolled in engineering and communication sciences curricula. The empirical studies are based on the use of W2000, a special-purpose design notation for the design of Web applications, but the hypotheses and results may apply to a wider class of modeling notations (e.g., OOHDM, WebML, or UWE). We started by investigating the relative importance of each design activity. We then assessed the accuracy of a priori design effort predictions and the influence of a few process-related factors on the effort needed for each design activity. We also analyzed the impact of attributes like the size and complexity of W2000 design artifacts on the total effort needed to design the user experience of web applications. In addition, we carried out a finer-grain analysis, by studying which of these attributes impact the effort devoted to the steps of the design phase that are followed when using W2000. © 2007 ACM.",Effort estimation; Empirical study; W2000; Web application design,Computer aided design; Computer simulation; Curricula; Information science; Software engineering; Effort estimation; Empirical study; Finer-grain analysis; W2000 design artifacts; Web application design; World Wide Web
Model checking the Java metalocking algorithm,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547358851&doi=10.1145%2f1243987.1243990&partnerID=40&md5=f0f20347687698eb692795063fb9ec62,"We report on our efforts to use the XMC model checker to model and verify the Java metalocking algorithm. XMC [Ramakrishna et al. 1997] is a versatile and efficient model checker for systems specified in XL, a highly expressive value-passing language. Metalocking [Agesen et al. 1999] is a highly-optimized technique for ensuring mutually exclusive access by threads to object monitor queues and, therefore; plays an essential role in allowing Java to offer concurrent access to objects. Metalocking can be viewed as a two-tiered scheme. At the upper level, the metalock level, a thread waits until it can enqueue itself on an object's monitor queue in a mutually exclusive manner. At the lower level, the monitor-lock level, enqueued threads race to obtain exclusive access to the object. Our abstract XL specification of the metalocking algorithm is fully parameterized, both on the number of threads M, and the number of objects N. It also captures a sophisticated optimization of the basic metalocking algorithm known as extra-fast locking and unlocking of uncontended objects. Using XMC, we show that for a variety of values of M and N, the algorithm indeed provides mutual exclusion and freedom from deadlock and lockout at the metalock level. We also show that, while the monitor-lock level of the protocol preserves mutual exclusion and deadlock-freedom, it is not lockout-free because the protocol's designers chose to give equal preference to awaiting threads and newly arrived threads. © 2007 ACM.",Java; Metalocking; Monitor queues; Mutual exclusion; Synchronized methods; XMC,Java programming language; Learning algorithms; Object oriented programming; Parameterization; Synchronization; Metalocking; Monitor queues; Mutual exclusion; Synchronized methods; XMC; Software testing
Metamodel-based model conformance and multiview consistency checking,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547245302&doi=10.1145%2f1243987.1243989&partnerID=40&md5=382707d0a89eb23a9d869d1ed4d96dbd,"Model-driven development, using languages such as UML and BON, often makes use of multiple diagrams (e.g., class and sequence diagrams) when modeling systems. These diagrams, presenting different views of a system of interest, may be inconsistent. A metamodel provides a unifying framework in which to ensure and check consistency, while at the same time providing the means to distinguish between valid and invalid models, that is, conformance. Two formal specifications of the metamodel for an object-oriented modeling language are presented, and it is shown how to use these specifications for model conformance and multiview consistency checking. Comparisons are made in terms of completeness and the level of automation each provide for checking multiview consistency and model conformance. The lessons learned from applying formal techniques to the problems of metamodeling, model conformance, and multiview consistency checking are summarized. © 2007 ACM.",Automated verification; Formal methods; Metamodeling; Multiview consistency,Automation; Computer programming languages; Object oriented programming; Software testing; Automated verification; Metamodeling; Multiview consistency; Formal methods
Test conditions for fault classes in Boolean specifications,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547272404&doi=10.1145%2f1243987.1243988&partnerID=40&md5=46f60213429d799d406ae922122ec00e,"Fault-based testing of software checks the software implementation for a set of faults. Two previous papers on fault-based testing [Kuhn 1999; Tsuchiya and Kikuno 2002] represent the required behavior of the software as a Boolean specification represented in Disjunctive Normal Form (DNF) and then show that faults may be organized in a hierarchy. This article extends these results by identifying necessary and sufficient conditions for fault-based testing. Unlike previous solutions, the formal analysis used to derive these conditions imposes no restrictions (such as DNF) on the form of the Boolean specification. © 2007 ACM.",Boolean specification; Fault classes; Fault-based testing,Boolean functions; Hierarchical systems; Software testing; Boolean specification; Fault-based testing; Fault tolerant computer systems
An empirical study of static program slice size,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247854816&doi=10.1145%2f1217295.1217297&partnerID=40&md5=0dc866255984cb6a40a42eb2571b8352,"This article presents results from a study of all slices from 43 programs, ranging up to 136,000 lines of code in size. The study investigates the effect of five aspects that affect slice size. Three slicing algorithms are used to study two algorithmic aspects: calling-context treatment and slice granularity. The remaining three aspects affect the upstream dependencies considered by the slicer. These include collapsing structure fields, removal of dead code, and the influence of points-to analysis. The results show that for the most precise slicer, the average slice contains just under one-third of the program. Furthermore, ignoring calling context causes a 50% increase in slice size, and while (coarse-grained) function-level slices are 33% larger than corresponding statement-level slices, they may be useful predictors of the (finer-grained) statement-level slice size. Finally, upstream analyses have an order of magnitude less influence on slice size. © 2007 ACM.",Program slicing; Slice size,Codes (symbols); Computer programming; Data reduction; Data structures; Learning algorithms; Program slicing; Slice granularity; Slice size; Computer software
ACM Transactions on Software Engineering and Methodology: Editorial,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247880215&doi=10.1145%2f1217295.1237801&partnerID=40&md5=797e0cb3fc9270472d6613d18d2f1065,[No abstract available],,
Polychronous design of embedded real-time applications,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247867910&doi=10.1145%2f1217295.1217298&partnerID=40&md5=5c1752071cdaa8ca48c598a1e2e0439e,"Embedded real-time systems consist of hardware and software that controls the behavior of a device or plant. They are ubiquitous in today's technological landscape and found in domains such as telecommunications, nuclear power, avionics, and medical technology. These systems are difficult to design and build because they must satisfy both functional and timing requirements to work correctly in their intended environment. Furthermore, embedded systems are often critical systems, where failure can lead to loss of life, loss of mission, or serious financial consequences. Because of the difficulty in creating these systems and the consequences of failure, they require rigorous and reliable design approaches. The synchronous approach is one possible answer to this demand. Its mathematical basis provides formal concepts that favor the trusted design of embedded real-time systems. The multiclock or polychronous model stands out from other synchronous specification models by its capability to enable the design of systems where each component holds its own activation clock as well as single-clocked systems in a uniform way. A great advantage is its convenience for component-based design approaches that enable modular development of increasingly complex modern systems. The expressiveness of its underlying semantics allows dealing with several issues of real-time design. This article exposes insights gained during recent years from the design of real-time applications within the polychronous framework. In particular, it shows promising results about the design of applications from the avionics domain. © 2007 ACM.",Avionics; IMA; Signal; Synchronous approach,Avionics; Computer software; Medical applications; Nuclear energy; Real time systems; Semantics; Real time design; Single clocked systems; Synchronous approach; Embedded systems
Foundations of incremental aspect model-checking,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247871252&doi=10.1145%2f1217295.1217296&partnerID=40&md5=3765fe5bb9b4bb67610d7ca7af7fac16,"Programs are increasingly organized around features, which are encapsulated using aspects and other linguistic mechanisms. Despite their growing popularity amongst developers, there is a dearth of techniques for computer-aided verification of programs that employ these mechanisms. We present the theoretical underpinnings for applying model checking to programs (expressed as state machines) written using these mechanisms. The analysis is incremental, examining only components that change rather than verifying the entire system every time one part of it changes. Our technique assumes that the set of pointcut designators is known statically, but the actual advice can vary. It handles both static and dynamic pointcut designators. We present the algorithm, prove it sound, and address several subtleties that arise, including cascading advice application and problems of circular reasoning. © 2007 ACM.",Aspect-oriented programming; Feature-oriented software; Incremental verification; Model checking; Modular verification,Algorithms; Computer programming; Software engineering; Aspect oriented programming; Feature oriented software; Incremental verification; Modular verification; Model checking
ACM Transactions on Software Engineering and Methodology: Editorial,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847672833&doi=10.1145%2f1189748.1189749&partnerID=40&md5=a242142eadaefb1d5fa8ad09182887cf,[No abstract available],,
Designing and comparing automated test oracles for GUI-based software applications,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847680195&doi=10.1145%2f1189748.1189752&partnerID=40&md5=32508708ec5d7d3444ee2b80b39d7559,"Test designers widely believe that the overall effectiveness and cost of software testing depends largely on the type and number of test cases executed on the software. This article shows that the test oracle, a mechanism that determines whether a software is executed correctly for a test case, also significantly impacts the fault detection effectiveness and cost of a test case. Graphical user interfaces (GUIs), which have become ubiquitous for interacting with today's software, have created new challenges for test oracle development. Test designers manually assert the expected values of specific properties of certain GUI widgets in each test case; during test execution, these assertions are used as test oracles to determine whether the GUI executed correctly. Since a test case for a GUI is a sequence of events, a test designer must decide: (1) what to assert; and (2) how frequently to check an assertion, for example, after each event in the test case or after the entire test case has completed execution. Variations of these two factors significantly impact the fault-detection ability and cost of a GUI test case. A technique to declaratively specify different types of automated GUI test oracles is described. Six instances of test oracles are developed and compared in an experiment on four software systems. The results show that test oracles do affect the fault detection ability of test cases in different and interesting ways: (1) Test cases significantly lose their fault detection ability when using weak test oracles; (2) in many cases, invoking a thorough oracle at the end of test case execution yields the best cost-benefit ratio; (3) certain test cases detect faults only if the oracle is invoked during a small window of opportunity during test execution; and (4) using thorough and frequently-executing test oracles can compensate for not having long test cases.",Graphical user interfaces; GUI state; GUI testing; Test oracles; User interfaces; Widgets,Automata theory; Computer software; Database systems; Software engineering; Software testing; Test oracles; Widgets; Graphical user interfaces
Representing concerns in source code,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847667616&doi=10.1145%2f1189748.1189751&partnerID=40&md5=68aa74c7dddecf26d2accfef2f7151e4,"A software modification task often addresses several concerns. A concern is anything a stakeholder may want to consider as a conceptual unit, including features, nonfunctional requirements, and design idioms. In many cases, the source code implementing a concern is not encapsulated in a single programming language module, and is instead scattered and tangled throughout a system. Inadequate separation of concerns increases the difficulty of evolving software in a correct and cost-effective manner. To make it easier to modify concerns that are not well modularized, we propose an approach in which the implementation of concerns is documented in artifacts, called concern graphs. Concern graphs are abstract models that describe which parts of the source code are relevant to different concerns. We present a formal model for concern graphs and the tool support we developed to enable software developers to create and use concern graphs during software evolution tasks. We report on five empirical studies, providing evidence that concern graphs support views and operations that facilitate the task of modifying the code implementing scattered concerns, are cost-effective to create and use, and robust enough to be used with different versions of a software system.",Aspect-oriented software development; Concern modeling; Java; Separation of concerns; Software evolution,Codes (symbols); Computer simulation; Cost effectiveness; Feature extraction; Graph theory; Java programming language; Scattering; Aspect oriented software development; Concern modeling; Software evolution; Source code; Stakeholders; Software engineering
A formal model of services,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847685370&doi=10.1145%2f1189748.1189753&partnerID=40&md5=ca62dfa9953ed857d829c7624eb497ca,"Service-oriented software systems rapidly gain importance across application domains: They emphasize functionality (services), rather structural entities (components), as the basic building block for system composition. More specifically, services coordinate the interplay of components to accomplish specific tasks. In this article, we establish a foundation of service orientation: Based on the Focus theory of distributed systems (see Broy and Stlen [2001]), we introduce a theory and formal model of services. In Focus, systems are composed of interacting components. A component is a total behavior. We introduce a formal model of services where, in contrast, a service is a partial behavior. For services and components, we work out foundational specification techniques and outline methodological development steps. We show how services can be structured and how software architectures can be composed of services and components. Although our emphasis is on a theoretical foundation of the notion of services, we demonstrate utility of the concepts we introduce by means of a running example from the automotive domain.",Assumption/commitment specifications; Components; Service engineering; Services; Software architecture,Automotive engineering; Codes (symbols); Distributed computer systems; Mathematical models; Software architecture; Specifications; Commitment specifications; Foundational specification techniques; Service oriented software; Software engineering
ACM Transactions on Software Engineering and Methodology: Editorial,2007,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847663900&doi=10.1145%2f1189748.1189750&partnerID=40&md5=7943e8f23394e1a1444b9f64fb7c7866,[No abstract available],,
Process modeling in web applications,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750930107&doi=10.1145%2f1178625.1178627&partnerID=40&md5=d7c7751caa0cd5e8b0c14341db52a482,"While Web applications evolve towards ubiquitous, enterprise-wide or multienterprise information systems, they face new requirements, such as the capability of managing complex processes spanning multiple users and organizations, by interconnecting software provided by different organizations. Significant efforts are currently being invested in application integration, to support the composition of business processes of different companies, so as to create complex, multiparty business scenarios. In this setting, Web applications, which were originally conceived to allow the user-to-system dialogue, are extended with Web services, which enable system-to-system interaction, and with process control primitives, which permit the implementation of the required business constraints. This article presents new Web engineering methods for the high-level specification of applications featuring business processes and remote services invocation. Process- and service-enabled Web applications benefit from the high-level modeling and automatic code generation techniques that have been fruitfully applied to conventional Web applications, broadening the class of Web applications that take advantage of these powerful software engineering techniques. All the concepts presented in this article are fully implemented within a CASE tool. © 2006 ACM.",Conceptual modeling; Web applications; Web engineering; Worfklows,Administrative data processing; Codes (symbols); Computer software; Information science; Societies and institutions; User interfaces; Conceptual modeling; Web applications; Web engineering; Worfklows; World Wide Web
Wrapper-based evolution of legacy information systems,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750898776&doi=10.1145%2f1178625.1178626&partnerID=40&md5=27328f61f81cc24f57e2e2bd69456a72,"System evolution most often implies the integration of legacy components, such as databases, with newly developed ones, leading to mixed architectures that suffer from severe heterogeneity problems. For instance, incorporating a new program in a legacy database application can create an integrity mismatch, since the database model and the program data view can be quite different (e.g. standard file model versus OO model). In addition, neither the legacy DBMS (too weak to address integrity issues correctly) nor the new program (that relies on data server responsibility) correctly cope with data integrity management. The component that can reconciliate these mismatched subsystems is the R / W wrapper, which allows any client program to read, but also to update the legacy data, while controlling the integrity constraints that are ignored by the legacy DBMS. This article describes a generic, technology-independent, R/W wrapper architecture, a methodology for specifying them in a disciplined way, and a CASE tool for generating most of the corresponding code. The key concept is that of implicit construct, which is a structure or a constraint that has not been declared in the database, but which is controlled by the legacy application code. The implicit constructs are elicited through reverse engineering techniques, and then translated into validation code in the wrapper. For instance, a wrapper can be generated for a collection of COBOL files in order to allow external programs to access them through a relational, object-oriented or XML interface, while offering referential integrity control. The methodology is based on a transformational approach that provides a formal way to build the wrapper schema and to specify inter-schema mappings. © 2006 ACM.",CASE tool; Data consistency; Data reverse-engineering; Evolution; Legacy database; Schema transformation; Wrapper,COBOL (programming language); Computer architecture; Computer programming; Database systems; Evolutionary algorithms; Information science; Problem solving; XML; CASE tool; Data consistency; Data reverse engineering; Schema transformation; Wrapper; Legacy systems
Efficient path conditions in dependence graphs for software safety analysis,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750912954&doi=10.1145%2f1178625.1178628&partnerID=40&md5=9b21fd9fd2ddd6d596363998a8d2bd7c,"A new method for software safety analysis is presented which uses program slicing and constraint solving to construct and analyze path conditions, conditions defined on a program's input variables which must hold for information flow between two points in a program. Path conditions are constructed from subgraphs of a program's dependence graph, specifically, slices and chops. The article describes how constraint solvers can be used to determine if a path condition is satisfiable and, if so, to construct a witness for a safety violation, such as an information flow from a program point at one security level to another program point at a different security level. Such a witness can prove useful in legal matters. The article reviews previous research on path conditions in program dependence graphs; presents new extensions of path conditions for arrays, pointers, abstract data types, and multi-threaded programs; presents new decomposition formulae for path conditions; demonstrates how interval analysis and BDDs (binary decision diagrams) can be used to reduce the scalability problem for path conditions; and presents case studies illustrating the use of path conditions in safety analysis. Applying interval analysis and BDDs is shown to overcome the combinatorial explosion that can occur in constructing path conditions. Case studies and empirical data demonstrate the usefulness of path conditions for analyzing practical programs, in particular, how illegal influences on safety-critical programs can be discovered and analyzed. © 2006 ACM.",Information flow control; Path condition; Program slicing; Safety analysis,Combinatorial mathematics; Computer programming; Computer software; Information analysis; Problem solving; Security of data; Information flow control; Path condition; Program slicing; Software safety analysis; Graph theory
HOTTest: A model-based test design technique for enhanced testing of domain-specific applications,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748528112&doi=10.1145%2f1151695.1151697&partnerID=40&md5=396688bb148f87f73f3face6aa31322c,"Model-based testing is an effective black-box test generation technique for applications. Existing model-based testing techniques, however, fail to capture implicit domain-specific properties, as they overtly rely on software artifacts such as design documents, requirement specifications, etc., for completeness of the test model. This article presents a technique, HOTTest, which uses a strongly typed domain-specific language to model the system under test. This allows extraction of type-related system invariants, which can be related to various domain-specific properties of the application. Thus, using HOTTest, it is possible to automatically extract and embed domain-specific requirements into the test models. In this article we describe HOTTest, its principles and methodology, and how it is possible to relate domain-specific properties to specific type constraints. HOTTest is described using the example of HaskellDB, which is a Haskell-based embedded domain-specific language for relational databases. We present an example application of the technique and compare the results to some other commonly used Model-based test automation techniques like ASML-based testing, UML-based testing, and EFSM-based testing. © 2006 ACM.",Database-specific test case generation; Domain-specific languages; Domain-specific testing; Haskell; HaskellDB; Model-based testing; Test case generation; Test generation tools,Automation; Computer programming languages; Database systems; Embedded systems; Mathematical models; Database-specific test case generation; Domain-specific languages; Domain-specific testing; HaskellDB; Haskells; Model-based testing; Test case generation; Test generation tools; Software engineering
LIME: A coordination model and middleware supporting mobility of hosts and agents,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748561143&doi=10.1145%2f1151695.1151698&partnerID=40&md5=969638976bdce49d0d0d0f46a415b4cc,"LIME (Linda in a mobile environment) is a model and middleware supporting the development of applications that exhibit the physical mobility of hosts, logical mobility of agents, or both. LIME adopts a coordination perspective inspired by work on the Linda model. The context for computation, represented in Linda by a globally accessible persistent tuple space, is refined in LIME to transient sharing of the identically named tuple spaces carried by individual mobile units. Tuple spaces are also extended with a notion of location and programs are given the ability to react to specified states. The resulting model provides a minimalist set of abstractions that facilitates the rapid and dependable development of mobile applications. In this article we illustrate the model underlying LIME, provide a formal semantic characterization for the operations it makes available to the application developer, present its current design and implementation, and discuss lessons learned in developing applications that involve physical mobility. © 2006 ACM.",Middleware; Mobile computing; Tuple spaces,Computation theory; Computer applications; Mathematical models; Mobile computing; Mobile units; Physical mobility; Tuple spaces; Middleware
Avoiding coincidental correctness in boundary value analysis,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748523455&doi=10.1145%2f1151695.1151696&partnerID=40&md5=e3681ab9b21cab26390c3813c679fc6b,"In partition analysis we divide the input domain to form subdomains on which the system's behaviour should be uniform. Boundary value analysis produces test inputs near each subdomain's boundaries to find failures caused by incorrect implementation of the boundaries. However, boundary value analysis can be adversely affected by coincidental correctness - the system produces the expected output, but for the wrong reason. This article shows how boundary value analysis can be adapted in order to reduce the likelihood of coincidental correctness. The main contribution is to cases of automated test data generation in which we cannot rely on the expertise of a tester. © 2006 ACM.",Boundary value analysis; Coincidental correctness; Domain faults; Test case generation,Data processing; Failure analysis; Systems analysis; Boundary value analysis; Coincidental correctness; Domain faults; Test case generation; Boundary value problems
Integrating automated test generation into the WYSIWYT spreadsheet testing methodology,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745389959&doi=10.1145%2f1131421.1131423&partnerID=40&md5=5210ba282cd0c425195da886ee7b21d4,"Spreadsheet languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that spreadsheets often contain faults. Thus, in previous work we presented a methodology that helps spreadsheet users test their spreadsheet formulas. Our empirical studies have shown that end users can use this methodology to test spreadsheets more adequately and efficiently; however, the process of generating test cases can still present a significant impediment. To address this problem, we have been investigating how to incorporate automated test case generation into our testing methodology in ways that support incremental testing and provide immediate visual feedback. We have used two techniques for generating test cases, one involving random selection and one involving a goal-oriented approach. We describe these techniques and their integration into our testing environment, and report results of an experiment examining their effectiveness and efficiency. © 2006 ACM.",End-user programming; End-user software engineering; Test case generation; Testing,Computation theory; Computer programming languages; Feedback; Spreadsheets; End-user programming; End-user software engineering; Test case generation; Automatic testing
SNIAFL: Towards a static noninteractive approach to feature location,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745348653&doi=10.1145%2f1131421.1131424&partnerID=40&md5=fe2a29d428b64f01f5d0405b631a45b0,"To facilitate software maintenance and evolution, a helpful step is to locate features concerned in a particular maintenance task. In the literature, both dynamic and interactive approaches have been proposed for feature location. In this article, we present a static and noninteractive method for achieving this objective. The main idea of our approach is to use information retrieval (IR) technology to reveal the basic connections between features and computational units in the source code. Due to the imprecision of retrieved connections, we use a static representation of the source code named BRCG (branch-reserving call graph) to further recover both relevant and specific computational units for each feature. A premise of our approach is that programmers should use meaningful names as identifiers. We also performed an experimental study based on two real-world software systems to evaluate our approach. According to experimental results, our approach is quite effective in acquiring the relevant and specific computational units for most features. © 2006 ACM.",BRCG; Feature location; Information retrieval; Program comprehension; Static analysis; Traceability,Computation theory; Identification (control systems); Information retrieval; Interactive computer systems; Statistical methods; BRCG; Feature location; Program comprehension; Traceability; Software engineering
The interpretation and utility of three cohesion metrics for object-oriented design,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745389057&doi=10.1145%2f1131421.1131422&partnerID=40&md5=42f767881632a09ebd7f7ee16bedf54b,"The concept of cohesion in a class has been the subject of various recent empirical studies and has been measured using many different metrics. In the structured programming paradigm, the software engineering community has adopted an informal yet meaningful and understandable definition of cohesion based on the work of Yourdon and Constantine. The object-oriented (OO) paradigm has formalised various cohesion measures, but the argument over the most meaningful of those metrics continues to be debated. Yet achieving highly cohesive software is fundamental to its comprehension and thus its maintainability. In this article we subject two object-oriented cohesion metrics, CAMC and NHD, to a rigorous mathematical analysis in order to better understand and interpret them. This analysis enables us to offer substantial arguments for preferring the NHD metric to CAMC as a measure of cohesion. Furthermore, we provide a complete understanding of the behaviour of these metrics, enabling us to attach a meaning to the values calculated by the CAMC and NHD metrics. In addition, we introduce a variant of the NHD metric and demonstrate that it has several advantages over CAMC and NHD. While it may be true that a generally accepted formal and informal definition of cohesion continues to elude the OO software engineering community, there seems considerable value in being able to compare, contrast, and interpret metrics which attempt to measure the same features of software. © 2006 ACM.",Cohesion,Computer programming; Numerical analysis; Object oriented programming; Software engineering; Cohesion; Cohesion metrics; Object-oriented (OO) paradigm; Programming paradigm; Metric system
UML-B: Formal modeling and design aided by UML,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745413804&doi=10.1145%2f1125808.1125811&partnerID=40&md5=2365076565cc60de3262c5f9b8dfac0e,"The emergence of the UML as a de facto standard for object-oriented modeling has been mirrored by the success of the B method as a practically useful formal modeling technique. The two notations have much to offer each other. The UML provides an accessible visualization of models facilitating communication of ideas but lacks formal precise semantics. B, on the other hand, has the precision to support animation and rigorous verification but requires significant effort in training to overcome the mathematical barrier that many practitioners perceive. We utilize a derivation of the B notation as an action and constraint language for the UML and define the semantics of UML entities via a translation into B. Through the UML-B profile we provide specializations of UML entities to support model refinement. The result is a formally precise variant of UML that can be used for refinement based, object-oriented behavioral modeling. The design of UML-B has been guided by industrial applications. © 2006 ACM.",Modeling; Refinement; UML-B,Animation; Computer aided design; Computer programming languages; Industrial applications; Object oriented programming; Semantics; Mathematical barriers; Modeling; Refinement; Rigorous verification; UML-B; Formal logic
Model driven security: From UML models to access control infrastructures,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745471209&doi=10.1145%2f1125808.1125810&partnerID=40&md5=a97f87be73880d72df837d214fb2a053,"We present a new approach to building secure systems. In our approach, which we call Model Driven Security, designers specify system models along with their security requirements and use tools to automatically generate system architectures from the models, including complete, configured access control infrastructures. Rather than fixing one particular modeling language for this process, we propose a general schema for constructing such languages that combines languages for modeling systems with languages for modeling security. We present several instances of this schema that combine (both syntactically and semantically) different UML modeling languages with a security modeling language for formalizing access control requirements. From models in the combined languages, we automatically generate access control infrastructures for server-based applications, built from declarative and programmatic access control mechanisms. The modeling languages and generation process are semantically well-founded and are based on an extension of Role-Based Access Control. We have implemented this approach in a UML-based CASE-tool and report on experiments. © 2006 ACM.",Metamodeling; Model Driven Architecture; Object Constraint Language; Role-Based Access Control; Security engineering; Unified Modeling Language,Computer simulation languages; Security systems; Semantics; Servers; Syntactics; Systems analysis; Metamodeling; Model Driven Architecture; Object Constraint Language; Role-Based Access Control; Security engineering; Unified Modeling Language; Computer programming languages
Symbolic model checking of UML activity diagrams,2006,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745339682&doi=10.1145%2f1125808.1125809&partnerID=40&md5=d8c946e8fb9ce86e2d919f665ce31385,"Two translations from activity diagrams to the input language of NuSMV, a symbolic model verifier, are presented. Both translations map an activity diagram into a finite state machine and are inspired by existing statechart semantics. The requirements-level translation defines state machines that can be efficiently verified, but are a bit unrealistic since they assume the perfect synchrony hypothesis. The implementation-level translation defines state machines that cannot be verified so efficiently, but that are more realistic since they do not use the perfect synchrony hypothesis. To justify the use of the requirements-level translation, we show that for a large class of activity diagrams and certain properties, both translations are equivalent: regardless of which translation is used, the outcome of model checking is the same. Moreover, for linear stuttering-closed properties, the implementation-level translation is equivalent to a slightly modified version of the requirements-level translation. We use the two translations to model check data integrity constraints for an activity diagram and a set of class diagrams that specify the data manipulated in the activities. Both translations have been implemented in two tools. We discuss our experiences in applying both translations to model check some large example activity diagrams. © 2006 ACM.",Activity diagrams; Model checking; Unified Modeling Language,Computer programming languages; Data structures; Semantics; Synchronization; Translation (languages); Activity diagrams; Class diagrams; Model checking; Unified Modeling Language; Graphic methods
A scalable formal method for design and automatic checking of user interfaces,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544450684&doi=10.1145%2f1061254.1061256&partnerID=40&md5=6cbf2057e08e1c1700432d7d6b87ed4b,"The article addresses the formal specification, design and implementation of the behavioral component of graphical user interfaces. The complex sequences of visual events and actions that constitute dialogs are specified by means of modular, communicating grammars called VEG (Visual Event Grammars), which extend traditional BNF grammars to make them more convenient to model dialogs. A VEG specification is independent of the actual layout of the GUI, but it can easily be integrated with various layout design toolkits. Moreover, a VEG specification may be verified with the model checker SPIN, in order to test consistency and correctness, to detect deadlocks and unreachable states, and also to generate test cases for validation purposes.Efficient code is automatically generated by the VEG toolkit, based on compiler technology. Realistic applications have been specified, verified and implemented, like a Notepad-style editor, a graph construction library and a large real application to medical software. It is also argued that VEG can be used to specify and test voice interfaces and multimodal dialogs. The major contribution of our work is blending together a set of features coming from GUI design, compilers, software engineering and formal verification. Even though we do not claim novelty in each of the techniques adopted for VEG, they have been united into a toolkit supporting all GUI design phases, that is, specification, design, verification and validation, linking to applications and coding. © 2005 ACM.",Applications of model checking; GUI design; Human-computer interaction (HCI),Computer software; Human computer interaction; Software engineering; Specifications; Application of model checking; Compliers; Graphical user interfaces (GUI) design; Graphical user interfaces
Toward an engineering discipline for grammarware,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745137818&doi=10.1145%2f1072997.1073000&partnerID=40&md5=bba8ac4a084a2f99991aa3f6dc329a7a,"Grammarware comprises grammars and all grammar-dependent software. The term grammar is meant here in the sense of all established grammar formalisms and grammar notations including context-free grammars, class dictionaries, and XML schemas as well as some forms of tree and graph grammars. The term grammar-dependent software refers to all software that involves grammar knowledge in an essential manner. Archetypal examples of grammar-dependent software are parsers, program converters, and XML document processors. Despite the pervasive role of grammars in software systems, the engineering aspects of grammarware are insufficiently understood. We lay out an agenda that is meant to promote research on increasing the productivity of grammarware development and on improving the quality of grammarware. To this end, we identify the problems with the current grammarware practices, the barriers that currently hamper research, and the promises of an engineering discipline for grammarware, its principles, and the research challenges that have to be addressed. © 2005 ACM.",Automated software engineering; Best practices; Generic language technology; Grammar-dependent software; Grammars; Grammarware; Language processing; Metamodeling; Model-driven development; Parsers; Software evolution; Software transformation,Computer graphics; Data processing; Knowledge acquisition; Text processing; XML; Automated software engineering; Best practices; Generic language technology; Grammar-dependent software; Grammars; Grammarware; Language processing; Model-driven development; Parsers; Software transformation; Software engineering
The impact of software engineering research on modern progamming languages,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745178092&doi=10.1145%2f1101815.1101818&partnerID=40&md5=636fabb393089f1e68f0cc7700231ff9,"Software engineering research and programming language design have enjoyed a symbiotic relationship, with traceable impacts since the 1970s, when these areas were first distinguished from one another. This report documents this relationship by focusing on several major features of current programming languages: data and procedural abstraction, types, concurrency, exceptions, and visual programming mechanisms. The influences are determined by tracing references in publications in both fields, obtaining oral histories from language designers delineating influences on them, and tracking cotemporal research trends and ideas as demonstrated by workshop topics, special issue publications, and invited talks in the two fields. In some cases there is conclusive data supporting influence. In other cases, there are circumstantial arguments (i.e., cotemporal ideas) that indicate influence. Using this approach, this study provides evidence of the impact of software engineering research on modern programming language design and documents the close relationship between these two fields. © 2005 ACM.",Programming Languages; Software Engineering,Computer programming; Computer programming languages; Database systems; Logic design; Technical presentations; Data support; Programming language design; Symbiotic relationship; Visual programming; Software engineering
ACM Transactions on Software Engineering and Methodology: Editorial,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544442203&doi=10.1145%2f1061254.1061255&partnerID=40&md5=5b312a065de2c7a015934d591b1a0bf9,[No abstract available],,
Software reuse for scientific computing through program generation,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544451045&doi=10.1145%2f1061254.1061257&partnerID=40&md5=7a7f2c39c373f4d361e0b1ac6767c91a,"We present a program-generation approach to address a software-reuse challenge in the area of scientific computing. More specifically, we describe the design of a program generator for the specification of subroutines that can be generic in the dimensions of arrays, parameter lists, and called subroutines. We describe the application of that approach to a real-world problem in scientific computing which requires the generic description of inverse ocean modeling tools. In addition to a compiler that can transform generic specifications into efficient Fortran code for models, we have also developed a type system that can identify possible errors already in the specifications. This type system is important for the acceptance of the program generator among scientists because it prevents a large class of errors in the generated code. © 2005 ACM.",Additional Key Words and Phrases: Software reuse; Fortran; Haskell; Inverse modeling; Ocean science; Program generator; Type system,Computer software; Natural sciences computing; Oceanography; Fortran; Haskell; Inverse modeling; Type system; Computer software reusability
Reasoning about inconsistencies in natural language requirements,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745172461&doi=10.1145%2f1072997.1072999&partnerID=40&md5=5e370a94f7be25fb1ffcab07d27aab41,"The use of logic in identifying and analyzing inconsistency in requirements from multiple stakeholders has been found to be effective in a number of studies. Nonmonotonic logic is a theoretically well-founded formalism that is especially suited for supporting the evolution of requirements. However, direct use of logic for expressing requirements and discussing them with stakeholders poses serious usability problems, since in most cases stakeholders cannot be expected to be fluent with formal logic. In this article, we explore the integration of natural language parsing techniques with default reasoning to overcome these difficulties. We also propose a method for automatically discovering inconsistencies in the requirements from multiple stakeholders, using both theoremproving and model-checking techniques, and show how to deal with them in a formal manner. These techniques were implemented and tested in a prototype tool called CARL. The effectiveness of the techniques and of the tool are illustrated by a classic example involving conflicting requirements from multiple stakeholders. © 2005 ACM.",Default logic; Inconsistency; Natural language; Requirements,Computer programming languages; Problem solving; Software engineering; Software prototyping; Default logic; Inconsistency; Natural languages; Requirements; Formal logic
A comprehensive approach for the development of modular software architecture description languages,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544450414&doi=10.1145%2f1061254.1061258&partnerID=40&md5=2049340bef8c72b37687ce6a017e210a,"Research over the past decade has revealed that modeling software architecture at the level of components and connectors is useful in a growing variety of contexts. This has led to the development of a plethora of notations for representing software architectures, each focusing on different aspects of the systems being modeled. In general, these notations have been developed without regard to reuse or extension. This makes the effort in adapting an existing notation to a new purpose commensurate with developing a new notation from scratch. To address this problem, we have developed an approach that allows for the rapid construction of new architecture description languages (ADLs). Our approach is unique because it encapsulates ADL features in modules that are composed to form ADLs. We achieve this by leveraging the extension mechanisms provided by XML and XML schemas. We have defined a set of generic, reusable ADL modules called xADL 2.0, useful as an ADL by itself, but also extensible to support new applications and domains. To support this extensibility, we have developed a set of reflective syntax-based tools that adapt to language changes automatically, as well as several semantically-aware tools that provide support for advanced features of xADL 2.0. We demonstrate the effectiveness, scalability, and flexibility of our approach through a diverse set of experiences. First, our approach has been applied in industrial contexts, modeling software architectures for aircraft software and spacecraft systems. Second, we show how xADL 2.0 can be extended to support the modeling features found in two different representations for modeling product-line architectures. Finally, we show how our infrastructure has been used to support its own development. The technical contribution of our infrastructure is augmented by several research contributions: the first decomposition of an architecture description language into modules, insights about how to develop new language modules and a process for integrating them, and insights about the roles of different kinds of tools in a modular ADL-based infrastructure. © 2005 ACM.",Architecture description languages; ArchStudio 3; xADL 2.0; XML,Computer architecture; Computer programming languages; Software engineering; XML; Architecture description languages; ArchStudio 3; XADL 2.0; Computer software
An extended fault class hierarchy for specification-based testing,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646025729&doi=10.1145%2f1072997.1072998&partnerID=40&md5=184b0d91c1f693b904b675e846dd0710,"Kuhn, followed by Tsuchiya and Kikuno, have developed a hierarchy of relationships among several common types of faults (such as variable and expression faults) for specification-based testing by studying the corresponding fault detection conditions. Their analytical results can help explain the relative effectiveness of various fault-based testing techniques previously proposed in the literature. This article extends and complements their studies by analyzing the relationships between variable and literal faults, and among literal, operator, term, and expression faults. Our analysis is more comprehensive and produces a richer set of findings that interpret previous empirical results, can be applied to the design and evaluation of test methods, and inform the way that test cases should be prioritized for earlier detection of faults. Although this work originated from the detection of faults related to specifications, our results are equally applicable to program-based predicate testing that involves logic expressions. © 2005 ACM.",Fault class analysis; Software testing; Specification-based testing; Test case generation,Computer software; Formal logic; Hierarchical systems; Logic design; Product design; Software engineering; Fault class analysis; Software testing; Specification-based testing; Test case generation; Fault tolerant computer systems
Reasoning about static and dynamic properties in alloy: A purely relational approach,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745161834&doi=10.1145%2f1101815.1101819&partnerID=40&md5=66a21cd027b1138b3ecf23108a5687b3,"We study a number of restrictions associated with the first-order relational specification language Alloy. The main shortcomings we address are: -the lack of a complete calculus for deduction in Alloy's underlying formalism, the so called relational logic, -the inappropriateness of the Alloy language for describing (and analyzing) properties regarding execution traces. The first of these points was not regarded as an important issue during the genesis of Alloy, and therefore has not been taken into account in the design of the relational logic. The second point is a consequence of the static nature of Alloy specifications, and has been partly solved by the developers of Alloy; however, their proposed solution requires a complicated and unstructured characterization of executions. We propose to overcome the first problem by translating relational logic to the equational calculus of fork algebras. Fork algebras provide a purely relational formalism close to Alloy, which possesses a complete equational deductive calculus. Regarding the second problem, we propose to extend Alloy by adding actions. These actions, unlike Alloy functions, do modify the state. Much the same as programs in dynamic logic, actions can be sequentially composed and iterated, allowing them to state properties of execution traces at an appropriate level of abstraction. Since automatic analysis is one of Alloy's main features, and this article aims to provide a deductive calculus for Alloy, we show that: -the extension hereby proposed does not sacrifice the possibility of using SAT solving techniques for automated analysis, -the complete calculus for the relational logic is straightforwardly extended to a complete calculus for the extension of Alloy. © 2005 ACM.",Alloy; Fork algebras; Relational specifications,Abstracting; Algebra; Automation; Formal logic; Logic design; Problem solving; Automatic analysis; Dynamic logic; Fork algebras; Relational specifications; Computer programming languages
Editorial,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008025808&doi=10.1145%2f1101815.1101816&partnerID=40&md5=eb0d72e12286f4ea6af9e295cbbfc93a,[No abstract available],,
Acknowledgement of Referees 2004,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024281116&doi=10.1145%2f1061254.1061259&partnerID=40&md5=509745521762d3d081fbdf3210719a19,[No abstract available],,
Impact of software engineering research on the practice of software configuration management,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745149304&doi=10.1145%2f1101815.1101817&partnerID=40&md5=687b663805b32b350186d56318707bf3,"Software Configuration Management (SCM) is an important discipline in professional software development and maintenance. The importance of SCM has increased as programs have become larger, more long lasting, and more mission and life critical. This article discusses the evolution of SCM technology from the early days of software development to the present, with a particular emphasis on the impact that university and industrial research has had along the way. Based on an analysis of the publication history and evolution in functionality of the available SCM systems, we trace the critical ideas in the field from their early inception to their eventual maturation in commercially and freely available SCM systems. In doing so, this article creates a detailed record of the critical value of SCM research and illustrates how research results have shaped the functionality of today's SCM systems. © 2005 ACM.",Data model; Process support; Research impact; Software configuration management; Software engineering; Versioning; Workspace management,Commerce; Computer software; Data structures; Industrial applications; Professional aspects; Research and development management; Process support; Research impacts; Software configuration management; Versioning; Workspace management; Software engineering
An empirical study of industrial requirements engineering process assessment and improvement,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13844298039&doi=10.1145%2f1044834.1044837&partnerID=40&md5=56d2f1070217b3b65765aea98b16062f,"This article describes an empirical study in industry of requirements engineering process maturity assessment and improvement. Our aims were to evaluate a requirements engineering process maturity model and to assess if improvements in requirements engineering process maturity lead to business improvements. We first briefly describe the process maturity model that we used and modifications to this model to accommodate process improvement. We present initial maturity assessment results for nine companies, describe how process improvements were selected and present data on how RE process maturity changed after these improvements were introduced. We discuss how business benefits were assessed and the difficulties of relating process maturity improvements to these business benefits. All companies reported business benefits and satisfaction with their participation in the study. Our conclusions are that the RE process maturity model is useful in supporting maturity assessment and in identifying process improvements and there is some evidence to suggest that process improvement leads to business benefits. However, whether these business benefits were a consequence of the changes to the RE process or whether these benefits resulted from side-effects of the study such as greater self-awareness of business processes remains an open question.",Empirical software engineering; Process measurement; Requirements engineering; Software process improvement,Computer software; Industrial engineering; Productivity; Quality control; Software engineering; Specifications; Statistical methods; Empiricial software engineering; Process measurement; Self-awareness; Software process improvement; Requirements engineering
Parameterized object sensitivity for points-to analysis for java,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13844296853&doi=10.1145%2f1044834.1044835&partnerID=40&md5=424a7a616d0efddd3de5b85251dc8b17,"The goal of points-to analysis for Java is to determine the set of objects pointed to by a reference variable or a reference object field. We present object sensitivity, a new form of context sensitivity for flow-insensitive points-to analysis for Java. The key idea of our approach is to analyze a method separately for each of the object names that represent run-time objects on which this method may be invoked. To ensure flexibility and practicality, we propose a parameterization framework that allows analysis designers to control the tradeoffs between cost and precision in the object-sensitive analysis. Side-effect analysis determines the memory locations that may be modified by the execution of a program statement. Def-use analysis identifies pairs of statements that set the value of a memory location and subsequently use that value. The information computed by such analyses has a wide variety of uses in compilers and software tools. This work proposes new versions of these analyses that are based on object-sensitive points-to analysis. We have implemented two instantiations of our parameterized object-sensitive points-to analysis. On a set of 23 Java programs, our experiments show that these analyses have comparable cost to a context-insensitive points-to analysis for Java which is based on Andersen's analysis for C. Our results also show that object sensitivity significantly improves the precision of side-effect analysis and call graph construction, compared to (1) context-insensitive analysis, and (2) context-sensitive points-to analysis that models context using the invoking call site. These experiments demonstrate that object-sensitive analyses can achieve substantial precision improvement, while at the same time remaining efficient and practical.",Class analysis; Context sensitivity; Def-use analysis; Points-to analysis; Side-effect analysis; Static analysis,Computer aided software engineering; Computer software maintenance; Context sensitive languages; Information analysis; Object oriented programming; Productivity; Sensitivity analysis; Class analysis; Context sensitivity; Context-insensitive analysis; Def-use analysis; Parameterization; Parameterized object sensitivity; Points-to analysis; Static analysis; Java programming language
Formal interpreters for diagram notations,2005,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13844272384&doi=10.1145%2f1044834.1044836&partnerID=40&md5=1f124d4f9c39fe6a4dc04ac4e9107096,"The article proposes an approach for defining extensible and flexible formal interpreters for diagram notations with significant dynamic semantics. More precisely, it addresses semi-formal diagram notations that have precisely-defined syntax, but informally defined (dynamic) semantics. These notations are often flexible to fit the different needs and expectations of users. Flexibility comes from the incompleteness or informality of the original definition and results in different interpretations, The approach defines interpreters by means of a mapping onto a semantic domain. Two sets of rules define the correspondences between the elements of the diagram notation and those of the semantic domain, and between events and states of the semantic domain and visual annotations on the elements of the diagram notation. Flexibility also leads to notation families, that is, sets of notations that share core concepts, but present slightly different interpretations. Existing approaches usually interpret these notations in isolation; the approach presented in this article allows the interpretation of a family as a whole. The feasibility of the approach is demonstrated through a prototype generator that allows users to implement special-purpose interpreters by defining relatively small sets of rules.",Graph transformation; High-level Petri nets; Semantics; Semi-formal notations,Control systems; Graph theory; Petri nets; Real time systems; Software engineering; Specifications; Diagram notations; Formal interpreters; Graph transformations; High-level petri nets; Semi-formal notations; Semantics
A framework for modeling and implementing visual notations with applications to software engineering,2004,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344329785&doi=10.1145%2f1040291.1040293&partnerID=40&md5=1a03c3266926b0fbc4037b10e810e9e6,"We present a framework for modeling visual notations and for generating the corresponding visual programming environments. The framework can be used for modeling the diagrammatic notations of software development methodologies, and to generate visual programming environments with CASE tools functionalities. This is accomplished through an underlying modeling process based on the visual notation syntactic model of eXtended Positional Grammars (XPG, for short), and the associated parsing methodology, XpLR. In particular, the process requires the modeling of the basic elements (visual symbols) of a visual notation, their syntactic properties, the relations between them, the syntactic rules to formally define the set of feasible visual sentences, and a set of semantic routines performing additional checks and translation tasks. Such a process is completely supported by the VLDesk system, which enables the automatic generation of an editor for drawing visual sentences, as well as a processor for their recognition, parsing, and translation into other notations. The proposed framework also provides the basis for the definition of a meta-CASE technology. In fact, we can customize the generated visual programming environment in terms of the supported visual notation, its syntactic properties, and the translation rules. We have used this framework to model several diagrammatic notations used in software development methodologies, including those of the Unified Modeling Language.",LR parsing; Meta-CASE; Metamodeling; Software engineering models; UML; Visual grammars; Visual notations,Abstracting; Automation; Computer programming languages; Computer software; Flowcharting; Formal languages; Formal logic; Semantics; LR parsing; Meta-computer-aided software engineering (CASE); Metamodeling; Unified modeling lanaguage (UML); Visual grammars; Visual notations; Software engineering
On test suite composition and cost-effective regression testing,2004,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344285019&doi=10.1145%2f1027092.1027093&partnerID=40&md5=595512ddf2958f12b07a25bd39d3c349,"Regression testing is an expensive testing process used to revalidate software as it evolves. Various methodologies for improving regression testing processes have been explored, but the cost-effectiveness of these methodologies has been shown to vary with characteristics of regression test suites. One such characteristic involves the way in which test inputs are composed into test cases within a test suite. This article reports the results of controlled experiments examining the effects of two factors in test suite composition - test suite granularity and test input grouping - on the costs and benefits of several regression-testing-related methodologies: retest-all, regression test selection, test suite reduction, and test case prioritization. These experiments consider the application of several specific techniques, from each of these methodologies, across ten releases each of two substantial software systems, using seven levels of test suite granularity and two types of test input grouping. The effects of granularity, technique, and grouping on the cost and fault-detection effectiveness of regression testing under the given methodologies are analyzed. This analysis shows that test suite granularity significantly affects several cost-benefit factors for the methodologies considered, while test input grouping has limited effects. Further, the results expose essential tradeoffs affecting the relationship between test suite design and regression testing cost-effectiveness, with several implications for practice.",Empirical studies; Regression testing; Test suite composition,Computer software; Cost effectiveness; Mathematical techniques; Set theory; Textbooks; Word processing; Empirical studies; Regression testing; Test case prioritization; Test suite composition; Regression analysis
Coupling and cohesion metrics for knowledge-based systems using frames and rules,2004,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344226078&doi=10.1145%2f1027092.1027094&partnerID=40&md5=bf2997edd501af5c6fb27cb6f627ed57,"Software systems and in particular also knowledge-based systems (KBS) become increasingly large and complex. In response to this challenge, software engineering has a long tradition of advocating modularity. This has also heavily influenced object-oriented development. For measuring certain important aspects of modularity, coupling and cohesion metrics have been developed. Metrics have also attracted considerable attention for object-oriented development. For KBS development, however, no such, metrics are available yet. This article presents the core of the first metrics suite for KBS development, its coupling and cohesion metrics. These metrics measure modularity in terms of the relations induced between slots of frames through their common references in rules. We show the soundness of these metrics according to theory and report on their usefulness in practice. As a consequence, we propose using our metrics in order to improve KBS development, and developing other important metrics and assessing their theoretical soundness along these lines.",Cohesion; Coupling; Knowledge-based systems,Computer software; Measurement theory; Object oriented programming; Set theory; Software engineering; Systems analysis; Cohesion metrics; Coupling metrics; Merged frames; Modularity; Knowledge based systems
Flow analysis for verifying properties of concurrent software systems,2004,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344305541&doi=10.1145%2f1040291.1040292&partnerID=40&md5=6a5233c1edae9a9e0898810cf7f93699,"This article describes FLAYERS, a finite-state verification approach that analyzes whether concurrent systems satisfy user-defined, behavioral properties. FLAYERS automatically creates a compact, event-based model of the system that supports efficient dataflow analysis. FLAYERS achieves this efficiency at the cost of precision. Analysts, however, can improve the precision of analysis results by selectively and judiciously incorporating additional semantic information into an analysis. We report on an empirical study of the performance of the FLAVERS/Ada toolset applied to a collection of multitasking Ada systems. This study indicates that sufficient precision for proving system properties can usually be achieved and that the cost for such analysis typically grows as a low-order polynomial in the size of the system.",Dataflow analysis; Finite-state verification; Model checking,Algorithms; Behavioral research; Computational complexity; Computer software; Costs; Data flow analysis; Mathematical models; Polynomials; Theorem proving; Concurrent programming-parallel programming; Finite-state verification; Model checking; Software/program verificastion; Software engineering
"Classifying data dependences in the presence of pointers for program comprehension, testing, and debugging",2004,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444224266&doi=10.1145%2f1018210.1018212&partnerID=40&md5=b18b513f0c9f24bed86c74c16dab8143,"Understanding data dependences in programs is important for many software-engineering activities, such as program understanding, impact analysis, reverse engineering, and debugging. The presence of pointers can cause subtle and complex data dependences that can be difficult to understand. For example, in languages such as C, an assignment made through a pointer dereference can assign a value to one of several variables, none of which may appear syntactically in that statement. In the first part of this article, we describe two techniques for classifying data dependences in the presence of pointer dereferences. The first technique classifies data dependences based on definition type, use type, and path type. The second technique classifies data dependences based on span. We present empirical results to illustrate the distribution of data-dependence types and spans for a set of real C programs. In the second part of the article, we discuss two applications of the classification techniques. First, we investigate different ways in which the classification can be used to facilitate data-flow testing. We outline an approach that uses types and spans of data dependences to determine the appropriate verification technique for different data dependences; we present empirical results to illustrate the approach. Second, we present a new slicing approach that computes slices based on types of data dependences. Based on the new approach, we define an incremental slicing technique that computes a slice in multiple steps. We present empirical results to illustrate the sizes of incremental slices and the potential usefulness of incremental slicing for debugging.",Data dependences; Data-flow testing; Debugging; Incremental slicing; Pointers; Program comprehension; Program slicing,Algorithms; C (programming language); Computer software selection and evaluation; Data flow analysis; Program debugging; Data dependences; Data flow testing; Pointers; Software engineering
Mae - A system model and environment for managing architectural evolution,2004,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444253327&doi=10.1145%2f1018210.1018213&partnerID=40&md5=ab6726b641521f586da89c6a74c482d2,"As with any other artifact produced as part of the software life cycle, software architectures evolve and this evolution must be managed. One approach to doing so would be to apply any of a host of existing configuration management systems, which have long been used successfully at the level of source code. Unfortunately, such an approach leads to many problems that prevent effective management of architectural evolution. To overcome these problems, we have developed an alternative approach centered on the use of an integrated architectural and configuration management system model. Because the system model combines architectural and configuration management concepts in a single representation, it has the distinct benefit that all architectural changes can be precisely captured and clearly related to each other - both at the fine-grained level of individual architectural elements and at the coarse-grained level of architectural configurations. To support the use of the system model, we have developed Mae, an architectural evolution environment through which users can specify architectures in a traditional manner, manage the evolution of the architectures using a check-out/check-in mechanism that tracks all changes, select a specific architectural configuration, and analyze the consistency of a selected configuration. We demonstrate the benefits of our approach by showing how the system model and its accompanying environment were used in the context of several representative projects.",Design environment; Evolution; Mae; System model,Codes (symbols); Computer programming languages; Mathematical models; Problem solving; Software engineering; Configuration management systems; Software architectures; Software life cycle; Computer software
Assembly instruction level reverse execution for debugging,2004,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444259962&doi=10.1145%2f1018210.1018211&partnerID=40&md5=9e0ca74d9e7e0ff0ecc2e7efcf88f424,"Assembly instruction level reverse execution provides a programmer with the ability to return a program to a previous state in its execution history via execution of a ""reverse program."" The ability to execute a program in reverse is advantageous for shortening software development time. Conventional techniques for recovering a state rely on saving the state into a record before the state is destroyed. However, state-saving causes significant memory and time overheads during forward execution. The proposed method introduces a reverse execution methodology at the assembly instruction level with low memory and time overheads. The methodology generates, from a program, a reverse program by which a destroyed state is almost always regenerated rather than being restored from a record. This significantly reduces state-saving. The methodology has been implemented on a PowerPC processor with a custom made debugger. As compared to previous work, all of which heavily use state-saving techniques, the experimental results show from 2X to 2206X reduction in run-time memory usage, from 1.5X to 403X reduction in forward execution time overhead and from 1.2X to 2.32X reduction in forward execution time for the tested benchmarks. Furthermore, due to the reduction in memory usage, our method can provide reverse execution in many cases where other methods run out of available memory. However, for cases where there is enough memory available, our method results in 1.16X to 1.89X slow down in reverse execution.",Debugging; Reverse code generation; Reverse execution,Algorithms; Codes (symbols); Data storage equipment; Program debugging; Program processors; Reverse engineering; Reverse code generation; Reverse execution; Run time memory usage; Software engineering
Incremental elaboration of scenario-based specifications and behavior models using implied scenarios,2004,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042823719&doi=10.1145%2f1005561.1005563&partnerID=40&md5=fe9d0028931031ce9e2c2a9f22d793a8,A process for elaborating system behavior that exploits the potential benefits of behavior modeling and scenario-based specifications was discussed. The concept that drives the elaboration process is that of implied scenarios. Implied scenarios identify gaps in scenario-based specifications that arise from specifying the global behavior of a system that can be implemented component-wise. They are the result of a mismatch between the behavioral and architectural aspects of scenario-based specifications.,Implied scenarios; LTSA; MSC; Negative scenarios,Approximation theory; Automation; Concurrent engineering; Distributed computer systems; Formal logic; Mathematical models; Specifications; Artifacts; Behavior modeling; Design tools; Mechanical verification; Software engineering
Term rewriting with traversal functions,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042810925&doi=10.1145%2f941566.941568&partnerID=40&md5=e5f6226580239f39c103270a4bc3f885,"Term rewriting is an appealing technique for performing program analysis and program transformation. Tree (term) traversal is frequently used but is not supported by standard term rewriting. We extend many-sorted, first-order term rewriting with traversal functions that automate tree traversal in a simple and type-safe way. Traversal functions can be bottom-up or top-down traversais and can either traverse all nodes in a tree or can stop the traversal at a certain depth as soon as a matching node is found. They can either define sort-preserving transformations or mappings to a fixed sort. We give small and somewhat larger examples of traversal functions and describe their operational semantics and implementation. An assessment of various applications and a discussion conclude the article. © 2003 ACM.",Automated tree traversal; Term rewriting; Types,Automation; Mapping; Mathematical transformations; Semantics; Software engineering; Trees (mathematics); Automated tree traversal; Term rewriting; Traversal functions; Types; Program processors
Address translation in telecommunication features,2004,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042826706&doi=10.1145%2f1005561.1005562&partnerID=40&md5=aba3f6f9f33415fb285e587bea976368,"Address translation causes a wide variety of interactions among telecommunication features. This article begins with a formal model of address translation and its effects, and with principles for understanding how features should interact in the presence of address translation. There is a simple and intuitive set of constraints on feature behavior so that features will interact according to the principles. This scheme (called ""ideal address translation"") has provable properties, is modular (explicit cooperation among features is not required), and supports extensibility (adding new features does not require changing old features). The article also covers reasoning in the presence of exceptions to the constraints, limitations of the theory, relation to real networks and protocols, and relation to other research.",Component architecture; Feature interaction; Formal methods; Network addressing; Network protocols; Network security; Requirements; Telecommunications,Computer software; Constraint theory; Engineering research; Network protocols; Reliability; Security of data; Specifications; Component architecture; Feature interaction; Formal methods; Network addressing; Telecommunication systems
Automatic testing equivalence verification of spi calculus specifications,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042538976&doi=10.1145%2f941566.941570&partnerID=40&md5=3f0bb2386aa6d216cfc4414998c4d475,"Testing equivalence is a powerful means for expressing the security properties of cryptographic protocols, but its formal verification is a difficult task because of the quantification over contexts on which it is based. Previous articles have provided insights into using theorem-proving for the verification of testing equivalence of spi calculus specifications. This article addresses the same verification problem, but uses a state exploration approach. The verification technique is based on the definition of an environment-sensitive, labeled transition system representing a spi calculus specification. Trace equivalence defined on such a transition system coincides with testing equivalence. Symbolic techniques are used to keep the set of traces finite. If a difference in the traces of two spi descriptions (typically a specification and the corresponding implementation of a protocol) is found, it can be used to automatically build the spi calculus description of an intruder process that can exploit the difference. © 2003 ACM.",Cryptographic protocols; Equivalence verification; State space exploration,Automation; Cryptography; Network protocols; Security systems; Cryptographic protocols; Equivalence verification; State space exploration; Symbolic techniques; Software engineering
Static analysis to support the evolution of exception structure in object-oriented systems,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22944452575&doi=10.1145%2f941566.941569&partnerID=40&md5=42fc4a0dd08f759b1e1bd14b24abf127,"Exception-handling mechanisms in modern programming languages provide a means to help software developers build robust applications by separating the normal control flow of a program from the control flow of the program under exceptional situations. Separating the exceptional structure from the code associated with normal operations bears some consequences. One consequence is that developers wishing to improve the robustness of a program must figure out which exceptions, if any, can flow to a point in the program. Unfortunately, in large programs, this exceptional control flow can be difficult, if not impossible, to determine. In this article, we present a model that encapsulates the minimal concepts necessary for a developer to determine exception flow for object-oriented languages that define exceptions as objects. Using these concepts, we describe why exception-flow information is needed to build and evolve robust programs. We then describe Jex, a static analysis tool we have developed to provide exception-flow information for Java systems based on this model. The Jex tool provides a view of the actual exception types that might arise at different program points and of the handlers that are present. Use of this tool on a collection of Java library and application source code demonstrates that the approach can be helpful to support both local and global improvements to the exception-handling structure of a system. © 2003 ACM.",Error handling; Exception flow; Exception handling; Exception structure; Program evolution; Static analysis,Computer programming languages; Error analysis; Java programming language; Program processors; Robustness (control systems); Software engineering; Error handling; Exception flow; Exception handling; Exception structure; Program evolution; Static analysis; Object oriented programming
A formal model for reasoning about adaptive qos-enabled middleware,2004,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042702899&doi=10.1145%2f1005561.1005564&partnerID=40&md5=d5f80deb02edfb6a48e5744ba9df6310,"Systems that provide distributed multimedia services are subject to constant evolution; customizable middleware is required to effectively manage this change. Middleware services for resource management execute concurrently with each other, and with application activities, and can, therefore, potentially interfere with each other. To ensure cost-effective QoS in distributed multimedia systems, safe composability of resource management services is essential. In this article, we present a meta-architectural framework, the Two-Level Actor Model (TLAM) for customizable QoS-based middleware, based on the actor model of concurrent active objects. Using TLAM, a semantic model for specifying and reasoning about components of open distributed systems, we show how a QoS brokerage service can be used to coordinate multimedia resource management services in a safe, flexible, and efficient manner. In particular, we show a system in which the multimedia actor behaviors satisfy the specified requirements and provide the required multimedia service. The behavior specification leaves open the possibility of a variety of algorithms for resource management. Furthermore, constraints are identified that are sufficient to guarantee noninterference among the multiple broker resource management services, as well as providing guidelines for the safe composition of additional services.","Meta-object models, actors; Middleware services; Multimedia; Quality-of-service; Reflection, theoretical foundations",Algorithms; Computer programming; Concurrent engineering; Distributed computer systems; Formal logic; Mathematical models; Quality of service; Computer-communication networks; Distributed programming; Program verification; Middleware
A formal approach for designing CORBA-based applications,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-26444450516&doi=10.1145%2f941566.941567&partnerID=40&md5=2cfb380b372af3d5aa50aab20b8529ab,"The design of distributed applications in a CORBA-based environment can be carried out by means of an incremental approach, which starts from the specification and leads to the high-level architectural design. This article discusses a methodology to transform a formal specification written in TRIO into a high-level design document written in an extension of TRIO, named TRIO/CORBA (TC). The TC language is suited to formally describe the high-level architecture of a CORBA-based application. As a result, designers are offered high-level concepts that precisely define the architectural elements of an application. Furthermore, TC offers mechanisms to extend its base semantics, and can be adapted to future developments and enhancements in the CORBA standard. The methodology and the associated language are presented through a case study derived from a real Supervision and Control System. © 2003 ACM.",,Computer architecture; Computer programming languages; Control systems; Distributed computer systems; Semantics; Associated language; CORBA-based applications; High-level architecture; TRIO/CORBA (TC) - computer programming language; Software engineering
Multi-valued symbolic model-checking,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038619995&doi=10.1145%2f990010.990011&partnerID=40&md5=e68e8b9bec6ffaaaa2d9b737d0eb390c,"This article introduces the concept of multi-valued model-checking and describes a multi-valued symbolic model-checker, X Chek. Multi-valued model-checking is a generalization of classical modelchecking, useful for analyzing models that contain uncertainty (lack of essential information) or inconsistency (contradictory information, often occurring when information is gathered from multiple sources). Multi-valued logics support the explicit modeling of uncertainty and disagreement by providing additional truth values in the logic. This article provides a theoretical basis for multi-valued model-checking and discusses some of its applications. A companion article [Chechik et al. 2002b] describes implementation issues in detail. The model-checker works for any member of a large class of multi-valued logics. Our modeling language is based on a generalization of Kripke structures, where both atomic propositions and transitions between states may take any of the truth values of a given multi-valued logic. Properties are expressed in X CTL, our multi-valued extension of the temporal logic CTL. We define the class of logics, present the theory of multi-valued sets and multi-valued relations used in our model-checking algorithm, and define the multi-valued extensions of CTL and Kripke structures. We explore the relationship between X CTL and CTL, and provide a symbolic model-checking algorithm for XCTL. We also address the use of fairness in multi-valued model-checking. Finally, we discuss some applications of the multi-valued model-checking approach.",χ Chek; CTL; Fairness; Inconsistency; Model-checking; Multi-valued logic; Partiality,Approximation theory; Finite element method; Probability distributions; Semantics; Set theory; Software engineering; ΧCheck; CTL; Fairness; Inconsistency; Model-checking; Multi-valued logic; Partiality; Many valued logics
A framework and tool support for the systematic testing of model-based specifications,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2542488419&doi=10.1145%2f990010.990012&partnerID=40&md5=340ef3d1697ffdd36c944c9ec2eb50d4,"Formal specifications can precisely and unambiguously define the required behavior of a software system or component. However, formal specifications are complex artifacts that need to be verified to ensure that they are consistent, complete, and validated against the requirements. Specification testing or animation tools exist to assist with this by allowing the specifier to interpret or execute the specification. However, currently little is known about how to do this effectively. This article presents a framework and tool support for the systematic testing of formal, model-based specifications. Several important generic properties that should be satisfied by model-based specifications are first identified. Following the idea of mutation analysis, we then use variants or mutants of the specification to check that these properties are satisfied. The framework also allows the specifier to test application-specific properties. All properties are tested for a range of states that are defined by the tester in the form of a testgraph, which is a directed graph that partially models the states and transitions of the specification being tested. Tool support is provided for the generation of the mutants, for automatically traversing the testgraph and executing the test cases, and for reporting any errors. The framework is demonstrated on a small specification and its application to three larger specifications is discussed. Experience indicates that the framework can be used effectively to test small to medium-sized specifications and that it can reveal a significant number of problems in these specifications.",Formal verification; Specification animation; Testgraphs; Testing,Automation; Boundary conditions; Graph theory; Software engineering; Specifications; Testing; Theorem proving; Formal verification; Model-based specifications; Specification animation; Testgraphs; Animation
A model-checking verification environment for mobile processes,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2542474345&doi=10.1145%2f990010.990013&partnerID=40&md5=85e148ef12d24639c3ed24b56aa66253,"This article presents a semantic-based environment for reasoning about the behavior of mobile systems. The verification environment, called HAL, exploits a novel automata-like model that allows finite-state verification of systems specified in the π-calculus. The HAL system is able to interface with several efficient toolkits (e.g. model-checkers) to determine whether or not certain properties hold for a given specification. We report experimental results on some case studies.",Mobile processes; Modal logics; Name-passing process calculi; Security; Transition systems,Computer programming; Computer simulation; Formal logic; Security systems; Semantics; Theorem proving; Mobile processes; Modal logics; Name-passing process calculi; Security; Transition systems; Finite automata
"Automatic high-quality reengineering of database programs by abstraction, transformation and reimplementation",2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442466828&doi=10.1145%2f958961.958962&partnerID=40&md5=8f1248ea1129d67cc8e778a501e5a40c,"Old-generation database models, such as the indexed-sequential, hierarchical, or network models, provide record-level access to their data, with all application logic residing in the hosting program. In contrast, relational databases can perform complex operations, such as filter, aggregation, and join, on multiple records without an external specification of the record-access logic. Programs written for relational databases attempt to move as much of the application logic as possible into the database, in order to make the most of the optimizations performed internally by the database. This conceptual gap between the programming styles makes automatic high-quality translation of programs written for the older database models to the relational model difficult. It is not enough to convert just the database-access operations, since this would result in unacceptably inefficient programs. It is necessary to convert parts of the application logic from the procedural style of the hosting program (which is almost always Cobol) to the declarative style of SQL. This article describes an automatic system, called MIDAS, that performs high-quality reengineering of legacy database programs in this way. MIDAS is based on the paradigm of translation by abstraction, transformation, and reimplementation. The abstract representation is based on the Plan Calculus, with the addition of Query Graphs, introduced in this article in order to abstract the temporal behavior of database access patterns. The results of MIDAS's translation were found to be superior to those of the naive translation that only converts database-access operations in terms of readability, size of code, speed, and network data traffic. Initial industrial experience with MIDAS also demonstrates the high quality of its translations on large-scale programs.",Database program reengineering; Query graphs; Temporal abstraction; The plan calculus,Computer programming; Database systems; Estimation; Graph theory; Optimization; Query languages; Reengineering; Database program reengineering; Query graph; Temporal abstraction; The plan calculus; Software engineering
Developing multiagent systems: The Gaia methodology,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442534971&doi=10.1145%2f958961.958963&partnerID=40&md5=14930c051dd80cd06e179dde75e9e2b9,"Systems composed of interacting autonomous agents offer a promising software engineering approach for developing applications in complex domains. However, this multiagent system paradigm introduces a number of new abstractions and design/development issues when compared with more traditional approaches to software development. Accordingly, new analysis and design methodologies, as well as new tools, are needed to effectively engineer such systems. Against this background, the contribution of this article is twofold. First, we synthesize and clarify the key abstractions of agent-based computing as they pertain to agent-oriented software engineering. In particular, we argue that a multiagent system can naturally be viewed and architected as a computational organization, and we identify the appropriate organizational abstractions that are central to the analysis and design of such systems. Second, we detail and extend the Gaia methodology for the analysis and design of multiagent systems. Gaia exploits the aforementioned organizational abstractions to provide clear guidelines for the analysis and design of complex and open software systems. Two representative case studies are introduced to exemplify Gaia's concepts and to show its use and effectiveness in different types of multiagent system.",Agent-oriented software engineering; Analysis and design methodologies; Distributed systems; Multiagent systems; Software architectures,Artificial intelligence; Computer aided design; Computer architecture; Distributed computer systems; Open systems; Software engineering; Agent-oriented software engineering; Analysis and design methodologies; Software architecture; Multi agent systems
Flexible consistency checking,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442459936&doi=10.1145%2f839268.839271&partnerID=40&md5=3442f183d1ea80c3b71e6dfa24553d58,"The problem of managing the consistency of heterogeneous, distributed software engineering documents is central to the development of large and complex systems. We show how this problem can be addressed using xlinkit, a lightweight framework for consistency checking that leverages standard Internet technologies. xlinkit provides flexibility, strong diagnostics, and support for distribution and document heterogeneity. We use xlinkit in a comprehensive case study that demonstrates how design, implementation and deployment information of an Enterprise JavaBeans system can be checked for consistency, and rechecked incrementally when changes are made.",CASE tools; Consistency management; Constraint checking; Multiple perspectives,Computer programming languages; Distributed computer systems; Internet; Large scale systems; Problem solving; XML; Consistency management; Constraint checking; Multiple perspectives; Computer aided software engineering
Higher-order architectural connectors,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442474189&doi=10.1145%2f839268.839272&partnerID=40&md5=b5d3e824f015ef51a3cdc9e32acfa92d,"We develop a notion of higher-order connector towards supporting the systematic construction of architectural connectors for software design. A higher-order connector takes connectors as parameters and allows for services such as security protocols and fault-tolerance mechanisms to be superposed over the interactions that are handled by the connectors passed as actual arguments. The notion is first illustrated over CommUnity, a parallel program design language that we have been using for formalizing aspects of architectural design. A formal, algebraic semantics is then presented which is independent of any Architectural Description Language. Finally, we discuss how our results can impact software design methods and tools.",Design; Languages; Theory,Computer programming; Mathematical models; Network protocols; Security of data; Semantics; Software prototyping; Global system; Software design; Software engineering
Feature specification and automated conflict detection,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442474190&doi=10.1145%2f839268.839270&partnerID=40&md5=532fe011fa84f0b246cde3bd9b622f1b,"Large software systems, especially in the telecommunications field, are often specified as a collection of features. We present a formal specification language for describing features, and a method of automatically detecting conflicts (""undesirable interactions"") amongst features at the specification stage. Conflict detection at this early stage can help prevent costly and time consuming problem fixes during implementation. Features are specified using temporal logic; two features conflict essentially if their specifications are mutually inconsistent under axioms about the underlying system behavior. We show how this inconsistency check may be performed automatically with existing model checking tools. In addition, the model checking tools can be used to provide witness scenarios, both when two features conflict as well as when the features are mutually consistent. Both types of witnesses are useful for refining the specifications. We have implemented a conflict detection tool, FIX (Feature Interaction eXtractor), which uses the model checker COSPAN for the inconsistency check. We describe our experience in applying this tool to a collection of telecommunications feature specifications obtained from the Telcordia (Bellcore) standards. Using FIX, we were able to detect most known interactions and some new ones, fully automatically, in a few hours processing time.",Feature interaction; Linear temporal logic; Telecommunications software and systems,Automation; Computational complexity; Feature extraction; Mathematical models; Network protocols; Problem solving; Telecommunication systems; Feature interaction; Linear temporal logic; Telecommunication software and systems; Software engineering
ACM Transaction on Software Engineering and Methodology: Editorial,2003,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442430466&doi=10.1145%2f839268.839269&partnerID=40&md5=888c25ecfe8788ac08faf922941a858b,[No abstract available],,
Alloy: A Lightweight Object Modelling Notation,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038507387&doi=10.1145%2f505145.505149&partnerID=40&md5=64484f730c33f081f095edc5dba634d7,"Alloy is a little language for describing structural properties. It offers a declaration syntax compatible with graphical object models, and a set-based formula syntax powerful enough to express complex constraints and yet amenable to a fully automatic semantic analysis. Its meaning is given by translation to an even smaller (formally defined) kernel. This paper presents the language in its entirety, and explains its motivation, contributions and deficiencies. © 2002, ACM. All rights reserved.",Design; Documentation; first-order logic; Languages; Object models; Z specification language,Automation; Computer graphics; Constraint theory; Formal logic; Semantics; Software engineering; First-order logic; Object models; Translation; Z specification language; Computer programming languages
Architecting families of software systems with process algebras,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542325378&doi=10.1145%2f606612.606614&partnerID=40&md5=a5d75b2511dfbbc951e0465e996e1f5a,"Software components can give rise to several kinds of architectural mismatches when assembled together in order to form a software system. A formal description of the architecture of the resulting component-based software system may help to detect such architectural mismatches and to single out the components that cause the mismatches. In this article, we concentrate on deadlock-related architectural mismatches arising from three different causes that we identify: incompatibility between two components due to a single interaction, incompatibility between two components due to the combination of several interactions, and lack of interoperability among a set of components forming a cyclic topology. We develop a process algebra-based architectural description language called PADL, which deals with all three causes through an architectural compatibility check and an architectural interoperability check relying on standard observational equivalences. The adequacy of the architectural compatibility check is assessed on a compressing proxy system, while the adequacy of the architectural interoperability check is assessed on a cruise control system. We then address the issue of scaling the architectural compatibility and interoperability checks to architectural styles through an extension of PADL. The formalization of an architectural style is complicated by the presence of two degrees of freedom within the set of instances of the style: variability of the internal behavior of the components and variability of the topology formed by the components. As a first step towards the solution of the problem, we propose an intermediate abstraction called architectural type, whose instances differ only for the internal behavior of their components. We define an efficient architectural conformity check based on a standard observa-tional equivalence to verify whether an architecture is an instance of an architectural type. We show that all the architectures conforming to the same architectural type possess the same compatibility and interoperability properties.",Architectural mismatch detection; Architectural styles; Design; Languages; Process algebras; Software architectures; Verification,Algebra; Computer architecture; Computer programming languages; Degrees of freedom (mechanics); Interpolation; Topology; Architectural compatibility; Component-based software systems; Software components; Software systems; Computer software
Behavior-consistent specialization of object life cycles,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542430430&doi=10.1145%2f504087.504091&partnerID=40&md5=5589ca73f61dca1e1477a3b25ef8b2bc,"Object-oriented design methodologies represent the behavior of instances of an object type not merely by a set of operations, but also by providing an overall description on how instances evolve over time. Such a description is often referred to as ""object life cycle."" Object-oriented systems organize object types in hierarchies in which subtypes inherit and specialize the structure and behavior of their supertypes. Past experience has shown that unrestricted use of inheritance mechanisms leads to system architectures that are hard to understand and to maintain, since arbitrary differences between supertype and subtype are possible. Evidently, this is not a desirable state of affairs and the behavior of a subtype should specialize the behavior of its supertype according to some clearly defined consistency criteria. Such criteria have been formulated in terms of type systems for semantic data models and object-oriented programming languages. But corresponding criteria for the specialization of object life cycles have so far not been thoroughly investigated. This paper defines such criteria in the realm of Object Behavior Diagrams, which have been originally developed for the design of object-oriented databases. Its main contributions are necessary and sufficient rules for checking behavior consistency between object life cycles of object types in specialization hierarchies with multiple inheritance.",Conceptual modeling; D.2.2 [Software Engineering]: Design Tools and Techniques - Object-oriented design methods; Design; Extension; Inheritance; Invocation consistency; Object behavior; Observation consistency; Refinement,Computer programming languages; Life cycle; Maps; Object oriented programming; Semantics; Theorem proving; Conceptual modeling; Invocation consistency; Object behavior; Refinement; Software engineering
An inheritance-based technique for building simulation proofs incrementally,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037999403&doi=10.1145%2f504087.504090&partnerID=40&md5=783321164b7067c2e14a861c49444c68,"This paper presents a formal technique for incremental construction of system specifications, algorithm descriptions, and simulation proofs showing that algorithms meet their specifications. The technique for building specifications and algorithms incrementally allows a child specification or algorithm to inherit from its parent by two forms of incremental modification: (a) signature extension, where new actions are added to the parent, and (b) specialization (subtyping), where the child's behavior is a specialization (restriction) of the parent's behavior. The combination of signature extension and specialization provides a powerful and expressive incremental modification mechanism for introducing new types of behavior without overriding behavior of the parent; this mechanism corresponds to the subclassing for extension form of inheritance. In the case when incremental modifications are applied to both a parent specification S and a parent algorithm A, the technique allows a simulation proof showing that the child algorithm A′ implements the child specification S′ to be constructed incrementally by extending a simulation proof that algorithm A implements specification S. The new proof involves reasoning about the modifications only, without repeating the reasoning done in the original simulation proof. The paper presents the technique mathematically, in terms of automata. The technique has been used to model and verify a complex middleware system; the methodology and results of that experiment are summarized in this paper.","D.2.1 [Software Engineering]: Requirements/Specifications - Methodologies (e.g., object-oriented, structured); F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs; Verification",Algorithms; Automata theory; Computer simulation; Cost effectiveness; Large scale systems; Incremental proof techniques; Proof reuse; Refinements; Simulating proofs; Software engineering
Automated abstraction of class diagrams,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11244349651&doi=10.1145%2f606612.606616&partnerID=40&md5=1de306f5eef5ceaff59580e51d847bba,"Designers can easily become overwhelmed with details when dealing with large class diagrams. This article presents an approach for automated abstraction that allows designers to ""zoom out"" on class diagrams to investigate and reason about their bigger picture. The approach is based on a large number of abstraction rules that individually are not very powerful, but when used together, can abstract complex class structures quickly. This article presents those abstraction rules and an algorithm for applying them. The technique was validated on over a dozen models where it was shown to be well suited for model understanding, consistency checking, and reverse engineering.",Class abstraction; Class diagrams; Class patterns; Design; Reverse engineering; Transformation; Unified modeling language,Abstracting; Algorithms; Logic programming; Reverse engineering; Class abstraction; Class patterns; Transformation; Unified modeling language; Software engineering
On fault classes and error detection capability of specification-based testing,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842421482&doi=10.1145%2f504087.504089&partnerID=40&md5=9e5f84a04fff0cbc4253fd981b4a7183,"In a previous paper, Kuhn [1999] showed that faults in Boolean specifications constitute a hierarchy with respect to detectability, and drew the conclusion that missing condition faults should be hypothesized to generate tests. However this conclusion was premature, since the relationships between missing condition faults and faults in other classes have not been sufficiently analyzed. In this note, we investigate such relationships, aiming to complement the work of Kuhn. As a result, we obtain an extended hierarchy of fault classes and reach a different conclusion.",D.2.1 [Software Engineering]: Requirements/Specifications; D.2.5 [Software Engineering]: Testing and Debugging; Fault classes; Specification-based testing; Verification,Boolean algebra; Error detection; Testing; Theorem proving; Boolean specifications; Fault classes; Specification-based testing; Software engineering
Mixin layers: An object-oriented implementation technique for refinements and collaboration-based designs,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141653318&doi=10.1145%2f505145.505148&partnerID=40&md5=3297864e82c41362945f1df2dc75c875,"A ""refinement"" is a functionality addition to a software project that can affect multiple dispersed implementation entities (functions, classes, etc.). In this paper, we examine large-scale refinements in terms of a fundamental object-oriented technique called collaboration-based design. We explain how collaborations can be expressed in existing programming languages or can be supported with new language constructs (which we have implemented as extensions to the Java language). We present a specific expression of large-scale refinements called mixin layers, and demonstrate how it overcomes the scalability difficulties that plagued prior work. We also show how we used mixin layers as the primary implementation technique for building an extensible Java compiler, JTS.",Collaboration-based design; Component-based software; Product-line architectures,Computer architecture; Computer supported cooperative work; Java programming language; Large scale systems; Object oriented programming; Program compilers; Collaboration-based design; Component-based software; Large-scale refinements; Product-line architectures; Software engineering
Temporal abstract classes and virtual temporal specifications for real-time systems,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344578358&doi=10.1145%2f567793.567794&partnerID=40&md5=5e8e646168032f6390e983b3497b8253,"The design and development of real-time systems is often a difficult and time-consuming task. System realization has become increasingly difficult due to the proliferation of larger and more complex applications. To offset some of these difficulties, real-time developers have turned to object-oriented methodology. The success of object-oriented concepts in the development of non-real-time programs motivates the relevance of these concepts to achieve similar gains from encapsulation and code reuse in the real-time domain. This article presents an approach of integrating real-time constraint specifications within the constructs of an object-oriented language, affording these constraints a status equivalent to other language elements. This has led to the definition of such novel concepts as temporal abstract classes, virtual temporal constraints, and temporal specification inheritance, which extends inheritance mechanisms to accommodate real-time constraint specifications. These extensions provide real-time developers with the ability to manage and maintain the temporal behavior of a real-time program in a comparable manner to its functional behavior.",D.3 [Software]: Programming Languages; D.3.2 [Programming Languages]: Language Classifications - Object-oriented languages; D.3.3 [Programming Languages]: Language Constructs and Features - Constraints; Design; Languages; Temporal constraint specification,C (programming language); Codes (symbols); Constraint theory; Object oriented programming; Software engineering; Time domain analysis; Virtual reality; Code sequences; Inheritance; Real-tine constraints; Temporal constraint specification; Real time systems
A formal design notation for real-time systems,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11244315081&doi=10.1145%2f505145.505146&partnerID=40&md5=ce423088beaf771fcc462cc5bda9f6e2,"The development of real-time systems is based on a variety of different methods and notations. Despite the purported benefits of formal methods, informal techniques still play a predominant role in current industrial practice. Formal and informal methods have been combined in various ways to smoothly introduce formal methods in industrial practice. The combination of real-time structured analysis (SA-RT) with Petri nets is among the most popular approaches, but has been applied only to requirements specifications. This paper extends SA-RT to specifications of the detailed design of embedded real-time systems, and combines the proposed notation with Petri nets.",Design of real time systems; Formal analysis of design specification; Formal design specification; Structured design,Embedded systems; Petri nets; Real time systems; Structural design; Structured programming; Design of real time systems; Formal analysis of design specification; Formal design specification; Structured design; Software engineering
Achieving extensibility through product-lines and domain-specific languages: A case study,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942713451&doi=10.1145%2f505145.505147&partnerID=40&md5=cacda533bf8ec0099c37acc161f5d719,"This is a case study in the use of product-line architectures (PLAs) and domain-specific languages (DSLs) to design an extensible command-and-control simulator for Army fire support. The reusable components of our PLA are layers or ""aspects"" whose addition or removal simultaneously impacts the source code of multiple objects in multiple, distributed programs. The complexity of our component specifications is substantially reduced by using a DSL for defining and refining state machines, abstractions that are fundamental to simulators. We present preliminary results that show how our PLA and DSL synergistically produce a more flexible way of implementing state-machine-based simulators than is possible with a pure Java implementation.",Aspects; Domain-specific languages; GenVoca; Refinements; Simulation,Codes (symbols); Computer architecture; Computer simulation; Computer software; Distributed computer systems; Simulators; Aspects; Domain-specific languages (DSLs); GenVoca; Product-line architectures (PLAs); Refinements; Computer programming languages
Equivalence analysis and its application in improving the efficiency of program slicing,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442614999&doi=10.1145%2f567793.567796&partnerID=40&md5=7f456481a8d77d3b07b908628bd6cef5,"Existing methods for handling pointer variables during dataflow analyses can make such analyses inefficient in both time and space because the data-flow analyses must store and propagate large sets of data facts that are introduced by dereferences of pointer variables. This article presents equivalence analysis, a general technique to improve the efficiency of data-flow analyses in the presence of pointer variables. The technique identifies equivalence relations among the memory locations accessed by a procedure, and ensures that two equivalent memory locations share the same set of data facts in a procedure and in the procedures that are called by that procedure. Thus, a data-flow analysis needs to compute the data-flow information for only a representative memory location in an equivalence class. The data-flow information for other memory locations in the equivalence class can be derived from that of the representative memory location. The article also shows the extension to an interprocedural slicing algorithm that uses equivalence analysis to improve the efficiency of the algorithm. Our empirical studies suggest that equivalence analysis may effectively improve the efficiency of many data-flow analyses.","D.2.5 [Software Engineering]: Testing and Debugging - Testing tools; D.2.7 [Software Engineering]: Distribution, Maintenance, and Enhancement - Restructuring, reverse engineering, and reengineering",Algorithms; Computer science; Data reduction; Data storage equipment; Database systems; Mathematical models; Optimization; Program compilers; Program debugging; Software engineering; Alias analysis; Data-flow analysis; Memory locations; Program slicing; Computer programming
Two case studies of open source software development: Apache and Mozilla,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142149841&doi=10.1145%2f567793.567795&partnerID=40&md5=9cb1afddffebdb1a8d9e739e385c52c3,"According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine data from two major open source projects, the Apache web server and the Mozilla browser. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution intervals for these OSS projects. We develop several hypotheses by comparing the Apache project with several commercial projects. We then test and refine several of these hypotheses, based on an analysis of Mozilla data. We conclude with thoughts about the prospects for high-performance commercial/open source process hybrids.","D.2.8 [Software Engineering] - Process metrics, Product metrics; D.2.9 [Software Engineering] - Life cycle, Productivity, Programming teams, Software process models, Software Quality assurance, Time estimation",Computer science; Database systems; Marketing; Philosophical aspects; Servers; Web browsers; World Wide Web; Apache; Code ownership; Defect density; Mozilla; Open source software; Repair intervals; Software engineering
Comparing test sets and criteria in the presence of test hypotheses and fault domains,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11244343902&doi=10.1145%2f606612.606615&partnerID=40&md5=33ef5d84bb7ca8c10a00f87ceef54cd9,"A number of authors have considered the problem of comparing test sets and criteria. Ideally test sets are compared using a preorder with the property that test set T1 is at least as strong as T2 if whenever T2 determines that an implementation p is faulty, T1 will also determine that p is faulty. This notion can be extended to test criteria. However, it has been noted that very few test sets and criteria are comparable under such an ordering; instead orderings are based on weaker properties such as subsumes. This article explores an alternative approach, in which comparisons are made in the presence of a test hypothesis or fault domain. This approach allows strong statements about fault detecting ability to be made and yet for a number of test sets and criteria to be comparable. It may also drive incremental test generation.",Theory; Verification,Set theory; Fault domains; Subsumes; Test sets; Verification; Software engineering
Modeling software architectures in the unified modeling language,2002,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0006101465&doi=10.1145%2f504087.504088&partnerID=40&md5=808b394ad1af747f4aedd158e216f0e3,"The Unified Modeling Language (UML) is a family of design notations that is rapidly becoming a de facto standard software design language. UML provides a variety of useful capabilities to the software designer, including multiple, interrelated design views, a semiformal semantics expressed as a UML meta model, and an associated language for expressing formal logic constraints on design elements. The primary goal of this work is an assessment of UML's expressive power for modeling software architectures in the manner in which a number of existing software architecture description languages (ADLs) model architectures. This paper presents two strategies for supporting architectural concerns within UML. One strategy involves using UML ""as is,"" while the other incorporates useful features of existing ADLs as UML extensions. We discuss the applicability, strengths, and weaknesses of the two strategies. The strategies are applied on three ADLs that, as a whole, represent a broad cross-section of present-day ADL capabilities. One conclusion of our work is that UML currently lacks support for capturing and exploiting certain architectural concerns whose importance has been demonstrated through the research and practice of software architectures. In particular, UML lacks direct support for modeling and exploiting architectural styles, explicit software connectors, and local and global architectural constraints.",C2; D.2.11 [Software Engineering]: Software Architectures; D.2.2 [Software Engineering]: Design Tools and Techniques; Design; Formal modeling; Languages; Object Constraint Language; Object-oriented design; Rapide; Software architecture; Standardization,Constraint theory; Mapping; Object oriented programming; Semantics; Formal modeling; Software architecture; Unified modeling language (UML); Software engineering
Modeling statecharts and activitycharts as signal equations,2001,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040486797&doi=10.1145%2f384189.384191&partnerID=40&md5=aaf412eb252af3e18e4b9afbfe1e0f24,"The languages for modeling reactive systems are of different styles, like the imperative, state-based ones and the declarative, data-flow ones. They are adapted to different application domains. This paper, through the example of the languages Statecharts and Signal, shows a way to give a model of an imperative specification (Statecharts) in a declarative, equational one (Signal). This model constitutes a formal model of the Statemate semantics of Statecharts, upon which formal analysis techniques can be applied. Being a transformation from an imperative to a declarative structure, it involves the definition of generic models for the explicit management of state (in the case of control as well as of data). In order to obtain a structural construction of the model, a hierarchical and modular organization is proposed, including proper management and propagation of control along the hierarchy. The results presented here cover the essential features of Statecharts as well as of another language of Statemate: Activitycharts. As a translation, it makes multiformalism specification possible, and provides support for the integrated operation of the languages. The motivation lies also in the perspective of gaining access to the various formal analysis and implementation tools of the synchronous technology, using the DC+ exchange format, as in the Sacres programming environment.",Behavioral modeling; Reactive systems; Signal; Statecharts; Statemate; Synchronous languages,
Automated deductive requirements analysis of critical systems,2001,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010021963&doi=10.1145%2f383876.383877&partnerID=40&md5=17d7e237e770e064bb560acd0c89208b,"We advocate the need for automated support to System Requirement Analysis in the development of time-and safety-critical computer-based systems. To this end we pursue an approach based on deductive analysis: high-level, real-world entities and notions, such as events, states, finite variability, cause-effect relations, are modeled through the temporal logic TRIO, and the resulting deductive system is implemented by means of the theorem prover PVS. Throughout the paper, the constructs and features of the deductive system are illustrated and validated by applying them to the well-known example of the Generalized Railway Crossing.","C.3 [Computer Systems Organization]: Special-Purpose and Application-Based Systems - Real-time and embedded systems; D.2.1 [Software Engineering]: Requirements/Specifications - Methodologies (e.g., object-oriented, structured)",
Parallel changes in large-scale software development: An observational case study,2001,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012957822&doi=10.1145%2f383876.383878&partnerID=40&md5=1c72a0a97866650a6ffa44c5509e7237,"An essential characteristic of large-scale software development is parallel development by teams of developers. How this parallel development is structured and supported has a profound effect on both the quality and timeliness of the product. We conduct an observational case study in which we collect and analyze the change and configuration management history of a legacy system to delineate the boundaries of, and to understand the nature of, the problems encountered in parallel development. The results of our studies are (1) that the degree of parallelism is very high - higher than considered by tool builders; (2) there are multiple levels of parallelism, and the data for some important aspects are uniform and consistent for all levels; (3) the tails of the distributions are long, indicating the tail, rather than the mean, must receive serious attention in providing solutions for these problems; and (4) there is a significant correlation between the degree of parallel work on a given component and the number of quality problems it has. Thus, the results of this study are important both for tool builders and for process and project engineers.","D.2.7 [Software Engineering]: Distribution, Maintenance, and Enhancement - Version control; D.2.8 [Software Engineering]: Metrics - Process metrics; D.2.9 [Software Engineering]: Management - Programming teams; Software configuration management",
Designing data marts for data warehouses,2001,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013106810&doi=10.1145%2f384189.384190&partnerID=40&md5=3691a15b5cd3dda4539be3da01d84aca,"Data warehouses are databases devoted to analytical processing. They are used to support decision-making activities in most modern business settings, when complex data sets have to be studied and analyzed. The technology for analytical processing assumes that data are presented in the form of simple data marts, consisting of a well-identified collection of facts and data analysis dimensions (star schema). Despite the wide diffusion of data warehouse technology and concepts, we still miss methods that help and guide the designer m identifying and extracting such data marts out of an enterprisewide information system, covering the upstream, requirement-driven stages of the design process. Many existing methods and tools support the activities related to the efficient implementation of data marts on top of specialized technology (such as the ROLAP or MOLAP data servers). This paper presents a method to support the identification and design of data marts. The method is based on three basic steps A first top-down step makes it possible to elicit and consolidate user requirements and expectations. This is accomplished by exploiting a goal-oriented process based on the Goal/Question/Metric paradigm developed at the University of Maryland. Ideal data marts are derived from user requirements. The second bottom-up step extracts candidate data marts from the conceptual schema of the information system. The final step compares ideal and candidate data marts to derive a collection of data marts that are supported by the underlying information system and maximally satisfy user requirements. The method presented in this paper has been validated in a real industrial case study concerning the business processes of a large telecommunications company.",Conceptual modeling; Data mart; Data warehouse; Design method; Software quality management,
Reasoning about code mobility with mobile UNITY,2001,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040027498&doi=10.1145%2f383876.383879&partnerID=40&md5=556113e0deccfd7a45e995912804a9c6,"Advancements in network technology have led to the emergence of new computing paradigms that challenge established programming practices by employing weak forms of consistency and dynamic forms of binding. Code mobility, for instance, allows for invocation-time binding between a code fragment and the location where it executes. Similarly, mobile computing allows hosts (and the software they execute) to alter their physical location. Despite apparent similarities, the two paradigms are distinct in their treatment of location and movement. This paper seeks to uncover a common foundation for the two paradigms by exploring the manner in which stereotypical forms of code mobility can be expressed in a programming notation developed for mobile computing. Several solutions to a distributed simulation problem are used to illustrate the modeling strategy and the ability to construct assertional-style proofs for programs that employ code mobility.",C.2.4 [Computer-Communication Networks]: Distributed Systems - Distributed applications; D.2.1 [Software Engineering]: Requirements/Specifications; D.2.4 [Software Engineering]: Software/Program Verification - Correctness proofs,
An empirical study of regression test selection techniques,2001,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011783904&doi=10.1145%2f367008.367020&partnerID=40&md5=6958a4356bbd03e487762cf5d1d0a270,"Regression testing is the process of validating modified software to detect whether new errors have been introduced into previously tested code and to provide confidence that modifications are correct. Since regression testing is an expensive process, researchers have proposed regression test selection techniques as a way to reduce some of this expense. These techniques attempt to reduce costs by selecting and running only a subset of the test cases in a program's existing test suite. Although there have been some analytical and empirical evaluations of individual techniques, to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been reported in the literature. We conducted an experiment to examine the relative costs and benefits of several regression test selection techniques. The experiment examined five techniques for reusing test cases, focusing on their relative abilities to reduce regression testing effort and uncover faults in modified programs. Our results highlight several differences between the techniques, and expose essential trade-offs that should be considered when choosing a technique for practical application.","D.2.5 [software engineering]: Testing and debugging - Testing tools (e.g., data generators, coverage testing); Debugging aids; Empirical study; Regression testing; Selective retest",
A comparative study of coarse- and fine-grained safe regression test-selection techniques,2001,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000699907&doi=10.1145%2f367008.367015&partnerID=40&md5=736813982503428cb111390b5b5c83fa,"Regression test-selection techniques reduce the cost of regression testing by selecting a subset of an existing test suite to use in retesting a modified program. Over the past two decades, numerous regression test-selection techniques have been described in the literature. Initial empirical studies of some of these techniques have suggested that they can indeed benefit testers, but so far, few studies have empirically compared different techniques. In this paper, we present the results of a comparative empirical study of two safe regression test-selection techniques. The techniques we studied have been implemented as the tools DejaVu and TestTube; we compared these tools in terms of a cost model incorporating precision (ability to eliminate unnecessary test cases), analysis cost, and test execution cost. Our results indicate, that in many instances, despite its relative lack of precision, TestTube can reduce the time required for regression testing as much as the more precise DejaVu. In other instances, particularly where the time required to execute test cases is long, DejaVu's superior precision gives it a clear advantage over TestTube. Such variations in relative performance can complicate a tester's choice of which tool to use. Our experimental results suggest that a hybrid regression test-selection tool that combines features of TestTube and DejaVu may be an answer to these complications; we present an initial case study that demonstrates the potential benefit of such a tool.",Algorithms; D.2.5 [software engineering]: Testing and debugging; Regression test selection; Regression testing; Verification,
Interprocedural control dependence,2001,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040027541&doi=10.1145%2f367008.367022&partnerID=40&md5=1c980549d7cd63ca1a7a0f3956c61792,"Program-dependence information is useful for a variety of applications, such as software testing and maintenance tasks, and code optimization. Properly defined, control and data dependences can be used to identify semantic dependences. To function effectively on whole programs, tools that utilize dependence information require information about interprocedural dependences: dependences that are identified by analyzing the interactions among procedures. Many techniques for computing interprocedural data dependences exist; however, virtually no attention has been paid to interprocedural control dependence. Analysis techniques that fail to account for interprocedural control dependences can suffer unnecessary imprecision and loss of safety. This article presents a definition of interprocedural control dependence that supports the relationship of control and data dependence to semantic dependence. The article presents two approaches for computing interprocedural control dependences, and empirical results pertaining to the use of those approaches.","D.2.5 [software engineering]: Testing and debugging - Debugging aids; Testing tools (e.g., data generators, coverage testing)",
A Slicing-Based Approach for Locating Type Errors,2001,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037580003&doi=10.1145%2f366378.366379&partnerID=40&md5=db5372435b962d732f14575270ca3b4a,"The effectiveness of a type-checking tool strongly depends on the accuracy of the positional information that is associated with type errors. We present an approach where the location associated with an error message e is defined as a slice Pe of the program P being type-checked. We show that this approach yields highly accurate positional information: Pe is a program that contains precisely those program constructs in P that caused error e. Semantically, we have the interesting property that type-checking Pe is guaranteed to produce the same error e. Our approach is completely language-independent and has been implemented for a significant subset of Pascal. We also report on experiments with object-oriented type systems, and with a subset of ML.",D.2.1 [Software Engineering]: Requirements/Specifications - Languages; D.3.4 [Programming Languages]: Processors - Translator writing systems and compiler generators; Tools,
TACCLE: A Methodology for Object-Oriented Software Testing at the Class and Cluster Levels,2001,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002908054&doi=10.1145%2f366378.366380&partnerID=40&md5=de794786f60ab143f25c2e2ec7386e33,"Object-oriented programming consists of several different levels of abstraction, namely, the algorithmic level, class level, cluster level, and system level. The testing of object-oriented software at the algorithmic and system levels is similar to conventional program testing. Testing at the class and cluster levels poses new challenges. Since methods and objects may interact with one another with unforeseen combinations and invocations, they are much more complex to simulate and test than the hierarchy of functional calls in conventional programs. In this paper, we propose a methodology for object-oriented software testing at the class and cluster levels. In class-level testing, it is essential to determine whether objects produced from the execution of implemented systems would preserve the properties defined by the specification, such as behavioral equivalence and nonequivalence. Our class-level testing methodology addresses both of these aspects. For the testing of behavioral equivalence, we propose to select fundamental pairs of equivalent ground terms as test cases using a black-box technique based on algebraic specifications, and then determine by means of a white-box technique whether the objects resulting from executing such test cases are observationally equivalent. To address the testing of behavioral nonequivalence, we have identified and analyzed several nontrivial problems in the current literature. We propose to classify term equivalence into four types, thereby setting up new concepts and deriving important properties. Based on these results, we propose an approach to deal with the problems in the generation of nonequivalent ground terms as test cases. Relatively little research has contributed to cluster-level testing. In this paper, we also discuss black-box testing at the cluster level. We illustrate the feasibility of using Contract, a formal specification language for the behavioral dependencies and interactions among cooperating objects of different classes in a given cluster. We propose an approach to test the interactions among different classes using every individual message-passing rule in the given Contract specification. We also present an approach to examine the interactions among composite message-passing sequences. We have developed four testing tools to support our methodology.","D.2.1 [Software Engineering]: Requirements/Specifications - Languages; D.2.5 [Software Engineering]: Testing and Debugging - Testing tools (e.g., data generators, coverage testing)",
A Methodology for Testing Spreadsheets,2001,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000377320&doi=10.1145%2f366378.366385&partnerID=40&md5=2a92600b1512575b3840a50bf5fd23d6,"Spreadsheet languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that spreadsheets often contain faults; thus, we would like to provide at least some of the benefits of formal testing methodologies to the creators of spreadsheets. This article presents a testing methodology that adapts data flow adequacy criteria and coverage monitoring to the task of testing spreadsheets. To accommodate the evaluation model used with spreadsheets, and the interactive process by which they are created, our methodology is incremental. To accommodate the users of spreadsheet languages, we provide an interface to our methodology that does not require an understanding of testing theory. We have implemented our testing methodology in the context of the Forms/3 visual spreadsheet language. We report on the methodology, its time and space costs, and the mapping from the testing strategy to the user interface. In an empirical study, we found that test suites created according to our methodology detected, on average, 81% of the faults in a set of faulty spreadsheets, significantly outperforming randomly generated test suites.",Algorithms; D.1.7 [Programming Techniques]: Visual Programming; D.2.5 [Software Engineering]: Testing and Debugging; D.2.6 [Software Engineering]: Programming Environments; H.4.1 [Information Systems Applications]: Office Automation; Languages,
Abstracting dependencies between software configuration items,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0004734138&doi=10.1145%2f332740.332743&partnerID=40&md5=784c06449dc2f2a9d5e58542228e8a77,"This article studies an abstract model of dependencies between software configuration items based on a theory of concurrent computation over a class of Petri nets called production nets. A general theory of build optimizations and their correctness is developed based on a form of abstract interpretation called a build abstraction; these are created during a build and are used to optimize subsequent builds. Various examples of such optimizations are discussed. The theory is used to show how correctness properties can be characterized and proved, and how optimizations can be composed and compared. Categories and Subject Descriptors: D.2.4 [Software Engineering]: Software/Program Verification; D.2.7 [Software Engineering]: Distribution, Maintenance, and Enhancement; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages General Terms: Languages, Management, Verification.",Abstract interpretation; Mathematical models of build dependencies; Petri nets; Software configuration management,
Using a coordination language to specify and analyze systems containing mobile components,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011314848&doi=10.1145%2f350887.350893&partnerID=40&md5=0c33d563b6f332c6d167fcf1c7797847,"New computing paradigms for network-aware applications need specification languages able to deal with the features of mobile code-based systems. A coordination language provides a formal framework in which the interaction of active entities can be expressed. A coordination language deals with the creation and destruction of code or complex agents, their communication activities, as well as their distribution and mobility in space. We show how the coordination language PoliS offers a flexible basis for the description and the automatic analysis of architectures of systems including mobile entities. PoliS is based on multiple tuple spaces and offers a basis for defining, studying, and controlling mobility as it allows decoupling mobile entities from their environment both in space and in time. The pattern-matching mechanism adopted for communication helps in abstracting from addressing issues. We have developed a model-checking technique for the automatic analysis of PoliS specifications. In the article we show how this technique can be applied to mobile code-based systems.",,
Using shape analysis to reduce finite-state models of concurrent Java programs,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001797054&doi=10.1145%2f332740.332741&partnerID=40&md5=0daff60d2e33d941d3c623d4a19cf701,"Finite-state verification (e.g., model checking) provides a powerful means to detect concurrency errors, which are often subtle and difficult to reproduce. Nevertheless, widespread use of this technology by developers is unlikely until tools provide automated support for extracting the required finite-state models directly from program source. Unfortunately, the dynamic features of modern languages such as Java complicate the construction of compact finite-state models for verification. In this article, we show how shape analysis, which has traditionally been used for computing alias information in optimizers, can be used to greatly reduce the size of finite-state models of concurrent Java programs by determining which heap-allocated variables are accessible only by a single thread, and which shared variables are protected by locks We also provide several other state-space reductions based on the semantics of Java monitors. A prototype implementation of the reductions demonstrates their effectiveness. Categories and Subject Descriptors: D.2.4 [Software Engineering]: Software/Program Veriflcation General Terms: Verification.",Concurrent systems; Finite-state verification; Java; Model extraction; Modeling; Shape analysis; State-space reductions,
A knowledge-based method for inferring semantic concepts from visual models of system behavior,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008874836&doi=10.1145%2f352591.352594&partnerID=40&md5=2840ff1fa30a1f404954037fd8bfbbf8,"Software designers use visual models, such as data flow/control flow diagrams or object collaboration diagrams, to express system behavior in a form that can be understood easily by users and by programmers, and from which designers can generate a software architecture. The research described in this paper is motivated by a desire to provide an automated designer's assistant that can generate software architectures for concurrent systems directly from behavioral models expressed visually as flow diagrams. To achieve this goal, an automated designer's assistant must be capable of interpreting flow diagrams in semantic, rather than syntactic, terms. While semantic concepts can be attached manually to diagrams using labels, such as stereotypes in the Unified Modeling Language (UML), this paper considers the possibility of providing automated assistance to infer appropriate tags for symbols on a flow diagram. The approach relies upon constructing an underlying metamodel that defines semantic concepts based upon (1) syntactic relationships among visual symbols and (2) inheritance relationships among semantic concepts. Given such a metamodel, a rule-based inference engine can, in many situations, infer the presence of semantic concepts on a flow diagram, and can tag symbols accordingly. Further, an object-oriented query system can compare semantic tags on diagram instances for conformance with their definition in the metamodel. To illustrate the approach, the paper describes a metamodel for data flow/control flow diagrams used in the context of a specific software modeling method, Concurrent Object-Based Real-time Analysis (COBRA). The metamodel is implemented using an expert-system shell, CLIPS V6.0, which integrates an object-oriented language with a rule-based inference engine. The paper applies the implemented metamodel to design software for an automobile cruise-control system and provides an evaluation of the approach based upon results from four case studies. For the case studies, the implemented metamodel recognized, automatically and correctly, the existence of 86% of all COBRA semantic concepts within the flow diagrams. Varying degrees of human assistance were used to correctly identify the remaining semantic concepts within the diagrams: in two percent of the cases the implemented metamodel reached tentative classifications that a designer was asked to confirm or override; in four percent of the cases a designer was asked to provide additional information before a concept was classified; in the remaining eight percent of the cases the designer was asked to identify the concept.",Concept classification systems; Concurrent systems; Design; Experimentation; Knowledge-based software engineering; Measurement; Semantic data modeling; Software design methods; Visual modeling,
Verifying security protocols with Brutus,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039334690&doi=10.1145%2f363516.363528&partnerID=40&md5=3b87c22de8da1237be1cca6845ce21bf,"Due to the rapid growth of the ""Internet"" and the ""World Wide Web"" security has become a very important concern in the design and implementation of software systems. Since security has become an important issue, the number of protocols in this domain has become very large. These protocols are very diverse in nature. If a software architect wants to deploy some of these protocols in a system, they have to be sure that the protocol has the right properties as dictated by the requirements of the system. In this article we present BRUTUS, a tool for verifying properties of security protocols. This tool can be viewed as a special-purpose model checker for security protocols. We also present reduction techniques that make the tool efficient. Experimental results are provided to demonstrate the efficiency of BRUTUS.",Authentication and secure payment protocols; D.2.4 [Software Engineering]: Software/Program Verification - Model checking; D.4.6 [Operating Systems]: Security and Protection - Verification; Formal methods; Model-checking; Security; Verification,
SMC: A symmetry-based model checker for verification of safety and liveness properties,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039332712&doi=10.1145%2f350887.350891&partnerID=40&md5=884cc941da47901b595d68b41aa9805a,"The article presents the SMC system. SMC can be used for checking safety and liveness properties of concurrent programs under different fairness assumptions. It is based on explicit state enumeration. It combats the state explosion problem by exploiting symmetries of the input concurrent program, usually present in the form of identical processes, in two different ways. Firstly, it reduces the number of explored states by identifying those states that are equivalent under the symmetries of the system; this is called process symmetry. Secondly, it reduces the number of edges explored from each state, in the reduced state graph, by exploiting the symmetry of a single state; this is called state symmetry. SMC works in an on-the-fly manner; it constructs the reduced state graph as and when it is needed. This method facilitates early termination, speeds up model checking, and reduces memory requirements. We employed SMC to check the correctness of, among other standard examples, the Link Layer part of the IEEE Standard 1394 ""Firewire"" high-speed serial bus protocol. SMC found deadlocks in the protocol. SMC was also used to check certain liveness properties. A report on the case study is included in the article.",D.2.4 [Software Engineering]: Software/Program Verification - Model checking; Formal methods; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - Mechanical verification,
A generic model for reflective design,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039332710&doi=10.1145%2f350887.350895&partnerID=40&md5=3756386fa767b040fe6b671d6483e2f8,"Rapid technological change has had an impact on the nature of software. This has led to new exigencies and to demands for software engineering paradigms that pay particular attention to meeting them. We advocate that such demands can be met, at least in large parts, through the adoption of software engineering processes that are founded on a reflective stance. To this end we turn our attention to the field of Design Rationale. We analyze and characterize Design Rationale approaches and show that despite surface differences between different approaches, they all tend to be variants of a relatively small set of static and dynamic affinities. We use the synthesis of static and dynamic affinities to develop a generic model for reflective design. The model is nonprescriptive and affects minimally the design process. It is context-independent and is intended to be used as a facilitator in participative design, supporting group communication and deliberation. The potential utility of the model is demonstrated through two examples, one from the world of business process design and the other from programming language design.",Design aids; Design rationale; Development; Participative; Reflective,
A compiler for analyzing cryptographic protocols using noninterference,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039926997&doi=10.1145%2f363516.363532&partnerID=40&md5=d50d9171721bc8a8a7fe9dbdd961a0b4,"The Security Process Algebra (SPA) is a CCS-like specification language where actions belong to two different levels of confidentiality. It has been used to define several noninterference-like security properties whose verification has been automated by the tool CoSeC. In recent years, a method for analyzing security protocols using SPA and CoSeC has been developed. Even if it has been useful in analyzing small security protocols, this method has shown to be error-prone, as it requires the protocol description and its environment to be written by hand. This problem has been solved by defining a protocol specification language more abstract than SPA, called VSP, and a compiler CVS that automatically generates the SPA specification for a given protocol described in VSP. The VSP/CVS technology is very powerful, and its usefulness is shown with some case studies: the Woo-Lam one-way authentication protocol, for which a new attack to authentication is found, and the Wide Mouthed Frog protocol, where different kinds of attack are detected and analyzed.","C.2.0 [Computer-Communication Networks]: General - Security and protection (e.g., firewalls); C.2.2 [Computer-Communication Networks]: Network Protocols - Protocol verification",
SAFKASI: A security mechanism for language-based systems,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039334688&doi=10.1145%2f363516.363520&partnerID=40&md5=b59254c1881e4ef597bc727178ad9f17,"In order to run untrusted code in the same process as trusted code, there must be a mechanism to allow dangerous calls to determine if their caller is authorized to exercise the privilege of using the dangerous routine. Java systems have adopted a technique called stack inspection to address this concern. But its original definition, in terms of searching stack frames, had an unclear relationship to the actual achievement of security, overconstrained the implementation of a Java system, limited many desirable optimizations such as method inlining and tail recursion, and generally interfered with interprocedural optimization. We present a new semantics for stack inspection based on a belief logic and its implementation using the calculus of security-passing style which addresses the concerns of traditional stack inspection. With security-passing style, we can efficiently represent the security context for any method activation, and we can build a new implementation strictly by rewriting the Java bytecodes before they are loaded by the system. No changes to the JVM or bytecode semantics are necessary. With a combination of static analysis and runtime optimizations, our prototype implementation shows reasonable performance (although traditional stack inspection is still faster), and is easier to consider for languages beyond Java. We call our system SAFKASI (the Security Architecture Formerly Known as Stack Inspection).",D.1.5 [Programming Techniques]: Object-oriented Programming; D.2.0 [Software Engineering]: General - Protection mechanisms; D.3.2 [Programming Languages]: Language Classifications - Object-oriented languages,
Composite model-checking: Verification with type-specific symbolic representations,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001950668&doi=10.1145%2f332740.332746&partnerID=40&md5=85c60662328034707551ca78e127c005,"There has been a surge of progress in automated verification methods based on state exploration. In areas like hardware design, these technologies are rapidly augmenting key phases of testing and validation. To date, one of the most successful of these methods has been symbolic model-checking, in which large finite-state machines are encoded into compact data structures such as Binary Decision Diagrams (BDDs), and are then checked for safety and liveness properties. However, these techniques have not realized the same success on software systems. One limitation is their inability to deal with infinite-state programs, even those with a single unbounded integer. A second problem is that of finding efficient representations for various variable types. We recently proposed a model-checker for integer-based systems that uses arithmetic constraints as the underlying state representation. While this approach easily verified some subtle, infinite-state concurrency problems, it proved inefficient in its treatment of boolean and (unordered) enumerated types -which are not efficiently representable using arithmetic constraints. In this article we present a new technique that combines the strengths of both BDD and arithmetic constraint representations. Our composite model merges multiple type-specific symbolic representations in a single model-checker. A system's transitions and fixpoint computations are encoded using both BDD (for boolean and enumerated types) and arithmetic constraint (for integers) representations, where the choice depends on the variable types. Our composite model-checking strategy can be extended to other symbolic representations provided that they support operations such as intersection, union, complement, equivalence checking, and relational image computation. We also present conservative approximation techniques for composite representations to address the undecidability of model-checking on infinite-state systems. We demonstrate the effectiveness of our approach by analyzing two example software specifications which include a mixture of booleans, integers, and enumerated types. One of them is a requirements specification for the control software of a nuclear reactor's cooling system, and the other one is a transport protocol specification. Categories and Subject Descriptors: D.2.4 [Software Engineering]: Software/Program Verification - Formal methods; Model checking; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs-Invariants; Mechanical verification; Pre-and postconditions; D.2.1 [Software Engineering]: Requirements/Specifications - Tools General Terms: Verification.",Binary decision diagrams; Presburger arithmetic; Symbolic model-checking,
Protecting privacy using the decentralizedlabel model,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001048139&doi=10.1145%2f363516.363526&partnerID=40&md5=48270142d5b4f0559e077c2f585d1f5c,"Stronger protection is needed for the confidentiality and integrity of data, because programs containing untrusted code are the rule rather than the exception. Information flow control allows the enforcement of end-to-end security policies, but has been difficult to put into practice. This article describes the decentralized label model, a new label model for control of information flow in systems with mutual distrust and decentralized authority. The model improves on existing multilevel security models by allowing users to declassify information in a decentralized way, and by improving support for fine-grained data sharing. It supports static program analysis of information flow, so that programs can be certified to permit only acceptable information flows, while largely avoiding the overhead of run-time checking. The article introduces the language Jif, an extension to Java that provides static checking of information flow using the decentralized label model.",Confidentiality; D.4.6 [Operating Systems]: Security and Protection -information flow controls; Declassification; Downgrading; End-to-end; Information flow controls; Integrity; Languages; Lattice; Policies; Principals; Roles; Security; Type checking,
"Proof linking: Modular verification of mobile programs in the presence of lazy, dynamic linking",2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000707669&doi=10.1145%2f363516.363523&partnerID=40&md5=e5e4a6a5313c60c91867cb2551e9e428,"Although mobile code systems typically employ link-time code verifiers to protect host computers from potentially malicious code, implementation flaws in the verifiers may still leave the host system vulnerable to attack. Compounding the inherent complexity of the verification algorithms themselves, the need to support lazy, dynamic linking in mobile code systems typically leads to architectures that exhibit strong interdependencies between the loader, the verifier, and the linker. To simplify verifier construction and provide improved assurances of verifier integrity, we propose a modular architecture based on the concept of proof linking. This architecture encapsulates the verification process and removes dependencies between the loader, the verifier, and the linker. We also formally model the process of proof linking and establish properties to which correct implementations must conform. As an example, we instantiate our architecture for the problem of Java bytecode verification and assess the correctness of this instantiation. Finally, we briefly discuss alternative mobile code verification architectures enabled by the proof-linking concept.",D.2.1 [Software Engineering]: Requirements/ Specifications - Domain-specific architectures; D.2.4 [Software Engineering]: Software/Program Verification - Correctness proofs; Formal methods,
Static checking of system behaviors using derived component assumptions,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000714870&doi=10.1145%2f352591.352593&partnerID=40&md5=ec14bcb53fdcc4a1f3d947bfff155438,"A critical challenge faced by the developer of a software system is to understand whether the system's components correctly integrate. While type theory has provided substantial help in detecting and preventing errors in mismatched static properties, much work remains in the area of dynamics. In particular, components make assumptions about their behavioral interaction with other components, but currently we have only limited ways in which to state those assumptions and to analyze those assumptions for correctness. We have formulated a method that begins to address this problem. The method operates at the architectural level so that behavioral integration errors, such as deadlock, can be revealed early and at a high level. For each component, a specification is given of its interaction behavior. From this specification, assumptions that the component makes about the corresponding interaction behavior of the external context are automatically derived. We have defined an algorithm that performs compatibility checks between finite representations of a component's context assumptions and the actual interaction behaviors of the components with which it is intended to interact. A configuration of a system is possible if and only if a successful way of matching actual behaviors with assumptions can be found. The state-space complexity of this algorithm is significantly less than that of comparable approaches, and in the worst case, the time complexity is comparable to the worst case of standard reachability analysis.",Assumptions; Chemical Abstract Machine model; Component-based systems; Design; Static analysis; Theory; Verification,
Corrigenda A Hierarchy-Aware Approach to Faceted Classification of Object-Oriented Components,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042956053&doi=10.1145%2f322993.322997&partnerID=40&md5=bb5bd5dce71067a0825aeba41d369a09,"This article presents a hierarchy-aware classification schema for object-oriented code, where software components are classified according to their behavioral characteristics, such as provided services, employed algorithms, and needed data. In the case of reusable application frameworks, these characteristics are constructed from their model, i.e., from the description of the abstract classes specifying both the framework structure and purpose. In conventional object libraries, the characteristics are extracted semiautomatically from class interfaces. Characteristics are term pairs, weighted to represent “how well” they describe component behavior. The set of characteristics associated with a given component forms its software descriptor. A descriptor base is presented where descriptors are organized on the basis of structured relationships, such as similarity and composition. The classification is supported by a thesaurus acting as a language-independent unified lexicon. The descriptor base is conceived for developers who, besides conventionally browsing the descriptors hierarchy, can query the system, specifying a set of desired functionalities and getting a ranked set of adaptable candidates. User feedback is taken into account in order to progressively ameliorate the quality of the descriptors according to the views of the user community. Feedback is made dependent of the user typology through a user profile. Experimental results in terms of recall and precision of the retrieval mechanism against a sample code base are reported. © 1999, ACM. All rights reserved.",Code analysis; component repositories; component retrieval; Documentation; software reuse; user feedback,
Law-governed interaction: A coordination and control mechanism for heterogeneous distributed systems,2000,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000788149&doi=10.1145%2f352591.352592&partnerID=40&md5=3e31351f7a81eb477fe875f42e2f4bee,"Software technology is undergoing a transition from monolithic systems, constructed according to a single overall design, into conglomerates of semiautonomous, heterogeneous and independently designed subsystems, constructed and managed by different organizations, with little, if any, knowledge of each other. Among the problems inherent in such conglomerates none is more serious than the difficulty to control the activities of the disparate agents operating in it, and the difficulty for such agents to coordinate their activities with each other. We argue that the nature of coordination and control required for such systems calls for the following principles to be satisfied: (1) coordination policies need to be enforced; (2) the enforcement needs to be decentralized; and (3) coordination policies need to be formulated explicitly - rather than being implicit in the code of the agents involved - and they should be enforced by means of a generic, broad spectrum mechanism; and (4) it should be possible to deploy and enforce a policy incrementally, without exacting any cost from agents and activities not subject to it. We describe a mechanism called law-governed interaction (LGI), currently implemented by the Moses toolkit, which has been designed to satisfy these principles. We show that LGI is at least as general as a conventional centralized coordination mechanism (CCM), and that it is more scalable, and generally more efficient, than CCM.",Coordination of heterogeneous agents; Design; Performance; Policy enforcement; Scalability; Security,
Reifying variants in configuration management,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041854269&doi=10.1145%2f310663.310668&partnerID=40&md5=441efa3c14f4aabff36d3f01a1686931,"Using a solid software configuration management (SCM) is mandatory to establish and maintain the integrity of the products of a software project throughout the project's software life cycle. Even with the help of sophisticated tools, handling the various dimensions of SCM can be a daunting (and costly) task for many projects. The contribution of this article is to (1) propose a method (based on the use creational design patterns) to simplify SCM by reifying the variants of an object-oriented software system into language-level objects and (2) show that newly available compilation technology makes this proposal attractive with respect to performance (memory footprint and execution time) by inferring which classes are needed for a specific configuration and optimizing the generated code accordingly.",Compilation technology; D.2 [Software]: Software Engineering; D.2.2 [Software Engineering]: Design Tools and Techniques; D.2.9 [Software Engineering]: Management - software configuration management; Eiffel; Mercure; Object-oriented analysis and design,
Software process validation: Quantitatively measuring the correspondence of a process to a model,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012168865&doi=10.1145%2f304399.304401&partnerID=40&md5=a414b94adef96cf5c7fff12d94e4eab0,"To a great extent, the usefulness of a formal model of a software process lies in its ability to accurately predict the behavior of the executing process. Similarly, the usefulness of an executing process lies largely in its ability to fulfill the requirements embodied in a formal model of the process. When process models and process executions diverge, something significant is happening. We have developed techniques for uncovering and measuring the discrepancies between models and executions, which we call process validation. Process validation takes a process execution and a process model, and measures the level of correspondence between the two. Our metrics are tailorable and give process engineers control over determining the severity of different types of discrepancies. The techniques provide detailed information once a high-level measurement indicates the presence of a problem. We have applied our process validation methods in an industrial case study, of which a portion is described in this article.",Balboa; D.2.6 [Software Engineering]: Programming environments; K.6.3 [Management of Computing and Information Systems]: Software management - software development; Management; Process validation; Software maintenance; Software process; Tools,
From formal models to formally based methods: An industrial experience,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040229816&doi=10.1145%2f295558.295566&partnerID=40&md5=71eb19ed4ffe593f0327e700f8e9ce3b,"We address the problem of increasing the impact of formal methods in the practice of industrial computer applications. We summarize the reasons why formal methods so far did not gain widespread use within the industrial environment despite several promising experiences. We suggest an evolutionary rather than revolutionary attitude in the introduction of formal methods in the practice of industrial applications, and we report on our long-standing experience which involves an academic institution, Politecnico di Milano, two main industrial partners, ENEL and CISE, and occasionally a few other industries. Our approach aims at augmenting an existing and fairly deeply rooted informal industrial methodology with our original formalism, the logic specification language TRIO. On the basis of the experiences we gained we argue that our incremental attitude toward the introduction of formal methods within the industry could be effective largely independently from the chosen formalism.",Design; Documentation; Formal models; Industrial applications; Object orientation; Specification; Supervision and control; Technology transfer; Verification,
Composition and refinement of discrete real-time systems,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842691752&doi=10.1145%2f295558.295560&partnerID=40&md5=45fc4b6acd9fed1ec7ea3a517a7852d1,"Reactive systems exhibit ongoing, possibly nonterminating, interaction with the environment. Real-time systems are reactive systems that must satisfy quantitative timing constraints. This paper presents a structured compositional design method for discrete real-time systems that can be used to combat the combinatorial explosion of states in the verification of large systems. A composition rule describes how the correctness of the system can be determined from the correctness of its modules, without knowledge of their internal structure. The advantage of compositional verification is clear. Each module is both simpler and smaller than the system itself. Composition requires the use of both model-checking and deductive techniques. A refinement rule guarantees that specifications of high-level modules are preserved by their implementations. The StateTime toolset is used to automate parts of compositional designs using a combination of model-checking and simulation. The design method is illustrated using a reactor shutdown system that cannot be verified using the StateTime toolset (due to the combinatorial explosion of states) without compositional reasoning. The reactor example also illustrates the use of the refinement rule.",Abstraction; Design; Model-checking; Modules; Refinement; State explosion; Temporal logic; Timed logic; Verification,
"GENOA - A customizable, front-end-retargetable source code analysis framework",1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000354558&doi=10.1145%2f304399.304402&partnerID=40&md5=2569c5f7d54d7aaf9aa4a2b6a6fc7157,"Code analysis tools provide support for such software engineering tasks as program understanding, software metrics, testing, and reengineering. In this article we describe GENOA, the framework underlying application generators such as Aria and GEN++ which have been used to generate a wide range of practical code analysis tools. This experience illustrates front-end retargetability of GENOA; we describe the features of the GENOA framework that allow it to be used with different front ends. While permitting arbitrary parse tree computations, the GENOA specification language has special, compact iteration operators that are tuned for expressing simple, polynomial-time analysis programs; in fact, there is a useful sublanguage of the GENOA language that can express precisely all (and only) polynomial-time (PTIME) analysis programs on parse trees. Thus, we argue that the GENOA language is a simple and convenient vehicle for implementing a range of analysis tools. We also argue that the ""front-end reuse"" approach of GENOA offers an important advantage for tools aimed at large software projects: the reuse of complex, expensive build procedures to run generated tools over large source bases. In this article, we describe the GENOA framework and our experiences with it.",Code inspection; D.2.3 [Software Engineering]: Coding tools and techniques; D.2.6 [Software Engineering]: Programming environments; Languages; Metrics; Reverse engineering; Source analysis,
The desert environment,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000515980&doi=10.1145%2f322993.322994&partnerID=40&md5=86670c3655ac3fe20aa3595ed27d392f,"The Desert software engineering environment is a suite of tools developed to enhance programmer productivity through increased tool integration. It introduces an inexpensive form of data integration to provide additional tool capabilities and information sharing among tools, uses a common editor to give high-quality semantic feedback and to integrate different types of software artifacts, and builds virtual files on demand to address specific tasks. All this is done in an open and extensible environment capable of handling large software systems.","D.2.3 [Software Engineering]: Coding Tools and Techniques; D.2.6 [Software Engineering]: Programming Environments; Design; Integrated programming environments, program editors",
Modeling mobile IP in mobile UNITY,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007744283&doi=10.1145%2f304399.304400&partnerID=40&md5=fb80fa7105bb0e3b287845a80bf01ab3,"With recent advances in wireless communication technology, mobile computing is an increasingly important area of research. A mobile system is one where independently executing components may migrate through some space during the course of the computation, and where the pattern of connectivity among the components changes as they move in and out of proximity. Mobile UNITY is a notation and proof logic for specifying and reasoning about mobile systems. In this article it is argued that Mobile UNITY contributes to the modular development of system specifications because of the declarative fashion in which coordination among components is specified. The packet-forwarding mechanism at the core of the Mobile IP protocol for routing to mobile hosts is taken as an example. A Mobile UNITY model of packet forwarding and the mobile system in which it must operate is developed. Proofs of correctness properties, including important real-time properties, are outlined, and the role of formal verification in the development of protocols such as Mobile IP is discussed. Additional Key Words and Phrases: Formal methods, mobile computing, Mobile UNITY, shared variables, synchronization, transient interactions, weak consistency.",D.2.4 [Software Engineering]: Software/Program verification - correctness proofs; D.3.1 [Programming Languages]: Formal definitions and theory - semantics,
A hierarchy-aware approach to faceted classification of object-oriented components,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000976981&doi=10.1145%2f310663.310665&partnerID=40&md5=8c7192c5308a24a37e6432d05d086813,"This article presents a hierarchy-aware classification schema for object-oriented code, where software components are classified according to their behavioral characteristics, such as provided services, employed algorithms, and needed data. In the case of reusable application frameworks, these characteristics are constructed from their model, i.e., from the description of the abstract classes specifying both the framework structure and purpose. In conventional object libraries, the characteristics are extracted semiautomatically from class interfaces. Characteristics are term pairs, weighted to represent ""how well"" they describe component behavior. The set of characteristics associated with a given component forms its software descriptor. A descriptor base is presented where descriptors are organized on the basis of structured relationships, such as similarity and composition. The classification is supported by a thesaurus acting as a language-independent unified lexicon. The descriptor base is conceived for developers who, besides conventionally browsing the descriptors hierarchy, can query the system, specifying a set of desired functionalities and getting a ranked set of adaptable candidates. User feedback is taken into account in order to progressively ameliorate the quality of the descriptors according to the views of the user community. Feedback is made dependent of the user typology through a user profile. Experimental results in terms of recall and precision of the retrieval mechanism against a sample code base are reported.",H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,
Fault classes and error detection capability of specification-based testing,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001647941&doi=10.1145%2f322993.322996&partnerID=40&md5=1891dd6b02e7d22100aa5dda89a057c5,"Some varieties of specification-based testing rely upon methods for generating test cases from predicates in a software specification. These methods derive various test conditions from logic expressions, with the aim of detecting different types of faults. Some authors have presented empirical results on the ability of specification-based test generation methods to detect failures. This article describes a method for computing the conditions that must be covered by a test set for the test set to guarantee detection of the particular fault class. It is shown that there is a coverage hierarchy to fault classes that is consistent with, and may therefore explain, experimental results on fault-based testing. The method is also shown to be effective for computing MCDC-adequate tests.",D.2.1 [Software Engineering]: Requirements/Specifications; D.2.4 [Software Engineering]: Software/Program Verification; D.2.5 [Software Engineering]: Testing and Debugging; Testing; Theory; Verification,
Checking safety properties using compositional reachability analysis,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001861464&doi=10.1145%2f295558.295570&partnerID=40&md5=965a952b9d126d4f95f57e88b45d5384,"The software architecture of a distributed program can be represented by a hierarchical composition of subsystems, with interacting processes at the leaves of the hierarchy. Compositional reachability analysis (CRA) is a promising state reduction technique which can be automated and used in stages to derive the overall behavior of a distributed program based on its architecture. CRA is particularly suitable for the analysis of programs that are subject to evolutionary change. When a program evolves, only the behaviors of those subsystems affected by the change need be reevaluated. The technique however has a limitation. The properties available for analysis are constrained by the set of actions that remain globally observable. Properties involving actions encapsulated by subsystems may therefore not be analyzed. In this article, we enhance the CRA technique to check safety properties which may contain actions that are not globally observable. To achieve this, the state machine model is augmented with a special trap state labeled as π. We propose a scheme to transform, in stages, a property that involves hidden actions to one that involves only globally observable actions. The enhanced technique also includes a mechanism aiming at reducing the debugging effort. The technique is illustrated using a gas station system example.",Compositional reachability analysis; Design; Distributed systems; Model checking; Safety properties; Static analysis; Theory; Verification,
PRIME - Toward process-integrated modeling environments,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000021616&doi=10.1145%2f322993.322995&partnerID=40&md5=a8b4f1bde36626534898ab55ac0d79dd,"Research in process-centered environments (PCEs) has focused on project management support and has neglected method guidance for the engineers performing the (software) engineering process. It has been dominated by the search for suitable process-modeling languages and enactment mechanisms. The consequences of process orientation on the computer-based engineering environments, i.e., the interactive tools used during process performance, have been studied much less. In this article, we present the PRIME (Process-Integrated Modeling Environments) framework which empowers method guidance through process-integrated tools. In contrast to the tools of PCEs, the process-integrated tools of PRIME adjust their behavior according to the current process situation and the method definitions. Process integration of PRIME tools is achieved through (1) the definition of tool models; (2) the integration of the tool models and the method definitions; (3) the interpretation of the integrated environment model by the tools, the process-aware control integration mechanism, and the enactment mechanism; and (4) the synchronization of the tools and the enactment mechanism based on a comprehensive interaction protocol. We sketch the implementation of PRIME as a reusable implementation framework which facilitates the realization of process-integrated tools as well as the process integration of external tools. We define a six-step procedure for building a PRIME-based process-integrated environment (PIE) and illustrate how PRIME facilitates change integration on an easy-to-adapt modeling level.",D.2.1 [Software Engineering]: Requirements/Specifications - Tools; D.2.2 [Software Engineering]: Design Tools and Techniques - Computer-aided software engineering (CASE),
Estimation of software reliability by stratified sampling,1999,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000150763&doi=10.1145%2f310663.310667&partnerID=40&md5=5c7f4fe0a5ef63c6f8a00dc3c05b5c99,A new approach to software reliability estimation is presented that combines operational testing with stratified sampling in order to reduce the number of program executions that must be checked manually for conformance to requirements. Automatic cluster analysis is applied to execution profiles in order to stratify captured operational executions. Experimental results are reported that suggest this approach can significantly reduce the cost of estimating reliability.,Beta testing; Cluster analysis; D.2.2 [Software Engineering]: Design Tools and Techniques; D.2.5 [Software Engineering]: Testing and Debugging; D.4.5 [Operating Systems]: Reliability; Operational testing; Software reliability; Software testing,
Errata: A Formal Basis for Architectural Connection,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989235129&doi=10.1145%2f287000.287031&partnerID=40&md5=86ea020ec1c6fcd3c40bee31d86026b2,"We present corrections to a previously published article which appeared in ACM Transaction on Software Engineering and Methodology 6, 3 (July 1997), pp. 213-249. © 1998, ACM. All rights reserved.",Design; Formal models; model-checking; module interconnection; software analysis; Theory; WRIGHT,
Addendum to “Delta Algorithms: An Empirical Analysis”,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025425579&doi=10.1145%2f292182.292200&partnerID=40&md5=535ca46feb11cd9a760d480e4326257b,"The authors supply machine configurations for experiments reported in “Delta Algorithms: An Empirical Analysis,” by Hunt et al. (ACM Trans. Softw. Eng. Methodol. 7, 2 (Apr. 1998), pp. 192-214). © 1998, ACM. All rights reserved.",Algorithms; Benchmark; delta encoding; differencing; Experimentation; Measurement; Performance,
In black and white: An integrated approach to class-level testing of object-oriented programs,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032108147&doi=10.1145%2f287000.287004&partnerID=40&md5=4b8f86c882541c2698999e02e37d6655,"Because of the growing importance of object-oriented programming, a number of testing strategies have been proposed. They are based either on pure black-box or white-box techniques. We propose in this article a methodology to integrate the black- and white-box techniques. The black-box technique is used to select test cases. The white-box technique is mainly applied to determine whether two objects resulting from the program execution of a test case are observationally equivalent. It is also used to select test cases in some situations. We define the concept of a fundamental pair as a pair of equivalent terms that are formed by replacing all the variables on both sides of an axiom by normal forms. We prove that an implementation is consistent with respect to all equivalent terms if and only if it is consistent with respect to all fundamental pairs. In other words, the testing coverage of fundamental pairs is as good as that of all possible term rewritings, and hence we need only concentrate on the testing of fundamental pairs. Our strategy is based on mathematical theorems. According to the strategy, we propose an algorithm for selecting a finite set of fundamental pairs as test cases. Given a pair of equivalent terms as a test case, we should then determine whether the objects that result from executing the implemented program are observationally equivalent. We prove, however, that the observational equivalence of objects cannot be determined using a finite set of observable contexts (which are operation sequences ending with an observer function) derived from any black-box technique. Hence we supplement our approach with a ""relevant observable context"" technique, which is a heuristic white-box technique to select a relevant finite subset of the set of observable contexts for determining the observational equivalence. The relevant observable contexts are constructed from a data member relevance graph (DRG), which is an abstraction of the given implementation for a given specification. A semiautomatic tool has been developed to support this technique. © 1998 ACM.",Algorithms; D.2.1 [Software Engineering]: Requirements/Specifications-languages; D.2.5 [Software Engineering]: Testing and Debugging - test data generators; D.3.2 [Programming Languages]: Language Classifications - object-oriented languages; Languages,Algorithms; Computer programming languages; Equivalence classes; Graph theory; Heuristic methods; Program debugging; Software engineering; Data member relevance graph; Software testing; Test data generators; Object oriented programming
Applying GQM in an Industrial Software Factory,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032182972&doi=10.1145%2f292182.292197&partnerID=40&md5=60debb7e2e3a4f6cc2a942a7ecf92503,"Goal/Question/Metric (GQM) is a paradigm for the systematic definition, establishment, and exploitation of measurement programs supporting the quantitative evaluation of software processes and products. Although GQM is a quite well-known method, detailed guidelines for establishing a GQM program in an industrial environment are still limited. Also, there are few reported experiences on the application of GQM to industrial cases. Finally, the technological support for GQM is still inadequate. This article describes the experience we have gained in applying GQM at Digital Laboratories in Italy. The procedures, experiences, and technology that have been employed in this study are largely reusable by other industrial organizations willing to introduce a GQM-based measurement program in their development environments.",D.2.2 [Software Engineering]: Tools and Techniques -computer-aided software engineering (CASE); D.2.8 [Software Engineering]: Metrics -performance measures; D.2.9 [Software Engineering]: Management - productivity; software quality assurance (SQA),Computer systems programming; Database systems; Goal/question/metric (GQM) paradigm; Software engineering
An Empirical Study of Static Call Graph Extractors,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032035858&doi=10.1145%2f279310.279314&partnerID=40&md5=c4a30485ebd05a30506d21db8f5b2571,"Informally, a call graph represents calls between entities in a given program. The call graphs that compilers compute to determine the applicability of an optimization must typically be conservative: a call may be omitted only if it can never occur in any execution of the program. Numerous software engineering tools also extract call graphs with the expectation that they will help software engineers increase their understanding of a program. The requirements placed on software engineering tools that compute call graphs are typically more relaxed than for compilers. For example, some false negatives - calls that can in fact take place in some execution of the program, but which are omitted from the call graph - may be acceptable, depending on the understanding task at hand. In this article, we empirically show a consequence of this spectrum of requirements by comparing the C call graphs extracted from three software systems (mapmaker, mosaic, and gcc) by nine tools (cflow, cawk, CIA, Field, GCT, Imagix, LSME, Mawk, and Rigiparse). A quantitative analysis of the call graphs extracted for each system shows considerable variation, a result that is counterintuitive to many experienced software engineers. A qualitative analysis of these results reveals a number of reasons for this variation: differing treatments of macros, function pointers, input formats, etc. The fundamental problem is not that variances among the graphs extracted by different tools exist, but that software engineers have little sense of the dimensions of approximation in any particular call graph. In this article, we describe and discuss the study, sketch a design space for static call graph extractors, and discuss the impact of our study on practitioners, tool developers, and researchers. Although this article considers only one kind of information, call graphs, many of the observations also apply to static extractors of other kinds of information, such as inheritance structures, file dependences, and references to global variables.",Call graphs; D.3.4 [Programming Languages]: Processors General Terms: Experimentation; Design space; Empirical study; Languages; Software system analysis; Static analysis,Computer aided software engineering; Computer software; Graph theory; Requirements engineering; Call graphs; Design space; Software system analysis; Query languages
Discovering models of software processes from event-based data,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032108294&doi=10.1145%2f287000.287001&partnerID=40&md5=b1d9f4b87e39f02b4407f2eff24d73f6,"Many software process methods and tools presuppose the existence of a formal model of a process. Unfortunately, developing a formal model for an on-going, complex process can be difficult, costly, and error prone. This presents a practical barrier to the adoption of process technologies, which would be lowered by automated assistance in creating formal models. To this end, we have developed a data analysis technique that we term process discovery. Under this technique, data describing process events are first captured from an on-going process and then used to generate a formal model of the behavior of that process. In this article we describe a Markov method that we developed specifically for process discovery, as well as describe two additional methods that we adopted from other domains and augmented for our purposes. The three methods range from the purely algorithmic to the purely statistical. We compare the methods and discuss their application in an industrial case study. © 1998 ACM.",Balboa; D.2.6 [Software Engineering]: Programming Environments; K.6.3 [Management of Computing and Information Systems]: Software management - software development; Management; Process discovery; Software maintenance; Software process; Tools,Algorithms; Computer programming; Data reduction; Industrial applications; Management information systems; Markov processes; Mathematical models; Statistical methods; Data describing process; Event based data; Process discovery; Computer aided software engineering
Supporting the Restructuring of Data Abstractions through Manipulation of a Program Visualization,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032036118&doi=10.1145%2f279310.279312&partnerID=40&md5=d7416d5f464b0a19fb8deed2229ea69e,"With a meaning-preserving restructuring tool, a software engineer can change a program's structure to ease future modifications. However, deciding how to restructure the program requires a global understanding of the program's structure, which cannot be derived easily by directly inspecting the source code. We describe a manipulate program visualization - the star diagram - that supports the restructuring task of encapsulating a global data structure. The star diagram graphically displays information pertinent to encapsulation, and direct manipulation of the diagram causes the underlying program to be restructured. The visualization compactly presents all statements in the program that use the given global data structure, helping the programmer to choose the functions that completely encapsulate it. Additionally, the visualization elides code unrelated to the data structure and to the task and collapses similar expressions to help the programmer identify frequently occurring code fragments and manipulate them together. The visualization is mapped directly to the program text, so manipulation of the visualization also restructures the program. We present the star diagram concept and describe an implementation of the star diagram built upon a meaning-preserving restructuring tool for Scheme. We also describe our creation of star diagram generators for C programs, and we test the scalability of the star diagram using large C and MUMPS programs.",D.2.2 [Software Engineering]: Tools and Techniques - modules and interfaces; D.2.7 [Software Engineering]: Distribution and Maintenance - restructuring; User interfaces,Computer aided software engineering; Data structures; Inspection; Restructuring; Star diagram; Visualization
Formalizing Space Shuttle software requirements: Four case studies,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032108080&doi=10.1145%2f287000.287023&partnerID=40&md5=f84ebf3587b26888541d562e80f6c68b,"This article describes four case studies in which requirements for new flight software subsystems on NASA's Space Shuttle were analyzed using mechanically supported formal methods. Three of the studies used standard formal specification and verification techniques, and the fourth used state exploration. These applications illustrate two theses: (1) formal methods complement conventional requirements analysis processes effectively and (2) formal methods confer benefits even when only selectively adopted and applied. The studies also illustrate the interplay of application maturity level and formal methods strategy, especially in areas such as technology transfer, legacy applications, and rapid formalization, and they raise interesting issues in problem domain modeling and in tailoring formal techniques to applications. © 1998 ACM.",D.2.1 [Software Engineering]: Requirements/Specifications-methodologies; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - logics of programs; Mechanical verification; Specification techniques; Tools,Computer programming; Formal logic; Mathematical models; Requirements engineering; Space shuttles; Technology transfer; Theorem proving; Flight software subsystems; Formal methods; Requirement analysis; State exploration; Computer aided software engineering
Delta Algorithms: An Empirical Analysis,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032036225&doi=10.1145%2f279310.279321&partnerID=40&md5=c656b0fb13f26c04cb42719e4bfc2a03,"Delta algorithms compress data by encoding one file in terms of another. This type of compression is useful in a number of situations: storing multiple versions of data, displaying differences, merging changes, distributing updates, storing backups, transmitting video sequences, and others. This article studies the performance parameters of several delta algorithms, using a benchmark of over 1,300 pairs of files taken from two successive releases of GNU software. Results indicate that modern delta compression algorithms based on Ziv-Lempel techniques significantly outperform diff, a popular but older delta compressor, in terms of compression ratio. The modern compressors also correlate better with the actual difference between files without sacrificing performance.",D.2.7 [Software Engineering]: Distribution and Maintenance - version control; D.2.8 [Software Engineering]: Metrics - performance measures; E.4 [Data]: Coding and information theory - data compaction and compression; E.5 [Data]: Files - backup/recovery,Algorithms; Computer software; Encoding (symbols); Delta algorithms; Data compression
Eliciting Software Process Models with the E3 Language,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032182804&doi=10.1145%2f292182.292194&partnerID=40&md5=de7b925ebc4a20021b0b7a34ffee760c,"Software processes are complex entities that demand careful understanding and improvement as they determine the quality of the resulting product. A necessary step toward the improvement of an organization's process is a clear description of the entities involved and of their mutual relationships. Process model elicitation aims at constructing this description under the shape of a software process model. The model is constructed by gathering, from several sources, process information which is often incomplete, inconsistent, and ambiguous. A process modeling language can be used to represent the model being elicited. However, elicitation requires process models to be understandable and well structured. These requirements are often not satisfied by available process modeling languages because of their bias toward process enaction rather than process description. This article presents a process modeling language and a support tool which are conceived especially for process model elicitation. The E3 language is an object-oriented modeling language with a graphical notation. In E3, associations are a means to express constraints and facilitate reuse. The E3p-draw tool supports the creation and management of E3 models and provides a view mechanism that enables inspection of models according to different perspectives.",D.1.5 [Programming Techniques]: Object-Oriented Programming; D.2.1 [Software Engineering]: Requirements/Specifications; D.2.2 [Software Engineering]: Tools and Techniques - computer-aided software engineering (CASE),Computer simulation languages; Constraint theory; Mathematical models; Object oriented programming; Software process models; Computer aided software engineering
"Managing Inconsistent Specifications: Reasoning, Analysis, and Action",1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032184383&doi=10.1145%2f292182.292187&partnerID=40&md5=c1c7dbc439cc1c0db4ad02fcaf777c30,"In previous work, we advocated continued development of specifications in the presence of inconsistency. To support this, we used classical logic to represent partial specifications and to identify inconsistencies between them. We now present an adaptation of classical logic, which we term quasi-classical (QC) logic, that allows continued reasoning in the presence of inconsistency. The adaptation is a weakening of classical logic that prohibits all trivial derivations, but still allows all resolvants of the assumptions to be derived. Furthermore, the connectives behave in a classical manner. We then present a development called labeled QC logic that records and tracks assumptions used in reasoning. This facilitates a logical analysis of inconsistent information. We discuss the application of labeled QC logic in the analysis of multiperspective specifications. Such specifications are developed by multiple participants who hold overlapping, often inconsistent, views of the systems they are developing.",D.2 [Software]: Software Engineering; D.2.1 [Software Engineering]: Requirements/Specifications - languages; D.2.2 [Software Engineering]: Tools and Techniques - computer-aided software engineering (CASE),Artificial intelligence; Computer systems programming; Software engineering; Quasi-classical (QC) logic; Formal logic
Software Process Modeling and Execution within Virtual Environments,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599399&doi=10.1145%2f268411.268415&partnerID=40&md5=cfe83c6dc87c0b16792d71b8f142e111,"In the past, multiuser virtual environments have been developed as venues for entertainment and social interaction. Recent research focuses instead on their utility in carrying out work in the real world. This research has identified the importance of a mapping between the real and the virtual that permits the representation of real tasks in the virtual environment. We investigate the use of virtual environments - in particular, MUDs (Multi-User Dimensions) - in the domain of software process. In so doing, we define a mapping, or metaphor, that permits the representation of software processes within a MUD. The system resulting from this mapping, called Promo, permits the modeling and execution of software processes by geographically dispersed agents.",Management; MOO; MUD; Programming Environments; PROMO; Software maintenance; Software Management - software development; Software process; Tools; Virtual environments,Computer simulation; Computer software; Mapping; Software process modeling; Virtual reality
Understanding the Sources of Variation in Software Inspections,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599398&doi=10.1145%2f268411.268421&partnerID=40&md5=4983ea144e987148a7b7894641238aff,"In a previous experiment, we determined how various changes in three structural elements of the software inspection process (team size and the number and sequencing of sessions) altered effectiveness and interval. Our results showed that such changes did not significantly influence the defect detection rate, but that certain combinations of changes dramatically increased the inspection interval. We also observed a large amount of unexplained variance in the data, indicating that other factors must be affecting inspection performance. The nature and extent of these other factors now have to be determined to ensure that they had not biased our earlier results. Also, identifying these other factors might suggest additional ways to improve the efficiency of inspections. Acting on the hypothesis that the ""inputs"" into the inspection process (reviewers, authors, and code units) were significant sources of variation, we modeled their effects on inspection performance. We found that they were responsible for much more variation in defect detection than was process structure. This leads us to conclude that better defect detection techniques, not better process structures, are the key to improving inspection effectiveness. The combined effects of process inputs and process structure on the inspection interval accounted for only a small percentage of the variance in inspection interval. Therefore, there must be other factors which need to be identified.",Empirical studies; Experimentation; Software inspection; Software process; Statistical models; Testing and Debugging - code inspections and walk-throughs,Computer software; Defects; Inspection; Software inspection; Statistical models; Computer aided software engineering
Toward Formalizing Structured Analysis,1998,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599337&doi=10.1145%2f268411.268429&partnerID=40&md5=0e43119e4333fc7a96aaccbc84f0005e,"Real-time extensions to structured analysis (SA/RT) are popular in industrial practice. Despite the large industrial experience and the attempts to formalize the various ""dialects,"" SA/RT notations are still imprecise and ambiguous. This article tries to identify the semantic problems of the requirements definition notation defined by Hatley and Pirbhai, one of the popular SA/RT ""dialects,"" and discusses possible solutions. As opposed to other articles that give their own interpretation, this article does not propose a specific semantics for the notation. This article identifies imprecisions, i.e., missing or partial information about features of the notation; it discusses ambiguities, i.e., elements of the definition that allow at least two different (""reasonable"") interpretations of features of the notation; and it lists extensions, i.e., features not belonging to the notation, but required by many industrial users and often supported by CASE tools. This article contributes by clarifying whether specific interpretations can be given unique semantics or retain ambiguities of the original definition. The article allows for the evaluation of formal definitions by indicating alternatives and consequences of the specific choices.",Design; Documentation; Hatley and Pirbhai's requirements definition notation; Informal versus formal specifications; Requirements/Specifications - methodologies; Structured analysis/real-time; Tools and Techniques - structured programming,Dynamic response; Real time systems; Specifications; Notation; Computer aided software engineering
Unified Versioning through Feature Logic,1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031258310&doi=10.1145%2f261640.261654&partnerID=40&md5=214ac328b2a80ce6904855354a83a240,"Software Configuration Management (SCM) suffers from tight coupling between SCM versioning models and the imposed SCM processes. In order to adapt SCM tools to SCM processes, rather than vice versa, we propose a unified versioning model, the version set model. Version sets denote versions, components, and configurations by feature terms, that is, Boolean terms over (feature : value)-attributions. Through feature logic, we deduce consistency of abstract configurations as well as features of derived components and describe how features propagate in the SCM process; using feature implications, we integrate change-oriented and version-oriented SCM models. We have implemented the version set model in an SCM system called ICE, for Incremental Configuration Environment. ICE is based on a featured file system (FFS), where version sets are accessed as virtual files and directories. Using the well-known C preprocessor (CPP) representation, users can view and edit multiple versions simultaneously, while only the differences between versions are stored. It turns out that all major SCM models can be realized and integrated efficiently on top of the FFS, demonstrating the flexible and unifying nature of the version set model.",D.2.6 [Software Engineering]: Programming Environments; D.2.7 [Software Engineering]: Distribution and Maintenance - version control; D.2.9 [Software Engineering]: Management - software configuration management; Programming teams,C (programming language); Computer simulation; Formal logic; Knowledge representation; Standardization; Theorem proving; Feature logic; Version sets; Software engineering
Hybrid Slicing: Integrating Dynamic Information with Static Analysis,1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031258855&doi=10.1145%2f261640.261644&partnerID=40&md5=c80d9f654801b917c9ff874bf591f806,"Program slicing is an effective technique for narrowing the focus of attention to the relevant parts of a program during the debugging process. However, imprecision is a problem in static slices, since they are based on all possible executions that reach a given program point rather than the specific execution under which the program is being debugged. Dynamic slices, based on the specific execution being debugged, are precise but incur high run-time overhead due to the tracing information that is collected during the program's execution. We present a hybrid slicing technique that integrates dynamic information from a specific execution into a static slice analysis. The hybrid slice produced is more precise than the static slice and less costly than the dynamic slice. The technique exploits dynamic information that is readily available during debugging-namely, breakpoint information and the dynamic call graph. This information is integrated into a static slicing analysis to more accurately estimate the potential paths taken by the program. The breakpoints and call/return points, used as reference points, divide the execution path into intervals. By associating each statement in the slice with an execution interval, hybrid slicing provides information as to when a statement was encountered during execution. Another attractive feature of our approach is that it allows the user to control the cost of hybrid slicing by limiting the amount of dynamic information used in computing the slice. We implemented the hybrid slicing technique to demonstrate the feasibility of our approach.",Algorithms; Breakpoint; D.2.5 [Software Engineering]: Testing and Debugging; Dynamic call graph; Dynamic slice; Experimentation; Hybrid slice; Static slice; Theory,Algorithms; Computation theory; Computer software; Software engineering; Breakpoint; Dynamic call graph; Dynamic slice; Hybrid slice; Static slice; Program debugging
A Formal Basis for Architectural Connection,1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031193084&doi=10.1145%2f258077.258078&partnerID=40&md5=87684000f9bd73480230142896c394bb,"As software systems become more complex, the overall system structure - or software architecture - becomes a central design problem. An important step toward an engineering discipline of software is a formal basis for describing and analyzing these designs. In this article we present a formal approach to one aspect of architectural design: the interactions among components. The key idea is to define architectural connectors as explicit semantic entities. These are specified as a collection of protocols that characterize each of the participant roles in an interaction and how these roles interact. We illustrate how this scheme can be used to define a variety of common architectural connectors. We further provide a formal semantics and show how this leads to a system in which architectural compatibility can be checked in a way analogous to type-checking in programming languages.",D.2.1 [Software Engineering]: Requirements/Specification - languages; D.2.2 [Software Engineering]: Tools and Techniques - modules and interfaces,Computational linguistics; Computer architecture; Computer software selection and evaluation; Formal languages; Formal logic; Network protocols; Formal models; Software analysis; Software engineering
Specification Matching of Software Components,1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031259286&doi=10.1145%2f261640.261641&partnerID=40&md5=1678845d671355ad617896fda346b694,"Specification matching is a way to compare two software components, based on descriptions of the component's behaviors. In the context of software reuse and library retrieval, it can help determine whether one component can be substituted for another or how one can be modified to fit the requirements of the other. In the context of object-oriented programming, it can help determine when one type is a behavioral subtype of another. We use formal specifications to describe the behavior of software components and, hence, to determine whether two components match. We give precise definitions of not just exact match, but, more relevantly, various flavors of relaxed match. These definitions capture the notions of generalization, specialization, and substitutability of software components. Since our formal specifications are pre- and postconditions written as predicates in first-order logic, we rely on theorem proving to determine match and mismatch. We give examples from our implementation of specification matching using the Larch Prover.","D.2.1 [Software Engineering]: Requirements/Specifications; D.2.2 [Software Engineering]: Tools and Techniques - software libraries; D.3.3 [Programming Languages]: Language Constructs and Features - modules, packages",Computer hardware description languages; Computer software; Formal logic; Subroutines; Theorem proving; Larch prover; Software components; Specification matching; Software engineering
Mobile UNITY: Reasoning and Specification in Mobile Computing,1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031189708&doi=10.1145%2f258077.258079&partnerID=40&md5=75b06633c24c0c28a5ca3fac609a7463,"Mobile computing represents a major point of departure from the traditional distributed-computing paradigm. The potentially very large number of independent computing units, a decoupled computing style, frequent disconnections, continuous position changes, and the location-dependent nature of the behavior and communication patterns present designers with unprecedented challenges in the areas of modularity and dependability. So far, the literature on mobile computing is dominated by concerns having to do with the development of protocols and services. This article complements this perspective by considering the nature of the underlying formal models that will enable us to specify and reason about such computations. The basic research goal is to characterize fundamental issues facing mobile computing. We want to achieve this in a manner analogous to the way concepts such as shared variables and message passing help us understand distributed computing. The pragmatic objective is to develop techniques that facilitate the verification and design of dependable mobile systems. Toward this goal we employ the methods of UNITY. To focus on what is essential, we center our study on ad hoc networks, whose singular nature is bound to reveal the ultimate impact of movement on the way one computes and communicates in a mobile environment. To understand interactions we start with the UNITY concepts of union and superposition and consider direct generalizations to transient interactions. The motivation behind the transient nature of the interactions comes from the fact that components can communicate with each other only when they are within a certain range. The notation we employ is a highly modular extension of the UNITY programming notation. Reasoning about mobile computations relies on extensions to the UNITY proof logic.",D.2.4 [Software Engineering]: Program Verification - correctness proofs; D.3.1 [Programming Languages]: Formal Definitions and Theory - semantics; D.3.3 [Programming Languages]: Language Constructs and Features - concurrent programming structures,Formal languages; Formal logic; Network protocols; Mobile computing; Software engineering
An evolutionary approach to constructing effective software reuse repositories,1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031124378&doi=10.1145%2f248233.248242&partnerID=40&md5=281dad8fc7c16450f01dd7076445d854,"Repositories for software reuse are faced with two interrelated problems: (1) acquiring the knowledge to initially construct the repository and (2) modifying the repository to meet the evolving and dynamic needs of software development organizations. Current software repository methods rely heavily on classification, which exacerbates acquisition and evolution problems by requiring costly classification and domain analysis efforts before a repository can be used effectively. This article outlines an approach that avoids these problems by choosing a retrieval method that utilizes minimal repository structure to effectively support the process of finding software components. The approach is demonstrated through a pair of proof-of-concept prototypes: PEEL, a tool to semiautomatically identify reusable components, and CodeFinder, a retrieval system that compensates for the lack of explicit knowledge structures through a spreading activation retrieval process. CodeFinder also allows component representations to be modified while users are searching for information. This mechanism adapts to the changing nature of the information in the repository and incrementally improves the repository while people use it. The combination of these techniques holds potential for designing software repositories that minimize up-front costs, effectively support the search process, and evolve with an organization's changing needs. © 1997 ACM.",Component repositories; Information retrieval; Software reuse,Data structures; Information retrieval systems; Query languages; User interfaces; CodeFinder retrieval system; Software components; Software library; Software package PEEL; Software reuse repositories; Computer aided software engineering
"Technical Correspondence Comments on ""a Reduced Test Suite for Protocol Conformance Testing""",1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031187567&doi=10.1145%2f258077.265733&partnerID=40&md5=3b3192688a081250d5bad432c00dd084,"A previous ACM TOSEM article of Ph. Bernhard (""A Reduced Test Suite for Protocol Conformance Testing,"" ACM Transactions on Software Engineering and Methodology, Vol. 3, No. 3, July 1994, pages 201-220) describes three new versions of the so-called W-method for solving the protocol-testing problem, i.e., solving the Mealy machine equivalence problem. The author claims that these versions all have the same fault detection capability as the original W-method. In this correspondence we prove that the results of that article are incorrect.",C.2.2. [Computer-Communication Networks]: Network Protocols - protocol verification; D.2.5 [Software Engineering]: Testing and Debugging - test data generators; Reliability; Theory; Verification,Computer software selection and evaluation; Fault tolerant computer systems; Network protocols; Problem solving; Program debugging; Mealy machine equivalence problem; Protocol testing problem; Software engineering
Four Dark Corners of Requirements Engineering,1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030834984&doi=10.1145%2f237432.237434&partnerID=40&md5=f71e71ce9ec06516233291c98b6e3d30,"Research in requirements engineering has produced an extensive body of knowledge, but there are four areas in which the foundation of the discipline seems weak or obscure. This article shines some light in the ""four dark corners,"" exposing problems and proposing solutions. We show that all descriptions involved in requirements engineering should be descriptions of the environment. We show that certain control information is necessary for sound requirements engineering, and we explain the close association between domain knowledge and refinement of requirements. Together these conclusions explain the precise nature of requirements, specifications, and domain knowledge, as well as the precise nature of the relationships among them. They establish minimum standards for what information should be represented in a requirements language. They also make it possible to determine exactly what it means for requirements engineering to be successfully completed.",Control of actions; Domain knowledge; Implementation bias; Refinement of requirements,Computer hardware description languages; Computer programming languages; Information science; Information theory; Terminology; Control of actions; Domain knowledge; Implementation bias; Refinement of requirements; Requirements engineering; Software engineering
Assessing Process-Centered Software Engineering Environments,1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031186803&doi=10.1145%2f258077.258080&partnerID=40&md5=3251a3744df51e1525c2308cd4f12da9,"Process-centered software engineering environments (PSEEs) are the most recent generation of environments supporting software development activities. They exploit an explicit representation of the process (called the process model) that specifies how to carry out software development activities, the roles and tasks of software developers, and how to use and control software development tools. A process model is therefore a vehicle to better understand and communicate the process. If it is expressed in a formal notation, it can be used to support a variety of activities such as process analysis, process simulation, and process enactment. PSEEs provide automatic support for these activities. They exploit languages based on different paradigms, such as Petri nets and rule-based systems. They include facilities to edit and analyze process models. By enacting the process model, a PSEE provides a variety of services, such as assistance for software developers, automation of routine tasks, invocation and control of software development tools, and enforcement of mandatory rules and practices. Several PSEEs have been developed, both as research projects and as commercial products. The initial deployment and exploitation of this technology have made it possible to produce a significant amount of experiences, comments, evaluations, and feedback. We still lack, however, consistent and comprehensive assessment methods that can be used to collect and organize this information. This article aims at contributing to the definition of such methods, by providing a systematic comparison grid and by accomplishing an initial evaluation of the state of the art in the field. This evaluation takes into account the systems that have been developed by the authors in the past five years, as well as the main characteristics of other well-known environments.",D.2.6 [Software Engineering]: Programming Environments; K.6.3 [Management of Computing and Information Systems]: Software Management - software development; software maintenance,Computer simulation; Computer software selection and evaluation; Knowledge based systems; Petri nets; Process centered software engineering environments (PSEE); Computer aided software engineering
The use of description logics in KBSE systems,1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031118022&doi=10.1145%2f248233.248253&partnerID=40&md5=347d836b1feda9c54a13ec7640f3d8f7,"The increasing size and complexity of many software systems demand a greater emphasis on capturing and maintaining knowledge at many different levels within the software development process. This knowledge includes descriptions of the hardware and software components and their behavior, external and internal design specifications, and support for system testing. The knowledge-based software engineering (KBSE) research paradigm is concerned with systems that use formally represented knowledge, with associated inference procedures, to support the various subactivities of software development. As they grow in scale, KBSE systems must balance expressivity and inferential power with the real demands of knowledge base construction, maintenance, performance, and comprehensibility. Description logics (DLs) possess several features - a terminological orientation, a formal semantics, and efficient reasoning procedures - which offer an effective tradeoff of these factors. We discuss three KBSE systems in which DLs capture some of the requisite knowledge needed to support design, coding, and testing activities. We then survey some alternative approaches (to DLs) in KBSE systems. We close with a discussion of the benefits of DLs and ways to address some of their limitations. © 1997 ACM.",Automated software engineering; Knowledge bases; Logics; Software development environments; Testing; Tools,Computational linguistics; Encoding (symbols); Formal languages; Formal logic; Inference engines; Knowledge based systems; Knowledge representation; Program debugging; Automated software engineering; Description logics; Formal semantics; Knowledge based software engineering; Computer aided software engineering
A Graphical Environment for the Design of Concurrent Real-Time Systems,1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030783264&doi=10.1145%2f237432.237438&partnerID=40&md5=c3075b2682e8ab2b470694d8d0208f92,"Concurrent real-time systems are among the most difficult systems to design because of the many possible interleavings of events and because of the timing requirements that must be satisfied. We have developed a graphical environment based on Real-Time Graphical Interval Logic (RTGIL) for specifying and reasoning about the designs of concurrent real-time systems. Specifications in the logic have an intuitive graphical representation that resembles the timing diagrams drawn by software and hardware engineers, with real-time constraints that bound the durations of intervals. The syntax-directed editor of the RTGIL environment enables the user to compose and edit graphical formulas on a workstation display; the automated theorem prover mechanically checks the validity of proofs in the logic; and the database and proof manager tracks proof dependencies and allows formulas to be stored and retrieved. This article describes the logic, methodology, and tools that comprise the prototype RTGIL environment and illustrates the use of the environment with an example application.",Automated deduction; Concurrent systems; Formal specification and verification; Graphical user interface; Real-time systems; Temporal logic,Computer aided software engineering; Computer hardware description languages; Database systems; File editors; Formal languages; Formal logic; Graphical user interfaces; Information retrieval systems; Systems analysis; Theorem proving; Automated deduction; Concurrent real time systems; Concurrent systems; Real time graphical interval logic; Temporal logic; Verification; Real time systems
"A safe, efficient regression test selection technique",1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031125046&doi=10.1145%2f248233.248262&partnerID=40&md5=1813810d5091ffc922f8c456e436bfd1,"Regression testing is an expensive but necessary maintenance activity performed on modified software to provide confidence that changes are correct and do not adversely affect other portions of the software. A regression test selection technique chooses, from an existing test set, tests that are deemed necessary to validate modified software. We present a new technique for regression test selection. Our algorithms construct control flow graphs for a procedure or program and its modified version and use these graphs to select tests that execute changed code from the original test suite. We prove that, under certain conditions, the set of tests our technique selects includes every test from the original test suite that can expose faults in the modified procedure or program. Under these conditions our algorithms are safe. Moreover, although our algorithms may select some tests that cannot expose faults, they are at least as precise as other safe regression test selection algorithms. Unlike many other regression test selection algorithms, our algorithms handle all language constructs and all types of program modifications. We have implemented our algorithms; initial empirical studies indicate that our technique can significantly reduce the cost of regression testing modified software. © 1997 ACM.",Regression test selection; Regression testing; Selective retest,Algorithms; Codes (symbols); Computer aided software engineering; Flowcharting; Regression analysis; Statistical tests; Program modification; Regression test selection; Regression testing; Selective retest; Program debugging
Task Dependence and Termination in Ada,1997,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030822426&doi=10.1145%2f237432.237459&partnerID=40&md5=d7647d684d074cdc64ad7dfdbe171896,"This article analyzes the semantics of task dependence and termination in Ada. We use a contour model of Ada tasking in examining the implications of and possible motivation for the rules that determine when procedures and tasks terminate during execution of an Ada program. The termination rules prevent the data that belong to run-time instances of scope units from being deallocated prematurely, but they are unnecessarily conservative in this regard. For task instances that are created by invoking a storage allocator, we show that the conservative termination policy allows heap storage to be managed more efficiently than a less conservative policy. The article also examines the manner in which the termination rules affect the synchronization of concurrent tasks. Master-slave and client-server applications are considered. We show that the rules for distributed termination of concurrent tasks guarantee that a task terminates only if it can no longer affect the outcome of an execution. The article is meant to give programmers a better understanding of Ada tasking and to help language designers assess the strengths and weaknesses of the termination model.",Ada tasking; Distributed termination; Master/dependent relation; Task termination; Tasking execution model,Computational linguistics; Computer networks; Distributed computer systems; Mathematical models; Parallel processing systems; Storage allocation (computer); Ada tasking; Distributed termination; Master dependent relation; Task termination; Tasking execution model; Ada (programming language)
A Framework for Formalizing Inconsistencies and Deviations in Human-Centered Systems,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030197387&doi=10.1145%2f234426.234427&partnerID=40&md5=74354e55d94f4da327700d0cd21f597c,"Most modern business activities are carried out by a combination of computerized tools and human agents. Typical examples are engineering design activities, office procedures, and banking systems. All these human-centered systems are characterized by the interaction among people, and between people and computerized tools. This interaction defines a process, whose effectiveness is essential to ensure the quality of the delivered products and/or services. To support these systems, process-centered environments and workflow management systems have been recently developed. They can be collectively identified with the term process technology. This technology is based on the explicit definition of the process to be followed (the process model). The model specifies the kind of support that has to be provided to human agents. An essential property that process technology must exhibit is the ability of tolerating, controlling, and supporting deviations and inconsistencies of the real-world behaviors with respect to the process model. This is necessary to provide consistent and effective support to the human-centered system, still maintaining a high degree of flexibility and adaptability to the evolving needs, preferences, and expertise of the human agents. This article presents a formal framework to characterize the interaction between a human-centered system and its automated support. It does not aim at introducing a new language or system to describe processes. Rather, it aims at identifying the basic properties and features that make it possible to formally define the concepts of inconsistency and deviation. This formal framework can then be used to compare existing solutions and guide future research work. Finally, we argue that our approach provides a general and reference framework to characterize the interaction between a computerized system and the (parts of) the real world it is supposed to describe and possibly control.",D.2.6 [Software Engineering]: Programming Environments; H.1.2 [Models and Principles]: User/Machine Systems - Human factors; K.6.3 [Management of Computing and Information Systems]: Software Management - Software development; Software maintenance,Computer software; Computer systems programming; Human engineering; Information management; Software engineering; User interfaces; Deviations; Human agents; Human centered systems; Inconsistencies; Human computer interaction
Using Failure Cost Information for Testing and Reliability Assessment,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030123636&doi=10.1145%2f227607.227608&partnerID=40&md5=dd511de1c2606647f9e4313ed8c37bee,A technique for incorporating failure cost information into algorithms designed to automatically generate software-load-testing suites is presented. A previously introduced reliability measure is also modified to incorporate this cost information. Examples are presented to show the usefulness of including failure cost information when testing or assessing software.,D.2 [Software]: Software Engineering; D.2.5 [Software Engineering]: Testing and debugging; Experimentation; Failure cost; Measurement; Reliability; Software testing; Test case selection,Algorithms; Markov processes; Measurements; Program debugging; Reliability; Resource allocation; Testing; Failure cost; Software load testing; Test case selection; Software engineering
Lightweight Lexical Source Model Extraction,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030197388&doi=10.1145%2f234426.234441&partnerID=40&md5=926cb4ce6284e987a7bfd13e1ed1082c,"Software engineers maintaining an existing software system often depend on the mechanized extraction of information from system artifacts. Some useful kinds of information - source models - are well known: call graphs, file dependences, etc. Predicting every kind of source model that a software engineer may need is impossible. We have developed a lightweight approach for generating flexible and tolerant source model extractors from lexical specifications. The approach is lightweight in that the specifications are relatively small and easy to write. It is flexible in that there are few constraints on the kinds of artifacts from which source models are extracted (e.g., we can extract from source code, structured data files, documentation, etc.). It is tolerant in that there are few constraints on the condition of the artifacts. For example, we can extract from source that cannot necessarily be compiled. Our approach extends the kinds of source models that can be easily produced from lexical information while avoiding the constraints and brittleness of most parser-based approaches. We have developed tools to support this approach and applied the tools to the extraction of a number of different source models (file dependences, event interactions, call graphs) from a variety of system artifacts (C, C++, CLOS, Eiffel, TCL, structured data). We discuss our approach and describe its application to extract source models not available using existing systems; for example, we compute the implicitly-invokes relation over Field tools. We compare and contrast our approach to the conventional lexical and syntactic approaches of generating source models.",D.2.2 [Software Engineering]: Tools and Techniques - Computer-aided software engineering (CASE); D.2.6 [Software Engineering]: Programming Environments; D.3.4 [Programming Languages]: Processors - Parsing,Computer software; Constraint theory; Graph theory; Mathematical models; Program compilers; Programming theory; Call graphs; Lightweight lexical source model extraction; Parsing; System artifacts; Computer aided software engineering
A Framework for Event-Based Software Integration,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030264523&doi=10.1145%2f235321.235324&partnerID=40&md5=1b6714362af57324480ede1508481088,"Although event-based software integration is one of the most prevalent approaches to loose integration, no consistent model for describing it exists. As a result, there is no uniform way to discuss event-based integration, compare approaches and implementations, specify new event-based approaches, or match user requirements with the capabilities of event-based integration systems. We attempt to address these shortcomings by specifying a generic framework for event-based integration, the EBI framework, that provides a flexible, object-oriented model for discussing and comparing event-based integration approaches. The EBI framework can model dynamic and static specification, composition, and decomposition and can be instantiated to describe the features of most common event-based integration approaches. We demonstrate how to use the framework as a reference model by comparing and contrasting three wellknown integration systems: FIELD, Polylith, and CORBA.",Control integration; CORBA; D.2 [Software]: Software Engineering; Event-based systems; FIELD; Interoperability; Performance; Polylith; Reference model; Software integration; Theory,Human computer interaction; Object oriented programming; Subroutines; Event based integration (EBI) framework; Event driven programming; Software integration; Computer aided software engineering
Context Constraints for Compositional Reachability Analysis,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030265144&doi=10.1145%2f235321.235323&partnerID=40&md5=6dcae9f1392a60ad26c837193d2a9eba,"Behavior analysis of complex distributed systems has led to the search for enhanced reachability analysis techniques which support modularity and which control the state explosion problem. While modularity has been achieved, state explosion is still a problem. Indeed, this problem may even be exacerbated, as a locally minimized subsystem may contain many states and transitions forbidden by its environment or context. Context constraints, specified as interface processes, are restrictions imposed by the environment on subsystem behavior. Recent research has suggested that the state explosion problem can be effectively controlled if context constraints are incorporated in compositional reachability analysis (CRA). Although theoretically very promising, the approach has rarely been used in practice because it generally requires a more complex computational model and does not contain a mechanism to derive context constraints automatically. This article presents a technique to automate the approach while using a similar computational model to that of CRA. Context constraints are derived automatically, based on a set of sufficient conditions for these constraints to be transparently included when building reachability graphs. As a result, the global reachability graph generated using the derived constraints is shown to be observationally equivalent to that generated by CRA without the inclusion of context constraints. Constraints can also be specified explicitly by users, based on their application knowledge. Erroneous constraints which contravene transparency can be identified together with an indication of the error sources. User-specified constraints can be combined with those generated automatically. The technique is illustrated using a clients/server system and other examples.","D.2.1 [Software Engineering]: Requirements/Specifications; D.2.2 [Software Engineering]: Tools and Techniques; D.3.2 [Programming Languages]: Language Classifications-concurrent, distributed, and parallel languages",Automation; Constraint theory; Distributed computer systems; Mathematical models; User interfaces; Compositional reachability analysis (CRA); Context constraints; Global reachability graph; State explosion problem; Context sensitive languages
An Experimental Determination of Sufficient Mutant Operators,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030121550&doi=10.1145%2f227607.227610&partnerID=40&md5=68bd4352ab4d68d6b0df663569bb9367,"Mutation testing is a technique for unit-testing software that, although powerful, is computationally expensive. The principal expense of mutation is that many variants of the test program, called mutants, must be repeatedly executed. This article quantifies the expense of mutation in terms of the number of mutants that are created, then proposes and evaluates a technique that reduces the number of mutants by an order of magnitude. Selective mutation reduces the cost of mutation testing by reducing the number of mutants. This article reports experimental results that compare selective mutation testing with standard, or nonselective, mutation testing, and results that quantify the savings achieved by selective mutation testing. The results support the hypothesis that selective mutation is almost as strong as nonselective mutation; in experimental trials selective mutation provides almost the same coverage as nonselective mutation, with a four-fold or more reduction in the number of mutants.",D.2.5 [Software Engineering]: Testing and debugging - test data generators; Experimentation; Measurement; Reliability,Evaluation; Measurements; Program debugging; Reliability; Testing; Selective mutation testing; Sufficient mutant operators; Test data generators; Unit testing; Software engineering
Testing by Means of Inductive Program Learning,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030121096&doi=10.1145%2f227607.227611&partnerID=40&md5=27e10dd5a5341d7c9fa68c3e57b815af,"Given a program P and a set of alternative programs script P sign, we generate a sequence of test cases that are adequate, in the sense that they distinguish the given program from all alternatives. The method is related to fault-based approaches to test case generation, but programs in script P sign need not be simple mutations of P. The technique for generating an adequate test set is based on the inductive learning of programs from finite sets of input-output examples: given a partial test set, we generate inductively a program P′ ∈ script P sign which is consistent with P on those input values; then we look for an input value that distinguishes P from P′, and we repeat the process until no program except P can be induced from the generated examples. We show that the obtained test set is adequate with respect to the alternatives belonging to script P sign. The method is made possible by a program induction procedure which has evolved from recent research in machine learning and inductive logic programming. An implemented version of the test case generation procedure is demonstrated on simple and more complex list-processing programs, and the scalability of the approach is discussed.",Algorithms; D.2.5 [Software Engineering]: Testing - test data generators; I.2.6 [Artificial Intelligence]: Learning - induction; Program induction by examples; Reliability,Algorithms; Artificial intelligence; Learning systems; Logic programming; Reliability; Testing; Alternative programs; List processing programs; Program induction by examples; Scalability; Test data generators; Software engineering
The STATEMATE Semantics of Statecharts,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030261971&doi=10.1145%2f235321.235322&partnerID=40&md5=ebb256f6601a543dcb9989ee352c4c50,"We describe the semantics of statecharts as implemented in the STATEMATE system. This was the first executable semantics defined for the language and has been in use for almost a decade. In terms of the controversy around whether changes made in a given step should take effect in the current step or in the next one, this semantics adopts the latter approach.",Behavioral modeling; D.2 [Software]: Software Engineering; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages; Languages; Reactive system; Semantics; Statechart; STATEMATE,Computational linguistics; Formal languages; Software engineering; Software package STATEMATE; Statecharts; Computer programming languages
Automated Consistency Checking of Requirements Specifications,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030197750&doi=10.1145%2f234426.234431&partnerID=40&md5=18b1bbdc100144773f08463d97a2f9a5,"This article describes a formal analysis technique, called consistency checking, for automatic detection of errors, such as type errors, nondeterminism, missing cases, and circular definitions, in requirements specifications. The technique is designed to analyze requirements specifications expressed in the SCR (Software Cost Reduction) tabular notation. As background, the SCR approach to specifying requirements is reviewed. To provide a formal semantics for the SCR notation and a foundation for consistency checking, a formal requirements model is introduced; the model represents a software system as a finite-state automaton, which produces externally visible outputs in response to changes in monitored environmental quantities. Results of two experiments are presented which evaluated the utility and scalability of our technique for consistency checking in a real-world avionics application. The role of consistency checking during the requirements phase of software development is discussed.",D.2.1 [Software Engineering]: Requirements/Specifications; D.2.2 [Software Engineering]: Tools and Techniques; D.2.4 [Software Engineering]: Program Verification; D.2.7 [Software Engineering]: Distribution and Maintenance - Documentation,Automata theory; Computational linguistics; Computer hardware description languages; Computer software; Errors; Finite automata; Formal languages; Programming theory; Automated consistency checking; Automatic detection of errors; Requirements specifications; Software cost reduction; Tabular notation; Software engineering
Reengineering of Configurations Based on Mathematical Concept Analysis,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030121527&doi=10.1145%2f227607.227613&partnerID=40&md5=e1d433d05996f9e23b45563ce5daa2ee,"We apply mathematical concept analysis to the problem of reengineering configurations. Concept analysis will reconstruct a taxonomy of concepts from a relation between objects and attributes. We use concept analysis to infer configuration structures from existing source code. Our tool NORA/RECS will accept source code, where configuration-specific code pieces are controlled by the preprocessor. The algorithm will compute a so-called concept lattice, which - when visually displayed - offers remarkable insight into the structure and properties of possible configurations. The lattice not only displays fine-grained dependencies between configurations, but also visualizes the overall quality of configuration structures according to software engineering principles. In a second step, interferences between configurations can be analyzed in order to restructure or simplify configurations. Interferences showing up in the lattice indicate high coupling and low cohesion between configuration concepts. Source files can then be simplified according to the lattice structure. Finally, we show how governing expressions can be simplified by utilizing an isomorphism theorem of mathematical concept analysis.",D.2.6 [Software Engineering]: Interactive Programming Environments; D.2.7 [Software Engineering]: Distribution and Maintenance - restructuring; version control; D.2.9 [Software Engineering]: Software Configuration Management; Design; Management; Theory,Algorithms; Computation theory; Design; Logic programming; Maintenance; Management; Concept lattices; Distribution; Interactive programming environments; Isomorphism theorem; Mathematical concept analysis; Restructuring; Software configuration management; Version control; Software engineering
Generating testing and analysis tools with Aria,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029713449&doi=10.1145%2f226155.226157&partnerID=40&md5=35e2f5353363773eb56a92dea2202653,"Many software testing and analysis tools manipulate graph representations of programs, such as abstract syntax trees or abstract semantics graphs. Handcrafting such tools in conventional programming languages can be difficult, error prone, and time consuming. Our approach is to use application generators targeted for the domain of graph-representation-based testing and analysis tools. Moreover, we generate the generators themselves, so that the development of tools based on different languages and/or representations can also be supported better. In this article we report on our experiences in developing and using a system called Aria that generates testing and analysis tools based on an abstract semantics graph representation for C and C++ called Reprise. Aria itself was generated by the Genoa system. We demonstrate the utility of Aria and, thereby, the power of our approach, by showing Aria's use in the development of a number of useful testing and analysis tools. © 1996 ACM.",D.1.m [Programming Techniques]: Miscellaneous - graph traversal; D.2.2 [Software Engineering]: Tools and Techniques - software libraries; tool generators; tool specification; D.2.5 [Software Engineering]: Testing and Debugging - coverage analyzers,Algorithms; C (programming language); Computational linguistics; Computer aided software engineering; Computer software selection and evaluation; Data structures; Graph theory; Program debugging; Systems analysis; Application generators; Aria; Genoa; Program dependence graphs; Program representations; Reprise; Software analysis; Software testing; Software engineering
Generation of formatters for context-free languages,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029703823&doi=10.1145%2f226155.226156&partnerID=40&md5=fc8feea149b8b79b7971af8e8577d131,"Good documentation is important for the production of reusable and maintainable software. For the production of accurate documentation it is necessary that the original program text is not copied manually to obtain a typeset version. Apart from being tedious, this will invariably introduce errors. The production of tools that support the production of legible and accurate documentation is a software engineering challenge in itself. We present an algebraic approach to the generation of tools that produce typographically effective presentations of computer programs. A specification of a formatter is generated from the context-free grammar of a (programming) language. These generated formatters translate abstract syntax trees of programs into box expressions. Box expressions are translated by language-independent interpreters of the box language into ASCII or TEX. The formatting rules that are generated can easily be tuned in order to get the desired formatting of programs. We demonstrate this by means of real-life applications. Furthermore, we give a practical solution for the problem of formatting comments, which occur in the original text. The formatter generation approach proposed in this article can be used to generate formatting programs for arbitrary programming environments. Our formatter generation approach can be used to automatically generate formatters that have to be programmed explicitly in other systems. © 1996 ACM.",D.2.1 [Software Engineering]: Requirements/Specifications - languages; D.2.3 [Software Engineering]: Coding - pretty printers; D.2.6 [Software Engineering]: Programming Environments,Computer hardware description languages; Computer programming languages; Context free grammars; Context free languages; Program interpreters; Rapid prototyping; Software engineering; Systems analysis; Document preparation; Formatter; Program generators; Program documentation
The chaining approach for software test data generation,1996,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029723725&doi=10.1145%2f226155.226158&partnerID=40&md5=8b3a4d69d38e3ff5cc5c05704ad04d7e,"Software testing is very labor intensive and expensive and accounts for a significant portion of software system development cost. If the testing process could be automated, the cost of developing software could be significantly reduced. Test data generation in program testing is the process of identifying a set of test data that satisfies a selected testing criterion, such as statement coverage and branch coverage. In this article we present a chaining approach for automated software test data generation which builds on the current theory of execution-oriented test data generation. In the chaining approach, test data are derived based on the actual execution of the program under test. For many programs, the execution of the selected statement may require prior execution of some other statements. The existing methods of test data generation may not efficiently generate test data for these types of programs because they only use control flow information of a program during the search process. The chaining approach uses data dependence analysis to guide the search process, i.e., data dependence analysis automatically identifies statements that affect the execution of the selected statement. The chaining approach uses these statements to form a sequence of statements that is to be executed prior to the execution of the selected statement. The experiments have shown that the chaining approach may significantly improve the chances of finding test data as compared to the existing methods of automated test data generation. © 1996 ACM.",D.2.5 [Software Engineering]: Testing and Debugging - test data generation; Data dependency; Dynamic analysis; Experimentation; Heuristics; Measurement; Performance; Program execution,Computer software; Computer software selection and evaluation; Data reduction; Heuristic programming; Performance; Software engineering; Chaining approach; Data dependency; Dynamic analysis; Program execution; Software test data generation; Program debugging
Distributed real-time system specification and verification in APTL,1993,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027677941&partnerID=40&md5=eba1b7e9417e4392faf9ac64803c7d1e,"In this article, we propose a language, Asynchronous Propositional Temporal Logic (APTL), for the specification and verification of distributed hard real-time systems. APTL extends the logic TPTL by dealing explicitly with multiple local clocks. We propose a distributed-system model which permits definition of inequalities asserting the temporal precedence of local clock readings. We show the expressiveness of APTL through two nontrivial examples. Our logic can be used to specify and reason about such important properties as bounded clock rate drifting.",,Computer aided design; Computer hardware description languages; Logic design; Real time systems; Software engineering; Infinite-state sequence; Multiclock system model; Propositional temporal logic; Logic design
Visual execution model for Ada tasking,1993,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027685419&partnerID=40&md5=25557f881af656a1d76c43578c4c3ab6,"A visual execution model for Ada tasking can help programmers attain a deeper understanding of the tasking semantics. It can illustrate subtleties in semantic definitions that are not apparent in natural language descriptions of Ada tasking, as well as the consequences of choices made in the language design. We describe a contour model of Ada tasking that depicts asynchronous tasks (threads of control), relationships between the environments in which tasks execute, and the manner in which tasks interact. The use of this high-level execution model makes it possible to see what happens during execution of a program. The paper provides an introduction to the contour model of Ada tasking and demonstrates its use.",,Computer simulation; Computer software; Computer vision; Control systems; Natural language processing systems; Block-structured language; Flight navigation process; Simulation and modeling; Visual execution model; Ada (programming language)
Conjunction as composition,1993,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027680647&partnerID=40&md5=47abc40251d325662f8fe039f78a324d,"Partial specifications written in many different specification languages can be composed if they are all given semantics in the same domain, or alternatively, all translated into a common style of predicate logic. The common semantic domain must be very general, the particular semantics assigned to each specification language must be conductive to composition, and there must be some means of communication that enables specifications to build on one another. The criteria for success are that a wide variety of specification languages should be accommodated, there should be no restrictions on where boundaries between languages can be placed, and intuitive expectations of the specifier should be met.",,Computer aided analysis; Computer networks; Data processing; Database management systems; Logic design; Data definition languages; Multiparadigm specification; Non temporal logics; Object oriented programming
Verification of Programs Sensitive to Heap Layout,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141097473&doi=10.1145%2f3508363&partnerID=40&md5=446a9ca5a9eea19f6602eff5763a9121,"Most C and C++ programs use dynamically allocated memory (often known as a heap) to store and organize their data. In practice, it can be useful to compare addresses of different heap objects, for instance, to store them in a binary search tree or a sorted array. However, comparisons of pointers to distinct objects are inherently ambiguous: The address order of two objects can be reversed in different executions of the same program, due to the nature of the allocation algorithm and other external factors.This poses a significant challenge to program verification, since a sound verifier must consider all possible behaviors of a program, including an arbitrary reordering of the heap. A naive verification of all possibilities, of course, leads to a combinatorial explosion of the state space: For this reason, we propose an under-approximating abstract domain that can be soundly refined to consider all relevant heap orderings.We have implemented the proposed abstract domain and evaluated it against several existing software verification tools on a collection of pointer-manipulating programs. In many cases, existing tools only consider a single fixed heap order, which is a source of unsoundness. We demonstrate that using our abstract domain, this unsoundness can be repaired at only a very modest performance cost. Additionally, we show that, even though many verifiers ignore it, ambiguous behavior is present in a considerable fraction of programs from software verification competition (sv-comp). © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",abstraction; Heap; pointers; program transformation; refinement,Binary trees; C++ (programming language); Computer software; Abstract domains; Abstraction; Allocation algorithm; Binary search trees; C programs; External factors; Heap; Pointer; Program transformations; Refinement; Verification
Verification Witnesses,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128489872&doi=10.1145%2f3477579&partnerID=40&md5=34a3b53d0c1eda5456813b86a0a6b3cd,"Over the last years, witness-based validation of verification results has become an established practice in software verification: An independent validator re-establishes verification results of a software verifier using verification witnesses, which are stored in a standardized exchange format. In addition to validation, such exchangable information about proofs and alarms found by a verifier can be shared across verification tools, and users can apply independent third-party tools to visualize and explore witnesses to help them comprehend the causes of bugs or the reasons why a given program is correct. To achieve the goal of making verification results more accessible to engineers, it is necessary to consider witnesses as first-class exchangeable objects, stored independently from the source code and checked independently from the verifier that produced them, respecting the important principle of separation of concerns. We present the conceptual principles of verification witnesses, give a description of how to use them, provide a technical specification of the exchange format for witnesses, and perform an extensive experimental study on the application of witness-based result validation, using the validators CPAchecker, UAutomizer, CPA-witness2test, and FShell-witness2test. © 2022 Copyright held by the owner/author(s).",certifying algorithm; correctness witness; data-flow analysis; formal methods; model checking; program analysis; software verification; Violation witness; witness validation,Data flow analysis; Formal verification; Program debugging; Certifying algorithms; Correctness witness; Data-flow analysis; Exchange format; Models checking; Program analysis; Software verification; Verification results; Violation witness; Witness validation; Model checking
Parallel test prioritization,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122128814&doi=10.1145%2f3471906&partnerID=40&md5=88afeadac71cd3aef19f2486649a86f5,"Although regression testing is important to guarantee the software quality in software evolution, it suffers from the widely known cost problem. To address this problem, existing researchers made dedicated efforts on test prioritization, which optimizes the execution order of tests to detect faults earlier; while practitioners in industry leveraged more computing resources to save the time cost of regression testing. By combining these two orthogonal solutions, in this article, we define the problem of parallel test prioritization, which is to conduct test prioritization in the scenario of parallel test execution to reduce the cost of regression testing. Different from traditional sequential test prioritization, parallel test prioritization aims at generating a set of test sequences, each of which is allocated in an individual computing resource and executed in parallel. In particular, we propose eight parallel test prioritization techniques by adapting the existing four sequential test prioritization techniques, by including and excluding testing time in prioritization. To investigate the performance of the eight parallel test prioritization techniques, we conducted an extensive study on 54 open-source projects and a case study on 16 commercial projects from Baidu, a famous search service provider with 600M monthly active users. According to the two studies, parallel test prioritization does improve the efficiency of regression testing, and cost-aware additional parallel test prioritization technique significantly outperforms the other techniques, indicating that this technique is a good choice for practical parallel testing. Besides, we also investigated the influence of two external factors, the number of computing resources and time allowed for parallel testing, and find that more computing resources indeed improve the performance of parallel test prioritization. In addition, we investigated the influence of two more factors, test granularity and coverage criterion, and find that parallel test prioritization can still accelerate regression testing in parallel scenario. Moreover, we investigated the benefit of parallel test prioritization on the regression testing process of continuous integration, considering both the cumulative acceleration performance and the overhead of prioritization techniques, and the results demonstrate the superiority of parallel test prioritization. © 2021 Association for Computing Machinery.",Parallel test prioritization; Parallel testing; Test prioritiization,Computer software selection and evaluation; Integration testing; Open source software; Testing; Computing resource; Parallel test; Parallel test prioritization; Parallel testing; Performance; Prioritization techniques; Regression testing; Sequential tests; Test prioritiization; Test prioritization; Regression analysis
Guaranteeing Timed Opacity using Parametric Timed Model Checking,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132483516&doi=10.1145%2f3502851&partnerID=40&md5=55202d62c9fcc948e5644a550596e299,"Information leakage can have dramatic consequences on systems security. Among harmful information leaks, the timing information leakage occurs whenever an attacker successfully deduces confidential internal information. In this work, we consider that the attacker has access (only) to the system execution time. We address the following timed opacity problem: given a timed system, a private location and a final location, synthesize the execution times from the initial location to the final location for which one cannot deduce whether the system went through the private location. We also consider the full timed opacity problem, asking whether the system is opaque for all execution times. We show that these problems are decidable for timed automata (TAs) but become undecidable when one adds parameters, yielding parametric timed automata (PTAs). We identify a subclass with some decidability results. We then devise an algorithm for synthesizing PTAs parameter valuations guaranteeing that the resulting TA is opaque. We finally show that our method can also apply to program analysis. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",IMITATOR; Opacity; parameter synthesis; timed automata,Automata theory; Computability and decidability; Location; Model checking; Time sharing systems; IMITATOR; Information leakage; Parameter synthesis; Parametric timed automata; Program analysis; System security; Timed Automata; Timed model checking; Timed systems; Timing information; Opacity
Just-In-Time Defect Prediction on JavaScript Projects: A Replication Study,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141065385&doi=10.1145%2f3508479&partnerID=40&md5=363469b3d5da9a29d1c8d0c42df77723,"Change-level defect prediction is widely referred to as just-in-time (JIT) defect prediction since it identifies a defect-inducing change at the check-in time, and researchers have proposed many approaches based on the language-independent change-level features. These approaches can be divided into two types: supervised approaches and unsupervised approaches, and their effectiveness has been verified on Java or C++ projects. However, whether the language-independent change-level features can effectively identify the defects of JavaScript projects is still unknown. Additionally, many researches have confirmed that supervised approaches outperform unsupervised approaches on Java or C++ projects when considering inspection effort. However, whether supervised JIT defect prediction approaches can still perform best on JavaScript projects is still unknown. Lastly, prior proposed change-level features are programming language-independent, whether programming language-specific change-level features can further improve the performance of JIT approaches on identifying defect-prone changes is also unknown.To address the aforementioned gap in knowledge, in this article, we collect and label the top-20 most starred JavaScript projects on GitHub. JavaScript is an extremely popular and widely used programming language in the industry. We propose five JavaScript-specific change-level features and conduct a large-scale empirical study (i.e., involving a total of 176,902 changes) and find that (1) supervised JIT defect prediction approaches (i.e., CBS+) still statistically significantly outperform unsupervised approaches on JavaScript projects when considering inspection effort; (2) JavaScript-specific change-level features can further improve the performance of approach built with language-independent features on identifying defect-prone changes; (3) the change-level features in the dimension of size (i.e., LT), diffusion (i.e., NF), and JavaScript-specific (i.e., SO and TC) are the most important features for indicating the defect-proneness of a change on JavaScript projects; and (4) project-related features (i.e., Stars, Branches, Def Ratio, Changes, Files, Defective, and Forks) have a high association with the probability of a change to be a defect-prone one on JavaScript projects. © 2022 Association for Computing Machinery.",Defect prediction; empirical study; JavaScript; just-in-time defect prediction,C++ (programming language); Forecasting; Just in time production; Defect prediction; Empirical studies; Javascript; Just-in-time; Just-in-time defect prediction; Language independents; Performance; Replication study; Unsupervised approaches; Defects
Predicting Patch Correctness Based on the Similarity of Failing Test Cases,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133510958&doi=10.1145%2f3511096&partnerID=40&md5=894387b672907a202713207bd5dcef12,"How do we know a generated patch is correct? This is a key challenging question that automated program repair (APR) systems struggle to address given the incompleteness of available test suites. Our intuition is that we can triage correct patches by checking whether each generated patch implements code changes (i.e., behavior) that are relevant to the bug it addresses. Such a bug is commonly specified by a failing test case. Towards predicting patch correctness in APR, we propose a novel yet simple hypothesis on how the link between the patch behavior and failing test specifications can be drawn: similar failing test cases should require similar patches. We then propose BATS, an unsupervised learning-based approach to predict patch correctness by checking patch Behavior Against failing Test Specification. BATS exploits deep representation learning models for code and patches: For a given failing test case, the yielded embedding is used to compute similarity metrics in the search for historical similar test cases to identify the associated applied patches, which are then used as a proxy for assessing the correctness of the APR-generated patches. Experimentally, we first validate our hypothesis by assessing whether ground-truth developer patches cluster together in the same way that their associated failing test cases are clustered. Then, after collecting a large dataset of 1,278 plausible patches (written by developers or generated by 32 APR tools), we use BATS to predict correct patches: BATS achieves AUC between 0.557 to 0.718 and recall between 0.562 and 0.854 in identifying correct patches. Our approach outperforms state-of-the-art techniques for identifying correct patches without the need for large labeled patch datasets-as is the case with machine learning-based approaches. While BATS is constrained by the availability of similar test cases, we show that it can still be complementary to existing approaches: When combined with a recent approach that relies on supervised learning, BATS improves the overall recall in detecting correct patches. We finally show that BATS is complementary to the state-of-the-art PATCH-SIM dynamic approach for identifying correct patches generated by APR tools. © 2022 Copyright held by the owner/author(s).",patch correctness; patch semantics; Program repair; test behavior,Codes (symbols); Deep learning; Forecasting; Large dataset; Learning systems; Repair; Software testing; Specifications; Statistical tests; Code changes; Learning-based approach; Patch correctness; Patch semantic; Program repair; Repair system; Repair tools; Test behavior; Test case; Test specifications; Semantics
Boosting Compiler Testing via Compiler Optimization Exploration,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133503005&doi=10.1145%2f3508362&partnerID=40&md5=2f235e517528036d437c3543472480d2,"Compilers are a kind of important software, and similar to the quality assurance of other software, compiler testing is one of the most widely-used ways of guaranteeing their quality. Compiler bugs tend to occur in compiler optimizations. Detecting optimization bugs needs to consider two main factors: (1) the optimization flags controlling the accessability of the compiler buggy code should be turned on; and (2) the test program should be able to trigger the buggy code. However, existing compiler testing approaches only consider the latter to generate effective test programs, but just run them under several pre-defined optimization levels (e.g.,-O0,-O1,-O2,-O3,-Os in GCC).To better understand the influence of compiler optimizations on compiler testing, we conduct the first empirical study, and find that (1) all the bugs detected under the widely-used optimization levels are also detected under the explored optimization settings (we call a combination of optimization flags turned on for compilation an optimization setting), while 83.54% of bugs are only detected under the latter; (2) there exist both inhibition effect and promotion effect among optimization flags for compiler testing, indicating the necessity and challenges of considering the factor of compiler optimizations in compiler testing.We then propose the first approach, called COTest, by considering both factors to test compilers. Specifically, COTest first adopts machine-learning (the XGBoost algorithm) to model the relationship between test programs and optimization settings, to predict the bug-triggering probability of a test program under an optimization setting. Then, it designs a diversity augmentation strategy to select a set of diverse candidate optimization settings for prediction for a test program. Finally, Top-K optimization settings are selected for compiler testing according to the predicted bug-triggering probabilities. Then, it designs a diversity augmentation strategy to select a set of diverse candidate optimization settings for prediction for a test program. Finally, Top-K optimization settings are selected for compiler testing according to the predicted bug-triggering probabilities. The experiments on GCC and LLVM demonstrate its effectiveness, especially COTest detects 17 previously unknown bugs, 11 of which have been fixed or confirmed by developers. © 2022 Association for Computing Machinery.",compiler optimization; Compiler testing; machine learning,Adaptive boosting; Codes (symbols); Forecasting; Program compilers; Quality assurance; Software testing; Testing; Accessability; Combination of optimizations; Compiler optimizations; Compiler testing; Empirical studies; Inhibition effect; Machine-learning; Optimisations; Optimization levels; Test projects; Machine learning
Women's Participation in Open Source Software: A Survey of the Literature,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139823673&doi=10.1145%2f3510460&partnerID=40&md5=6fe12174e5fe37bd180b5e916b611ab2,"Women are underrepresented in Open Source Software (OSS) projects, as a result of which, not only do women lose career and skill development opportunities, but the projects themselves suffer from a lack of diversity of perspectives. Practitioners and researchers need to understand more about the phenomenon; however, studies about women in open source are spread across multiple fields, including information systems, software engineering, and social science. This article systematically maps, aggregates, and synthesizes the state-of-the-art on women's participation in OSS. It focuses on women contributors' representation and demographics, how they contribute, their motivations and challenges, and strategies employed by communities to attract and retain women. We identified 51 articles (published between 2000 and 2021) that investigated women's participation in OSS. We found evidence in these papers about who are the women who contribute, what motivates them to contribute, what types of contributions they make, challenges they face, and strategies proposed to support their participation. According to these studies, only about 5% of projects were reported to have women as core developers, and women authored less than 5% of pull-requests, but had similar or even higher rates of pull-request acceptances than men. Women make both code and non-code contributions, and their motivations to contribute include learning new skills, altruism, reciprocity, and kinship. Challenges that women face in OSS are mainly social, including lack of peer parity and non-inclusive communication from a toxic culture. We found 10 strategies reported in the literature, which we mapped to the reported challenges. Based on these results, we provide guidelines for future research and practice. © 2022 Association for Computing Machinery.",bias; challenges; female; Gender; motivation,Motivation; Open systems; Professional aspects; Social sciences computing; Bias; Career development; Challenge; Female; Gender; Open source software projects; Open-source; Open-source softwares; Skills development; System softwares; Open source software
Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones via Deep Learning,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141091337&doi=10.1145%2f3502852&partnerID=40&md5=77d7c133464a9fd537b7563269f76e78,"In recent years, applying deep learning to detect semantic code clones has received substantial attention from the research community. Accordingly, various evaluation benchmark datasets, with the most popular one as BigCloneBench, are constructed and selected as benchmarks to assess and compare different deep learning models for detecting semantic clones. However, there is no study to investigate whether an evaluation benchmark dataset such as BigCloneBench is properly used to evaluate models for detecting semantic code clones. In this article, we present an experimental study to show that BigCloneBench typically includes semantic clone pairs that use the same identifier names, which however are not used in non-semantic-clone pairs. Subsequently, we propose an undesirable-by-design Linear-Model that considers only which identifiers appear in a code fragment; this model can achieve high effectiveness for detecting semantic clones when evaluated on BigCloneBench, even comparable to state-of-the-art deep learning models recently proposed for detecting semantic clones. To alleviate these issues, we abstract a subset of the identifier names (including type, variable, and method names) in BigCloneBench to result in AbsBigCloneBench and use AbsBigCloneBench to better assess the effectiveness of deep learning models on the task of detecting semantic clones. © 2022 Association for Computing Machinery.",Code clone detection; dataset collection; deep learning,Abstracting; Cloning; Codes (symbols); Deep learning; Learning systems; Benchmark datasets; Code clone; Code clone detection; Code fragments; Dataset collection; Deep learning; Learning models; Linear modeling; Research communities; Semantic codes; Semantics
Correlating Automated and Human Evaluation of Code Documentation Generation Quality,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141053972&doi=10.1145%2f3502853&partnerID=40&md5=fa283ff07b9bb59036a38d7f3161c916,"Automatic code documentation generation has been a crucial task in the field of software engineering. It not only relieves developers from writing code documentation but also helps them to understand programs better. Specifically, deep-learning-based techniques that leverage large-scale source code corpora have been widely used in code documentation generation. These works tend to use automatic metrics (such as BLEU, METEOR, ROUGE, CIDEr, and SPICE) to evaluate different models. These metrics compare generated documentation to reference texts by measuring the overlapping words. Unfortunately, there is no evidence demonstrating the correlation between these metrics and human judgment. We conduct experiments on two popular code documentation generation tasks, code comment generation and commit message generation, to investigate the presence or absence of correlations between these metrics and human judgments. For each task, we replicate three state-of-the-art approaches and the generated documentation is evaluated automatically in terms of BLEU, METEOR, ROUGE-L, CIDEr, and SPICE. We also ask 24 participants to rate the generated documentation considering three aspects (i.e., language, content, and effectiveness). Each participant is given Java methods or commit diffs along with the target documentation to be rated. The results show that the ranking of generated documentation from automatic metrics is different from that evaluated by human annotators. Thus, these automatic metrics are not reliable enough to replace human evaluation for code documentation generation tasks. In addition, METEOR shows the strongest correlation (with moderate Pearson correlation r about 0.7) to human evaluation metrics. However, it is still much lower than the correlation observed between different annotators (with a high Pearson correlation r about 0.8) and correlations that are reported in the literature for other tasks (e.g., Neural Machine Translation [39]). Our study points to the need to develop specialized automated evaluation metrics that can correlate more closely to human evaluation metrics for code generation tasks. © 2022 Association for Computing Machinery.",Code documentation generation; empirical study; evaluation metrics,Correlation methods; Deep learning; SPICE; Automated evaluation; Automatic codes; Automatic metrics; Code documentation generation; Empirical studies; Evaluation metrics; Human evaluation; Human judgments; Pearson correlation; Writing codes; Quality control
Accessibility in Software Practice: A Practitioner's Perspective,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140960884&doi=10.1145%2f3503508&partnerID=40&md5=3a091e2764efe4c536098e9d0dc113ce,"Being able to access software in daily life is vital for everyone, and thus accessibility is a fundamental challenge for software development. However, given the number of accessibility issues reported by many users, e.g., in app reviews, it is not clear if accessibility is widely integrated into current software projects and how software projects address accessibility issues. In this article, we report a study of the critical challenges and benefits of incorporating accessibility into software development and design. We applied a mixed qualitative and quantitative approach for gathering data from 15 interviews and 365 survey respondents from 26 countries across five continents to understand how practitioners perceive accessibility development and design in practice. We got 44 statements grouped into eight topics on accessibility from practitioners' viewpoints and different software development stages. Our statistical analysis reveals substantial gaps between groups, e.g., practitioners have Direct vs. Indirect accessibility relevant work experience when they reviewed the summarized statements. These gaps might hinder the quality of accessibility development and design, and we use our findings to establish a set of guidelines to help practitioners be aware of accessibility challenges and benefit factors. We suggest development teams put accessibility as a first-class consideration throughout the software development process, and we also propose some remedies to resolve the gaps between groups and to highlight key future research directions to incorporate accessibility into software design and development. © 2022 Association for Computing Machinery.",Accessibility development and design; challenges; empirical study; practitioner,Application programs; Surveys; 'current; Accessibility development and design; Challenge; Critical challenges; Daily lives; Empirical studies; Practitioner; Qualitative and quantitative approaches; Software practices; Software project; Software design
Uncertainty-aware Prediction Validator in Deep Learning Models for Cyber-physical System Data,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141027814&doi=10.1145%2f3527451&partnerID=40&md5=05477bf3af557dfe6f20c394ca287037,"The use of Deep learning in Cyber-Physical Systems (CPSs) is gaining popularity due to its ability to bring intelligence to CPS behaviors. However, both CPSs and deep learning have inherent uncertainty. Such uncertainty, if not handled adequately, can lead to unsafe CPS behavior. The first step toward addressing such uncertainty in deep learning is to quantify uncertainty. Hence, we propose a novel method called NIRVANA (uNcertaInty pRediction ValidAtor iN Ai) for prediction validation based on uncertainty metrics. To this end, we first employ prediction-time Dropout-based Neural Networks to quantify uncertainty in deep learning models applied to CPS data. Second, such quantified uncertainty is taken as the input to predict wrong labels using a support vector machine, with the aim of building a highly discriminating prediction validator model with uncertainty values. In addition, we investigated the relationship between uncertainty quantification and prediction performance and conducted experiments to obtain optimal dropout ratios. We conducted all the experiments with four real-world CPS datasets. Results show that uncertainty quantification is negatively correlated to prediction performance of a deep learning model of CPS data. Also, our dropout ratio adjustment approach is effective in reducing uncertainty of correct predictions while increasing uncertainty of wrong predictions. © 2022 Copyright held by the owner/author(s).",Cyber-physical Systems; deep learning; prediction validation; Uncertainty,Deep learning; Embedded systems; Forecasting; Learning systems; Support vector machines; Uncertainty analysis; Cybe-physical systems; Cyber-physical systems; Deep learning; Learning models; Novel methods; Prediction performance; Prediction validation; System behaviors; Uncertainty; Uncertainty quantifications; Cyber Physical System
Monitoring Constraints and Metaconstraints with Temporal Logics on Finite Traces,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132285300&doi=10.1145%2f3506799&partnerID=40&md5=751da2a34ad55a06fddf21a74f5a9aa9,"Runtime monitoring is a central operational decision support task in business process management. It helps process executors to check on-the-fly whether a running process instance satisfies business constraints of interest, providing an immediate feedback when deviations occur. We study runtime monitoring of properties expressed in ltlf, a variant of the classical ltl (Linear-time Temporal Logic) that is interpreted over finite traces, and in its extension ldlf, a powerful logic obtained by combining ltlf with regular expressions. We show that ldlf is able to declaratively express, in the logic itself, not only the constraints to be monitored, but also the de facto standard rv-LTL monitors. On the one hand, this enables us to directly employ the standard characterization of ldlf based on finite-state automata to monitor constraints in a fine-grained way. On the other hand, it provides the basis for declaratively expressing sophisticated metaconstraints that predicate on the monitoring state of other constraints, and to check them by relying on standard logical services instead of ad hoc algorithms. We then report on how this approach has been effectively implemented using Java to manipulate ldlf formulae and their corresponding monitors, and the RuM rule mining suite as underlying infrastructure. © 2022 Association for Computing Machinery.",business process monitoring; metaconstraints; operational decision support; process constraints; runtime verification; Temporal logics,Computer circuits; Decision support systems; Enterprise resource management; Process monitoring; Business Process; Business process monitoring; Decision support task; Finite traces; Metaconstraint; Operational decision support; Process constraints; Process management; Run-time verification; Runtime Monitoring; Temporal logic
NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130728492&doi=10.1145%2f3490489&partnerID=40&md5=4a2b36fff06c0dab5095153ffb174652,"Deep learning has recently been widely applied to many applications across different domains, e.g., image classification and audio recognition. However, the quality of Deep Neural Networks (DNNs) still raises concerns in the practical operational environment, which calls for systematic testing, especially in safety-critical scenarios. Inspired by software testing, a number of structural coverage criteria are designed and proposed to measure the test adequacy of DNNs. However, due to the blackbox nature of DNN, the existing structural coverage criteria are difficult to interpret, making it hard to understand the underlying principles of these criteria. The relationship between the structural coverage and the decision logic of DNNs is unknown. Moreover, recent studies have further revealed the non-existence of correlation between the structural coverage and DNN defect detection, which further posts concerns on what a suitable DNN testing criterion should be.In this article, we propose the interpretable coverage criteria through constructing the decision structure of a DNN. Mirroring the control flow graph of the traditional program, we first extract a decision graph from a DNN based on its interpretation, where a path of the decision graph represents a decision logic of the DNN. Based on the control flow and data flow of the decision graph, we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic. The higher the path coverage, the more diverse decision logic the DNN is expected to be explored. Our large-scale evaluation results demonstrate that: The path in the decision graph is effective in characterizing the decision of the DNN, and the proposed coverage criteria are also sensitive with errors, including natural errors and adversarial examples, and strongly correlate with the output impartiality.  © 2022 Association for Computing Machinery.",Deep learning testing; model interpretation; testing coverage criteria,Computer circuits; Data flow analysis; Flow graphs; Graphic methods; Safety engineering; Safety testing; Software testing; Audio-recognition; Coverage criteria; Decision graphs; Decision logic; Deep learning testing; Different domains; Images classification; Model interpretations; Path coverage; Testing coverage criterion; Deep neural networks
Context-Aware Code Change Embedding for Better Patch Correctness Assessment,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130752121&doi=10.1145%2f3505247&partnerID=40&md5=896d9827ba64bf2c9e1c335c5f5a6100,"Despite the capability in successfully fixing more and more real-world bugs, existing Automated Program Repair (APR) techniques are still challenged by the long-standing overfitting problem (i.e., a generated patch that passes all tests is actually incorrect). Plenty of approaches have been proposed for automated patch correctness assessment (APCA). Nonetheless, dynamic ones (i.e., those that needed to execute tests) are time-consuming while static ones (i.e., those built on top of static code features) are less precise. Therefore, embedding techniques have been proposed recently, which assess patch correctness via embedding token sequences extracted from the changed code of a generated patch. However, existing techniques rarely considered the context information and program structures of a generated patch, which are crucial for patch correctness assessment as revealed by existing studies. In this study, we explore the idea of context-Aware code change embedding considering program structures for patch correctness assessment. Specifically, given a patch, we not only focus on the changed code but also take the correlated unchanged part into consideration, through which the context information can be extracted and leveraged. We then utilize the AST path technique for representation where the structure information from AST node can be captured. Finally, based on several pre-defined heuristics, we build a deep learning based classifier to predict the correctness of the patch. We implemented this idea as Cache and performed extensive experiments to assess its effectiveness. Our results demonstrate that Cache can (1) perform better than previous representation learning based techniques (e.g., Cache relatively outperforms existing techniques by 6%, 3%, and 16%, respectively under three diverse experiment settings), and (2) achieve overall higher performance than existing APCA techniques while even being more precise than certain dynamic ones including PATCH-SIM (92.9% vs. 83.0%). Further results reveal that the context information and program structures leveraged by Cache contributed significantly to its outstanding performance.  © 2022 Association for Computing Machinery.",Automated program repair; deep learning; patch correctness,Automation; Deep learning; Program debugging; Repair; Software testing; Automated program repair; Code changes; Context information; Context-Aware; Deep learning; Embeddings; Information structures; Patch correctness; Performance; Program structures; Embeddings
Verification of Distributed Systems via Sequential Emulation,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130748937&doi=10.1145%2f3490387&partnerID=40&md5=48395a4ef4ba51925484bb8dfce9c48c,"Sequential emulation is a semantics-based technique to automatically reduce property checking of distributed systems to the analysis of sequential programs. An automated procedure takes as input a formal specification of a distributed system, a property of interest, and the structural operational semantics of the specification language and generates a sequential program whose execution traces emulate the possible evolutions of the considered system. The problem as to whether the property of interest holds for the system can then be expressed either as a reachability or as a termination query on the program. This allows to immediately adapt mature verification techniques developed for general-purpose languages to domain-specific languages, and to effortlessly integrate new techniques as soon as they become available. We test our approach on a selection of concurrent systems originated from different contexts from population protocols to models of flocking behaviour. By combining a comprehensive range of program verification techniques, from traditional symbolic execution to modern inductive-based methods such as property-directed reachability, we are able to draw consistent and correct verification verdicts for the considered systems.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Concurrency; distribution; domain-specific languages; process algebra; program verification; reachability; semantics-based verification; sequentialization; structural operational semantics; termination,Formal specification; Problem oriented languages; Specification languages; Concurrency; Distribution; Process algebras; Program Verification; Property; Reachability; Semantic-based verification; Sequentialization; Structural operational semantics; Termination; Semantics
Super-optimization of Smart Contracts,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135842178&doi=10.1145%2f3506800&partnerID=40&md5=432eb751c3c5b689813a1ffb68b5ec8c,"Smart contracts are programs deployed on a blockchain. They are executed for a monetary fee paid in gas-a clear optimization target for smart contract compilers. Because smart contracts are a young, fast-moving field without (manually) fine-tuned compilers, they highly benefit from automated and adaptable approaches, especially as smart contracts are effectively immutable, and as such need a high level of assurance. This makes them an ideal domain for applying formal methods. Super-optimization is a technique to find the best translation of a block of instructions by trying all possible sequences of instructions that produce the same result. We present a framework for super-optimizing smart contracts based on Max-SMT with two main ingredients: (1) a stack functional specification extracted from the basic blocks of a smart contract, which is simplified using rules capturing the semantics of arithmetic, bit-wise, and relational operations, and (2) the synthesis of optimized blocks, which finds-by means of an efficient SMT encoding-basic blocks with minimal gas cost whose stack functional specification is equal (modulo commutativity) to the extracted one.We implemented our framework in the tool syrup 2.0. Through large-scale experiments on real-world smart contracts, we analyze performance improvements for different SMT encodings, as well as tradeoffs between quality of optimizations and required optimization time. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Max-SMT solvers; optimization; Smart contracts; synthesis,Encoding (symbols); Formal methods; Program compilers; Semantics; Basic blocks; Block-chain; Commutativity; Encodings; Functional specification; Gas cost; Max-SMT solver; Moving field; Optimisations; Relational operations; Smart contract
An Empirical Study of the Effectiveness of an Ensemble of Stand-Alone Sentiment Detection Tools for Software Engineering Datasets,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130699711&doi=10.1145%2f3491211&partnerID=40&md5=9d05b455d45a7ee5551bbd764d064c06,"Sentiment analysis in software engineering (SE) has shown promise to analyze and support diverse development activities. Recently, several tools are proposed to detect sentiments in software artifacts. While the tools improve accuracy over off-The-shelf tools, recent research shows that their performance could still be unsatisfactory. A more accurate sentiment detector for SE can help reduce noise in analysis of software scenarios where sentiment analysis is required. Recently, combinations, i.e., hybrids of stand-Alone classifiers are found to offer better performance than the stand-Alone classifiers for fault detection. However, we are aware of no such approach for sentiment detection for software artifacts. We report the results of an empirical study that we conducted to determine the feasibility of developing an ensemble engine by combining the polarity labels of stand-Alone SE-specific sentiment detectors. Our study has two phases. In the first phase, we pick five SE-specific sentiment detection tools from two recently published papers by Lin et al. [29, 30], who first reported negative results with stand alone sentiment detectors and then proposed an improved SE-specific sentiment detector, POME [29]. We report the study results on 17,581 units (sentences/documents) coming from six currently available sentiment benchmarks for software engineering. We find that the existing tools can be complementary to each other in 85-95% of the cases, i.e., one is wrong but another is right. However, a majority voting-based ensemble of those tools fails to improve the accuracy of sentiment detection. We develop Sentisead, a supervised tool by combining the polarity labels and bag of words as features. Sentisead improves the performance (F1-score) of the individual tools by 4% (over Senti4SD [5])-100% (over POME [29]). The initial development of Sentisead occurred before we observed the use of deep learning models for SE-specific sentiment detection. In particular, recent papers show the superiority of advanced language-based pre-Trained transformer models (PTM) over rule-based and shallow learning models. Consequently, in a second phase, we compare and improve Sentisead infrastructure using the PTMs. We find that a Sentisead infrastructure with RoBERTa as the ensemble of the five stand-Alone rule-based and shallow learning SE-specific tools from Lin et al. [29, 30] offers the best F1-score of 0.805 across the six datasets, while a stand-Alone RoBERTa shows an F1-score of 0.801.  © 2022 Association for Computing Machinery.",ensemble classifier; machine learning; Sentiment analysis,Classification (of information); Deep learning; Fault detection; Learning algorithms; Software engineering; Detection tools; Empirical studies; Ensemble-classifier; F1 scores; Learning models; Performance; Rule based; Sentiment analysis; Software artefacts; Stand -alone; Sentiment analysis
XCode: Towards Cross-Language Code Representation with Large-Scale Pre-Training,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130754433&doi=10.1145%2f3506696&partnerID=40&md5=b2c60159ed514b3de7d7cdd2aaa6e896,"Source code representation learning is the basis of applying artificial intelligence to many software engineering tasks such as code clone detection, algorithm classification, and code summarization. Recently, many works have tried to improve the performance of source code representation from various perspectives, e.g., introducing the structural information of programs into latent representation. However, when dealing with rapidly expanded unlabeled cross-language source code datasets from the Internet, there are still two issues. Firstly, deep learning models for many code-specific tasks still suffer from the lack of high-quality labels. Secondly, the structural differences among programming languages make it more difficult to process multiple languages in a single neural architecture.To address these issues, in this article, we propose a novel Cross-language Code representation with a large-scale pre-Training (XCode) method. Concretely, we propose to use several abstract syntax trees and ELMo-enhanced variational autoencoders to obtain multiple pre-Trained source code language models trained on about 1.5 million code snippets. To fully utilize the knowledge across programming languages, we further propose a Shared Encoder-Decoder (SED) architecture which uses the multi-Teacher single-student method to transfer knowledge from the aforementioned pre-Trained models to the distilled SED. The pre-Trained models and SED will cooperate to better represent the source code. For evaluation, we examine our approach on three typical downstream cross-language tasks, i.e., source code translation, code clone detection, and code-To-code search, on a real-world dataset composed of programming exercises with multiple solutions. Experimental results demonstrate the effectiveness of our proposed approach on cross-language code representations. Meanwhile, our approach performs significantly better than several code representation baselines on different downstream tasks in terms of multiple automatic evaluation metrics.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code representation; cross-language; Deep learning; neural networks; pre-Training,Abstracting; Cloning; Codes (symbols); Computational linguistics; Deep learning; Network architecture; Network coding; Translation (languages); Code clone detection; Code representation; Cross languages; Deep learning; Encoder-decoder; Large-scales; Neural-networks; Pre-training; Source code representations; Source codes; Trees (mathematics)
Adversarial Robustness of Deep Code Comment Generation,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141028390&doi=10.1145%2f3501256&partnerID=40&md5=9a77cdc6b56a9e6db3c8e6734bca57fb,"Deep neural networks (DNNs) have shown remarkable performance in a variety of domains such as computer vision, speech recognition, and natural language processing. Recently they also have been applied to various software engineering tasks, typically involving processing source code. DNNs are well-known to be vulnerable to adversarial examples, i.e., fabricated inputs that could lead to various misbehaviors of the DNN model while being perceived as benign by humans. In this paper, we focus on the code comment generation task in software engineering and study the robustness issue of the DNNs when they are applied to this task. We propose ACCENT(Adversarial Code Comment gENeraTor), an identifier substitution approach to craft adversarial code snippets, which are syntactically correct and semantically close to the original code snippet, but may mislead the DNNs to produce completely irrelevant code comments. In order to improve the robustness, ACCENT also incorporates a novel training method, which can be applied to existing code comment generation models. We conduct comprehensive experiments to evaluate our approach by attacking the mainstream encoder-decoder architectures on two large-scale publicly available datasets. The results show that ACCENT efficiently produces stable attacks with functionality-preserving adversarial examples, and the generated examples have better transferability compared with the baselines. We also confirm, via experiments, the effectiveness in improving model robustness with our training method. © 2022 Association for Computing Machinery.",adversarial attack; Code comment generation; deep learning; robustness,Codes (symbols); Large dataset; Natural language processing systems; Software engineering; Speech recognition; Adversarial attack; Code comment generation; Deep learning; Engineering tasks; Language processing; Natural languages; Performance; Robustness; Speech recognition languages; Training methods; Deep neural networks
"Towards Robustness of Deep Program Processing Models-Detection, Estimation, and Enhancement",2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130755119&doi=10.1145%2f3511887&partnerID=40&md5=d3dd8a684ba5c1be920054b79edf80a5,"Deep learning (DL) has recently been widely applied to diverse source code processing tasks in the software engineering (SE) community, which achieves competitive performance (e.g., accuracy). However, the robustness, which requires the model to produce consistent decisions given minorly perturbed code inputs, still lacks systematic investigation as an important quality indicator. This article initiates an early step and proposes a framework CARROT for robustness detection, measurement, and enhancement of DL models for source code processing. We first propose an optimization-based attack technique CARROTA to generate valid adversarial source code examples effectively and efficiently. Based on this, we define the robustness metrics and propose robustness measurement toolkit CARROTM, which employs the worst-case performance approximation under the allowable perturbations. We further propose to improve the robustness of the DL models by adversarial training (CARROTT) with our proposed attack techniques. Our in-depth evaluations on three source code processing tasks (i.e., functionality classification, code clone detection, defect prediction) containing more than 3 million lines of code and the classic or SOTA DL models, including GRU, LSTM, ASTNN, LSCNN, TBCNN, CodeBERT, and CDLH, demonstrate the usefulness of our techniques for g¶ effective and efficient adversarial example detection, g• tight robustness estimation, and effective robustness enhancement.  © 2022 Association for Computing Machinery.",adversarial attack; big code; robustness enhancement; Source code processing,Codes (symbols); Computer programming languages; Software engineering; Adversarial attack; Big code; Detection enhancement; Detection estimation; Learning models; Processing model; Program processing; Robustness enhancement; Source code processing; Source codes; Long short-term memory
Verifix: Verified Repair of Programming Assignments,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139329042&doi=10.1145%2f3510418&partnerID=40&md5=a098815bf748cbc9d688bace53d14890,"Automated feedback generation for introductory programming assignments is useful for programming education. Most works try to generate feedback to correct a student program by comparing its behavior with an instructor's reference program on selected tests. In this work, our aim is to generate verifiably correct program repairs as student feedback. A student-submitted program is aligned and composed with a reference solution in terms of control flow, and the variables of the two programs are automatically aligned via predicates describing the relationship between the variables. When verification attempt for the obtained aligned program fails, we turn a verification problem into a MaxSMT problem whose solution leads to a minimal repair. We have conducted experiments on student assignments curated from a widely deployed intelligent tutoring system. Our results show that generating verified repair without sacrificing the overall repair rate is possible. In fact, our implementation, Verifix, is shown to outperform Clara, a state-of-the-art tool, in terms of repair rate. This shows the promise of using verified repair to generate high confidence feedback in programming pedagogy settings. © 2022 Association for Computing Machinery.",Automated program repair; intelligent tutoring system,Computer aided instruction; Education computing; Feedback; Software testing; Students; Automated feedback; Automated program repair; Intelligent tutoring; Intelligent tutoring system; Introductory programming; Programming assignments; Programming education; Repair rate; Student project; Tutoring system; Repair
Examining Penetration Tester Behavior in the Collegiate Penetration Testing Competition,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130725367&doi=10.1145%2f3514040&partnerID=40&md5=1b3a8405c393eb6998508481a24161c9,"Penetration testing is a key practice toward engineering secure software. Malicious actors have many tactics at their disposal, and software engineers need to know what tactics attackers will prioritize in the first few hours of an attack. Projects like MITRE ATT&CK™ provide knowledge, but how do people actually deploy this knowledge in real situations A penetration testing competition provides a realistic, controlled environment with which to measure and compare the efficacy of attackers. In this work, we examine the details of vulnerability discovery and attacker behavior with the goal of improving existing vulnerability assessment processes using data from the 2019 Collegiate Penetration Testing Competition (CPTC). We constructed 98 timelines of vulnerability discovery and exploits for 37 unique vulnerabilities discovered by 10 teams of penetration testers. We grouped related vulnerabilities together by mapping to Common Weakness Enumerations and MITRE ATT&CK™. We found that (1) vulnerabilities related to improper resource control (e.g., session fixation) are discovered faster and more often, as well as exploited faster, than vulnerabilities related to improper access control (e.g., weak password requirements), (2) there is a clear process followed by penetration testers of discovery/collection to lateral movement/pre-Attack. Our methodology facilitates quicker analysis of vulnerabilities in future CPTC events.  © 2022 Association for Computing Machinery.",attacker behavior; cybersecurity competitions; discoverability; likelihood of discovery; metrics; penetration testing; Security; vulnerabilities; vulnerability assessment,Cybersecurity; Software testing; Attacker behavior; Cyber security; Cybersecurity competition; Discoverability; Likelihood of discovery; Metric; Penetration testing; Security; Vulnerability; Vulnerability assessments; Access control
How Do Successful and Failed Projects Differ? A Socio-Technical Analysis,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141080213&doi=10.1145%2f3504003&partnerID=40&md5=0fe7f797dbb21d4a14c347236e7ce9d0,"Software development is at the intersection of the social realm, involving people who develop the software, and the technical realm, involving artifacts (code, docs, etc.) that are being produced. It has been shown that a socio-technical perspective provides rich information about the state of a software project.In particular, we are interested in socio-technical factors that are associated with project success. For this purpose, we frame the task as a network classification problem. We show how a set of heterogeneous networks composed of social and technical entities can be jointly embedded in a single vector space enabling mathematically sound comparisons between distinct software projects. Our approach is specifically designed using intuitive metrics stemming from network analysis and statistics to ease the interpretation of results in the context of software engineering wisdom. Based on a selection of 32 open source projects, we perform an empirical study to validate our approach considering three prediction scenarios to test the classification model's ability generalizing to (1) randomly held-out project snapshots, (2) future project states, and (3) entirely new projects.Our results provide evidence that a socio-technical perspective is superior to a pure social or technical perspective when it comes to early indicators of future project success. To our surprise, the methodology proposed here even shows evidence of being able to generalize to entirely novel (project hold-out set) software projects reaching predication accuracies of 80%, which is a further testament to the efficacy of our approach and beyond what has been possible so far. In addition, we identify key features that are strongly associated with project success. Our results indicate that even relatively simple socio-technical networks capture highly relevant and interpretable information about the early indicators of future project success. © 2022 Association for Computing Machinery.",Empirical software engineering; information system success; quantitative software engineering; socio-technical networks; statistical network analysis,Heterogeneous networks; Open source software; Software design; Early indicators; Empirical Software Engineering; Information systems success; Project success; Quantitative software engineering; Socio-technical networks; Socio-technical perspective; Sociotechnical; Software project; Statistical network analysis; Vector spaces
A Common Terminology for Software Risk Management,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141060522&doi=10.1145%2f3498539&partnerID=40&md5=55ce60aa80ef7b27eb57f9012cb59e7d,"In order to improve and sustain their competitiveness over time, organisations nowadays need to undertake different initiatives to adopt frameworks, models and standards that will allow them to align and improve their business processes. In spite of these efforts, organisations may still encounter governance and management problems. This is where Risk Management (RM) can play a major role, since its purpose is to contribute to the creation and preservation of value in the context of the organisation's processes. RM is a complex and subjective activity that requires experience and a high level of knowledge about risks, and it is for this reason that standardisation institutions and researchers have made great efforts to define initiatives to overcome these challenges. However, the RM field nevertheless presents a lack of uniformity in its terms and concepts, due to the different contexts and scopes of application, a situation that can generate ambiguities and misunderstandings. To address these issues, this paper aims to present an ontology called SRMO (Software Risk Management Ontology), which seeks to unify the terms and concepts associated with RM and provide an integrated and holistic view of risk. In doing so, the Pipeline framework has been applied in order to assure and verify the quality of the proposed ontology, and it has been implemented in Protégé and validated by means of competency questions. Three application scenarios of this ontology demonstrating their usefulness in the software engineering field are presented in this paper. We believe that this ontology can be useful for organisations that are interested in: (i) establishing an RM strategy from an integrated approach, (ii) defining the elements that help to identify risks and the criteria that support decision-making in risk assessment, and (iii) helping the involved stakeholders during the process of risk management. © 2022 Association for Computing Machinery.",integrated risk management; ISO 31000; Risk management; risk ontology,Application programs; Competition; Decision making; Ontology; Risk assessment; Business Process; Framework models; Integrated risk management; ISO 31000; Management IS; Management problems; Ontology's; Risk ontologies; Risks management; Software risk management; Risk management
L2S: A Framework for Synthesizing the Most Probable Program under a Specification,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130749654&doi=10.1145%2f3487570&partnerID=40&md5=cfdbef147c13790387a352a08a0e2534,"In many scenarios, we need to find the most likely program that meets a specification under a local context, where the local context can be an incomplete program, a partial specification, natural language description, and so on. We call such a problem program estimation. In this article, we propose a framework, LingLong Synthesis Framework (L2S), to address this problem. Compared with existing work, our work is novel in the following aspects. (1) We propose a theory of expansion rules to describe how to decompose a program into choices. (2) We propose an approach based on abstract interpretation to efficiently prune off the program sub-space that does not satisfy the specification. (3) We prove that the probability of a program is the product of the probabilities of choosing expansion rules, regardless of the choosing order. (4) We reduce the program estimation problem to a pathfinding problem, enabling existing pathfinding algorithms to solve this problem.L2S has been applied to program generation and program repair. In this article, we report our instantiation of this framework for synthesizing conditional expressions (L2S-Cond) and repairing conditional statements (L2S-Hanabi). The experiments on L2S-Cond show that each option enabled by L2S, including the expansion rules, the pruning technique, and the use of different pathfinding algorithms, plays a major role in the performance of the approach. The default configuration of L2S-Cond correctly predicts nearly 60% of the conditional expressions in the top 5 candidates. Moreover, we evaluate L2S-Hanabi on 272 bugs from two real-world Java defects benchmarks, namely Defects4J and Bugs.jar. L2S-Hanabi correctly fixes 32 bugs with a high precision of 84%. In terms of repairing conditional statement bugs, L2S-Hanabi significantly outperforms all existing approaches in both precision and recall.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",expansion rules; Program estimation; program repair; program synthesis,Abstracting; Defects; Repair; Specifications; Conditional expressions; Expansion rule; Local contexts; Most likely; Natural languages; Partial specifications; Path-finding algorithms; Program estimation; Program repair; Program synthesis; Expansion
Stateful Serverless Computing with Crucial,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130708185&doi=10.1145%2f3490386&partnerID=40&md5=b447031c4eea85972afbdb76e21e965e,"Serverless computing greatly simplifies the use of cloud resources. In particular, Function-As-A-Service (FaaS) platforms enable programmers to develop applications as individual functions that can run and scale independently. Unfortunately, applications that require fine-grained support for mutable state and synchronization, such as machine learning (ML) and scientific computing, are notoriously hard to build with this new paradigm. In this work, we aim at bridging this gap. We present Crucial, a system to program highly-parallel stateful serverless applications. Crucial retains the simplicity of serverless computing. It is built upon the key insight that FaaS resembles to concurrent programming at the scale of a datacenter. Accordingly, a distributed shared memory layer is the natural answer to the needs for fine-grained state management and synchronization. Crucial allows to port effortlessly a multi-Threaded code base to serverless, where it can benefit from the scalability and pay-per-use model of FaaS platforms. We validate Crucial with the help of micro-benchmarks and by considering various stateful applications. Beyond classical parallel tasks (e.g., a Monte Carlo simulation), these applications include representative ML algorithms such as k-means and logistic regression. Our evaluation shows that Crucial obtains superior or comparable performance to Apache Spark at similar cost (18%-40% faster). We also use Crucial to port (part of) a state-of-The-Art multi-Threaded ML library to serverless. The ported application is up to 30% faster than with a dedicated high-end server. Finally, we attest that Crucial can rival in performance with a single-machine, multi-Threaded implementation of a complex coordination problem. Overall, Crucial delivers all these benefits with less than 6% of changes in the code bases of the evaluated applications.  © 2022 Copyright held by the owner/author(s).",FaaS; in-memory; Serverless; stateful; synchronization,Application programs; Benchmarking; Coordination reactions; Intelligent systems; K-means clustering; Memory architecture; Monte Carlo methods; Scheduling algorithms; Concurrent programming; Fine grained; Function-as-A-service; Highly parallels; In-memory; Multithreaded; Performance; Serverless; Service platforms; Stateful; Synchronization
"Automated, Cost-effective, and Update-driven App Testing",2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136791725&doi=10.1145%2f3502297&partnerID=40&md5=c97df5bfe196eec186765c97af540615,"Apps' pervasive role in our society led to the definition of test automation approaches to ensure their dependability. However, state-of-the-art approaches tend to generate large numbers of test inputs and are unlikely to achieve more than 50% method coverage.In this article, we propose a strategy to achieve significantly higher coverage of the code affected by updates with a much smaller number of test inputs, thus alleviating the test oracle problem.More specifically, we present ATUA, a model-based approach that synthesizes App models with static analysis, integrates a dynamically refined state abstraction function and combines complementary testing strategies, including (1) coverage of the model structure, (2) coverage of the App code, (3) random exploration, and (4) coverage of dependencies identified through information retrieval. Its model-based strategy enables ATUA to generate a small set of inputs that exercise only the code affected by the updates. In turn, this makes common test oracle solutions more cost-effective, as they tend to involve human effort.A large empirical evaluation, conducted with 72 App versions belonging to nine popular Android Apps, has shown that ATUA is more effective and less effort-intensive than state-of-the-art approaches when testing App updates. © 2022 Association for Computing Machinery.",Android testing; information retrieval; model-based testing; regression testing; upgrade testing,Android (operating system); Cost effectiveness; Model checking; Software testing; Static analysis; Android testing; Cost effective; Model based testing; Oracle problem; Regression testing; State-of-the-art approach; Test Automation; Test inputs; Test oracles; Upgrade testing; Information retrieval
Detecting and Augmenting Missing Key Aspects in Vulnerability Descriptions,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130745459&doi=10.1145%2f3498537&partnerID=40&md5=cfefc2fdede272d38e8ad9966eeb8b54,"Security vulnerabilities have been continually disclosed and documented. For the effective understanding, management, and mitigation of the fast-growing number of vulnerabilities, an important practice in documenting vulnerabilities is to describe the key vulnerability aspects, such as vulnerability type, root cause, affected product, impact, attacker type, and attack vector. In this article, we first investigate 133,639 vulnerability reports in the Common Vulnerabilities and Exposures (CVE) database over the past 20 years. We find that 56%, 85%, 38%, and 28% of CVEs miss vulnerability type, root cause, attack vector, and attacker type, respectively. By comparing the differences of the latest updated CVE reports across different databases, we observe that 1,476 missing key aspects in 1,320 CVE descriptions were augmented manually in the National Vulnerability Database (NVD), which indicates that the vulnerability database maintainers try to complete the vulnerability descriptions in practice to mitigate such a problem.To help complete the missing information of key vulnerability aspects and reduce human efforts, we propose a neural-network-based approach called PMA to predict the missing key aspects of a vulnerability based on its known aspects. We systematically explore the design space of the neural network models and empirically identify the most effective model design in the scenario. Our ablation study reveals the prominent correlations among vulnerability aspects when predicting. Trained with historical CVEs, our model achieves 88%, 71%, 61%, and 81% in F1 for predicting the missing vulnerability type, root cause, attacker type, and attack vector of 8,623 ""future""CVEs across 3 years, respectively. Furthermore, we validate the predicting performance of key aspect augmentation of CVEs based on the manually augmented CVE data collected from NVD, which confirms the practicality of our approach. We finally highlight that PMA has the ability to reduce human efforts by recommending and augmenting missing key aspects for vulnerability databases, and to facilitate other research works such as severity level prediction of CVEs based on the vulnerability descriptions.  © 2022 Association for Computing Machinery.",CVE; data augmentation; deep neural network; vulnerability description,Database systems; Deep neural networks; Network security; Attack vector; Common vulnerabilities and exposures; Data augmentation; Exposure database; Missing information; National vulnerability database; Root cause; Security vulnerabilities; Vulnerability database; Vulnerability description; Forecasting
Predictive Models in Software Engineering: Challenges and Opportunities,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130759581&doi=10.1145%2f3503509&partnerID=40&md5=9fe3adb7cd22469cc37df72481195af2,"Predictive models are one of the most important techniques that are widely applied in many areas of software engineering. There have been a large number of primary studies that apply predictive models and that present well-performed studies in various research domains, including software requirements, software design and development, testing and debugging, and software maintenance. This article is a first attempt to systematically organize knowledge in this area by surveying a body of 421 papers on predictive models published between 2009 and 2020. We describe the key models and approaches used, classify the different models, summarize the range of key application areas, and analyze research results. Based on our findings, we also propose a set of current challenges that still need to be addressed in future work and provide a proposed research road map for these opportunities.  © 2022 Association for Computing Machinery.",deep learning; machine learning; Predictive models; software engineering; survey,Deep learning; Program debugging; Software design; Software testing; Well testing; Application analysis; Deep learning; Development testing; Engineering challenges; Key models; Predictive models; Research domains; Software design and development; Software requirements; Testing and debugging; Surveys
Do Developers Really Know How to Use Git Commands A Large-scale Study Using Stack Overflow,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130719752&doi=10.1145%2f3494518&partnerID=40&md5=b1fc87df14a3b9847e382dbf196a3d9f,"Git, a cross-platform and open source distributed version control tool, provides strong support for non-linear development and is capable of handling everything from small to large projects with speed and efficiency. It has become an indispensable tool for millions of software developers and is the de facto standard of version control in software development nowadays. However, despite its widespread use, developers still frequently face difficulties when using various Git commands to manage projects and collaborate. To better help developers use Git, it is necessary to understand the issues and difficulties that they may encounter when using Git. Unfortunately, this problem has not yet been comprehensively studied. To fill this knowledge gap, in this article, we conduct a large-scale study on Stack Overflow, a popular Q&A forum for developers. We extracted and analyzed 80,370 relevant questions from Stack Overflow, and reported the increasing popularity of the Git command questions. By analyzing the questions, we identified the Git commands that are frequently asked and those that are associated with difficult questions on Stack Overflow to help understand the difficulties developers may encounter when using Git commands. In addition, we conducted a survey to understand how developers learn Git commands in practice, showing that self-learning is the primary learning approach. These findings provide a range of actionable implications for researchers, educators, and developers.  © 2022 Association for Computing Machinery.",Git commands; Stack Overflow; user survey,Information management; Open source software; Surveys; Technology transfer; Control tools; Cross-platform; Distributed version controls; Git command; Large-scale studies; Linear development; Non linear; Open-source; Stack overflow; User surveys; Software design
Predictive Mutation Analysis via the Natural Language Channel in Source Code,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141085074&doi=10.1145%2f3510417&partnerID=40&md5=2a6787410806107da1a2e5cb56c6ab4f,"Mutation analysis can provide valuable insights into both the system under test and its test suite. However, it is not scalable due to the cost of building and testing a large number of mutants. Predictive Mutation Testing (PMT) has been proposed to reduce the cost of mutation testing, but it can only provide statistical inference about whether a mutant will be killed or not by the entire test suite. We propose Seshat, a Predictive Mutation Analysis (PMA) technique that can accurately predict the entire kill matrix, not just the Mutation Score (MS) of the given test suite. Seshat exploits the natural language channel in code, and learns the relationship between the syntactic and semantic concepts of each test case and the mutants it can kill, from a given kill matrix. The learnt model can later be used to predict the kill matrices for subsequent versions of the program, even after both the source and test code have changed significantly. Empirical evaluation using the programs in Defects4J shows that Seshat can predict kill matrices with an average F-score of 0.83 for versions that are up to years apart. This is an improvement in F-score by 0.14 and 0.45 points over the state-of-the-art PMT technique and a simple coverage-based heuristic, respectively. Seshat also performs as well as PMT for the prediction of the MS only. When applied to a mutant-based fault localisation technique, the predicted kill matrix by Seshat is successfully used to locate faults within the top 10 position, showing its usefulness beyond prediction of MS. Once Seshat trains its model using a concrete mutation analysis, the subsequent predictions made by Seshat are on average 39 times faster than actual test-based analysis. We also show that Seshat can be successfully applied to automatically generated test cases with an experiment using EvoSuite. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; Mutation analysis,Deep learning; Semantics; Software testing; Deep learning; F-score; Learn+; matrix; Mutation analysis; Mutation score; Mutation testing; Natural languages; Source codes; Test case; Forecasting
Testing the Plasticity of Reinforcement Learning-based Systems,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137559262&doi=10.1145%2f3511701&partnerID=40&md5=be2ef027e7f94e0dcaaeef60c6310659,"The dataset available for pre-release training of a machine-learning based system is often not representative of all possible execution contexts that the system will encounter in the field. Reinforcement Learning (RL) is a prominent approach among those that support continual learning, i.e., learning continually in the field, in the post-release phase. No study has so far investigated any method to test the plasticity of RL-based systems, i.e., their capability to adapt to an execution context that may deviate from the training one.We propose an approach to test the plasticity of RL-based systems. The output of our approach is a quantification of the adaptation and anti-regression capabilities of the system, obtained by computing the adaptation frontier of the system in a changed environment. We visualize such frontier as an adaptation/anti-regression heatmap in two dimensions, or as a clustered projection when more than two dimensions are involved. In this way, we provide developers with information on the amount of changes that can be accommodated by the continual learning component of the system, which is key to decide if online, in-the-field learning can be safely enabled or not. © 2022 Association for Computing Machinery.",empirical software engineering; reinforcement learning; Software testing,Learning systems; Plasticity; Software testing; Continual learning; Empirical Software Engineering; Execution context; Heatmaps; Machine-learning; Reinforcement learnings; Software testings; Two-dimensions; Reinforcement learning
Mutant Reduction Evaluation: What is There and What is Missing?,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141044309&doi=10.1145%2f3522578&partnerID=40&md5=366a4311418b2fec5aa0517ca68fff0b,"Background. Mutation testing is a commonly used defect injection technique for evaluating the effectiveness of a test suite. However, it is usually computationally expensive. Therefore, many mutation reduction strategies, which aim to reduce the number of mutants, have been proposed.Problem. It is important to measure the ability of a mutation reduction strategy to maintain test suite effectiveness evaluation. However, existing evaluation indicators are unable to measure the ""order-preserving ability"", i.e., to what extent the mutation score order among test suites is maintained before and after mutation reduction. As a result, misleading conclusions can be achieved when using existing indicators to evaluate the reduction effectiveness.Objective. We aim to propose evaluation indicators to measure the ""order-preserving ability""of a mutation reduction strategy, which is important but missing in our community.Method. Given a test suite on a Software Under Test (SUT) with a set of original mutants, we leverage the test suite to generate a group of test suites that have a partial order relationship in defect detecting ability. When evaluating a reduction strategy, we first construct two partial order relationships among the generated test suites in terms of mutation score, one with the original mutants and another with the reduced mutants. Then, we measure the extent to which the partial order under the original mutants remains unchanged in the partial order under the reduced mutants. The more partial order is unchanged, the stronger the Order Preservation (OP) of the mutation reduction strategy is, and the more effective the reduction strategy is. Furthermore, we propose Effort-aware Relative Order Preservation (EROP) to measure how much gain a mutation reduction strategy can provide compared with a random reduction strategy.Result. The experimental results show that OP and EROP are able to efficiently measure the ""order-preserving ability""of a mutation reduction strategy. As a result, they have a better ability to distinguish various mutation reduction strategies compared with the existing evaluation indicators. In addition, we find that Subsuming Mutant Selection (SMS) and Clustering Mutant Selection (CMS) are more effective than the other strategies under OP and EROP.Conclusion. We suggest, for the researchers, that OP and EROP should be used to measure the effectiveness of a mutant reduction strategy, and for the practitioners, that SMS and CMS should be given priority in practice. © 2022 Association for Computing Machinery.",evaluation; Mutant reduction; order preservation; test suites,Defects; Testing; % reductions; Evaluation; Evaluation indicators; Mutant reduction; Order preservation; Order preserving; Partial order; Reduction strategy; Relative order; Test suite; Software testing
Deep Reinforcement Learning for Black-box Testing of Android Apps,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121574266&doi=10.1145%2f3502868&partnerID=40&md5=5af1be5241003fcb7ce0369efd945137,"The state space of Android apps is huge, and its thorough exploration during testing remains a significant challenge. The best exploration strategy is highly dependent on the features of the app under test. Reinforcement Learning (RL) is a machine learning technique that learns the optimal strategy to solve a task by trial and error, guided by positive or negative reward, rather than explicit supervision. Deep RL is a recent extension of RL that takes advantage of the learning capabilities of neural networks. Such capabilities make Deep RL suitable for complex exploration spaces such as one of Android apps. However, state-of-the-art, publicly available tools only support basic, Tabular RL. We have developed ARES, a Deep RL approach for black-box testing of Android apps. Experimental results show that it achieves higher coverage and fault revelation than the baselines, including state-of-the-art tools, such as TimeMachine and Q-Testing. We also investigated the reasons behind such performance qualitatively, and we have identified the key features of Android apps that make Deep RL particularly effective on them to be the presence of chained and blocking activities. Moreover, we have developed FATE to fine-tune the hyperparameters of Deep RL algorithms on simulated apps, since it is computationally expensive to carry it out on real apps. © 2022 Association for Computing Machinery.",Android testing; Deep reinforcement learning,Android (operating system); Black-box testing; Deep learning; Learning systems; Android apps; Android testing; Deep reinforcement learning; Exploration strategies; Learn+; Machine learning techniques; Optimal strategies; Reinforcement learnings; State of the art; State-space; Reinforcement learning
ReCDroid+: Automated End-To-End Crash Reproduction from Bug Reports for Android Apps,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130693930&doi=10.1145%2f3488244&partnerID=40&md5=bc092ecf18e7e37d2c13667438e10cda,"The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers heavily rely on bug reports in issue tracking systems to reproduce failures (e.g., crashes). However, the process of crash reproduction is often manually done by developers, making the resolution of bugs inefficient, especially given that bug reports are often written in natural language. To improve the productivity of developers in resolving bug reports, in this paper, we introduce a novel approach, called ReCDroid+, that can automatically reproduce crashes from bug reports for Android apps. ReCDroid+ uses a combination of natural language processing (NLP), deep learning, and dynamic GUI exploration to synthesize event sequences with the goal of reproducing the reported crash. We have evaluated ReCDroid+ on 66 original bug reports from 37 Android apps. The results show that ReCDroid+ successfully reproduced 42 crashes (63.6% success rate) directly from the textual description of the manually reproduced bug reports. A user study involving 12 participants demonstrates that ReCDroid+ can improve the productivity of developers when resolving crash bug reports.  © 2022 Association for Computing Machinery.",Android GUI testing; Bug report; bug reproducing,Cell proliferation; Deep learning; Graphical user interfaces; Natural language processing systems; Productivity; Program debugging; Android applications; Android apps; Android GUI testing; Bug reports; Bug reproducing; End to end; GUI testing; Issue Tracking; Mobile applications; Tracking system; Android (operating system)
An Empirical Study on Data Distribution-Aware Test Selection for Deep Learning Enhancement,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128902819&doi=10.1145%2f3511598&partnerID=40&md5=7c6e65e122a8e898179d9094aa0c6ac8,"Similar to traditional software that is constantly under evolution, deep neural networks need to evolve upon the rapid growth of test data for continuous enhancement (e.g., adapting to distribution shift in a new environment for deployment). However, it is labor intensive to manually label all of the collected test data. Test selection solves this problem by strategically choosing a small set to label. Via retraining with the selected set, deep neural networks will achieve competitive accuracy. Unfortunately, existing selection metrics involve three main limitations: (1) using different retraining processes, (2) ignoring data distribution shifts, and (3) being insufficiently evaluated. To fill this gap, we first conduct a systemically empirical study to reveal the impact of the retraining process and data distribution on model enhancement. Then based on our findings, we propose DAT, a novel distribution-aware test selection metric. Experimental results reveal that retraining using both the training and selected data outperforms using only the selected data. None of the selection metrics perform the best under various data distributions. By contrast, DAT effectively alleviates the impact of distribution shifts and outperforms the compared metrics by up to five times and 30.09% accuracy improvement for model enhancement on simulated and in-the-wild distribution shift scenarios, respectively. © 2022 Copyright held by the owner/author(s).",data distribution; Deep learning testing; test selection,Software testing; Testing; Data distribution; Deep learning testing; Empirical studies; Labour-intensive; Learning enhancements; Process distribution; Rapid growth; Shift-and; Test data; Test selection; Deep neural networks
An Empirical Study of the Impact of Hyperparameter Tuning and Model Optimization on the Performance Properties of Deep Neural Networks,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130696263&doi=10.1145%2f3506695&partnerID=40&md5=d5681b4747343bb5d42042fa30f1f330,"Deep neural network (DNN) models typically have many hyperparameters that can be configured to achieve optimal performance on a particular dataset. Practitioners usually tune the hyperparameters of their DNN models by training a number of trial models with different configurations of the hyperparameters, to find the optimal hyperparameter configuration that maximizes the training accuracy or minimizes the training loss. As such hyperparameter tuning usually focuses on the model accuracy or the loss function, it is not clear and remains under-explored how the process impacts other performance properties of DNN models, such as inference latency and model size. On the other hand, standard DNN models are often large in size and computing-intensive, prohibiting them from being directly deployed in resource-bounded environments such as mobile devices and Internet of Things (IoT) devices. To tackle this problem, various model optimization techniques (e.g., pruning or quantization) are proposed to make DNN models smaller and less computing-intensive so that they are better suited for resource-bounded environments. However, it is neither clear how the model optimization techniques impact other performance properties of DNN models such as inference latency and battery consumption, nor how the model optimization techniques impact the effect of hyperparameter tuning (i.e., the compounding effect). Therefore, in this paper, we perform a comprehensive study on four representative and widely-Adopted DNN models, i.e., CNN image classification, Resnet-50, CNN text classification, and LSTM sentiment classification, to investigate how different DNN model hyperparameters affect the standard DNN models, as well as how the hyperparameter tuning combined with model optimization affect the optimized DNN models, in terms of various performance properties (e.g., inference latency or battery consumption). Our empirical results indicate that tuning specific hyperparameters has heterogeneous impact on the performance of DNN models across different models and different performance properties. In particular, although the top tuned DNN models usually have very similar accuracy, they may have significantly different performance in terms of other aspects (e.g., inference latency). We also observe that model optimization has a confounding effect on the impact of hyperparameters on DNN model performance. For example, two sets of hyperparameters may result in standard models with similar performance but their performance may become significantly different after they are optimized and deployed on the mobile device. Our findings highlight that practitioners can benefit from paying attention to a variety of performance properties and the confounding effect of model optimization when tuning and optimizing their DNN models.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep neural network; DNN model optimization; DNN model performance; hyperparameter tuning,Classification (of information); Electric batteries; Internet of things; Long short-term memory; Text processing; Deep neural network model optimization; Deep neural network model performance; Hyper-parameter; Hyperparameter tuning; Model optimization; Network model performance; Neural network model; Performance; Performance properties; Deep neural networks
Opinion Mining for Software Development: A Systematic Literature Review,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130717954&doi=10.1145%2f3490388&partnerID=40&md5=c0b890c0f7bed7a7d3d0a005c86f1d40,"Opinion mining, sometimes referred to as sentiment analysis, has gained increasing attention in software engineering (SE) studies. SE researchers have applied opinion mining techniques in various contexts, such as identifying developers' emotions expressed in code comments and extracting users' critics toward mobile apps. Given the large amount of relevant studies available, it can take considerable time for researchers and developers to figure out which approaches they can adopt in their own studies and what perils these approaches entail.We conducted a systematic literature review involving 185 papers. More specifically, we present (1) well-defined categories of opinion mining-related software development activities, (2) available opinion mining approaches, whether they are evaluated when adopted in other studies, and how their performance is compared, (3) available datasets for performance evaluation and tool customization, and (4) concerns or limitations SE researchers might need to take into account when applying/customizing these opinion mining techniques. The results of our study serve as references to choose suitable opinion mining tools for software development activities and provide critical insights for the further development of opinion mining techniques in the SE domain.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Opinion mining; sentiment analysis; software engineering,Data mining; Petroleum reservoir evaluation; Software design; Customisation; Further development; Large amounts; Mobile app; Opinion mining; Performance; Performance tools; Performances evaluation; Sentiment analysis; Systematic literature review; Sentiment analysis
Turnover of Companies in OpenStack: Prevalence and Rationale,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139087833&doi=10.1145%2f3510849&partnerID=40&md5=d1409f0f2692440698f01cbf712027a6,"To achieve commercial goals, companies have made substantial contributions to large open-source software (OSS) ecosystems such as OpenStack and have become the main contributors. However, they often withdraw their employees for a variety of reasons, which may affect the sustainability of OSS projects. While the turnover of individual contributors has been extensively investigated, there is a lack of knowledge about the nature of companies' withdrawal. To this end, we conduct a mixed-methods empirical study on OpenStack to reveal how common company withdrawals were, to what degree withdrawn companies made contributions, and what the rationale behind withdrawals was. By analyzing the commit data of 18 versions of OpenStack, we find that the number of companies that have left is increasing and even surpasses the number of companies that have joined in later versions. Approximately 12% of the companies in each version have exited by the next version. Compared to the sustaining companies that joined in the same version, the withdrawn companies tend to have a weaker contribution intensity but contribute to a similar scope of repositories in OpenStack. Through conducting a developer survey, we find four aspects of reasons for companies' withdrawal from OpenStack: company, community, developer, and project. The most common reasons lie in the company aspect, i.e., the company either achieved its goals or failed to do so. By fitting the survival analysis model, we find that commercial goals are associated with the probability of the company's withdrawal, and that a company's contribution intensity and scale are positively correlated with its retention. Maintaining good retention is important but challenging for OSS ecosystems, and our results may shed light on potential approaches to improve company retention and reduce the negative impact of company withdrawal. © 2022 Copyright held by the owner/author(s).",commercial participation; company withdrawal; open source ecosystem; Software development; survival analysis,Bioinformatics; Open source software; Open systems; Platform as a Service (PaaS); Software design; Commercial participation; Company withdrawal; Empirical studies; Mixed method; Open source ecosystem; Open source software projects; Open-source; Open-source softwares; Software ecosystems; Survival analysis; Ecosystems
Time-Travel Investigation: Toward Building a Scalable Attack Detection Framework on Ethereum,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130705437&doi=10.1145%2f3505263&partnerID=40&md5=96b5134cd489b7a440678985cabbaa20,"Ethereum has been attracting lots of attacks, hence there is a pressing need to perform timely investigation and detect more attack instances. However, existing systems suffer from the scalability issue due to the following reasons. First, the tight coupling between malicious contract detection and blockchain data importing makes them infeasible to repeatedly detect different attacks. Second, the coarse-grained archive data makes them inefficient to replay transactions. Third, the separation between malicious contract detection and runtime state recovery consumes lots of storage.In this article, we propose a scalable attack detection framework named EthScope, which overcomes the scalability issue by neatly re-organizing the Ethereum state and efficiently locating suspicious transactions. It leverages the fine-grained state to support the replay of arbitrary transactions and proposes a well-designed schema to optimize the storage consumption. The performance evaluation shows that EthScope can solve the scalability issue, i.e., efficiently performing a large-scale analysis on billions of transactions, and a speedup of around when replaying transactions. It also has lower storage consumption compared with existing systems. Further analysis shows that EthScope can help analysts understand attack behaviors and detect more attack instances.  © 2022 Association for Computing Machinery.",attack detection; Ethereum; vulnerability,Scalability; Attack detection; Block-chain; Detection framework; Different attacks; Existing systems; Pressung; Scalability issue; Tight coupling; Time travel; Vulnerability; Digital storage
BiRD: Race Detection in Software Binaries under Relaxed Memory Models,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139891252&doi=10.1145%2f3498538&partnerID=40&md5=b221d89418ded6049a91946c9831f8e0,"Instruction reordering and interleavings in program execution under relaxed memory semantics result in non-intuitive behaviors, making it difficult to provide assurances about program correctness. Studies have shown that up to 90% of the concurrency bugs reported by state-of-the-art static analyzers are false alarms. As a result, filtering false alarms and detecting real concurrency bugs is a challenging problem. Unsurprisingly, this problem has attracted the interest of the research community over the past few decades. Nonetheless, many of the existing techniques rely on analyzing source code, rarely consider the effects introduced by compilers, and assume a sequentially consistent memory model. In a practical setting, however, developers often do not have access to the source code, and even commodity architectures such as x86 and ARM are not sequentially consistent.In this work, we present Bird, a prototype tool, to dynamically detect harmful data races in x86 binaries under relaxed memory models, TSO and PSO. Bird employs source-DPOR to explore all distinct feasible interleavings for a multithreaded application. Our evaluation of Bird on 42 publicly available benchmarks and its comparison with the state-of-the-art tools indicate Bird's potential in effectively detecting data races in software binaries. © 2022 Association for Computing Machinery.",dynamic race detection; Relaxed memory models; software binaries; TSO and PSO,Codes (symbols); Errors; Semantics; Concurrency bugs; Dynamic race detection; Falsealarms; Interleavings; Race detection; Relaxed memory models; Software binary; Source codes; State of the art; TSO and PSO; Birds
Applying Bayesian Analysis Guidelines to Empirical Software Engineering Data: The Case of Programming Languages and Code Quality,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130710474&doi=10.1145%2f3490953&partnerID=40&md5=f51af049efd96e1b1de161be45f7c130,"Statistical analysis is the tool of choice to turn data into information and then information into empirical knowledge. However, the process that goes from data to knowledge is long, uncertain, and riddled with pitfalls. To be valid, it should be supported by detailed, rigorous guidelines that help ferret out issues with the data or model and lead to qualified results that strike a reasonable balance between generality and practical relevance. Such guidelines are being developed by statisticians to support the latest techniques for Bayesian data analysis. In this article, we frame these guidelines in a way that is apt to empirical research in software engineering.To demonstrate the guidelines in practice, we apply them to reanalyze a GitHub dataset about code quality in different programming languages. The dataset's original analysis [Ray et al. 55] and a critical reanalysis [Berger et al. 6] have attracted considerable attention-in no small part because they target a topic (the impact of different programming languages) on which strong opinions abound. The goals of our reanalysis are largely orthogonal to this previous work, as we are concerned with demonstrating, on data in an interesting domain, how to build a principled Bayesian data analysis and to showcase its benefits. In the process, we will also shed light on some critical aspects of the analyzed data and of the relationship between programming languages and code quality-such as the impact of project-specific characteristics other than the used programming language.The high-level conclusions of our exercise will be that Bayesian statistical techniques can be applied to analyze software engineering data in a way that is principled, flexible, and leads to convincing results that inform the state-of-The-Art while highlighting the boundaries of its validity. The guidelines can support building solid statistical analyses and connecting their results. Thus, they can help buttress continued progress in empirical software engineering research.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bayesian data analysis; empirical software engineering; guidelines; programming languages; statistical analysis,Codes (symbols); Data handling; High level languages; Information analysis; Software engineering; Statistics; Analysis guidelines; Bayesian Analysis; Bayesian data analysis; Code quality; Empirical knowledge; Empirical Software Engineering; Guideline; Programming language; Reanalysis; Software engineering data; Quality control
Using Personality Detection Tools for Software Engineering Research: How Far Can We Go,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130738696&doi=10.1145%2f3491039&partnerID=40&md5=040994406417413f82fe3efaab32f2bd,"Assessing the personality of software engineers may help to match individual traits with the characteristics of development activities such as code review and testing, as well as support managers in team composition. However, self-Assessment questionnaires are not a practical solution for collecting multiple observations on a large scale. Instead, automatic personality detection, while overcoming these limitations, is based on off-The-shelf solutions trained on non-Technical corpora, which might not be readily applicable to technical domains like software engineering. In this article, we first assess the performance of general-purpose personality detection tools when applied to a technical corpus of developers' e-mails retrieved from the public archives of the Apache Software Foundation. We observe a general low accuracy of predictions and an overall disagreement among the tools. Second, we replicate two previous research studies in software engineering by replacing the personality detection tool used to infer developers' personalities from pull-request discussions and e-mails. We observe that the original results are not confirmed, i.e., changing the tool used in the original study leads to diverging conclusions. Our results suggest a need for personality detection tools specially targeted for the software engineering domain.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",automatic personality recognition; Big Five; Computational personality detection; Five-Factor Model; IBM personality insights; LIWC; negative results; replication,Electronic mail; Human resource management; Software testing; Well testing; Automatic personality recognition; Big five; Computational personality detection; Five-Factor Model; IBM personality insight; LIWC; Negative result; Personality detections; Personality recognition; Replication; Surveys
Context-and Fairness-Aware In-Process Crowdworker Recommendation,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130682205&doi=10.1145%2f3487571&partnerID=40&md5=24c28b573faab1626d25d89feb28de3b,"Identifying and optimizing open participation is essential to the success of open software development. Existing studies highlighted the importance of worker recommendation for crowdtesting tasks in order to improve bug detection efficiency, i.e., detect more bugs with fewer workers. However, there are a couple of limitations in existing work. First, these studies mainly focus on one-Time recommendations based on expertise matching at the beginning of a new task. Second, the recommendation results suffer from severe popularity bias, i.e., highly experienced workers are recommended in almost all the tasks, while less experienced workers rarely get recommended. This article argues the need for context-and fairness-Aware in-process crowdworker recommendation in order to address these limitations. We motivate this study through a pilot study, revealing the prevalence of long-sized non-yielding windows, i.e., no new bugs are revealed in consecutive test reports during the process of a crowdtesting task. This indicates the potential opportunity for accelerating crowdtesting by recommending appropriate workers in a dynamic manner, so that the non-yielding windows could be shortened. Besides, motivated by the popularity bias in existing crowdworker recommendation approach, this study also aims at alleviating the unfairness in recommendations.Driven by these observations, this article proposes a context-and fairness-Aware in-process crowdworker recommendation approach, iRec2.0, to detect more bugs earlier, shorten the non-yielding windows, and alleviate the unfairness in recommendations. It consists of three main components: (1) the modeling of dynamic testing context, (2) the learning-based ranking component, and (3) the multi-objective optimization-based re-ranking component. The evaluation is conducted on 636 crowdtesting tasks from one of the largest crowdtesting platforms, and results show the potential of iRec2.0 in improving the cost-effectiveness of crowdtesting by saving the cost, shortening the testing process, and alleviating the unfairness among workers. In detail, iRec2.0 could shorten the non-yielding window by a median of 50%-66% in different application scenarios, and consequently have potential of saving testing cost by a median of 8%-12%. Meanwhile, the recommendation frequency of the crowdworker drop from 34%-60% to 5%-26% under different scenarios, indicating its potential in alleviating the unfairness among crowdworkers.  © 2022 Association for Computing Machinery.",Crowdsourced testing; fair recommendation; multi-objective optimization; worker recommendation,Cost effectiveness; Crowdsourcing; Efficiency; Software design; Bug detection; Crowdsourced testing; Detection efficiency; Fair recommendation; In-process; Matchings; Multi-objectives optimization; Pilot studies; Worker recommendation; Workers'; Multiobjective optimization
"All in One: Design, Verification, and Implementation of SNOW-optimal Read Atomic Transactions",2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130689998&doi=10.1145%2f3494517&partnerID=40&md5=8a5320b3d624c8915aac0fde18a48cd0,"Distributed read atomic transactions are important building blocks of modern cloud databases that magnificently bridge the gap between data availability and strong data consistency. The performance of their transactional reads is particularly critical to the overall system performance, as many real-world database workloads are dominated by reads. Following the SNOW design principle for optimal reads, we develop LORA, a novel SNOW-optimal algorithm for distributed read atomic transactions. LORA completes its reads in exactly one round trip, even in the presence of conflicting writes, without imposing additional overhead to the communication, and it outperforms the state-of-The-Art read atomic algorithms.To guide LORA's development, we present a rewriting-logic-based framework and toolkit for design, verification, implementation, and evaluation of distributed databases. Within the framework, we formalize LORA and mathematically prove its data consistency guarantees. We also apply automatic model checking and statistical verification to validate our proofs and to estimate LORA's performance. We additionally generate from the formal model a correct-by-construction distributed implementation for testing and performance evaluation under realistic deployments. Our design-level and implementation-based experimental results are consistent, which together demonstrate LORA's promising data consistency and performance achievement.  © 2022 Association for Computing Machinery.",data consistency; Model checking; statistical verification; the SNOW theorem,Atoms; Snow; Atomic transaction; Building blockes; Cloud database; Data consistency; Design implementation; Design verification; Models checking; Performance; Statistical verification; The SNOW theorem; Model checking
On the Faults Found in REST APIs by Automated Test Generation,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130688588&doi=10.1145%2f3491038&partnerID=40&md5=2e9af2e9b85872e84920878c08e52b41,"RESTful web services are often used for building a wide variety of enterprise applications. The diversity and increased number of applications using RESTful APIs means that increasing amounts of resources are spent developing and testing these systems. Automation in test data generation provides a useful way of generating test data in a fast and efficient manner. However, automated test generation often results in large test suites that are hard to evaluate and investigate manually.This article proposes a taxonomy of the faults we have found using search-based software testing techniques applied on RESTful APIs. The taxonomy is a first step in understanding, analyzing, and ultimately fixing software faults in web services and enterprise applications. We propose to apply a density-based clustering algorithm to the test cases evolved during the search to allow a better separation between different groups of faults. This is needed to enable engineers to highlight and focus on the most serious faults.Tests were automatically generated for a set of eight case studies, seven open-source and one industrial. The test cases generated during the search are clustered based on the reported last executed line and based on the error messages returned, when such error messages were available. The tests were manually evaluated to determine their root causes and to obtain additional information.The article presents a taxonomy of the faults found based on the manual analysis of 415 faults in the eight case studies and proposes a method to support the classification using clustering of the resulting test cases.  © 2022 Association for Computing Machinery.",automated Software Testing; automated Test Generation; evolutionary Algorithms; Search-based software testing,Application programs; Automation; Clustering algorithms; Genetic algorithms; Open source software; Software testing; Testing; Web services; Websites; Automated software testing; Automated test generations; Case-studies; Enterprise applications; Error messages; RESTful Web services; Search-based software testing; Test case; Test data generation; Test data in; Taxonomies
Industry-Academia Research Collaboration and Knowledge Co-creation: Patterns and Anti-patterns,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130686289&doi=10.1145%2f3494519&partnerID=40&md5=5cd8ff083e58e17a8c9f472b23b4143d,"Increasing the impact of software engineering research in the software industry and the society at large has long been a concern of high priority for the software engineering community. The problem of two cultures, research conducted in a vacuum (disconnected from the real world), or misaligned time horizons are just some of the many complex challenges standing in the way of successful industry-Academia collaborations. This article reports on the experience of research collaboration and knowledge co-creation between industry and academia in software engineering as a way to bridge the research-practice collaboration gap. Our experience spans 14 years of collaboration between researchers in software engineering and the European and Norwegian software and IT industry. Using the participant observation and interview methods, we have collected and afterwards analyzed an extensive record of qualitative data. Drawing upon the findings made and the experience gained, we provide a set of 14 patterns and 14 anti-patterns for industry-Academia collaborations, aimed to support other researchers and practitioners in establishing and running research collaboration projects in software engineering.  © 2022 Copyright held by the owner/author(s).",anti-patterns; collaboration gap; collaboration model; Industry-Academia collaboration; knowledge transfer; patterns; research co-creation; research collaboration; software engineering; technology transfer,Bridges; Software engineering; Technology transfer; Anti-patterns; Co-creation; Collaboration gap; Collaboration models; Industry-academia collaboration; Knowledge transfer; Pattern; Research co-creation; Research collaborations; Software industry; Knowledge management
Continuous and Proactive Software Architecture Evaluation: An IoT Case,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130680776&doi=10.1145%2f3492762&partnerID=40&md5=d66f5e08e5cd8c78054cfbfe1b44f258,"Design-Time evaluation is essential to build the initial software architecture to be deployed. However, experts' assumptions made at design-Time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts' design-Time decisions can be challenged at run-Time. A continuous architecture evaluation that systematically assesses and intertwines design-Time and run-Time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Continuous evaluation; IoT; software architecture evaluation; time series forecasting,Cost effectiveness; Machine learning; Software architecture; Architecture evaluation; Continuous evaluation; Design time; Future performance; Performance; Pro-active approach; Runtimes; Software architecture evaluation; Time series forecasting; Uncertainty; Internet of things
OSS Effort Estimation Using Software Features Similarity and Developer Activity-Based Metrics,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130703591&doi=10.1145%2f3485819&partnerID=40&md5=97df93cb23a9c992d35160023fa7ac2d,"Software development effort estimation (SDEE) generally involves leveraging the information about the effort spent in developing similar software in the past. Most organizations do not have access to sufficient and reliable forms of such data from past projects. As such, the existing SDEE methods suffer from low usage and accuracy.We propose an efficient SDEE method for open source software, which provides accurate and fast effort estimates. The significant contributions of our article are (i) novel SDEE software metrics derived from developer activity information of various software repositories, (ii) an SDEE dataset comprising the SDEE metrics' values derived from approximately 13,000 GitHub repositories from 150 different software categories, and (iii) an effort estimation tool based on SDEE metrics and a software description similarity model. Our software description similarity model is basically a machine learning model trained using the PVA on the software product descriptions of GitHub repositories. Given the software description of a newly envisioned software, our tool yields an effort estimate for developing it.Our method achieves the highest standardized accuracy score of 87.26% (with Cliff's δ= 0.88 at 99.999% confidence level) and 42.7% with the automatically transformed linear baseline model. Our software artifacts are available at https://doi.org/10.5281/zenodo.5095723.  © 2022 Association for Computing Machinery.",developer activity; Effort estimation; software development effort; software maintenance; software planning,Open source software; Open systems; Software design; Activity-based; Developer activities; Effort estimates; Effort Estimation; Effort estimation methods; Estimation Metric; Similarity models; Software description; Software development effort; Software features; Computer software maintenance
Guided Feature Identification and Removal for Resource-constrained Firmware,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130751793&doi=10.1145%2f3487568&partnerID=40&md5=0b5566ac4060bba552abb0b8d8abcf40,"IoT firmware oftentimes incorporates third-party components, such as network-oriented middleware and media encoders/decoders. These components consist of large and mature codebases, shipping with a variety of non-critical features. Feature bloat increases code size, complicates auditing/debugging, and reduces stability. This is problematic for IoT devices, which are severely resource-constrained and must remain operational in the field for years.Unfortunately, identification and complete removal of code related to unwanted features requires familiarity with codebases of interest, cumbersome manual effort, and may introduce bugs. We address these difficulties by introducing PRAT, a system that takes as input the codebase of software of interest, identifies and maps features to code, presents this information to a human analyst, and removes all code belonging to unwanted features. PRAT solves the challenge of identifying feature-related code through a novel form of differential dynamic analysis and visualizes results as user-friendly feature graphs.Evaluation on diverse codebases shows superior code removal compared to both manual feature deactivation and state-of-art debloating tools, and generality across programming languages. Furthermore, a user study comparing PRAT to manual code analysis shows that it can significantly simplify the feature identification workflow.  © 2021 Association for Computing Machinery.",Program analysis; program verification,Codes (symbols); Internet of things; Middleware; Code size; Critical features; Dynamics analysis; Encoder-decoder; Features identification; Identification and removal; Network oriented; Program analysis; Program Verification; Third parties; Firmware
In-IDE Code Generation from Natural Language: Promise and Challenges,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130746008&doi=10.1145%2f3487569&partnerID=40&md5=c3eec9a3c5fea777fbd8d8e771ec9584,"A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning concept into code, especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from natural language queries, but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer-written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. In this article, we perform the first comprehensive investigation of the promise and challenges of using such technology inside the PyCharm IDE, asking, ""At the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?""To facilitate the study, we first develop a plugin for the PyCharm IDE that implements a hybrid of code generation and code retrieval functionality, and we orchestrate virtual environments to enable collection of many user events (e.g., web browsing, keystrokes, fine-grained code edits). We ask developers with various backgrounds to complete 7 varieties of 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Further analysis identifies several pain points that could improve the effectiveness of future machine learning-based code generation/retrieval developer assistants and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies on this topic, as well as development of better code generation models.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code generation; code retrieval; empirical study; Natural language programming assistant,Application programming interfaces (API); Codes (symbols); Engineering education; Integrodifferential equations; Learning algorithms; Machine learning; Natural language processing systems; Productivity; Software design; Virtual reality; Code retrievals; Codegeneration; Empirical studies; Machine learning methods; Natural language programming assistant; Natural language queries; Natural languages; Plug-ins; Retrieval accuracy; Work-flows; Data visualization
How Software Refactoring Impacts Execution Time,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130721100&doi=10.1145%2f3485136&partnerID=40&md5=f9a34a6c34f1e01727e0e5282eebc127,"Refactoring aims at improving the maintainability of source code without modifying its external behavior. Previous works proposed approaches to recommend refactoring solutions to software developers. The generation of the recommended solutions is guided by metrics acting as proxy for maintainability (e.g., number of code smells removed by the recommended solution). These approaches ignore the impact of the recommended refactorings on other non-functional requirements, such as performance, energy consumption, and so forth. Little is known about the impact of refactoring operations on non-functional requirements other than maintainability.We aim to fill this gap by presenting the largest study to date to investigate the impact of refactoring on software performance, in terms of execution time. We mined the change history of 20 systems that defined performance benchmarks in their repositories, with the goal of identifying commits in which developers implemented refactoring operations impacting code components that are exercised by the performance benchmarks. Through a quantitative and qualitative analysis, we show that refactoring operations can significantly impact the execution time. Indeed, none of the investigated refactoring types can be considered ""safe""in ensuring no performance regression. Refactoring types aimed at decomposing complex code entities (e.g., Extract Class/Interface, Extract Method) have higher chances of triggering performance degradation, suggesting their careful consideration when refactoring performance-critical code.  © 2021 Association for Computing Machinery.",execution time; performance; refactoring; Software maintainability,Benchmarking; Codes (symbols); Computer software maintenance; Energy utilization; Code smell; Execution time; External behavior; Non-functional requirements; Performance; Refactorings; Software developer; Software maintainability; Software refactoring; Source codes; Maintainability
A Tale of Two Cities: Software Developers Working from Home during the COVID-19 Pandemic,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126178653&doi=10.1145%2f3487567&partnerID=40&md5=4af73c3bf0ae3e229e839ed92c585acd,"The COVID-19 pandemic has shaken the world to its core and has provoked an overnight exodus of developers who normally worked in an office setting to working from home. The magnitude of this shift and the factors that have accompanied this new unplanned work setting go beyond what the software engineering community has previously understood to be remote work. To find out how developers and their productivity were affected, we distributed two surveys (with a combined total of 3,634 responses that answered all required questions) weeks apart to understand the presence and prevalence of the benefits, challenges, and opportunities to improve this special circumstance of remote work. From our thematic qualitative analysis and statistical quantitative analysis, we find that there is a dichotomy of developer experiences influenced by many different factors (that for some are a benefit, while for others a challenge). For example, a benefit for some was being close to family members but for others having family members share their working space and interrupting their focus, was a challenge. Our surveys led to powerful narratives from respondents and revealed the scale at which these experiences exist to provide insights as to how the future of (pandemic) remote work can evolve.  © 2021 Copyright held by the owner/author(s).",COVID-19; work from home,Software engineering; COVID-19; Engineering community; Qualitative analysis; Shift-and; Software developer; Work from home; Working space; Surveys
Why Do Smart Contracts Self-Destruct? Investigating the Selfdestruct Function on Ethereum,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130710048&doi=10.1145%2f3488245&partnerID=40&md5=bfecfcbf1c8e6522cc763a93063351e6,"The selfdestruct function is provided by Ethereum smart contracts to destroy a contract on the blockchain system. However, it is a double-edged sword for developers. On the one hand, using the selfdestruct function enables developers to remove smart contracts (SCs) from Ethereum and transfers Ethers when emergency situations happen, e.g., being attacked. On the other hand, this function can increase the complexity for the development and open an attack vector for attackers. To better understand the reasons why SC developers include or exclude the selfdestruct function in their contracts, we conducted an online survey to collect feedback from them and summarize the key reasons. Their feedback shows that 66.67% of the developers will deploy an updated contract to the Ethereum after destructing the old contract. According to this information, we propose a method to find the self-destructed contracts (also called predecessor contracts) and their updated version (successor contracts) by computing the code similarity. By analyzing the difference between the predecessor contracts and their successor contracts, we found five reasons that led to the death of the contracts; two of them (i.e., Unmatched ERC20 Token and Limits of Permission) might affect the life span of contracts. We developed a tool named LifeScope to detect these problems. LifeScope reports 0 false positives or negatives in detecting Unmatched ERC20 Token. In terms of Limits of Permission, LifeScope achieves 77.89% of F-measure and 0.8673 of AUC in average. According to the feedback of developers who exclude selfdestruct functions, we propose suggestions to help developers use selfdestruct functions in Ethereum smart contracts better.  © 2021 Association for Computing Machinery.",empirical study; ethereum; selfdestruct function; Smart contract,Ethereum; Attack vector; Block-chain; Code similarities; Emergency situation; Empirical studies; False positive; Lifespans; Online surveys; Self destruct; Selfdestruct function; Smart contract
A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130684004&doi=10.1145%2f3485275&partnerID=40&md5=379dd545a2331d2255302016c7875232,"An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross-cutting area of work, from its modern inception to the present, this article presents a systematic literature review of research at the intersection of SE & DL. The review canvasses work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23 unique SE tasks. We center our analysis around the components of learning, a set of principles that governs the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research and highlights likely areas of fertile exploration for the future.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; literature review; machine learning; neural networks; software engineering,Deep learning; Engineering education; 'current; Automated features; Deep learning; Development tasks; Feature engineerings; Learning techniques; Literature reviews; Neural-networks; Software engineering research; Systematic literature review; Software engineering
Why Do Developers Reject Refactorings in Open-Source Projects?,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130733813&doi=10.1145%2f3487062&partnerID=40&md5=41738a55b3b573e3fe83258b2b6bd82c,"Refactoring operations are behavior-preserving changes aimed at improving source code quality. While refactoring is largely considered a good practice, refactoring proposals in pull requests are often rejected after the code review. Understanding the reasons behind the rejection of refactoring contributions can shed light on how such contributions can be improved, essentially benefiting software quality.This article reports a study in which we manually coded rejection reasons inferred from 330 refactoring-related pull requests from 207 open-source Java projects. We surveyed 267 developers to assess their perceived prevalence of these identified rejection reasons, further complementing the reasons.Our study resulted in a comprehensive taxonomy consisting of 26 refactoring-related rejection reasons and 21 process-related rejection reasons. The taxonomy, accompanied with representative examples and highlighted implications, provides developers with valuable insights on how to ponder and polish their refactoring contributions, and indicates a number of directions researchers can pursue toward better refactoring recommenders.  © 2021 Association for Computing Machinery.",empirical software engineering; Refactoring,Computer software selection and evaluation; Open source software; Open systems; Code review; Empirical Software Engineering; Good practices; Open source projects; Open-source; Refactorings; Source code qualities; Taxonomies
Software Engineering for AI-Based Systems: A Survey,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130727000&doi=10.1145%2f3487043&partnerID=40&md5=fbf7ac61842908f81cd945ae6a1009e4,"AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.  © 2022 Copyright held by the owner/author(s).",AI-based systems; artificial intelligence; Software engineering; systematic mapping study,Application programs; Artificial intelligence; Computer software selection and evaluation; Curricula; Software testing; Speech recognition; AI-based system; Autonomous driving; Classifieds; Learn+; Property; Research areas; Software-systems; State of the art; Synthesised; Systematic mapping studies; Mapping
ConE: A Concurrent Edit Detection Tool for Large-scale Software Development,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130754615&doi=10.1145%2f3478019&partnerID=40&md5=bf5e3f92e813bc232884ae985eb8b416,"Modern, complex software systems are being continuously extended and adjusted. The developers responsible for this may come from different teams or organizations, and may be distributed over the world. This may make it difficult to keep track of what other developers are doing, which may result in multiple developers concurrently editing the same code areas. This, in turn, may lead to hard-to-merge changes or even merge conflicts, logical bugs that are difficult to detect, duplication of work, and wasted developer productivity. To address this, we explore the extent of this problem in the pull-request-based software development model. We study half a year of changes made to six large repositories in Microsoft in which at least 1,000 pull requests are created each month. We find that files concurrently edited in different pull requests are more likely to introduce bugs. Motivated by these findings, we design, implement, and deploy a service named Concurrent Edit Detector (ConE) that proactively detects pull requests containing concurrent edits, to help mitigate the problems caused by them. ConE has been designed to scale, and to minimize false alarms while still flagging relevant concurrently edited files. Key concepts of ConE include the detection of the Extent of Overlap between pull requests, and the identification of Rarely Concurrently Edited Files. To evaluate ConE, we report on its operational deployment on 234 repositories inside Microsoft. ConE assessed 26,000 pull requests and made 775 recommendations about conflicting changes, which were rated as useful in over 70% (554) of the cases. From interviews with 48 users, we learned that they believed ConE would save time in conflict resolution and avoiding duplicate work, and that over 90% intend to keep using the service on a daily basis.  © 2021 Copyright held by the owner/author(s).",distributed software development; merge conflict; pull request; Pull-based software development,Mergers and acquisitions; Complex software systems; Detection tools; Distributed software development; Keep track of; Large-scales; Merge conflict; MicroSoft; Pull request; Pull-based software development; Software development models; Software design
Measuring and Modeling Group Dynamics in Open-Source Software Development: A Tensor Decomposition Approach,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130730485&doi=10.1145%2f3473139&partnerID=40&md5=ced0cefc3790769ac2321cd0f17bdc44,"Many open-source software projects depend on a few core developers, who take over both the bulk of coordination and programming tasks. They are supported by peripheral developers, who contribute either via discussions or programming tasks, often for a limited time. It is unclear what role these peripheral developers play in the programming and communication efforts, as well as the temporary task-related sub-groups in the projects. We mine code-repository data and mailing-list discussions to model the relationships and contributions of developers in a social network and devise a method to analyze the temporal collaboration structures in communication and programming, learning about the strength and stability of social sub-groups in open-source software projects. Our method uses multi-modal social networks on a series of time windows. Previous work has reduced the network structure representing developer collaboration to networks with only one type of interaction, which impedes the simultaneous analysis of more than one type of interaction. We use both communication and version-control data of open-source software projects and model different types of interaction over time. To demonstrate the practicability of our measurement and analysis method, we investigate 10 substantial and popular open-source software projects and show that, if sub-groups evolve, modeling these sub-groups helps predict the future evolution of interaction levels of programmers and groups of developers. Our method allows maintainers and other stakeholders of open-source software projects to assess instabilities and organizational changes in developer interaction and can be applied to different use cases in organizational analysis, such as understanding the dynamics of a specific incident or discussion.  © 2021 by the owner/author(s).",Coordination; group structures; open-source software; repository mining; tensor decomposition,Open systems; Social networking (online); Social sciences computing; Software design; Tensors; Coordination; Group dynamics; Group structure; Model groups; Open source software projects; Open-source software development; Programming tasks; Repository mining; Sub-groups; Tensor decomposition; Open source software
SemMT: A Semantic-Based Testing Approach for Machine Translation Systems,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130742894&doi=10.1145%2f3490488&partnerID=40&md5=dd76ec4fa1b6c0784b251c19b2bc71e4,"Machine translation has wide applications in daily life. In mission-critical applications such as translating official documents, incorrect translation can have unpleasant or sometimes catastrophic consequences. This motivates recent research on the testing methodologies for machine translation systems. Existing methodologies mostly rely on metamorphic relations designed at the textual level (e.g., Levenshtein distance) or syntactic level (e.g., distance between grammar structures) to determine the correctness of translation results. However, these metamorphic relations do not consider whether the original and the translated sentences have the same meaning (i.e., semantic similarity). To address this problem, in this article we propose SemMT, an automatic testing approach for machine translation systems based on semantic similarity checking. SemMT applies round-trip translation and measures the semantic similarity between the original and the translated sentences. Our insight is that the semantics concerning logical relations and quantifiers in sentences can be captured by regular expressions (or deterministic finite automata) where efficient semantic equivalence/similarity checking algorithms can be applied. Leveraging the insight, we propose three semantic similarity metrics and implement them in SemMT. We compared SemMT with related state-of-the-art testing techniques, demonstrating the effectiveness of mistranslation detection. The experiment results show that SemMT outperforms existing metrics, achieving an increase of 34.2% and 15.4% on accuracy and F-score, respectively. We also study the possibility of further enhancing the performance by combining various metrics. Finally, we discuss a solution to locate the suspicious trip in round-trip translation, which provides hints for bug diagnosis.  © 2022 Association for Computing Machinery.",Machine translation; metamorphic testing; semantic equivalent; semantic similarity; testing,Automatic testing; Computational linguistics; Computer aided language translation; Semantics; Catastrophic consequences; Daily lives; Machine translation systems; Metamorphic relations; Metamorphic testing; Mission critical applications; Recent researches; Round trip; Semantic equivalents; Semantic similarity; Machine translation
Editorial: A Retrospective and Prospective Reflection,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130742013&doi=10.1145%2f3523278&partnerID=40&md5=dc059c01e4e3f0856c0822e57799869c,[No abstract available],,
Buddy Stacks: Protecting Return Addresses with Efficient Thread-Local Storage and Runtime Re-Randomization,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130716342&doi=10.1145%2f3494516&partnerID=40&md5=e7cdf18fe70b3e0310fe3ad1aa509da6,"Shadow stacks play an important role in protecting return addresses to mitigate ROP attacks. Parallel shadow stacks, which shadow the call stack of each thread at the same constant offset for all threads, are known not to support multi-threading well. On the other hand, compact shadow stacks must maintain a separate shadow stack pointer in thread-local storage (TLS), which can be implemented in terms of a register or the per-thread Thread-Control-Block (TCB), suffering from poor compatibility in the former or high performance overhead in the latter. In addition, shadow stacks are vulnerable to information disclosure attacks.In this paper, we propose to mitigate ROP attacks for single- and multi-threaded server programs running on general-purpose computing systems by using a novel stack layout, called a buddy stack (referred to as Bustk), that is highly performant, compatible with existing code, and provides meaningful security. These goals are met due to three novel design aspects in Bustk. First, Bustk places a parallel shadow stack just below a thread's call stack (as each other's buddies allocated together), avoiding the need to maintain a separate shadow stack pointer and making it now well-suited for multi-threading. Second, Bustk uses an efficient stack-based thread-local storage mechanism, denoted STK-TLS, to store thread-specific metadata in two TLS sections just below the shadow stack in dual redundancy (as each other's buddies), so that both can be accessed and updated in a lightweight manner from the call stack pointer rsp alone. Finally, Bustk re-randomizes continuously (on the order of milliseconds) the return addresses on the shadow stack by using a new microsecond-level runtime re-randomization technique, denoted STK-MSR. This mechanism aims to obsolete leaked information, making it extremely unlikely for the attacker to hijack return addresses, particularly against a server program that sits often tens of milliseconds away from the attacker.Our evaluation using web servers, Nginx and Apache Httpd, shows that Bustk works well in terms of performance, compatibility, and security provided, with its parallel shadow stacks incurring acceptable memory overhead for real-world applications and its STK-TLS mechanism costing only two pages per thread. In particular, Bustk can protect the Nginx and Apache servers with an adaptive 1-ms re-randomization policy (without observable overheads when IO is intensive, with about 17,000 requests per second). In addition, we have also evaluated Bustk using other non-server applications, Firefox, Python, LLVM, JDK and SPEC CPU2006, to demonstrate further the same degree of performance and compatibility provided, but the protection provided for, say, browsers, is weaker (since network-access delays can no longer be assumed).  © 2022 Association for Computing Machinery.",buddy stack; CFI; ROP; runtime re-randomization; Shadow stack,Computer architecture; Petroleum reservoir evaluation; Python; Seebeck effect; Buddy stack; CFI; Multi-threading; Performance; Randomisation; ROP; Runtime re-randomization; Runtimes; Shadow stack; Stack pointers; Random processes
A Study on Blockchain Architecture Design Decisions and Their Security Attacks and Threats,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130703491&doi=10.1145%2f3502740&partnerID=40&md5=78433ed530bc228d8451ce6f53443a6e,"Blockchain is a disruptive technology intended to implement secure decentralised distributed systems, in which transactional data can be shared, stored, and verified by participants of the system without needing a central authentication/verification authority. Blockchain-based systems have several architectural components and variants, which architects can leverage to build secure software systems. However, there is a lack of studies to assist architects in making architecture design and configuration decisions for blockchain-based systems. This knowledge gap may increase the chance of making unsuitable design decisions and producing configurations prone to potential security risks. To address this limitation, we report our comprehensive systematic literature review to derive a taxonomy of commonly used architecture design decisions in blockchain-based systems. We map each of these decisions to potential security attacks and their posed threats. MITRE's attack tactic categories and Microsoft STRIDE threat modeling are used to systematically classify threats and their associated attacks to identify potential attacks and threats in blockchain-based systems. Our mapping approach aims to guide architects to make justifiable design decisions that will result in more secure implementations.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",architecture decision; Blockchain; design decisions; security threat classification,Design; Network security; Security systems; Architecture decisions; Architecture designs; Block-chain; Decentralized distributed systems; Design decisions; Disruptive technology; Security attacks; Security threat classification; Security threats; Threat classifications; Blockchain
Analyzing Uncertainty in Release Planning: A Method and Experiment for Fixed-Date Release Cycles,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130736612&doi=10.1145%2f3490487&partnerID=40&md5=daec68db4ef0fa69ebb519aa3c85d1ae,"Release planning - deciding what features to implement in upcoming releases of a software system - is a critical activity in iterative software development. Many release planning methods exist, but most ignore the inevitable uncertainty in estimating software development effort and business value. The article's objective is to study whether analyzing uncertainty during release planning generates better release plans than if uncertainty is ignored. To study this question, we have developed a novel release planning method under uncertainty, called BEARS, that models uncertainty using Bayesian probability distributions and recommends release plans that maximize expected net present value and expected punctuality. We then compare release plans recommended by BEARS to those recommended by methods that ignore uncertainty on 32 release planning problems. The experiment shows that BEARS recommends release plans with higher expected net present value and expected punctuality than methods that ignore uncertainty, thereby indicating the harmful effects of ignoring uncertainty during release planning. These results highlight the importance of eliciting and analyzing uncertainty in software effort and value estimations and call for increased research in these areas.  © 2021 by the owner/author(s).",Bayesian analysis; decision analysis; Release planning; search-based software engineering; software economics; value-based software engineering,Decision theory; Economics; Iterative methods; Software design; Uncertainty analysis; Bayesian Analysis; Planning method; Release cycles; Release planning; Search-based; Search-based software engineering; Software economics; The net present value (NPV); Uncertainty; Value-based software engineerings; Probability distributions
Classifying Mobile Applications Using Word Embeddings,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130693676&doi=10.1145%2f3474827&partnerID=40&md5=e7fa0d31fff639caf347ac119bcdb5ce,"Modern application stores enable developers to classify their apps by choosing from a set of generic categories, or genres, such as health, games, and music. These categories are typically static - new categories do not necessarily emerge over time to reflect innovations in the mobile software landscape. With thousands of apps classified under each category, locating apps that match a specific consumer interest can be a challenging task. To overcome this challenge, in this article, we propose an automated approach for classifying mobile apps into more focused categories of functionally related application domains. Our aim is to enhance apps visibility and discoverability. Specifically, we employ word embeddings to generate numeric semantic representations of app descriptions. These representations are then classified to generate more cohesive categories of apps. Our empirical investigation is conducted using a dataset of 600 apps, sampled from the Education, Health&Fitness, and Medical categories of the Apple App Store. The results show that our classification algorithms achieve their best performance when app descriptions are vectorized using GloVe, a count-based model of word embeddings. Our findings are further validated using a dataset of Sharing Economy apps and the results are evaluated by 12 human subjects. The results show that GloVe combined with Support Vector Machines can produce app classifications that are aligned to a large extent with human-generated classifications.  © 2021 Association for Computing Machinery.",App classification; app store; fastText; GloVe; word embeddings; Word2Vec,Application programs; Health; Semantics; Support vector machines; App classification; App stores; Classifieds; Embeddings; Fasttext; Glove; Mobile applications; Modern applications; Word embedding; Word2vec; Embeddings
Feature Matching-based Approaches to Improve the Robustness of Android Visual GUI Testing,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130700818&doi=10.1145%2f3477427&partnerID=40&md5=42a950bb6ed4801737953fee2755a723,"In automated Visual GUI Testing (VGT) for Android devices, the available tools often suffer from low robustness to mobile fragmentation, leading to incorrect results when running the same tests on different devices.To soften these issues, we evaluate two feature matching-based approaches for widget detection in VGT scripts, which use, respectively, the complete full-screen snapshot of the application (Fullscreen) and the cropped images of its widgets (Cropped) as visual locators to match on emulated devices.Our analysis includes validating the portability of different feature-based visual locators over various apps and devices and evaluating their robustness in terms of cross-device portability and correctly executed interactions. We assessed our results through a comparison with two state-of-the-art tools, EyeAutomate and Sikuli.Despite a limited increase in the computational burden, our Fullscreen approach outperformed state-of-the-art tools in terms of correctly identified locators across a wide range of devices and led to a 30% increase in passing tests.Our work shows that VGT tools' dependability can be improved by bridging the testing and computer vision communities. This connection enables the design of algorithms targeted to domain-specific needs and thus inherently more usable and robust.  © 2021 Association for Computing Machinery.",feature matching; Mobile computing; software testing; visual GUI testing,Android (operating system); Graphical user interfaces; Computational burden; Feature-based; Features matching; GUI testing; Mobile-computing; Software testings; State of the art; Testing tools; Two-state; Visual GUI testing; Software testing
What You See is What it Means! Semantic Representation Learning of Code based on Visualization and Transfer Learning,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130697770&doi=10.1145%2f3485135&partnerID=40&md5=52a1303320e21c7950828ae961ad8df7,"Recent successes in training word embeddings for Natural Language Processing (NLP) tasks have encouraged a wave of research on representation learning for source code, which builds on similar NLP methods. The overall objective is then to produce code embeddings that capture the maximum of program semantics. State-of-the-art approaches invariably rely on a syntactic representation (i.e., raw lexical tokens, abstract syntax trees, or intermediate representation tokens) to generate embeddings, which are criticized in the literature as non-robust or non-generalizable. In this work, we investigate a novel embedding approach based on the intuition that source code has visual patterns of semantics. We further use these patterns to address the outstanding challenge of identifying semantic code clones. We propose the WySiWiM (What You See Is What It Means"") approach where visual representations of source code are fed into powerful pre-trained image classification neural networks from the field of computer vision to benefit from the practical advantages of transfer learning. We evaluate the proposed embedding approach on the task of vulnerable code prediction in source code and on two variations of the task of semantic code clone identification: code clone detection (a binary classification problem), and code classification (a multi-classification problem). We show with experiments on the BigCloneBench (Java), Open Judge (C) that although simple, our WySiWiM approach performs as effectively as state-of-the-art approaches such as ASTNN or TBCNN. We also showed with data from NVD and SARD that WySiWiM representation can be used to learn a vulnerable code detector with reasonable performance (accuracy 1/490%). We further explore the influence of different steps in our approach, such as the choice of visual representations or the classification algorithm, to eventually discuss the promises and limitations of this research direction.  © 2021 Copyright held by the owner/author(s).",embedding; representation learning; Semantic clones; visual representation,C (programming language); Classification (of information); Cloning; Codes (symbols); Embeddings; Natural language processing systems; Syntactics; Trees (mathematics); Code clone; Embeddings; Representation learning; Semantic clone; Semantic codes; Semantic representation; Source codes; State-of-the-art approach; Transfer learning; Visual representations; Semantics
A Practical Approach for Dynamic Taint Tracking with Control-flow Relationships,2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130729910&doi=10.1145%2f3485464&partnerID=40&md5=c73c9267146b16fa93d3804ae81f5fbf,"Dynamic taint tracking, a technique that traces relationships between values as a program executes, has been used to support a variety of software engineering tasks. Some taint tracking systems only consider data flows and ignore control flows. As a result, relationships between some values are not reflected by the analysis. Many applications of taint tracking either benefit from or rely on these relationships being traced, but past works have found that tracking control flows resulted in over-tainting, dramatically reducing the precision of the taint tracking system. In this article, we introduce Conflux, alternative semantics for propagating taint tags along control flows. Conflux aims to reduce over-tainting by decreasing the scope of control flows and providing a heuristic for reducing loop-related over-tainting. We created a Java implementation of Conflux and performed a case study exploring the effect of Conflux on a concrete application of taint tracking, automated debugging. In addition to this case study, we evaluated Conflux's accuracy using a novel benchmark consisting of popular, real-world programs. We compared Conflux against existing taint propagation policies, including a state-of-the-art approach for reducing control-flow-related over-tainting, finding that Conflux had the highest F1 score on 43 out of the 48 total tests.  © 2021 by the owner/author(s).",control flow analysis; dynamic information flow; Taint tracking,Program debugging; Tracking (position); Case-studies; Control-flow; Control-flow analysis; Dynamic information; Dynamic information flow; Dynamic taint tracking; Flow relationship; Information flows; Taint tracking; Tracking system; Semantics
"Model Transformation Development Using Automated Requirements Analysis, Metamodel Matching, and Transformation by Example",2022,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130755580&doi=10.1145%2f3471907&partnerID=40&md5=f15e997dedf7462b07afecd215b95b6a,"In this article, we address how the production of model transformations (MT) can be accelerated by automation of transformation synthesis from requirements, examples, and metamodels. We introduce a synthesis process based on metamodel matching, correspondence patterns between metamodels, and completeness and consistency analysis of matches. We describe how the limitations of metamodel matching can be addressed by combining matching with automated requirements analysis and model transformation by example (MTBE) techniques.We show that in practical examples a large percentage of required transformation functionality can usually be constructed automatically, thus potentially reducing development effort. We also evaluate the efficiency of synthesised transformations.Our novel contributions are:The concept of correspondence patterns between metamodels of a transformation.Requirements analysis of transformations using natural language processing (NLP) and machine learning (ML).Symbolic MTBE using ""predictive specification""to infer transformations from examples.Transformation generation in multiple MT languages and in Java, from an abstract intermediate language.  © 2021 Association for Computing Machinery.",automated software engineering; Model transformations; model-driven engineering; requirements engineering,Abstracting; Learning algorithms; Natural language processing systems; Requirements engineering; Software engineering; Automated software engineering; Correspondence patterns; Meta model; Meta-model matching; Meta-model transformations; Model transformation; Model-driven Engineering; Requirement analysis; Requirement engineering; Synthesis process; Automation
A survey of flaky tests,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121796897&doi=10.1145%2f3476105&partnerID=40&md5=072ea8803b2fdcb6353fdc29cbf89fd3,"Tests that fail inconsistently, without changes to the code under test, are described as flaky. Flaky tests do not give a clear indication of the presence of software bugs and thus limit the reliability of the test suites that contain them. A recent survey of software developers found that 59% claimed to deal with flaky tests on a monthly, weekly, or daily basis. As well as being detrimental to developers, flaky tests have also been shown to limit the applicability of useful techniques in software testing research. In general, one can think of flaky tests as being a threat to the validity of any methodology that assumes the outcome of a test only depends on the source code it covers. In this article, we systematically survey the body of literature relevant to flaky test research, amounting to 76 papers. We split our analysis into four parts: addressing the causes of flaky tests, their costs and consequences, detection strategies, and approaches for their mitigation and repair. Our findings and their implications have consequences for how the software-testing community deals with test flakiness, pertinent to practitioners and of interest to those wanting to familiarize themselves with the research area. © 2021 Association for Computing Machinery.",Flaky tests; Software testing,Program debugging; Software reliability; Surveys; Well testing; Flaky test; Research areas; Software bug; Software developer; Software testings; Source codes; Test research; Software testing
When and How to Make Breaking Changes: Policies and Practices in 18 Open Source Software Ecosystems,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112115164&doi=10.1145%2f3447245&partnerID=40&md5=2989fc0c30486baa16c307bd4d9f0120,"Open source software projects often rely on package management systems that help projects discover, incorporate, and maintain dependencies on other packages, maintained by other people. Such systems save a great deal of effort over ad hoc ways of advertising, packaging, and transmitting useful libraries, but coordination among project teams is still needed when one package makes a breaking change affecting other packages. Ecosystems differ in their approaches to breaking changes, and there is no general theory to explain the relationships between features, behavioral norms, ecosystem outcomes, and motivating values. We address this through two empirical studies. In an interview case study, we contrast Eclipse, NPM, and CRAN, demonstrating that these different norms for coordination of breaking changes shift the costs of using and maintaining the software among stakeholders, appropriate to each ecosystem's mission. In a second study, we combine a survey, repository mining, and document analysis to broaden and systematize these observations across 18 ecosystems. We find that all ecosystems share values such as stability and compatibility, but differ in other values. Ecosystems' practices often support their espoused values, but in surprisingly diverse ways. The data provides counterevidence against easy generalizations about why ecosystem communities do what they do.  © 2021 Owner/Author.",collaboration; dependency management; qualitative research; semantic versioning; Software ecosystems,Human resource management; Open source software; Open systems; Document analysis; Empirical studies; General theory; Open source software projects; Package managements; Project team; Repository mining; Ecosystems
Toward a Holistic Approach to Verification and Validation of Autonomous Cognitive Systems,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111445988&doi=10.1145%2f3447246&partnerID=40&md5=afc31c0706dc3595e844e2a54317a113,"When applying formal verification to a system that interacts with the real world, we must use a model of the environment. This model represents an abstraction of the actual environment, so it is necessarily incomplete and hence presents an issue for system verification. If the actual environment matches the model, then the verification is correct; however, if the environment falls outside the abstraction captured by the model, then we cannot guarantee that the system is well behaved. A solution to this problem consists in exploiting the model of the environment used for statically verifying the system's behaviour and, if the verification succeeds, using it also for validating the model against the real environment via runtime verification. The article discusses this approach and demonstrates its feasibility by presenting its implementation on top of a framework integrating the Agent Java PathFinder model checker. A high-level Domain Specific Language is used to model the environment in a user-friendly way; the latter is then compiled to trace expressions for both static formal verification and runtime verification. To evaluate our approach, we apply it to two different case studies: an autonomous cruise control system and a simulation of the Mars Curiosity rover.  © 2021 ACM.",autonomous systems; MCAPL; model checking; Runtime verification; trace expressions,Cognitive systems; Computer aided software engineering; Computer control; Model checking; Problem oriented languages; Speed control; Actual environments; Autonomous cruise control systems; High-level domain; Holistic approach; Real environments; Run-time verification; System verifications; Verification-and-validation; Formal verification
An Empirical Study of the Impact of Data Splitting Decisions on the Performance of AIOps Solutions,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112089100&doi=10.1145%2f3447876&partnerID=40&md5=0d99dffa4e4e92cee8cdb909f824dc31,"AIOps (Artificial Intelligence for IT Operations) leverages machine learning models to help practitioners handle the massive data produced during the operations of large-scale systems. However, due to the nature of the operation data, AIOps modeling faces several data splitting-related challenges, such as imbalanced data, data leakage, and concept drift. In this work, we study the data leakage and concept drift challenges in the context of AIOps and evaluate the impact of different modeling decisions on such challenges. Specifically, we perform a case study on two commonly studied AIOps applications: (1) predicting job failures based on trace data from a large-scale cluster environment and (2) predicting disk failures based on disk monitoring data from a large-scale cloud storage environment. First, we observe that the data leakage issue exists in AIOps solutions. Using a time-based splitting of training and validation datasets can significantly reduce such data leakage, making it more appropriate than using a random splitting in the AIOps context. Second, we show that AIOps solutions suffer from concept drift. Periodically updating AIOps models can help mitigate the impact of such concept drift, while the performance benefit and the modeling cost of increasing the update frequency depend largely on the application data and the used models. Our findings encourage future studies and practices on developing AIOps solutions to pay attention to their data-splitting decisions to handle the data leakage and concept drift challenges.  © 2021 ACM.",AIOps; concept drift; data leakage; failure prediction; machine learning engineering; model maintenance,Artificial intelligence; Large scale systems; Security of data; Application data; Data splitting; Empirical studies; Imbalanced data; Large-scale clusters; Machine learning models; Modeling decisions; Performance benefits; Digital storage
Leveraging Control Flow Knowledge in SMT Solving of Program Verification,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112096657&doi=10.1145%2f3446211&partnerID=40&md5=05e6d926390e3170f79b1911c364d90d,"Satisfiability modulo theories (SMT) solvers have been widely applied as the reasoning engine for diverse software analysis and verification technologies. The efficiency of the SMT solver has significant effects on the performance of these technologies. However, current SMT solvers are designed for the general purpose of constraint solving. Lots of useful knowledge of programs cannot be utilized during SMT solving. As a result, the SMT solver may spend much effort to explore redundant search space. In this article, we propose a novel approach to utilizing control-flow knowledge in SMT solving. With this technique, the search space can be considerably reduced, and the efficiency of SMT solving is observably improved. We conducted extensive experiments on credible benchmarks. The results show significant improvements of our approach.  © 2021 ACM.",control-flow graph; Program verification; satisfiability modulo theory,Efficiency; Verification; Constraint Solving; Control flows; Program Verification; Reasoning engine; Satisfiability modulo Theories; Search spaces; Smt solvers; Software analysis; Knowledge management
Eagle: CFL-Reachability-Based Precision-Preserving Acceleration of Object-Sensitive Pointer Analysis with Partial Context Sensitivity,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112082244&doi=10.1145%2f3450492&partnerID=40&md5=0e5a0452d74e8f6a3a33684319f82acd,"Object sensitivity is widely used as a context abstraction for computing the points-to information context-sensitively for object-oriented programming languages such as Java. Due to the combinatorial explosion of contexts in large object-oriented programs, k-object-sensitive pointer analysis (under k-limiting), denoted k-obj, is often inefficient even when it is scalable for small values of k, where k ĝ 1/2 2 holds typically. A recent popular approach for accelerating k-obj trades precision for efficiency by instructing k-obj to analyze only some methods in a program context-sensitively, determined heuristically by a pre-analysis. In this article, we investigate how to develop a fundamentally different approach, Eagle, for designing a pre-analysis that can make k-obj run significantly faster while maintaining its precision. The novelty of Eagle is to enable k-obj to analyze a method with partial context sensitivity (i.e., context-sensitively for only some of its selected variables/allocation sites) by solving a context-free-language (CFL) reachability problem based on a new CFL-reachability formulation of k-obj. By regularizing one CFL for specifying field accesses and using another CFL for specifying method calls, we have formulated Eagle as a fully context-sensitive taint analysis (without k-limiting) that is both effective (by selecting the variables/allocation sites to be analyzed by k-obj context-insensitively so as to reduce the number of context-sensitive facts inferred by k-obj in the program) and efficient (by running linearly in terms of the number of pointer assignment edges in the program). As Eagle represents the first precision-preserving pre-analysis, our evaluation focuses on demonstrating its significant performance benefits in accelerating k-obj for a set of popular Java benchmarks and applications, with call graph construction, may-fail-casting, and polymorphic call detection as three important client analyses. © 2021 ACM.",CFL-reachability; object sensitivity; Pointer analysis,Benchmarking; Context free languages; Java programming language; Call graph construction; Combinatorial explosion; Context sensitive; Context sensitivity; Information contexts; Object-oriented program; Performance benefits; Reachability problem; Object oriented programming
How Far Have We Progressed in Identifying Self-admitted Technical Debts? A Comprehensive Empirical Study,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112052871&doi=10.1145%2f3447247&partnerID=40&md5=64e9a5d0a33bf2cdbda8593b4b2d2ac4,"Background. Self-admitted technical debt (SATD) is a special kind of technical debt that is intentionally introduced and remarked by code comments. Those technical debts reduce the quality of software and increase the cost of subsequent software maintenance. Therefore, it is necessary to find out and resolve these debts in time. Recently, many automatic approaches have been proposed to identify SATD. Problem. Popular IDEs support a number of predefined task annotation tags for indicating SATD in comments, which have been used in many projects. However, such clear prior knowledge is neglected by existing SATD identification approaches when identifying SATD. Objective. We aim to investigate how far we have really progressed in the field of SATD identification by comparing existing approaches with a simple approach that leverages the predefined task tags to identify SATD. Method. We first propose a simple heuristic approach that fuzzily Matches task Annotation Tags (MAT) in comments to identify SATD. In nature, MAT is an unsupervised approach, which does not need any data to train a prediction model and has a good understandability. Then, we examine the real progress in SATD identification by comparing MAT against existing approaches. Result. The experimental results reveal that: (1) MAT has a similar or even superior performance for SATD identification compared with existing approaches, regardless of whether non-effort-aware or effort-aware evaluation indicators are considered; (2) the SATDs (or non-SATDs) correctly identified by existing approaches are highly overlapped with those identified by MAT; and (3) supervised approaches misclassify many SATDs marked with task tags as non-SATDs, which can be easily corrected by their combinations with MAT. Conclusion. It appears that the problem of SATD identification has been (unintentionally) complicated by our community, i.e., the real progress in SATD comments identification is not being achieved as it might have been envisaged. We hence suggest that, when many task tags are used in the comments of a target project, future SATD identification studies should use MAT as an easy-to-implement baseline to demonstrate the usefulness of any newly proposed approach.  © 2021 ACM.",baseline; code comment; match; Self-admitted technical debt; task annotation tag,Heuristic methods; Automatic approaches; Empirical studies; Evaluation indicators; Heuristic approach; Identification approach; Quality of softwares; Understandability; Unsupervised approaches; Predictive analytics
Software Architectural Migration: An Automated Planning Approach,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112052902&doi=10.1145%2f3461011&partnerID=40&md5=d598b9e928704db5a346d4497f276e20,"Software architectural designs are usually changed over time to support emerging technologies and to adhere to new principles. Architectural migration is an important activity that helps to transform the architectural styles applied during a system's design with the result of modernising the system. If not performed correctly, this process could lead to potential system failures. This article presents an automated approach to refactoring architectural design and to planning the evolution process. With our solution, the architectural design can be refactored, ensuring that system functionality is preserved. Furthermore, the architectural migration process allows the system to be safely and incrementally transformed. We have evaluated our approach with five real-world software applications. The results prove the effectiveness of our approach and identify factors that impact the performance of architectural verification and migration planning. An interesting finding is that planning algorithms generate migration plans that differ in term of their relative efficiency.  © 2021 ACM.",architectural migration; Blockchain; microservice; Software architecture; software modernisation,Application programs; Architectural style; Automated planning; Emerging technologies; Planning algorithms; Relative efficiency; Software applications; Software architectural; System functionality; Architectural design
The Agile Success Model: A Mixed-methods Study of a Large-scale Agile Transformation,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112042701&doi=10.1145%2f3464938&partnerID=40&md5=fdec5ca2953c54a22124b402eb891b57,"Organizations are increasingly adopting Agile frameworks for their internal software development. Cost reduction, rapid deployment, requirements and mental model alignment are typical reasons for an Agile transformation. This article presents an in-depth field study of a large-scale Agile transformation in a mission-critical environment, where stakeholders' commitment was a critical success factor. The goal of such a transformation was to implement mission-oriented features, reducing costs and time to operate in critical scenarios. The project lasted several years and involved over 40 professionals. We report how a hierarchical and plan-driven organization exploited Agile methods to develop a Command & Control (C2) system. Accordingly, we first abstract our experience, inducing a success model of general use for other comparable organizations by performing a post-mortem study. The goal of the inductive research process was to identify critical success factors and their relations. Finally, we validated and generalized our model through Partial Least Squares - Structural Equation Modelling, surveying 200 software engineers involved in similar projects. We conclude the article with data-driven recommendations concerning the management of Agile projects.  © 2021 ACM.",Agile; ethnography; grounded theory; mixed methods research; multivariate analysis; partial least squares; structural equation modeling,Enterprise resource planning; Least squares approximations; Software design; Agile transformations; Critical success factor; Mission-critical environments; Mission-oriented; Partial least square (PLS); Post mortem studies; Rapid deployments; Structural equation modelling; Cost reduction
Diversifying Focused Testing for Unit Testing,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112081320&doi=10.1145%2f3447265&partnerID=40&md5=402001c564e76cc21560601175706987,"Software changes constantly, because developers add new features or modifications. This directly affects the effectiveness of the test suite associated with that software, especially when these new modifications are in a specific area that no test case covers. This article tackles the problem of generating a high-quality test suite to cover repeatedly a given point in a program, with the ultimate goal of exposing faults possibly affecting the given program point. Both search-based software testing and constraint solving offer ready, but low-quality, solutions to this: Ideally, a maximally diverse covering test set is required, whereas search and constraint solving tend to generate test sets with biased distributions. Our approach, Diversified Focused Testing (DFT), uses a search strategy inspired by GödelTest. We artificially inject parameters into the code branching conditions and use a bi-objective search algorithm to find diverse inputs by perturbing the injected parameters, while keeping the path conditions still satisfiable. Our results demonstrate that our technique, DFT, is able to cover a desired point in the code at least 90% of the time. Moreover, adding diversity improves the bug detection and the mutation killing abilities of the test suites. We show that DFT achieves better results than focused testing, symbolic execution, and random testing by achieving from 3% to 70% improvement in mutation score and up to 100% improvement in fault detection across 105 software subjects.  © 2021 ACM.",DFT; diversity; focused testing; GödelTest; Testing,Design for testability; Fault detection; Logic programming; Testing; Constraint Solving; High Quality Test; Random testing; Search Algorithms; Search strategies; Search-based software testing; Software change; Symbolic execution; Software testing
Evaluation of Software Architectures under Uncertainty: A Systematic Literature Review,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112056578&doi=10.1145%2f3464305&partnerID=40&md5=d8848c23197dc458877c480ad2da715d,"Context: Evaluating software architectures in uncertain environments raises new challenges, which require continuous approaches. We define continuous evaluation as multiple evaluations of the software architecture that begins at the early stages of the development and is periodically and repeatedly performed throughout the lifetime of the software system. Numerous approaches have been developed for continuous evaluation; to handle dynamics and uncertainties at run-time, over the past years, these approaches are still very few, limited, and lack maturity. Objective: This review surveys efforts on architecture evaluation and provides a unified terminology and perspective on the subject. Method: We conducted a systematic literature review to identify and analyse architecture evaluation approaches for uncertainty including continuous and non-continuous, covering work published between 1990-2020. We examined each approach and provided a classification framework for this field. We present an analysis of the results and provide insights regarding open challenges. Major results and conclusions: The survey reveals that most of the existing architecture evaluation approaches typically lack an explicit linkage between design-time and run-time. Additionally, there is a general lack of systematic approaches on how continuous architecture evaluation can be realised or conducted. To remedy this lack, we present a set of necessary requirements for continuous evaluation and describe some examples.  © 2021 ACM.",Continuous software architecture evaluation; design-time software architecture evaluation; run-time software architecture evaluation; uncertainty,Surveys; Architecture evaluation; Classification framework; Continuous approach; Evaluating software architectures; Existing architectures; Software systems; Systematic literature review; Uncertain environments; Uncertainty analysis
Understanding Software-2.0: A Study of Machine Learning Library Usage and Evolution,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112100545&doi=10.1145%2f3453478&partnerID=40&md5=280818c136419ff6529d7eff1f603bbf,"Enabled by a rich ecosystem of Machine Learning (ML) libraries, programming using learned models, i.e., Software-2.0, has gained substantial adoption. However, we do not know what challenges developers encounter when they use ML libraries. With this knowledge gap, researchers miss opportunities to contribute to new research directions, tool builders do not invest resources where automation is most needed, library designers cannot make informed decisions when releasing ML library versions, and developers fail to use common practices when using ML libraries. We present the first large-scale quantitative and qualitative empirical study to shed light on how developers in Software-2.0 use ML libraries, and how this evolution affects their code. Particularly, using static analysis we perform a longitudinal study of 3,340 top-rated open-source projects with 46,110 contributors. To further understand the challenges of ML library evolution, we survey 109 developers who introduce and evolve ML libraries. Using this rich dataset we reveal several novel findings. Among others, we found an increasing trend of using ML libraries: The ratio of new Python projects that use ML libraries increased from 2% in 2013 to 50% in 2018. We identify several usage patterns including the following: (i) 36% of the projects use multiple ML libraries to implement various stages of the ML workflows, (ii) developers update ML libraries more often than the traditional libraries, (iii) strict upgrades are the most popular for ML libraries among other update kinds, (iv) ML library updates often result in cascading library updates, and (v) ML libraries are often downgraded (22.04% of cases). We also observed unique challenges when evolving and maintaining Software-2.0 such as (i) binary incompatibility of trained ML models and (ii) benchmarking ML models. Finally, we present actionable implications of our findings for researchers, tool builders, developers, educators, library vendors, and hardware vendors.  © 2021 ACM.",empirical studies; Machine learning libraries; Software-2.0,Libraries; Machine learning; Open source software; Static analysis; Empirical studies; Informed decision; Knowledge gaps; Longitudinal study; Open source projects; Usage patterns; Work-flows; Digital libraries
Context-aware Retrieval-based Deep Commit Message Generation,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112049240&doi=10.1145%2f3464689&partnerID=40&md5=5d423e98459a4d20a8521c757b965263,"Commit messages recorded in version control systems contain valuable information for software development, maintenance, and comprehension. Unfortunately, developers often commit code with empty or poor quality commit messages. To address this issue, several studies have proposed approaches to generate commit messages from commit diffs. Recent studies make use of neural machine translation algorithms to try and translate git diffs into commit messages and have achieved some promising results. However, these learning-based methods tend to generate high-frequency words but ignore low-frequency ones. In addition, they suffer from exposure bias issues, which leads to a gap between training phase and testing phase. In this article, we propose CoRec to address the above two limitations. Specifically, we first train a context-aware encoder-decoder model that randomly selects the previous output of the decoder or the embedding vector of a ground truth word as context to make the model gradually aware of previous alignment choices. Given a diff for testing, the trained model is reused to retrieve the most similar diff from the training set. Finally, we use the retrieval diff to guide the probability distribution for the final generated vocabulary. Our method combines the advantages of both information retrieval and neural machine translation. We evaluate CoRec on a dataset from Liu et al. and a large-scale dataset crawled from 10K popular Java repositories in Github. Our experimental results show that CoRec significantly outperforms the state-of-the-art method NNGen by 19% on average in terms of BLEU.  © 2021 ACM.",Commit message generation; information retrieval; neural machine translation,Computational linguistics; Computer aided language translation; Decoding; Large dataset; Probability distributions; Encoder-decoder; High-frequency words; Large-scale dataset; Learning-based methods; Machine translations; State-of-the-art methods; Training phase; Version control system; Software design
Recommending Faulty Configurations for Interacting Systems under Test Using Multi-objective Search,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112035253&doi=10.1145%2f3464939&partnerID=40&md5=09d7106ab1ce339ee5fa9b82745e92e7,"Modern systems, such as cyber-physical systems, often consist of multiple products within/across product lines communicating with each other through information networks. Consequently, their runtime behaviors are influenced by product configurations and networks. Such systems play a vital role in our daily life; thus, ensuring their correctness by thorough testing becomes essential. However, testing these systems is particularly challenging due to a large number of possible configurations and limited available resources. Therefore, it is important and practically useful to test these systems with specific configurations under which products will most likely fail to communicate with each other. Motivated by this, we present a search-based configuration recommendation (SBCR) approach to recommend faulty configurations for the system under test (SUT) based on cross-product line (CPL) rules. CPL rules are soft constraints, constraining product configurations while indicating the most probable system states with a certain degree of confidence. In SBCR, we defined four search objectives based on CPL rules and combined them with six commonly applied search algorithms. To evaluate SBCR (i.e., SBCRNSGA-II, SBCRIBEA, SBCRMoCell, SBCRSPEA2, SBCRPAES, and SBCRSMPSO), we performed two case studies (Cisco and Jitsi) and conducted difference analyses. Results show that for both of the case studies, SBCR significantly outperformed random search-based configuration recommendation (RBCR) for 86% of the total comparisons based on six quality indicators, and 100% of the total comparisons based on the percentage of faulty configurations (PFC). Among the six variants of SBCR, SBCRSPEA2 outperformed the others in 85% of the total comparisons based on six quality indicators and 100% of the total comparisons based on PFC.  © 2021 ACM.",configuration recommendation; interacting products; mined rules; multi-objective search; Product line; testing,Information services; Degree of confidence; Information networks; Interacting system; Product configuration; Quality indicators; Runtime behaviors; Search Algorithms; System under test; Embedded systems
Specifying with Interface and Trait Abstractions in Abstract State Machines: A Controlled Experiment,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112090460&doi=10.1145%2f3450968&partnerID=40&md5=c95c6507ec41cd9a5fcf7dd038b2a0a3,"State Machine (ASM) theory is a well-known state-based formal method. As in other state-based formal methods, the proposed specification languages for ASMs still lack easy-to-comprehend abstractions to express structural and behavioral aspects of specifications. Our goal is to investigate object-oriented abstractions such as interfaces and traits for ASM-based specification languages. We report on a controlled experiment with 98 participants to study the specification efficiency and effectiveness in which participants needed to comprehend an informal specification as problem (stimulus) in form of a textual description and express a corresponding solution in form of a textual ASM specification using either interface or trait syntax extensions. The study was carried out with a completely randomized design and one alternative (interface or trait) per experimental group. The results indicate that specification effectiveness of the traits experiment group shows a better performance compared to the interfaces experiment group, but specification efficiency shows no statistically significant differences. To the best of our knowledge, this is the first empirical study studying the specification effectiveness and efficiency of object-oriented abstractions in the context of formal methods.  © 2021 ACM.",abstract state machines; CASM; controlled experiment; effectiveness; efficiency; Empirical software engineering; interfaces; language constructs; specification; traits,Efficiency; Formal specification; Specification languages; Abstract state machines; Completely randomized designs; Controlled experiment; Corresponding solutions; Effectiveness and efficiencies; Experimental groups; Statistically significant difference; Textual description; Interface states
Speeding up Data Manipulation Tasks with Alternative Implementations: An Exploratory Study,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112068274&doi=10.1145%2f3456873&partnerID=40&md5=0d154fea1043ae84536e0700d56b2224,"As data volume and complexity grow at an unprecedented rate, the performance of data manipulation programs is becoming a major concern for developers. In this article, we study how alternative API choices could improve data manipulation performance while preserving task-specific input/output equivalence. We propose a lightweight approach that leverages the comparative structures in Q&A sites to extracting alternative implementations. On a large dataset of Stack Overflow posts, our approach extracts 5,080 pairs of alternative implementations that invoke different data manipulation APIs to solve the same tasks, with an accuracy of 86%. Experiments show that for 15% of the extracted pairs, the faster implementation achieved >10x speedup over its slower alternative. We also characterize 68 recurring alternative API pairs from the extraction results to understand the type of APIs that can be used alternatively. To put these findings into practice, we implement a tool, AlterApi7, to automatically optimize real-world data manipulation programs. In the 1,267 optimization attempts on the Kaggle dataset, 76% achieved desirable performance improvements with up to orders-of-magnitude speedup. Finally, we discuss notable challenges of using alternative APIs for optimizing data manipulation programs. We hope that our study offers a new perspective on API recommendation and automatic performance optimization.  © 2021 ACM.",API selection; data manipulation; empirical study; mining software repository; performance optimization,Computer software; Software engineering; Data manipulations; Data volume; Exploratory studies; Faster implementation; Input/output equivalence; Orders of magnitude; Performance optimizations; Stack overflow; Large dataset
Automatically Identifying the Quality of Developer Chats for Post Hoc Use,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112088835&doi=10.1145%2f3450503&partnerID=40&md5=24f4ca01cd2ac37c615b26db3108ed7d,"Software engineers are crowdsourcing answers to their everyday challenges on Q&A forums (e.g., Stack Overflow) and more recently in public chat communities such as Slack, IRC, and Gitter. Many software-related chat conversations contain valuable expert knowledge that is useful for both mining to improve programming support tools and for readers who did not participate in the original chat conversations. However, most chat platforms and communities do not contain built-in quality indicators (e.g., accepted answers, vote counts). Therefore, it is difficult to identify conversations that contain useful information for mining or reading, i.e., conversations of post hoc quality. In this article, we investigate automatically detecting developer conversations of post hoc quality from public chat channels. We first describe an analysis of 400 developer conversations that indicate potential characteristics of post hoc quality, followed by a machine learning-based approach for automatically identifying conversations of post hoc quality. Our evaluation of 2,000 annotated Slack conversations in four programming communities (python, clojure, elm, and racket) indicates that our approach can achieve precision of 0.82, recall of 0.90, F-measure of 0.86, and MCC of 0.57. To our knowledge, this is the first automated technique for detecting developer conversations of post hoc quality.  © 2021 ACM.",Online software developer chats; quality of social content,Computer programming; Turing machines; Automated techniques; Clojure; Expert knowledge; F measure; Programming community; Programming support; Quality indicators; Stack overflow; Quality control
On the Impact of Sample Duplication in Machine-Learning-Based Android Malware Detection,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103045173&doi=10.1145%2f3446905&partnerID=40&md5=98c83e83073d0ea6c3d9e52185d1ccc2,"Malware detection at scale in the Android realm is often carried out using machine learning techniques. State-of-the-art approaches such as DREBIN and MaMaDroid are reported to yield high detection rates when assessed against well-known datasets. Unfortunately, such datasets may include a large portion of duplicated samples, which may bias recorded experimental results and insights. In this article, we perform extensive experiments to measure the performance gap that occurs when datasets are de-duplicated. Our experimental results reveal that duplication in published datasets has a limited impact on supervised malware classification models. This observation contrasts with the finding of Allamanis on the general case of machine learning bias for big code. Our experiments, however, show that sample duplication more substantially affects unsupervised learning models (e.g., malware family clustering). Nevertheless, we argue that our fellow researchers and practitioners should always take sample duplication into consideration when performing machine-learning-based (via either supervised or unsupervised learning) Android malware detections, no matter how significant the impact might be.  © 2021 ACM.",android; dataset; Duplication; machine learning; malware detection,Android (operating system); Classification (of information); Large dataset; Learning systems; Malware; Unsupervised learning; Android malware; High detection rate; Machine learning techniques; Malware classifications; Malware detection; Malware families; Performance gaps; State-of-the-art approach; Mobile security
Are Multi-Language Design Smells Fault-Prone? An Empirical Study,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105712915&doi=10.1145%2f3432690&partnerID=40&md5=812682629fd031ca4ebcb6e46da42649,"Nowadays, modern applications are developed using components written in different programming languages and technologies. The cost benefits of reuse and the advantages of each programming language are two main incentives behind the proliferation of such systems. However, as the number of languages increases, so do the challenges related to the development and maintenance of these systems. In such situations, developers may introduce design smells (i.e., anti-patterns and code smells) which are symptoms of poor design and implementation choices. Design smells are defined as poor design and coding choices that can negatively impact the quality of a software program despite satisfying functional requirements. Studies on mono-language systems suggest that the presence of design smells may indicate a higher risk of future bugs and affects code comprehension, thus making systems harder to maintain. However, the impact of multi-language design smells on software quality such as fault-proneness is yet to be investigated. In this article, we present an approach to detect multi-language design smells in the context of JNI systems. We then investigate the prevalence of those design smells and their impacts on fault-proneness. Specifically, we detect 15 design smells in 98 releases of 9 open-source JNI projects. Our results show that the design smells are prevalent in the selected projects and persist throughout the releases of the systems. We observe that, in the analyzed systems, 33.95% of the files involving communications between Java and C/C++ contain occurrences of multi-language design smells. Some kinds of smells are more prevalent than others, e.g., Unused Parameters, Too Much Scattering, and Unused Method Declaration. Our results suggest that files with multi-language design smells can often be more associated with bugs than files without these smells, and that specific smells are more correlated to fault-proneness than others. From analyzing fault-inducing commit messages, we also extracted activities that are more likely to introduce bugs in smelly files. We believe that our findings are important for practitioners as it can help them prioritize design smells during the maintenance of multi-language systems.  © 2021 ACM.",anti-patterns; code smells; Design smells; empirical studies; mining software repositories; multi-language systems,Computer software selection and evaluation; Odors; Open source software; Program debugging; Software quality; Code comprehension; Design and implementations; Empirical studies; Fault proneness; Functional requirement; Modern applications; Multi languages; Software program; C++ (programming language)
Taming Reflection: An Essential Step Toward Whole-program Analysis of Android Apps,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105739317&doi=10.1145%2f3440033&partnerID=40&md5=891eb8e5197dd7eb0d2e5e51db63eab3,"Android developers heavily use reflection in their apps for legitimate reasons. However, reflection is also significantly used for hiding malicious actions. Unfortunately, current state-of-the-art static analysis tools for Android are challenged by the presence of reflective calls, which they usually ignore. Thus, the results of their security analysis, e.g., for private data leaks, are incomplete, given the measures taken by malware writers to elude static detection. We propose a new instrumentation-based approach to address this issue in a non-invasive way. Specifically, we introduce to the community a prototype tool called DroidRA, which reduces the resolution of reflective calls to a composite constant propagation problem and then leverages the COAL solver to infer the values of reflection targets. After that, it automatically instruments the app to replace reflective calls with their corresponding Java calls in a traditional paradigm. Our approach augments an app so that it can be more effectively statically analyzable, including by such static analyzers that are not reflection-aware. We evaluate DroidRA on benchmark apps as well as on real-world apps, and we demonstrate that it can indeed infer the target values of reflective calls and subsequently allow state-of-the-art tools to provide more sound and complete analysis results.  © 2021 ACM.",Android; DroidRA; reflection; static analysis,Android (operating system); Malware; Static analysis; Constant propagation; Non-invasive way; Security analysis; Sound and complete; State of the art; Static analyzers; Static detections; Whole-program analysis; Mobile security
Editorial,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105701853&doi=10.1145%2f3450737&partnerID=40&md5=13693f4e5d5950fd6f9b709a011700af,[No abstract available],,
An Adaptive Search Budget Allocation Approach for Search-Based Test Case Generation,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105719732&doi=10.1145%2f3446199&partnerID=40&md5=faf7f2e3323927ba2c8051c7b35ec20d,"Search-based techniques have been successfully used to automate test case generation. Such approaches allocate a fixed search budget to generate test cases aiming at maximizing code coverage. The search budget plays a crucial role; due to the hugeness of the search space, the higher the assigned budget, the higher the expected coverage. Code components have different structural properties that may affect the ability of search-based techniques to achieve a high coverage level. Thus, allocating a fixed search budget for all the components is not recommended and a component-specific search budget should be preferred. However, deciding the budget to assign to a given component is not a trivial task. In this article, we introduce Budget Optimization for Testing (BOT), an approach to adaptively allocate the search budget to the classes under test. BOT requires information about the branch coverage that will be achieved on each class with a given search budget. Therefore, we also introduce BRANCHOS, an approach that predicts coverage in a budget-aware way. The results of our experiments show that (i) BRANCHOS can approximate the branch coverage in time with a low error, and (ii) BOT can significantly increase the coverage achieved by a test generation tool and the effectiveness of generated tests.  © 2021 ACM.",Search budget allocation; test case generation,Testing; Adaptive search; Branch coverage; Budget allocation; Code components; Code coverage; Search spaces; Test case generation; Test generations; Budget control
A Hybrid Approach to Formal Verification of Higher-Order Masked Arithmetic Programs,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105754255&doi=10.1145%2f3428015&partnerID=40&md5=6701db77e537f4cf52b96f0c4db7369d,"Side-channel attacks, which are capable of breaking secrecy via side-channel information, pose a growing threat to the implementation of cryptographic algorithms. Masking is an effective countermeasure against side-channel attacks by removing the statistical dependence between secrecy and power consumption via randomization. However, designing efficient and effective masked implementations turns out to be an error-prone task. Current techniques for verifying whether masked programs are secure are limited in their applicability and accuracy, especially when they are applied. To bridge this gap, in this article, we first propose a sound type system, equipped with an efficient type inference algorithm, for verifying masked arithmetic programs against higher-order attacks. We then give novel model-counting-based and pattern-matching-based methods that are able to precisely determine whether the potential leaky observable sets detected by the type system are genuine or simply spurious. We evaluate our approach on various implementations of arithmetic cryptographic programs. The experiments confirm that our approach outperforms the state-of-the-art baselines in terms of applicability, accuracy, and efficiency.  © 2021 ACM.",cryptographic programs; Formal verification; higher-order masking; side-channel attacks,Formal verification; Inference engines; Pattern matching; Cryptographic algorithms; Error prone tasks; Hybrid approach; Model Counting; Side-channel information; State of the art; Statistical dependence; Type Inference Algorithm; Side channel attack
Toward an Objective Measure of Developers' Cognitive Activities,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105759540&doi=10.1145%2f3434643&partnerID=40&md5=ab4914da5eabd2a98283d19a0b257038,"Understanding how developers carry out different computer science activities with objective measures can help to improve productivity and guide the use and development of supporting tools in software engineering. In this article, we present two controlled experiments involving 112 students to explore multiple computing activities (code comprehension, code review, and data structure manipulations) using three different objective measures including neuroimaging (functional near-infrared spectroscopy (fNIRS) and functional magnetic resonance imaging (fMRI)) and eye tracking. By examining code review and prose review using fMRI, we find that the neural representations of programming languages vs. natural languages are distinct. We can classify which task a participant is undertaking based solely on brain activity, and those task distinctions are modulated by expertise. We leverage insights from the psychological notion of spatial ability to decode the neural representations of several fundamental data structures and their manipulations using fMRI, fNIRS, and eye tracking. We examine list, array, tree, and mental rotation tasks and find that data structure and spatial operations use the same focal regions of the brain but to different degrees: they are related but distinct neural tasks. We demonstrate best practices and describe the implication and tradeoffs between fMRI, fNIRS, eye tracking, and self-reporting for software engineering research.  © 2021 ACM.",code comprehension; data structures; eye tracking; Neuroimaging; prose review,Brain; Codes (symbols); Data structures; Eye tracking; Infrared devices; Magnetic resonance imaging; Near infrared spectroscopy; Productivity; Software engineering; Trees (mathematics); Code comprehension; Cognitive activities; Computing activity; Controlled experiment; Data structure manipulation; Functional magnetic resonance imaging; Functional near-infrared spectroscopy (fnirs); Neural representations; Functional neuroimaging
How Should i Improve the UI of My App?: A Study of User Reviews of Popular Apps in the Google Play,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105749854&doi=10.1145%2f3447808&partnerID=40&md5=5a5c27ad29b54bd48587b13d8c9e8e46,"UI (User Interface) is an essential factor influencing users' perception of an app. However, it is hard for even professional designers to determine if the UI is good or not for end-users. Users' feedback (e.g., user reviews in the Google Play) provides a way for app owners to understand how the users perceive the UI. In this article, we conduct an in-depth empirical study to analyze the UI issues of mobile apps. In particular, we analyze more than 3M UI-related reviews from 22,199 top free-to-download apps and 9,380 top non-free apps in the Google Play Store. By comparing the rating of UI-related reviews and other reviews of an app, we observe that UI-related reviews have lower ratings than other reviews. By manually analyzing a random sample of 1,447 UI-related reviews with a 95% confidence level and a 5% interval, we identify 17 UI-related issues types that belong to four categories (i.e., ""Appearance,""""Interaction,""""Experience,""and ""Others""). In these issue types, we find ""Generic Review""is the most occurring one. ""Comparative Review""and ""Advertisement""are the most negative two UI issue types. Faced with these UI issues, we explore the patterns of interaction between app owners and users. We identify eight patterns of how app owners dialogue with users about UI issues by the review-response mechanism. We find ""Apology or Appreciation""and ""Information Request""are the most two frequent patterns. We find updating UI timely according to feedback is essential to satisfy users. Besides, app owners could also fix UI issues without updating UI, especially for issue types belonging to ""Interaction""category. Our findings show that there exists a positive impact if app owners could actively interact with users to improve UI quality and boost users' satisfactoriness about the UIs.  © 2021 ACM.",Mobile app reviews; the Google Play Store; user interface,Computer software; Software engineering; Confidence levels; Empirical studies; Google plays; Information request; Professional designers; Random sample; Response mechanisms; Users' perception; User interfaces
Automatic API Usage Scenario Documentation from Technical Q&A Sites,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105705200&doi=10.1145%2f3439769&partnerID=40&md5=6d954767e952c73c6ecf28b4474e6ee8,"The online technical Q&A site Stack Overflow (SO) is popular among developers to support their coding and diverse development needs. To address shortcomings in API official documentation resources, several research works have thus focused on augmenting official API documentation with insights (e.g., code examples) from SO. The techniques propose to add code examples/insights about APIs into its official documentation. Recently, surveys of software developers find that developers in SO consider the combination of code examples and reviews about APIs as a form of API documentation, and that they consider such a combination to be more useful than official API documentation when the official resources can be incomplete, ambiguous, incorrect, and outdated. Reviews are opinionated sentences with positive/negative sentiments. However, we are aware of no previous research that attempts to automatically produce API documentation from SO by considering both API code examples and reviews. In this article, we present two novel algorithms that can be used to automatically produce API documentation from SO by combining code examples and reviews towards those examples. The first algorithm is called statistical documentation, which shows the distribution of positivity and negativity around the code examples of an API using different metrics (e.g., star ratings). The second algorithm is called concept-based documentation, which clusters similar and conceptually relevant usage scenarios. An API usage scenario contains a code example, a textual description of the underlying task addressed by the code example, and the reviews (i.e., opinions with positive and negative sentiments) from other developers towards the code example. We deployed the algorithms in Opiner, a web-based platform to aggregate information about APIs from online forums. We evaluated the algorithms by mining all Java JSON-based posts in SO and by conducting three user studies based on produced documentation from the posts. The first study is a survey, where we asked the participants to compare our proposed algorithms against a Javadoc-syle documentation format (called as Type-based documentation in Opiner). The participants were asked to compare along four development scenarios (e.g., selection, documentation). The participants preferred our proposed two algorithms over type-based documentation. In our second user study, we asked the participants to complete four coding tasks using Opiner and the API official and informal documentation resources. The participants were more effective and accurate while using Opiner. In a subsequent survey, more than 80% of participants asked the Opiner documentation platform to be integrated into the formal API documentation to complement and improve the API official documentation.  © 2021 ACM.",API; crowd-sourced developer forum; documentation; usage scenario,Application programming interfaces (API); Surveys; Development needs; Development scenarios; Negative sentiments; Positive/negative; Software developer; Textual description; Usage scenarios; Web based platform; Clustering algorithms
Architecting Internet of Things Systems with Blockchain,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105753279&doi=10.1145%2f3442412&partnerID=40&md5=d7eb1b6b8e59a6e00937b06092f1e175,"Blockchain offers a distributed ledger to record data collected from Internet of Thing (IoT) devices as immutable and tamper-proof transactions and securely shared among authorized participants in a Peer-to-Peer (P2P) network. Despite the growing interest in using blockchain for securing IoT systems, there is a general lack of systematic research and comprehensive review of the design issues on the integration of blockchain and IoT from the software architecture perspective. This article presents a catalog of architectural tactics for the design of IoT systems supported by blockchain as a result of a Systematic Literature Review (SLR) on IoT and blockchain to extract the commonly reported quality attributes, design decisions, and relevant architectural tactics for the architectural design of this category of systems. Our findings are threefold:<?brk?> (i) identification of security, scalability, performance, and interoperability as the commonly reported quality attributes; (ii) a catalog of twelve architectural tactics for the design of IoT systems supported by blockchain; and (iii) gaps in research that include tradeoffs among quality attributes and identified tactics. These tactics might provide architects and designers with different options when searching for an optimal architectural design that meets the quality attributes of interest and constraints of a system.  © 2021 ACM.",architectural tactics; blockchain; Internet of things; software requirements,Architectural design; Blockchain; Interoperability; Peer to peer networks; Design decisions; Design issues; Internet of Things (IOT); Peer to Peer (P2P) network; Quality attributes; Systematic literature review (SLR); Systematic research; Tamper proof; Internet of things
Predicting Performance Anomalies in Software Systems at Run-time,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105728506&doi=10.1145%2f3440757&partnerID=40&md5=06cbffcd2ba26f2dc8baee3443e49fb1,"High performance is a critical factor to achieve and maintain the success of a software system. Performance anomalies represent the performance degradation issues (e.g., slowing down in system response times) of software systems at run-time. Performance anomalies can cause a dramatically negative impact on users' satisfaction. Prior studies propose different approaches to detect anomalies by analyzing execution logs and resource utilization metrics after the anomalies have happened. However, the prior detection approaches cannot predict the anomalies ahead of time; such limitation causes an inevitable delay in taking corrective actions to prevent performance anomalies from happening. We propose an approach that can predict performance anomalies in software systems and raise anomaly warnings in advance. Our approach uses a Long-Short Term Memory neural network to capture the normal behaviors of a software system. Then, our approach predicts performance anomalies by identifying the early deviations from the captured normal system behaviors. We conduct extensive experiments to evaluate our approach using two real-world software systems (i.e., Elasticsearch and Hadoop). We compare the performance of our approach with two baselines. The first baseline is one state-to-the-art baseline called Unsupervised Behavior Learning. The second baseline predicts performance anomalies by checking if the resource utilization exceeds pre-defined thresholds. Our results show that our approach can predict various performance anomalies with high precision (i.e., 97-100%) and recall (i.e., 80-100%), while the baselines achieve 25-97% precision and 93-100% recall. For a range of performance anomalies, our approach can achieve sufficient lead times that vary from 20 to 1,403 s (i.e., 23.4 min). We also demonstrate the ability of our approach to predict the performance anomalies that are caused by real-world performance bugs. For predicting performance anomalies that are caused by real-world performance bugs, our approach achieves 95-100% precision and 87-100% recall, while the baselines achieve 49-83% precision and 100% recall. The obtained results show that our approach outperforms the existing anomaly prediction approaches and is able to predict performance anomalies in real-world systems.  © 2021 ACM.",LSTM neural network; Performance anomaly prediction; software systems,Computer software; Anomaly predictions; Corrective actions; Detection approach; Performance anomaly; Performance degradation; Real-world performance; Resource utilizations; Users' satisfactions; Forecasting
Developing Cost-Effective Blockchain-Powered Applications: A Case Study of the Gas Usage of Smart Contract Transactions in the Ethereum Blockchain Platform,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105717964&doi=10.1145%2f3431726&partnerID=40&md5=40ac2326a0424b76bb9bff017fea0eeb,"Ethereum is a blockchain platform that hosts and executes smart contracts. Executing a function of a smart contract burns a certain amount of gas units (a.k.a., gas usage). The total gas usage depends on how much computing power is necessary to carry out the execution of the function. Ethereum follows a free-market policy for deciding the transaction fee for executing a transaction. More specifically, transaction issuers choose how much they are willing to pay for each unit of gas (a.k.a., gas price). The final transaction fee corresponds to the gas price times the gas usage. Miners process transactions to gain mining rewards, which come directly from these transaction fees. The flexibility and the inherent complexity of the gas system pose challenges to the development of blockchain-powered applications. Developers of blockchain-powered applications need to translate requests received in the frontend of their application into one or more smart contract transactions. Yet, it is unclear how developers should set the gas parameters of these transactions given that (i) miners are free to prioritize transactions whichever way they wish and (ii) the gas usage of a contract transaction is only known after the transaction is processed and included in a new block. In this article, we analyze the gas usage of Ethereum transactions that were processed between Oct. 2017 and Feb. 2019 (the Byzantium era). We discover that (i) most miners prioritize transactions based on their gas price only, (ii) 25% of the functions that received at least 10 transactions have an unstable gas usage (coefficient of variation = 19%), and (iii) a simple prediction model that operates on the recent gas usage of a function achieves an R-Squared of 0.76 and a median absolute percentage error of 3.3%. We conclude that (i) blockchain-powered application developers should be aware that transaction prioritization in Ethereum is frequently done based solely on the gas price of transactions (e.g., a higher transaction fee does not necessarily imply a higher transaction priority) and act accordingly and (ii) blockchain-powered application developers can leverage gas usage prediction models similar to ours to make more informed decisions to set the gas price of their transactions. Lastly, based on our findings, we list and discuss promising avenues for future research.  © 2021 ACM.",blockchain; Ethereum; Gas usage; smart contracts,Blockchain; Cost effectiveness; Ethereum; Miners; Predictive analytics; Application developers; Coefficient of variation; Computing power; Gas parameters; Informed decision; Inherent complexity; Percentage error; Prediction model; Gases
IntDroid: Android Malware Detection Based on API Intimacy Analysis,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105741673&doi=10.1145%2f3442588&partnerID=40&md5=414d1b2de80007dd4d4794434dc5d216,"Android, the most popular mobile operating system, has attracted millions of users around the world. Meanwhile, the number of new Android malware instances has grown exponentially in recent years. On the one hand, existing Android malware detection systems have shown that distilling the program semantics into a graph representation and detecting malicious programs by conducting graph matching are able to achieve high accuracy on detecting Android malware. However, these traditional graph-based approaches always perform expensive program analysis and suffer from low scalability on malware detection. On the other hand, because of the high scalability of social network analysis, it has been applied to complete large-scale malware detection. However, the social-network-analysis-based method only considers simple semantic information (i.e., centrality) for achieving market-wide mobile malware scanning, which may limit the detection effectiveness when benign apps show some similar behaviors as malware. In this article, we aim to combine the high accuracy of traditional graph-based method with the high scalability of social-network-analysis-based method for Android malware detection. Instead of using traditional heavyweight static analysis, we treat function call graphs of apps as complex social networks and apply social-network-based centrality analysis to unearth the central nodes within call graphs. After obtaining the central nodes, the average intimacies between sensitive API calls and central nodes are computed to represent the semantic features of the graphs. We implement our approach in a tool called IntDroid and evaluate it on a dataset of 3,988 benign samples and 4,265 malicious samples. Experimental results show that IntDroid is capable of detecting Android malware with an F-measure of 97.1% while maintaining a True-positive Rate of 99.1%. Although the scalability is not as fast as a social-network-analysis-based method (i.e., MalScan), compared to a traditional graph-based method, IntDroid is more than six times faster than MaMaDroid. Moreover, in a corpus of apps collected from GooglePlay market, IntDroid is able to identify 28 zero-day malware that can evade detection of existing tools, one of which has been downloaded and installed by more than ten million users. This app has also been flagged as malware by six anti-virus scanners in VirusTotal, one of which is Symantec Mobile Insight.  © 2021 ACM.",Android malware; API intimacy; centrality; social network,Commerce; Computer viruses; Graph theory; Graphic methods; Palmprint recognition; Scalability; Semantics; Social networking (online); Static analysis; Complex social networks; Function-call graphs; Graph representation; Graph-based methods; High scalabilities; Mobile operating systems; Semantic information; True positive rates; Android (operating system)
Facet-oriented Modelling,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105718956&doi=10.1145%2f3428076&partnerID=40&md5=ca14da0ae47c3a67271d0afcf2dca7ab,"Models are the central assets in model-driven engineering (MDE), as they are actively used in all phases of software development. Models are built using metamodel-based languages, and so objects in models are typed by a metamodel class. This typing is static, established at creation time, and cannot be changed later. Therefore, objects in MDE are closed and fixed with respect to the class they conform to, the fields they have, and the well-formedness constraints they must comply with. This hampers many MDE activities, like the reuse of model-related artefacts such as transformations, the opportunistic or dynamic combination of metamodels, or the dynamic reconfiguration of models. To alleviate this rigidity, we propose making model objects open so that they can acquire or drop so-called facets. These contribute with a type, fields and constraints to the objects holding them. Facets are defined by regular metamodels, hence being a lightweight extension of standard metamodelling. Facet metamodels may declare usage interfaces, as well as laws that govern the assignment of facets to objects (or classes). This article describes our proposal, reporting on a theory, analysis techniques, and an implementation. The benefits of the approach are validated on the basis of five case studies dealing with annotation models, transformation reuse, multi-view modelling, multi-level modelling, and language product lines.  © 2021 ACM.",flexible modelling; MetaDepth; Metamodelling; role-based modelling,Dynamic models; Software design; Analysis techniques; Case-studies; Dynamic combination; Dynamic re-configuration; Meta-modelling; Model-driven Engineering; Multi-view modelling; Product-lines; Modeling languages
DeepWukong: Statically Detecting Software Vulnerabilities Using Deep Graph Neural Network,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105713814&doi=10.1145%2f3436877&partnerID=40&md5=f2f88288533288e48de75486035787b6,"Static bug detection has shown its effectiveness in detecting well-defined memory errors, e.g., memory leaks, buffer overflows, and null dereference. However, modern software systems have a wide variety of vulnerabilities. These vulnerabilities are extremely complicated with sophisticated programming logic, and these bugs are often caused by different bad programming practices, challenging existing bug detection solutions. It is hard and labor-intensive to develop precise and efficient static analysis solutions for different types of vulnerabilities, particularly for those that may not have a clear specification as the traditional well-defined vulnerabilities. This article presents DeepWukong, a new deep-learning-based embedding approach to static detection of software vulnerabilities for C/C++ programs. Our approach makes a new attempt by leveraging advanced recent graph neural networks to embed code fragments in a compact and low-dimensional representation, producing a new code representation that preserves high-level programming logic (in the form of control-and data-flows) together with the natural language information of a program. Our evaluation studies the top 10 most common C/C++ vulnerabilities during the past 3 years. We have conducted our experiments using 105,428 real-world programs by comparing our approach with four well-known traditional static vulnerability detectors and three state-of-the-art deep-learning-based approaches. The experimental results demonstrate the effectiveness of our research and have shed light on the promising direction of combining program analysis with deep learning techniques to address the general static code analysis challenges.  © 2021 ACM.",graph embedding; Static analysis; vulnerabilities,Computer circuits; Data flow analysis; Deep learning; Deep neural networks; Flow graphs; Neural networks; Static analysis; Clear specifications; Graph neural networks; High-level programming; Learning-based approach; Low-dimensional representation; Programming practices; Software vulnerabilities; Static code analysis; C++ (programming language)
A Formal Framework of Software Product Line Analyses,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105736632&doi=10.1145%2f3442389&partnerID=40&md5=bff3aa5e314485a3469283a648f291a2,"A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.  © 2021 Owner/Author.",product-line analysis; Software product lines,Model checking; Analysis strategies; Formal framework; Formal proofs; Functional programs; Lift analysis; Product-line analysis; Product-lines; Software Product Line; Data flow analysis
Why an Android App Is Classified as Malware: Toward Malware Classification Interpretation,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102881672&doi=10.1145%2f3423096&partnerID=40&md5=4048be185ca71064e9f28055c71c382e,"Machine learning-(ML) based approach is considered as one of the most promising techniques for Android malware detection and has achieved high accuracy by leveraging commonly used features. In practice, most of the ML classifications only provide a binary label to mobile users and app security analysts. However, stakeholders are more interested in the reason why apps are classified as malicious in both academia and industry. This belongs to the research area of interpretable ML but in a specific research domain (i.e., mobile malware detection). Although several interpretable ML methods have been exhibited to explain the final classification results in many cutting-edge Artificial Intelligent-based research fields, until now, there is no study interpreting why an app is classified as malware or unveiling the domain-specific challenges. In this article, to fill this gap, we propose a novel and interpretable ML-based approach (named XMal) to classify malware with high accuracy and explain the classification result meanwhile. (1) The first classification phase of XMal hinges multi-layer perceptron and attention mechanism and also pinpoints the key features most related to the classification result. (2) The second interpreting phase aims at automatically producing neural language descriptions to interpret the core malicious behaviors within apps. We evaluate the behavior description results by leveraging a human study and an in-depth quantitative analysis. Moreover, we further compare XMal with the existing interpretable ML-based methods (i.e., Drebin and LIME) to demonstrate the effectiveness of XMal. We find that XMal is able to reveal the malicious behaviors more accurately. Additionally, our experiments show that XMal can also interpret the reason why some samples are misclassified by ML classifiers. Our study peeks into the interpretable ML through the research of Android malware detection and analysis. © 2021 Association for Computing Machinery. All rights reserved.",Android malware; interpretability; interpretable AI; machine learning,Android (operating system); Lime; Malware; Multilayer neural networks; Artificial intelligent; Attention mechanisms; Classification results; Language description; Malicious behavior; Malware classifications; Multi layer perceptron; Research domains; Mobile security
Enabledness-based Testing of Object Protocols,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102859224&doi=10.1145%2f3415153&partnerID=40&md5=6f6a915ff4989c10f8e5733c5af48c29,"A significant proportion of classes in modern software introduce or use object protocols, prescriptions on the temporal orderings of method calls on objects. This article studies search-based test generation techniques that aim to exploit a particular abstraction of object protocols (enabledness preserving abstractions (EPAs)) to find failures. We define coverage criteria over an extension of EPAs that includes abnormal method termination and define a search-based test case generation technique aimed at achieving high coverage. Results suggest that the proposed case generation technique with a fitness function that aims at combined structural and extended EPA coverage can provide better failure-detection capabilities not only for protocol failures but also for general failures when compared to random testing and search-based test generation for standard structural coverage. © 2021 ACM.",Automatic test generation; enabledness-preserving abstractions; genetic algorithms,Computer software; Coverage criteria; Failure detection; Fitness functions; Generation techniques; Random testing; Temporal ordering; Test case generation; Test generations; Software engineering
Killing Stubborn Mutants with Symbolic Execution,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102871082&doi=10.1145%2f3425497&partnerID=40&md5=6348333e77782ab5bff11a14ad5f1640,"We introduce SEMu, a Dynamic Symbolic Execution technique that generates test inputs capable of killing stubborn mutants (killable mutants that remain undetected after a reasonable amount of testing). SEMu aims at mutant propagation (triggering erroneous states to the program output) by incrementally searching for divergent program behaviors between the original and the mutant versions. We model the mutant killing problem as a symbolic execution search within a specific area in the programs' symbolic tree. In this framework, the search area is defined and controlled by parameters that allow scalable and cost-effective mutant killing. We integrate SEMu in KLEE and experimented with Coreutils (a benchmark frequently used in symbolic execution studies). Our results show that our modeling plays an important role in mutant killing. Perhaps more importantly, our results also show that, within a two-hour time limit, SEMu kills 37% of the stubborn mutants, where KLEE kills none and where the mutant infection strategy (strategy suggested by previous research) kills 17%. © 2021 ACM.",Mutants; symbolic execution,Cost effectiveness; Cost effective; Dynamic symbolic executions; Program behavior; Search area; Specific areas; Symbolic execution; Test inputs; Model checking
Leveraging the Defects Life Cycle to Label Affected Versions and Defective Classes,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102863338&doi=10.1145%2f3433928&partnerID=40&md5=7515124db953f04a3fdb161ab1cb7329,"Two recent studies explicitly recommend labeling defective classes in releases using the affected versions (AV) available in issue trackers (e.g., Jira). This practice is coined as the realistic approach. However, no study has investigated whether it is feasible to rely on AVs. For example, how available and consistent is the AV information on existing issue trackers? Additionally, no study has attempted to retrieve AVs when they are unavailable. The aim of our study is threefold: (1) to measure the proportion of defects for which the realistic method is usable, (2) to propose a method for retrieving the AVs of a defect, thus making the realistic approach usable when AVs are unavailable, (3) to compare the accuracy of the proposed method versus three SZZ implementations. The assumption of our proposed method is that defects have a stable life cycle in terms of the proportion of the number of versions affected by the defects before discovering and fixing these defects. Results related to 212 open-source projects from the Apache ecosystem, featuring a total of about 125,000 defects, reveal that the realistic method cannot be used in the majority (51%) of defects. Therefore, it is important to develop automated methods to retrieve AVs. Results related to 76 open-source projects from the Apache ecosystem, featuring a total of about 6,250,000 classes, affected by 60,000 defects, and spread over 4,000 versions and 760,000 commits, reveal that the proportion of the number of versions between defect discovery and fix is pretty stable (standard deviation <2) - across the defects of the same project. Moreover, the proposed method resulted significantly more accurate than all three SZZ implementations in (i) retrieving AVs, (ii) labeling classes as defective, and (iii) in developing defects repositories to perform feature selection. Thus, when the realistic method is unusable, the proposed method is a valid automated alternative to SZZ for retrieving the origin of a defect. Finally, given the low accuracy of SZZ, researchers should consider re-executing the studies that have used SZZ as an oracle and, in general, should prefer selecting projects with a high proportion of available and consistent AVs. © 2021 ACM.",Affected version; defect origin; developing defects repository; SZZ,Ecosystems; Life cycle; A-stable; Automated methods; Defect discovery; Open source projects; Standard deviation; Defects
Why My Code Summarization Model Does Not Work: Code Comment Improvement with Category Prediction,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102903027&doi=10.1145%2f3434280&partnerID=40&md5=b767831931e01eda6243ef02e877bd4e,"Code summarization aims at generating a code comment given a block of source code and it is normally performed by training machine learning algorithms on existing code block-comment pairs. Code comments in practice have different intentions. For example, some code comments might explain how the methods work, while others explain why some methods are written. Previous works have shown that a relationship exists between a code block and the category of a comment associated with it. In this article, we aim to investigate to which extent we can exploit this relationship to improve code summarization performance. We first classify comments into six intention categories and manually label 20,000 code-comment pairs. These categories include ""what,""""why,""""how-to-use,""""how-it-is-done,""""property,""and ""others.""Based on this dataset, we conduct an experiment to investigate the performance of different state-of-the-art code summarization approaches on the categories. We find that the performance of different code summarization approaches varies substantially across the categories. Moreover, the category for which a code summarization model performs the best is different for the different models. In particular, no models perform the best for ""why""and ""property""comments among the six categories. We design a composite approach to demonstrate that comment category prediction can boost code summarization to reach better results. The approach leverages classified code-category labeled data to train a classifier to infer categories. Then it selects the most suitable models for inferred categories and outputs the composite results. Our composite approach outperforms other approaches that do not consider comment categories and obtains a relative improvement of 8.57% and 16.34% in terms of ROUGE-L and BLEU-4 score, respectively. © 2021 ACM.",code comment; Code summarization; comment classification,Learning algorithms; Code blocks; Source codes; State of the art; Summarization models; Training machines; Machine learning
History-based Model Repair Recommendations,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102895577&doi=10.1145%2f3419017&partnerID=40&md5=cc0a13f163461203d5638bef6720a56f,"Models in Model-driven Engineering are primary development artifacts that are heavily edited in all stages of software development and that can become temporarily inconsistent during editing. In general, there are many alternatives to resolve an inconsistency, and which one is the most suitable depends on a variety of factors. As also proposed by recent approaches to model repair, it is reasonable to leave the actual choice and approval of a repair alternative to the discretion of the developer. Model repair tools can support developers by proposing a list of the most promising repairs. Such repair recommendations will be only accepted in practice if the generated proposals are plausible and understandable, and if the set as a whole is manageable. Current approaches, which mostly focus on exhaustive search strategies, exploring all possible model repairs without considering the intention of historic changes, fail in meeting these requirements. In this article, we present a new approach to generate repair proposals that aims at inconsistencies that have been introduced by past incomplete edit steps that can be located in the version history of a model. Such an incomplete edit step is either undone or it is extended to a full execution of a consistency-preserving edit operation. The history-based analysis of inconsistencies as well as the generation of repair recommendations are fully automated, and all interactive selection steps are supported by our repair tool called REVISION. We evaluate our approach using histories of real-world models obtained from popular open-source modeling projects hosted in the Eclipse Git repository, including the evolution of the entire UML meta-model. Our experimental results confirm our hypothesis that most of the inconsistencies, namely, 93.4, can be resolved by complementing incomplete edits. 92.6% of the generated repair proposals are relevant in the sense that their effect can be observed in the models' histories. 94.9% of the relevant repair proposals are ranked at the topmost position. © 2021 ACM.",consistency; history analysis; Model repair; recommendations,Open source software; Fully automated; Historic changes; History-based modeling; Model-driven Engineering; New approaches; Open-source model; Repair tools; Search strategies; Software design
Test Selection for Deep Learning Systems,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102902047&doi=10.1145%2f3417330&partnerID=40&md5=6f10481bae14fcc39709cfbb05b82ea0,"Testing of deep learning models is challenging due to the excessive number and complexity of the computations involved. As a result, test data selection is performed manually and in an ad hoc way. This raises the question of how we can automatically select candidate data to test deep learning models. Recent research has focused on defining metrics to measure the thoroughness of a test suite and to rely on such metrics to guide the generation of new tests. However, the problem of selecting/prioritising test inputs (e.g., to be labelled manually by humans) remains open. In this article, we perform an in-depth empirical comparison of a set of test selection metrics based on the notion of model uncertainty (model confidence on specific inputs). Intuitively, the more uncertain we are about a candidate sample, the more likely it is that this sample triggers a misclassification. Similarly, we hypothesise that the samples for which we are the most uncertain are the most informative and should be used in priority to improve the model by retraining. We evaluate these metrics on five models and three widely used image classification problems involving real and artificial (adversarial) data produced by five generation algorithms. We show that uncertainty-based metrics have a strong ability to identify misclassified inputs, being three times stronger than surprise adequacy and outperforming coverage-related metrics. We also show that these metrics lead to faster improvement in classification accuracy during retraining: up to two times faster than random selection and other state-of-the-art metrics on all models we considered. © 2021 ACM.",Deep learning testing; software engineering; software testing,Learning systems; Testing; Uncertainty analysis; Classification accuracy; Empirical - comparisons; Generation algorithm; Misclassifications; Model uncertainties; Random selection; Recent researches; State of the art; Deep learning
Beyond Tests: Program Vulnerability Repair via Crash Constraint Extraction,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102851834&doi=10.1145%2f3418461&partnerID=40&md5=f23b792c1844aef61bd7c5929a1e335f,"Automated program repair is an emerging technology that seeks to automatically rectify program errors and vulnerabilities. Repair techniques are driven by a correctness criterion that is often in the form of a test suite. Such test-based repair may produce overfitting patches, where the patches produced fail on tests outside the test suite driving the repair. In this work, we present a repair method that fixes program vulnerabilities without the need for a voluminous test suite. Given a vulnerability as evidenced by an exploit, the technique extracts a constraint representing the vulnerability with the help of sanitizers. The extracted constraint serves as a proof obligation that our synthesized patch should satisfy. The proof obligation is met by propagating the extracted constraint to locations that are deemed to be ""suitable""fix locations. An implementation of our approach (ExtractFix) on top of the KLEE symbolic execution engine shows its efficacy in fixing a wide range of vulnerabilities taken from the ManyBugs benchmark, real-world CVEs and Google's OSS-Fuzz framework. We believe that our work presents a way forward for the overfitting problem in program repair by generalizing observable hazards/vulnerabilities (as constraint) from a single failing test or exploit. © 2021 ACM.",Automated program repair; constraint extraction and propagation; overfitting,Testing; Constraint extraction; Correctness criterion; Emerging technologies; Over fitting problem; Program Vulnerability; Proof obligations; Repair techniques; Symbolic execution; Software testing
Are Comments on Stack Overflow Well Organized for Easy Retrieval by Developers?,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102863919&doi=10.1145%2f3434279&partnerID=40&md5=9ac4164280af6d88cb6433e852071a2f,"Many Stack Overflow answers have associated informative comments that can strengthen them and assist developers. A prior study found that comments can provide additional information to point out issues in their associated answer, such as the obsolescence of an answer. By showing more informative comments (e.g., the ones with higher scores) and hiding less informative ones, developers can more effectively retrieve information from the comments that are associated with an answer. Currently, Stack Overflow prioritizes the display of comments, and, as a result, 4.4 million comments (possibly including informative comments) are hidden by default from developers. In this study, we investigate whether this mechanism effectively organizes informative comments. We find that (1) the current comment organization mechanism does not work well due to the large amount of tie-scored comments (e.g., 87% of the comments have 0-score) and (2) in 97.3% of answers with hidden comments, at least one comment that is possibly informative is hidden while another comment with the same score is shown (i.e., unfairly hidden comments). The longest unfairly hidden comment is more likely to be informative than the shortest one. Our findings highlight that Stack Overflow should consider adjusting the comment organization mechanism to help developers effectively retrieve informative comments. Furthermore, we build a classifier that can effectively distinguish informative comments from uninformative comments. We also evaluate two alternative comment organization mechanisms (i.e., the Length mechanism and the Random mechanism) based on text similarity and the prediction of our classifier. © 2021 ACM.",commenting; crowdsourced knowledge sharing; Empirical software engineering; Q8A website; stack overflow,Obsolescence; Large amounts; Random mechanisms; Stack overflow; Text similarity; Taxonomies
Test Data Generation for Path Coverage of MPI Programs Using SAEO,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102871828&doi=10.1145%2f3423132&partnerID=40&md5=d7e9c751f48bfaa369cf8a55a2960241,"Message-passing interface (MPI) programs, a typical kind of parallel programs, have been commonly used in various applications. However, it generally takes exhaustive computation to run these programs when generating test data to test them. In this article, we propose a method of test data generation for path coverage of MPI programs using surrogate-assisted evolutionary optimization, which can efficiently generate test data with high quality. We first divide a sample set of a program into a number of clusters according to the multi-mode characteristic of the coverage problem, with each cluster training a surrogate model. Then, we estimate the fitness of each individual using one or more surrogate models when generating test data through evolving a population. Finally, a small number of representative individuals are selected to execute the program, with the purpose of obtaining their real fitness, to guide the subsequent evolution of the population. We apply the proposed method to seven benchmark MPI programs and compare it with several state-of-the-art approaches. The experimental results show that the proposed method can generate test data with reduced computation, thus improving the testing efficiency. © 2021 ACM.",evolutionary optimization; MPI program; path coverage; surrogate model; test data generation,Message passing; Optimization; Population statistics; Testing; Coverage problem; Evolutionary optimizations; Message passing interface; Number of clusters; Parallel program; State-of-the-art approach; Test data generation; Testing efficiency; Application programs
Interpreting Deep Learning-based Vulnerability Detector Predictions Based on Heuristic Searching,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102875804&doi=10.1145%2f3429444&partnerID=40&md5=cda88485b7efdce571aeacd20d4734a3,"Detecting software vulnerabilities is an important problem and a recent development in tackling the problem is the use of deep learning models to detect software vulnerabilities. While effective, it is hard to explain why a deep learning model predicts a piece of code as vulnerable or not because of the black-box nature of deep learning models. Indeed, the interpretability of deep learning models is a daunting open problem. In this article, we make a significant step toward tackling the interpretability of deep learning model in vulnerability detection. Specifically, we introduce a high-fidelity explanation framework, which aims to identify a small number of tokens that make significant contributions to a detector's prediction with respect to an example. Systematic experiments show that the framework indeed has a higher fidelity than existing methods, especially when features are not independent of each other (which often occurs in the real world). In particular, the framework can produce some vulnerability rules that can be understood by domain experts for accepting a detector's outputs (i.e., true positives) or rejecting a detector's outputs (i.e., false-positives and false-negatives). We also discuss limitations of the present study, which indicate interesting open problems for future research. © 2021 ACM.",deep learning; Explainable AI; sensitivity analysis; vulnerability detection,Heuristic algorithms; Knowledge acquisition; Learning systems; Domain experts; False negatives; False positive; Interpretability; Learning models; Software vulnerabilities; Systematic experiment; Vulnerability detection; Deep learning
"An Empirical Study on Type Annotations: Accuracy, Speed, and Suggestion Effectiveness",2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102884879&doi=10.1145%2f3439775&partnerID=40&md5=5a6e5fe53df9d0958944e96e07280074,"Type annotations connect variables to domain-specific types. They enable the power of type checking and can detect faults early. In practice, type annotations have a reputation of being burdensome to developers. We lack, however, an empirical understanding of how and why they are burdensome. Hence, we seek to measure the baseline accuracy and speed for developers making type annotations to previously unseen code. We also study the impact of one or more type suggestions. We conduct an empirical study of 97 developers using 20 randomly selected code artifacts from the robotics domain containing physical unit types. We find that subjects select the correct physical type with just 51% accuracy, and a single correct annotation takes about 2 minutes on average. Showing subjects a single suggestion has a strong and significant impact on accuracy both when correct and incorrect, while showing three suggestions retains the significant benefits without the negative effects. We also find that suggestions do not come with a time penalty. We require subjects to explain their annotation choices, and we qualitatively analyze their explanations. We find that identifier names and reasoning about code operations are the primary clues for selecting a type. We also examine two state-of-the-art automated type annotation systems and find opportunities for their improvement. © 2021 ACM.",annotations; automated static analysis; dimensional analysis; physical units; program analysis; robotic systems; software reliability; Type checking,Computer software; Baseline accuracy; Domain specific; Empirical studies; Two-state; Type annotations; Typechecking; Software engineering
Emoji-powered Sentiment and Emotion Detection from Software Developers' Communication Data,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102899601&doi=10.1145%2f3424308&partnerID=40&md5=e34e520c58a37a18dbd2ebe0c168a796,"Sentiment and emotion detection from textual communication records of developers have various application scenarios in software engineering (SE). However, commonly used off-the-shelf sentiment/emotion detection tools cannot obtain reliable results in SE tasks and misunderstanding of technical knowledge is demonstrated to be the main reason. Then researchers start to create labeled SE-related datasets manually and customize SE-specific methods. However, the scarce labeled data can cover only very limited lexicon and expressions. In this article, we employ emojis as an instrument to address this problem. Different from manual labels that are provided by annotators, emojis are self-reported labels provided by the authors themselves to intentionally convey affective states and thus are suitable indications of sentiment and emotion in texts. Since emojis have been widely adopted in online communication, a large amount of emoji-labeled texts can be easily accessed to help tackle the scarcity of the manually labeled data. Specifically, we leverage Tweets and GitHub posts containing emojis to learn representations of SE-related texts through emoji prediction. By predicting emojis containing in each text, texts that tend to surround the same emoji are represented with similar vectors, which transfers the sentiment knowledge contained in emoji usage to the representations of texts. Then we leverage the sentiment-aware representations as well as manually labeled data to learn the final sentiment/emotion classifier via transfer learning. Compared to existing approaches, our approach can achieve significant improvement on representative benchmark datasets, with an average increase of 0.036 and 0.049 in macro-F1 in sentiment and emotion detection, respectively. Further investigations reveal that the large-scale Tweets make a key contribution to the power of our approach. This finding informs future research not to unilaterally pursue the domain-specific resource but try to transform knowledge from the open domain through ubiquitous signals such as emojis. Finally, we present the open challenges of sentiment and emotion detection in SE through a qualitative analysis of texts misclassified by our approach. © 2021 ACM.",Emoji; emotion; sentiment; software engineering,Labeled data; Transfer learning; Application scenario; Benchmark datasets; Communication records; Emotion detection; On-line communication; Qualitative analysis; Reliable results; Software developer; Application programs
Adversarial Specification Mining,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101388218&doi=10.1145%2f3424307&partnerID=40&md5=a7c9f0d6a4bb45fb099da48dbcf44304,"There have been numerous studies on mining temporal specifications from execution traces. These approaches learn finite-state automata (FSA) from execution traces when running tests. To learn accurate specifications of a software system, many tests are required. Existing approaches generalize from a limited number of traces or use simple test generation strategies. Unfortunately, these strategies may not exercise uncommon usage patterns of a software system. To address this problem, we propose a new approach, adversarial specification mining, and develop a prototype, Diversity through Counter-examples (DICE). DICE has two components: DICE-Tester and DICE-Miner. After mining Linear Temporal Logic specifications from an input test suite, DICE-Tester adversarially guides test generation, searching for counterexamples to these specifications to invalidate spurious properties. These counterexamples represent gaps in the diversity of the input test suite. This process produces execution traces of usage patterns that were unrepresented in the input test suite. Next, we propose a new specification inference algorithm, DICE-Miner, to infer FSAs using the traces, guided by the temporal specifications. We find that the inferred specifications are of higher quality than those produced by existing state-of-the-art specification miners. Finally, we use the FSAs in a fuzzer for servers of stateful protocols, increasing its coverage. © 2021 ACM.",fuzzing; search-based test generation; Specification mining,Fuzzy inference; Inference engines; Miners; Software testing; Testing; Counter examples; Linear temporal logic specifications; Software systems; Specification inferences; Specification mining; State of the art; Temporal specification; Test generations; Specifications
Sleads: Scalable and Cost-effective Dynamic Dependence Analysis of Distributed Systems via Reinforcement Learning,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099876671&doi=10.1145%2f3379345&partnerID=40&md5=e4cb4b086ae2d3e6b1e2510ec2436f6d,"Distributed software systems are increasingly developed and deployed today. Many of these systems are supposed to run continuously. Given their critical roles in our society and daily lives, assuring the quality of distributed systems is crucial. Analyzing runtime program dependencies has long been a fundamental technique underlying numerous tool support for software quality assurance. Yet conventional approaches to dynamic dependence analysis face severe scalability barriers when theyare applied to real-world distributed systems, due to the unbounded executions to be analyzed in addition to common efficiency challenges suffered by dynamic analysis in general. In this article, wepresent SEADS, a distributed, online, and cost-effective dynamic dependence analysis framework thataims at scaling the analysis to real-world distributed systems. The analysis itself is distributed to exploit the distributed computing resources (e.g., a cluster) of the system under analysis; it works online to overcome the problem with unbounded execution traces while running continuously withthe system being analyzed to provide timely querying of analysis results (i.e., runtime dependence set of any given query). Most importantly, given a user-specified time budget, the analysis automatically adjusts itself to better cost-effectiveness tradeoffs (than otherwise) while respecting the budget by changing various analysis parameters according to the time being spent by the dependence analysis. At the core of the automatic adjustment is our application of a reinforcement learning method for the decision making - deciding which configuration to adjust to according to the current configuration and its associated analysis cost with respect to the user budget. We have implemented SEADS for Java and applied it to eight real-world distributed systems with continuous executions. Our empirical results revealed the efficiency and scalability advantages of our framework over a conventional dynamic analysis, at least for dynamic dependence computation at method level. While we demonstrate it in the context of dynamic dependence analysis in this article, the methodology for achieving and maintaining scalability and greater cost-effectiveness against continuously running systems is more broadly applicable to other dynamic analyses.  © 2020 ACM.",cost-effectiveness; Distributed systems; dynamic analysis; reinforcement learning; scalability,Budget control; Cluster computing; Computer software selection and evaluation; Cost effectiveness; Costs; Decision making; Distributed database systems; Efficiency; Learning systems; Online systems; Quality assurance; Reinforcement learning; Scalability; Software quality; Automatic adjustment; Conventional approach; Current configuration; Dependence analysis; Distributed computing resources; Distributed software system; Distributed systems; Reinforcement learning method; Cost benefit analysis
Technical Q8A Site Answer Recommendation via Question Boosting,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099879031&doi=10.1145%2f3412845&partnerID=40&md5=781618622a879215a95cba19b79edb90,"Software developers have heavily used online question-and-answer platforms to seek help to solve their technical problems. However, a major problem with these technical Q8A sites is ""answer hungriness,""i.e., a large number of questions remain unanswered or unresolved, and users have to wait for a long time or painstakingly go through the provided answers with various levels of quality. To alleviate this time-consuming problem, we propose a novel DEEPANS neural network-based approach to identify the most relevant answer among a set of answer candidates. Our approach follows a three-stage process: question boosting, label establishment, and answer recommendation. Given apost, we first generate a clarifying question as a way of question boosting. We automatically establish the positive, neutral+, neutral-, and negative training samples via label establishment. When it comes to answer recommendation, we sort answer candidates by the matching scores calculated by our neural network-based model. To evaluate the performance of our proposed model, we conducted a large-scale evaluation on four datasets, collected from the real-world technical Q8A sites (i.e., Ask Ubuntu, Super User, Stack Overflow Python, and Stack Overflow Java). Our experimental results show that our approach significantly outperforms several state-of-the-art baselines in automatic evaluation. We also conducted a user study with 50 solved/unanswered/unresolved questions. The user-study results demonstrate that our approach is effective in solving the answer-hungry problem by recommending the most relevant answers from historical archives.  © 2020 ACM.",CQA; deep neural network; question answering; question boosting; sequence-to-sequence; weakly supervised learning,Large dataset; Automatic evaluation; Historical archive; Network-based approach; Network-based modeling; Software developer; State of the art; Three-stage process; Training sample; Neural networks
A Practical Approach to Verification of Floating-Point C/C++ Programs with math.h/cmath Functions,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099885955&doi=10.1145%2f3410875&partnerID=40&md5=77968a058a40f0a1eb3b5020c6326701,"Verification of C/C<bold>++</bold> programs has seen considerable progress in several areas, but not for programs that use these languages' mathematical libraries. The reason is that all libraries in widespread use come with no guarantees about the computed results. This would seem to prevent any attempt at formal verification of programs that use them: without a specificationfor the functions, no conclusion can be drawn statically about the behavior of the program. We propose an alternative to surrender. We introduce a pragmatic approach that leverages the fact that most <monospace>math.h/cmath</monospace> functions are almost piecewise monotonic: as we discovered through exhaustive testing, they may have glitches, often of very small size and in smallnumbers. We develop interval refinement techniques for such functions based on a modified dichotomic search, which enable verification via symbolic execution based model checking, abstract interpretation, and test data generation. To the best of our knowledge, our refinement algorithms are the first in the literature to be able to handle non-correctly rounded function implementations, enablingverification in the presence of the most common implementations. We experimentally evaluate our approach on real-world code, showing its ability to detect or rule out anomalous behaviors.  © 2020 ACM.",abstract interpretation; constraint propagation; Floating-point numbers; model checking; program verification; symbolic execution,Digital arithmetic; Formal verification; Libraries; Model checking; Abstract interpretations; Anomalous behavior; Exhaustive testing; Mathematical library; Piecewise monotonic; Refinement algorithms; Refinement techniques; Test data generation; C++ (programming language)
Automated Patch Transplantation,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099879333&doi=10.1145%2f3412376&partnerID=40&md5=e6538742b1861b7ab937a0aee7247242,"Automated program repair is an emerging area that attempts to patch software errors and vulnerabilities. In this article, we formulate and study a problem related to automated repair, namely automated patch transplantation. A patch for an error in a donor program is automatically adapted andinserted into a ""similar""target program. We observe that despite standard procedures forvulnerability disclosures and publishing of patches, many un-patched occurrences remain in the wild. One of the main reasons is the fact that various implementations of the same functionality may exist and, hence, published patches need to be modified and adapted. In this article, we therefore propose and implement a workflow for transplanting patches. Our approach centers on identifying patchinsertion points, as well as namespaces translation across programs via symbolic execution. Experimental results to eliminate five classes of errors highlight our ability to fix recurring vulnerabilities across various programs through transplantation. We report that in 20 of 24 fixing tasks involving eight application subjects mostly involving file processing programs, we successfully transplanted the patch and validated the transplantation through differential testing. Since the publication of patches make an un-patched implementation more vulnerable, our proposed techniques should serve a long-standing need in practice.  © 2020 ACM.",code transplantation; dynamic program analysis; patch transplantation; Program repair,Automation; Errors; Program translators; Differential testing; File processing; Namespaces; Software errors; Standard procedures; Symbolic execution; Application programs
RegionTrack: A Trace-Based Sound and Complete Checker to Debug Transactional Atomicity Violations and Non-Serializable Traces,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099876069&doi=10.1145%2f3412377&partnerID=40&md5=44d36932c82788e7f73ece658cefd213,"Atomicity is a correctness criterion to reason about isolated code regions in a multithreaded program when they are executed concurrently. However, dynamic instances of these code regions, called transactions, may fail to behave atomically, resulting in transactional atomicity violations. Existing dynamic online atomicity checkers incur either false positives or false negatives in detecting transactions experiencing transactional atomicity violations. This article proposes <monospace>RegionTrack</monospace>. <monospace>RegionTrack</monospace> tracks cross-thread dependences at the event, dynamic subregion, and transaction levels. It maintains both dynamic subregions within selected transactions and transactional happens-before relations through its novel timestamp propagation approach. We prove that <monospace>RegionTrack</monospace> is sound and complete in detecting both transactional atomicity violations and non-serializable traces.To the best of our knowledge, it is the first online technique that precisely captures the transitively closed set of happens-before relations over all conflicting events with respect to every running transaction for the above two kinds of issues. We have evaluated <monospace>RegionTrack</monospace> on 19 subjects of the DaCapo and the Java Grande Forum benchmarks. The empirical results confirm that <monospace>RegionTrack</monospace> precisely detected all those transactions which experienced transactional atomicity violations and identified all non-serializable traces. The overall results also show that <monospace>RegionTrack</monospace> incurred 1.10x and 1.08x lower memory and runtime overheads than <monospace>Velodrome</monospace>and 2.10x and 1.21x lower than <monospace>Aerodrome</monospace>, respectively. Moreover, it incurred 2.89x lower memory overhead than <monospace>DoubleChecker</monospace>. Onaverage, <monospace>Velodrome</monospace> detected about 55% fewer violations than <monospace>RegionTrack</monospace>, which in turn reported about 3%-70% fewer violations than <monospace>DoubleChecker</monospace>.  © 2020 ACM.",conflict serializability; debugging; dynamic analysis; linearizability; non-serializable traces; Transactional atomicity violation,Computer software; Atomicity violations; Correctness criterion; Memory overheads; Multi-threaded programs; Online technique; Runtime overheads; Sound and complete; Transaction level; Software engineering
Stream Gen: Model-driven Development of Distributed Streaming Applications,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099877794&doi=10.1145%2f3408895&partnerID=40&md5=e2a263a19bae28d149018b72b48f3631,"Distributed streaming applications, i.e., applications that process massive streams of data ina distributed fashion, are becoming increasingly popular to tame the velocity and the volume of BigData. Nevertheless, the widespread adoption of data-intensive processing is still limited by the non-trivial design paradigms involved, which deal with the unboundedness and volume of involved data streams and by the many distributed streaming platforms, each with its own characteristics and APIs. In this article, we present StreamGen, a Model-Driven Engineering tool to simplify the design of such streaming applications and automatically generate the corresponding code. StreamGen is able toautomatically generate fully working and processing-ready code for different target platforms (e.g., Apache Spark, Apache Flink). Evaluation shows that (i) StreamGen is general enough to model and generate the code, offering comparable performance against a preexisting similar and well-known application; (ii) the tool is fully compliant with streaming concepts defined as part of the Google Dataflow Model; and (iii) users with little computer science background and limited experience with big data have been able to work with StreamGen and create/refactor an application in a matter of minutes.  © 2021 ACM.",big data architectures; Model-driven engineering; streaming applications,Data flow analysis; Petroleum reservoir evaluation; Data intensive; Dataflow model; Distributed streaming; Model driven development; Model-driven Engineering; Non-trivial; Science background; Streaming applications; Data streams
An Empirical Study of Developer Discussions in the Gitter Platform,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099877283&doi=10.1145%2f3412378&partnerID=40&md5=140dbe26f06fd974e59443c82d6f87fd,"Developer chatrooms (e.g., the Gitter platform) are gaining popularity as a communication channel among developers. In developer chatrooms, a developer (asker) posts questions and other developers (respondents) respond to the posted questions. The interaction between askers and respondents results in a discussion thread. Recent studies show that developers use chatrooms to inquire about issues, discuss development ideas, and help each other. However, prior work focuses mainly on analyzing individual messages of a chatroom without analyzing the discussion thread in a chatroom. Developer chatroom discussions are context-sensitive, entangled, and include multiple participants that make it hard to accurately identify threads. Therefore, prior work has limited capability to show the interactions among developers within a chatroom by analyzing only individual messages. In this article, we perform an in-depth analysis of the Gitter platform (i.e., developer chatrooms) by analyzing 6,605,248 messages of 709 chatrooms. To analyze the characteristics of the posted questions andthe impact on the response behavior (e.g., whether the posted questions get responses), we propose an approach that identifies discussion threads in chatrooms with high precision (i.e., 0.81 F-score). Our results show that inactive members responded more often and unique questions take longer discussion time than simple questions. We also find that clear and concise questions are more likely to be responded to than poorly written questions. We further manually analyze a randomly selected sample of 384 threads to examine how respondents resolve the raised questions. We observe that more than 80% of the studied threads are resolved. Advanced-level/beginner-level questions along with theedited questions are the mostly resolved questions. Our results can help the project maintainers understand the nature of the discussion threads (e.g., the topic trends). Project maintainers can also benefit from our thread identification approach to spot the common repeated threads and use thesethreads as frequently asked questions (FAQs) to improve the documentation of their projects.  ©2020 ACM.",Chat disentanglement; developer chatrooms; developer threads; Gitter; mixed-effect models; thread identification,Computer software; Context sensitive; Empirical studies; Frequently asked questions; High-precision; Identification approach; In-depth analysis; Inactive member; Response behavior; Software engineering
Verification of Program Transformations with Inductive Refinement Types,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099877444&doi=10.1145%2f3409805&partnerID=40&md5=43ce386eec38cab049deddbbdbfab424,"High-level transformation languages like Rascal include expressive features for manipulating large abstract syntax trees: first-class traversals, expressive pattern matching, backtracking, and generalized iterators. We present the design and implementation of an abstract interpretation tool,Rabit, for verifying inductive type and shape properties for transformations written in such languages. We describe how to perform abstract interpretation based on operational semantics, specifically focusing on the challenges arising when analyzing the expressive traversals and pattern matching.Finally, we evaluate Rabit on a series of transformations (normalization, desugaring, refactoring, code generators, type inference, etc.) showing that we can effectively verify stated properties.  © 2021 ACM.",abstract interpretation; static analysis; Transformation languages,Abstracting; Model checking; Pattern matching; Semantics; Trees (mathematics); Abstract interpretations; Abstract Syntax Trees; Code generators; Design and implementations; High-level transformations; Operational semantics; Program transformations; Type inferences; High level languages
Security Smells in Ansible and Chef Scripts: A Replication Study,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099877605&doi=10.1145%2f3408897&partnerID=40&md5=f212b471ecf9a4e653d32ca983a3ef35,"Context: Security smells are recurring coding patterns that are indicative of security weakness and require further inspection. As infrastructure as code (IaC) scripts, such as Ansible and Chefscripts, are used to provision cloud-based servers and systems at scale, security smells in IaC scripts could be used to enable malicious users to exploit vulnerabilities in the provisioned systems.Goal: The goal of this article is to help practitioners avoid insecure coding practices while developing infrastructure as code scripts through an empirical study of security smells in Ansible and Chef scripts. Methodology: We conduct a replication study where we apply qualitative analysis with 1,956 IaC scripts to identify security smells for IaC scripts written in two languages: Ansible and Chef. We construct a static analysis tool called Security Linter for Ansible and Chef scripts (SLAC) to automatically identify security smells in 50,323 scripts collected from 813 open source software repositories. We also submit bug reports for 1,000 randomly selected smell occurrences. Results:We identify two security smells not reported in prior work: missing default in case statement and no integrity check. By applying SLAC we identify 46,600 occurrences of security smells that include7,849 hard-coded passwords. We observe agreement for 65 of the responded 94 bug reports, which suggests the relevance of security smells for Ansible and Chef scripts amongst practitioners. Conclusion: We observe security smells to be prevalent in Ansible and Chef scripts, similarly to that of the Puppet scripts. We recommend practitioners to rigorously inspect the presence of the identified security smells in Ansible and Chef scripts using (i) code review, and (ii) static analysis tools.  © 2021 ACM.",Ansible; chef; configuration as code; configuration scripts; devops; devsecops; empirical study; infrastructure as code; insecure coding; security; smell; static analysis,Odors; Open source software; Open systems; Cloud-based; Code review; Coding patterns; Empirical studies; Integrity check; Qualitative analysis; Replication study; Security weakness; Static analysis
Uncertainty-wise Requirements Prioritization with Search,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099887403&doi=10.1145%2f3408301&partnerID=40&md5=245a28dbb965adee1e43d39b0f31ba17,"Requirements review is an effective technique to ensure the quality of requirements in practice, especially in safety-critical domains (e.g., avionics systems, automotive systems). In such contexts, a typical requirements review process often prioritizes requirements, due to limited time andmonetary budget, by, for instance, prioritizing requirements with higher implementation cost earlier in the review process. However, such a requirement implementation cost is typically estimated by stakeholders who often lack knowledge about (future) requirements implementation scenarios, which leads to uncertainty in cost overrun. In this article, we explicitly consider such uncertainty (quantified as cost overrun probability) when prioritizing requirements based on the assumption that a requirement with higher importance, a higher number of dependencies to other requirements, and higher implementation cost will be reviewed with the higher priority. Motivated by this, we formulate four objectives for uncertainty-wise requirements prioritization: maximizing the importance of requirements, requirements dependencies, the implementation cost of requirements, and cost overrun probability. These four objectives are integrated as part of our search-based uncertainty-wise requirements prioritization approach with tool support, named as URP. We evaluated six Multi-Objective SearchAlgorithms (MOSAs) (i.e., NSGA-II, NSGA-III, MOCell, SPEA2, IBEA, and PAES) together with Random Search (RS) using three real-world datasets (i.e., the RALIC, Word, and ReleasePlanner datasets) and 19 synthetic optimization problems. Results show that all the selected MOSAs can solve the requirements prioritization problem with significantly better performance than RS. Among them, IBEA was over 40% better than RS in terms of permutation effectiveness for the first 10% of prioritized requirements in the prioritization sequence of all three datasets. In addition, IBEA achieved the best performance in terms of the convergence of solutions, and NSGA-III performed the best when consideringboth the convergence and diversity of nondominated solutions.  © 2020 ACM.",multi-objective search algorithm; Requirements prioritization; uncertainty,Budget control; Safety engineering; Uncertainty analysis; Convergence of solutions; Implementation cost; Nondominated solutions; Real-world datasets; Requirements dependencies; Requirements prioritization; Safety-critical domain; Synthetic optimization; Costs
Mastering Variation in Human Studies: The Role of Aggregation,2021,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099881036&doi=10.1145%2f3406544&partnerID=40&md5=8f2755fbe2b883f43aadbcc26bc59497,"The human factor is prevalent in empirical software engineering research. However, human studies often do not use the full potential of analysis methods by combining analysis of individual tasks and participants with an analysis that aggregates results over tasks and/or participants. This may hide interesting insights of tasks and participants and may lead to false conclusions by overrating or underrating single-task or participant performance. We show that studying multiple levels of aggregation of individual tasks and participants allows researchers to have both insights from individual variations as well as generalized, reliable conclusions based on aggregated data. Our literature survey revealed that most human studies perform either a fully aggregated analysis or an analysis of individual tasks. To show that there is important, non-trivial variation when including human participants, we reanalyze 12 published empirical studies, thereby changing the conclusions or making them more nuanced. Moreover, we demonstrate the effects of different aggregation levels by answering a novel research question on published sets of fMRI data. We show that when more data are aggregated, the results become more accurate. This proposed technique can help researchers to find a sweet spot in the tradeoff between cost of a study and reliability of conclusions.  © 2020 ACM.",data aggregation; guidelines; Human studies,Aggregated datum; Aggregation level; Analysis method; Empirical Software Engineering; Empirical studies; Literature survey; Multiple levels; Research questions; Software engineering
Measuring Task Conflict and Person Conflict in Software Testing,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092745948&doi=10.1145%2f3395029&partnerID=40&md5=5f3a4afadf803f67385dd7d0cadca95e,"Task-related conflict and person-related conflict in software testing are inevitable and can impact the effectiveness and efficiency of the software development process. The dimensionality of conflict in software testing is reasonably well understood, although in past research both types of conflict have frequently been modeled as reflective constructs that can obstruct the effectiveness of their use as organizational assessment and training tools. One contribution of this study is an empirical model of conflict sources in software engineering; such sources of conflict threaten to derail efficient software development outcomes in firms. A second contribution of this research is the development of a formative measurement model for purposes of development of assessing task conflict and person conflict in software teams. These validated measures can be utilized as training and development instruments for on-the-job remediation of development team conflict. As is indicated in the organizational behavior and software engineering literature, deploying valid measures of workplace stressors such as conflict can lead to the managerial application of effective strategies and tactics to improve workplace morale and satisfaction, to the great benefit of productivity and retention.  © 2020 ACM.",formative measurement; person conflict; software development; Software testing; task conflict,Application programs; Software design; Well testing; Development teams; Effectiveness and efficiencies; Empirical model; Formative measurements; Organizational assessment; Organizational behavior; Software development process; Training and development; Software testing
ISENSE2.0: Improving Completion-aware Crowdtesting Management with Duplicate Tagger and Sanity Checker,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092718445&doi=10.1145%2f3394602&partnerID=40&md5=fb5e9eb6d4b95a7af1fe8ede089da8ad,"Software engineers get questions of ""how much testing is enough""on a regular basis. Existing approaches in software testing management employ experience-, risk-, or value-based analysis to prioritize and manage testing processes. However, very few is applicable to the emerging crowdtesting paradigm to cope with extremely limited information and control over unknown, online crowdworkers. In practice, deciding when to close a crowdtesting task is largely done by experience-based guesswork and frequently results in ineffective crowdtesting. More specifically, it is found that an average of 32% testing cost was wasteful spending in current crowdtesting practice. This article intends to address this challenge by introducing automated decision support for monitoring and determining appropriate time to close crowdtesting tasks. To that end, it first investigates the necessity and feasibility of close prediction of crowdtesting tasks based on an industrial dataset. Next, it proposes a close prediction approach named iSENSE2.0, which applies incremental sampling technique to process crowdtesting reports arriving in chronological order and organizes them into fixed-sized groups as dynamic inputs. Then, a duplicate tagger analyzes the duplicate status of received crowd reports, and a CRC-based (Capture-ReCapture) close estimator generates the close decision based on the dynamic bug arrival status. In addition, a coverage-based sanity checker is designed to reinforce the stability and performance of close prediction. Finally, the evaluation of iSENSE2.0 is conducted on 56,920 reports of 306 crowdtesting tasks from one of the largest crowdtesting platforms. The results show that a median of 100% bugs can be detected with 30% saved cost. The performance of iSENSE2.0 does not demonstrate significant difference with the state-of-the-art approach iSENSE, while the later one relies on the duplicate tag, which is generally considered as time-consuming and tedious to obtain.  © 2020 ACM.",capture-recapture; close prediction; Crowdsourced testing; term coverage; test management,Decision support systems; Forecasting; Risk assessment; Capture-recapture; Chronological order; Decision supports; Incremental samplings; Limited information; Software testing managements; State-of-the-art approach; Testing process; Software testing
Modular Tree Network for Source Code Representation Learning,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092714268&doi=10.1145%2f3409331&partnerID=40&md5=d905cdf7f3fff0be2096f74ffa0ca9ba,"Learning representation for source code is a foundation of many program analysis tasks. In recent years, neural networks have already shown success in this area, but most existing models did not make full use of the unique structural information of programs. Although abstract syntax tree (AST)-based neural models can handle the tree structure in the source code, they cannot capture the richness of different types of substructure in programs. In this article, we propose a modular tree network that dynamically composes different neural network units into tree structures based on the input AST. Different from previous tree-structural neural network models, a modular tree network can capture the semantic differences between types of AST substructures. We evaluate our model on two tasks: program classification and code clone detection. Our model achieves the best performance compared with state-of-the-art approaches in both tasks, showing the advantage of leveraging more elaborate structure information of the source code.  © 2020 ACM.",code clone detection; Deep learning; neural networks; program classification,Computer programming languages; Network coding; Semantics; Trees (mathematics); Abstract Syntax Trees; Code clone detection; Neural network model; Program classifications; Source code representations; State-of-the-art approach; Structural information; Structure information; Neural networks
Why Developers Refactor Source Code: A Mining-based Study,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092715119&doi=10.1145%2f3408302&partnerID=40&md5=cecdd7556b97a32248f6437d748fe573,"Refactoring aims at improving code non-functional attributes without modifying its external behavior. Previous studies investigated the motivations behind refactoring by surveying developers. With the aim of generalizing and complementing their findings, we present a large-scale study quantitatively and qualitatively investigating why developers perform refactoring in open source projects. First, we mine 287,813 refactoring operations performed in the history of 150 systems. Using this dataset, we investigate the interplay between refactoring operations and process (e.g., previous changes/fixes) and product (e.g., quality metrics) metrics. Then, we manually analyze 551 merged pull requests implementing refactoring operations and classify the motivations behind the implemented refactorings (e.g., removal of code duplication). Our results led to (i) quantitative evidence of the relationship existing between certain process/product metrics and refactoring operations and (ii) a detailed taxonomy, generalizing and complementing the ones existing in the literature, of motivations pushing developers to refactor source code.  © 2020 ACM.",empirical software engineering; Refactoring,Motivation; Code duplication; External behavior; Large-scale studies; Non-functional; Open source projects; Quality metrics; Refactorings; Source codes; Open source software
Smart Contract Repair,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092691115&doi=10.1145%2f3402450&partnerID=40&md5=bdc47055036de9c4e06e4a32fff7455c,"Smart contracts are automated or self-enforcing contracts that can be used to exchange assets without having to place trust in third parties. Many commercial transactions use smart contracts due to their potential benefits in terms of secure peer-to-peer transactions independent of external parties. Experience shows that many commonly used smart contracts are vulnerable to serious malicious attacks, which may enable attackers to steal valuable assets of involving parties. There is, therefore, a need to apply analysis and automated repair techniques to detect and repair bugs in smart contracts before being deployed. In this work, we present the first general-purpose automated smart contract repair approach that is also gas-aware. Our repair method is search-based and searches among mutations of the buggy contract. Our method also considers the gas usage of the candidate patches by leveraging our novel notion of gas dominance relationship. We have made our smart contract repair tool SCRepair available open-source, for investigation by the wider community.  © 2020 ACM.",Program repair; smart contract,Computer software; Software engineering; Commercial transactions; Dominance relationships; Malicious attack; Potential benefits; Repair methods; Repair techniques; Repair tools; Third parties; Automation
Handling SQL Databases in Automated System Test Generation,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092710127&doi=10.1145%2f3391533&partnerID=40&md5=9f7966a5f46f597c930eb6a3ee5a616c,"Automated system test generation for web/enterprise systems requires either a sequence of actions on a GUI (e.g., clicking on HTML links and form buttons) or direct HTTP calls when dealing with web services (e.g., REST and SOAP). When doing white-box testing of such systems, their code can be analyzed, and the same type of heuristics (e.g., the branch distance) used in search-based unit testing can be employed to improve performance. However, web/enterprise systems do often interact with a database. To obtain higher coverage and find new faults, the state of the databases needs to be taken into account when generating white-box tests. In this work, we present a novel heuristic to enhance search-based software testing of web/enterprise systems, which takes into account the state of the accessed databases. Furthermore, we enable the generation of SQL data directly from the test cases. This is useful when it is too difficult or time consuming to generate the right sequence of events to put the database in the right state. Also, it is useful when dealing with databases that are ""read-only""for the system under test, and the actual data are generated by other services. We implemented our technique as an extension of EVOMASTER, where system tests are generated in the JUnit format. Experiments on six RESTful APIs (five open-source and one industrial) show that our novel techniques improve coverage significantly (up to +16.5%), finding seven new faults in those systems.  © 2020 ACM.",automated test generation; database; REST; SBST; SQL; system testing; web service,Automation; Database systems; Direct sequence systems; HTTP; Open source software; Software testing; Testing; Web services; Automated systems; Improve performance; Novel techniques; Search-based software testing; Sequence of actions; Sequence of events; System under test; White-box testing; Open systems
Using Relative Lines of Code to Guide Automated Test Generation for Python,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092704835&doi=10.1145%2f3408896&partnerID=40&md5=69e6dc1d6944c342f2e7a4df8ab3dd84,"Raw lines of code (LOC) is a metric that does not, at first glance, seem extremely useful for automated test generation. It is both highly language-dependent and not extremely meaningful, semantically, within a language: one coder can produce the same effect with many fewer lines than another. However, relative LOC, between components of the same project, turns out to be a highly useful metric for automated testing. In this article, we make use of a heuristic based on LOC counts for tested functions to dramatically improve the effectiveness of automated test generation. This approach is particularly valuable in languages where collecting code coverage data to guide testing has a very high overhead. We apply the heuristic to property-based Python testing using the TSTL (Template Scripting Testing Language) tool. In our experiments, the simple LOC heuristic can improve branch and statement coverage by large margins (often more than 20%, up to 40% or more) and improve fault detection by an even larger margin (usually more than 75% and up to 400% or more). The LOC heuristic is also easy to combine with other approaches and is comparable to, and possibly more effective than, two well-established approaches for guiding random testing.  © 2020 ACM.",Automated test generation; static code metrics; testing heuristics,Automation; Fault detection; High level languages; Automated test generations; Automated testing; Code coverage; Large margins; Lines of code; Property-based; Random testing; Statement coverage; Well testing
Practical Accuracy Estimation for Efficient Deep Neural Network Testing,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092739576&doi=10.1145%2f3394112&partnerID=40&md5=2e19bbb87ab18a3658c90bfb1f29a558,"Deep neural network (DNN) has become increasingly popular and DNN testing is very critical to guarantee the correctness of DNN, i.e., the accuracy of DNN in this work. However, DNN testing suffers from a serious efficiency problem, i.e., it is costly to label each test input to know the DNN accuracy for the testing set, since labeling each test input involves multiple persons (even with domain-specific knowledge) in a manual way and the testing set is large-scale. To relieve this problem, we propose a novel and practical approach, called PACE (which is short for Practical ACcuracy Estimation), which selects a small set of test inputs that can precisely estimate the accuracy of the whole testing set. In this way, the labeling costs can be largely reduced by just labeling this small set of selected test inputs. Besides achieving a precise accuracy estimation, to make PACE more practical it is also required that it is interpretable, deterministic, and as efficient as possible. Therefore, PACE first incorporates clustering to interpretably divide test inputs with different testing capabilities (i.e., testing different functionalities of a DNN model) into different groups. Then, PACE utilizes the MMD-critic algorithm, a state-of-the-art example-based explanation algorithm, to select prototypes (i.e., the most representative test inputs) from each group, according to the group sizes, which can reduce the impact of noise due to clustering. Meanwhile, PACE also borrows the idea of adaptive random testing to select test inputs from the minority space (i.e., the test inputs that are not clustered into any group) to achieve great diversity under the required number of test inputs. The two parallel selection processes (i.e., selection from both groups and the minority space) compose the final small set of selected test inputs. We conducted an extensive study to evaluate the performance of PACE based on a comprehensive benchmark (i.e., 24 pairs of DNN models and testing sets) by considering different types of models (i.e., classification and regression models, high-accuracy and low-accuracy models, and CNN and RNN models) and different types of test inputs (i.e., original, mutated, and automatically generated test inputs). The results demonstrate that PACE is able to precisely estimate the accuracy of the whole testing set with only 1.181%∼2.302% deviations, on average, significantly outperforming the state-of-the-art approaches.  © 2020 ACM.",Deep neural network testing; labeling; test input selection; test optimization,Benchmarking; Classification (of information); Deep neural networks; Recurrent neural networks; Regression analysis; Accuracy estimation; Adaptive random testing; Automatically generated; Domain-specific knowledge; Practical accuracy; Regression model; State of the art; State-of-the-art approach; Automatic test pattern generation
Fine-grained Code Coverage Measurement in Automated Black-box Android Testing,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092701707&doi=10.1145%2f3395042&partnerID=40&md5=9bb9c73f2856881c6813ade332066e0c,"Today, there are millions of third-party Android applications. Some of them are buggy or even malicious. To identify such applications, novel frameworks for automated black-box testing and dynamic analysis are being developed by the Android community. Code coverage is one of the most common metrics for evaluating effectiveness of these frameworks. Furthermore, code coverage is used as a fitness function for guiding evolutionary and fuzzy testing techniques. However, there are no reliable tools for measuring fine-grained code coverage in black-box Android app testing. We present the Android Code coVerage Tool, ACVTool for short, that instruments Android apps and measures code coverage in the black-box setting at class, method and instruction granularity. ACVTool has successfully instrumented 96.9% of apps in our experiments. It introduces a negligible instrumentation time overhead, and its runtime overhead is acceptable for automated testing tools. We demonstrate practical value of ACVTool in a large-scale experiment with Sapienz, a state-of-the-art automated testing tool. Using ACVTool on the same cohort of apps, we have compared different coverage granularities applied by Sapienz in terms of the found amount of crashes. Our results show that none of the applied coverage granularities clearly outperforms others in this aspect.  © 2020 ACM.",Android; automated software testing; code coverage; instrumentation,Android (operating system); Automation; Testing; Android applications; Automated testing tools; Fitness functions; Large scale experiments; Runtime overheads; State of the art; Testing technique; Third parties; Black-box testing
Generating Question Titles for Stack Overflow from Mined Code Snippets,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092728971&doi=10.1145%2f3401026&partnerID=40&md5=0f7f22fff32659e4c88ea0e591f47ef8,"Stack Overflow has been heavily used by software developers as a popular way to seek programming-related information from peers via the internet. The Stack Overflow community recommends users to provide the related code snippet when they are creating a question to help others better understand it and offer their help. Previous studies have shown that a significant number of these questions are of low-quality and not attractive to other potential experts in Stack Overflow. These poorly asked questions are less likely to receive useful answers and hinder the overall knowledge generation and sharing process. Considering one of the reasons for introducing low-quality questions in SO is that many developers may not be able to clarify and summarize the key problems behind their presented code snippets due to their lack of knowledge and terminology related to the problem, and/or their poor writing skills, in this study we propose an approach to assist developers in writing high-quality questions by automatically generating question titles for a code snippet using a deep sequence-to-sequence learning approach. Our approach is fully data-driven and uses an attention mechanism to perform better content selection, a copy mechanism to handle the rare-words problem and a coverage mechanism to eliminate word repetition problem. We evaluate our approach on Stack Overflow datasets over a variety of programming languages (e.g., Python, Java, Javascript, C# and SQL) and our experimental results show that our approach significantly outperforms several state-of-the-art baselines in both automatic and human evaluation. We have released our code and datasets to facilitate other researchers to verify their ideas and inspire the follow up work.  © 2020 ACM.",question generation; question quality; sequence-to-sequence; Stack overflow,Problem oriented languages; Attention mechanisms; Human evaluation; Knowledge generations; Sequence learning; Software developer; Stack overflow; State of the art; Writing skills; Codes (symbols)
Computing Alignments ofWell-Formed Process Models using Local Search,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088305464&doi=10.1145%2f3394056&partnerID=40&md5=25447071e08b67d69b1b8c62125d8321,"The alignment of observed and modeled behavior is an essential element for organizations, since it opens the door for conformance checking and enhancement of processes. The state-of-the-art technique for computing alignments has exponential time and space complexity, hindering its applicability for medium and large instances. In this article, a novel approach is presented to tackle the challenge of computing an alignment for large-problem instances that correspond to well-formed process models. Given an observed trace, first it uses a novel replay technique to find an initial candidate trace in the model. Then a local search framework is applied to try to improve the alignment until no further improvement is possible. The implementation of the presented technique reveals a magnificent reduction both in computation time and in memory usage. Moreover, although the proposed technique does not guarantee the derivation of an alignment with minimal cost, the experiments show that in practice the quality of the obtained solutions is close to optimal. © 2020 ACM.",conformance checking; event logs; Process mining; process models,Computer software; Software engineering; Computation time; Conformance checking; Essential elements; Exponential time; Large problems; Minimal cost; Process model; State-of-the-art techniques; Local search (optimization)
Monotone Precision and Recall Measures for Comparing Executions and Specifications of Dynamic Systems,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088269950&doi=10.1145%2f3387909&partnerID=40&md5=f47f5644b1714513d41dd78155e440e6,"The behavioural comparison of systems is an important concern of software engineering research. For example, the areas of specification discovery and specification mining are concerned with measuring the consistency between a collection of execution traces and a program specification. This problem is also tackled in process mining with the help of measures that describe the quality of a process specification automatically discovered from execution logs. Though various measures have been proposed, it was recently demonstrated that they neither fulfil essential properties, such as monotonicity, nor can they handle infinite behaviour. In this article, we address this research problem by introducing a new framework for the definition of behavioural quotients. We prove that corresponding quotients guarantee desired properties that existing measures have failed to support. We demonstrate the application of the quotients for capturing precision and recall measures between a collection of recorded executions and a system specification. We use a prototypical implementation of these measures to contrast their monotonic assessment with measures that have been defined in prior research. © 2020 ACM.",behavioural analysis; behavioural comparison; conformance checking; coverage; entropy; fitness; precision; process mining; recall; System comparison,Software engineering; Precision and recall; Process specification; Program specification; Prototypical implementation; Research problems; Specification discovery; Specification mining; System specification; Specifications
Testing Relative to Usage Scope: Revisiting Software Coverage Criteria,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088286226&doi=10.1145%2f3389126&partnerID=40&md5=883002e90e319743711778eba4b72a53,"Coverage criteria provide a useful and widely used means to guide software testing; however, indiscriminately pursuing full coverage may not always be convenient or meaningful, as not all entities are of interest in any usage context. We aim at introducing a more meaningful notion of coverage that takes into account how the software is going to be used. Entities that are not going to be exercised by the user should not contribute to the coverage ratio. We revisit the definition of coverage measures, introducing a notion of relative coverage. According to this notion, we provide a definition and a theoretical framework of relative coverage, within which we discuss implications on testing theory and practice. Through the evaluation of three different instances of relative coverage, we could observe that relative coverage measures provide a more effective strategy than traditional ones: we could reach higher coverage measures, and test cases selected by relative coverage could achieve higher reliability. We hint at several other useful implications of relative coverage notion on different aspects of software testing. © 2020 ACM.",Coverage testing; relative coverage,Computer software; Software engineering; Coverage criteria; Coverage ratio; Test case; Testing theories; Theoretical framework; Usage context; Usage scopes; Software testing
Wireframe-based UI Design Search through Image Autoencoder,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088317548&doi=10.1145%2f3391613&partnerID=40&md5=0a741b3721ebddb97fbed6e636ea4cb6,"UI design is an integral part of software development. For many developers who do not have much UI design experience, exposing them to a large database of real-application UI designs can help them quickly build up a realistic understanding of the design space for a software feature and get design inspirations from existing applications. However, existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face of the great variations in UI designs. In this article, we propose a deep-learning-based UI design search engine to fill in the gap. The key innovation of our search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs. We implement our approach for Android UI design search, and conduct extensive experiments with artificially created relevant UI designs and human evaluation of UI design search results. Our experiments confirm the superior performance of our search engine over existing image-similarity or component-matching-based methods and demonstrate the usefulness of our search engine in real-world UI design tasks. © 2020 ACM.",Android; auto-encoder; deep learning; UI search,Application programs; Database systems; Deep learning; Image analysis; Learning systems; Software design; High-fidelity; Human evaluation; Image similarity; Integral part; Keyword-based; Large database; Real applications; Software features; Search engines
KLEESpectre: Detecting Information Leakage through Speculative Cache Attacks via Symbolic Execution,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088297802&doi=10.1145%2f3385897&partnerID=40&md5=9de26bae02303e64f2deafbdc34b4221,"Spectre-style attacks disclosed in early 2018 expose data leakage scenarios via cache side channels. Specifically, speculatively executed paths due to branch mis-prediction may bring secret data into the cache, which are then exposed via cache side channels even after the speculative execution is squashed. Symbolic execution is a well-known test generation method to cover program paths at the level of the application software. In this article, we extend symbolic execution with modeling of cache and speculative execution. Our tool KLEESPECTRE, built on top of the KLEE symbolic execution engine, can thus provide a testing engine to check for data leakage through the cache side channel as shown via Spectre attacks. Our symbolic cache model can verify whether the sensitive data leakage due to speculative execution can be observed by an attacker at a given program point. Our experiments show that KLEESPECTRE can effectively detect data leakage along speculatively executed paths and our cache model can make the leakage detection more precise. © 2020 ACM.",cache side channel; software security; Spectre attacks; symbolic execution,Application programs; Engines; Model checking; Security of data; Software testing; Cache modeling; Information leakage; Leakage detection; Program points; Sensitive datas; Speculative execution; Symbolic execution; Test generations; Side channel attack
Multi-objective Integer Programming Approaches for Solving the Multi-criteria Test-suite Minimization Problem: Towards Sound and Complete Solutions of a Particular Search-based Software-engineering Problem,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088312800&doi=10.1145%2f3392031&partnerID=40&md5=cdadda9795ff1360cf99714c26374b1b,"Test-suite minimization is one key technique for optimizing the software testing process. Due to the need to balance multiple factors, multi-criteria test-suite minimization (MCTSM) becomes a popular research topic in the recent decade. The MCTSM problem is typically modeled as integer linear programming (ILP) problem and solved with weighted-sum single objective approach. However, there is no existing approach that can generate sound (i.e., being Pareto-optimal) and complete (i.e., covering the entire Pareto front) Pareto-optimal solution set, to the knowledge of the authors. In this work, we first prove that the ILP formulation can accurately model the MCTSM problem and then propose the multi-objective integer programming (MOIP) approaches to solve it. We apply our MOIP approaches on three specific MCTSM problems and compare the results with those of the cutting-edge methods, namely, NonlinearFormulation_LinearSolver (NF_LS) and two Multi-Objective Evolutionary Algorithms (MOEAs). The results show that our MOIP approaches can always find sound and complete solutions on five subject programs, using similar or significantly less time than NF_LS and two MOEAs do. The current experimental results are quite promising, and our approaches have the potential to be applied for other similar search-based software engineering problems. © 2020 ACM.",big-M method; CWMOIP; multi-objective integer programming; Regression testing; search-based software engineering; test-suite minimization; ϵ-constraint method,Evolutionary algorithms; Pareto principle; Software testing; Integer Linear Programming; Multi objective evolutionary algorithms; Multiple factors; Pareto optimal solutions; Search-based software engineering; Single objective; Sound and complete; Test suite minimization; Integer programming
Psc2code: Denoising Code Extraction from Programming Screencasts,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088298066&doi=10.1145%2f3392093&partnerID=40&md5=178d718076d0b9f6d7b4e103e4d47035,"Programming screencasts have become a pervasive resource on the Internet, which help developers learn new programming technologies or skills. The source code in programming screencasts is an important and valuable information for developers. But the streaming nature of programming screencasts (i.e., a sequence of screen-captured images) limits the ways that developers can interact with the source code in the screencasts. Many studies use the Optical Character Recognition (OCR) technique to convert screen images (also referred to as video frames) into textual content, which can then be indexed and searched easily. However, noisy screen images significantly affect the quality of source code extracted by OCR, for example, no-code frames (e.g., PowerPoint slides, web pages of API specification), non-code regions (e.g., Package Explorer view, Console view), and noisy code regions with code in completion suggestion popups. Furthermore, due to the code characteristics (e.g., long compound identifiers like ItemListener), even professional OCR tools cannot extract source code without errors from screen images. The noisy OCRed source code will negatively affect the downstream applications, such as the effective search and navigation of the source code content in programming screencasts. In this article, we propose an approach named psc2code to denoise the process of extracting source code from programming screencasts. First, psc2code leverages the Convolutional Neural Network (CNN) based image classification to remove non-code and noisy-code frames. Then, psc2code performs edge detection and clustering-based image segmentation to detect sub-windows in a code frame, and based on the detected sub-windows, it identifies and crops the screen region that is most likely to be a code editor. Finally, psc2code calls the API of a professional OCR tool to extract source code from the cropped code regions and leverages the OCRed cross-frame information in the programming screencast and the statistical language model of a large corpus of source code to correct errors in the OCRed source code. We conduct an experiment on 1,142 programming screencasts from YouTube. We find that our CNN-based image classification technique can effectively remove the non-code and noisy-code frames, which achieves an F1-score of 0.95 on the valid code frames. We also find that psc2code can significantly improve the quality of the OCRed source code by truly correcting about half of incorrectly OCRed words. Based on the source code denoised by psc2code, we implement two applications: (1) a programming screencast search engine; (2) an interaction-enhanced programming screencast watching tool. Based on the source code extracted from the 1,142 collected programming screencasts, our experiments show that our programming screencast search engine achieves the precision@5, 10, and 20 of 0.93, 0.81, and 0.63, respectively. We also conduct a user study of our interaction-enhanced programming screencast watching tool with 10 participants. This user study shows that our interaction-enhanced watching tool can help participants learn the knowledge in the programming video more efficiently and effectively. © 2020 ACM.",code search; deep learning; Programming videos,Computer programming languages; Convolutional neural networks; Edge detection; Image classification; Image segmentation; Natural language processing systems; Optical character recognition; Search engines; Websites; API specifications; Classification technique; Code extraction; Downstream applications; Optical character recognition (OCR); Programming technology; Statistical language modeling; Textual content; Application programming interfaces (API)
Unveiling Elite Developers Activities in Open Source Projects,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088308753&doi=10.1145%2f3387111&partnerID=40&md5=6f236349b2f9bc32ccd27e5bc5cc1bfc,"Open source developers, particularly the elite developers who own the administrative privileges for a project, maintain a diverse portfolio of contributing activities. They not only commit source code but also exert significant efforts on other communicative, organizational, and supportive activities. However, almost all prior research focuses on specific activities and fails to analyze elite developers' activities in a comprehensive way. To bridge this gap, we conduct an empirical study with fine-grained event data from 20 large open source projects hosted on GITHUB. We investigate elite developers' contributing activities and their impacts on project outcomes. Our analyses reveal three key findings: (1) elite developers participate in a variety of activities, of which technical contributions (e.g., coding) only account for a small proportion; (2) as the project grows, elite developers tend to put more effort into supportive and communicative activities and less effort into coding; and (3) elite developers' efforts in nontechnical activities are negatively correlated with the project's outcomes in terms of productivity and quality in general, except for a positive correlation with the bug fix rate (a quality indicator). These results provide an integrated view of elite developers' activities and can inform an individual's decision making about effort allocation, which could lead to improved project outcomes. The results also provide implications for supporting these elite developers. © 2020 ACM.",developers' activity; Elite developers; open source software (OSS); productivity; project outcomes; software quality,Decision making; Project management; Effort allocation; Empirical studies; Open source developers; Open source projects; Positive correlations; Quality indicators; Specific activity; Technical contribution; Open source software
Editorial,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088309579&doi=10.1145%2f3402931&partnerID=40&md5=bb33a901e7a2b570c7399e871b540236,[No abstract available],,
Visualizing Distributed System Executions,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084931804&doi=10.1145%2f3375633&partnerID=40&md5=42771bed75fe0436c183863966e86226,"Distributed systems pose unique challenges for software developers. Understanding the system's communication topology and reasoning about concurrent activities of system hosts can be difficult. The standard approach, analyzing system logs, can be a tedious and complex process that involves reconstructing a system log from multiple hosts' logs, reconciling timestamps among hosts with non-synchronized clocks, and understanding what took place during the execution encoded by the log. This article presents a novel approach for tackling three tasks frequently performed during analysis of distributed system executions: (1) understanding the relative ordering of events, (2) searching for specific patterns of interaction between hosts, and (3) identifying structural similarities and differences between pairs of executions. Our approach consists of XVector, which instruments distributed systems to capture partial ordering information that encodes the happens-before relation between events, and ShiViz, which processes the resulting logs and presents distributed system executions as interactive time-space diagrams. Two user studies with a total of 109 students and a case study with 2 developers showed that our method was effective, helping participants answer statistically significantly more system-comprehension questions correctly, with a very large effect size. © 2020 ACM.",Distributed systems; log analysis; program comprehension,Network security; Communication topologies; Complex Processes; Concurrent activities; Distributed systems; Software developer; Structural similarity; Synchronized clocks; System comprehensions; Distributed computer systems
Predicting Node Failures in an Ultra-Large-Scale Cloud Computing Platform,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084928234&doi=10.1145%2f3385187&partnerID=40&md5=2cdbd4ded79597910530cffb23e1df31,"Many software services today are hosted on cloud computing platforms, such as Amazon EC2, due to many benefits like reduced operational costs. However, node failures in these platforms can impact the availability of their hosted services and potentially lead to large financial losses. Predicting node failures before they actually occur is crucial, as it enables DevOps engineers to minimize their impact by performing preventative actions. However, such predictions are hard due to many challenges like the enormous size of the monitoring data and the complexity of the failure symptoms. AIOps (Artificial Intelligence for IT Operations), a recently introduced approach in DevOps, leverages data analytics and machine learning to improve the quality of computing platforms in a cost-effective manner. However, the successful adoption of such AIOps solutions requires much more than a top-performing machine learning model. Instead, AIOps solutions must be trustable, interpretable, maintainable, scalable, and evaluated in context. To cope with these challenges, in this article we report our process of building an AIOps solution for predicting node failures for an ultra-large-scale cloud computing platform at Alibaba. We expect our experiences to be of value to researchers and practitioners, who are interested in building and maintaining AIOps solutions for large-scale cloud computing platforms. © 2020 ACM.",AIOps; cloud computing; failure prediction; ultra-large-scale platforms,Cost effectiveness; Data Analytics; DevOps; Forecasting; Losses; Machine learning; Cloud computing platforms; Computing platform; Cost effective; Financial loss; In-buildings; Machine learning models; Preventative actions; Software services; Cloud computing
Assessing and Improving Malware Detection Sustainability through App Evolution Studies,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084915340&doi=10.1145%2f3371924&partnerID=40&md5=c8a259297673f77076ae5242ae09644f,"Machine learning-based classification dominates current malware detection approaches for Android. However, due to the evolution of both the Android platform and its user apps, existing such techniques are widely limited by their reliance on new malware samples, which may not be timely available, and constant retraining, which is often very costly. As a result, new and emerging malware slips through, as seen from the continued surging of malware in the wild. Thus, a more practical detector needs not only to be accurate on particular datasets but, more critically, to be able to sustain its capabilities over time without frequent retraining. In this article, we propose and study the sustainability problem for learning-based app classifiers. We define sustainability metrics and compare them among five state-of-the-art malware detectors for Android. We further developed DroidSpan, a novel classification system based on a new behavioral profile for Android apps that captures sensitive access distribution from lightweight profiling. We evaluated the sustainability of DroidSpan versus the five detectors as baselines on longitudinal datasets across the past eight years, which include 13,627 benign apps and 12,755 malware. Through our extensive experiments, we showed that DroidSpan significantly outperformed all the baselines in substainability at reasonable costs, by 6%-32% for same-period detection and 21%-37% for over-time detection. The main takeaway, which also explains the superiority of DroidSpan, is that the use of features consistently differentiating malware from benign apps over time is essential for sustainable learning-based malware detection, and that these features can be learned from studies on app evolution. © 2020 ACM.",Android apps; evolution; malware detection; sustainability,Android (operating system); E-learning; Malware; Sustainable development; Android apps; Android platforms; Behavioral profiles; Classification system; Malware detection; Period detections; Sustainability metrics; Time detection; Mobile security
Editorial,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085071026&doi=10.1145%2f3383775&partnerID=40&md5=8fa2c0d4750e471d45e14f7a6f538bdf,[No abstract available],,
Practical Constraint Solving for Generating System Test Data,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084922206&doi=10.1145%2f3381032&partnerID=40&md5=1051ee49222d15c7f7dd977c172f318d,"The ability to generate test data is often a necessary prerequisite for automated software testing. For the generated data to be fit for their intended purpose, the data usually have to satisfy various logical constraints. When testing is performed at a system level, these constraints tend to be complex and are typically captured in expressive formalisms based on first-order logic. Motivated by improving the feasibility and scalability of data generation for system testing, we present a novel approach, whereby we employ a combination of metaheuristic search and Satisfiability Modulo Theories (SMT) for constraint solving. Our approach delegates constraint solving tasks to metaheuristic search and SMT in such a way as to take advantage of the complementary strengths of the two techniques. We ground our work on test data models specified in UML, with OCL used as the constraint language. We present tool support and an evaluation of our approach over three industrial case studies. The results indicate that, for complex system test data generation problems, our approach presents substantial benefits over the state-of-the-art in terms of applicability and scalability. © 2020 ACM.",metaheuristic search; model-driven engineering; OCL; SMT; System testing; test data generation; UML,Ability testing; Formal logic; Logic programming; Scalability; Software testing; Automated software testing; Constraint language; Constraint Solving; First order logic; Industrial case study; Logical constraints; Meta-heuristic search; Satisfiability modulo Theories; Search engines
Quality Indicators in Search-based Software Engineering,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084934772&doi=10.1145%2f3375636&partnerID=40&md5=a61ae0f5c8daff5f411715b1a00e8bd9,"Search-Based Software Engineering (SBSE) researchers who apply multi-objective search algorithms (MOSAs) often assess the quality of solutions produced by MOSAs with one or more quality indicators (QIs). However, SBSE lacks evidence providing insights on commonly used QIs, especially about agreements among them and their relations with SBSE problems and applied MOSAs. Such evidence about QIs agreements is essential to understand relationships among QIs, identify redundant QIs, and consequently devise guidelines for SBSE researchers to select appropriate QIs for their specific contexts. To this end, we conducted an extensive empirical evaluation to provide insights on commonly used QIs in the context of SBSE, by studying agreements among QIs with and without considering differences of SBSE problems and MOSAs. In addition, by defining a systematic process based on three common ways of comparing MOSAs in SBSE, we present additional observations that were automatically produced based on the results of our empirical evaluation. These observations can be used by SBSE researchers to gain a better understanding of the commonly used QIs in SBSE, in particular, regarding their agreements. Finally, based on the results, we also provide a set of guidelines for SBSE researchers to select appropriate QIs for their particular context. © 2020 ACM.",agreement; multi-objective search algorithm; quality indicator; Search-based software engineering,Computer software; Software engineering; Empirical evaluations; Multi objective; Quality indicators; Quality of solution; Search Algorithms; Search based software engineering (SBSE); Search-based software engineering; Systematic process; Software quality
A Defect Estimator for Source Code,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084921082&doi=10.1145%2f3384517&partnerID=40&md5=e3ddab0d43372eea6183eda912d58544,"An important issue faced during software development is to identify defects and the properties of those defects, if found, in a given source file. Determining defectiveness of source code assumes significance due to its implications on software development and maintenance cost. We present a novel system to estimate the presence of defects in source code and detect attributes of the possible defects, such as the severity of defects. The salient elements of our system are: (i) a dataset of newly introduced source code metrics, called PROgramming CONstruct (PROCON) metrics, and (ii) a novel Machine-Learning (ML)-based system, called Defect Estimator for Source Code (DESCo), that makes use of PROCON dataset for predicting defectiveness in a given scenario. The dataset was created by processing 30,400+ source files written in four popular programming languages, viz., C, C++, Java, and Python. The results of our experiments show that DESCo system outperforms one of the state-of-the-art methods with an improvement of 44.9%. To verify the correctness of our system, we compared the performance of 12 different ML algorithms with 50+ different combinations of their key parameters. Our system achieves the best results with SVM technique with a mean accuracy measure of 80.8%. © 2020 ACM.",AI in software engineering; automated software engineering; Maintaining software; software defect prediction; software faults and failures; software metrics; source code mining,Defects; Object oriented programming; Software design; Support vector machines; Accuracy measures; Ml algorithms; Software development and maintenances; Source code metrics; Source codes; Source files; State-of-the-art methods; C++ (programming language)
"On the monitoring of decentralized specifications: Semantics, properties, analysis, and simulation",2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079408292&doi=10.1145%2f3355181&partnerID=40&md5=bad25201ab214528d1b466b19c840ded,"We introduce two complementary approaches to monitor decentralized systems. The first approach relies on systems with a centralized specification, i.e., when the specification is written for the behavior of the entire system. To do so, our approach introduces a data structure that (i) keeps track of the execution of an automaton (ii) has predictable parameters and size, and (iii) guarantees strong eventual consistency. The second approach defines decentralized specifications wherein multiple specifications are provided for separate parts of the system.We study two properties of decentralized specifications pertaining to monitorability and compatibility between specification and architecture. We also present a general algorithm for monitoring decentralized specifications. We map three existing algorithms to our approaches and provide a framework for analyzing their behavior. Furthermore, we present THEMIS, a framework for designing such decentralized algorithms and simulating their behavior. We demonstrate the usage of THEMIS to compare multiple algorithms and validate the trends predicted by the analysis in two scenarios: a synthetic benchmark and the Chiron user interface. © 2020 Association for Computing Machinery.",Automata; Decentralized monitoring; Eventual consistency; Monitoring; Runtime verification; Simulation,Monitoring; Semantics; User interfaces; Automata; Decentralized monitoring; Eventual consistency; Run-time verification; Simulation; Specifications
How C++ templates are used for generic programming: An empirical study on 50 open source systems,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079374330&doi=10.1145%2f3356579&partnerID=40&md5=e750a16357b1c00a3afbb399c262698d,"Generic programming is a key paradigm for developing reusable software components. The inherent support for generic constructs is therefore important in programming languages. As for C++, the generic construct, templates, has been supported since the language was first released. However, little is currently known about how C++ templates are actually used in developing real software. In this study, we conduct an experiment to investigate the use of templates in practice. We analyze 1,267 historical revisions of 50 open source systems, consisting of 566 million lines of C++ code, to collect the data of the practical use of templates. We perform statistical analyses on the collected data and produce many interesting results. We uncover the following important findings: (1) templates are practically used to prevent code duplication, but this benefit is largely confined to a few highly used templates; (2) function templates do not effectively replace C-style generics, and developers with a C background do not show significant preference between the two language constructs; (3) developers seldom convert dynamic polymorphism to static polymorphism by using CRTP (Curiously Recursive Template Pattern); (4) the use of templates follows a power-law distribution in most cases, and C++ developers who prefer using templates are those without other language background; (5) C developer background seems to override C++ project guidelines. These findings are helpful not only for researchers to understand the tendency of template use but also for tool builders to implement better tools to support generic programming. © 2020 Royal Society of Chemistry. All rights reserved.",C++; Empirical study; Generic programming; Programming language; Template,Cesium; Codes (symbols); Computer programming languages; Computer software reusability; Open source software; Open systems; Polymorphism; Dynamic polymorphism; Empirical studies; Generic programming; Language constructs; Power law distribution; Recursive Templates; Reusable software components; Template; C++ (programming language)
Many-objective test suite generation for software product lines,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079396930&doi=10.1145%2f3361146&partnerID=40&md5=4ef646ab8609206125be8a31b229f543,"A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem.We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms. © 2020 Association for Computing Machinery.",Multi-objective optimisation; Software product line; Test prioritisation; Test selection,Multiobjective optimization; Software design; Testing; Domain knowledge; Evolution strategies; Feature modeling; Objective functions; Objective optimisation; Software Product Line; Software product lines; Test selection; Software testing
Toward better evolutionary program repair: An integrated approach,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079395447&doi=10.1145%2f3360004&partnerID=40&md5=47666a703e7421fdabc995668fd02041,"Bug repair is a major component of software maintenance, which requires a huge amount of manpower. Evolutionary computation, particularly genetic programming (GP), is a class of promising techniques for automating this time-consuming and expensive process. Although recent research in evolutionary program repair has made significant progress, major challenges still remain. In this article, we propose ARJA-e, a new evolutionary repair system for Java code that aims to address challenges for the search space, search algorithm, and patch overfitting. To determine a search space that is more likely to contain correct patches, ARJA-e combines two sources of fix ingredients (i.e., the statement-level redundancy assumption and repair templates) with contextual analysis-based search space reduction, thereby leveraging their complementary strengths. To encode patches in GP more properly, ARJA-e unifies the edits at different granularities into statement-level edits and then uses a lower-granularity patch representation that is characterized by the decoupling of statements for replacement and statements for insertion. ARJA-e also uses a finer-grained fitness function that canmake full use of semantic information contained in the test suite, which is expected to better guide the search of GP. To alleviate patch overfitting, ARJA-e further includes a postprocessing tool that can serve the purposes of overfit detection and patch ranking.We evaluate ARJA-e on 224 real Java bugs from Defects4J and compare it with the state-of-the-art repair techniques. The evaluation results show that ARJA-e can correctly fix 39 bugs in terms of the patches ranked first, achieving substantial performance improvements over the state of the art. In addition, we analyze the effect of the components of ARJA-e qualitatively and quantitatively to demonstrate their effectiveness and advantages. © 2020 Association for Computing Machinery.",Evolutionary computation; Genetic improvement; Genetic programming; Program repair,Evolutionary algorithms; Genetic algorithms; Genetic programming; Java programming language; Program debugging; Semantics; Contextual analysis; Different granularities; Evaluation results; Genetic improvements; Integrated approach; Search Algorithms; Search space reduction; Semantic information; Repair
Is static analysis able to identify unnecessary source code?,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079380240&doi=10.1145%2f3368267&partnerID=40&md5=c4662e32b71015bfdb483f48dab1bd43,"Grown software systems often contain code that is not necessary anymore. Such unnecessary code wastes resources during development and maintenance, for example, when preparing code for migration or certification. Running a profiler may reveal code that is not used in production, but it is often time-consuming to obtain representative data in this way. We investigate to what extent a static analysis approach, which is based on code stability and code centrality, is able to identify unnecessary code and whether its recommendations are relevant in practice. To study the feasibility and usefulness of our approach, we conducted a study involving 14 open-source and closedsource software systems. As there is no perfect oracle for unnecessary code, we compared recommendations for unnecessary code with historical cleanups, runtime usage data, and feedback from 25 developers of five software projects. Our study shows that recommendations generated from stability and centrality information point to unnecessary code that cannot be identified by dead code detectors. Developers confirmed that 34% of recommendations were indeed unnecessary and deleted 20% of the recommendations shortly after our interviews. Overall, our results suggest that static analysis can provide quick feedback on unnecessary code and is useful in practice. © 2020 Royal Society of Chemistry. All rights reserved.",Code centrality; Code stability; Unnecessary code,Codes (symbols); Feedback; Open source software; Open systems; Analysis approach; Code centrality; Open sources; Software project; Software systems; Source codes; Unnecessary code; Usage data; Static analysis
Automatically generating systemc code from HCSP formal models,2020,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079378578&doi=10.1145%2f3360002&partnerID=40&md5=8b2dc7e88d8883e622b7e70b688b321a,"In model-driven design of embedded systems, how to generate code from high-level control models seamlessly and correctly is challenging. This is because hybrid systems are involved with continuous evolution, discrete jumps, and the complicated entanglement between them, while code only contains discrete actions. In this article, we investigate the code generation from Hybrid Communicating Sequential Processes (HCSP), a formal hybrid control model, to SystemC. We first introduce the notion of approximate bisimulation as a criterion to check the consistency between two different systems, especially between the original control model and the final generated code. We prove that it is decidable whether two HCSPs are approximately bisimilar in bounded time and unbounded time with some conditions, respectively. For both the cases, we present two sets of rules correspondingly for discretizing HCSPs and prove that the original HCSP model and the corresponding discretization are approximately bisimilar. Furthermore, based on the discretization, we define a transformation function to map a discretized HCSP model to SystemC code such that they are also approximately bisimilar.We finally implement a tool to automatically realize the translation from HCSP to SystemC code and illustrate our approach through some case studies. © 2020 Royal Society of Chemistry. All rights reserved.",Approximate bisimulation; Code generation; Hybrid CSP,Computer operating procedures; Hybrid systems; Approximate bisimulation; Code Generation; Communicating sequential process; High level control; Hybrid controls; Hybrid CSP; Model driven design; Transformation functions; Embedded systems
Desen : Specification of sociotechnical systems via patterns of regulation and control,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077051977&doi=10.1145%2f3365664&partnerID=40&md5=3dc2058a3dc2246476712f6a6757ea50,"We address the problem of engineering a sociotechnical system (STS)with respect to its stakeholders' requirements. We motivate a two-tier STS conception composed of a technical tier that provides control mechanisms and describes what actions are allowed by the software components, and a social tier that characterizes the stakeholders' expectations of each other in terms of norms. We adopt agents as computational entities, each representing a different stakeholder. Unlike previous approaches, our framework, Desen, incorporates the social dimension into the formal verification process. Thus, Desen supports agents potentially violating applicable norms-a consequence of their autonomy. In addition to requirements verification, Desen supports refinement of STS specifications via design patterns to meet stated requirements.We evaluate Desen at three levels. We illustrate how Desen carries out refinement via the application of patterns on a hospital emergency scenario. We show via a human-subject study that a design process based on our patterns is helpful for participants who are inexperienced in conceptual modeling and norms. We provide an agent-based environment to simulate the hospital emergency scenario to compare STS specifications (including participant solutions from the human-subject study) with metrics indicating social welfare and norm compliance, and other domain dependent metrics. © 2019 Association for Computing Machinery.",Agent-oriented software engineering; Design patterns; Norms; Security and privacy requirements; Simulation,Formal verification; Hospitals; Specifications; Agent Oriented Software Engineering; Design Patterns; Norms; Security and privacy; Simulation; Autonomous agents
Recommending new features from mobile app descriptions,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075677304&doi=10.1145%2f3344158&partnerID=40&md5=a7c3c677c118f348baa233485de75b54,"The rapidly evolving mobile applications (apps) have brought great demand for developers to identify new features by inspecting the descriptions of similar apps and acquire missing features for their apps. Unfortunately, due to the huge number of apps, this manual process is time-consuming and unscalable. To help developers identify new features, we propose a new approach named SAFER. In this study, we first develop a tool to automatically extract features from app descriptions. Then, given an app, we leverage the topic model to identify its similar apps based on the extracted features and API names of apps. Finally, we design a feature recommendation algorithm to aggregate and recommend the features of identified similar apps to the specified app. Evaluated over a collection of 533 annotated features from 100 apps, SAFER achieves a Hit@15 score of up to 78.68% and outperforms the baseline approach KNN+ by 17.23% on average. In addition, we also compare SAFER against a typical technique of recommending features from user reviews, i.e., CLAP. Experimental results reveal that SAFER is superior to CLAP by 23.54% in terms of Hit@15. © 2019 ACM.",Domain analysis; Feature recommender system; Mobile applications; Topic model,Computer software; Software engineering; Domain analysis; Manual process; Missing features; Mobile applications; New approaches; Recommendation algorithms; Topic Modeling; User reviews; Mobile computing
Differential testing of certificate validation in ssl/tls implementations: an rfc-guided approach,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075552613&doi=10.1145%2f3355048&partnerID=40&md5=8844a90703ee418e25afa42324a52d6e,"Certificate validation in Secure Sockets Layer or Transport Layer Security protocol (SSL/TLS) is critical to Internet security. Thus, it is significant to check whether certificate validation in SSL/TLS implementations is correctly implemented. With this motivation,we propose a novel differential testing approach that is based on the standard Request for Comments (RFC). First, rules of certificates are extracted automatically from RFCs. Second, low-level test cases are generated through dynamic symbolic execution. Third, high-level test cases, i.e., certificates, are assembled automatically. Finally, with the assembled certificates being test cases, certificate validations in SSL/TLS implementations are tested to reveal latent vulnerabilities or bugs. Our approach named RFCcert has the following advantages: (1) certificates of RFCcert are discrepancy-Targeted, since they are assembled according to standards instead of genetics; (2) with the obtained certificates, RFCcert not only reveals the invalidity of traditional differential testing but also is able to conduct testing that traditional differential testing cannot do; and (3) the supporting tool of RFCcert has been implemented and extensive experiments show that the approach is effective in finding bugs of SSL/TLS implementations. In addition, by providing seed certificates for mutation approaches with RFCcert, the ability of mutation approaches in finding distinct discrepancies is significantly enhanced. © 2019 Association for Computing Machinery. All rights reserved.",certificate validation; Differential testing; dynamic symbolic execution; request for comments; SSL/TLS,Model checking; Certificate validations; Differential testing; Dynamic symbolic executions; Request for comments; SSL/TLS; Program debugging
Editorial,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075658426&doi=10.1145%2f3363297&partnerID=40&md5=f3ab4db6973ffe2b92d6a09e37182447,[No abstract available],,
Precise learn-to-rank fault localization using dynamic and static features of target programs,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074644798&doi=10.1145%2f3345628&partnerID=40&md5=de9a0e468968fbb226e032388768d5dd,"Finding the root cause of a bug requires a significant effort from developers. Automated fault localization techniques seek to reduce this cost by computing the suspiciousness scores (i.e., the likelihood of program entities being faulty). Existing techniques have been developed by utilizing input features of specific types for the computation of suspiciousness scores, such as program spectrum or mutation analysis results. This article presents a novel learn-to-rank fault localization technique called PRecise machINe-learning-based fault loCalization tEchnique (PRINCE). PRINCE uses genetic programming (GP) to combine multiple sets of localization input features that have been studied separately until now. For dynamic features, PRINCE encompasses both Spectrum Based Fault Localization (SBFL) and Mutation Based Fault Localization (MBFL) techniques. It also uses static features, such as dependency information and structural complexity of program entities. All such information is used by GP to train a ranking model for fault localization. The empirical evaluation on 65 real-world faults from CoREBench, 84 artificial faults from SIR, and 310 real-world faults from Defects4J shows that PRINCE outperforms the state-of-the-art SBFL, MBFL, and learn-to-rank techniques significantly. PRINCE localizes a fault after reviewing 2.4% of the executed statements on average (4.2 and 3.0 times more precise than the best of the compared SBFL and MBFL techniques, respectively). Also, PRINCE ranks 52.9% of the target faults within the top ten suspicious statements. © 2019 ACM.",Fault localization; Machine learning; Mutation analysis; Source file characteristics,Genetic algorithms; Learning algorithms; Learning systems; Machine learning; Software testing; Dependency informations; Dynamic features; Empirical evaluations; Fault localization; Mutation analysis; Source files; State of the art; Structural complexity; Genetic programming
An empirical study on learning bug-fixing patches in the wild via neural machine translation,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071901882&doi=10.1145%2f3340544&partnerID=40&md5=40a2dcc5e2aa1eb452ce7b108e1bebbf,"Millions of open source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub in order to extract meaningful examples of such bugfixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation, we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9-50% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second. © 2019 Association for Computing Machinery.",Bug-fixes; Neural machine translation,Codes (symbols); Computational linguistics; Computer aided language translation; Open source software; Open systems; Program debugging; Software design; Trees (mathematics); Abstract Syntax Trees; Bug fixes; Development history; Empirical investigation; Empirical studies; Encoder-decoder; Machine translations; Open source projects; Learning systems
The virtual developer: Integrating code generation and manual development with conflict resolution,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071944902&doi=10.1145%2f3340545&partnerID=40&md5=8b6784c0e315c06393d66374ba4445c4,"Model Driven Development (MDD) requires proper tools to derive the implementation code from the application models. However, the integration of handwritten and generated code is a long-standing issue that affects the adoption of MDD in the industry. This article presents a model and code co-evolution approach that addresses such a problem a posteriori, using the standard collision detection capabilities of Version Control Systems to support the semi-automatic merge of the two types of code. We assess the proposed approach by contrasting it with the more traditional template-based, forward-engineering process, adopted by most MDD tools. © 2019 Association for Computing Machinery.",Agile development; Code generation; Model driven development,Computer software; Software engineering; Agile development; Application models; Code Generation; Collision detection; Conflict Resolution; Forward engineerings; Model driven development; Version control system; Codes (symbols)
Automated reuse of model transformations through typing requirements models,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071907716&doi=10.1145%2f3340108&partnerID=40&md5=0e7a03a44e78e6bfbd8d83de5326e1f5,"Model transformations are key elements of model-driven engineering, where they are used to automate the manipulation of models. However, they are typed with respect to concrete source and target meta-models, making their reuse for other (even similar) meta-models challenging. To improve this situation, we propose capturing the typing requirements for reusing a transformation with other meta-models by the notion of a typing requirements model (TRM). A TRM describes the prerequisites that amodel transformation imposes on the source and targetmeta-models to obtain a correct typing. The key observation is that any meta-model pair that satisfies the TRM is a valid reuse context for the transformation at hand. A TRM is made of two domain requirement models (DRMs) describing the requirements for the source and target meta-models, and a compatibility model expressing dependencies between them. We define a notion of refinement between DRMs and see meta-models as a special case of DRM. We provide a catalogue of valid refinements and describe how to automatically extract a TRM from an ATL transformation. The approach is supported by our tool TOTEM. We report on two experiments-based on transformations developed by third parties and meta-model mutation techniques-validating the correctness and completeness of our TRM extraction procedure and confirming the power of TRMs to encode variability and support flexible reuse. © 2019 Association for Computing Machinery.",ATL; Meta-modelling; Model transformation; Model transformation reuse; Refinement,Computer software; Software engineering; Extraction procedure; Flexible reuse; Meta-modelling; Model transformation; Model-driven Engineering; Refinement; Requirements Models; Third parties; Object oriented programming
Verifying and quantifying side-channel resistance of masked software implementations,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075681115&doi=10.1145%2f3330392&partnerID=40&md5=76e2a8e79ccc475289789f3cf9802285,"Power side-channel attacks, capable of deducing secret data using statistical analysis, have become a serious threat. Random masking is a widely used countermeasure for removing the statistical dependence between secret data and side-channel information. Although there are techniques for verifying whether a piece of software code is perfectly masked, they are limited in accuracy and scalability. To bridge this gap, we propose a refinement-based method for verifying masking countermeasures. Our method is more accurate than prior type-inference-based approaches and more scalable than prior model-counting-based approaches using SAT or SMT solvers. Indeed, our method can be viewed as a gradual refinement of a set of type-inference rules for reasoning about distribution types. These rules are kept abstract initially to allow fast deduction and then made concrete when the abstract version is not able to resolve the verification problem. We also propose algorithms for quantifying the amount of side-channel information leakage from a software implementation using the notion of quantitative masking strength. We have implemented our method in a software tool and evaluated it on cryptographic benchmarks including AES and MAC-Keccak. The experimental results show that our method significantly outperforms state-of-the-art techniques in terms of accuracy and scalability. © 2019 ACM.",AES; Cryptographic software; Differential power analysis; MAC-Keccak; Perfect masking; Quantitative masking strength; Satisfiability modulo theory (SMT); Type inference,Computer software; Scalability; Cryptographic software; Differential power Analysis; MAC-Keccak; Quantitative masking strength; Satisfiability modulo Theories; Type inferences; Side channel attack
Efficient verification of concurrent systems using synchronisation analysis and sat/smt solving,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075556871&doi=10.1145%2f3335149&partnerID=40&md5=1f54c61b2f3a58339d19d769f09ab089,"This article investigates how the use of approximations can make the formal verification of concurrent systems scalable.We propose the idea of synchronisation analysis to automatically capture global invariants and approximate reachability.We calculate invariants on how components participate on global system synchronisations and use a notion of consistency between these invariants to establish whether components can effectively communicate to reach some system state. Our synchronisation-Analysis techniques try to show either that a system state is unreachable by demonstrating that components cannot agree on the order they participate in system rules or that a system state is unreachable by demonstrating components cannot agree on the number of times they participate on system rules. These fully automatic techniques are applied to check deadlock and local-deadlock freedom in the PairStatic framework. It extends Pair (a recent framework where we use pure pairwise analysis of components and SAT checkers to check deadlock and local-deadlock freedom) with techniques to carry out synchronisation analysis. So, not only can it compute the same local invariants that Pair does, it can leverage global invariants found by synchronisation analysis, thereby improving the reachability approximation and tightening our verifications.We implement PairStatic in our DeadlOx tool using SAT/SMT and demonstrate the improvements they create in checking (local) deadlock freedom. © 2019 Association for Computing Machinery. All rights reserved.",Approximate verification; Concurrent systems; Deadlock freedom; Invariants; Local-deadlock freedom; Reachability approximation; SAT solving; SMT solving; Synchronisation analysis,Formal verification; Approximate verification; Concurrent systems; Deadlock freedom; Invariants; Reachability; SAT-solving; Synchronization
Neural network-based detection of self-Admitted technical debt: From performance to explainability,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075020632&doi=10.1145%2f3324916&partnerID=40&md5=999bb85d4d49055dd77ca73552f91fe7,"Technical debt is a metaphor to reflect the tradeoff software engineers make between short-Term benefits and long-Term stability. Self-Admitted technical debt (SATD), a variant of technical debt, has been proposed to identify debt that is intentionally introduced during software development, e.g., temporary fixes and workarounds. Previous studies have leveraged human-summarized patterns (which represent n-gram phrases that can be used to identify SATD) or text-mining techniques to detect SATD in source code comments. However, several characteristics of SATD features in code comments, such as vocabulary diversity, project uniqueness, length, and semantic variations, pose a big challenge to the accuracy of pattern or traditional text-mining-based SATD detection, especially for cross-project deployment. Furthermore, although traditional text-mining-based method outperforms pattern-based method in prediction accuracy, the text features it uses are less intuitive than human-summarized patterns, which makes the prediction results hard to explain. To improve the accuracy of SATD prediction, especially for cross-project prediction, we propose a Convolutional Neural Network (CNN) based approach for classifying code comments as SATD or non-SATD. To improve the explainability of our model's prediction results, we exploit the computational structure of CNNs to identify key phrases and patterns in code comments that are most relevant to SATD. We have conducted an extensive set of experiments with 62,566 code comments from 10 open-source projects and a user study with 150 comments of another three projects. Our evaluation confirms the effectiveness of different aspects of our approach and its superior performance, generalizability, adaptability, and explainability over current state-of-The-Art traditional text-mining-based methods for SATD classification. © 2019 Association for Computing Machinery. All rights reserved.",Convolutional neural network; cross project prediction; Model adaptability; model explainability; Model generalizability; Self-Admitted technical debt,Convolution; Data mining; Forecasting; Natural language processing systems; Neural networks; Open source software; Open systems; Semantics; Software design; Computational structure; Convolutional neural network; Long term stability; Open source projects; Pattern based method; Source code comments; Technical debts; Text mining techniques; Text processing
Theoretical and practical aspects of linking operational and algebraic semantics for MDESL,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075701781&doi=10.1145%2f3295699&partnerID=40&md5=8a92f127686af6b96dc4ee901481f62f,"Verilog is a hardware description language (HDL) that has been standardized and widely used in industry. Multithreaded discrete event simulation language (MDESL) is a Verilog-like language. It contains interesting features such as event-driven computation and shared-variable concurrency. This article considers how the algebraic semantics links with the operational semantics for MDESL. Our approach is from both the theoretical and practical aspects. The link is proceeded by deriving the operational semantics from the algebraic semantics. First, we present the algebraic semantics for MDESL. We introduce the concept of head normal form. Second, we present the strategy of deriving operational semantics from algebraic semantics.We also investigate the soundness and completeness of the derived operational semantics with respect to the derivation strategy. Our theoretical approach is complemented by a practical one, and we use the theorem proof assistant Coq to formalize the algebraic laws and the derived operational semantics. Meanwhile, the soundness and completeness of the derived operational semantics is also verified via the mechanical approach in Coq. Our approach is a novel way to formalize and verify the correctness and equivalence of different semantics for MDESL in both a theoretical approach and a practical approach. © 2019 ACM.",Coq; Multithreaded discrete event simulation language; Semantics relating; Unifying theories of programming (UTP); Verilog,Algebra; Computer simulation languages; Discrete event simulation; Semantics; Theorem proving; Algebraic semantic; Multithreaded; Operational semantics; Proof assistant; Shared variables; Soundness and completeness; Theoretical approach; Unifying Theories of Programming; Computer hardware description languages
Automated n-way program merging for facilitating family-based analyses of variant-rich software,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075666296&doi=10.1145%2f3313789&partnerID=40&md5=99a6e17f18105eebf1eb5d4a4c9ef70a,"Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-assurance techniques to such variant-rich programs, as compared to variant-by-variant approaches. Unfortunately, these strategies require a single program representation superimposing all program variants in a syntactically well-formed, semantically sound, and variant-preserving manner, which is usually not available and manually hard to obtain in practice. In this article, we present a novel methodology, called SiMPOSE, for automatically generating superimpositions of existing program variants to facilitate family-based analyses of variant-rich software. To this end, we propose a novel N-way model-merging methodology to integrate the control-flow automaton (CFA) representations of N given variants of a C program into one unified CFArepresentation. CFAconstitute a unified program abstraction used by many recent software-analysis tools for automated quality assurance. To cope with the inherent complexity of N-way model-merging, our approach (1) utilizes principles of similarity-propagation to reduce the number of potential N-way matches, and (2) enables us to decompose a set of N variants into arbitrary subsets and to incrementally derive an N-way superimposition from partial superimpositions.We apply our tool implementation of SiMPOSE to a selection of realistic C programs, frequently considered for experimental evaluation of program-analysis techniques. In particular, we investigate applicability and efficiency/effectiveness tradeoffs of our approach by applying SiMPOSE in the context of family-based unit-test generation as well as model-checking as sample program-analysis techniques. Our experimental results reveal very impressive efficiency improvements by an average factor of up to 2.6 for test-generation and up to 2.4 for model-checking under stable effectiveness, as compared to variant-by-variant approaches, thus amortizing the additional effort required for merging. In addition, our results show that merging all N variants at once produces, in almost all cases, clearly more precise results than incremental step-wise 2-way merging. Finally, our comparison with major existing N-way merging techniques shows that SiMPOSE constitutes, in most cases, the best efficiency/effectiveness trade-off. © 2019 ACM.",Control flow automata; Model matching; Program merging; Quality assurance; Variability encoding,Economic and social effects; Efficiency; Merging; Model checking; Quality assurance; Quality control; Software testing; Control flows; Efficiency improvement; Experimental evaluation; Improving efficiency; Model matching; Program representations; Similarity propagation; Unit test generations; C (programming language)
Editorial,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068477538&doi=10.1145%2f3339833&partnerID=40&md5=b7cf72b0ee8641838953ce74b2263a5e,[No abstract available],,
Developing and evaluating objective termination criteria for random testing,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074605635&doi=10.1145%2f3339836&partnerID=40&md5=17b2c0bc18cfe0ee171984fd566b98b2,"Random testing is a software testing technique through which programs are tested by generating and executing random inputs. Because of its unstructured nature, it is difficult to determine when to stop a random testing process. Faults may be missed if the process is stopped prematurely, and resources may be wasted if the process is run too long. In this article, we propose two promising termination criteria, “All Equivalent” (AEQ) and “All Included in One” (AIO), applicable to random testing. These criteria stop random testing once the process has reached a code-coverage-based saturation point after which additional testing effort is unlikely to provide additional effectiveness. We model and implement them in the context of a general random testing process composed of independent random testing sessions. Thirty-six experiments involving GUI testing and unit testing of Java applications have demonstrated that the AEQ criteria is generally able to stop the process when a code coverage equal or very near to the saturation level is reached, while AIO is able to stop the process earlier in cases it reaches the saturation level of coverage. In addition, the performance of the two criteria has been compared against other termination criteria adopted in the literature. © 2019 Association for Computing Machinery.",Code coverage; Random testing; Saturation effect,Codes (symbols); Testing; Code coverage; Java applications; Random testing; Saturation effects; Saturation levels; Saturation point; Software testing techniques; Termination criteria; Software testing
Runtime fault detection in programmed molecular systems,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065758452&doi=10.1145%2f3295740&partnerID=40&md5=c0fae5f7e63f58b824cf23118a205ea6,"Watchdog timers are devices that are commonly used to monitor the health of safety-critical hardware and software systems. Their primary function is to raise an alarm if the monitored systems fail to emit periodic “heartbeats” that signal their well-being. In this article, we design and verify a molecular watchdog timer for monitoring the health of programmed molecular nanosystems. This raises new challenges, because our molecular watchdog timer and the system that it monitors both operate in the probabilistic environment of chemical kinetics, where many failures are certain to occur and it is especially hard to detect the absence of a signal. Our molecular watchdog timer is the result of an incremental design process that uses goal-oriented requirements engineering, simulation, stochastic analysis, and software verification tools. We demonstrate the molecular watchdog's functionality by having it monitor a molecular oscillator. Both the molecular watchdog timer and the oscillator are implemented as chemical reaction networks, which are the current programming language of choice for many molecular programming applications. © 2019 Association for Computing Machinery.",Chemical reaction networks; Molecular programming; Molecular system safety; Probabilistic model checking; Requirements engineering,Alarm systems; Chemical reactions; Computer aided software engineering; Computer software; Fault detection; Model checking; Nanosystems; Requirements engineering; Safety engineering; Stochastic systems; Chemical reaction networks; Goal-oriented requirements engineering; Incremental design process; Molecular programming; Molecular systems; Probabilistic model checking; Safety-critical hardware; Software verification tools; Computer systems programming
"Domain analysis and description principles, techniques, and modelling languages",2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065778713&doi=10.1145%2f3295738&partnerID=40&md5=b800d50d19182fb8fbdf45ed2140dcb2,"Domain science and engineering marks a new area of computing science. Just as we are formalising the syntax and semantics of programming languages, so we are formalising the syntax and semantics of human-assisted domains. Just as physicists are studying the natural physical world, endowing it with mathematical models, so we, computing scientists, are studying these domains, endowing them with mathematical models, A difference between the endeavours of physicists and ours lies in the tools: The physics models are based on classical mathematics, differential equations and integrals, and so on; our models are based on mathematical logic, set theory, and algebra [1]. Where physicists thus classically use a variety of differential and integral calculi to model the physical world, we shall be using the analysis and description calculi presented in this article to model primarily artifactual domains. © 2019 Copyright held by the owner/author(s).",Domain analysis and description calculi; Domain engineering; Transcendental deduction,Biomineralization; Computation theory; Differential equations; Differentiation (calculus); Pathology; Semantics; Syntactics; Computing science; Computing scientists; Domain analysis; Domain engineering; Physics models; Science and engineering; Semantics of programming languages; Transcendental deduction; Modeling languages
Editorial,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065799666&doi=10.1145%2f3317953&partnerID=40&md5=4bdc5590ea993c16c37f404b67a788ff,[No abstract available],,
Editorial from the incoming editor-in-chief,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062327029&doi=10.1145%2f3301290&partnerID=40&md5=63d79dbe2b90d88914b86ce6e452f68f,[No abstract available],,
Understanding and analyzing Java reflection,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062349093&doi=10.1145%2f3295739&partnerID=40&md5=2ed945abd8fd7f584beeb57c666a7127,"Java reflection has been widely used in a variety of applications and frameworks. It allows a software system to inspect and change the behaviour of its classes, interfaces, methods, and fields at runtime, enabling the software to adapt to dynamically changing runtime environments. However, this dynamic language feature imposes significant challenges to static analysis, because the behaviour of reflection-rich software is logically complex and statically hard to predict. As a result, existing static analysis tools either ignore reflection or handle it partially, resulting in missed, important behaviours, i.e., unsound results. Therefore, improving or even achieving soundness in static reflection analysis-an analysis that infers statically the behaviour of reflective code-will provide significant benefits to many analysis clients, such as bug detectors, security analyzers, and program verifiers. In this article, we provide a comprehensive understanding of Java reflection through examining its underlying concept, API, and real-world usage, and, building on this, we introduce a new static approach to resolving Java reflection effectively in practice. We have implemented our reflection analysis in an open-source tool, called Solar, and evaluated its effectiveness extensively with large Java programs and libraries. Our experimental results demonstrate that Solar is able to (1) resolve reflection more soundly than the state-of-the-art reflection analyses; (2) automatically and accurately identify the parts of the program where reflection is resolved unsoundly or imprecisely; and (3) guide users to iteratively refine the analysis results by using lightweight annotations until their specific requirements are satisfied. © 2019 Association for Computing Machinery.",Java reflection; Points-to analysis; Reflection analysis; Static analysis,Iterative methods; Open source software; Static analysis; Dynamic languages; Java reflections; Open source tools; Points-to analysis; Program verifiers; Reflection analysis; Runtime environments; Security analyzers; Java programming language
Isolation modeling and analysis based on mobility,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062363607&doi=10.1145%2f3306606&partnerID=40&md5=1b0c154792cf7b33010992a5b44afa06,"In a mobile system, mobility refers to a change in position of a mobile object with respect to time and its reference point, whereas isolation means the isolation relationship between mobile objects under some scheduling policies. Inspired by event-based formal models and the ambient calculus, we first propose the two types of special events, entering and exiting an ambient, as movement events to model and analyze mobility. Based on mobility, we then introduce the notion of the isolation of mobile objects for ambients. To ensure the isolation, a priority policy needs to be used to schedule the movement of mobile objects. However, traditional scheduling policies focus on task scheduling and depend on the strong hypothesis: The scheduled tasks are independent-that is, the scheduled tasks do not affect each other. In a practical mobile system, mobile objects and ambients interact with each other. It is difficult to separate a mobile system into independent tasks. We finally present an automatic approach for generating a priority scheduling policy without considering the preceding assumption. The approach can guarantee the isolation of the mobile objects for ambients in a mobile system. Experiments demonstrate these results. © 2019 Association for Computing Machinery.",Dependency structure; Isolation; Mobility; Scheduling policy,Calculations; Carrier mobility; Differentiation (calculus); Automatic approaches; Dependency structures; Independent tasks; Isolation; Model and analysis; Priority policies; Priority scheduling; Scheduling policies; Scheduling
Farewell editorial from the outgoing editor-in-chief,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062344577&doi=10.1145%2f3301288&partnerID=40&md5=325d1e8edeb9151fc1df4f5425533c7a,[No abstract available],,
How understandable are pattern-based behavioral constraints for novice software designers?,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062353209&doi=10.1145%2f3306608&partnerID=40&md5=f688078fc2651ca6305efe45e072e61a,"This article reports a controlled experiment with 116 participants on the understandability of representative graphical and textual pattern-based behavioral constraint representations from the viewpoint of novice software designers. Particularly, graphical and textual behavioral constraint patterns present in the declarative business process language Declare and textual behavioral constraints based on Property Specification Patterns are the subjects of this study. In addition to measuring the understandability construct, this study assesses subjective aspects such as perceived difficulties regarding learning and application of the tested approaches. An interesting finding of this study is the overall low achieved correctness in the experimental tasks, which seems to indicate that pattern-based behavioral constraint representations are hard to understand for novice software designers in the absence of additional supportive measures. The results of the descriptive statistics regarding achieved correctness are slightly in favor of the textual representations, but the inference statistics do not indicate any significant differences in terms of understandability between graphical and textual behavioral constraint representations. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Behavioral constraints; Controlled experiment; Declarative business processes; Property specification patterns; Understandability,Computer software; Software engineering; Behavioral constraints; Business Process; Controlled experiment; Property Specification; Understandability; Specifications
Status quo in requirements engineering: A theory and a global family of surveys,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062345543&doi=10.1145%2f3306607&partnerID=40&md5=9573c309d0e6a610ba68a59b8d480dd4,"Requirements Engineering (RE) has established itself as a software engineering discipline over the past decades. While researchers have been investigating the RE discipline with a plethora of empirical studies, attempts to systematically derive an empirical theory in context of the RE discipline have just recently been started. However, such a theory is needed if we are to define and motivate guidance in performing high quality RE research and practice. We aim at providing an empirical and externally valid foundation for a theory of RE practice, which helps software engineers establish effective and efficient RE processes in a problem-driven manner. We designed a survey instrument and an engineer-focused theory that was first piloted in Germany and, after making substantial modifications, has now been replicated in 10 countries worldwide. We have a theory in the form of a set of propositions inferred from our experiences and available studies, as well as the results from our pilot study in Germany. We evaluate the propositions with bootstrapped confidence intervals and derive potential explanations for the propositions. In this article, we report on the design of the family of surveys, its underlying theory, and the full results obtained from the replication studies conducted in 10 countries with participants from 228 organisations. Our results represent a substantial step forward towards developing an empirical theory of RE practice. The results reveal, for example, that there are no strong differences between organisations in different countries and regions, that interviews, facilitated meetings and prototyping are the most used elicitation techniques, that requirements are often documented textually, that traces between requirements and code or design documents are common, that requirements specifications themselves are rarely changed and that requirements engineering (process) improvement endeavours are mostly internally driven. Our study establishes a theory that can be used as starting point for many further studies for more detailed investigations. Practitioners can use the results as theory-supported guidance on selecting suitable RE methods and techniques. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Replication; Requirements engineering; Survey research; Theory,Requirements engineering; Software engineering; Confidence interval; Elicitation techniques; Engineering disciplines; Replication; Requirements specifications; Survey instruments; Survey research; Theory; Surveys
The state of empirical evaluation in static feature location,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060890866&doi=10.1145%2f3280988&partnerID=40&md5=7b90f98a5169d43d48348e443e28867c,"Feature location (FL) is the task of finding the source code that implements a specific, user-observable functionality in a software system. It plays a key role in many software maintenance tasks and a wide variety of Feature Location Techniques (FLTs), which rely on source code structure or textual analysis, have been proposed by researchers. As FLTs evolve and more novel FLTs are introduced, it is important to perform comparison studies to investigate “Which are the best FLTs?” However, an initial reading of the literature suggests that performing such comparisons would be an arduous process, based on the large number of techniques to be compared, the heterogeneous nature of the empirical designs, and the lack of transparency in the literature. This article presents a systematic review of 170 FLT articles, published between the years 2000 and 2015. Results of the systematic review indicate that 95% of the articles studied are directed towards novelty, in that they propose a novel FLT. Sixty-nine percent of these novel FLTs are evaluated through standard empirical methods but, of those, only 9% use baseline technique(s) in their evaluations to allow cross comparison with other techniques. The heterogeneity of empirical evaluation is also clearly apparent: altogether, over 60 different FLT evaluation metrics are used across the 170 articles, 272 subject systems have been used, and 235 different benchmarks employed. The review also identifies numerous user input formats as contributing to the heterogeneity. Analysis of the existing research also suggests that only 27% of the FLTs presented might be reproduced from the published material. These findings suggest that comparison across the existing body of FLT evaluations is very difficult. We conclude by providing guidelines for empirical evaluation of FLTs that may ultimately help to standardise empirical research in the field, cognisant of FLTs with different goals, leveraging common practices in existing empirical evaluations and allied with rationalisations. This is seen as a step towards standardising evaluation in the field, thus facilitating comparison across FLTs. © 2018 Association for Computing Machinery.",And Phrases: Feature location; Bug location; Concept location; Requirement traceability,Computer software; Software engineering; Concept locations; Empirical evaluations; Empirical research; Evaluation metrics; Feature location; Requirement traceabilitys; Software-maintenance tasks; Systematic Review; Location
RESTful API automated test case generation with Evomaster,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060893585&doi=10.1145%2f3293455&partnerID=40&md5=14353befe9bb34f5bac0dc903d37e2d8,"RESTful APIs are widespread in industry, especially in enterprise applications developed with a microservice architecture. A RESTful web service will provide data via an API over the network using HTTP, possibly interacting with databases and other web services. Testing a RESTful API poses challenges, because inputs/outputs are sequences of HTTP requests/responses to a remote server. Many approaches in the literature do black-box testing, because often the tested API is a remote service whose code is not available. In this article, we consider testing from the point of view of the developers, who have full access to the code that they are writing. Therefore, we propose a fully automated white-box testing approach, where test cases are automatically generated using an evolutionary algorithm. Tests are rewarded based on code coverage and fault-finding metrics. However, REST is not a protocol but rather a set of guidelines on how to design resources accessed over HTTP endpoints. For example, there are guidelines on how related resources should be structured with hierarchical URIs and how the different HTTP verbs should be used to represent well-defined actions on those resources. Test-case generation for RESTful APIs that only rely on white-box information of the source code might not be able to identify how to create prerequisite resources needed before being able to test some of the REST endpoints. Smart sampling techniques that exploit the knowledge of best practices in RESTful API design are needed to generate tests with predefined structures to speed up the search. We implemented our technique in a tool called EvoMaster, which is open source. Experiments on five open-source, yet non-trivial, RESTful services show that our novel technique automatically found 80 real bugs in those applications. However, obtained code coverage is lower than the one achieved by the manually written test suites already existing in those services. Research directions on how to further improve such an approach are therefore discussed, such as the handling of SQL databases. © 2019 Association for Computing Machinery.",REST; Software engineering; Testing; Web service,Automatic test pattern generation; Codes (symbols); HTTP; Hypertext systems; Open source software; Program debugging; Software engineering; Testing; Web services; Websites; Automated test case generation; Automatically generated; Enterprise applications; REST; RESTful Services; RESTful Web services; Test case generation; White-box testing; Black-box testing
An active learning approach for improving the accuracy of automated domain model extraction,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060897284&doi=10.1145%2f3293454&partnerID=40&md5=e58d0ed1ca0337597a95b6a571301108,"Domain models are a useful vehicle for making the interpretation and elaboration of natural-language requirements more precise. Advances in natural-language processing (NLP) have made it possible to automatically extract from requirements most of the information that is relevant to domain model construction. However, alongside the relevant information, NLP extracts from requirements a significant amount of information that is superfluous (not relevant to the domain model). Our objective in this article is to develop automated assistance for filtering the superfluous information extracted by NLP during domain model extraction. To this end, we devise an active-learning-based approach that iteratively learns from analysts’ feedback over the relevance and superfluousness of the extracted domain model elements and uses this feedback to provide recommendations for filtering superfluous elements. We empirically evaluate our approach over three industrial case studies. Our results indicate that, once trained, our approach automatically detects an average of ≈45% of the superfluous elements with a precision of ≈96%. Since precision is very high, the automatic recommendations made by our approach are trustworthy. Consequently, analysts can dispose of a considerable fraction – nearly half – of the superfluous elements with minimal manual work. The results are particularly promising, as they should be considered in light of the non-negligible subjectivity that is inherently tied to the notion of relevance. © 2019 Association for Computing Machinery.",Active learning; Case study research; Domain modeling; Natural-language requirements; Requirements engineering,Artificial intelligence; Extraction; Learning algorithms; Modeling languages; Natural language processing systems; Requirements engineering; Active Learning; Amount of information; Automated assistance; Case study research; Domain model; Industrial case study; NAtural language processing; Natural language requirements; Information filtering
Software effort interval prediction via Bayesian inference and synthetic bootstrap resampling,2019,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060873241&doi=10.1145%2f3295700&partnerID=40&md5=4c48c686e0bdb1efcfff5a6220d6d2ec,"Software effort estimation (SEE) usually suffers from inherent uncertainty arising from predictive model limitations and data noise. Relying on point estimation only may ignore the uncertain factors and lead project managers (PMs) to wrong decision making. Prediction intervals (PIs) with confidence levels (CLs) present a more reasonable representation of reality, potentially helping PMs to make better-informed decisions and enable more flexibility in these decisions. However, existing methods for PIs either have strong limitations or are unable to provide informative PIs. To develop a “better” effort predictor, we propose a novel PI estimator called Synthetic Bootstrap ensemble of Relevance Vector Machines (SynB-RVM) that adopts Bootstrap resampling to produce multiple RVM models based on modified training bags whose replicated data projects are replaced by their synthetic counterparts. We then provide three ways to assemble those RVM models into a final probabilistic effort predictor, from which PIs with CLs can be generated. When used as a point estimator, SynB-RVM can either significantly outperform or have similar performance compared with other investigated methods. When used as an uncertain predictor, SynB-RVM can achieve significantly narrower PIs compared to its base learner RVM. Its hit rates and relative widths are no worse than the other compared methods that can provide uncertain estimation. © 2019 Association for Computing Machinery.",Bootstrap resampling; Ensemble learning; Prediction intervals with confidence levels; Relevance vector machine; Software effort estimation; Software risk management; Synthetic replacement; Uncertain effort estimation,Bayesian networks; Forecasting; Inference engines; Personnel training; Risk management; Bootstrap resampling; Confidence levels; Effort Estimation; Ensemble learning; Relevance Vector Machine; Software effort estimation; Software risk management; Synthetic replacement; Risk perception
Refactoring multi-level models,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060848726&doi=10.1145%2f3280985&partnerID=40&md5=f7cfd5e59823bde30cca69286f63feb4,"Multi-level modelling promotes flexibility in modelling by enabling the use of several meta-levels instead of just two, as is the case in mainstream two-level modelling approaches. While this approach leads to simpler models for some scenarios, it introduces an additional degree of freedom as designers can decide the meta-level where an element should reside, having to ascertain the suitability of such decisions. In this respect, model refactorings have been successfully applied in the context of two-level modelling to rearrange the elements of a model while preserving its meaning. Following this idea, we propose a catalogue of 17 novel refactorings specific to multi-level models. Their objective is to help designers in rearranging elements across and within meta-levels and exploring the consequences. In this article, we detail each refactoring in the catalogue, show a classification across different dimensions, and describe the support we provide in our MetaDepth tool. We present two experiments to assess two aspects of our refactorings. The first one validates the predicted semantic side effects of the refactorings on the basis of more than 210.000 refactoring applications. The second one measures the impact of refactorings on three quality attributes of multi-level models. © 2018 Association for Computing Machinery.",And Phrases: Meta-modelling; MetaDepth; Model refactoring; Multi-level modelling,Pinch effect; Semantics; Degree of freedom; Meta-modelling; MetaDepth; Multilevel model; Multilevels; Quality attributes; Refactorings; Two-level modelling; Degrees of freedom (mechanics)
Variability-aware static analysis at scale: An empirical study,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060876505&doi=10.1145%2f3280986&partnerID=40&md5=647a73472a8ed63499cc8ae7bfcb61dc,"The advent of variability management and generator technology enables users to derive individual system variants from a configurable code base by selecting desired configuration options. This approach gives rise to the generation of possibly billions of variants, which, however, cannot be efficiently analyzed for bugs and other properties with classic analysis techniques. To address this issue, researchers and practitioners have developed sampling heuristics and, recently, variability-aware analysis techniques. While sampling reduces the analysis effort significantly, the information obtained is necessarily incomplete, and it is unknown whether state-of-the-art sampling techniques scale to billions of variants. Variability-aware analysis techniques process the configurable code base directly, exploiting similarities among individual variants with the goal of reducing analysis effort. However, while being promising, so far, variability-aware analysis techniques have been applied mostly only to small academic examples. To learn about the mutual strengths and weaknesses of variability-aware and sample-based static-analysis techniques, we compared the two by means of seven concrete control-flow and data-flow analyses, applied to five real-world subject systems: Busybox, OpenSSL, SQLite, the x86 Linux kernel, and uClibc. In particular, we compare the efficiency (analysis execution time) of the static analyses and their effectiveness (potential bugs found). Overall, we found that variability-aware analysis outperforms most sample-based static-analysis techniques with respect to efficiency and effectiveness. For example, checking all variants of OpenSSL with a variability-aware static analysis is faster than checking even only two variants with an analysis that does not exploit similarities among variants. © 2018 Association for Computing Machinery.",And Phrases: Highly configurable systems; Configuration sampling; TypeChef; Variability-aware analysis,Computer operating systems; Efficiency; Static analysis; Analysis techniques; Configurable systems; Configuration options; Configuration sampling; Sampling technique; TypeChef; Variability management; Variability-Aware; Data flow analysis
Oracles for testing software timeliness with uncertainty,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057183986&doi=10.1145%2f3280987&partnerID=40&md5=a4316b47a3ea78a5f9085c6f30bcc181,"Uncertainty in timing properties (e.g., detection time of external events) is a common occurrence in embedded software systems, since these systems interact with complex physical environments. Such time uncertainty leads to non-determinism. For example, time-triggered operations may either generate different valid outputs across different executions or experience failures (e.g., results not being generated in the expected time window) that occur only occasionally over many executions. For these reasons, time uncertainty makes the generation of effective test oracles for timing requirements a challenging task. To address the above challenge, we propose Stochastic Testing with Unique Input Output Sequences, an approach for the automated generation of stochastic oracles that verify the capability of a software system to fulfill timing constraints in the presence of time uncertainty. Such stochastic oracles entail the statistical analysis of repeated test case executions based on test output probabilities predicted by means of statistical model checking. Results from two industrial case studies in the automotive domain demonstrate that this approach improves the fault detection effectiveness of tests suites derived from timed automata compared to traditional approaches. © 2018 Association for Computing Machinery.",Probabilistic unique input output sequences; Test oracles generation; Time uncertainty; Timing specifications,Embedded systems; Fault detection; Model checking; Stochastic models; Stochastic systems; Testing; Embedded software systems; Fault detection effectiveness; Input-output; Statistical model checking; Test oracles; Time uncertainty; Timing specifications; Traditional approaches; Software testing
Recommending who to follow in the software engineering Twitter space,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060513044&doi=10.1145%2f3266426&partnerID=40&md5=9e68112c2f2d01ddf79048e7ff21deec,"With the advent of social media, developers are increasingly using it in their software development activities. Twitter is one of the popular social mediums used by developers. A recent study by Singer et al. found that software developers use Twitter to “keep up with the fast-paced development landscape.” Unfortunately, due to the general-purpose nature of Twitter, it’s challenging for developers to use Twitter for their development activities. Our survey with 36 developers who use Twitter in their development activities highlights that developers are interested in following specialized software gurus who share relevant technical tweets. To help developers perform this task, in this work we propose a recommendation system to identify specialized software gurus. Our approach first extracts different kinds of features that characterize a Twitter user and then employs a two-stage classification approach to generate a discriminative model, which can differentiate specialized software gurus in a particular domain from other Twitter users that generate domain-related tweets (aka domain-related Twitter users). We have investigated the effectiveness of our approach in finding specialized software gurus for four different domains (JavaScript, Android, Python, and Linux) on a dataset of 86,824 Twitter users who generate 5,517,878 tweets over 1 month. Our approach can differentiate specialized software experts from other domain-related Twitter users with an F-Measure of up to 0.820. Compared with existing Twitter domain expert recommendation approaches, our proposed approach can outperform their F-Measure by at least 7.63%. © 2018 Association for Computing Machinery.",Recommendation systems; Software engineering; Twitter,Computer operating systems; High level languages; Recommender systems; Social networking (online); Software engineering; Classification approach; Development activity; Different domains; Discriminative models; Domain experts; Software developer; Specialized software; Twitter; Software design
Shadow symbolic execution for testing software patches,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060894611&doi=10.1145%2f3208952&partnerID=40&md5=1504f2512e864a520abc1e237ddc9d46,"While developers are aware of the importance of comprehensively testing patches, the large effort involved in coming up with relevant test cases means that such testing rarely happens in practice. Furthermore, even when test cases are written to cover the patch, they often exercise the same behaviour in the old and the new version of the code. In this article, we present a symbolic execution-based technique that is designed to generate test inputs that cover the new program behaviours introduced by a patch. The technique works by executing both the old and the new version in the same symbolic execution instance, with the old version shadowing the new one. During this combined shadow execution, whenever a branch point is reached where the old and the new version diverge, we generate a test input exercising the divergence and comprehensively test the new behaviours of the new version. We evaluate our technique on the Coreutils patches from the CoREBench suite of regression bugs, and show that it is able to generate test inputs that exercise newly added behaviours and expose some of the regression bugs. 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",And Phrases: Symbolic patch testing; Cross-version checks; Regression bugs,Model checking; Regression analysis; Testing; Branch points; Cross-version checks; New programs; Patch testing; Regression bugs; Symbolic execution; Test inputs; Testing software; Software testing
Test-equivalence analysis for automatic patch generation,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055834590&doi=10.1145%2f3241980&partnerID=40&md5=8b652172ab80df3bad5917d0e9e5e6c8,"Automated program repair is a problem of finding a transformation (called a patch) of a given incorrect program that eliminates the observable failures. It has important applications such as providing debugging aids, automatically grading student assignments, and patching security vulnerabilities. A common challenge faced by existing repair techniques is scalability to large patch spaces, since there are many candidate patches that these techniques explicitly or implicitly consider. The correctness criteria for program repair is often given as a suite of tests. Current repair techniques do not scale due to the large number of test executions performed by the underlying search algorithms. In this work, we address this problem by introducing a methodology of patch generation based on a test-equivalence relation (if two programs are “test-equivalent” for a given test, they produce indistinguishable results on this test). We propose two test-equivalence relations based on runtime values and dependencies, respectively, and present an algorithm that performs on-the-fly partitioning of patches into test-equivalence classes. Our experiments on real-world programs reveal that the proposed methodology drastically reduces the number of test executions and therefore provides an order of magnitude efficiency improvement over existing repair techniques, without sacrificing patch quality. © 2018 Association for Computing Machinery.",Dynamic program analysis; Program repair; Program synthesis,Equivalence classes; Grading; Program debugging; Repair; Set theory; Correctness criterion; Dynamic program analysis; Efficiency improvement; Equivalence relations; Program synthesis; Real world projects; Security vulnerabilities; Student assignments; Software testing
Multi-objective optimization of energy consumption of GUIs in android apps,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060843179&doi=10.1145%2f3241742&partnerID=40&md5=1470d5b6c49a17098c6302c10b57fda9,"The number of mobile devices sold worldwide has exponentially increased in recent years, surpassing that of personal computers in 2011. Such devices daily download and run millions of apps that take advantage of modern hardware features (e.g., multi-core processors, large Organic Light-Emitting Diode—OLED—screens, etc.) to offer exciting user experiences. Clearly, there is a cost to pay in terms of energy consumption and, in particular, of reduced battery life. This has pushed researchers to investigate how to reduce the energy consumption of apps, for example, by optimizing the color palette used in the app's GUI. Whilst past research in this area aimed at optimizing energy while keeping an acceptable level of contrast, this article proposes an approach, named Gui Energy Multi-objective optiMization for Android apps (GEMMA), for generating color palettes using a multi-objective optimization technique, which produces color solutions optimizing energy consumption and contrast while using consistent colors with respect to the original color palette. The empirical evaluation demonstrates (i) substantial improvements in terms of the three different objectives, (ii) a concrete reduction of the energy consumption as assessed by a hardware power monitor, (iii) the attractiveness of the generated color compositions for apps' users, and (iv) the suitability of GEMMA to be adopted in industrial contexts. © 2018 Association for Computing Machinery.",And Phrases: Energy consumption; Empirical study; Mobile applications,Color; Computer hardware; Energy utilization; Graphical user interfaces; Multiobjective optimization; Organic light emitting diodes (OLED); Personal computers; Color composition; Empirical evaluations; Empirical studies; Hardware features; Industrial context; Mobile applications; Multi-core processor; Multi-objective optimization techniques; Android (operating system)
The ABC of software engineering research,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054014259&doi=10.1145%2f3241743&partnerID=40&md5=d26816efd8657be6df726ef77fa0c0aa,"A variety of research methods and techniques are available to SE researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the ABC framework for SE research, which offers a holistic view of eight archetypal research strategies. ABC refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The ABC framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key SE domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance SE research. © 2018 Copyright held by the owner/author(s).",Research methodology; Research strategy,Computer software; Global software engineering; Inherent limitations; Precise measurements; Research designs; Research goals; Research methodologies; Research strategy; Two-dimension; Software engineering
Spectrum-based fault localization in model transformations,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053920175&doi=10.1145%2f3241744&partnerID=40&md5=0c80a77d44eefc1dc35ea71f9142f53b,"Model transformations play a cornerstone role in Model-Driven Engineering (MDE), as they provide the essential mechanisms for manipulating and transforming models. The correctness of software built using MDE techniques greatly relies on the correctness of model transformations. However, it is challenging and error prone to debug them, and the situation gets more critical as the size and complexity of model transformations grow, where manual debugging is no longer possible. Spectrum-Based Fault Localization (SBFL) uses the results of test cases and their corresponding code coverage information to estimate the likelihood of each program component (e.g., statements) of being faulty. In this article we present an approach to apply SBFL for locating the faulty rules in model transformations. We evaluate the feasibility and accuracy of the approach by comparing the effectiveness of 18 different state-of-the-art SBFL techniques at locating faults in model transformations. Evaluation results revealed that the best techniques, namely Kulcynski2, Mountford, Ochiai, and Zoltar, lead the debugger to inspect a maximum of three rules to locate the bug in around 74% of the cases. Furthermore, we compare our approach with a static approach for fault localization in model transformations, observing a clear superiority of the proposed SBFL-based method. © 2018 Association for Computing Machinery.",Debugging; Fault localization; Model transformation; Spectrum-based; Testing,Computer debugging; Computer software; Software engineering; Testing; Evaluation results; Fault localization; Model transformation; Model-driven Engineering; Program components; Spectrum-based; State of the art; Transforming models; Software testing
Linear programming as a baseline for software effort estimation,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053899656&doi=10.1145%2f3234940&partnerID=40&md5=f7d4c7beb63122a61d355a12ff133855,"Software effort estimation studies still suffer from discordant empirical results (i.e., conclusion instability) mainly due to the lack of rigorous benchmarking methods. So far only one baseline model, namely, Automatically Transformed Linear Model (ATLM), has been proposed yet it has not been extensively assessed. In this article, we propose a novel method based on Linear Programming (dubbed as Linear Programming for Effort Estimation, LP4EE) and carry out a thorough empirical study to evaluate the effectiveness of both LP4EE and ATLM for benchmarking widely used effort estimation techniques. The results of our study confirm the need to benchmark every other proposal against accurate and robust baselines. They also reveal that LP4EE is more accurate than ATLM for 17% of the experiments and more robust than ATLM against different data splits and cross-validation methods for 44% of the cases. These results suggest that using LP4EE as a baseline can help reduce conclusion instability. We make publicly available an open-source implementation of LP4EE in order to facilitate its adoption in future studies. © 2018 Association for Computing Machinery.",Benchmarking; Linear programming; Software effort estimation,Benchmarking; Linear programming; Baseline models; Benchmarking methods; Cross-validation methods; Effort Estimation; Empirical studies; Linear modeling; Open source implementation; Software effort estimation; Open source software
Editorial,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053377680&doi=10.1145%2f3264424&partnerID=40&md5=926e8346b484c955a4984b5ff7d5da11,[No abstract available],,
Maintaining architecture-implementation conformance to support architecture centrality: From single system to product line development,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053937481&doi=10.1145%2f3229048&partnerID=40&md5=4901d52782256bae0710bd38c6417137,"Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution. © 2018 ACM.",Architectural evolution; Architecture-centric development; Architecture-centric feature traceability; Architecture-implementation mapping; Variability conformance,Codes (symbols); Mapping; Open source software; Open systems; Architectural evolution; Architectural models; Architecture-centric; Open source system; Product line architecture; Product line development; System development; Variability conformance; Computer architecture
Prove it! Inferring formal proof scripts from CafeOBJ proof scores,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053862604&doi=10.1145%2f3208951&partnerID=40&md5=d348b69d22958d1c689638053f909dbe,"CafeOBJ is a language for writing formal specifications for a wide variety of software and hardware systems and for verifying their properties. CafeOBJ makes it possible to verify properties by using either proof scores, which consists of reducing goal-related terms in user-defined modules, or by using theorem proving. While the former is more flexible, it lacks the formal support to ensure that a property has been really proven. On the other hand, theorem proving might be too strict, since only a predefined set of commands can be applied to the current goal; hence, it hardens the verification of properties. In order to take advantage of the benefits of both techniques, we have extended CafeInMaude, a CafeOBJ interpreter implemented in Maude, with the CafeInMaude Proof Assistant (CiMPA) and the CafeInMaude Proof Generator (CiMPG). CiMPA is a proof assistant for proving inductive properties on CafeOBJ specifications that uses Maude metalevel features to allow programmers to create and manipulate CiMPA proofs. On the other hand, CiMPG provides a minimal set of annotations for identifying proof scores and generating CiMPA scripts for these proof scores. In this article, we present the CiMPA and CiMPG, detailing the behavior of the CiMPA and the algorithm underlying the CiMPG and illustrating the power of the approach by using the QLOCK protocol. Finally, we present some benchmarks that give us confidence in the matureness and usefulness of these tools. © 2018 ACM.",CafeOBJ; Proof scores; Script inference; Theorem Proving,Formal specification; CafeOBJ; Formal proofs; Proof assistant; Proof scores; Script inference; Software and hardwares; User-defined modules; Theorem proving
STADS: Software testing as species discovery,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053872377&doi=10.1145%2f3210309&partnerID=40&md5=287e999c5e908a5cf8a0a44ada688e90,"A fundamental challenge of software testing is the statistically well-grounded extrapolation from program behaviors observed during testing. For instance, a security researcher who has run the fuzzer for a week has currently no means (1) to estimate the total number of feasible program branches, given that only a fraction has been covered so far; (2) to estimate the additional time required to cover 10% more branches (or to estimate the coverage achieved in one more day, respectively); or (3) to assess the residual risk that a vulnerability exists when no vulnerability has been discovered. Failing to discover a vulnerability does not mean that none exists-even if the fuzzer was run for a week (or a year). Hence, testing provides no formal correctness guarantees. In this article, I establish an unexpected connection with the otherwise unrelated scientific field of ecology and introduce a statistical framework that models Software Testing and Analysis as Discovery of Species (STADS). For instance, in order to study the species diversity of arthropods in a tropical rain forest, ecologists would first sample a large number of individuals from that forest, determine their species, and extrapolate from the properties observed in the sample to properties of the whole forest. The estimations (1) of the total number of species, (2) of the additional sampling effort required to discover 10% more species, or (3) of the probability to discover a new species are classical problems in ecology. The STADS framework draws from over three decades of research in ecological biostatistics to address the fundamental extrapolation challenge for automated test generation. Our preliminary empirical study demonstrates a good estimator performance even for a fuzzer with adaptive sampling bias-AFL, a state-of-the-art vulnerability detection tool. The STADS framework provides statistical correctness guarantees with quantifiable accuracy. © 2018 ACM.",Code coverage; Discovery probability; Extrapolation; Fuzzing; Measure of confidence; Measure of progress; Reliability; Security; Species coverage; Statistical guarantees; Stopping rule,Biodiversity; Ecology; Estimation; Extrapolation; Forestry; Reliability; Risk assessment; Risk management; Risk perception; Code coverage; Fuzzing; Measure of confidence; Measure of progress; Security; Species coverage; Statistical guarantee; Stopping rule; Software testing
FEMOSAA: Feature-guided and knee-driven multi-objective optimization for self-adaptive software,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050635841&doi=10.1145%2f3204459&partnerID=40&md5=8ba36357058f70d1656e097afb704742,"Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, we present Feature-guided and knEe-driven Multi-Objective optimization for Self-Adaptive softwAre (FEMOSAA), a novel framework that automatically synergizes the feature model and Multi-Objective Evolutionary Algorithm (MOEA) to optimize SAS at runtime. FEMOSAA operates in two phases: at design time, FEMOSAA automatically transposes the engineers' design of SAS, expressed as a feature model, to fit the MOEA, creating new chromosome representation and reproduction operators. At runtime, FEMOSAA utilizes the feature model as domain knowledge to guide the search and further extend the MOEA, providing a larger chance for finding better solutions. In addition, we have designed a new method to search for the knee solutions, which can achieve a balanced tradeoff. We comprehensively evaluated FEMOSAA on two running SAS: One is a highly complex SAS with various adaptable real-world software under the realistic workload trace; another is a service-oriented SAS that can be dynamically composed from services. In particular, we compared the effectiveness and overhead of FEMOSAA against four of its variants and three other search-based frameworks for SAS under various scenarios, including three commonly applied MOEAs, two workload patterns, and diverse conflicting quality objectives. The results reveal the effectiveness of FEMOSAA and its superiority over the others with high statistical significance and nontrivial effect sizes. © ACM.",Feature model; Multi-objective evolutionary algorithm; Multi-objective optimization; Performance engineering; Search-based software engineering; Self-adaptive system,Adaptive systems; Computer software; Energy utilization; Evolutionary algorithms; Feature modeling; Multi objective evolutionary algorithms; Performance engineering; Search-based software engineering; Self-adaptive system; Multiobjective optimization
Detecting the behavior of design patterns through model checking and dynamic analysis,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042474652&doi=10.1145%2f3176643&partnerID=40&md5=9b67853cd971c4592fa5303de1144b65,"We present a method and tool (ePAD) for the detection of design pattern instances in source code. The approach combines static analysis, based on visual language parsing and model checking, and dynamic analysis, based on source code instrumentation. Visual language parsing and static source code analysis identify candidate instances satisfying the structural properties of design patterns. Successively, model checking statically verifies the behavioral aspects of the candidates recovered in the previous phase. We encode the sequence of messages characterizing the correct behaviour of a pattern as Linear Temporal Logic (LTL) formulae and the sequence diagram representing the possible interaction traces among the objects involved in the candidates as Promela specifications. The model checker SPIN verifies that candidates satisfy the LTL formulae. Dynamic analysis is then performed on the obtained candidates by instrumenting the source code and monitoring those instances at runtime through the execution of test cases automatically generated using a search-based approach. The effectiveness of ePAD has been evaluated by detecting instances of 12 creational and behavioral patterns from six publicly available systems. The results reveal that ePAD outperforms other approaches by recovering more actual instances. Furthermore, on average ePAD achieves better results in terms of correctness and completeness. © 2018 ACM.",Design pattern recovery; Reverse engineering; Software maintenance,Automatic test pattern generation; Codes (symbols); Computer programming languages; Computer software maintenance; Pattern recognition; Recovery; Reverse engineering; Static analysis; Temporal logic; Visual languages; Automatically generated; Behavioral patterns; Design pattern recovery; Design Patterns; Linear temporal logic; Sequence diagrams; Source Code Instrumentation; Visual language parsing; Model checking
Configuring software product lines by combining many-Objective optimization and SAT solvers,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042467188&doi=10.1145%2f3176644&partnerID=40&md5=fb2cea57b2f9c3437b2414f95057ba6d,"A feature model (FM) is a compact representation of the information of all possible products from software product lines. The optimal feature selection involves the simultaneous optimization of multiple (usually more than three) objectives in a large and highly constrained search space. By combining our previous work on many-objective evolutionary algorithm (i.e., VaEA) with two different satisfiability (SAT) solvers, this article proposes a new approach named SATVaEA for handling the optimal feature selection problem. In SATVaEA, an FM is simplified with the number of both features and constraints being reduced greatly. We enhance the search of VaEA by using two SAT solvers: one is a stochastic local search–based SAT solver that can quickly repair infeasible configurations, whereas the other is a conflict-driven clause-learning SAT solver that is introduced to generate diversified products. We evaluate SATVaEA on 21 FMs with up to 62,482 features, including two models with realistic values for feature attributes. The experimental results are promising, with SATVaEA returning 100% valid products on almost all FMs. For models with more than 10,000 features, the search in SATVaEA takes only a few minutes. Concerning both effectiveness and efficiency, SATVaEA significantly outperforms other state-of-the-art algorithms. © 2018 ACM.",Many-objective optimization; Optimal feature selection; Satisfiability (SAT) solvers; Vector angle–based evolutionary algorithm (VaEA),Computer software; Constrained optimization; Decision theory; Evolutionary algorithms; Feature extraction; Formal logic; Model checking; Stochastic systems; Effectiveness and efficiencies; Many-objective optimizations; Optimal feature selections; Satisfiability solvers; Simultaneous optimization; State-of-the-art algorithms; Stochastic local searches; Vector angle; Optimization
An empirical study of meta- and hyper-heuristic search for multi-objective release planning,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054215179&doi=10.1145%2f3196831&partnerID=40&md5=26cfa0ac14d11a4fcf596812e83f0a03,"A variety of meta-heuristic search algorithms have been introduced for optimising software release planning. However, there has been no comprehensive empirical study of different search algorithms across multiple different real-world datasets. In this article, we present an empirical study of global, local, and hybrid metaand hyper-heuristic search-based algorithms on 10 real-world datasets. We find that the hyper-heuristics are particularly effective. For example, the hyper-heuristic genetic algorithm significantly outperformed the other six approaches (and with high effect size) for solution quality 85% of the time, and was also faster than all others 70% of the time. Furthermore, correlation analysis reveals that it scales well as the number of requirements increases. © 2018 ACM.",Hyper-heuristics; Meta-heuristics; Strategic release planning,Genetic algorithms; Heuristic methods; Learning algorithms; Modular robots; Correlation analysis; Hyper-heuristics; Meta heuristics; Meta-heuristic search algorithms; Real-world datasets; Release planning; Search Algorithms; Software release planning; Heuristic algorithms
Inferring extended probabilistic finite-state automaton models from software executions,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054202141&doi=10.1145%2f3196883&partnerID=40&md5=3e1b64710448d89c2183cd373accda40,"Behavioral models are useful tools in understanding how programs work. Although several inference approaches have been introduced to generate extended finite-state automatons from software execution traces, they suffer from accuracy, flexibility, and decidability issues. In this article, we apply a hybrid technique to use both reinforcement learning and stochastic modeling to generate an extended probabilistic finite state automaton from software traces. Our approach - ReHMM (Reinforcement learning-based Hidden Markov Modelling) - is able to address the problems of inflexibility and un-decidability reported in other state-of-theart approaches. Experimental results indicate that ReHMM outperforms other inference algorithms. © 2018 ACM.",Dynamic analysis; EFSA; Hidden Markov model; PFSA; Reinforcement learning,Computability and decidability; Dynamic analysis; Hidden Markov models; Inference engines; Stochastic systems; Behavioral model; EFSA; Finite state; Hybrid techniques; Inference algorithm; PFSA; Probabilistic finite state automaton; Software execution; Reinforcement learning
Assessing the refactoring of brain methods,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053393119&doi=10.1145%2f3191314&partnerID=40&md5=209705745b17c3fd66711cd7711c60ec,"Code smells are a popular mechanism for identifying structural design problems in software systems. Several tools have emerged to support the detection of code smells and propose some refactorings. However, existing tools do not guarantee that a smell will be automatically fixed by means of refactorings. This article presents Bandago, an automated approach to fix a specific type of code smell called Brain Method. A Brain Method centralizes the intelligence of a class and manifests itself as a long and complex method that is difficult to understand and maintain by developers. For each Brain Method, Bandago recommends several refactoring solutions to remove the smell using a search strategy based on simulated annealing. Our approach has been evaluated with several open-source Java applications, and the results show that Bandago can automatically fix more than 60% of Brain Methods. Furthermore, we conducted a survey with 35 industrial developers that showed evidence about the usefulness of the refactorings proposed by Bandago. Also, we compared the performance of the Bandago against that of a third-party refactoring tool. © 2018 ACM.",,Odors; Simulated annealing; Structural design; Automated approach; Complex methods; Java applications; Refactoring tools; Search strategies; Software systems; Structural design problems; Third parties; Open source software
Editorial,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054211269&doi=10.1145%2f3205909&partnerID=40&md5=608d43a7112fcdf72bedcc244e294d89,[No abstract available],,
How far we have progressed in the journey? An examination of cross-project defect prediction,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054209479&doi=10.1145%2f3183339&partnerID=40&md5=5594d430ea3d0df9061ebf5903a3fde7,"Background. Recent years have seen an increasing interest in cross-project defect prediction (CPDP), which aims to apply defect prediction models built on source projects to a target project. Currently, a variety of (complex) CPDP models have been proposed with a promising prediction performance. Problem. Most, if not all, of the existing CPDP models are not compared against those simple module size models that are easy to implement and have shown a good performance in defect prediction in the literature. Objective.We aim to investigate how far we have really progressed in the journey by comparing the performance in defect prediction between the existing CPDP models and simple module size models. Method.We first use module size in the target project to build two simple defect prediction models, Manual- Down and ManualUp, which do not require any training data from source projects. ManualDown considers a larger module as more defect-prone, while ManualUp considers a smaller module as more defect-prone. Then, we take the following measures to ensure a fair comparison on the performance in defect prediction between the existing CPDP models and the simple module size models: using the same publicly available data sets, using the same performance indicators, and using the prediction performance reported in the original cross-project defect prediction studies. Result. The simple module size models have a prediction performance comparable or even superior to most of the existing CPDP models in the literature, including many newly proposed models. Conclusion. The results caution us that, if the prediction performance is the goal, the real progress in CPDP is not being achieved as it might have been envisaged. We hence recommend that future studies should include ManualDown/ManualUp as the baseline models for comparison when developing new CPDP models to predict defects in a complete target project. © 2018 ACM.",Cross-project; Defect prediction; Model; Supervised; Unsupervised,Defects; Models; Baseline models; Cross-project; Defect prediction; Defect prediction models; Performance indicators; Prediction performance; Supervised; Unsupervised; Forecasting
"Lightweight, obfuscation-Resilient detection and family identification of android malware",2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042506850&doi=10.1145%2f3162625&partnerID=40&md5=7a3352faf903219081a7e20c0bb45306,"The number of malicious Android apps is increasing rapidly. Android malware can damage or alter other files or settings, install additional applications, and so on. To determine such behaviors, a security analyst can significantly benefit from identifying the family to which an Android malware belongs rather than only detecting if an app is malicious. Techniques for detecting Android malware, and determining their families, lack the ability to handle certain obfuscations that aim to thwart detection. Moreover, some prior techniques face scalability issues, preventing them from detecting malware in a timely manner. 1 To address these challenges, we present a novel machine-learning-based Android malware detection and family identification approach, RevealDroid, that operates without the need to perform complex program analyses or to extract large sets of features. Specifically, our selected features leverage categorized Android API usage, reflection-based features, and features from native binaries of apps. We assess RevealDroid for accuracy, efficiency, and obfuscation resilience using a large dataset consisting of more than 54,000 malicious and benign apps. Our experiments show that RevealDroid achieves an accuracy of 98% in detection of malware and an accuracy of 95% in determination of their families. We further demonstrate RevealDroid’s superiority against state-of-the-art approaches. © 2018 ACM.",Android malware; Lightweight; Machine learning; Native code; Obfuscation; Reflection,Application programming interfaces (API); Artificial intelligence; Computer crime; Learning systems; Malware; Reflection; Android malware; Identification approach; Lightweight; Native code; Obfuscation; Scalability issue; Sets of features; State-of-the-art approach; Android (operating system)
Global and local deadlock freedom in BIP,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040461304&doi=10.1145%2f3152910&partnerID=40&md5=f350ece9495e61d11ee91f2aa08ffcf7,"We present a criterion for checking local and global deadlock freedom of finite state systems expressed in BIP: A component-based framework for constructing complex distributed systems. Our criterion is evaluated by model-checking a set of subsystems of the overall large system. If satisfied in small subsystems, it implies deadlock-freedom of the overall system. If not satisfied, then we re-evaluate over larger subsystems, which improves the accuracy of the check. When the subsystem being checked becomes the entire system, our criterion becomes complete for deadlock-freedom. Hence our criterion only fails to decide deadlock freedom because of computational limitations: state-space explosion sets in when the subsystems become too large. Our method thus combines the possibility of fast response together with theoretical completeness. Other criteria for deadlock freedom, in contrast, are incomplete in principle, and so may fail to decide deadlock freedom even if unlimited computational resources are available. Also, our criterion certifies freedom from local deadlock, in which a subsystem is deadlocked while the rest of the system executes. Other criteria only certify freedom from global deadlock.We present experimental results for dining philosophers and for a multi-Token-based resource allocation system,which subsumes several data arbiters and schedulers, including Milner's token-based scheduler. © 2018 ACM.",Alternation; Completeness; Nondeterminism,Philosophical aspects; Scheduling; Alternation; Completeness; Complex distributed system; Component-based framework; Computational limitations; Computational resources; Non-determinism; Resource allocation systems; Model checking
Understanding and combating memory bloat in managed data-intensive systems,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040459020&doi=10.1145%2f3162626&partnerID=40&md5=36382a6f43a7f104172039fb5797311f,"The past decade has witnessed increasing demands on data-driven business intelligence that led to the proliferation of data-intensive applications. A managed object-oriented programming language such as Java is often the developer's choice for implementing such applications, due to its quick development cycle and rich suite of libraries and frameworks. While the use of such languages makes programming easier, their automated memory management comes at a cost. When the managed runtime meets large volumes of input data, memory bloat is significantly magnified and becomes a scalability-prohibiting bottleneck. This article first studies, analytically and empirically, the impact of bloat on the performance and scalability of large-scale, real-world data-intensive systems. To combat bloat, we design a novel compiler framework, called Facade, that can generate highly efficient data manipulation code by automatically transforming the data path of an existing data-intensive application. The key treatment is that in the generated code, the number of runtime heap objects created for data classes in each thread is (almost) statically bounded, leading to significantly reduced memory management cost and improved scalability. We have implemented Facade and used it to transform seven common applications on three real-world, already well-optimized data processing frameworks: GraphChi, Hyracks, and GPS. Our experimental results are very positive: The generated programs have (1) achieved a 3% to 48% execution time reduction and an up to 88× GC time reduction, (2) consumed up to 50% less memory, and (3) scaled to much larger datasets. © 2018 ACM.",Big data; Managed languages; Memory management; Performance optimization,Data handling; Metadata; Object oriented programming; Scalability; Automated memory management; Data manipulations; Data-intensive application; Data-intensive systems; Development cycle; Memory management; Performance and scalabilities; Performance optimizations; Big data
Variability bugs in highly configurable systems: A qualitative analysis,2018,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042483857&doi=10.1145%2f3149119&partnerID=40&md5=6a5e70b47356af68e4aab729a4de35d7,"Variability-sensitive verification pursues effective analysis of the exponentially many variants of a program family. Several variability-aware techniques have been proposed, but researchers still lack examples of concrete bugs induced by variability, occurring in real large-scale systems. A collection of real world bugs is needed to evaluate tool implementations of variability-sensitive analyses by testing them on real bugs. We present a qualitative study of 98 diverse variability bugs (i.e., bugs that occur in some variants and not in others) collected from bug-fixing commits in the Linux, Apache, BusyBox, and Marlin repositories. We analyze each of the bugs, and record the results in a database. For each bug, we create a self-contained simplified version and a simplified patch, in order to help researchers who are not experts on these subject studies to understand them, so that they can use these bugs for evaluation of their tools. In addition, we provide single-function versions of the bugs, which are useful for evaluating intra-procedural analyses. A web-based user interface for the database allows to conveniently browse and visualize the collection of bugs. Our study provides insights into the nature and occurrence of variability bugs in four highly-configurable systems implemented in C/C++, and shows in what ways variability hinders comprehension and the uncovering of software bugs. © 2018 ACM.",Bugs; Feature interactions; Linux; Software variability,C (programming language); Computer operating systems; Large scale systems; Linux; User interfaces; Bugs; Configurable systems; Effective analysis; Feature interactions; Procedural analysis; Qualitative analysis; Sensitive analysis; Software variabilities; Program debugging
Early evaluation of implementation alternatives of composite data structures toward maintainability,2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032974011&doi=10.1145%2f3132731&partnerID=40&md5=cde6849b83fe82a42877d1f478437331,"Selecting between different design options is a crucial decision for object-oriented software developers that affects code quality characteristics. Conventionally developers use their experience to make such decisions, which leads to suboptimal results regarding code quality. In this article, a formal model for providing early estimates of quality metrics of object-oriented software implementation alternatives is proposed. The model supports software developers in making fast decisions in a systematic way early during the design phase to achieve improved code characteristics. The approach employs a comparison model related to the application of the Visitor design pattern and inheritance-based implementation on structures following the Composite design pattern. The model captures maintainability as a metric of software quality and provides precise assessments of the quality of each implementation alternative. Furthermore, the model introduces the structural maintenance cost metric based on which the progressive analysis of the maintenance process is introduced. The proposed approach has been applied to several test cases for different relevant quality metrics. The results prove that the proposed model delivers accurate estimations. Thus, the proposed methodology can be used for comparing different implementation alternatives against various measures and quality factors before code development, leading to reduced effort and cost for software maintenance. c 2017 ACM.",Composition; Visitor,Chemical analysis; Codes (symbols); Computer software selection and evaluation; Cost benefit analysis; Maintainability; Maintenance; Accurate estimation; Comparison modeling; Composite design pattern; Maintenance process; Object oriented software; Software developer; Structural maintenance; Visitor; Object oriented programming
Editorial,2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032903314&doi=10.1145%2f3136621&partnerID=40&md5=fe8aba58489b5d245d6c66d8152e654c,[No abstract available],,
A logic-based approach for the verification of UML timed models,2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032993901&doi=10.1145%2f3106411&partnerID=40&md5=b5ea93eddd2799bf0f60ff3e2b078d55,"This article presents a novel technique to formally verify models of real-time systems captured through a set of heterogeneous UML diagrams. The technique is based on the following key elements: (i) a subset of Unified Modeling Language (UML) diagrams, called Coretto UML (C-UML), which allows designers to describe the components of the system and their behavior through several kinds of diagrams (e.g., state machine diagrams, sequence diagrams, activity diagrams, interaction overview diagrams), and stereotypes taken from the UML Profile for Modeling and Analysis of Real-Time and Embedded Systems; (ii) a formal semantics of C-UML diagrams, defined through formulae of the metric temporal logic Tempo Reale ImplicitO (TRIO); and (iii) a tool, called Corretto, which implements the aforementioned semantics and allows users to carry out formal verification tasks on modeled systems. We validate the feasibility of our approach through a set of different case studies, taken from both the academic and the industrial domain. © 2017 ACM.",Formal semantics; Formal verification; Metric temporal logic; Timed systems,C (programming language); Carry logic; Computer circuits; Embedded systems; Formal methods; Formal verification; Graphic methods; Interactive computer systems; Machine components; Modeling languages; Real time systems; Semantics; Temporal logic; Time sharing systems; Formal Semantics; Logic-based approach; Metric temporal logic; Model and analysis; Real-time and embedded systems; Sequence diagrams; State machine diagrams; Timed systems; Unified Modeling Language
Fixing faults in C and Java Source Code: Abbreviated vs. full-word identifier names,2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027103491&doi=10.1145%2f3104029&partnerID=40&md5=6e2c54b58fc57ff82423b9b8aac92d06,"We carried out a family of controlled experiments to investigate whether the use of abbreviated identifier names, with respect to full-word identifier names, affects fault fixing in C and Java source code. This family consists of an original (or baseline) controlled experiment and three replications.We involved 100 participants with different backgrounds and experiences in total. Overall results suggested that there is no difference in terms of effort, effectiveness, and efficiency to fix faults, when source code contains either only abbreviated or only full-word identifier names. We also conducted a qualitative study to understand the values, beliefs, and assumptions that inform and shape fault fixing when identifier names are either abbreviated or full-word.We involved in this qualitative study six professional developers with 1-3 years of work experience. A number of insights emerged from this qualitative study and can be considered a useful complement to the quantitative results from our family of experiments. One of the most interesting insights is that developers, when working on source code with abbreviated identifier names, adopt a more methodical approach to identify and fix faults by extending their focus point and only in a few cases do they expand abbreviated identifiers. © 2017 ACM.",,Codes (symbols); Computer programming languages; Java programming language; Controlled experiment; Focus points; Java source codes; Methodical approach; Qualitative study; Quantitative result; Source codes; Work experience; C (programming language)
Human competitiveness of Genetic Programming in spectrum-based Fault Localisation: Theoretical and empirical analysis,2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027014180&doi=10.1145%2f3078840&partnerID=40&md5=63816594f68d57d67da280af58d76078,"We report on the application of Genetic Programming to Software Fault Localisation, a problem in the area of Search-Based Software Engineering (SBSE). We give both empirical and theoretical evidence for the human competitiveness of the evolved fault localisation formulæ under the single fault scenario, compared to those generated by human ingenuity and reported in many papers, published over more than a decade. Though there have been previous human competitive results claimed for SBSE problems, this is the first time that evolved solutions have been formally proved to be human competitive. We further prove that no future human investigation could outperform the evolved solutions. We complement these proofs with an empirical analysis of both human and evolved solutions, which indicates that the evolved solutions are not only theoretically human competitive, but also convey similar practical benefits to human-evolved counterparts. © 2017 ACM.",Genetic programming; Search-based software engineering; Spectrum-based fault localisation,Application programs; Competition; Genetic algorithms; Software engineering; Empirical analysis; Human competitiveness; Localisation; Search based software engineering (SBSE); Search-based software engineering; Single fault; Software fault; Genetic programming
Predicting query quality for applications of text retrieval to software engineering tasks,2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027020471&doi=10.1145%2f3078841&partnerID=40&md5=0699299d4b311c05b652066ac0dcee0c,"Context: Since the mid-2000s, numerous recommendation systems based on text retrieval (TR) have been proposed to support software engineering (SE) tasks such as concept location, traceability link recovery, code reuse, impact analysis, and so on. The success of TR-based solutions highly depends on the query submitted, which is either formulated by the developer or automatically extracted from software artifacts. Aim: We aim at predicting the quality of queries submitted to TR-based approaches in SE. This can lead to benefits for developers and for the quality of software systems alike. For example, knowing when a query is poorly formulated can save developers the time and frustration of analyzing irrelevant search results. Instead, they could focus on reformulating the query. Also, knowing if an artifact used as a query leads to irrelevant search results may uncover underlying problems in the query artifact itself. Method: We introduce an automatic query quality prediction approach for software artifact retrieval by adapting NL-inspired solutions to their use on software data. We present two applications and evaluations of the approach in the context of concept location and traceability link recovery, where TR has been applied most often in SE. For concept location, we use the approach to determine if the list of retrieved code elements is likely to contain code relevant to a particular change request or not, in which case, the queries are good candidates for reformulation. For traceability link recovery, the queries represent software artifacts. In this case, we use the query quality prediction approach to identify artifacts that are hard to trace to other artifacts and may therefore have a low intrinsic quality for TR-based traceability link recovery. Results: For concept location, the evaluation shows that our approach is able to correctly predict the quality of queries in 82% of the cases, on average, using very little training data. In the case of traceability recovery, the proposed approach is able to detect hard to trace artifacts in 74% of the cases, on average. Conclusions: The results of our evaluation on applications for concept location and traceability link recovery indicate that our approach can be used to predict the results of a TR-based approach by assessing the quality of the text query. This can lead to saved effort and time, as well as the identification of software artifacts that may be difficult to trace using TR. © 2017 ACM.",Artifact traceability; Concept location; Text retrieval,Application programs; Codes (symbols); Computer software reusability; Forecasting; Information retrieval; Location; Recovery; Software engineering; Artifact traceability; Concept locations; Impact analysis; Quality of softwares; Quality prediction; Software artifacts; Text retrieval; Traceability links; Quality control
Parallel algorithms for generating distinguishing sequences for observable non-deterministic FSMs,2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026487161&doi=10.1145%2f3051121&partnerID=40&md5=382a23d5a0a8667c7a7194c86a9f59b6,"A distinguishing sequence (DS) for a finite-state machine (FSM) is an input sequence that distinguishes every pair of states of the FSM. There are techniques that generate a test sequence with guaranteed fault detection power, and it has been found that shorter test sequences can be produced if DSs are used. Despite these benefits, however, until recently the only published DS generation algorithms have been for deterministic FSMs. This article develops a massively parallel algorithm, which can be used in Graphics Processing Units (GPUs) Computing, to generate DSs from partial observable non-deterministic FSMs. We also present the results of experiments using randomly generated FSMs and some benchmark FSMs. The results are promising and indicate that the proposed algorithm can derive DSs from partial observable non-deterministic FSMs with 32,000 states in an acceptable amount of time. © 2017 ACM.",Distinguishing sequences; Finite state machine,Computer graphics; Fault detection; Finite automata; Graphics processing unit; Program processors; Distinguishing sequences; Generation algorithm; Input sequence; Massively parallels; Test sequence; Parallel algorithms
Ensuring the consistency of adaptation through interand intra-component dependency analysis,2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019884971&doi=10.1145%2f3063385&partnerID=40&md5=51494c4ec5fd70a657efa2ff64ed93bf,"Dynamic adaptation should not leave a software system in an inconsistent state, as it could lead to failure. Prior research has used inter-component dependency models of a system to determine a safe interval for the adaptation of its components, where the most important tradeoff is between disruption in the operations of the system and reachability of safe intervals. This article presents Savasana, which automatically analyzes a software system's code to extract both inter- and intra-component dependencies. In this way, Savasana is able to obtain more fine-grained models compared to previous approaches. Savasana then uses the detailed models to find safe adaptation intervals that cannot be determined using techniques from prior research. This allows Savasana to achieve a better tradeoff between disruption and reachability. The article demonstrates how Savasana infers safe adaptation intervals for components of a software system under various use cases and conditions. © 2017 ACM.",Adaptive software; Component-based software; Update criteria,Software engineering; Adaptive software; Component based software; Dependency analysis; Detailed models; Dynamic adaptations; Inter-component dependencies; Software systems; Update criteria; Computer software
Augmenting field data for testing systems subject to incremental requirements changes,2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019854327&doi=10.1145%2f3053430&partnerID=40&md5=4ac2f9797b4dd76d2aecc34f5d137975,"When testing data processing systems, software engineers often use real-world data to perform system-level testing. However, in the presence of new data requirements, software engineers may no longer benefit from having real-world data with which to perform testing. Typically, new test inputs complying with the new requirements have to be manually written. We propose an automated model-based approach that combines data modelling and constraint solving to modify existing field data to generate test inputs for testing new data requirements. The approach scales in the presence of complex and structured data, thanks to both the reuse of existing field data and the adoption of an innovative input generation algorithm based on slicing the model into parts. We validated the scalability and effectiveness of the proposed approach using an industrial case study. The empirical study shows that the approach scales in the presence of large amounts of structured and complex data. The approach can produce, within a reasonable time, test input data that is over ten times larger in size than the data generated with constraint solving only. We also demonstrate that the generated test inputs achieve more code coverage than the test cases implemented by experienced software engineers. © 2017 ACM.",Alloy; Data Processing Systems; Model Slicing; System Testing,Alloying; Codes (symbols); Engineers; Logic programming; Software testing; Testing; Constraint Solving; Data processing systems; Generation algorithm; Industrial case study; Model slicing; Requirements change; System level testing; System testing; Data handling
"A posteriori typing for model-driven engineering: Concepts, analysis, and applications",2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019876383&doi=10.1145%2f3063384&partnerID=40&md5=9350ed83de2ecf60821133dc0ce2495d,"Model-Driven Engineering (MDE) is founded on the ability to create and process models conforming to a meta-model. In this context, classes in a meta-model are used in two ways: as templates to create objects and as (static) classifiers for them. These two aspects are inherently tied in most meta-modelling approaches, which results in unnecessarily rigid systems and hinders reusability of MDE artefacts. In this work, we discuss the benefits of decoupling object creation from typing in MDE. Thus, we rely on standard mechanisms for object creation, and propose a posteriori typing as a means to retype objects and enable multiple, partial, dynamic typings. This approach enhances flexibility; permits unanticipated reuse, as model management operations defined for a meta-model can be reused with other models once they get reclassified; and enables bidirectional model transformation by reclassification. In particular, we propose two mechanisms to realise model retyping and show their underlying theory and analysis methods.We show the feasibility of the approach by an implementation atop our meta-modelling tool METADEPTH and present several applications of retypings (transformations, reuse, and dynamicity). © 2017 ACM.",A-posteriori model typing; Bidirectionality; Dynamic typing; METADEPTH; Model transformations; Model-driven engineering; Partial typing; Reusability,Reusability; Bidirectionality; Dynamic typing; METADEPTH; Model transformation; Model typing; Model-driven Engineering; Partial typing; Dynamics
Generating API call rules from version history and stack overflow posts,2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019853701&doi=10.1145%2f2990497&partnerID=40&md5=17c99680ae808869e26c9a4a51bc5c86,"Researchers have shown that related functions can be mined from groupings of functions found in the version history of a system. Our first contribution is to expand this approach to a community of applications and set of similar applications. Android developers use a set of application programming interface (API) calls when creating apps. These API calls are used in similar ways across multiple applications. By clustering co-changing API calls used by 230 Android apps across 12k versions, we are able to predict the API calls that individual app developers will use with an average precision of 75% and recall of 22%. When we make predictions from the same category of app, such as Finance, we attain precision and recall of 81% and 28%, respectively. Our second contribution can be characterized as programmers who discussed these functions were also interested in these functions. Informal discussions on Stack Overflow provide a rich source of information about related API calls as developers provide solutions to common problems. By grouping API calls contained in each positively voted answer posts, we are able to create rules that predict the calls that app developers will use in their own apps with an average precision of 66% and recall of 13%. For comparison purposes, we developed a baseline by clustering co-changing API calls for each individual app and generated association rules from them. The baseline predicts API calls used by app developers with a precision and recall of 36% and 23%, respectively. © 2017 ACM.",API method calls; Association rule mining; Community of applications; Informal documentation; Stack Overflow; Version history,Android (operating system); Association rules; Forecasting; Android apps; API calls; API method calls; Multiple applications; Precision and recall; Related functions; Stack overflow; Application programming interfaces (API)
Dynamic dependence summaries,2017,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011325157&doi=10.1145%2f2968444&partnerID=40&md5=7bc40ffbf57130bc69991d0398130189,"Software engineers construct modern-day software applications by building on existing software libraries and components that they necessarily do not author themselves. Thus, contemporary software applications rely heavily on existing standard and third-party libraries for their execution and behavior. As such, effective runtime analysis of such a software application's behavior is met with new challenges. To perform dynamic analysis of a software application, all transitively dependent external libraries must also be monitored and analyzed at each layer of the software application's call stack. However, monitoring and analyzing large and often numerous external libraries may prove to be prohibitively expensive. Moreover, an overabundance of library-level analyses may obfuscate the details of the actual software application's dynamic behavior. In other words, the extensive use of existing libraries by a software application renders the results of its dynamic analysis both expensive to compute and difficult to understand. We model software component behavior as dynamically observed data- and control dependencies between inputs and outputs of a software component. Such data- and control dependencies are monitored at a fine-grain instruction-level and are collected as dynamic execution traces for software runs. As an approach to address the complexities and expenses associated with analyzing dynamically observable behaviorofsoftware components, we summarize and reuse the data- and control dependencies between the inputs and outputs of software components. Dynamically monitored data- and control dependencies, between the inputs and outputs of software components, upon summarization are called dynamic dependence summaries. Software components, equipped with dynamic dependence summaries, afford the omission of their exhaustive runtime analysis. Nonetheless, the reuse of dependence summaries would necessitate the abstraction of any concrete runtime information enclosed within the summary, thus potentially causing a loss in the information modeled by the dependence summary. Therefore, benefits to the efficiency of dynamic analyses that use such summarization may be afforded with losses of accuracy. As such, we evaluate the potential accuracy loss and the potential performance gain with the use of dynamic dependence summaries. Our results show, on average, a 13× speedup with the use of dynamic dependence summaries, with an accuracy of 90% in a real-world software engineering task. © 2017 ACM.",Dependence analysis; Dynamic analysis; Dynamic slicing; Summaries,Computer software reusability; Dynamic analysis; Dependence analysis; Dynamic slicing; Modeling softwares; Run-time information; Software applications; Software component; Software libraries; Summaries; Application programs
Impact-driven process model repair,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994496335&doi=10.1145%2f2980764&partnerID=40&md5=259a1cd75eff54cfc8a7b9bd4da12211,"The abundance of event data in today's information systems makes it possible to ""confront"" process models with the actual observed behavior. Process mining techniques use event logs to discover process models that describe the observed behavior, and to check conformance of process modelsby diagnosing deviations between models and reality. In many situations, it is desirable to mediate between a preexisting model and observed behavior. Hence, we would like to repair the model while improving the correspondence between model and log as much as possible. The approach presented in this article assigns predefined costs to repair actions (allowing inserting or skipping of activities). Given a maximum degree of change, we search for models that are optimal in terms of fitness - that is, the fraction of behavior in the log not possible according to the model is minimized. To compute fitness, we need to align the model and log, which can be time consuming. Hence, finding an optimal repair may be intractable. We propose different alternative approaches to speed up repair. The number of alignment computations can be reduced dramatically while still returning nearoptimal repairs. The different approaches have been implemented using the process mining framework ProM and evaluated using real-life logs. © 2016 ACM.",Event log; Process mining; Process model; Process model repair; Repair recommendation,Data mining; Event log; Event logs; Maximum degree; Near-optimal; Process mining; Process model; Process Modeling; Speed up; Repair
Stochastic performance analysis of global software development teams,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984984855&doi=10.1145%2f2955093&partnerID=40&md5=c7c2cefd726d0ac7eef9c2533797f965,"Measuring productivity in globally distributed projects is crucial to improve team performance. These measures often display information on whether a given project is moving forward or starts to demonstrate undesired behaviors. In this paper we are interested in showing how analytical models could deliver insights for the behavior of specific distributed software collaboration projects. We present a model for distributed projects using stochastic automata networks (SAN) formalism to estimate, for instance, the required level of coordination for specific project configurations. We focus our attention on the level of interaction among project participants and its close relation with team's productivity. The models are parameterized for different scenarios and solved using numerical methods to obtain exact solutions. We vary the team's expertise and support levels to measure the impact on the overall project performance. As results, we present our derived productivity index for all scenarios and we state implications found in order to analyze popular preconceptions in GSD area, confirming some, and refusing others. Finally, we foresee ways to extend the models to represent more intricate behaviors and communication patterns that are usually present in globally distributed software projects. © 2016 ACM.",Analytical modeling; Global software development; Performance analysis; Stochastic automata networks,Analytical models; Automata theory; Numerical methods; Productivity; Stochastic models; Stochastic systems; Collaboration projects; Communication pattern; Distributed projects; Distributed software; Global software development; Performance analysis; Project participants; Stochastic automata networks; Software design
Hierarchical program paths,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984863659&doi=10.1145%2f2963094&partnerID=40&md5=84d060ad7aa5ab2a3af2f8835cb3b709,"Complete dynamic control flow is a fundamental kind of execution profile about program executions with a wide range of applications. Tracing the dynamic control flow of program executions for a brief period easily generates a trace consisting of billions of control flow events. The number of events in such a trace is large, making both path tracing and path querying to incur significant slowdowns. A major class of path tracing techniques is to design novel trace representations that can be generated efficiently, and encode the inputted sequences of such events so that the inputted sequences are still derivable from the encoded and smaller representations. The control flow semantics in such representations have, however, become obscure, which makes implementing path queries on such a representation inefficient and the design of such queries complicated. We propose a novel two-phase path tracing framework-Hierarchical Program Path (HPP)- To model the complete dynamic control flow of an arbitrary number of executions of a program. In Phase 1, HPP monitors each execution, and efficiently generates a stream of events, namely HPPTree, representing a novel tree-based representation of control flow for each thread of control in the execution. In Phase 2, given a set of such event streams, HPP identifies all the equivalent instances of the same exercised interprocedural path in all the corresponding HPPTree instances, and represents each such equivalent set of paths with a single subgraph, resulting in our compositional graph-based trace representation, namely, HPPDAG. Thus, an HPPDAG instance has the potential to be significantly smaller in size than the corresponding HPPTree instances, and still completely preserves the control flow semantics of the traced executions. Control flow queries over all the traced executions can also be directly performed on the single HPPDAG instance instead of separately processing the trace representation of each execution followed by aggregating their results. We validate HPP using the SPLASH2 and SPECint 2006 benchmarks. Compared to the existing technique, named BLPT (Ball-Larus-based Path Tracing), HPP generates significantly smaller trace representations and incurs fewer slowdowns to the native executions in online tracing of Phase 1. The HPPDAG instances generated in Phase 2 are significantly smaller than their corresponding BLPT and HPPTree traces.We show that HPPDAG supports efficient backtrace querying, which is a representative path query based on complete control flow trace. Finally, we illustrate the ease of use of HPPDAG by building a novel and highly efficient path profiling technique to demonstrate the applicability of HPPDAG. © 2016 ACM.",Hierarchical and compositional representation; Interprocedural path; Path tracing,Codes (symbols); Graphic methods; Semantics; Arbitrary number; Complete control; Compositional representation; Dynamic controls; Inter-procedural; Path tracing; Program execution; Trace representations; Application programs
The effect of program and model structure on the effectiveness of MC/DC test adequacy coverage,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029351387&doi=10.1145%2f2934672&partnerID=40&md5=4d07ed296770d92e15dd8951ce81cfd6,"Test adequacy metrics defined over the structure of a program, such as Modified Condition and Decision Coverage (MC/DC), are used to assess testing efforts. However, MC/DC can be ""cheated"" by restructuring a program to make it easier to achieve the desired coverage. This is concerning, given the importance of MC/DC in assessing the adequacy of test suites for critical systems domains. In this work, we have explored the impact of implementation structure on the efficacy of test suites satisfying the MC/DC criterion using four real-world avionics systems. Our results demonstrate that test suites achieving MC/DC over implementations with structurally complex Boolean expressions are generally larger and more effective than test suites achieving MC/DC over functionally equivalent, but structurally simpler, implementations. Additionally, we found that test suites generated over simpler implementations achieve significantly lower MC/DC and fault-finding effectiveness when applied to complex implementations, whereas test suites generated over the complex implementation still achieve high MC/DC and attain high fault finding over the simpler implementation. By measuring MC/DC over simple implementations, we can significantly reduce the cost of testing, but in doing so, we also reduce the effectiveness of the testing process. Thus, developers have an economic incentive to ""cheat"" the MC/DC criterion, but this cheating leads to negative consequences. Accordingly, we recommend that organizations require MC/DC over a structurally complex implementation for testing purposes to avoid these consequences. © 2016 ACM.",Coverage; Fault finding,Computer software; Software engineering; Boolean expressions; Coverage; Critical systems; Decision coverage; Economic incentive; Fault finding; Implementation structure; Modified conditions; Software testing
Multi-step learning and adaptive search for learning complex model transformations from examples,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975316777&doi=10.1145%2f2904904&partnerID=40&md5=652eb2047e41a293a051b4b72e7318f3,"Model-driven engineering promotes models as main development artifacts. As several models may be manipulated during the software-development life cycle, model transformations ensure their consistency by automating model generation and update tasks. However, writing model transformations requires much knowledge and effort that detract from their benefits. To address this issue, Model Transformation by Example (MTBE) aims to learn transformation programs from source and target model pairs supplied as examples. In this article, we tackle the fundamental issues that prevent the existing MTBE approaches from efficiently solving the problem of learning model transformations. We show that, when considering complex transformations, the search space is too large to be explored by naive search techniques. We propose an MTBE process to learn complex model transformations by considering three common requirements: element context and state dependencies and complex value derivation. Our process relies on two strategies to reduce the size of the search space and to better explore it, namely, multi-step learning and adaptive search. We experimentally evaluate our approach on seven model transformation problems. The learned transformation programs are able to produce perfect target models in three transformation cases, whereas precision and recall values larger than 90% are recorded for the four remaining cases. © 2016 ACM.",Genetic programming; Model transformation; Model transformation by example; Model-driven engineering; Simulated annealing,Computer software; Genetic algorithms; Genetic programming; Life cycle; Simulated annealing; Complex transformations; Model generation; Model transformation; Model-driven Engineering; Precision and recall; Search technique; Software development life cycle; Transformation Program; Software design
Learning weighted assumptions for compositional verification of markov decision processes,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975256386&doi=10.1145%2f2907943&partnerID=40&md5=d099ba88227de28718879311f7408b18,"Probabilistic models are widely deployed in various systems. To ensure their correctness, verification techniques have been developed to analyze probabilistic systems. We propose the first sound and complete learning-based compositional verification technique for probabilistic safety properties on concurrent systems where each component is an Markov decision process. Different from previous works, weighted assumptions are introduced to attain completeness of our framework. Since weighted assumptions can be implicitly represented by multiterminal binary decision diagrams (MTBDDs), we give an L∗-based learning algorithm for MTBDDs to infer weighted assumptions. Experimental results suggest promising outlooks for our compositional technique. © 2016 ACM.",Algorithmic learning; Compositional verification; Probabilistic model checking,Behavioral research; Binary decision diagrams; Markov processes; Model checking; Algorithmic learning; Compositional verification; Markov Decision Processes; Multi-terminal binary decision diagrams; Probabilistic model checking; Probabilistic models; Probabilistic systems; Verification techniques; Learning algorithms
Mining privacy goals from privacy policies using hybridized task recomposition,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975317416&doi=10.1145%2f2907942&partnerID=40&md5=a18c59f493e69249c2f27917e6441b4c,"Privacy policies describe high-level goals for corporate data practices; regulators require industries to make available conspicuous, accurate privacy policies to their customers. Consequently, software requirements must conform to those privacy policies. To help stakeholders extract privacy goals from policies, we introduce a semiautomated framework that combines crowdworker annotations, natural language typed dependency parses, and a reusable lexicon to improve goal-extraction coverage, precision, and recall. The framework evaluation consists of a five-policy corpus governing web and mobile information systems, yielding an average precision of 0.73 and recall of 0.83. The results show that no single framework element alone is sufficient to extract goals; however, the overall framework compensates for elemental limitations. Human annotators are highly adaptive at discovering annotations in new texts, but those annotations can be inconsistent and incomplete; dependency parsers lack sophisticated, tacit knowledge, but they can perform exhaustive text search for prospective requirements indicators; and while the lexicon may never completely saturate, the lexicon terms can be reliably used to improve recall. Lexical reuse reduces false negatives by 41%, increasing the average recall to 0.85. Last, crowd workers were able to identify and remove false positives by around 80%, which improves average precision to 0.93. © 2016 ACM.",Crowdsourcing; Natural language processing; Privacy; Requirements extraction,Crowdsourcing; Data privacy; Extraction; Dependency parser; Framework evaluation; High-level goals; Mobile information systems; Natural languages; Privacy policies; Software requirements; Tacit knowledge; Natural language processing systems
Using cohesion and coupling for software remodularization: Is it enough?,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978101496&doi=10.1145%2f2928268&partnerID=40&md5=1c388de6ebf530b610d80e9c7eeedf91,"Refactoring and, in particular, remodularization operations can be performed to repair the design of a software system and remove the erosion caused by software evolution. Various approaches have been proposed to support developers during the remodularization of a software system. Most of these approaches are based on the underlying assumption that developers pursue an optimal balance between cohesion and coupling when modularizing the classes of their systems. Thus, a remodularization recommender proposes a solution that implicitly provides a (near) optimal balance between such quality attributes. However, there is still no empirical evidence that such a balance is the desideratum by developers. This article aims at analyzing both objectively and subjectively the aforementioned phenomenon. Specifically, we present the results of (1) a large study analyzing the modularization quality, in terms of package cohesion and coupling, of 100 open-source systems, and (2) a survey conducted with 29 developers aimed at understanding the driving factors they consider when performing modularization tasks. The results achieved have been used to distill a set of lessons learned that might be considered to design more effective remodularization recommenders. © 2016 ACM.",Remodularization; Software quality,Adhesion; Computer software selection and evaluation; Modular construction; Open source software; Cohesion and couplings; Modularization qualities; Open source system; Quality attributes; Remodularization; Software Evolution; Software Quality; Software systems; Open systems
Multi-criteria code refactoring using search-based software engineering: An industrial case study,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978052454&doi=10.1145%2f2932631&partnerID=40&md5=2d9a6e26e31902da8615a06995af7e2d,"One of the most widely used techniques to improve the quality of existing software systems is refactoring-the process of improving the design of existing code by changing its internal structure without altering its external behavior. While it is important to suggest refactorings that improve the quality and structure of the system, many other criteria are also important to consider, such as reducing the number of code changes, preserving the semantics of the software design and not only its behavior, and maintaining consistency with the previously applied refactorings. In this article, we propose a multi-objective search-based approach for automating the recommendation of refactorings. The process aims at finding the optimal sequence of refactorings that (i) improves the quality by minimizing the number of design defects, (ii) minimizes code changes required to fix those defects, (iii) preserves design semantics, and (iv) maximizes the consistency with the previously code changes. We evaluated the efficiency of our approach using a benchmark of six opensource systems, 11 different types of refactorings (move method, move field, pull up method, pull up field, push down method, push down field, inline class, move class, extract class, extract method, and extract interface) and six commonly occurring design defect types (blob, spaghetti code, functional decomposition, data class, shotgun surgery, and feature envy) through an empirical study conducted with experts. In addition, we performed an industrial validation of our technique, with 10 software engineers, on a large project provided by our industrial partner. We found that the proposed refactorings succeed in preserving the design coherence of the code, with an acceptable level of code change score while reusing knowledge from recorded refactorings applied in the past to similar contexts. © 2016 ACM.",Multi-objective optimization; Refactoring; Search-based software engineering; Software evolution; Software maintenance,Codes (symbols); Computer software maintenance; Defects; Multiobjective optimization; Optimal systems; Semantics; Software engineering; Functional decomposition; Industrial case study; Industrial partners; Industrial validation; Minimizing the number of; Refactorings; Search-based software engineering; Software Evolution; Software design
Editorial,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971554704&doi=10.1145%2f2904898&partnerID=40&md5=0765ecc9b2fa22b77a425ec9551ddb85,[No abstract available],,
Understanding JavaScript event-based interactions with clematis,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971529351&doi=10.1145%2f2876441&partnerID=40&md5=d2f5a11914a89b6c07fd948870fcbec0,"Web applications have become one of the fastest-growing types of software systems today. Despite their popularity, understanding the behavior of modern web applications is still a challenging endeavor for developers during development and maintenance tasks. The challenges mainly stem from the dynamic, event-driven, and asynchronous nature of the JavaScript language.We propose a generic technique for capturing low-level event-based interactions in a web application and mapping those to a higher-level behavioral model. This model is then transformed into an interactive visualization, representing episodes of triggered causal and temporal events, related JavaScript code executions, and their impact on the dynamic DOM state. Our approach, implemented in a tool called CLEMATIS, allows developers to easily understand the complex dynamic behavior of their application at three different semantic levels of granularity. Furthermore, CLEMATIS helps developers bridge the gap between test cases and program code by localizing the fault related to a test assertion. The results of our industrial controlled experiment show that CLEMATIS is capable of improving the comprehension task accuracy by 157% while reducing the task completion time by 47%. A follow-up experiment reveals that CLEMATIS improves the fault localization accuracy of developers by a factor of two. © 2016 ACM.",Event-based interactions; Fault localization; JavaScript; Program comprehension; Web applications,High level languages; Semantics; Software testing; Visualization; World Wide Web; Event-based; Fault localization; Javascript; Program comprehension; WEB application; Application programs
SIP: Optimal product selection from feature models using many-objective evolutionary optimization,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966293263&doi=10.1145%2f2897760&partnerID=40&md5=6dc73813c5d6455cca83f63af7345deb,"A feature model specifies the sets of features that define valid products in a software product line. Recent work has considered the problem of choosing optimal products from a feature model based on a set of user preferences, with this being represented as a many-objective optimization problem. This problem has been found to be difficult for a purely search-based approach, leading to classicalmany-objective optimization algorithms being enhanced either by adding in a valid product as a seed or by introducing additional mutation and replacement operators that use an SAT solver. In this article, we instead enhance the search in two ways: by providing a novel representation and by optimizing first on the number of constraints that hold and only then on the other objectives. In the evaluation, we also used feature models with realistic attributes, in contrast to previous work that used randomly generated attribute values. The results of experiments were promising, with the proposed (SIP) method returning valid products with six published feature models and a randomly generated feature model with 10,000 features. For the model with 10,000 features, the search took only a few minutes. © 2016 ACM.",Product selection,Algorithms; Optimization; Attribute values; Evolutionary optimizations; Feature modeling; Many-objective optimizations; Objective optimization; Product selection; Sets of features; Software Product Line; Software design
Less is more: Estimating probabilistic rewards over partial system explorations,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966415680&doi=10.1145%2f2890494&partnerID=40&md5=4e487cdf97f21556ff36754eb225aeb1,"Model-based reliability estimation of systems can provide useful insights early in the development process. However, computational complexity of estimating metrics such as mean time to first failure (MTTFF), turnaround time (TAT), or other domain-based quantitative measures can be prohibitive both in time, space, and precision. In this article, we present an alternative to exhaustive model exploration, as in probabilistic model checking, and partial random exploration, as in statistical model checking. Our hypothesis is that a (carefully crafted) partial systematic exploration of a system model can provide better bounds for these quantitative model metrics at lower computation cost. We present a novel automated technique for metric estimation that combines simulation, invariant inference, and probabilistic model checking. Simulation produces a probabilistically relevant set of traces from which a state invariant is inferred. The invariant characterises a partial model, which is then exhaustively explored using probabilistic model checking. We report on experiments that suggest that metric estimation using this technique (for both fully probabilistic models and those exhibiting nondeterminism) can be more effective than (full-model) probabilistic and statistical model checking, especially for system models for which the events of interest are rare. © 2016 ACM.",Estimation; Model checking; Partial verification; Probability; Quantitativemodelling,Estimation; Probability; Turnaround time; Mean time to first failure; Partial verification; Probabilistic model checking; Quantitative measures; Quantitativemodelling; Reliability estimation; Statistical model checking; Systematic exploration; Model checking
Concurrency debugging with differential schedule projections,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966393631&doi=10.1145%2f2885495&partnerID=40&md5=0304532928a7d7332d0a8d7c14e88723,"We present Symbiosis: a concurrency debugging technique based on novel differential schedule projections (DSPs). A DSP shows the small set of memory operations and dataflows responsible for a failure, as well as a reordering of those elements that avoids the failure. To build a DSP, Symbiosis first generates a full, failing, multithreaded schedule via thread path profiling and symbolic constraint solving. Symbiosis selectively reorders events in the failing schedule to produce a nonfailing, alternate schedule. A DSP reports the ordering and dataflow differences between the failing and nonfailing schedules. Our evaluation on buggy real-world software and benchmarks shows that, in practical time, Symbiosis generates DSPs that both isolate the small fraction of event orders and dataflows responsible for the failure and report which event reorderings prevent failing. In our experiments, DSPs contain 90% fewer events and 96% fewer dataflows than the full failure-inducing schedules. We also conducted a user study that shows that, by allowing developers to focus on only a few events, DSPs reduce the amount of time required to understand the bug's root cause and find a valid fix. © 2016 ACM.",Bug localization; Concurrency; Constraint solving; Differential schedule projection,Logic programming; Bug localizations; Concurrency; Constraint Solving; Differential schedule projection; Memory operations; Multithreaded; Path profiling; Root cause; Codes (symbols)
Control explicit-data symbolic model checking,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966322264&doi=10.1145%2f2888393&partnerID=40&md5=21e770f491b7743d5de8b6a291b486bb,"Automatic verification of programs and computer systems with data nondeterminism (e.g., reading from user input) represents a significant and well-motivated challenge. The case of parallel programs is especially difficult, because then also the control flow nontrivially complicates the verification process. We apply the techniques of explicit-state model checking to account for the control aspects of a program to be verified and use set-based reduction of the data flow, thus handling the two sources of nondeterminism separately. We build the theory of set-based reduction using first-order formulae in the bit-vector theory to encode the sets of variable evaluations representing program data. These representations are tested for emptiness and equality (statematching) during the verification, and we harnessmodern satisfiabilitymodulo theory solvers to implement these tests. We design two methods of implementing the state matching, one using quantifiers and one that is quantifier-free, and we provide both analytical and experimental comparisons. Further experiments evaluate the efficiency of the set-based reduction method, showing the classical, explicit approach to fail to scale with the size of data domains. Finally, we propose and evaluate two heuristics to decrease the number of expensive satisfiability queries, together yielding a 10-fold speedup. © 2016 ACM.",Model checking; Modular arithmetic; Static analysis,Data flow analysis; Formal logic; Model checking; Static analysis; Automatic verification; Experimental comparison; Explicit-state model checking; Modular arithmetic; Parallel program; Reduction method; Symbolic model checking; Verification process; Data reduction
DiaPro: Unifying dynamic impact analyses for improved and variable cost-effectiveness,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966447556&doi=10.1145%2f2894751&partnerID=40&md5=4442b441d34938103a965aa6b043c996,"Impact analysis not only assists developers with change planning and management, but also facilitates a range of other client analyses, such as testing and debugging. In particular, for developers working in the context of specific program executions, dynamic impact analysis is usually more desirable than static approaches, as it produces more manageable and relevant results with respect to those concrete executions. However, existing techniques for this analysis mostly lie on two extremes: either fast, but too imprecise, or more precise, yet overly expensive. In practice, both more cost-effective techniques and variable costeffectiveness trade-offs are in demand to fit a variety of usage scenarios and budgets of impact analysis. This article aims to fill the gap between these two extremes with an array of cost-effective analyses and, more broadly, to explore the cost and effectiveness dimensions in the design space of impact analysis. We present the development and evaluation of DiaPro, a framework that unifies a series of impact analyses, including three new hybrid techniques that combine static and dynamic analyses. Harnessing both static dependencies and multiple forms of dynamic data including method-execution events, statement coverage, and dynamic points-to sets, DiaPro prunes false-positive impacts with varying strength for variant effectiveness and overheads. The framework also facilitates an in-depth examination of the effects of various program information on the cost-effectiveness of impact analysis. We applied DiaPro to ten Java applications in diverse scales and domains, evaluating it thoroughly on both arbitrary and repository-based queries from those applications. We show that the three new analyses are all significantly more effective than existing alternatives while remaining efficient, and the DiaPro framework, as a whole, provides flexible cost-effectiveness choices for impact analysis with the best options for variable needs and budgets. Our study results also suggest that hybrid techniques tend to be much more cost-effective than purely dynamic approaches, in general, and that statement coverage has mostly stronger effects than dynamic points-to sets on the cost-effectiveness of dynamic impact analysis, while static dependencies have even stronger effects than both forms of dynamic data. © 2016 ACM.",Costeffectiveness; Coverage; Dependence analysis; Impact analysis; Points-to,Budget control; Cost effectiveness; Costs; Economic and social effects; Java programming language; Cost-effective analysis; Coverage; Dependence analysis; Dynamic impact analysis; Impact analysis; Points-to; Static and dynamic analysis; Testing and debugging; Cost benefit analysis
Inflow and retention in oss communities with commercial involvement: A case study of three hybrid projects,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969961445&doi=10.1145%2f2876443&partnerID=40&md5=79bad1af7ef552e8ef9529d8b4db2d2f,"Motivation: Open-source projects are often supported by companies, but such involvement often affects the robust contributor inflow needed to sustain the project and sometimes prompts key contributors to leave. To capture user innovation and to maintain quality of software and productivity of teams, these projects need to attract and retain contributors. Aim: We want to understand and quantify how inflow and retention are shaped by policies and actions of companies in three application server projects. Method: We identified three hybrid projects implementing the same JavaEE specification and used published literature, online materials, and interviews to quantify actions and policies companies used to get involved. We collected project repository data, analyzed affiliation history of project participants, and used generalized linear models and survival analysis to measure contributor inflow and retention. Results: We identified coherent groups of policies and actions undertaken by sponsoring companies as three models of community involvement and quantified tradeoffs between the inflow and retention each model provides. We found that full control mechanisms and high intensity of commercial involvement were associated with a decrease of external inflow and with improved retention. However, a shared control mechanism was associated with increased external inflow contemporaneously with the increase of commercial involvement. Implications: Inspired by a natural experiment, our methods enabled us to quantify aspects of the balance between community and private interests in open- source software projects and provide clear implications for the structure of future open-source communities.",Commercial involvement; Contributor inflow; Contributor retention; Extent and intensity of involvement; Hybrid project; Natural experiment,Hybrid materials; Open systems; Software engineering; Commercial involvement; Contributor inflow; Contributor retention; Extent and intensity of involvement; Hybrid project; Natural experiment; Open source software
A stack memory abstraction and symbolic analysis framework for executables,2016,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971268062&doi=10.1145%2f2897511&partnerID=40&md5=4590840fd9ca2b07d143ff4c688a943b,"This article makes three contributions regarding reverse-engineering of executables. First, techniques are presented for recovering a precise and correct stack-memory model in executables while addressing executable-specific challenges such as indirect control transfers. Next, the enhanced memory model is employed to define a novel symbolic analysis framework for executables that can perform the same types of program analyses as source-level tools. Third, a demand-driven framework is presented to enhance the scalability of the symbolic analysis framework. Existing symbolic analysis frameworks for executables fail to simultaneously maintain the properties of correct representation, a precise stack-memory model, and scalability. Furthermore, they ignore memory-allocated variables when defining symbolic analysis mechanisms. Our methods do not use symbolic, relocation or debug information, which are usually absent in deployed binaries. We describe our framework, highlighting the novel intellectual contributions of our approach and demonstrating its efficacy and robustness. Our techniques improve the precision of existing stack-memory models by 25%, enhance scalability of our basic symbolic analysis mechanism by 10×, and successfully uncovers five previously undiscovered information-flow vulnerabilities in several widely used programs. © 2016 ACM.",Executable code; Information-flow security; Program analysis,Reverse engineering; Security of data; Demand-driven; Executable codes; Indirect control; Information flow security; Information flows; Memory modeling; Program analysis; Symbolic analysis; Scalability
Testing Updated Apps by Adapting Learned Models,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198653975&doi=10.1145%2f3664601&partnerID=40&md5=ba47ec0970c08466a3716f483ae5b88f,"Although App updates are frequent and software engineers would like to verify updated features only, automated testing techniques verify entire Apps and are thus wasting resources. We present Continuous Adaptation of Learned Models (CALM), an automated App testing approach that efficiently test App updates by adapting App models learned when automatically testing previous App versions. CALM focuses on functional testing. Since functional correctness can be mainly verified through the visual inspection of App screens, CALM minimizes the number of App screens to be visualized by software testers while maximizing the percentage of updated methods and instructions exercised. Our empirical evaluation shows that CALM exercises a significantly higher proportion of updated methods and instructions than six state-of-the-art approaches, for the same maximum number of App screens to be visually inspected. Further, in common update scenarios, where only a small fraction of methods are updated, CALM is even quicker to outperform all competing approaches in a more significant way. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",android testing; Model reuse; model-based testing; regression testing; update testing,Android (operating system); Model checking; Software testing; Android testing; Automated testing; Functional correctness; Functional testing; Model based testing; Model reuse; Regression testing; Testing technique; Update testing; Visual inspection; Application programs
Automatic Repair of Quantum Programs via Unitary Operation,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198727699&doi=10.1145%2f3664604&partnerID=40&md5=6663b64a1ff177727e2d10bd5a418db6,"With the continuous advancement of quantum computing (QC), the demand for high-quality quantum programs (QPs) is growing. To avoid program failure, in software engineering, the technology of automatic program repair (APR) employs appropriate patches to remove potential bugs without the intervention of a human. However, the method tailored for repairing defective QPs is still absent. This article proposes, to the best of our knowledge, a new APR method named UnitAR that can repair QPs via unitary operation automatically. Based on the characteristics of superposition and entanglement in QC, the article constructs an algebraic model and adopts a generate-and-validate approach for the repair procedure. Furthermore, the article presents two schemes that can respectively promote the efficiency of generating patches and guarantee the effectiveness of applying patches. For the purpose of evaluating the proposed method, the article selects 29 mutated versions as well as five real-world buggy programs as the objects and introduces two traditional APR approaches GenProg and TBar as baselines. According to the experiments, UnitAR can fix 23 buggy programs, and this method demonstrates the highest efficiency and effectiveness among three APR approaches. Besides, the experimental results further manifest the crucial roles of two constituents involved in the framework of UnitAR. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",automatic program repair; Quantum computing; quantum software engineering; S-ADA; software cybernetics; unitary operation,Efficiency; Program debugging; Quantum computers; Software engineering; Automatic program repair; Automatic programs; High quality; Programme failures; Quantum Computing; Quantum software engineering; Repair methods; S-ADA; Software cybernetics; Unitary operation; Repair
What Makes a Good TODO Comment?,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198737404&doi=10.1145%2f3664811&partnerID=40&md5=e0ba7f2b45492e0e94604f7783d14674,"Software development is a collaborative process that involves various interactions among individuals and teams. TODO comments in source code play a critical role in managing and coordinating diverse tasks during this process. However, this study finds that a large proportion of open-source project TODO comments are left unresolved or take a long time to be resolved. About 46.7% of TODO comments in open-source repositories are of low-quality (e.g., TODOs that are ambiguous, lack information, or are useless to developers). This highlights the need for better TODO practices. In this study, we investigate four aspects regarding the quality of TODO comments in open-source projects: (1) the prevalence of low-quality TODO comments; (2) the key characteristics of high-quality TODO comments; (3) how are TODO comments of different quality managed in practice; and (4) the feasibility of automatically assessing TODO comment quality. Examining 2,863 TODO comments from Top100 GitHub Java repositories, we propose criteria to identify high-quality TODO comments and provide insights into their optimal composition. We discuss the lifecycle of TODO comments with varying quality. To assist developers, we construct deep learning-based methods that show promising performance in identifying the quality of TODO comments, potentially enhancing development efficiency and code quality. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",comment lifecycle; comment quality; Documentation,Codes (symbols); Deep learning; Life cycle; Open source software; Open systems; Collaborative process; Comment lifecycle; Comment quality; Documentation; High quality; Key characteristics; Low qualities; Open source projects; Open source repositories; Source codes; Software design
The IDEA of Us: An Identity-Aware Architecture for Autonomous Systems,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198856209&doi=10.1145%2f3654439&partnerID=40&md5=cf54786209bb8c283a6fdb5ce1e29662,"Autonomous systems, such as drones and rescue robots, are increasingly used during emergencies. They deliver services and provide situational awareness that facilitate emergency management and response. To do so, they need to interact and cooperate with humans in their environment. Human behaviour is uncertain and complex, so it can be difficult to reason about it formally. In this article, we propose IDEA: an adaptive software architecture that enables cooperation between humans and autonomous systems, by leveraging the social identity approach. This approach establishes that group membership drives human behaviour. Identity and group membership are crucial during emergencies, as they influence cooperation among survivors. IDEA systems infer the social identity of surrounding humans, thereby establishing their group membership. By reasoning about groups, we limit the number of cooperation strategies the system needs to explore. IDEA systems select a strategy from the equilibrium analysis of game-theoretic models that represent interactions between group members and the IDEA system. We demonstrate our approach using a search-and-rescue scenario, in which an IDEA rescue robot optimises evacuation by collaborating with survivors. Using an empirically validated agent-based model, we show that the deployment of the IDEA system can reduce median evacuation time by 13.6%.  © 2024 Copyright held by the owner/author(s).",agent-based modelling; Autonomous systems; game theory; social identity,Autonomous agents; Behavioral research; Computational methods; Risk management; Simulation platform; Adaptive software; Agent-based model; Autonomous system; Emergency management; Emergency response; Group memberships; Human behaviors; Rescue robot; Situational awareness; Social identity; Game theory
MR-Scout: Automated Synthesis of Metamorphic Relations from Existing Test Cases,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198288433&doi=10.1145%2f3656340&partnerID=40&md5=468dbd6c923b293101c6061ac88d55fe,"Metamorphic Testing (MT) alleviates the oracle problem by defining oracles based on metamorphic relations (MRs) that govern multiple related inputs and their outputs. However, designing MRs is challenging, as it requires domain-specific knowledge. This hinders the widespread adoption of MT. We observe that developer-written test cases can embed domain knowledge that encodes MRs. Such encoded MRs could be synthesized for testing not only their original programs but also other programs that share similar functionalities.In this article, we propose MR-Scout to automatically synthesize MRs from test cases in open-source software (OSS) projects. MR-Scout first discovers MR-encoded test cases (MTCs), and then synthesizes the encoded MRs into parameterized methods (called codified MRs), and filters out MRs that demonstrate poor quality for new test case generation. MR-Scout discovered over 11,000 MTCs from 701 OSS projects. Experimental results show that over 97% of codified MRs are of high quality for automated test case generation, demonstrating the practical applicability of MR-Scout. Furthermore, codified-MRs-based tests effectively enhance the test adequacy of programs with developer-written tests, leading to 13.52% and 9.42% increases in line coverage and mutation score, respectively. Our qualitative study shows that 55.76% to 76.92% of codified MRs are easily comprehensible for developers.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",automated test case generation; metamorphic relation; metamorphic testing; Software testing,Automation; Domain Knowledge; Open source software; Open systems; Automated synthesis; Automated test-case generations; Domain knowledge; Domain-specific knowledge; Metamorphic relations; Metamorphic testing; Open source software projects; Oracle problem; Software testings; Test case; Software testing
A Survey of Source Code Search: A 3-Dimensional Perspective,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198841959&doi=10.1145%2f3656341&partnerID=40&md5=2ac8613f58af612aa6268725198ab180,"(Source) code search is widely concerned by software engineering researchers because it can improve the productivity and quality of software development. Given a functionality requirement usually described in a natural language sentence, a code search system can retrieve code snippets that satisfy the requirement from a large-scale code corpus, e.g., GitHub. To realize effective and efficient code search, many techniques have been proposed successively. These techniques improve code search performance mainly by optimizing three core components, including query understanding component, code understanding component, and query-code matching component. In this article, we provide a 3-dimensional perspective survey for code search. Specifically, we categorize existing code search studies into query-end optimization techniques, code-end optimization techniques, and match-end optimization techniques according to the specific components they optimize. These optimization techniques are proposed to enhance the performance of specific components, and thus the overall performance of code search. Considering that each end can be optimized independently and contributes to the code search performance, we treat each end as a dimension. Therefore, this survey is 3-dimensional in nature, and it provides a comprehensive summary of each dimension in detail. To understand the research trends of the three dimensions in existing code search studies, we systematically review 68 relevant literatures. Different from existing code search surveys that only focus on the query end or code end or introduce various aspects shallowly (including codebase, evaluation metrics, modeling technique, etc.), our survey provides a more nuanced analysis and review of the evolution and development of the underlying techniques used in the three ends. Based on a systematic review and summary of existing work, we outline several open challenges and opportunities at the three ends that remain to be addressed in future work.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code-end optimization; deep learning; match-end optimization; query-end optimization; Source code search,Codes (symbols); Computer programming languages; Software design; Code search; Code-end optimization; Deep learning; Match-end optimization; Optimisations; Optimization techniques; Query-end optimization; Source code searches; Deep learning
Reducing the Impact of Time Evolution on Source Code Authorship Attribution via Domain Adaptation,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198736972&doi=10.1145%2f3652151&partnerID=40&md5=00ffed2a81027ca60c6793db04b0139a,"Source code authorship attribution is an important problem in practical applications such as plagiarism detection, software forensics, and copyright disputes. Recent studies show that existing methods for source code authorship attribution can be significantly affected by time evolution, leading to a decrease in attribution accuracy year by year. To alleviate the problem of Deep Learning (DL)-based source code authorship attribution degrading in accuracy due to time evolution, we propose a new framework called Time Domain Adaptation (TimeDA) by adding new feature extractors to the original DL-based code attribution framework that enhances the learning ability of the original model on source domain features without requiring new or more source data. Moreover, we employ a centroid-based pseudo-labeling strategy using neighborhood clustering entropy for adaptive learning to improve the robustness of DL-based code authorship attribution. Experimental results show that TimeDA can significantly enhance the robustness of DL-based source code authorship attribution to time evolution, with an average improvement of 8.7% on the Java dataset and 5.2% on the C++ dataset. In addition, our TimeDA benefits from employing the centroid-based pseudo-labeling strategy, which significantly reduced the model training time by 87.3% compared to traditional unsupervised domain adaptive methods. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Authorship attribution; deep learning; domain adaptation; source code; time evolution,Application programs; C++ (programming language); Codes (symbols); Learning systems; Time domain analysis; Authorship attribution; Deep learning; Detection software; Domain adaptation; Labeling strategy; Plagiarism detection; Software copyrights; Source codes; Time domain; Time evolutions; Deep learning
Mobile Application Online Cross-Project Just-in-Time Software Defect Prediction Framework,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198744513&doi=10.1145%2f3664607&partnerID=40&md5=86481fec6b7b9bc4406076c989c99b6f,"As mobile applications evolve rapidly, their fast iterative update nature leads to an increase in software defects. Just-In-Time Software Defect Prediction (JIT-SDP) offers immediate feedback on code changes. For new applications without historical data, researchers have proposed Cross-Project JIT-SDP (CP JIT-SDP). Existing CP JIT-SDP approaches are designed for offline scenarios where target data is available in advance. However, target data in real-world applications usually arrives online in a streaming manner, making online CP JIT-SDP face cross-project distribution differences and target project data concept drift challenges in online scenarios. These challenges often co-exist during application development, and their interactions cause model performance to degrade. To address these issues, we propose an online CP JIT-SDP framework called COTL. Specifically, COTL consists of two stages: offline and online. In the offline stage, the cross-domain structure preserving projection algorithm is used to reduce the cross-project distribution differences. In the online stage, target data arrives sequentially over time. By reducing the differences in marginal and conditional distributions between offline and online data for target project, concept drift is mitigated and classifier weights are updated online. Experimental results on 15 mobile application benchmark datasets show that COTL outperforms 13 benchmark methods on four performance metrics. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",concept drift; cross-project just-in-time software defect prediction; mobile applications bug prediction; Online transfer learning,Application programs; Benchmarking; Defects; E-learning; Forecasting; Just in time production; Mobile computing; Bug predictions; Concept drifts; Cross-project just-in-time software defect prediction; Just-in-time; Mobile application bug prediction; Mobile applications; Offline; Online transfer learning; Software defect prediction; Transfer learning; Iterative methods
On the Impact of Lower Recall and Precision in Defect Prediction for Guiding Search-based Software Testing,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198710073&doi=10.1145%2f3655022&partnerID=40&md5=50b430aa866fe1a97824de468d3e645b,"Defect predictors, static bug detectors, and humans inspecting the code can propose locations in the program that are more likely to be buggy before they are discovered through testing. Automated test generators such as search-based software testing (SBST) techniques can use this information to direct their search for test cases to likely buggy code, thus speeding up the process of detecting existing bugs in those locations. Often the predictions given by these tools or humans are imprecise, which can misguide the SBST technique and may deteriorate its performance. In this article, we study the impact of imprecision in defect prediction on the bug detection effectiveness of SBST. Our study finds that the recall of the defect predictor, i.e., the proportion of correctly identified buggy code, has a significant impact on bug detection effectiveness of SBST with a large effect size. More precisely, the SBST technique detects 7.5 fewer bugs on average (out of 420 bugs) for every 5% decrements of the recall. However, the effect of precision, a measure for false alarms, is not of meaningful practical significance, as indicated by a very small effect size. In the context of combining defect prediction and SBST, our recommendation is to increase the recall of defect predictors as a primary objective and precision as a secondary objective. In our experiments, we find that 75% precision is as good as 100% precision. To account for the imprecision of defect predictors, in particular low recall values, SBST techniques should be designed to search for test cases that also cover the predicted non-buggy parts of the program, while prioritising the parts that have been predicted as buggy. © 2024 Copyright held by the owner/author(s).",automated test generation; defect prediction; Search-based software testing,Codes (symbols); Forecasting; Program debugging; Software testing; Automated test; Automated test generations; Bug detection; Bug detector; Defect prediction; Effect size; Recall and precision; Search-based software testing; Software testing techniques; Test case; Defects
Advanced White-Box Heuristics for Search-Based Fuzzing of REST APIs,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193823892&doi=10.1145%2f3652157&partnerID=40&md5=933e858605b5b9c5c0b7e85ff71fc654,"Due to its importance and widespread use in industry, automated testing of REST APIs has attracted major interest from the research community in the last few years. However, most of the work in the literature has been focused on black-box fuzzing. Although existing fuzzers have been used to automatically find many faults in existing APIs, there are still several open research challenges that hinder the achievement of better results (e.g., in terms of code coverage and fault finding). For example, under-specified schemas are a major issue for black-box fuzzers. Currently, EvoMaster is the only existing tool that supports white-box fuzzing of REST APIs. In this paper, we provide a series of novel white-box heuristics, including for example how to deal with under-specified constrains in API schemas, as well as under-specified schemas in SQL databases. Our novel techniques are implemented as an extension to our open-source, search-based fuzzer EvoMaster. An empirical study on 14 APIs from the EMB corpus, plus one industrial API, shows clear improvements of the results in some of these APIs.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",fuzzing; OpenAPI; REST; SBST; schema; SQL; Web API,Black boxes; Fuzzing; OpenAPI; REST; SBST; Schema; Search-based; SQL; Web API; White box
Focused Test Generation for Autonomous Driving Systems,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198850503&doi=10.1145%2f3664605&partnerID=40&md5=25cadfbfdbd90043d97d49fbbbf06450,"Testing Autonomous Driving Systems (ADSs) is crucial to ensure their reliability when navigating complex environments. ADSs may exhibit unexpected behaviours when presented, during operation, with driving scenarios containing features inadequately represented in the training dataset. To address this shift from development to operation, developers must acquire new data with the newly observed features. This data can be then utilised to fine tune the ADS, so as to reach the desired level of reliability in performing driving tasks. However, the resource-intensive nature of testing ADSs requires efficient methodologies for generating targeted and diverse tests.In this work, we introduce a novel approach, DeepAtash-LR, that incorporates a surrogate model into the focused test generation process. This integration significantly improves focused testing effectiveness and applicability in resource-intensive scenarios. Experimental results show that the integration of the surrogate model is fundamental to the success of DeepAtash-LR. Our approach was able to generate an average of up to 60× more targeted, failure-inducing inputs compared to the baseline approach. Moreover, the inputs generated by DeepAtash-LR were useful to significantly improve the quality of the original ADS through fine tuning.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",autonomous driving systems; deep learning; search based software engineering; Software testing,Deep learning; Integration testing; Autonomous driving; Autonomous driving system; Complex environments; Deep learning; Driving systems; Search based software engineering; Search-based; Software testings; Surrogate modeling; Test generations; Autonomous vehicles
Testing Multi-Subroutine Quantum Programs: From Unit Testing to Integration Testing,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198714636&doi=10.1145%2f3656339&partnerID=40&md5=a1207ab9a23dc345d5bbcaee41618a6a,"Quantum computing has emerged as a promising field with the potential to revolutionize various domains by harnessing the principles of quantum mechanics. As quantum hardware and algorithms continue to advance, developing high-quality quantum software has become crucial. However, testing quantum programs poses unique challenges due to the distinctive characteristics of quantum systems and the complexity of multi-subroutine programs. This article addresses the specific testing requirements of multi-subroutine quantum programs. We begin by investigating critical properties by surveying existing quantum libraries and providing insights into the challenges of testing these programs. Building upon this understanding, we focus on testing criteria and techniques based on the whole testing process perspective, spanning from unit testing to integration testing. We delve into various aspects, including IO analysis, quantum relation checking, structural testing, behavior testing, integration of subroutine pairs, and test case generation. We also introduce novel testing principles and criteria to guide the testing process. We conduct comprehensive testing on typical quantum subroutines, including diverse mutants and randomized inputs, to evaluate our proposed approach. The analysis of failures provides valuable insights into the effectiveness of our testing methodology. Additionally, we present case studies on representative multi-subroutine quantum programs, demonstrating the practical application and effectiveness of our proposed testing principles and criteria.  © 2024 Copyright held by the owner/author(s).",integration testing; Quantum computing; software testing; unit testing,Application programs; Integration; Quantum computers; Quantum optics; Subroutines; Critical properties; High quality; Principles and criterion; Quantum Computing; Quantum system; Software testings; Testing criteria; Testing process; Testing requirements; Unit testing; Integration testing
Deep Domain Adaptation With Max-Margin Principle for Cross-Project Imbalanced Software Vulnerability Detection,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198828623&doi=10.1145%2f3664602&partnerID=40&md5=be25d5fcd7a190a9bcd8fe341eea9563,"Software vulnerabilities (SVs) have become a common, serious, and crucial concern due to the ubiquity of computer software. Many AI-based approaches have been proposed to solve the software vulnerability detection (SVD) problem to ensure the security and integrity of software applications (in both the development and testing phases). However, there are still two open and significant issues for SVD in terms of (i) learning automatic representations to improve the predictive performance of SVD, and (ii) tackling the scarcity of labeled vulnerability datasets that conventionally need laborious labeling effort by experts. In this paper, we propose a novel approach to tackle these two crucial issues. We first exploit the automatic representation learning with deep domain adaptation for SVD. We then propose a novel cross-domain kernel classifier leveraging the max-margin principle to significantly improve the transfer learning process of SVs from imbalanced labeled into imbalanced unlabeled projects. Our approach is the first work that leverages solid body theories of the max-margin principle, kernel methods, and bridging the gap between source and target domains for imbalanced domain adaptation (DA) applied in cross-project SVD. The experimental results on real-world software datasets show the superiority of our proposed method over state-of-the-art baselines. In short, our method obtains a higher performance on F1-measure, one of the most important measures in SVD, from 1.83% to 6.25% compared to the second highest method in the used datasets.  © 2024 Copyright held by the owner/author(s).",automated cross-project vulnerability detection; Software security,Application programs; Deep learning; Learning systems; Security of data; Automated cross-project vulnerability detection; Detection problems; Development and testing; Development phasis; Domain adaptation; Software applications; Software security; Software vulnerabilities; Testing phase; Vulnerability detection; Software testing
Fairness Testing of Machine Translation Systems,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198860702&doi=10.1145%2f3664608&partnerID=40&md5=d520f1627cd7484a2419c470b2a85a24,"Machine translation is integral to international communication and extensively employed in diverse human-related applications. Despite remarkable progress, fairness issues persist within current machine translation systems. In this article, we propose FairMT, an automated fairness testing approach tailored for machine translation systems. FairMT operates on the assumption that translations of semantically similar sentences, containing protected attributes from distinct demographic groups, should maintain comparable meanings. It comprises three key steps: (1) test input generation, producing inputs covering various demographic groups; (2) test oracle generation, identifying potential unfair translations based on semantic similarity measurements; and (3) regression, discerning genuine fairness issues from those caused by low-quality translation. Leveraging FairMT, we conduct an empirical study on three leading machine translation systems-Google Translate, T5, and Transformer. Our investigation uncovers up to 832, 1,984, and 2,627 unfair translations across the three systems, respectively. Intriguingly, we observe that fair translations tend to exhibit superior translation performance, challenging the conventional wisdom of a fairness-performance tradeoff prevalent in the fairness literature.  © 2024 Copyright held by the owner/author(s).",Fairness testing; machine translation; metamorphic testing; protected attributes,Computational linguistics; Computer aided language translation; Population statistics; Semantics; 'current; Demographic groups; Fairness testing; International communication; Machine translation systems; Machine translations; Metamorphic testing; Protected attribute; Test inputs; Test oracles; Machine translation
Enhancing GUI Exploration Coverage of Android Apps with Deep Link-Integrated Monkey,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198746691&doi=10.1145%2f3664810&partnerID=40&md5=f65d8900dc90c9fe9972a0f5304a3df6,"Mobile apps are ubiquitous in our daily lives for supporting different tasks such as reading and chatting. Despite the availability of many GUI testing tools, app testers still struggle with low testing code coverage due to tools frequently getting stuck in loops or overlooking activities with concealed entries. This results in a significant amount of testing time being spent on redundant and repetitive exploration of a few GUI pages. To address this, we utilize Android’s deep links, which assist in triggering Android intents to lead users to specific pages and introduce a deep link-enhanced exploration method. This approach, integrated into the testing tool Monkey, gives rise to Delm (Deep Link-enhanced Monkey). Delm oversees the dynamic exploration process, guiding the tool out of meaningless testing loops to unexplored GUI pages. We provide a rigorous activity context mock-up approach for triggering existing Android intents to discover more activities with hidden entrances. We conduct experiments to evaluate Delm’s effectiveness on activity context mock-up, activity coverage, method coverage, and crash detection. The findings reveal that Delm can mock up more complex activity contexts and significantly outperform state-of-the-art baselines with 27.2% activity coverage, 21.13% method coverage, and 23.81% crash detection. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",android app exploration; android GUI testing; app analysis; GUI,Android (operating system); Mockups; Activity contexts; Android app exploration; Android apps; Android GUI testing; App analyse; Daily lives; GUI testing; Mobile app; Mock up; Testing tools; Graphical user interfaces
Risky Dynamic Typing-related Practices in Python: An Empirical Study,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198738278&doi=10.1145%2f3649593&partnerID=40&md5=ffaf70a46eb403db4d4febcb8896515c,"Python’s dynamic typing nature provides developers with powerful programming abstractions. However, many type-related bugs are accumulated in code bases of Python due to the misuse of dynamic typing. The goal of this article is to aid in the understanding of developers’ high-risk practices toward dynamic typing and the early detection of type-related bugs. We first formulate the rules of six types of risky dynamic typing-related practices (type smells for short) in Python. We then develop a rule-based tool named RUPOR, which builds an accurate type base to detect type smells. Our evaluation shows that RUPOR outperforms the existing type smell detection techniques (including the Large Language Models–based approaches, Mypy, and PYDYPE) on a benchmark of 900 Python methods. Based on RUPOR, we conduct an empirical study on 25 real-world projects. We find that type smells are significantly related to the occurrence of post-release faults. The fault-proneness prediction model built with type smell features slightly outperforms the model built without them. We also summarize the common patterns, including inserting type check to fix type smell bugs. These findings provide valuable insights for preventing and fixing type-related bugs in the programs written in dynamic-typed languages. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bug fixing; Dynamic typing; empirical study; Python,High level languages; Odors; Program debugging; Bug-fixing; Dynamic typing; Empirical studies; Fault-proneness prediction; Language model; Model based approach; Prediction modelling; Programming abstractions; Real world projects; Rule based; Python
Early and Realistic Exploitability Prediction of Just-Disclosed Software Vulnerabilities: How Reliable Can It Be?,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198662650&doi=10.1145%2f3654443&partnerID=40&md5=b4c826ea524a1c478c61ca991cb6c656,"With the rate of discovered and disclosed vulnerabilities escalating, researchers have been experimenting with machine learning to predict whether a vulnerability will be exploited. Existing solutions leverage information unavailable when a CVE is created, making them unsuitable just after the disclosure. This paper experiments with early exploitability prediction models driven exclusively by the initial CVE record, i.e., the original description and the linked online discussions. Leveraging NVD and Exploit Database, we evaluate 72 prediction models trained using six traditional machine learning classifiers, four feature representation schemas, and three data balancing algorithms. We also experiment with five pre-trained large language models (LLMs). The models leverage seven different corpora made by combining three data sources, i.e., CVE description, Security Focus, and BugTraq. The models are evaluated in a realistic, time-aware fashion by removing the training and test instances that cannot be labeled “neutral” with sufficient confidence. The validation reveals that CVE descriptions and Security Focus discussions are the best data to train on. Pretrained LLMs do not show the expected performance, requiring further pre-training in the security domain. We distill new research directions, identify possible room for improvement, and envision automated systems assisting security experts in assessing the exploitability. © 2024 Copyright held by the owner/author(s).",Exploitability prediction; machine learning; natural language processing; software vulnerabilities; text mining,Automation; Balancing; Classification (of information); Learning algorithms; Machine learning; Natural language processing systems; Exploitability prediction; Language model; Language processing; Machine-learning; Model-driven; Natural language processing; Natural languages; Prediction modelling; Software vulnerabilities; Text-mining; Forecasting
Replication in Requirements Engineering: The NLP for RE Case,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198652329&doi=10.1145%2f3658669&partnerID=40&md5=3918fc9b42fefa45ad1557cb40771fb7,"Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Despite its empirical vocation, RE research has given limited attention to replication of NLP for RE studies. Replication is hampered by several factors, including the context specificity of the studies, the heterogeneity of the tasks involving NLP, the tasks’ inherent hairiness, and, in turn, the heterogeneous reporting structure. To address these issues, we propose a new artifact, referred to as ID-Card, whose goal is to provide a structured summary of research papers emphasizing replication-relevant information. We construct the ID-Card through a structured, iterative process based on design science. In this article: (i) we report on hands-on experiences of replication; (ii) we review the state-of-the-art and extract replication-relevant information: (iii) we identify, through focus groups, challenges across two typical dimensions of replication: data annotation and tool reconstruction; and (iv) we present the concept and structure of the ID-Card to mitigate the identified challenges. This study aims to create awareness of replication in NLP for RE. We propose an ID-Card that is intended to foster study replication but can also be used in other contexts, e.g., for educational purposes. © 2024 Copyright held by the owner/author(s).",annotation; ID card; Natural Language Processing (NLP); replication; Requirements Engineering (RE); tool reconstruction,Employment; Iterative methods; Natural language processing systems; Annotation; ID cards; Language processing; Language processing techniques; Natural language processing; Natural languages; Replication; Requirement engineering; Tool reconstruction; Requirements engineering
On the Way to SBOMs: Investigating Design Issues and Solutions in Practice,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198621547&doi=10.1145%2f3654442&partnerID=40&md5=8c78497e9f85812d05d63f6ad31a6ee7,"The increase of software supply chain threats has underscored the necessity for robust security mechanisms, among which the Software Bill of Materials (SBOM) stands out as a promising solution. SBOMs, by providing a machine-readable inventory of software composition details, play a crucial role in enhancing transparency and traceability within software supply chains. This empirical study delves into the practical challenges and solutions associated with the adoption of SBOMs through an analysis of 4,786 GitHub discussions across 510 SBOM-related projects. Through repository mining and analysis, this research delineates key topics, challenges, and solutions intrinsic to the effective utilization of SBOMs. Furthermore, we shed light on commonly used tools and frameworks for SBOM generation, exploring their respective strengths and limitations. This study underscores a set of findings, for example, there are four phases of the SBOM life cycle, and each phase has a set of SBOM development activities and issues; in addition, this study emphasizes the role SBOM play in ensuring resilient software development practices and the imperative of their widespread adoption and integration to bolster supply chain security. The insights of our study provide vital input for future work and practical advancements in this topic. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",empirical study; mining software repository; SBOM; software bill of materials; Software supply chain,Computer software; Software design; Supply chains; Bill of materials; Design issues; Empirical studies; Mining software; Mining software repository; Software bill of material; Software repositories; Software supply chains; Life cycle
Data Complexity: A New Perspective for Analyzing the Difficulty of Defect Prediction Tasks,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198614605&doi=10.1145%2f3649596&partnerID=40&md5=73f4737acd8ac0f6fff42587b790ffa3,"Defect prediction is crucial for software quality assurance and has been extensively researched over recent decades. However, prior studies rarely focus on data complexity in defect prediction tasks, and even less on understanding the difficulties of these tasks from the perspective of data complexity. In this article, we conduct an empirical study to estimate the hardness of over 33,000 instances, employing a set of measures to characterize the inherent difficulty of instances and the characteristics of defect datasets. Our findings indicate that: (1) instance hardness in both classes displays a right-skewed distribution, with the defective class exhibiting a more scattered distribution; (2) class overlap is the primary factor influencing instance hardness and can be characterized through feature, structural, and instance-level overlap; (3) no universal preprocessing technique is applicable to all datasets, and it may not consistently reduce data complexity, fortunately, dataset complexity measures can help identify suitable techniques for specific datasets; (4) integrating data complexity information into the learning process can enhance an algorithm’s learning capacity. In summary, this empirical study highlights the crucial role of data complexity in defect prediction tasks, and provides a novel perspective for advancing research in defect prediction techniques. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data complexity; Defect prediction; instance hardness; machine learning,Computer software selection and evaluation; Defects; Forecasting; Hardness; Learning algorithms; Quality assurance; Characteristics of defect; Data complexity; Defect prediction; Empirical studies; Instance hardness; Machine-learning; Prediction tasks; Primary factors; Right-skewed distributions; Software quality assurance; Machine learning
On Estimating the Feasible Solution Space of Multi-objective Testing Resource Allocation,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198625101&doi=10.1145%2f3654444&partnerID=40&md5=444d5445ec32f4d4ff18c33ac307b953,"The multi-objective testing resource allocation problem (MOTRAP) is concerned on how to reasonably plan the testing time of software testers to save the cost and improve the reliability as much as possible. The feasible solution space of a MOTRAP is determined by its variables (i.e., the time invested in each component) and constraints (e.g., the pre-specified reliability, cost, or time). Although a variety of state-of-the-art constrained multi-objective optimisers can be used to find individual solutions in this space, their search remains inefficient and expensive due to the fact that this space is very tiny compared to the large search space. The decision maker may often suffer a prolonged but unsuccessful search that fails to return a feasible solution. In this work, we first formulate a heavily constrained MOTRAP on the basis of an architecture-based model, in which reliability, cost, and time are optimised under the pre-specified multiple constraints on reliability, cost, and time. Then, to estimate the feasible solution space of this specific MOTRAP, we develop theoretical and algorithmic approaches to deduce new tighter lower and upper bounds on variables from constraints. Importantly, our approach can help the decision maker identify whether their constraint settings are practicable, and meanwhile, the derived bounds can just enclose the tiny feasible solution space and help off-the-shelf constrained multi-objective optimisers make the search within the feasible solution space as much as possible. Additionally, to further make good use of these bounds, we propose a generalised bound constraint handling method that can be readily employed by constrained multi-objective optimisers to pull infeasible solutions back into the estimated space with theoretical guarantee. Finally, we evaluate our approach on application and empirical cases. Experimental results reveal that our approach significantly enhances the efficiency, effectiveness, and robustness of off-the-shelf constrained multi-objective optimisers and state-of-the-art bound constraint handling methods at finding high-quality solutions for the decision maker. These improvements may help the decision maker take the stress out of setting constraints and selecting constrained multi-objective optimisers and facilitate the testing planning more efficiently and effectively. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bound constraint handling; constrained multi-objective optimisers; estimation of feasible solution space; lower and upper bounds; Multi-objective multi-constraint testing resource allocation,Constrained optimization; Constraint handling; Decision making; Multiobjective optimization; Software reliability; Software testing; Bound constraint handling; Bound constraints; Constrained multi-objective optimiser; Constraint handling; Estimation of feasible solution space; Feasible solution spaces; Lower and upper bounds; Multi objective; Multi-constraints; Multi-objective multi-constraint testing resource allocation; Resources allocation; Testing resources; Resource allocation
Help Them Understand: Testing and Improving Voice User Interfaces,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198855836&doi=10.1145%2f3654438&partnerID=40&md5=91b8dba61128620675cef0395bf8683c,"Voice-based virtual assistants are becoming increasingly popular. Such systems provide frameworks to developers for building custom apps. End-users can interact with such apps through a Voice User Interface (VUI), which allows the user to use natural language commands to perform actions. Testing such apps is not trivial: The same command can be expressed in different semantically equivalent ways. In this article, we introduce VUI-UPSET, an approach that adapts chatbot-testing approaches to VUI-testing. We conducted an empirical study to understand how VUI-UPSET compares to two state-of-the-art approaches (i.e., a chatbot testing technique and ChatGPT) in terms of (i) correctness of the generated paraphrases, and (ii) capability of revealing bugs. To this aim, we analyzed 14,898 generated paraphrases for 40 Alexa Skills. Our results show that VUI-UPSET generates more bug-revealing paraphrases than the two baselines with, however, ChatGPT being the approach generating the highest percentage of correct paraphrases. We also tried to use the generated paraphrases to improve the skills. We tried to include in the voice interaction models of the skills (i) only the bug-revealing paraphrases, (ii) all the valid paraphrases. We observed that including only bug-revealing paraphrases is sometimes not sufficient to make all the tests pass.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",NLP;; software testing; Voice user interfaces,Program debugging; User interfaces; Chatbots; Empirical studies; End-users; Interface testings; Natural languages; NLP;; Software testings; Two-state; Virtual assistants; Voice user interface; Software testing
Do Code Summarization Models Process Too Much Information? Function Signature May Be All That Is Needed,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198858676&doi=10.1145%2f3652156&partnerID=40&md5=a75251eed72a380e60820228133b807a,"With the fast development of large software projects, automatic code summarization techniques, which summarize the main functionalities of a piece of code using natural languages as comments, play essential roles in helping developers understand and maintain large software projects. Many research efforts have been devoted to building automatic code summarization approaches. Typical code summarization approaches are based on deep learning models. They transform the task into a sequence-to-sequence task, which inputs source code and outputs summarizations in natural languages. All code summarization models impose different input size limits, such as 50 to 10,000, for the input source code. However, how the input size limit affects the performance of code summarization models still remains under-explored. In this article, we first conduct an empirical study to investigate the impacts of different input size limits on the quality of generated code comments. To our surprise, experiments on multiple models and datasets reveal that setting a low input size limit, such as 20, does not necessarily reduce the quality of generated comments.Based on this finding, we further propose to use function signatures instead of full source code to summarize the main functionalities first and then input the function signatures into code summarization models. Experiments and statistical results show that inputs with signatures are, on average, more than 2 percentage points better than inputs without signatures and thus demonstrate the effectiveness of involving function signatures in code summarization. We also invite programmers to do a questionnaire to evaluate the quality of code summaries generated by two inputs with different truncation levels. The results show that function signatures generate, on average, 9.2% more high-quality comments than full code.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Code summarization; empirical study; function signature,Codes (symbols); Computer programming languages; Automatic codes; Code summarization; Empirical studies; Function signatures; Input size; Input sources; Natural languages; Software project; Source codes; Summarization models; Deep learning
MTL-TRANSFER: Leveraging Multi-task Learning and Transferred Knowledge for Improving Fault Localization and Program Repair,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198729306&doi=10.1145%2f3654441&partnerID=40&md5=0c84477035045e2634ae975e6a3e72a0,"Fault localization (FL) and automated program repair (APR) are two main tasks of automatic software debugging. Compared with traditional methods, deep learning-based approaches have been demonstrated to achieve better performance in FL and APR tasks. However, the existing deep learning-based FL methods ignore the deep semantic features or only consider simple code representations. And for APR tasks, existing template-based APR methods are weak in selecting the correct fix templates for more effective program repair, which are also not able to synthesize patches via the embedded end-to-end code modification knowledge obtained by training models on large-scale bug-fix code pairs. Moreover, in most of FL and APR methods, the model designs and training phases are performed separately, leading to ineffective sharing of updated parameters and extracted knowledge during the training process. This limitation hinders the further improvement in the performance of FL and APR tasks. To solve the above problems, we propose a novel approach called MTL-TRANSFER, which leverages a multi-task learning strategy to extract deep semantic features and transferred knowledge from different perspectives. First, we construct a large-scale open-source bug datasets and implement 11 multi-task learning models for bug detection and patch generation sub-tasks on 11 commonly used bug types, as well as one multi-classifier to learn the relevant semantics for the subsequent fix template selection task. Second, an MLP-based ranking model is leveraged to fuse spectrum-based, mutation-based and semantic-based features to generate a sorted list of suspicious statements. Third, we combine the patches generated by the neural patch generation sub-task from the multi-task learning strategy with the optimized fix template selecting order gained from the multi-classifier mentioned above. Finally, the more accurate FL results, the optimized fix template selecting order, and the expanded patch candidates are combined together to further enhance the overall performance of APR tasks. Our extensive experiments on widely-used benchmark Defects4J show that MTL-TRANSFER outperforms all baselines in FL and APR tasks, proving the effectiveness of our approach. Compared with our previously proposed FL method TRANSFER-FL (which is also the state-of-the-art statement-level FL method), MTL-TRANSFER increases the faults hit by 8/11/12 on Top-1/3/5 metrics (92/159/183 in total). And on APR tasks, the number of successfully repaired bugs of MTL-TRANSFER under the perfect localization setting reaches 75, which is 8 more than our previous APR method TRANSFER-PR. Furthermore, another experiment to simulate the actual repair scenarios shows that MTL-TRANSFER can successfully repair 15 and 9 more bugs (56 in total) compared with TBar and TRANSFER, which demonstrates the effectiveness of the combination of our optimized FL and APR components. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",automated program repair; deep neural networks; Fault localization; multi-task learning; neural machine translation; transfer learning,Classification (of information); Data mining; Knowledge management; Large datasets; Learning algorithms; Learning systems; Linearization; Open source software; Program debugging; Repair; Semantics; Transfer learning; Automated program repair; Fault localization; Large-scales; Learning strategy; Localization method; Multitask learning; Performance; Repair methods; Semantic features; Transfer learning; Deep neural networks
"Supporting Emotional Intelligence, Productivity and Team Goals while Handling Software Requirements Changes",2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198653631&doi=10.1145%2f3664600&partnerID=40&md5=7468212fe83d27f47c558ae096649f05,"Background: Research shows that emotional intelligence (EI) should be used alongside cognitive intelligence during requirements change (RC) handling in Software Engineering (SE), especially in agile settings. Objective: We wanted to study the role of EI in-depth during RC handling. Method: We conducted a mixed-methods study (an interview study followed by a survey study) with 124 software practitioners. Findings: We found the causal condition, intervening condition and causes lead to key direct consequences of regulating own emotions, managing relationships, and extended consequences of sustaining productivity, setting and sustaining team goals. We found several strategies of supporting EI during RC handling. Further, we found strong correlations between six strategies and one being aware of own emotions, regulating own emotions, sustaining team productivity, and setting and sustaining team goals. Conclusion: Empathising with others and tracking commitments and decisions as a team are key strategies that have strong correlations between managing emotions, between sustaining team productivity, and between setting and sustaining team goals. To the best of our knowledge, the framework we present in this paper is the first theoretical framework on EI in SE research. We provide recommendations for software practitioners to consider during RC handling. © 2024 Copyright held by the owner/author(s).",affects; agile; changes; emotional intelligence; Emotions; human factors; productivity; requirements; socio-technical grounded theory; software engineering; software teams; team goals; well-being; workplace awareness,Requirements engineering; Software engineering; Affect; Agile; Change; Emotion; Emotional intelligence; Grounded theory; Requirement; Socio-technical grounded theory; Sociotechnical; Software teams; Team goal; Well being; Workplace awareness; Human engineering
BatFix: Repairing language model-based transpilation,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198857460&doi=10.1145%2f3658668&partnerID=40&md5=d7a00957039b74095b48dc3e3f4416a9,"To keep up with changes in requirements, frameworks, and coding practices, software organizations might need to migrate code from one language to another. Source-to-source migration, or transpilation, is often a complex, manual process. Transpilation requires expertise both in the source and target language, making it highly laborious and costly. Languages models for code generation and transpilation are becoming increasingly popular. However, despite capturing code-structure well, code generated by language models is often spurious and contains subtle problems. We propose BatFix, a novel approach that augments language models for transpilation by leveraging program repair and synthesis to fix the code generated by these models. BatFix takes as input both the original program, the target program generated by the machine translation model, and a set of test cases and outputs a repaired program that passes all test cases. Experimental results show that our approach is agnostic to language models and programming languages. BatFix can locate bugs spawning multiple lines and synthesize patches for syntax and semantic bugs for programs migrated from Java to C++ and Python to C++ from multiple language models, including, OpenAI's Codex.  © 2024 Copyright held by the owner/author(s).",automated refactoring; machine learning; Program analysis; transpilation,C++ (programming language); Computational linguistics; Computer aided language translation; Machine learning; Machine translation; Program debugging; Semantics; Software testing; Automated refactoring; Change in requirements; Language model; Machine-learning; Model-based OPC; Program analysis; Refactorings; Software organization; Test case; Transpilation; Python
DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198848495&doi=10.1145%2f3644388&partnerID=40&md5=ce819dad9f43db7ef677a01f41015d75,"Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. In particular, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this article, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault-revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by selecting diverse mispredicted inputs. The experimental results conducted on four widely used datasets and five DNN models show that in terms of fault-revealing ability, (1) white-box, coverage-based approaches fare poorly, (2) DeepGD outperforms existing black-box test selection approaches in terms of fault detection, and (3) DeepGD also leads to better guidance for DNN model retraining when using selected inputs to augment the training set.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning model evaluation; Deep neural network; diversity; DNN fault detection; model retraining guidance; multi-objective optimization; test case selection; uncertainty metrics,Air navigation; Fault detection; Large datasets; Learning algorithms; Multiobjective optimization; Natural language processing systems; Neural network models; Speech recognition; Statistical tests; Deep learning model evaluation; Deep neural network fault detection; Diversity; Faults detection; Learning models; Model evaluation; Model retraining guidance; Multi-objectives optimization; Network faults; Test case selection; Uncertainty; Uncertainty metric; Deep neural networks
Navigating the Complexity of Generative AI Adoption in Software Engineering,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195509905&doi=10.1145%2f3652154&partnerID=40&md5=6ef98a08def3bacb62f3c20ece06ec93,"This article explores the adoption of Generative Artificial Intelligence (AI) tools within the domain of software engineering, focusing on the influencing factors at the individual, technological, and social levels. We applied a convergent mixed-methods approach to offer a comprehensive understanding of AI adoption dynamics. We initially conducted a questionnaire survey with 100 software engineers, drawing upon the Technology Acceptance Model, the Diffusion of Innovation Theory, and the Social Cognitive Theory as guiding theoretical frameworks. Employing the Gioia methodology, we derived a theoretical model of AI adoption in software engineering: the Human-AI Collaboration and Adaptation Framework. This model was then validated using Partial Least Squares-Structural Equation Modeling based on data from 183 software engineers. Findings indicate that at this early stage of AI integration, the compatibility of AI tools within existing development workflows predominantly drives their adoption, challenging conventional technology acceptance theories. The impact of perceived usefulness, social factors, and personal innovativeness seems less pronounced than expected. The study provides crucial insights for future AI tool design and offers a framework for developing effective organizational implementation strategies.  © 2024 Copyright held by the owner/author(s).",empirical software engineering; Generative AI; large language models; technology adaption,Least squares approximations; Social aspects; Software engineering; Artificial intelligence tools; Diffusions of innovation theories; Empirical Software Engineering; Generative artificial intelligence; Language model; Large language model; Mixed method; Questionnaire surveys; Technology acceptance model; Technology adaption; Digital storage
KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189459114&doi=10.1145%2f3643675&partnerID=40&md5=88657176048a0dbfc3d0a49f087be7b5,"Commit messages are natural language descriptions of code changes, which are important for software evolution such as code understanding and maintenance. However, previous methods are trained on the entire dataset without considering the fact that a portion of commit messages adhere to good practice (i.e., good-practice commits), while the rest do not. On the basis of our empirical study, we discover that training on good-practice commits significantly contributes to the commit message generation. Motivated by this finding, we propose a novel knowledge-aware denoising learning method called KADEL. Considering that good-practice commits constitute only a small proportion of the dataset, we align the remaining training samples with these good-practice commits. To achieve this, we propose a model that learns the commit knowledge by training on good-practice commits. This knowledge model enables supplementing more information for training samples that do not conform to good practice. However, since the supplementary information may contain noise or prediction errors, we propose a dynamic denoising training method. This method composes a distribution-aware confidence function and a dynamic distribution list, which enhances the effectiveness of the training process. Experimental results on the whole MCMD dataset demonstrate that our method overall achieves state-of-the-art performance compared with previous methods. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Commit message generation; denoising training; knowledge introducing,Codes (symbols); Knowledge management; Learning systems; Code changes; Commit message generation; De-noising; Denoising training; Good practices; Knowledge introducing; Language description; Natural languages; Software Evolution; Training sample; Sampling
SGuard+: Machine Learning Guided Rule-Based Automated Vulnerability Repair on Smart Contracts,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195883563&doi=10.1145%2f3641846&partnerID=40&md5=282e695b120885a168dcdac86a1784c0,"Smart contracts are becoming appealing targets for hackers because of the vast amount of cryptocurrencies under their control. Asset loss due to the exploitation of smart contract codes has increased significantly in recent years. To guarantee that smart contracts are vulnerability-free, there are many works to detect the vulnerabilities of smart contracts, but only a few vulnerability repair works have been proposed. Repairing smart contract vulnerabilities at the source code level is attractive as it is transparent to users, whereas existing repair tools, such as SCRepair and sGuard, suffer from many limitations: (1) ignoring the code of vulnerability prevention; (2) possibly applying the repair to the wrong statements and changing the original business logic of smart contracts; and (3) showing poor performance in terms of time and gas overhead. In this work, we propose machine learning guided rule-based automated vulnerability repair on smart contracts to improve the effectiveness and efficiency of sGuard. To address the limitations mentioned above, we design the features that characterize both the symptoms of vulnerabilities and the methods of vulnerability prevention to learn various vulnerability patterns and reduce false positives. Additionally, a fine-grained localization algorithm is designed by traversing the nodes of the abstract syntax tree, and we refine and extend the repair rules of sGuard to preserve the original business logic of smart contracts and support new vulnerability types. Our tool, named sGuard+, reduces time overhead based on machine learning models, and reduces gas overhead by fewer code changes and precise patching. In our experiment, we collect a publicly available vulnerability dataset from CVE, SWC, and SmartBugs Curated as a ground truth for evaluations. Overall, sGuard+ repairs more vulnerabilities with less time and gas overhead than state-of-the-art tools. Furthermore, we reproduce about 9,000 historical transactions for regression testing. It is shown that sGuard+ has no impact on the original business logic of smart contracts.  © 2024 Copyright held by the owner/author(s).",machine learning; smart contract; Vulnerability repair,Codes (symbols); Computer circuits; Machine learning; Personal computing; Repair; Trees (mathematics); Business logic; Effectiveness and efficiencies; Machine-learning; Poor performance; Repair tools; Repair works; Rule based; Source codes; Vulnerability prevention; Vulnerability repair; Smart contract
Fine-grained Coverage-based Fuzzing - RCR Report,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195520949&doi=10.1145%2f3649592&partnerID=40&md5=8f46bad2144d079dda2042962be96026,"This is the RCR report of the artifact for the article ""Fine-grained Coverage-based Fuzzing.""This report contains scripts and pre-build binary programs to reproduce the results presented in the main article. The artifact is released on Zenodo with DOI: 10.5281/zenodo.7275184. We claim the artifact to be available, functional, and reusable. The technology skills needed to review the artifact are knowing how to use Linux/Unix terminal and a basic understanding of Docker.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code coverage criteria; Fuzzing; mutation testing,Binary programs; Code coverage; Code coverage criteria; Coverage criteria; Fine grained; Fuzzing; Mutation testing; Technology skills; Software testing
Machine Translation Testing via Syntactic Tree Pruning,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195863318&doi=10.1145%2f3640329&partnerID=40&md5=84d170e5ff6022a4ea11ec7ac757137c,"Machine translation systems have been widely adopted in our daily life, making life easier and more convenient. Unfortunately, erroneous translations may result in severe consequences, such as financial losses. This requires to improve the accuracy and the reliability of machine translation systems. However, it is challenging to test machine translation systems because of the complexity and intractability of the underlying neural models. To tackle these challenges, we propose a novel metamorphic testing approach by syntactic tree pruning (STP) to validate machine translation systems. Our key insight is that a pruned sentence should have similar crucial semantics compared with the original sentence. Specifically, STP (1) proposes a core semantics-preserving pruning strategy by basic sentence structures and dependency relations on the level of syntactic tree representation, (2) generates source sentence pairs based on the metamorphic relation, and (3) reports suspicious issues whose translations break the consistency property by a bag-of-words model. We further evaluate STP on two state-of-the-art machine translation systems (i.e., Google Translate and Bing Microsoft Translator) with 1,200 source sentences as inputs. The results show that STP accurately finds 5,073 unique erroneous translations in Google Translate and 5,100 unique erroneous translations in Bing Microsoft Translator (400% more than state-of-the-art techniques), with 64.5% and 65.4% precision, respectively. The reported erroneous translations vary in types and more than 90% of them are not found by state-of-the-art techniques. There are 9,393 erroneous translations unique to STP, which is 711.9% more than state-of-the-art techniques. Moreover, STP is quite effective in detecting translation errors for the original sentences with a recall reaching 74.0%, improving state-of-the-art techniques by 55.1% on average.  © 2024 Copyright held by the owner/author(s).",machine translation; metamorphic testing; Software testing,Computational linguistics; Computer aided language translation; Information retrieval; Losses; Machine translation; Semantics; Syntactics; Daily lives; Google translate; Machine translation systems; Machine translations; Metamorphic testing; MicroSoft; Software testings; State-of-the-art techniques; Syntactic trees; Tree pruning; Software testing
Generating Python Type Annotations from Type Inference: How Far Are We?,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195633748&doi=10.1145%2f3652153&partnerID=40&md5=093693801886728cdf28fe616b332997,"In recent years, dynamic languages such as Python have become popular due to their flexibility and productivity. The lack of static typing makes programs face the challenges of fixing type errors, early bug detection, and code understanding. To alleviate these issues, PEP 484 introduced optional type annotations for Python in 2014, but unfortunately, a large number of programs are still not annotated by developers. Annotation generation tools can utilize type inference techniques. However, several important aspects of type annotation generation are overlooked by existing works, such as in-depth effectiveness analysis, potential improvement exploration, and practicality evaluation. And it is unclear how far we have been and how far we can go.In this paper, we set out to comprehensively investigate the effectiveness of type inference tools for generating type annotations, applying three categories of state-of-the-art tools on a carefully-cleaned dataset. First, we use a comprehensive set of metrics and categories, finding that existing tools have different effectiveness and cannot achieve both high accuracy and high coverage. Then, we summarize six patterns to present the limitations in type annotation generation. Next, we implement a simple but effective tool to demonstrate that existing tools can be improved in practice. Finally, we conduct a controlled experiment showing that existing tools can reduce the time spent annotating types and determine more precise types, but cannot reduce subjective difficulty. Our findings point out the limitations and improvement directions in type annotation generation, which can inspire future work.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",empirical study; Python; Type annotations; type inference,Computer software; High level languages; Bug detection; Code understanding; Dynamic languages; Empirical studies; Generation tools; Inference techniques; Static typing; Type annotations; Type errors; Type inferences; Python
Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187400144&doi=10.1145%2f3638247&partnerID=40&md5=37a4b47d9e8fefa20f0d887f69306946,"The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e., prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we systematically review, summarise, refine and extend the concept of AI chain by incorporating the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematize AI chain engineering methodology. We also develop a no-code integrated development environment, Prompt Sapper, which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AI chain engineering; large language models; No/Low code; SE for AI; visual programming,Application programming interfaces (API); Codes (symbols); Computational linguistics; Computer software reusability; AI chain engineering; Foundation models; Image modeling; Language model; Large language model; Natural languages; No/low code; Production tools; SE for AI; Visual programming; Visual languages
Analyzing and Detecting Information Types of Developer Live Chat Threads,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195450966&doi=10.1145%2f3643677&partnerID=40&md5=d4b867c93d3fb48a4710bfa60951a2d1,"Online chatrooms serve as vital platforms for information exchange among software developers. With multiple developers engaged in rapid communication and diverse conversation topics, the resulting chat messages often manifest complexity and lack structure. To enhance the efficiency of extracting information from chat threads, automatic mining techniques are introduced for thread classification. However, previous approaches still grapple with unsatisfactory classification accuracy due to two primary challenges that they struggle to adequately capture long-distance dependencies within chat threads and address the issue of category imbalance in labeled datasets. To surmount these challenges, we present a topic classification approach for chat information types named EAEChat. Specifically, EAEChat comprises three core components: the text feature encoding component captures contextual text features using a multi-head self-attention mechanism-based text feature encoder, and a siamese network is employed to mitigate overfitting caused by limited data; the data augmentation component expands a small number of categories in the training dataset using a technique tailored to developer chat messages, effectively tackling the challenge of imbalanced category distribution; the non-text feature encoding component employs a feature fusion model to integrate deep text features with manually extracted non-text features. Evaluation across three real-world projects demonstrates that EAEChat, respectively, achieves an average precision, recall, and F1-score of 0.653, 0.651, and 0.644, and it marks a significant 7.60% improvement over the state-of-the-art approaches. These findings confirm the effectiveness of our method in proficiently classifying developer chat messages in online chatrooms. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data augmentation; deep learning; Developer chatroom; information type classification,Deep learning; Encoding (symbols); Signal encoding; Social networking (online); Chat rooms; Data augmentation; Deep learning; Developer chatroom; Encodings; Information exchanges; Information type classification; Information types; Text feature; Type classifications; Classification (of information)
Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195521336&doi=10.1145%2f3644387&partnerID=40&md5=2dcc69e38f047bf191ac50a08b2384b8,"As a new programming paradigm, deep neural networks (DNNs) have been increasingly deployed in practice, but the lack of robustness hinders their applications in safety-critical domains. While there are techniques for verifying DNNs with formal guarantees, they are limited in scalability and accuracy. In this article, we present a novel counterexample-guided abstraction refinement (CEGAR) approach for scalable and exact verification of DNNs. Specifically, we propose a novel abstraction to break down the size of DNNs by over-approximation. The result of verifying the abstract DNN is conclusive if no spurious counterexample is reported. To eliminate each spurious counterexample introduced by abstraction, we propose a novel counterexample-guided refinement that refines the abstract DNN to exclude the spurious counterexample while still over-approximating the original one, leading to a sound, complete yet efficient CEGAR approach. Our approach is orthogonal to and can be integrated with many existing verification techniques. For demonstration, we implement our approach using two promising tools, Marabou and Planet, as the underlying verification engines, and evaluate on widely used benchmarks for three datasets ACAS, Xu, MNIST, and CIFAR-10. The results show that our approach can boost their performance by solving more problems in the same time limit, reducing on average 13.4%-86.3% verification time of Marabou on almost all the verification tasks, and reducing on average 8.3%-78.0% verification time of Planet on all the verification tasks. Compared to the most relevant CEGAR-based approach, our approach is 11.6-26.6 times faster. © 2024 Copyright held by the owner/author(s).",abstraction; CEGAR; formal verification; Neural networks; refinement; robustness,Abstracting; Formal verification; Safety engineering; Abstraction; Break down; Counterexample-guided abstraction refinement; Neural-networks; Programming paradigms; Refinement; Robustness; Safety-critical domain; Verification task; Verification techniques; Deep neural networks
An Empirical Analysis of Issue Templates Usage in Large-Scale Projects on GitHub,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185688542&doi=10.1145%2f3643673&partnerID=40&md5=e0c6d2807e9496dceab25ba64d2208fe,"GitHub Issues is a widely used issue tracking tool in open-source software projects. Originally designed with broad flexibility, its lack of standardization led to incomplete issue reports, impeding software development and maintenance efficiency. To counteract this, GitHub introduced issue templates in 2016, which rapidly became popular. Our study assesses the current use and evolution of these templates in large-scale open-source projects and their impact on issue tracking metrics, including resolution time, number of reopens, and number of issue comments. Employing a comprehensive analysis of 350 templates from 100 projects, we also evaluated over 1.9 million issues for template conformity and impact. Additionally, we solicited insights from open-source software maintainers through a survey. Our findings highlight issue templates' extensive usage in 99 of the 100 surveyed projects, with a growing preference for YAML-based templates, a more structured template variant. Projects with a template exhibited markedly reduced resolution time (381.02 days to 103.18 days) and reduced issue comment count (4.95 to 4.32) compared to those without. The use of YAML-based templates further significantly decreased resolution time, the number of reopenings, and the discussion extent. Thus, our research underscores issue templates' positive impact on large-scale open-source projects, offering recommendations for improved effectiveness.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bug tracking; empirical study; GitHub issues; issue forms; Issue templates; issue tracking,Open systems; Software design; Bug tracking; Empirical analysis; Empirical studies; Github issue; Issue form; Issue template; Issue Tracking; Large-scales; Open source projects; Resolution time; Open source software
Beyond Fidelity: Explaining Vulnerability Localization of Learning-Based Detectors,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195857631&doi=10.1145%2f3641543&partnerID=40&md5=da3dd0cfd40f41c19e37605b5b09130a,"Vulnerability detectors based on deep learning (DL) models have proven their effectiveness in recent years. However, the shroud of opacity surrounding the decision-making process of these detectors makes it difficult for security analysts to comprehend. To address this, various explanation approaches have been proposed to explain the predictions by highlighting important features, which have been demonstrated effective in domains such as computer vision and natural language processing. Unfortunately, there is still a lack of in-depth evaluation of vulnerability-critical features, such as fine-grained vulnerability-related code lines, learned and understood by these explanation approaches. In this study, we first evaluate the performance of ten explanation approaches for vulnerability detectors based on graph and sequence representations, measured by two quantitative metrics including fidelity and vulnerability line coverage rate. Our results show that fidelity alone is insufficent for evaluating these approaches, as fidelity incurs significant fluctuations across different datasets and detectors. We subsequently check the precision of the vulnerability-related code lines reported by the explanation approaches, and find poor accuracy in this task among all of them. This can be attributed to the inefficiency of explainers in selecting important features and the presence of irrelevant artifacts learned by DL-based detectors.  © 2024 Copyright held by the owner/author(s).",Coverage Rate; Explanation Approaches; Fidelity; Vulnerability Detection,Codes (symbols); Decision making; Natural language processing systems; Code line; Coverage rate; Decision-making process; Explanation approach; Fidelity; Important features; Learning models; Localisation; Securities analysts; Vulnerability detection; Deep learning
Non-Autoregressive Line-Level Code Completion,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195505309&doi=10.1145%2f3649594&partnerID=40&md5=8e895ee00b4e0374b6efb3ec686b60d7,"Software developers frequently use code completion tools to accelerate software development by suggesting the following code elements. Researchers usually employ AutoRegressive (AR) decoders to complete code sequences in a left-to-right, token-by-token fashion. To improve the accuracy and efficiency of code completion, we argue that tokens within a code statement have the potential to be predicted concurrently. In this article, we first conduct an empirical study to analyze the dependency among the target tokens in line-level code completion. The results suggest that it is potentially practical to generate all statement tokens in parallel. To this end, we introduce SANAR, a simple and effective syntax-aware non-autoregressive model for line-level code completion. To further improve the quality of the generated code, we propose an adaptive and syntax-aware sampling strategy to boost the model's performance. The experimental results obtained from two widely used datasets indicate that our model outperforms state-of-the-art code completion approaches of similar model size by a considerable margin, and is faster than these models with up to 9× speed-up. Moreover, the extensive results additionally demonstrate that the enhancements achieved by SANAR become even more pronounced with larger model sizes, highlighting their significance. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Code completion; neural networks; non-autoregressive generation,Codes (symbols); Network coding; Syntactics; Auto-regressive; Code completions; Code sequences; Completion tools; Empirical studies; Model size; Neural-networks; Non-autoregressive generation; Simple++; Software developer; Software design
DinoDroid: Testing Android Apps Using Deep Q-Networks,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195583752&doi=10.1145%2f3652150&partnerID=40&md5=0ef33c32c40399501a701c479b1f6955,"The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers need to guarantee the quality of mobile apps before it is released to the market. There have been many approaches using different strategies to test the GUI of mobile apps. However, they still need improvement due to their limited effectiveness. In this article, we propose DinoDroid, an approach based on deep Q-networks to automate testing of Android apps. DinoDroid learns a behavior model from a set of existing apps and the learned model can be used to explore and generate tests for new apps. DinoDroid is able to capture the fine-grained details of GUI events (e.g., the content of GUI widgets) and use them as features that are fed into deep neural network, which acts as the agent to guide app exploration. DinoDroid automatically adapts the learned model during the exploration without the need of any modeling strategies or pre-defined rules. We conduct experiments on 64 open-source Android apps. The results showed that DinoDroid outperforms existing Android testing tools in terms of code coverage and bug detection.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep q-networks; Mobile testing; reinforcement learning,Android (operating system); Deep neural networks; Graphical user interfaces; Open source software; Android applications; Android apps; Behaviour models; Deep q-network; Fine grained; Learn+; Mobile applications; Mobile testing; New applications; Reinforcement learnings; Reinforcement learning
Try with Simpler - An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195499099&doi=10.1145%2f3644386&partnerID=40&md5=d44d9fa2d8801f4e5d6b1f98d58aa48e,"With the rapid development of deep learning (DL), the recent trend of log-based anomaly detection focuses on extracting semantic information from log events (i.e., templates of log messages) and designing more advanced DL models for anomaly detection. Indeed, the effectiveness of log-based anomaly detection can be improved, but these DL-based techniques further suffer from the limitations of more heavy dependency on training data (such as data quality or data labels) and higher costs in time and resources due to the complexity and scale of DL models, which hinder their practical use. On the contrary, the techniques based on traditional machine learning or data mining algorithms are less dependent on training data and more efficient but produce worse effectiveness than DL-based techniques, which is mainly caused by the problem of unseen log events (some log events in incoming log messages are unseen in training data) confirmed by our motivating study. Intuitively, if we can improve the effectiveness of traditional techniques to be comparable with advanced DL-based techniques, then log-based anomaly detection can be more practical. Indeed, an existing study in the other area (i.e., linking questions posted on Stack Overflow) has pointed out that traditional techniques with some optimizations can indeed achieve comparable effectiveness with the state-of-the-art DL-based technique, indicating the feasibility of enhancing traditional log-based anomaly detection techniques to some degree.Inspired by the idea of ""try-with-simpler,""we conducted the first empirical study to explore the potential of improving traditional techniques for more practical log-based anomaly detection. In this work, we optimized the traditional unsupervised PCA (Principal Component Analysis) technique by incorporating a lightweight semantic-based log representation in it, called SemPCA, and conducted an extensive study to investigate the potential of SemPCA for more practical log-based anomaly detection. By comparing seven log-based anomaly detection techniques (including four DL-based techniques, two traditional techniques, and SemPCA) on both public and industrial datasets, our results show that SemPCA achieves comparable effectiveness as advanced supervised/semi-supervised DL-based techniques while being much more stable under insufficient training data and more efficient, demonstrating that the traditional technique can still excel after small but useful adaptation. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Anomaly detection; deep learning; empirical study; log analysis; machine learning,Data mining; Deep learning; Principal component analysis; Semantics; Anomaly detection; Deep learning; Empirical studies; Learning models; Log analysis; Machine-learning; Principal-component analysis; Simple++; Traditional techniques; Training data; Anomaly detection
Lessons Learned from Developing a Sustainability Awareness Framework for Software Engineering Using Design Science,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195488520&doi=10.1145%2f3649597&partnerID=40&md5=7dad7ae6bbff346bd97880cb22ad00e4,"To foster a sustainable society within a sustainable environment, we must dramatically reshape our work and consumption activities, most of which are facilitated through software. Yet, most software engineers hardly consider the effects on the sustainability of the IT products and services they deliver. This issue is exacerbated by a lack of methods and tools for this purpose. Despite the practical need for methods and tools that explicitly support consideration of the effects that IT products and services have on the sustainability of their intended environments, such methods and tools remain largely unavailable. Thus, urgent research is needed to understand how to design such tools for the IT community properly. In this article, we describe our experience using design science to create the Sustainability Awareness Framework (SusAF), which supports software engineers in anticipating and mitigating the potential sustainability effects during system development. More specifically, we identify and present the challenges faced during this process. The challenges that we have faced and addressed in the development of the SusAF are likely to be relevant to others who aim to create methods and tools to integrate sustainability analysis into their IT products and services development. Thus, the lessons learned in SusAF development are shared for the benefit of researchers and other professionals who design tools for that end. © 2024 Copyright held by the owner/author(s).",IT products; IT services; sustainability analysis,Product design; Software engineering; Design science; Integrate sustainability; IT products; IT services; Product and service development; Product and services; Sustainability analysis; Sustainable environment; Sustainable society; System development; Sustainable development
On the Reliability and Explainability of Language Models for Program Generation,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192111957&doi=10.1145%2f3641540&partnerID=40&md5=4ae02d81375f48db579529b2231d3373,"Recent studies have adopted pre-trained language models, such as CodeT5 and CodeGPT, for automated program generation tasks like code generation, repair, and translation. Numerous language model based approaches have been proposed and evaluated on various benchmark datasets, demonstrating promising performance. However, there is still uncertainty about the reliability of these models, particularly their realistic ability to consistently transform code sequences. This raises a question: are these techniques sufficiently trustworthy for automated program generation? Consequently, further research is needed to understand model logic and assess reliability and explainability. To bridge these research gaps, we conduct a thorough empirical study of eight popular language models on five representative datasets to determine the capabilities and limitations of automated program generation approaches. We further employ advanced explainable AI approaches to highlight the tokens that significantly contribute to the code transformation. We discover that state-of-the-art approaches suffer from inappropriate performance evaluation stemming from severe data duplication, causing overoptimistic results. Our explainability analysis reveals that, in various experimental scenarios, language models can recognize code grammar and structural information, but they exhibit limited robustness to changes in input sequences. Overall, more rigorous evaluation approaches and benchmarks are critical to enhance the reliability and explainability of automated program generation moving forward. Our findings provide important guidelines for this goal.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated program generation; empirical analysis; explainable AI,Automation; Codes (symbols); Computational linguistics; Computer aided language translation; Cosine transforms; Program translators; Reliability analysis; Automated program generation; Benchmark datasets; Code translation; Codegeneration; Empirical analysis; Explainable AI; Language model; Model based approach; Performance; Program generation; Benchmarking
Fairness Testing: A Comprehensive Survey and Analysis of Trends,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195494519&doi=10.1145%2f3652155&partnerID=40&md5=224147c76de68a99b98647359b9e5d73,"Unfair behaviors of Machine Learning (ML) software have garnered increasing attention and concern among software engineers. To tackle this issue, extensive research has been dedicated to conducting fairness testing of ML software, and this article offers a comprehensive survey of existing studies in this field. We collect 100 papers and organize them based on the testing workflow (i.e., how to test) and testing components (i.e., what to test). Furthermore, we analyze the research focus, trends, and promising directions in the realm of fairness testing. We also identify widely adopted datasets and open-source tools for fairness testing.  © 2024 Copyright held by the owner/author(s).",analysis; fairness testing; Machine learning; survey; trends,Open source software; Software testing; Analyse; Fairness testing; Machine learning software; Machine-learning; Open source tools; Research focus; Survey and analysis; Tests and testing; Trend; Work-flows; Machine learning
Enumerating Valid Non-Alpha-Equivalent Programs for Interpreter Testing,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195457464&doi=10.1145%2f3647994&partnerID=40&md5=d88c67e3caa07987ec63427aa010e42b,"Skeletal program enumeration (SPE) can generate a great number of test programs for validating the correctness of compilers or interpreters. The classic SPE generates programs by exhaustively enumerating all possible variable usage patterns into a given syntactic structure. Even though it is capable of producing many test programs, the exhaustive enumeration strategy generates a large number of invalid programs, which may waste plenty of testing time and resources. To address the problem, this article proposes a tree-based SPE technique. Compared to the state-of-the-art, the key merit of the tree-based approach is that it allows us to take the dependency information into consideration when producing test programs and, thus, make it possible to (1) directly generate non-equivalent programs and (2) apply dominance relations to eliminate invalid test programs that have undefined variables. Hence, our approach significantly saves the cost of the naïve SPE approach. We have implemented our approach into an automated testing tool, IFuzzer, and applied it to test eight different implementations of Python interpreters, including CPython, PyPy, IronPython, Jython, RustPython, GPython, Pyston, and Codon. In three months of fuzzing, IFuzzer detected 142 bugs, of which 87 have been confirmed to be previously unknown bugs, of which 34 have been fixed. Compared to the state-of-the-art SPE techniques, IFuzzer takes only 61.0% of the time cost given the same number of testing seeds and improves 5.3% source code function coverage in the same time budget of testing. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",fuzz testing; Interpreter testing; program enumeration,Budget control; Program compilers; Program interpreters; Python; Syntactics; Enumeration techniques; Exhaustive enumeration; Fuzz Testing; Interpreter testing; Program enumeration; State of the art; Syntactic structure; Test projects; Testing time; Usage patterns; Software testing
Communicating Study Design Trade-offs in Software Engineering,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195548858&doi=10.1145%2f3649598&partnerID=40&md5=e07c7d0a35433a819aa1c307d8f8e4ec,"Reflecting on the limitations of a study is a crucial part of the research process. In software engineering studies, this reflection is typically conveyed through discussions of study limitations or threats to validity. In current practice, such discussions seldom provide sufficient insight to understand the rationale for decisions taken before and during the study, and their implications. We revisit the practice of discussing study limitations and threats to validity and identify its weaknesses. We propose to refocus this practice of self-reflection to a discussion centered on the notion of trade-offs. We argue that documenting trade-offs allows researchers to clarify how the benefits of their study design decisions outweigh the costs of possible alternatives. We present guidelines for reporting trade-offs in a way that promotes a fair and dispassionate assessment of researchers' work.  © 2024 Copyright held by the owner/author(s).",Empirical software engineering; empirical study design; metascience; research design trade-offs; research validity; threats to validity,Commerce; Economic and social effects; Engineering research; Design tradeoff; Empirical Software Engineering; Empirical studies; Empirical study design; Metascience; Research design trade-offs; Research designs; Research validity; Study design; Threat to validity; Software engineering
RAPID: Zero-Shot Domain Adaptation for Code Search with Pre-Trained Models,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193792863&doi=10.1145%2f3641542&partnerID=40&md5=3cf0b85cb4b31969d7aa62f2a30d5824,"Code search, which refers to the process of identifying the most relevant code snippets for a given natural language query, plays a crucial role in software maintenance. However, current approaches heavily rely on labeled data for training, which results in performance decreases when confronted with cross-domain scenarios including domain- or project-specific situations. This decline can be attributed to their limited ability to effectively capture the semantics associated with such scenarios. To tackle the aforementioned problem, we propose a zeRo-shot domAin adaPtion with pre-traIned moDels framework for code search named RAPID. The framework first generates synthetic data by pseudo labeling, then trains the CodeBERT with sampled synthetic data. To avoid the influence of noisy synthetic data and enhance the model performance, we propose a mixture sampling strategy to obtain hard negative samples during training. Specifically, the mixture sampling strategy considers both relevancy and diversity to select the data that are hard to be distinguished by the models. To validate the effectiveness of our approach in zero-shot settings, we conduct extensive experiments and find that RAPID outperforms the CoCoSoDa and UniXcoder model by an average of 15.7% and 10%, respectively, as measured by the MRR metric. When trained on full data, our approach results in an average improvement of 7.5% under the MRR metric using CodeBERT. We observe that as the model's performance in zero-shot tasks improves, the impact of hard negatives diminishes. Our observation also indicates that fine-tuning CodeT5 for generating pseudo labels can enhance the performance of the code search model, and using only 100-shot samples can yield comparable results to the supervised baseline. Furthermore, we evaluate the effectiveness of RAPID in real-world code search tasks in three GitHub projects through both human and automated assessments. Our findings reveal RAPID exhibits superior performance, e.g., an average improvement of 18% under the MRR metric over the top-performing model.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Code search; domain adaption; pre-trained models; software maintenance; zero-shot learning,Codes (symbols); Computer software maintenance; Natural language processing systems; Zero-shot learning; 'current; Code search; Domain adaptation; Domain adaptions; Modeling performance; Natural language queries; Performance; Pre-trained model; Sampling strategies; Synthetic data; Semantics
Dynamic Transitive Closure-based Static Analysis through the Lens of Quantum Search,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195478554&doi=10.1145%2f3644389&partnerID=40&md5=bb4674e55841362aa544f9cde39d4cfb,"Many existing static analysis algorithms suffer from cubic bottlenecks because of the need to compute a dynamic transitive closure (DTC). For the first time, this article studies the quantum speedups on searching subtasks in DTC-based static analysis algorithms using quantum search (e.g., Grover's algorithm). We first introduce our oracle implementation in Grover's algorithm for DTC-based static analysis and illustrate our quantum search subroutine. Then, we take two typical DTC-based analysis algorithms: context-free-language reachability and set constraint-based analysis, and show that our quantum approach can reduce the time complexity of these two algorithms to truly subcubic (), yielding better results than the upper bound (O(N3/log N)) of existing classical algorithms. Finally, we conducted a classical simulation of Grover's search to validate our theoretical approach, due to the current quantum hardware limitation of lacking a practical, large-scale, noise-free quantum machine. We evaluated the correctness and efficiency of our approach using IBM Qiskit on nine open-source projects and randomly generated edge-labeled graphs/constraints. The results demonstrate the effectiveness of our approach and shed light on the promising direction of applying quantum algorithms to address the general challenges in static analysis. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CFL-reachability; grover's search; set constraint-based analysis,Context free languages; Quantum theory; CFL-reachability; Constraint-based analysis; Grover's Algorithm; Grover's search; Quantum search; Reachability; Set constraint-based analyse; Set constraints; Static-analysis algorithm; Transitive closure; Static analysis
Learning Failure-Inducing Models for Testing Software-Defined Networks,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195878361&doi=10.1145%2f3641541&partnerID=40&md5=a07897116ced7b33d4168e76c1fb58f8,"Software-defined networks (SDN) enable flexible and effective communication systems that are managed by centralized software controllers. However, such a controller can undermine the underlying communication network of an SDN-based system and thus must be carefully tested. When an SDN-based system fails, in order to address such a failure, engineers need to precisely understand the conditions under which it occurs. In this article, we introduce a machine learning-guided fuzzing method, named FuzzSDN, aiming at both (1) generating effective test data leading to failures in SDN-based systems and (2) learning accurate failure-inducing models that characterize conditions under which such system fails. To our knowledge, no existing work simultaneously addresses these two objectives for SDNs. We evaluate FuzzSDN by applying it to systems controlled by two open-source SDN controllers. Furthermore, we compare FuzzSDN with two state-of-the-art methods for fuzzing SDNs and two baselines for learning failure-inducing models. Our results show that (1) compared to the state-of-the-art methods, FuzzSDN generates at least 12 times more failures, within the same time budget, with a controller that is fairly robust to fuzzing and (2) our failure-inducing models have, on average, a precision of 98% and a recall of 86%, significantly outperforming the baselines.  © 2024 Copyright held by the owner/author(s).",fuzzing; machine learning; software testing; Software-defined networks,Budget control; Controllers; Machine learning; Open source software; Open systems; Software defined networking; Condition; Effective communication; Flexible communication; Fuzzing; Machine-learning; Network based systems; Software testings; Software-defined networks; State-of-the-art methods; Testing software; Software testing
Test Input Prioritization for 3D Point Clouds,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195460684&doi=10.1145%2f3643676&partnerID=40&md5=61c858095f7afe51100977ee7a53c66d,"3D point cloud applications have become increasingly prevalent in diverse domains, showcasing their efficacy in various software systems. However, testing such applications presents unique challenges due to the high-dimensional nature of 3D point cloud data and the vast number of possible test cases. Test input prioritization has emerged as a promising approach to enhance testing efficiency by prioritizing potentially misclassified test cases during the early stages of the testing process. Consequently, this enables the early labeling of critical inputs, leading to a reduction in the overall labeling cost. However, applying existing prioritization methods to 3D point cloud data is constrained by several factors: (1) inadequate consideration of crucial spatial information, and (2) susceptibility to noises inherent in 3D point cloud data. In this article, we propose PCPrior, the first test prioritization approach specifically designed for 3D point cloud test cases. The fundamental concept behind PCPrior is that test inputs closer to the decision boundary of the model are more likely to be predicted incorrectly. To capture the spatial relationship between a point cloud test and the decision boundary, we propose transforming each test (a point cloud) into a low-dimensional feature vector, toward indirectly revealing the underlying proximity between a test and the decision boundary. To achieve this, we carefully design a group of feature generation strategies, and for each test input, we generate four distinct types of features, namely spatial features, mutation features, prediction features, and uncertainty features. Through a concatenation of the four feature types, PCPrior assembles a final feature vector for each test. Subsequently, a ranking model is employed to estimate the probability of misclassification for each test based on its feature vector. Finally, PCPrior ranks all tests based on their misclassification probabilities. We conducted an extensive study based on 165 subjects to evaluate the performance of PCPrior, encompassing both natural and noisy datasets. The results demonstrate that PCPrior outperforms all of the compared test prioritization approaches, with an average improvement of 10.99% to 66.94% on natural datasets and 16.62% to 53% on noisy datasets. © 2024 Copyright held by the owner/author(s).",deep neural network; labeling; learning to rank; Test input prioritization,Application programs; 3D point cloud; Decision boundary; Features vector; Labelings; Point cloud data; Prioritization; Test case; Test input prioritization; Test inputs; Test prioritization; Deep neural networks
Precisely Extracting Complex Variable Values from Android Apps,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195447709&doi=10.1145%2f3649591&partnerID=40&md5=057468053fbe2a9526013ba46e8d4a39,"Millions of users nowadays rely on their smartphones to process sensitive data through apps from various vendors and sources. Therefore, it is vital to assess these apps for security vulnerabilities and privacy violations. Information such as to which server an app connects through which protocol, and which algorithm it applies for encryption, are usually encoded as variable values and arguments of API calls. However, extracting these values from an app is not trivial. The source code of an app is usually not available, and manual reverse engineering is cumbersome with binary sizes in the tens of megabytes. Current automated tools, however, cannot retrieve values that are computed at runtime through complex transformations.In this article, we present ValDroid, a novel static analysis tool for automatically extracting the set of possible values for a given variable at a given statement in the Dalvik byte code of an Android app. We evaluate ValDroid against existing approaches (JSA, Violist, DroidRA, Harvester, BlueSeal, StringHound, IC3, and COAL) on benchmarks and 794 real-world apps. ValDroid greatly outperforms existing tools. It provides an average F1 score of more than 90%, while only requiring 0.1 s per value on average. For many data types including Network Connections and Dynamic Code Loading, its recall is more than twice the recall of the best existing approaches. © 2024 Copyright held by the owner/author(s).",Android; mobile; security; Static analysis; value analysis,Android (operating system); Codes (symbols); Complex networks; Cryptography; Mobile security; Reverse engineering; Sensitive data; Android; Android apps; Complex variable; Mobile; Privacy violation; Security; Security vulnerabilities; Security/privacy; Sensitive datas; Smart phones; Static analysis
Automated Mapping of Vulnerability Advisories onto their Fix Commits in Open Source Repositories,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195458543&doi=10.1145%2f3649590&partnerID=40&md5=ec95681d71677b8e5339ea30c6cb7656,"The lack of comprehensive sources of accurate vulnerability data represents a critical obstacle to studying and understanding software vulnerabilities (and their corrections). In this article, we present an approach that combines heuristics stemming from practical experience and machine-learning (ML) - specifically, natural language processing (NLP) - to address this problem. Our method consists of three phases. First, we construct an advisory record object containing key information about a vulnerability that is extracted from an advisory, such as those found in the National Vulnerability Database (NVD). These advisories are expressed in natural language. Second, using heuristics, a subset of candidate fix commits is obtained from the source code repository of the affected project, by filtering out commits that can be identified as unrelated to the vulnerability at hand. Finally, for each of the remaining candidate commits, our method builds a numerical feature vector reflecting the characteristics of the commit that are relevant to predicting its match with the advisory at hand. Based on the values of these feature vectors, our method produces a ranked list of candidate fixing commits. The score attributed by the ML model to each feature is kept visible to the users, allowing them to easily interpret the predictions.We implemented our approach and we evaluated it on an open data set, built by manual curation, that comprises 2,391 known fix commits corresponding to 1,248 public vulnerability advisories. When considering the top-10 commits in the ranked results, our implementation could successfully identify at least one fix commit for up to 84.03% of the vulnerabilities (with a fix commit on the first position for 65.06% of the vulnerabilities). Our evaluation shows that our method can reduce considerably the manual effort needed to search open-source software (OSS) repositories for the commits that fix known vulnerabilities. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code-level vulnerability data; common vulnerabilities and exposures (CVE); machine learning applied to software security; mining software repositories; National Vulnerability Database (NVD); Open source software; software security,Codes (symbols); Machine learning; Natural language processing systems; Numerical methods; Open Data; Open systems; Code-level vulnerability data; Common vulnerabilities and exposures; Common vulnerability and exposure; Machine learning applied to software security; Machine-learning; Mining software; Mining software repository; National vulnerability database; Open-source softwares; Software repositories; Software security; Open source software
Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195840166&doi=10.1145%2f3643671&partnerID=40&md5=b10e112a5818c85cca0d319ede7758ab,"The adoption of deep neural networks (DNNs) in safety-critical contexts is often prevented by the lack of effective means to explain their results, especially when they are erroneous. In our previous work, we proposed a white-box approach (HUDD) and a black-box approach (SAFE) to automatically characterize DNN failures. They both identify clusters of similar images from a potentially large set of images leading to DNN failures. However, the analysis pipelines for HUDD and SAFE were instantiated in specific ways according to common practices, deferring the analysis of other pipelines to future work. In this article, we report on an empirical evaluation of 99 different pipelines for root cause analysis of DNN failures. They combine transfer learning, autoencoders, heatmaps of neuron relevance, dimensionality reduction techniques, and different clustering algorithms. Our results show that the best pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters almost exclusively capturing images of the same failure scenario, thus facilitating root cause analysis. Further, it generates distinct clusters for each root cause of failure, thus enabling engineers to detect all the unsafe scenarios. Interestingly, these results hold even for failure scenarios that are only observed in a small percentage of the failing images.  © 2024 Copyright held by the owner/author(s).",clustering; DNN debugging; DNN explanation; DNN functional safety analysis; transfer learning,Clustering algorithms; Image analysis; Pipelines; Safety engineering; Clusterings; Deep neural network debugging; Deep neural network explanation; Deep neural network functional safety analyse; Functional Safety; Network debugging; Network failure; Root cause analysis; Safety analysis; Transfer learning; Deep neural networks
Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191647655&doi=10.1145%2f3643674&partnerID=40&md5=cb13494b6b545d16b1d6c2f5e8ddc785,"Since its introduction in November 2022, ChatGPT has rapidly gained popularity due to its remarkable ability in language understanding and human-like responses. ChatGPT, based on GPT-3.5 architecture, has shown great promise for revolutionizing various research fields, including code generation. However, the reliability and quality of code generated by ChatGPT remain unexplored, raising concerns about potential risks associated with the widespread use of ChatGPT-driven code generation.In this article, we systematically study the quality of 4,066 ChatGPT-generated programs of code implemented in two popular programming languages, i.e., Java and Python, for 2,033 programming tasks. The goal of this work is threefold. First, we analyze the correctness of ChatGPT on code generation tasks and uncover the factors that influence its effectiveness, including task difficulty, programming language, time that tasks are introduced, and program size. Second, we identify and characterize potential issues with the quality of ChatGPT-generated code. Last, we provide insights into how these issues can be mitigated. Experiments highlight that out of 4,066 programs generated by ChatGPT, 2,756 programs are deemed correct, 1,082 programs provide wrong outputs, and 177 programs contain compilation or runtime errors. Additionally, we further analyze other characteristics of the generated code through static analysis tools, such as code style and maintainability, and find that 1,930 ChatGPT-generated code snippets suffer from maintainability issues. Subsequently, we investigate ChatGPT's self-repairing ability and its interaction with static analysis tools to fix the errors uncovered in the previous step. Experiments suggest that ChatGPT can partially address these challenges, improving code quality by more than 20%, but there are still limitations and opportunities for improvement. Overall, our study provides valuable insights into the current limitations of ChatGPT and offers a roadmap for future research and development efforts to enhance the code generation capabilities of artificial intelligence models such as ChatGPT. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated code generation; ChatGPT; code analysis,Computer software; Maintainability; Python; Quality control; Analysis tools; Automated code generation; ChatGPT; Code analysis; Code quality; Codegeneration; Human like; Language understanding; Quality issues; Research fields; Static analysis
Mapping APIs in Dynamic-typed Programs by Leveraging Transfer Learning,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191557616&doi=10.1145%2f3641848&partnerID=40&md5=26651dd639b2f53f4fbe2cc6a1a73478,"Application Programming Interface (API) migration is a common task for adapting software across different programming languages and platforms, where manually constructing the mapping relations between APIs is indeed time-consuming and error-prone. To facilitate this process, many automated API mapping approaches have been proposed. However, existing approaches were mainly designed and evaluated for mapping APIs of statically-typed languages, while their performance on dynamically-typed languages remains unexplored. In this article, we conduct the first extensive study to explore existing API mapping approaches’ performance for mapping APIs in dynamically-typed languages, for which we have manually constructed a high-quality dataset. According to the empirical results, we have summarized several insights. In particular, the source code implementations of APIs can significantly improve the effectiveness of API mapping. However, due to the confidentiality policy, they may not be available in practice. To overcome this, we propose a novel API mapping approach, named Matl, which leverages the transfer learning technique to learn the semantic embeddings of source code implementations from large-scale open-source repositories and then transfers the learned model to facilitate the mapping of APIs. In this way, Matl can produce more accurate API embedding of its functionality for more effective mapping without knowing the source code of the APIs. To evaluate the performance of Matl, we have conducted an extensive study by comparing Matl with state-of-the-art approaches. The results demonstrate that Matl is indeed effective as it improves the state-of-the-art approach by at least 18.36% for mapping APIs of dynamically-typed language and by 30.77% for mapping APIs of the statically-typed language. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",API mapping; program transformation; transfer learning,Application programming interfaces (API); Application programs; Computer programming languages; Embeddings; Open source software; Open systems; Semantics; Application programming interface mapping; Applications programming interfaces; Dynamically typed languages; Interface mapping; Interface migration; Performance; Program transformations; Source code implementation; State-of-the-art approach; Transfer learning; Mapping
A Smart Status Based Monitoring Algorithm for the Dynamic Analysis of Memory Safety,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191610492&doi=10.1145%2f3637227&partnerID=40&md5=6945fb8bc9a4ca4524b62b4c3a25cc01,"C is a dominant programming language for implementing system and low-level embedded software. Unfortunately, the unsafe nature of its low-level control of memory often leads to memory errors. Dynamic analysis has been widely used to detect memory errors at runtime. However, existing monitoring algorithms for dynamic analysis are not yet satisfactory, as they cannot deterministically and completely detect some types of errors, such as segment confusion errors, sub-object overflows, use-after-frees and memory leaks. We propose a new monitoring algorithm, namely Smatus, short for smart status, that improves memory safety by performing comprehensive dynamic analysis. The key innovation is to maintain at runtime a small status node for each memory object. A status node records the status value and reference count of an object, where the status value denotes the liveness and segment type of this object, and the reference count tracks the number of pointer variables pointing to this object. Smatus maintains at runtime a pointer metadata for each pointer variable, to record not only the base and bound of a pointer’s referent but also the address of the referent’s status node. All the pointers pointing to the same referent share the same status node in their pointer metadata. A status node is smart in the sense that it is automatically deleted when it becomes useless (indicated by its reference count reaching zero). To the best of our knowledge, Smatus represents the most comprehensive approach of its kind. We have evaluated Smatus by using a large set of programs including the NIST Software Assurance Reference Dataset, MSBench, MiBench, SPEC and stress testing benchmarks. In terms of effectiveness (detecting different types of memory errors), Smatus outperforms state-of-the-art tools, Google’s AddressSanitizer, SoftBoundCETS and Valgrind, as it is capable of detecting more errors. In terms of performance (the time and memory overheads), Smatus outperforms SoftBoundCETS and Valgrind in terms of both lower time and memory overheads incurred, and is on par with AddressSanitizer in terms of the time and memory overhead tradeoff made (with much lower memory overheads incurred). © 2024 Copyright held by the owner/author(s)",bug finding; dynamic analysis; memory errors; monitoring algorithm; Software quality; software testing,C (programming language); Computer software selection and evaluation; Large datasets; Metadata; Quality control; Safety engineering; Safety testing; Software testing; Statistical tests; Bug finding; Dynamics analysis; Memory error; Memory leaks; Memory overheads; Memory safety; Monitoring algorithms; Runtimes; Software Quality; Software testings; Errors
Using Voice and Biofeedback to Predict User Engagement during Product Feedback Interviews,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191605504&doi=10.1145%2f3635712&partnerID=40&md5=fbe996e9e885fd4e078c44e73ade7627,"Capturing users’ engagement is crucial for gathering feedback about the features of a software product. In a market-driven context, current approaches to collecting and analyzing users’ feedback are based on techniques leveraging information extracted from product reviews and social media. These approaches are hardly applicable in contexts where online feedback is limited, as for the majority of apps, and software in general. In such cases, companies need to resort to face-to-face interviews to get feedback on their products. In this article, we propose to utilize biometric data, in terms of physiological and voice features, to complement product feedback interviews with information about the engagement of the user on product-relevant topics. We evaluate our approach by interviewing users while gathering their physiological data (i.e., biofeedback) using an Empatica E4 wristband, and capturing their voice through the default audio-recorder of a common laptop. Our results show that we can predict users’ engagement by training supervised machine learning algorithms on biofeedback and voice data, and that voice features alone can be sufficiently effective. The best configurations evaluated achieve an average F1 ∼ 70% in terms of classification performance, and use voice features only. This work is one of the first studies in requirements engineering in which biometrics are used to identify emotions. Furthermore, this is one of the first studies in software engineering that considers voice analysis. The usage of voice features can be particularly helpful for emotion-aware feedback collection in remote communication, either performed by human analysts or voice-based chatbots, and can also be exploited to support the analysis of meetings in software engineering research. © 2024 Copyright held by the owner/author(s).",,Biofeedback; Biometrics; Learning algorithms; Physiology; Software engineering; 'current; Face-to-face interview; In contexts; Market driven; Online feedback; Product reviews; Social media; Software products; User engagement; User feedback; Supervised learning
Estimating Uncertainty in Labeled Changes by SZZ Tools on Just-In-Time Defect Prediction,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191541271&doi=10.1145%2f3637226&partnerID=40&md5=84f9224fb6ced500edcb7456e9ef7dc9,"The aim of Just-In-Time (JIT) defect prediction is to predict software changes that are prone to defects in a project in a timely manner, thereby improving the efficiency of software development and ensuring software quality. Identifying changes that introduce bugs is a critical task in just-in-time defect prediction, and researchers have introduced the SZZ approach and its variants to label these changes. However, it has been shown that different SZZ algorithms introduce noise to the dataset to a certain extent, which may reduce the predictive performance of the model. To address this limitation, we propose the Confident Learning Imbalance (CLI) model. The model identifies and excludes samples whose labels may be corrupted by estimating the joint distribution of noisy labels and true labels, and mitigates the impact of noisy data on the performance of the prediction model. The CLI consists of two components: identifying noisy data (Confident Learning Component) and generating a predicted probability matrix for imbalanced data (Imbalanced Data Probabilistic Prediction Component). The IDPP component generates precise predicted probabilities for each instance in the training set, while the CL component uses the generated predicted probability matrix and noise labels to clean up the noise and build a classification model. We evaluate the performance of our model through extensive experiments on a total of 126,526 changes from ten Apache open source projects, and the results show that our model outperforms the baseline methods. © 2024 Copyright held by the owner/author(s).",confident learning; imbalance; Just-in-time defect prediction; SZZ tools,Classification (of information); Computer software selection and evaluation; Defects; Just in time production; Matrix algebra; Open source software; Software design; Uncertainty analysis; Confident learning; Defect prediction; Imbalance; Imbalanced data; Just-in-time; Just-in-time defect prediction; Noisy data; Performance; Probability matrixes; SZZ tool; Forecasting
Measuring and Clustering Heterogeneous Chatbot Designs,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191582142&doi=10.1145%2f3637228&partnerID=40&md5=5e2dffababd5bfebc32ef6ad65400f5d,"Conversational agents, or chatbots, have become popular to access all kind of software services. They provide an intuitive natural language interface for interaction, available from a wide range of channels including social networks, web pages, intelligent speakers or cars. In response to this demand, many chatbot development platforms and tools have emerged. However, they typically lack support to statically measure properties of the chatbots being built, as indicators of their size, complexity, quality or usability. Similarly, there are hardly any mechanisms to compare and cluster chatbots developed with heterogeneous technologies. To overcome this limitation, we propose a suite of 21 metrics for chatbot designs, as well as two clustering methods that help in grouping chatbots along their conversation topics and design features. Both the metrics and the clustering methods are defined on a neutral chatbot design language, becoming independent of the implementation platform. We provide automatic translations of chatbots defined on some major platforms into this neutral notation to perform the measurement and clustering. The approach is supported by our tool Asymob, which we have used to evaluate the metrics and the clustering methods over a set of 259 Dialogflow and Rasa chatbots from open-source repositories. The results open the door to incorporating the metrics within chatbot development processes for the early detection of quality issues, and to exploit clustering to organise large collections of chatbots into significant groups to ease chatbot comprehension, search and comparison. © 2024 Association for Computing Machinery. All rights reserved.",Chatbot design; clustering; metrics; model-driven engineering; quality assurance,Natural language processing systems; Open source software; Open systems; Quality assurance; Software agents; Websites; Chatbot design; Chatbots; Clustering methods; Clusterings; Conversational agents; Metric; Model-driven Engineering; Natural language interfaces; Software services; Web-page; Cluster analysis
"Characterizing Deep Learning Package Supply Chains in PyPI: Domains, Clusters, and Disengagement",2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191605197&doi=10.1145%2f3640336&partnerID=40&md5=e15b666b92637fb5b9b4a051ea4bfc17,"Deep learning (DL) frameworks have become the cornerstone of the rapidly developing DL field. Through installation dependencies specified in the distribution metadata, numerous packages directly or transitively depend on DL frameworks, layer after layer, forming DL package supply chains (SCs), which are critical for DL frameworks to remain competitive. However, vital knowledge on how to nurture and sustain DL package SCs is still lacking. Achieving this knowledge may help DL frameworks formulate effective measures to strengthen their SCs to remain competitive and shed light on dependency issues and practices in the DL SC for researchers and practitioners. In this paper, we explore the domains, clusters, and disengagement of packages in two representative PyPI DL package SCs to bridge this knowledge gap. We analyze the metadata of nearly six million PyPI package distributions and construct version-sensitive SCs for two popular DL frameworks: TensorFlow and PyTorch. We find that popular packages (measured by the number of monthly downloads) in the two SCs cover 34 domains belonging to eight categories. Applications, Infrastructure, and Sciences categories account for over 85% of popular packages in either SC and TensorFlow and PyTorch SC have developed specializations on Infrastructure and Applications packages, respectively. We employ the Leiden community detection algorithm and detect 131 and 100 clusters in the two SCs. The clusters mainly exhibit four shapes: Arrow, Star, Tree, and Forest with increasing dependency complexity. Most clusters are Arrow or Star, while Tree and Forest clusters account for most packages (Tensorflow SC: 70.7%, PyTorch SC: 92.9%). We identify three groups of reasons why packages disengage from the SC (i.e., remove the DL framework and its dependents from their installation dependencies): dependency issues, functional improvements, and ease of installation. The most common reason in TensorFlow SC is dependency incompatibility and in PyTorch SC is to simplify functionalities and reduce installation size. Our study provides rich implications for DL framework vendors, researchers, and practitioners on the maintenance and dependency management practices of PyPI DL SCs. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; evolution; PyPI ecosystem; software structure; Software supply chain,Deep learning; Installation; Metadata; Stars; Deep learning; Effective measures; Evolution; Knowledge gaps; Learning fields; Learning frameworks; Learning packages; PyPI ecosystem; Software structures; Software supply chains; Supply chains
Rigorous Assessment of Model Inference Accuracy using Language Cardinality,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191560617&doi=10.1145%2f3640332&partnerID=40&md5=7dadaeba3f121e5beddc9faa9f68545d,"Models such as finite state automata are widely used to abstract the behavior of software systems by capturing the sequences of events observable during their execution. Nevertheless, models rarely exist in practice and, when they do, get easily outdated; moreover, manually building and maintaining models is costly and error-prone. As a result, a variety of model inference methods that automatically construct models from execution traces have been proposed to address these issues. However, performing a systematic and reliable accuracy assessment of inferred models remains an open problem. Even when a reference model is given, most existing model accuracy assessment methods may return misleading and biased results. This is mainly due to their reliance on statistical estimators over a finite number of randomly generated traces, introducing avoidable uncertainty about the estimation and being sensitive to the parameters of the random trace generative process. This article addresses this problem by developing a systematic approach based on analytic combinatorics that minimizes bias and uncertainty in model accuracy assessment by replacing statistical estimation with deterministic accuracy measures. We experimentally demonstrate the consistency and applicability of our approach by assessing the accuracy of models inferred by state-of-the-art inference tools against reference models from established specification mining benchmarks. © 2024 Copyright held by the owner/author(s).",behavioral comparison; conformance checking; formal specifications; machine learning; model assessment; Model inference; precision; process mining; recall; software engineering; specification mining,Abstracting; Machine learning; Accuracy assessment; Behavioral comparison; Conformance checking; Machine-learning; Model assessment; Model inference; Precision; Process mining; Recall; Specification mining; Formal specification
Assessing Effectiveness of Test Suites: What Do We Know and What Should We Do?,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191996836&doi=10.1145%2f3635713&partnerID=40&md5=22178ea4be1e9d87175d08a50b04052c,"Background. Software testing is a critical activity for ensuring the quality and reliability of software systems. To evaluate the effectiveness of different test suites, researchers have developed a variety of metrics. Problem. However, comparing these metrics is challenging due to the lack of a standardized evaluation framework including comprehensive factors. As a result, researchers often focus on single factors (e.g., size), which finally leads to different or even contradictory conclusions. After comparing dozens of pieces of work in detail, we have found two main problems most troubling to our community: (1) researchers tend to oversimplify the description of the ground truth they use, and (2) data involving real defects is not suitable for analysis using traditional statistical indicators. Objective. We aim at scrutinizing the whole process of comparing test suites for our community. Method. To hit this aim, we propose a framework ASSENT (evAluating teSt Suite EffectiveNess meTrics) to guide the follow-up research for evaluating a test suite effectiveness metric. ASSENT consists of three fundamental components: ground truth, benchmark test suites, and agreement indicator. Its functioning is as follows: first, users clarify the ground truth for determining the real order in effectiveness among test suites. Second, users generate a set of benchmark test suites and derive their ground truth order in effectiveness. Third, users use the metric to derive the order in effectiveness for the same test suites. Finally, users calculate the agreement indicator between the two orders derived by two metrics. Result. With ASSENT, we are able to compare the accuracy of different test suite effectiveness metrics. We apply ASSENT to evaluate representative test suite effectiveness metrics, including mutation score and code coverage metrics. Our results show that, based on the real faults, mutation score, and subsuming mutation score are the best metrics to quantify test suite effectiveness. Meanwhile, by using mutants instead of real faults, test effectiveness will be overestimated by more than 20% in values. Conclusion. We recommend that the standardized evaluation framework ASSENT should be used for evaluating and comparing test effectiveness metrics in the future work. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",coverage testing; mutation testing; order preservation; statistical indicators; Test suite effectiveness,Benchmarking; Software reliability; Benchmark tests; Coverage testing; Effectiveness metrics; Evaluation framework; Ground truth; Mutation score; Mutation testing; Order preservation; Statistical indicators; Test suite effectiveness; Software testing
Battling against Protocol Fuzzing: Protecting Networked Embedded Devices from Dynamic Fuzzers,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191561702&doi=10.1145%2f3641847&partnerID=40&md5=fce6014c9c6cf8725ac4caa502d376e1,"Networked Embedded Devices (NEDs) are increasingly targeted by cyberattacks, mainly due to their widespread use in our daily lives. Vulnerabilities in NEDs are the root causes of these cyberattacks. Although deployed NEDs go through thorough code audits, there can still be considerable exploitable vulnerabilities. Existing mitigation measures like code encryption and obfuscation adopted by vendors can resist static analysis on deployed NEDs, but are ineffective against protocol fuzzing. Attackers can easily apply protocol fuzzing to discover vulnerabilities and compromise deployed NEDs. Unfortunately, prior anti-fuzzing techniques are impractical as they significantly slow down NEDs, hampering NED availability. To address this issue, we propose Armor—the first anti-fuzzing technique specifically designed for NEDs. First, we design three adversarial primitives–delay, fake coverage, and forged exception–to break the fundamental mechanisms on which fuzzing relies to effectively find vulnerabilities. Second, based on our observation that inputs from normal users consistent with the protocol specification and certain program paths are rarely executed with normal inputs, we design static and dynamic strategies to decide whether to activate the adversarial primitives. Extensive evaluations show that Armor incurs negligible time overhead and effectively reduces the code coverage (e.g., line coverage by 22%-61%) for fuzzing, significantly outperforming the state of the art. © 2024 Copyright held by the owner/author(s).",anti-fuzzing; Internet of Things; protocol fuzzing,Armor; Codes (symbols); Cryptography; Static analysis; Anti-fuzzing; Code Audit; Code encryption; Code obfuscation; Cyber-attacks; Daily lives; Mitigation measures; Networked embedded devices; Protocol fuzzing; Root cause; Internet of things
Test Generation Strategies for Building Failure Models and Explaining Spurious Failures,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191584064&doi=10.1145%2f3638246&partnerID=40&md5=4393e5750d0d6ac59dcbc6e5ec330d23,"Test inputs fail not only when the system under test is faulty but also when the inputs are invalid or unrealistic. Failures resulting from invalid or unrealistic test inputs are spurious. Avoiding spurious failures improves the effectiveness of testing in exercising the main functions of a system, particularly for compute-intensive (CI) systems where a single test execution takes significant time. In this article, we propose to build failure models for inferring interpretable rules on test inputs that cause spurious failures. We examine two alternative strategies for building failure models: (1) machine learning (ML)-guided test generation and (2) surrogate-assisted test generation. ML-guided test generation infers boundary regions that separate passing and failing test inputs and samples test inputs from those regions. Surrogate-assisted test generation relies on surrogate models to predict labels for test inputs instead of exercising all the inputs. We propose a novel surrogate-assisted algorithm that uses multiple surrogate models simultaneously, and dynamically selects the prediction from the most accurate model. We empirically evaluate the accuracy of failure models inferred based on surrogate-assisted and ML-guided test generation algorithms. Using case studies from the domains of cyber-physical systems and networks, we show that our proposed surrogate-assisted approach generates failure models with an average accuracy of 83%, significantly outperforming ML-guided test generation and two baselines. Further, our approach learns failure-inducing rules that identify genuine spurious failures as validated against domain knowledge. © 2024 Association for Computing Machinery. All rights reserved.",failure models; machine learning; Search-based testing; spurious failures; surrogate models; test-input validity,Domain Knowledge; Embedded systems; Building failure; Failure modelling; Machine-learning; Search-based testing; Spurious failure; Surrogate modeling; Systems under tests; Test generations; Test inputs; Test-input validity; Machine learning
Smart Contract Code Repair Recommendation based on Reinforcement Learning and Multi-metric Optimization,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191541012&doi=10.1145%2f3637229&partnerID=40&md5=01f8aba450b8308f61e9cafdfb26976d,"A smart contract is a kind of code deployed on the blockchain that executes automatically once an event triggers a clause in the contract. Since smart contracts involve businesses such as asset transfer, they are more vulnerable to attacks, so it is crucial to ensure the security of smart contracts. Because a smart contract cannot be tampered with once deployed on the blockchain, for smart contract developers, it is necessary to fix vulnerabilities before deployment. Compared with many vulnerability detection tools for smart contracts, the amount of automatic fix approaches for smart contracts is relatively limited. These approaches mainly use defined pattern-based methods or heuristic search algorithms for vulnerability repairs. In this article, we propose RLRep, a reinforcement learning-based approach to provide smart contract repair recommendations for smart contract developers automatically. This approach adopts an agent to provide repair action suggestions based on the vulnerable smart contract without any supervision, which can solve the problem of missing labeled data in machine learning-based repair methods. We evaluate our approach on a dataset containing 853 smart contract programs (programming language: Solidity) with different kinds of vulnerabilities. We split them into training and test sets. The result shows that our approach can provide 54.97% correct repair recommendations for smart contracts. © 2024 Copyright held by the owner/author(s).",Repair recommendation; smart contract,Blockchain; Codes (symbols); Heuristic algorithms; Heuristic methods; Reinforcement learning; Repair; Block-chain; Detection tools; Event trigger; Learning metrics; Multi-metrics; Optimisations; Pattern based method; Reinforcement learnings; Repair recommendation; Vulnerability detection; Smart contract
Understanding Real-Time Collaborative Programming: A Study of Visual Studio Live Share,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191538881&doi=10.1145%2f3643672&partnerID=40&md5=342888f418b9a5049498063890f5d48f,"Real-time collaborative programming (RCP) entails developers working simultaneously, regardless of their geographic locations. RCP differs from traditional asynchronous online programming methods, such as Git or SVN, where developers work independently and update the codebase at separate times. Although various real-time code collaboration tools (e.g., Visual Studio Live Share, Code with Me, and Replit) have kept emerging in recent years, none of the existing studies explicitly focus on a deep understanding of the processes or experiences associated with RCP. To this end, we combine interviews and an e-mail survey with the users of Visual Studio Live Share, aiming to understand (i) the scenarios, (ii) the requirements, and (iii) the challenges when developers participate in RCP. We find that developers participate in RCP in 18 different scenarios belonging to six categories, e.g., pair programming, group debugging, and code review. However, existing users’ attitudes toward the usefulness of the current RCP tools in these scenarios were significantly more negative than the expectations of potential users. As for the requirements, the most critical category is live editing, followed by the need for sharing terminals to enable hosts and guests to run commands and see the results, as well as focusing and following, which involves “following” the host’s edit location and “focusing” the guests’ attention on the host with a notification. Under these categories, we identify 17 requirements, but most of them are not well supported by current tools. In terms of challenges, we identify 19 challenges belonging to seven categories. The most severe category of challenges is lagging followed by permissions and conflicts. The above findings indicate that the current RCP tools and even collaborative environment need to be improved greatly and urgently. Based on these findings, we discuss the recommendations for different stakeholders, including practitioners, tool designers, and researchers. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",online collaboration; Real-time collaboration; software development; synchronous collaboration,Studios; 'current; Collaborative programming; Geographic location; On-line collaborations; On-line programming; Programming tools; Real-time collaboration; Real-time collaborative; Synchronous collaboration; Visual studios; Software design
PACE: A Program Analysis Framework for Continuous Performance Prediction,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191538538&doi=10.1145%2f3637230&partnerID=40&md5=93ac055b6062fb3ebc1803b56c193d7f,"Software development teams establish elaborate continuous integration pipelines containing automated test cases to accelerate the development process of software. Automated tests help to verify the correctness of code modifications decreasing the response time to changing requirements. However, when the software teams do not track the performance impact of pending modifications, they may need to spend considerable time refactoring existing code. This article presents PACE, a program analysis framework that provides continuous feedback on the performance impact of pending code updates. We design performance microbenchmarks by mapping the execution time of functional test cases given a code update. We map microbenchmarks to code stylometry features and feed them to predictors for performance predictions. Our experiments achieved significant performance in predicting code performance, outperforming current state-of-the-art by 75% on neural-represented code stylometry features. © 2024 Copyright held by the owner/author(s).",Code Stylometry Features; Current Code State; Microbenchmarking,Forecasting; Software testing; Analysis frameworks; Automated test; Code stylometry feature; Current code state; Current codes; Micro-benchmarking; Performance prediction; Program analysis; Stylometry; Test case; Software design
ARCTURUS: Full Coverage Binary Similarity Analysis with Reachability-guided Emulation,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191546124&doi=10.1145%2f3640337&partnerID=40&md5=44b0a12dd0141fe83363aea8f55b59a1,"Binary code similarity analysis is extremely useful, since it provides rich information about an unknown binary, such as revealing its functionality and identifying reused libraries. Robust binary similarity analysis is challenging, as heavy compiler optimizations can make semantically similar binaries have gigantic syntactic differences. Unfortunately, existing semantic-based methods still suffer from either incomplete coverage or low accuracy. In this article, we propose ARCTURUS, a new technique that can achieve high code coverage and high accuracy simultaneously by manipulating program execution under the guidance of code reachability. Our key insight is that the compiler must preserve program semantics (e.g., dependences between code fragments) during compilation; therefore, the code reachability, which implies the interdependence between code, is invariant across code transformations. Based on the above insight, our key idea is to leverage the stability of code reachability to manipulate the program execution such that deep code logic can also be covered in a consistent way. Experimental results show that ARCTURUS achieves an average precision of 87.8% with 100% block coverage, outperforming compared methods by 38.4%, on average. ARCTURUS takes only 0.15 second to process one function, on average, indicating that it is efficient for practical use. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Binary code similarity; emulation; program analysis; reverse engineering,Cosine transforms; Program compilers; Reverse engineering; Semantics; Binary code similarity; Code coverage; Code similarities; Compiler optimizations; Emulation; High-accuracy; Program analysis; Program execution; Reachability; Similarity analysis; Binary codes
Method-level Bug Prediction: Problems and Promises,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191562298&doi=10.1145%2f3640331&partnerID=40&md5=47dec488e7026939f00a38e097469919,"Fixing software bugs can be colossally expensive, especially if they are discovered in the later phases of the software development life cycle. As such, bug prediction has been a classic problem for the research community. As of now, the Google Scholar site generates ∼113,000 hits if searched with the “bug prediction” phrase. Despite this staggering effort by the research community, bug prediction research is criticized for not being decisively adopted in practice. A significant problem of the existing research is the granularity level (i.e., class/file level) at which bug prediction is historically studied. Practitioners find it difficult and time-consuming to locate bugs at the class/file level granularity. Consequently, method-level bug prediction has become popular in the past decade. We ask, are these method-level bug prediction models ready for industry use? Unfortunately, the answer is no. The reported high accuracies of these models dwindle significantly if we evaluate them in different realistic time-sensitive contexts. It may seem hopeless at first, but, encouragingly, we show that future method-level bug prediction can be improved significantly. In general, we show how to reliably evaluate future method-level bug prediction models and how to improve them by focusing on four different improvement avenues: building noise-free bug data, addressing concept drift, selecting similar training projects, and developing a mixture of models. Our findings are based on three publicly available method-level bug datasets and a newly built bug dataset of 774, 051 Java methods originating from 49 open-source software projects. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code complexity; code metrics; maintenance; McCabe; Method-level bug prediction,Codes (symbols); Forecasting; Open source software; Open systems; Program debugging; Software design; Bug predictions; Class file; Code complexity; Code metrics; File levels; Future methods; Mccabe; Method-level bug predictions; Prediction modelling; Research communities; Life cycle
Compiler Autotuning through Multiple-phase Learning,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192221762&doi=10.1145%2f3640330&partnerID=40&md5=189fa9931558aafa9f2fdaa58b32c557,"Widely used compilers like GCC and LLVM usually have hundreds of optimizations controlled by optimization flags, which are enabled or disabled during compilation to improve the runtime performance (e.g., small execution time) of the compiler program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to manually tune compiler optimization flags. In the literature, a number of autotuning techniques have been proposed, which tune optimization flags for a compiled program by comparing its actual runtime performance with different optimization flag combinations. Due to the huge search space and heavy actual runtime cost, these techniques suffer from the widely recognized efficiency problem. To reduce the heavy runtime cost, in this article we propose a lightweight learning approach that uses a small number of actual runtime performance data to predict the runtime performance of a compiled program with various optimization flag combinations. Furthermore, to reduce the search space, we design a novel particle swarm algorithm that tunes compiler optimization flags with the prediction model. To evaluate the performance of the proposed approach, CompTuner, we conduct an extensive experimental study on two popular C compilers, GCC and LLVM, with two widely used benchmarks, cBench and PolyBench. The experimental results show that CompTuner significantly outperforms the six compared techniques, including the state-of-the-art technique BOCA. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Compiler; compiler autotuning; multiple phase learning; particle swarm optimization,Benchmarking; C (programming language); Particle swarm optimization (PSO); Swarm intelligence; Autotuning; Compiler; Compiler autotuning; Compiler optimizations; Multiple phase learning; Optimisations; Particle swarm; Particle swarm optimization; Runtime performance; Swarm optimization; Program compilers
Test Optimization in DNN Testing: A Survey,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191802395&doi=10.1145%2f3643678&partnerID=40&md5=64bf5a6eb1e24337a7fb0fc901598d20,"This article presents a comprehensive survey on test optimization in deep neural network (DNN) testing. Here, test optimization refers to testing with low data labeling effort. We analyzed 90 papers, including 43 from the software engineering (SE) community, 32 from the machine learning (ML) community, and 15 from other communities. Our study: (i) unifies the problems as well as terminologies associated with low-labeling cost testing, (ii) compares the distinct focal points of SE and ML communities, and (iii) reveals the pitfalls in existing literature. Furthermore, we highlight the research opportunities in this domain. © 2024 Copyright held by the owner/author(s).",DNN testing; low-labeling cost; Test optimization,Costs; Well testing; Data labelling; Deep neural network testing; Engineering community; Engineering learning communities; Focal points; Labelings; Low-labeling cost; Machine learning communities; Research opportunities; Test optimization; Deep neural networks
Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep Learning Projects,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191553460&doi=10.1145%2f3638245&partnerID=40&md5=58423756d0f7efd8b152726c6b054c6e,"Deep Learning (DL) models have rapidly advanced, focusing on achieving high performance through testing model accuracy and robustness. However, it is unclear whether DL projects, as software systems, are tested thoroughly or functionally correct when there is a need to treat and test them like other software systems. Therefore, we empirically study the unit tests in open-source DL projects, analyzing 9,129 projects from GitHub. We find that: (1) unit tested DL projects have positive correlation with the open-source project metrics and have a higher acceptance rate of pull requests; (2) 68% of the sampled DL projects are not unit tested at all; (3) the layer and utilities (utils) of DL models have the most unit tests. Based on these findings and previous research outcomes, we built a mapping taxonomy between unit tests and faults in DL projects. We discuss the implications of our findings for developers and researchers and highlight the need for unit testing in open-source DL projects to ensure their reliability and stability. The study contributes to this community by raising awareness of the importance of unit testing in DL projects and encouraging further research in this area. © 2024 Copyright held by the owner/author(s).",Deep learning; unit testing,Acceptance tests; Open source software; Software testing; Deep learning; Empirical studies; Learning models; Learning projects; Open-source; Performance; Software-systems; Testing models; Unit testing; Unit tests; Deep learning
Exploring Semantic Redundancy using Backdoor Triggers: A Complementary Insight into the Challenges Facing DNN-based Software Vulnerability Detection,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191556290&doi=10.1145%2f3640333&partnerID=40&md5=716d57a89d8840b4df93fc484e06f992,"To detect software vulnerabilities with better performance, deep neural networks (DNNs) have received extensive attention recently. However, these vulnerability detection DNN models trained with code representations are vulnerable to specific perturbations on code representations. This motivates us to rethink the bane of software vulnerability detection and find function-agnostic features during code representation which we name as semantic redundant features. This paper first identifies a tight correlation between function-agnostic triggers and semantic redundant feature space (where the redundant features reside) in these DNN models. For correlation identification, we propose a novel Backdoor-based Semantic Redundancy Exploration (BSemRE) framework. In BSemRE, the sensitivity of the trained models to function-agnostic triggers is observed to verify the existence of semantic redundancy in various code representations. Specifically, acting as the typical manifestations of semantic redundancy, naming conventions, ternary operators and identically-true conditions are exploited to generate function-agnostic triggers. Extensive comparative experiments on 1,613,823 samples of eight representative vulnerability datasets and state-of-the-art code representation techniques and vulnerability detection models demonstrate that the existence of semantic redundancy determines the upper trustworthiness limit of DNN-based software vulnerability detection. To the best of our knowledge, this is the first work exploring the bane of software vulnerability detection using backdoor triggers. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",backdoor triggers; deep neural networks; function-agnostic; semantic redundancy; Software vulnerability detection,Deep neural networks; Semantic Web; Semantics; Backdoor trigger; Backdoors; Code representation; Function-agnostic; Network-based software; Redundant features; Semantic redundancy; Software vulnerabilities; Software vulnerability detection; Vulnerability detection; Redundancy
RE Methods for Virtual Reality Software Product Development: A Mapping Study,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192242917&doi=10.1145%2f3649595&partnerID=40&md5=77c44e88018837b6a290f8f5da7fa8af,"Software practitioners use various methods in Requirements Engineering (RE) to elicit, analyze, and specify the requirements of enterprise products. The methods impact the final product characteristics and influence product delivery. Ad-hoc usage of the methods by software practitioners can lead to inconsistency and ambiguity in the product. With the notable rise in enterprise products, games, and so forth across various domains, Virtual Reality (VR) has become an essential technology for the future. The methods adopted for RE for developing VR products requires a detailed study. This article presents a mapping study on RE methods prescribed and used for developing VR applications including requirements elicitation, requirements analysis, and requirements specification. Our study provides insights into the use of such methods in the VR community and suggests using specific RE methods in various fields of interest. We also discuss future directions in RE for VR products. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",industrial practices; requirements elicitation; Software requirements; virtual reality,Mapping; Virtual reality; Engineering methods; Industrial practices; Mapping studies; Product characteristics; Product delivery; Requirement engineering; Requirements elicitation; Software practitioners; Software product development; Software requirements; Requirements engineering
Mitigating Debugger-based Attacks to Java Applications with Self-debugging,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191581426&doi=10.1145%2f3631971&partnerID=40&md5=9bebdab94d591ab3a1898af9309cd956,"Java bytecode is a quite high-level language and, as such, it is fairly easy to analyze and decompile with malicious intents, e.g., to tamper with code and skip license checks. Code obfuscation was a first attempt to mitigate malicious reverse-engineering based on static analysis. However, obfuscated code can still be dynamically analyzed with standard debuggers to perform step-wise execution and to inspect (or change) memory content at important execution points, e.g., to alter the verdict of license validity checks. Although some approaches have been proposed to mitigate debugger-based attacks, they are only applicable to binary compiled code and none address the challenge of protecting Java bytecode. In this article, we propose a novel approach to protect Java bytecode from malicious debugging. Our approach is based on automated program transformation to manipulate Java bytecode and split it into two binary processes that debug each other (i.e., a self-debugging solution). In fact, when the debugging interface is already engaged, an additional malicious debugger cannot attach. To be resilient against typical attacks, our approach adopts a series of technical solutions, e.g., an encoded channel is shared by the two processes to avoid leaking information, an authentication protocol is established to avoid Man-in-the-middle attacks, and the computation is spread between the two processes to prevent the attacker to replace or terminate either of them. We test our solution on 18 real-world Java applications, showing that our approach can effectively block the most common debugging tasks (either with the Java debugger or the GNU debugger) while preserving the functional correctness of the protected programs. While the final decision on when to activate this protection is still up to the developers, the observed performance overhead was acceptable for common desktop application domains. © 2024 Copyright held by the owner/author(s).",Anti-debugging; maliciuos reverse engineering; man at the end attacks; tampering attacks,Application programs; Data obfuscation; High level languages; Java programming language; Network security; Open source software; Program debugging; Reverse engineering; Anti-debugging; Debuggers; High-level language; Higher-level languages; Java applications; Java byte codes; Maliciuos reverse engineering; Man at the end attack; Self-debugging; Tampering attacks; Static analysis
Deceiving Humans and Machines Alike: Search-based Test Input Generation for DNNs Using Variational Autoencoders,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191562947&doi=10.1145%2f3635706&partnerID=40&md5=ebe6e18275e45ae0e914962313a94d73,"Due to the rapid adoption of Deep Neural Networks (DNNs) into larger software systems, testing of DNN-based systems has received much attention recently. While many different test adequacy criteria have been suggested, we lack effective test input generation techniques. Inputs such as images of real-world objects and scenes are not only expensive to collect but also difficult to randomly sample. Consequently, current testing techniques for DNNs tend to apply small local perturbations to existing inputs to generate new inputs. We propose SINVAD (Search-based Input space Navigation using Variational AutoencoDers), a way to sample from, and navigate over, a space of realistic inputs that resembles the true distribution in the training data. Our input space is constructed using Variational Autoencoders (VAEs), and navigated through their latent vector space. Our analysis shows that the VAE-based input space is well-aligned with human perception of what constitutes realistic inputs. Further, we show that this space can be effectively searched to achieve various testing scenarios, such as boundary testing of two different DNNs or analyzing class labels that are difficult for the given DNN to distinguish. Guidelines on how to design VAE architectures are presented as well. Our results have the potential to open the field to meaningful exploration through the space of highly structured images. © 2024 Copyright held by the owner/author(s).",deep neural network; search-based software engineering; Test data generation,Learning systems; Navigation; Software testing; Vector spaces; Auto encoders; Input space; Large software systems; Network based systems; Search-based; Search-based software engineering; Software system testing; Test adequacy criteria; Test data generation; Test inputs; Deep neural networks
Editorial: ICSE and the Incredible Contradictions of Software Engineering,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191584662&doi=10.1145%2f3656301&partnerID=40&md5=6c812060b9be65072c05959edf1ba4d6,[No abstract available],,
Industry Practices for Challenging Autonomous Driving Systems with Critical Scenarios,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191562794&doi=10.1145%2f3640334&partnerID=40&md5=ec0ef9445d986d769f6730a7cf3b59aa,"Testing autonomous driving systems for safety and reliability is essential, yet complex. A primary challenge is identifying relevant test scenarios, especially the critical ones that may expose hazards or harm to autonomous vehicles and other road users. Although numerous approaches and tools for critical scenario identification are proposed, the industry practices for selection, implementation, and evaluation of approaches, are not well understood. Therefore, we aim at exploring practical aspects of how autonomous driving systems are tested, particularly the identification and use of critical scenarios. We interviewed 13 practitioners from 7 companies in autonomous driving in Sweden. We used thematic modeling to analyse and synthesize the interview data. As a result, we present 9 themes of practices and 4 themes of challenges related to critical scenarios. Our analysis indicates there is little joint effort in the industry, despite every approach has its own limitations, and tools and platforms are lacking. To that end, we recommend the industry and academia combine different approaches, collaborate among different stakeholders, and continuously learn the field. The contributions of our study are exploration and synthesis of industry practices and related challenges for critical scenario identification and testing, and potential increase of industry relevance for future studies. © 2024 Copyright held by the owner/author(s).",autonomous driving systems; challenges; Critical scenario identification; industry practices; interview; testing,Petroleum reservoir evaluation; Safety testing; Autonomous driving; Autonomous driving system; Autonomous Vehicles; Challenge; Critical scenario identification; Driving systems; Industry practices; Interview; Scenario identifications; Test scenario; Autonomous vehicles
Identifying and Explaining Safety-critical Scenarios for Autonomous Vehicles via Key Features,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191597187&doi=10.1145%2f3640335&partnerID=40&md5=b053bd8717bf63ab7dc0c72d7c59f872,"Ensuring the safety of autonomous vehicles (AVs) is of utmost importance, and testing them in simulated environments is a safer option than conducting in-field operational tests. However, generating an exhaustive test suite to identify critical test scenarios is computationally expensive, as the representation of each test is complex and contains various dynamic and static features, such as the AV under test, road participants (vehicles, pedestrians, and static obstacles), environmental factors (weather and light), and the road’s structural features (lanes, turns, road speed, etc.). In this article, we present a systematic technique that uses Instance Space Analysis (ISA) to identify the significant features of test scenarios that affect their ability to reveal the unsafe behaviour of AVs. ISA identifies the features that best differentiate safety-critical scenarios from normal driving and visualises the impact of these features on test scenario outcomes (safe/unsafe) in two dimensions. This visualisation helps to identify untested regions of the instance space and provides an indicator of the quality of the test suite in terms of the percentage of feature space covered by testing. To test the predictive ability of the identified features, we train five Machine Learning classifiers to classify test scenarios as safe or unsafe. The high precision, recall, and F1 scores indicate that our proposed approach is effective in predicting the outcome of a test scenario without executing it and can be used for test generation, selection, and prioritisation. © 2024 Copyright held by the owner/author(s).",feature-impact analysis; instance space analysis; search-based software testing; Testing autonomous vehicles,Autonomous vehicles; Roads and streets; Safety engineering; Software testing; Autonomous Vehicles; Feature-impact analyse; Impact analysis; Instance space analyse; Key feature; Search-based software testing; Simulated environment; Space analysis; Test scenario; Testing autonomous vehicle; Safety testing
Bug Analysis in Jupyter Notebook Projects: An Empirical Study,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191572066&doi=10.1145%2f3641539&partnerID=40&md5=3f4c00213ec29a1e91e15aeb731d658a,"Computational notebooks, such as Jupyter, have been widely adopted by data scientists to write code for analyzing and visualizing data. Despite their growing adoption and popularity, few studies have been found to understand Jupyter development challenges from the practitioners’ point of view. This article presents a systematic study of bugs and challenges that Jupyter practitioners face through a large-scale empirical investigation. We mined 14,740 commits from 105 GitHub open source projects with Jupyter Notebook code. Next, we analyzed 30,416 StackOverflow posts, which gave us insights into bugs that practitioners face when developing Jupyter Notebook projects. Next, we conducted 19 interviews with data scientists to uncover more details about Jupyter bugs and to gain insight into Jupyter developers’ challenges. Finally, to validate the study results and proposed taxonomy, we conducted a survey with 91 data scientists. We highlight bug categories, their root causes, and the challenges that Jupyter practitioners face. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bugs; empirical study; interviews; Jupyter Notebooks; mining software repositories (MSR); StackOverflow,Open source software; Open systems; Bug; Empirical studies; Interview; Jupyter notebook; Large-scales; Mining software; Mining software repository; Software repositories; Stackoverflow; Systematic study; Program debugging
EASE: An Effort-aware Extension of Unsupervised Key Class Identification Approaches,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191979502&doi=10.1145%2f3635714&partnerID=40&md5=8eea763a0461161e99c7712309ab8532,"Key class identification approaches aim at identifying the most important classes to help developers, especially newcomers, start the software comprehension process. So far, many supervised and unsupervised approaches have been proposed; however, they have not considered the effort to comprehend classes. In this article, we identify the challenge of “effort-aware key class identification”; to partially tackle it, we propose an approach, EASE, which is implemented through a modification to existing unsupervised key class identification approaches to take into consideration the effort to comprehend classes. First, EASE chooses a set of network metrics that has a wide range of applications in the existing unsupervised approaches and also possesses good discriminatory power. Second, EASE normalizes the network metric values of classes to quantify the probability of any class to be a key class and utilizes Cognitive Complexity to estimate the effort required to comprehend classes. Third, EASE proposes a metric, RKCP, to measure the relative key-class proneness of classes and further uses it to sort classes in descending order. Finally, an effort threshold is utilized, and the top-ranked classes within the threshold are identified as the cost-effective key classes. Empirical results on a set of 18 software systems show that (i) the proposed effort-aware variants perform significantly better in almost all (≈98.33%) the cases, (ii) they are superior to most of the baseline approaches with only several exceptions, and (iii) they are scalable to large-scale software systems. Based on these findings, we suggest that (i) we should resort to effort-aware key class identification techniques in budget-limited scenarios; and (ii) when using different techniques, we should carefully choose the weighting mechanism to obtain the best performance. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",complex networks; Key classes; network metrics; program comprehension; static analysis,Budget control; Computer software; Cost effectiveness; Static analysis; Cognitive complexity; Comprehension process; Discriminatory power; Identification approach; Key class; Metric values; Network metrics; Program comprehension; Software comprehension; Unsupervised approaches; Complex networks
Enablers and Barriers of Empathy in Software Developer and User Interactions: A Mixed Methods Case Study,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191592324&doi=10.1145%2f3641849&partnerID=40&md5=6ef4c2f9a38115d3d1722588f173671f,"Software engineering (SE) requires developers to collaborate with stakeholders, and understanding their emotions and perspectives is often vital. Empathy is a concept characterising a person’s ability to understand and share the feelings of another. However, empathy continues to be an under-researched human aspect in SE. We studied how empathy is practised between developers and end users using a mixed methods case study. We used an empathy test, observations, and interviews to collect data and socio-technical grounded theory and descriptive statistics to analyse data. We identified the nature of awareness required to trigger empathy and enablers of empathy. We discovered barriers to empathy and a set of potential strategies to overcome these barriers. We report insights on emerging relationships and present a set of recommendations and potential future works on empathy and SE for software practitioners and SE researchers. © 2024 Copyright held by the owner/author(s).",awareness; barriers; Empathy; enablers; end users; human aspects; software developers; software engineering,Computation theory; Awareness; Barrier; Case-studies; Empathy; Enabler; End-users; Human aspects; Mixed method; Software developer; User interaction; Software engineering
Building Domain-Specific Machine Learning Workflows: A Conceptual Framework for the State of the Practice,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192273012&doi=10.1145%2f3638243&partnerID=40&md5=f931bc25feb42afa378fc861172658d0,"Domain experts are increasingly employing machine learning to solve their domain-specific problems. This article presents to software engineering researchers the six key challenges that a domain expert faces in addressing their problem with a computational workflow, and the underlying executable implementation. These challenges arise out of our conceptual framework which presents the “route” of transformations that a domain expert may choose to take while developing their solution. To ground our conceptual framework in the state of the practice, this article discusses a selection of available textual and graphical workflow systems and their support for the transformations described in our framework. Example studies from the literature in various domains are also examined to highlight the tools used by the domain experts as well as a classification of the domain specificity and machine learning usage of their problem, workflow, and implementation. The state of the practice informs our discussion of the six key challenges, where we identify which challenges and transformations are not sufficiently addressed by available tools. We also suggest possible research directions for software engineering researchers to increase the automation of these tools and disseminate best-practice techniques between software engineering and various scientific domains. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Computational workflow; domain experts; machine learning; machine learning pipelines; software engineering framework; workflow composition,Engineering education; Pipelines; Software engineering; Computational workflows; Conceptual frameworks; Domain experts; Engineering frameworks; Machine learning pipeline; Machine-learning; Software engineering framework; State of the practice; Workflow composition; Machine learning
Octopus: Scaling Value-Flow Analysis via Parallel Collection of Realizable Path Conditions,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191729159&doi=10.1145%2f3632743&partnerID=40&md5=0943f13bca219aa4055d4a338255fcbc,"Value-flow analysis is a fundamental technique in program analysis, benefiting various clients, such as memory corruption detection and taint analysis. However, existing efforts suffer from the low potential speedup that leads to a deficiency in scalability. In this work, we present a parallel algorithm Octopus to collect path conditions for realizable paths efficiently. Octopus builds on the realizability decomposition to collect the intraprocedural path conditions of different functions simultaneously on-demand and obtain realizable path conditions by concatenation, which achieves a high potential speedup in parallelization. We implement Octopus as a tool and evaluate it over 15 real-world programs. The experiment shows that Octopus significantly outperforms the state-of-the-art algorithms. Particularly, it detects NULL-pointer-dereference bugs for the project llvm with 6.3 MLoC within 6.9 minutes under the 40-thread setting. We also state and prove several theorems to demonstrate the soundness, completeness, and high potential speedup of Octopus. Our empirical and theoretical results demonstrate the great potential of Octopus in supporting various program analysis clients. The implementation has officially deployed at Ant Group, scaling the nightly code scan for massive FinTech applications. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",parallel computation; Value-flow analysis,Molluscs; High potential; Intraprocedural paths; Memory corruption; On demands; Parallel Computation; Path condition; Program analysis; Realizability; Scalings; Value flow analysis; Computation theory
The Lost World: Characterizing and Detecting Undiscovered Test Smells,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191739480&doi=10.1145%2f3631973&partnerID=40&md5=479e5ef8384e9ae455a3a4a6d71c3a41,"Test smell refers to poor programming and design practices in testing and widely spreads throughout software projects. Considering test smells have negative impacts on the comprehension and maintenance of test code and even make code-under-test more defect-prone, it thus has great importance in mining, detecting, and refactoring them. Since Deursen et al. introduced the definition of ""test smell"", several studies worked on discovering new test smells from test specifications and software practitioners' experience. Indeed, many bad testing practices are ""observed""by software developers during creating test scripts rather than through academic research and are widely discussed in the software engineering community (e.g., Stack Overflow) [70, 94]. However, no prior studies explored new bad testing practices from software practitioners' discussions, formally defined them as new test smell types, and analyzed their characteristics, which plays a bad role for developers in knowing these bad practices and avoiding using them during test code development. Therefore, we pick up those challenges and act by working on systematic methods to explore new test smell types from one of the most mainstream developers' Q&A platforms, i.e., Stack Overflow. We further investigate the harmfulness of new test smells and analyze possible solutions for eliminating them. We find that some test smells make it hard for developers to fix failed test cases and trace their failing reasons. To exacerbate matters, we have identified two types of test smells that pose a risk to the accuracy of test cases. Next, we develop a detector to detect test smells from software. The detector is composed of six detection methods for different smell types. These detection methods are both wrapped with a set of syntactic rules based on the code patterns extracted from different test smells and developers' code styles. We manually construct a test smell dataset from seven popular Java projects and evaluate the effectiveness of our detector on it. The experimental results show that our detector achieves high performance in precision, recall, and F1 score. Then, we utilize our detector to detect smells from 919 real-world Java projects to explore whether the six test smells are prevalent in practice. We observe that these test smells are widely spread in 722 out of 919 Java projects, which demonstrates that they are prevalent in real-world projects. Finally, to validate the usefulness of test smells in practice, we submit 56 issue reports to 53 real-world projects with different smells. Our issue reports achieve 76.4% acceptance by conducting sentiment analysis on developers' replies. These evaluations confirm the effectiveness of our detector and the prevalence and practicality of new test smell types on real-world projects. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",empirical study; mining software repositories; Test smell; test smell detection,Codes (symbols); Java programming language; Odors; Statistical tests; Empirical studies; Mining software; Mining software repository; Real world projects; Software practitioners; Software repositories; Stack overflow; Test code; Test smell; Test smell detection; Software testing
Ethics in the Age of AI: An Analysis of AI Practitioners' Awareness and Challenges,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186918210&doi=10.1145%2f3635715&partnerID=40&md5=e93c750c8cf6cb2182e77e3b7103bdfe,"Ethics in AI has become a debated topic of public and expert discourse in recent years. But what do people who build AI - AI practitioners - have to say about their understanding of AI ethics and the challenges associated with incorporating it into the AI-based systems they develop? Understanding AI practitioners' views on AI ethics is important as they are the ones closest to the AI systems and can bring about changes and improvements. We conducted a survey aimed at understanding AI practitioners' awareness of AI ethics and their challenges in incorporating ethics. Based on 100 AI practitioners' responses, our findings indicate that the majority of AI practitioners had a reasonable familiarity with the concept of AI ethics, primarily due to workplace rules and policies. Privacy protection and security was the ethical principle that the majority of them were aware of. Formal education/training was considered somewhat helpful in preparing practitioners to incorporate AI ethics. The challenges that AI practitioners faced in the development of ethical AI-based systems included (i) general challenges, (ii) technology-related challenges, and (iii) human-related challenges. We also identified areas needing further investigation and provided recommendations to assist AI practitioners and companies in incorporating ethics into AI development. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AI ethics; AI practitioners; awareness; challenges; survey,AI ethic; AI practitioner; AI systems; Awareness; Challenge; Education training; Ethical principles; Formal education; Privacy protection; Protection and security; Ethical technology
Algorithm Selection for Software Verification Using Graph Neural Networks,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191983408&doi=10.1145%2f3637225&partnerID=40&md5=7ea361e61f50db38724f756ceb47e39f,"The field of software verification has produced a wide array of algorithmic techniques that can prove a variety of properties of a given program. It has been demonstrated that the performance of these techniques can vary up to 4 orders of magnitude on the same verification problem. Even for verification experts, it is difficult to decide which tool will perform best on a given problem. For general users, deciding the best tool for their verification problem is effectively impossible. In this work, we present Graves, a selection strategy based on graph neural networks (GNNs). Graves generates a graph representation of a program from which a GNN predicts a score for a verifier that indicates its performance on the program. We evaluate Graves on a set of 10 verification tools and over 8,000 verification problems and find that it improves the state-of-the-art in verification algorithm selection by 12%, or 8 percentage points. Further, it is able to verify 9% more problems than any existing verifier on our test set. Through a qualitative study on model interpretability, we find strong evidence that the Graves model learns to base its predictions on factors that relate to the unique features of the algorithmic techniques.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Algorithm selection; graph neural networks,Verification; Algorithm selection; Algorithmic techniques; Graph neural networks; Graph representation; Orders of magnitude; Performance; Property; Software verification; Verification problems; Verification tools; Graph neural networks
SourcererJBF: A Java Build Framework For Large-Scale Compilation,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191740106&doi=10.1145%2f3635710&partnerID=40&md5=6596584bf206167355675dce8dcc8653,"Researchers and tool developers working on dynamic analysis, software testing, automated program repair, verification, and validation, need large compiled, compilable, and executable code corpora to test their ideas. The publicly available corpora are relatively small, and/or non-compilable, and/or non-executable. Developing a compiled code corpus is a laborious activity demanding significant manual effort and human intervention. To facilitate large-scale program analysis research, we develop SourcererJBF, a Java Build Framework that can automatically build a large Java code corpus without project-specific instructions and human intervention. To generate a compiled code corpus, SourcererJBF creates an offline knowledge base by collecting external dependencies from the project directories and existing build scripts (if available). It constructs indices of those collected external dependencies that enable a fast search for resolving dependencies during the project compilation. As the output of the large-scale compilation, it produces JAigantic, a compilable Java corpus containing compiled projects, their bytecode, dependencies, normalized build script, and build command. We evaluated SourcererJBF's effectiveness, correctness, performance, and scalability in a large collection of Java projects. Our experimental results demonstrate that SourcererJBF is significantly effective and scalable in building large Java code corpus. Besides, it substantiates reasonable performance and correctness similar to projects' existing build systems. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated builds; build systems; Java; program analysis,Java programming language; Knowledge based systems; Verification; Analysis softwares; Automated build; Build systems; Dynamics analysis; Human intervention; Java; Java codes; Large-scales; ON dynamics; Program analysis; Software testing
Compositional Verification of First-Order Masking Countermeasures against Power Side-Channel Attacks,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191729314&doi=10.1145%2f3635707&partnerID=40&md5=a3b6a3c26f878c625f2db1dd5b3ccbd5,"Power side-channel attacks allow an adversary to efficiently and effectively steal secret information (e.g., keys) by exploiting the correlation between secret data and runtime power consumption, hence posing a serious threat to software security, particularly cryptographic implementations. Masking is a commonly used countermeasure against such attacks, which breaks the statistical dependence between secret data and side-channel leaks via randomization. In a nutshell, a variable is represented by a vector of shares armed with random variables, called masking encoding, on which cryptographic computations are performed. While compositional verification for the security of masked cryptographic implementations has received much attention because of its high efficiency, existing compositional approaches either use implicitly fixed pre-conditions that may not be fulfilled by state-of-the-art efficient implementations, or require user-provided hard-coded pre-conditions that are time consuming and highly non-trivial, even for an expert. In this article, we tackle the compositional verification problem of first-order masking countermeasures, where first-order means that the adversary is allowed to access only one intermediate computation result. Following the literature, we consider countermeasures given as gadgets, which are special procedures whose inputs are masking encodings of variables. We introduce a new security notion parameterized by an explicit pre-condition for each gadget, as well as composition rules for reasoning about masking countermeasures against power side-channel attacks. We propose accompanying efficient algorithms to automatically infer proper pre-conditions, based on which our new compositional approach can efficiently and automatically prove security for masked implementations. We implement our approaches as a tool MaskCV and conduct experiments on publicly available masked cryptographic implementations including 10 different full AES implementations. The experimental results confirm the effectiveness and efficiency of our approach. © 2024 Copyright held by the owner/author(s).",compositional verification; cryptographic programs; Formal verification; masking countermeasures; side-channel attacks,Efficiency; Encoding (symbols); Side channel attack; Compositional verification; Condition; Cryptographic implementation; Cryptographic program; CryptoGraphics; First order; Masking countermeasure; Power; Secret data; Side-channel attacks; Formal verification
Attack as Detection: Using Adversarial Attack Methods to Detect Abnormal Examples,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191570653&doi=10.1145%2f3631977&partnerID=40&md5=f649b5d0cf4ff6650cb36ed7d61f3302,"As a new programming paradigm, deep learning (DL) has achieved impressive performance in areas such as image processing and speech recognition, and has expanded its application to solve many real-world problems. However, neural networks and DL are normally black-box systems; even worse, DL-based software are vulnerable to threats from abnormal examples, such as adversarial and backdoored examples constructed by attackers with malicious intentions as well as unintentionally mislabeled samples. Therefore, it is important and urgent to detect such abnormal examples. Although various detection approaches have been proposed respectively addressing some specific types of abnormal examples, they suffer from some limitations; until today, this problem is still of considerable interest. In this work, we first propose a novel characterization to distinguish abnormal examples from normal ones based on the observation that abnormal examples have significantly different (adversarial) robustness from normal ones. We systemically analyze those three different types of abnormal samples in terms of robustness and find that they have different characteristics from normal ones. As robustness measurement is computationally expensive and hence can be challenging to scale to large networks, we then propose to effectively and efficiently measure robustness of an input sample using the cost of adversarially attacking the input, which was originally proposed to test robustness of neural networks against adversarial examples. Next, we propose a novel detection method, named attack as detection (A2D for short), which uses the cost of adversarially attacking an input instead of robustness to check if it is abnormal. Our detection method is generic, and various adversarial attack methods could be leveraged. Extensive experiments show that A2D is more effective than recent promising approaches that were proposed to detect only one specific type of abnormal examples. We also thoroughly discuss possible adaptive attack methods to our adversarial example detection method and show that A2D is still effective in defending carefully designed adaptive adversarial attack methods - for example, the attack success rate drops to 0% on CIFAR10. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial examples; backdoored samples; Deep learning; detection; mislabeled samples; neural networks,Deep learning; Image processing; Adversarial example; Attack methods; Backdoored sample; Deep learning; Detection; Detection methods; Mislabeled sample; Neural-networks; Performance; Programming paradigms; Speech recognition
PTM-APIRec: Leveraging Pre-trained Models of Source Code in API Recommendation,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191725250&doi=10.1145%2f3632745&partnerID=40&md5=77ad9f0aab3c0d569bc9fbdb4b9fc117,"Recommending APIs is a practical and essential feature of IDEs. Improving the accuracy of API recommendations is an effective way to improve coding efficiency. With the success of deep learning in software engineering, the state-of-the-art (SOTA) performance of API recommendation is also achieved by deep-learning-based approaches. However, existing SOTAs either only consider the API sequences in the code snippets or rely on complex operations for extracting hand-crafted features, all of which have potential risks in under-encoding the input code snippets and further resulting in sub-optimal recommendation performance. To this end, this article proposes to utilize the code understanding ability of existing general code Pre-Training Models to fully encode the input code snippet to improve the accuracy of API Recommendation, namely, PTM-APIRec. To ensure that the code semantics of the input are fully understood and the API recommended actually exists, we use separate vocabularies for the input code snippet and the APIs to be predicted. The experimental results on the JDK and Android datasets show that PTM-APIRec surpasses existing approaches. Besides, an effective way to improve the performance of PTM-APIRec is to enhance the pre-trained model with more pre-training data (which is easier to obtain than API recommendation datasets). Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",API recommendation; Code Completion; Code Pre-training,Deep learning; Optimal systems; Semantics; Software engineering; API recommendation; Code completions; Code pre-training; Coding efficiency; Complex operations; Essential features; Learning-based approach; Pre-training; Source codes; State-of-the-art performance; Encoding (symbols)
A Post-training Framework for Improving the Performance of Deep Learning Models via Model Transformation,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191698797&doi=10.1145%2f3630011&partnerID=40&md5=85850fdb269b5f5c9dca18c37e23375c,"Deep learning (DL) techniques have attracted much attention in recent years and have been applied to many application scenarios. To improve the performance of DL models regarding different properties, many approaches have been proposed in the past decades, such as improving the robustness and fairness of DL models to meet the requirements for practical use. Among existing approaches, post-training is an effective method that has been widely adopted in practice due to its high efficiency and good performance. Nevertheless, its performance is still limited due to the incompleteness of training data. Additionally, existing approaches are always specifically designed for certain tasks, such as improving model robustness, which cannot be used for other purposes.In this article, we aim to fill this gap and propose an effective and general post-training framework, which can be adapted to improve the model performance from different aspects. Specifically, it incorporates a novel model transformation technique that transforms a classification model into an isomorphic regression model for fine-tuning, which can effectively overcome the problem of incomplete training data by forcing the model to strengthen the memory of crucial input features and thus improve the model performance eventually. To evaluate the performance of our framework, we have adapted it to two emerging tasks for improving DL models, i.e., robustness and fairness improvement, and conducted extensive studies by comparing it with state-of-the-art approaches. The experimental results demonstrate that our framework is indeed general, as it is effective in both tasks. Specifically, in the task of robustness improvement, our approach Dare has achieved the best results on 61.1% cases (vs. 11.1% cases achieved by baselines). In the task of fairness improvement, our approach FMT can effectively improve the fairness without sacrificing the accuracy of the models. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep neural network; delta debugging; model fairness; model robustness,Classification (of information); Learning systems; Metadata; Regression analysis; Delta debugging; Learning models; Learning techniques; Model fairness; Model robustness; Model transformation; Modeling performance; Performance; Training framework; Via modeling; Deep neural networks
An Extractive-and-Abstractive Framework for Source Code Summarization,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192012599&doi=10.1145%2f3632742&partnerID=40&md5=2122910047817e87f23912f47bef7dea,"(Source) Code summarization aims to automatically generate summaries/comments for given code snippets in the form of natural language. Such summaries play a key role in helping developers understand and maintain source code. Existing code summarization techniques can be categorized into extractive methods and abstractive methods. The extractive methods extract a subset of important statements and keywords from the code snippet using retrieval techniques and generate a summary that preserves factual details in important statements and keywords. However, such a subset may miss identifier or entity naming, and consequently, the naturalness of the generated summary is usually poor. The abstractive methods can generate human-written-like summaries leveraging encoder-decoder models. However, the generated summaries often miss important factual details.To generate human-written-like summaries with preserved factual details, we propose a novel extractive-and-abstractive framework. The extractive module in the framework performs the task of extractive code summarization, which takes in the code snippet and predicts important statements containing key factual details. The abstractive module in the framework performs the task of abstractive code summarization, which takes in the code snippet and important statements in parallel and generates a succinct and human-written-like natural language summary. We evaluate the effectiveness of our technique, called EACS, by conducting extensive experiments on three datasets involving six programming languages. Experimental results show that EACS significantly outperforms state-of-the-art techniques for all three widely used metrics, including BLEU, METEOR, and ROUGH-L. In addition, the human evaluation demonstrates that the summaries generated by EACS have higher naturalness and informativeness and are more relevant to given code snippets.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",abstractive code summarization program comprehension; Code summarization; extractive code summarization,Codes (symbols); Natural language processing systems; Abstractive code summarization program comprehension; Code summarization; Encoder-decoder; Extractive code summarization; Human evaluation; Natural languages; Program comprehension; Retrieval techniques; Source codes; State-of-the-art techniques; Computer programming languages
Learning-based Relaxation of Completeness Requirements for Data Entry Forms,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191708136&doi=10.1145%2f3635708&partnerID=40&md5=c351e8204cdd034e9bc83576e3354e31,"Data entry forms use completeness requirements to specify the fields that are required or optional to fill for collecting necessary information from different types of users. However, because of the evolving nature of software, some required fields may not be applicable for certain types of users anymore. Nevertheless, they may still be incorrectly marked as required in the form; we call such fields obsolete required fields. Since obsolete required fields usually have ""not-null""validation checks before submitting the form, users have to enter meaningless values in such fields to complete the form submission. These meaningless values threaten the quality of the filled data and could negatively affect stakeholders or learning-based tools that use the data. To avoid users filling meaningless values, existing techniques usually rely on manually written rules to identify the obsolete required fields and relax their completeness requirements. However, these techniques are ineffective and costly.In this article, we propose LACQUER, a learning-based automated approach for relaxing the completeness requirements of data entry forms. LACQUER builds Bayesian Network models to automatically learn conditions under which users had to fill meaningless values. To improve its learning ability, LACQUER identifies the cases where a required field is only applicable for a small group of users and uses SMOTE, an oversampling technique, to generate more instances on such fields for effectively mining dependencies on them. During the data entry session, LACQUER predicts the completeness requirement of a target based on the already filled fields and their conditional dependencies in the trained model.Our experimental results show that LACQUER can accurately relax the completeness requirements of required fields in data entry forms with precision values ranging between 0.76 and 0.90 on different datasets. LACQUER can prevent users from filling 20% to 64% of meaningless values, with negative predictive values (i.e., the ability to correctly predict a field as ""optional"") between 0.72 and 0.91. Furthermore, LACQUER is efficient; it takes at most 839 ms to predict the completeness requirement of an instance. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",completeness requirements relaxation; data entry forms; Form filling; machine learning; software data quality; user interfaces,Bayesian networks; Filling; Learning systems; Machine learning; Automated approach; Bayesian network models; Completeness requirement relaxation; Data entry form; Form filling; Learn+; Machine-learning; Nature of software; Software data quality; Validation checks; User interfaces
Vision Transformer Inspired Automated Vulnerability Repair,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191687911&doi=10.1145%2f3632746&partnerID=40&md5=8c8d2ffb7dc8af2dd1f9e2bad2af1db8,"Recently, automated vulnerability repair approaches have been widely adopted to combat increasing software security issues. In particular, transformer-based encoder-decoder models achieve competitive results. Whereas vulnerable programs may only consist of a few vulnerable code areas that need repair, existing AVR approaches lack a mechanism guiding their model to pay more attention to vulnerable code areas during repair generation. In this article, we propose a novel vulnerability repair framework inspired by the Vision Transformer based approaches for object detection in the computer vision domain. Similar to the object queries used to locate objects in object detection in computer vision, we introduce and leverage vulnerability queries (VQs) to locate vulnerable code areas and then suggest their repairs. In particular, we leverage the cross-attention mechanism to achieve the cross-match between VQs and their corresponding vulnerable code areas. To strengthen our cross-match and generate more accurate vulnerability repairs, we propose to learn a novel vulnerability mask (VM) and integrate it into decoders' cross-attention, which makes our VQs pay more attention to vulnerable code areas during repair generation. In addition, we incorporate our VM into encoders' self-attention to learn embeddings that emphasize the vulnerable areas of a program. Through an extensive evaluation using the real-world 5,417 vulnerabilities, our approach outperforms all of the automated vulnerability repair baseline methods by 2.68% to 32.33%. Additionally, our analysis of the cross-attention map of our approach confirms the design rationale of our VM and its effectiveness. Finally, our survey study with 71 software practitioners highlights the significance and usefulness of AI-generated vulnerability repairs in the realm of software security. The training code and pre-trained models are available at https://github.com/awsm-research/VQM.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",automated vulnerability repair; Software security,Automation; Computer vision; Decoding; Object detection; Object recognition; Signal encoding; Attention mechanisms; Automated vulnerability repair; Embeddings; Encoder-decoder; Learn+; Objects detection; Real-world; Security issues; Software security; Vulnerable area; Repair
Understanding Developers Well-being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic - RCR Report,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191749643&doi=10.1145%2f3640338&partnerID=40&md5=fef85e79a43e8e10491ec7719e9b8a21,"The artifact accompanying the paper ""Understanding Developers Well-Being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic""provides a comprehensive set of tools, data, and scripts that were utilized in the longitudinal study. Spanning 24 months, from April 2020 to April 2022, the study delves into the shifts in well-being, productivity, social contacts, needs, and several other variables of software engineers during the COVID-19 pandemic. The artifact facilitates the reproduction of the study's findings, offering a deeper insight into the systematic changes observed in various variables, such as well-being, quality of social contacts, and emotional loneliness. By providing access to the evidence-generating mechanisms and the generated data, the artifact ensures transparency and reproducibility and allows researchers to use our rich dataset to test their own research question. This Replicated Computational Results report aims to detail the contents of the artifact, its relevance to the main paper, and guidelines for its effective utilization. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",COVID-19; future of work; IJARS Model; longitudinal study; well-being,Cell proliferation; Statistical tests; Future of works; Generating mechanism; IJARS model; Longitudinal analysis; Longitudinal study; Reproducibilities; Social contacts; Systematic changes; Well being; Well productivity; COVID-19
Causality-driven Testing of Autonomous Driving Systems,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191985111&doi=10.1145%2f3635709&partnerID=40&md5=ff8ace0c9bb6e6b17e2519d7d3c4d1c4,"Testing Autonomous Driving Systems (ADS) is essential for safe development of self-driving cars. For thorough and realistic testing, ADS are usually embedded in a simulator and tested in interaction with the simulated environment. However, their high complexity and the multiple safety requirements lead to costly and ineffective testing. Recent techniques exploit many-objective strategies and ML to efficiently search the huge input space. Despite the indubitable advances, the need for smartening the search keep being pressing. This article presents CART (CAusal-Reasoning-driven Testing), a new technique that formulates testing as a causal reasoning task. Learning causation, unlike correlation, allows assessing the effect of actively changing an input on the output, net of possible confounding variables. CART first infers the causal relations between test inputs and outputs, then looks for promising tests by querying the learnt model. Only tests suggested by the model are run on the simulator. An extensive empirical evaluation, using Pylot as ADS and CARLA as simulator, compares CART with state-of-the-art algorithms used recently on ADS. CART shows a significant gain in exposing more safety violations and does so more efficiently. More broadly, the work opens to a wider exploitation of causal learning beside (or on top of) ML for testing-related tasks.  © 2024 Copyright held by the owner/author(s).",AI testing; autonomous vehicles; causal reasoning; search-based software testing; Self-driving cars,Safety testing; Software testing; AI testing; Autonomous driving; Autonomous Vehicles; Causal reasoning; Driving systems; High complexity; Input space; Safety requirements; Search-based software testing; Simulated environment; Autonomous vehicles
Improving Automated Program Repair with Domain Adaptation,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191753874&doi=10.1145%2f3631972&partnerID=40&md5=f98a329a1367458477bcd80d208c32cc,"Automated Program Repair (APR) is defined as the process of fixing a bug/defect in the source code, by an automated tool. APR tools have recently experienced promising results by leveraging state-of-the-art Neural Language Processing (NLP) techniques. APR tools such as TFix and CodeXGLUE that combine text-to-text transformers with software-specific techniques are outperforming alternatives, these days. However, in most APR studies, the train and test sets are chosen from the same set of projects (i.e., when APR fixes a bug in the test set from project A, the model has already seen example fixed bugs from project A in the training set). In the real world, however, APR models are meant to be generalizable to new and different projects. Therefore, there is a potential threat that reported APR models with high effectiveness perform poorly when the characteristics of the new project or its bugs are different than the training set's (""Domain Shift"").In this study, we first define the problem of domain shift in automated program repair. Next, we measure the potential damage of domain shift on two recent APR models (TFix and CodeXGLUE). Based on this observation, we then propose a domain adaptation framework that can adapt an APR model for a given target project. We conduct an empirical study with three domain adaptation methods FullFineTuning, TuningWithLightWeightAdapterLayers, and CurriculumLearning and two APR models on 2,672 bugs from 12 projects.The results show that our proposed framework on average can improve the effectiveness of TFix by 13.05% and CodeXGLUE by 48.78%, in terms of ""Exact Match"". Through experiments, we also show that the framework provides high efficiency and reliability (in terms of ""Exposure Bias""). Using synthetic data to domain adapt TFix and CodeXGLUE on the projects with no data (Zero-shot learning), also results in an average improvement of 5.76% and 17.62% for TFix and CodeXGLUE, respectively. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated program repair; CodeBERT; deep learning; domain adaptation; neural machine translation; transformers,Automation; Computational linguistics; Neural machine translation; Program translators; Repair; Automated program repair; CodeBERT; Deep learning; Domain adaptation; Repair models; Repair tools; Source codes; Test sets; Training sets; Transformer; Deep learning
Measurement of Embedding Choices on Cryptographic API Completion Tasks,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191892430&doi=10.1145%2f3625291&partnerID=40&md5=c02864d39b6e87a1005da6b1e13d0592,"In this article, we conduct a measurement study to comprehensively compare the accuracy impacts of multiple embedding options in cryptographic API completion tasks. Embedding is the process of automatically learning vector representations of program elements. Our measurement focuses on design choices of three important aspects, program analysis preprocessing, token-level embedding, and sequence-level embedding. Our findings show that program analysis is necessary even under advanced embedding. The results show 36.20% accuracy improvement, on average, when program analysis preprocessing is applied to transfer bytecode sequences into API dependence paths. With program analysis and the token-level embedding training, the embedding dep2vec improves the task accuracy from 55.80% to 92.04%. Moreover, only a slight accuracy advantage (0.55%, on average) is observed by training the expensive sequence-level embedding compared with the token-level embedding. Our experiments also suggest the differences made by the data. In the cross-app learning setup and a data scarcity scenario, sequence-level embedding is more necessary and results in a more obvious accuracy improvement (5.10%). © 2024 Copyright held by the owner/author(s).",API dependency; cryptography; deep learning; embedding; Java; Neural code completion; neural networks; program analysis; secure coding,Cryptography; Deep neural networks; E-learning; Embeddings; Java programming language; API dependency; Code completions; Deep learning; Embeddings; Java; Neural code; Neural code completion; Neural-networks; Program analysis; Secure coding; Network coding
Deep Is Better? An Empirical Comparison of Information Retrieval and Deep Learning Approaches to Code Summarization,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191709929&doi=10.1145%2f3631975&partnerID=40&md5=c11db772951bef9a864f26bbe738d9db,"Code summarization aims to generate short functional descriptions for source code to facilitate code comprehension. While Information Retrieval (IR) approaches that leverage similar code snippets and corresponding summaries have led the early research, Deep Learning (DL) approaches that use neural models to capture statistical properties between code and summaries are now mainstream. Although some preliminary studies suggest that IR approaches are more effective in some cases, it is currently unclear how effective the existing approaches can be in general, where and why IR/DL approaches perform better, and whether the integration of IR and DL can achieve better performance. Consequently, there is an urgent need for a comprehensive study of the IR and DL code summarization approaches to provide guidance for future development in this area. This article presents the first large-scale empirical study of 18 IR, DL, and hybrid code summarization approaches on five benchmark datasets. We extensively compare different types of approaches using automatic metrics, we conduct quantitative and qualitative analyses of where and why IR and DL approaches perform better, respectively, and we also study hybrid approaches for assessing the effectiveness of integrating IR and DL. The study shows that the performance of IR approaches should not be underestimated, that while DL models perform better in predicting tokens from method signatures and capturing structural similarities in code, simple IR approaches tend to perform better in the presence of code with high similarity or long reference summaries, and that existing hybrid approaches do not perform as well as individual approaches in their respective areas of strength. Based on our findings, we discuss future research directions for better code summarization. Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",Code summarization; deep learning; information retrieval,Codes (symbols); Deep learning; Large datasets; Code comprehension; Code summarization; Deep learning; Empirical comparison; Hybrid approach; Information retrieval approach; Learning approach; Neural modelling; Performance; Source codes; Information retrieval
Understanding Developers Well-Being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197743414&doi=10.1145%2f3638244&partnerID=40&md5=0e4e24fdaea270526e8cf60dd50e9d4f,"The COVID-19 pandemic has brought significant and enduring shifts in various aspects of life, including increased flexibility in work arrangements. In a longitudinal study, spanning 24 months with six measurement points from April 2020 to April 2022, we explore changes in well-being, productivity, social contacts, and needs of software engineers during this time. Our findings indicate systematic changes in various variables. For example, well-being and quality of social contacts increased while emotional loneliness decreased as lockdown measures were relaxed. Conversely, people's boredom and productivity remained stable. Furthermore, a preliminary investigation into the future of work at the end of the pandemic revealed a consensus among developers for a preference of hybrid work arrangements. We also discovered that prior job changes and low job satisfaction were consistently linked to intentions to change jobs if current work conditions do not meet developers' needs. This highlights the need for software organizations to adapt to various work arrangements to remain competitive employers. Building upon our findings and the existing literature, we introduce the Integrated Job Demands-Resources and Self-Determination (IJARS) Model as a comprehensive framework to explain the well-being and productivity of software engineers during the COVID-19 pandemic. Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",COVID-19; future of work; IJARS model; longitudinal study; well-being,
Representation Learning for Stack Overflow Posts: How Far Are We?,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192010921&doi=10.1145%2f3635711&partnerID=40&md5=a4824e042d89d61c26287b630880a1e4,"The tremendous success of Stack Overflow has accumulated an extensive corpus of software engineering knowledge, thus motivating researchers to propose various solutions for analyzing its content. The performance of such solutions hinges significantly on the selection of representation models for Stack Overflow posts. As the volume of literature on Stack Overflow continues to burgeon, it highlights the need for a powerful Stack Overflow post representation model and drives researchers' interest in developing specialized representation models that can adeptly capture the intricacies of Stack Overflow posts. The state-of-the-art (SOTA) Stack Overflow post representation models are Post2Vec and BERTOverflow, which are built upon neural networks such as convolutional neural network and transformer architecture (e.g., BERT). Despite their promising results, these representation methods have not been evaluated in the same experimental setting. To fill the research gap, we first empirically compare the performance of the representation models designed specifically for Stack Overflow posts (Post2Vec and BERTOverflow) in a wide range of related tasks (i.e., tag recommendation, relatedness prediction, and API recommendation). The results show that Post2Vec cannot further improve the SOTA techniques of the considered downstream tasks, and BERTOverflow shows surprisingly poor performance. To find more suitable representation models for the posts, we further explore a diverse set of transformer-based models, including (1) general domain language models (RoBERTa, Longformer, and GPT2) and (2) language models built with software engineering related textual artifacts (CodeBERT, GraphCodeBERT, seBERT, CodeT5, PLBart, and CodeGen). This exploration shows that models like CodeBERT and RoBERTa are suitable for representing Stack Overflow posts. However, it also illustrates the ""No Silver Bullet""concept, as none of the models consistently wins against all the others. Inspired by the findings, we propose SOBERT, which employs a simple yet effective strategy to improve the representation models of Stack Overflow posts by continuing the pre-training phase with the textual artifact from Stack Overflow. The overall experimental results demonstrate that SOBERT can consistently outperform the considered models and increase the SOTA performance significantly for all the downstream tasks. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",pre-trained models; Stack Overflow; transformers,Convolutional neural networks; Software engineering; Down-stream; Engineering knowledge; Language model; Neural-networks; Performance; Pre-trained model; Representation model; Stack overflow; State of the art; Transformer; Computational linguistics
Reusing Convolutional Neural Network Models through Modularization and Composition,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191901580&doi=10.1145%2f3632744&partnerID=40&md5=270034b3fda06c25500c4c1187a6c48f,"With the widespread success of deep learning technologies, many trained deep neural network (DNN) models are now publicly available. However, directly reusing the public DNN models for new tasks often fails due to mismatching functionality or performance. Inspired by the notion of modularization and composition in software reuse, we investigate the possibility of improving the reusability of DNN models in a more fine-grained manner. Specifically, we propose two modularization approaches named CNNSplitter and GradSplitter, which can decompose a trained convolutional neural network (CNN) model for N-class classification into N small reusable modules. Each module recognizes one of the N classes and contains a part of the convolution kernels of the trained CNN model. Then, the resulting modules can be reused to patch existing CNN models or build new CNN models through composition. The main difference between CNNSplitter and GradSplitter lies in their search methods: the former relies on a genetic algorithm to explore search space, while the latter utilizes a gradient-based search method. Our experiments with three representative CNNs on three widely used public datasets demonstrate the effectiveness of the proposed approaches. Compared with CNNSplitter, GradSplitter incurs less accuracy loss, produces much smaller modules (19.88% fewer kernels), and achieves better results on patching weak models. In particular, experiments on GradSplitter show that (1) by patching weak models, the average improvement in terms of precision, recall, and F1-score is 17.13%, 4.95%, and 11.47%, respectively, and (2) for a new task, compared with the models trained from scratch, reusing modules achieves similar accuracy (the average loss of accuracy is only 2.46%) without a costly training process. Our approaches provide a viable solution to the rapid development and improvement of CNN models.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CNN modularization; convolutional neural network; Model reuse; module composition,Computer software reusability; Convolution; Convolutional neural networks; Deep neural networks; Genetic algorithms; Modular construction; Reusability; Convolutional neural network; Convolutional neural network modularization; Learning technology; Model reuse; Modularizations; Module composition; Neural network model; Search method; Weak models; Neural network models
Poison Attack and Poison Detection on Deep Source Code Processing Models,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191813162&doi=10.1145%2f3630008&partnerID=40&md5=369b25e1a8460a3fdb2d12d7c8f48b96,"In the software engineering (SE) community, deep learning (DL) has recently been applied to many source code processing tasks, achieving state-of-the-art results. Due to the poor interpretability of DL models, their security vulnerabilities require scrutiny. Recently, researchers have identified an emergent security threat to DL models, namely, poison attacks. The attackers aim to inject insidious backdoors into DL models by poisoning the training data with poison samples. The backdoors mean that poisoned models work normally with clean inputs but produce targeted erroneous results with inputs embedded with specific triggers. By using triggers to activate backdoors, attackers can manipulate poisoned models in security-related scenarios (e.g., defect detection) and lead to severe consequences. To verify the vulnerability of deep source code processing models to poison attacks, we present a poison attack approach for source code named CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable and functionality-preserving poison samples and effectively attack deep source code processing models by poisoning the training data with poison samples. To defend against poison attacks, we further propose an effective poison detection approach named CodeDetector. CodeDetector can automatically identify poison samples in the training data. We apply CodePoisoner and CodeDetector to six deep source code processing models, including defect detection, clone detection, and code repair models. The results show that ¶ CodePoisoner conducts successful poison attacks with a high attack success rate (average: 98.3%, maximum: 100%). It validates that existing deep source code processing models have a strong vulnerability to poison attacks. • CodeDetector effectively defends against multiple poison attack approaches by detecting (maximum: 100%) poison samples in the training data. We hope this work can help SE researchers and practitioners notice poison attacks and inspire the design of more advanced defense techniques. Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",deep learning; Poison attack; poison detection; source code processing,Chemical detection; Codes (symbols); Computer programming languages; Defects; Software engineering; Backdoors; Deep learning; Deep sources; Learning models; Poison attack; Poison detection; Processing model; Source code processing; Source codes; Training data; Deep learning
Early Validation and Verification of System Behaviour in Model-based Systems Engineering: A Systematic Literature Review,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191732317&doi=10.1145%2f3631976&partnerID=40&md5=8d9ea0253c25491b2e7ff5ec015d59bf,"In the Systems Engineering (SE) domain there has been a paradigm shift from document-based to model-based system development artefacts; in fact, new methodologies are emerging to meet the increasing complexity of current systems and the corresponding growing need of digital workflows. In this regard, Model-Based Systems Engineering (MBSE) is considered as a key enabler by many central players of the SE community. MBSE has reached an adequate level of maturity, and there exist documented success stories in its adoption in industry. In particular, one significant benefit of utilising MBSE when compared to the traditional manual and document-centric workflows is that models are available from early phases of systems development; these enable a multitude of analyses prior any implementation effort together with other relevant capabilities, like the automation of development tasks. Nonetheless, it is noticeable there is a lack of a common understanding for how formal analyses for the verification and validation (V&V) of systems behaviour, specifically in the early phases of development, could be placed in an MBSE setting.In this article, we report on the planning, execution, and results of a systematic literature review regarding the early V&V of systems behaviour in the context of model-based systems engineering. The review aims to provide a structured representation of the state of the art with respect to motivations, proposed solutions, and limitations. From an initial set of potentially relevant 701 peer-reviewed publications we selected 149 primary studies, which we analysed according to a rigorous data extraction, analysis, and synthesis process. Based on our results, early V&V has usually the goal of checking the quality of a system design to avoid discovering flaws when parts are being concretely realised; SysML is a de facto standard for describing the system under study, while the solutions for the analyses tend to be varied; also V&V analyses tend to target varied properties with a slight predominance of functional concerns, and following the variation mentioned so far the proposed solutions are largely context specific; the proposed approaches are usually presented without explicit limitations, while when limitations are discussed, readiness of the solutions, handling of analyses simplifications/assumptions, and languages/tools integration are among the most frequently mentioned issues.Based on the survey results and the standard SE practices, we discuss how the current state-of-the-art MBSE supports early V&V of systems behaviour with a special focus on industrial adoption and identify relevant challenges to be researched further. © 2024 Copyright held by the owner/author(s).",MBSE; system behaviour; systematic literature review; validation; verification,Data mining; Early verification; Engineering domains; Model-based system engineerings; Paradigm shifts; State of the art; System behaviors; Systematic literature review; Validation; Validation and verification; Verification-and-validation; Systems analysis
How Are Multilingual Systems Constructed: Characterizing Language Use and Selection in Open-Source Multilingual Software,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189255123&doi=10.1145%2f3631967&partnerID=40&md5=97fd5937b43ab9063b7e1b0ae2783665,"For many years now, modern software is known to be developed in multiple languages (hence termed as multilingual or multi-language software). Yet, to date, we still only have very limited knowledge about how multilingual software systems are constructed. For instance, it is not yet really clear how different languages are used, selected together, and why they have been so in multilingual software development. Given the fact that using multiple languages in a single software project has become a norm, understanding language use and selection (i.e., language profile) as a basic element of the multilingual construction in contemporary software engineering is an essential first step.In this article, we set out to fill this gap with a large-scale characterization study on language use and selection in open-source multilingual software. We start with presenting an updated overview of language use in 7,113 GitHub projects spanning the 5 past years by characterizing overall statistics of language profiles, followed by a deeper look into the functionality relevance/justification of language selection in these projects through association rule mining. We proceed with an evolutionary characterization of 1,000 GitHub projects for each of the 10 past years to provide a longitudinal view of how language use and selection have changed over the years, as well as how the association between functionality and language selection has been evolving.Among many other findings, our study revealed a growing trend of using three to five languages in one multilingual software project and the noticeable stableness of top language selections. We found a non-trivial association between language selection and certain functionality domains, which was less stable than that with individual languages over time. In a historical context, we also have observed major shifts in these characteristics of multilingual systems both in contrast to earlier peer studies and along the evolutionary timeline. Our findings offer essential knowledge on the multilingual construction in modern software development. Based on our results, we also provide insights and actionable suggestions for both researchers and developers of multilingual systems.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",association mining; evolutionary characterization; functionality relevance; language profile; language selection; language use; Multilingual software,Data mining; Open source software; Open systems; Association mining; Evolutionary characterization; Functionality relevance; Language profile; Language selection; Language use; Multilingual software; Multilingual system; Multiple languages; Open-source; Software design
Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191901627&doi=10.1145%2f3630252&partnerID=40&md5=60e47859a497ad5574b438ca3e6ed9ac,"When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g., the number of closed pull requests in 12 months time). The training data for this task may be very small (e.g., 5 years of data, collected every month means just 60 rows of training data). The models generated from such tiny datasets can make many prediction errors.Those errors can be tamed by a landscape analysis that selects better learner control parameters. Our niSNEAK tool (a) clusters the data to find the general landscape of the hyperparameters, then (b) explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g., FLASH, HYPEROPT, OPTUNA).The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as C = number of commits, I = number of closed issues, and R = number of closed pull requests, niSNEAK's 12-month prediction errors are {I=0%, R=33% C=47%}, whereas other methods have far larger errors of {I=61%,R=119% C=149%}. We conjecture that niSNEAK works so well since it finds the most informative regions of the hyperparameters, then jumps to those regions. Other methods (that do not reflect over the landscape) can waste time exploring less informative options.Based on the preceding, we recommend landscape analytics (e.g., niSNEAK) especially when learning from very small datasets. This article only explores the application of niSNEAK to project health. That said, we see nothing in principle that prevents the application of this technique to a wider range of problems.To assist other researchers in repeating, improving, or even refuting our results, all our scripts and data are available on GitHub at https://github.com/zxcv123456qwe/niSneak.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Hyperparameter tuning; indepedent variable clustering; software health,Forecasting; Open source software; Clusterings; Hyper-parameter; Hyperparameter tuning; Indepedent variable clustering; Landscape analysis; Prediction errors; Project healths; Software health; Software project; Training data; Errors
Testing of Deep Reinforcement Learning Agents with Surrogate Models,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191729137&doi=10.1145%2f3631970&partnerID=40&md5=8b2b40e79057f526086425f0ab5623b8,"Deep Reinforcement Learning (DRL) has received a lot of attention from the research community in recent years. As the technology moves away from game playing to practical contexts, such as autonomous vehicles and robotics, it is crucial to evaluate the quality of DRL agents.In this article, we propose a search-based approach to test such agents. Our approach, implemented in a tool called Indago, trains a classifier on failure and non-failure environment (i.e., pass) configurations resulting from the DRL training process. The classifier is used at testing time as a surrogate model for the DRL agent execution in the environment, predicting the extent to which a given environment configuration induces a failure of the DRL agent under test. The failure prediction acts as a fitness function, guiding the generation towards failure environment configurations, while saving computation time by deferring the execution of the DRL agent in the environment to those configurations that are more likely to expose failures.Experimental results show that our search-based approach finds 50% more failures of the DRL agent than state-of-the-art techniques. Moreover, such failures are, on average, 78% more diverse; similarly, the behaviors of the DRL agent induced by failure configurations are 74% more diverse.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",reinforcement learning; Software testing,Autonomous agents; Deep learning; Intelligent agents; Learning systems; Software testing; Autonomous robotics; Autonomous Vehicles; Game playing; Reinforcement learning agent; Reinforcement learnings; Research communities; Search-based; Software testings; Surrogate modeling; Training process; Reinforcement learning
Safety of Perception Systems for Automated Driving: A Case Study on Apollo,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191941712&doi=10.1145%2f3631969&partnerID=40&md5=e6b4033834f8f9280d1ef00ad55de9d7,"The automotive industry is now known for its software-intensive and safety-critical nature. The industry is on a path to the holy grail of completely automating driving, starting from relatively simple operational areas like highways. One of the most challenging, evolving, and essential parts of automated driving is the software that enables understanding of surroundings and the vehicle's own as well as surrounding objects' relative position, otherwise known as the perception system. Current generation perception systems are formed by a combination of traditional software and machine learning-related software. With automated driving systems transitioning from research to production, it is imperative to assess their safety.We assess the safety of Apollo, the most popular open-source automotive software, at the design level for its use on a Dutch highway. We identified 58 safety requirements, 38 of which are found to be fulfilled at the design level. We observe that all requirements relating to traditional software are fulfilled, while most requirements specific to machine learning systems are not. This study unveils issues that need immediate attention; and directions for future research to make automated driving safe.  © 2024 Copyright held by the owner/author(s).",architecture assessment; artificial intelligence; Automated driving; functional safety; ISO 21448; ISO 26262; machine learning; perception system; requirement elicitation; safety of intended functionality; safety requirements; self-driving,Automation; Automotive industry; Machine learning; Motor transportation; Open source software; Open systems; Architecture assessment; Automated driving; Functional Safety; ISO 21448; ISO 26262; Machine-learning; Perception systems; Requirements elicitation; Safety of intended functionality; Safety requirements; Self drivings; Accident prevention
How Important Are Good Method Names in Neural Code Generation? A Model Robustness Perspective,2024,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175683705&doi=10.1145%2f3630010&partnerID=40&md5=2d0ca3c2d8696c5898bf384745119725,"Pre-trained code generation models (PCGMs) have been widely applied in neural code generation, which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this article, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs from a model robustness perspective. Specifically, we propose a novel approach, named neuRAl coDe generAtor Robustifier (RADAR). RADAR consists of two components: RADAR-Attack and RADAR-Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR-Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR-Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR-Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering. Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",adversarial examples; Code generation; passive defense; pre-trained model; robustness,Codes (symbols); Network security; Software engineering; Adversarial example; Codegeneration; Good methods; Model robustness; Neural code; Passive defense; Performance; Pre-trained model; Robustness; State of the art; Semantics
Learning to Detect Memory-related Vulnerabilities,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183325353&doi=10.1145%2f3624744&partnerID=40&md5=ada411ab2849e459230c34255ab66df2,"Memory-related vulnerabilities can result in performance degradation or even program crashes, constituting severe threats to the security of modern software. Despite the promising results of deep learning (DL)-based vulnerability detectors, there exist three main limitations: (1) rich contextual program semantics related to vulnerabilities have not yet been fully modeled; (2) multi-granularity vulnerability features in hierarchical code structure are still hard to be captured; and (3) heterogeneous flow information is not well utilized. To address these limitations, in this article, we propose a novel DL-based approach, called MVD+, to detect memory-related vulnerabilities at the statement-level. Specifically, it conducts both intraprocedural and interprocedural analysis to model vulnerability features, and adopts a hierarchical representation learning strategy, which performs syntax-aware neural embedding within statements and captures structured context information across statements based on a novel Flow-Sensitive Graph Neural Networks, to learn both syntactic and semantic features of vulnerable code. To demonstrate the performance, we conducted extensive experiments against eight state-of-the-art DL-based approaches as well as five well-known static analyzers on our constructed dataset with 6,879 vulnerabilities in 12 popular C/C++ applications. The experimental results confirmed that MVD+ can significantly outperform current state-of-the-art baselines and make a great trade-off between effectiveness and efficiency.  © 2023 Copyright held by the owner/author(s).",abstract syntax tree; flow analysis; graph neural networks; Statement-level vulnerability detection,Abstracting; C++ (programming language); Deep learning; Economic and social effects; Flow graphs; Learning systems; Syntactics; Trees (mathematics); Abstract Syntax Trees; Flow analysis; Graph neural networks; Learning-based approach; Multi-granularity; Performance degradation; Program semantics; State of the art; Statement-level vulnerability detection; Vulnerability detection; Semantics
DRIVE: Dockerfile Rule Mining and Violation Detection,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183332211&doi=10.1145%2f3617173&partnerID=40&md5=56b6390bbf5ee7e6215b1fbf6f7acb62,"A Dockerfile defines a set of instructions to build Docker images, which can then be instantiated to support containerized applications. Recent studies have revealed a considerable amount of quality issues with Dockerfiles. In this article, we propose a novel approach, Dockerfiles Rule mIning and Violation dEtection (DRIVE), to mine implicit rules and detect potential violations of such rules in Dockerfiles. DRIVE first parses Dockerfiles and transforms them to an intermediate representation. It then leverages an efficient sequential pattern mining algorithm to extract potential patterns. With heuristic-based reduction and moderate human intervention, potential rules are identified, which can then be utilized to detect potential violations of Dockerfiles. DRIVE identifies 34 semantic rules and 19 syntactic rules including 9 new semantic rules that have not been reported elsewhere. Extensive experiments on real-world Dockerfiles demonstrate the efficacy of our approach. © 2023 Association for Computing Machinery. All rights reserved.",configuration files; Docker; dockerfile; pattern mining; violation detection,Data mining; Configuration files; Docker; Dockerfile; Intermediate representations; Pattern mining; Quality issues; Rule mining; Rule violation; Semantic rules; Violation detections; Semantics
Hierarchical Distribution-aware Testing of Deep Learning,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183332338&doi=10.1145%2f3625290&partnerID=40&md5=8089772268c340024f8e7a23e735b0ce,"With its growing use in safety/security-critical applications, Deep Learning (DL) has raised increasing concerns regarding its dependability. In particular, DL has a notorious problem of lacking robustness. Input added with adversarial perturbations, i.e., Adversarial Examples (AEs), are easily mispredicted by the DL model. Despite recent efforts made in detecting AEs via state-of-the-art attack and testing methods, they are normally input distribution-agnostic and/or disregard the perceptual quality of adversarial perturbations. Consequently, the detected AEs are irrelevant inputs in the application context or noticeably unrealistic to humans. This may lead to a limited effect on improving the DL model's dependability, as the testing budget is likely to be wasted on detecting AEs that are encountered very rarely in its real-life operations. In this article, we propose a new robustness testing approach for detecting AEs that considers both the feature-level distribution and the pixel-level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. First, we select test seeds based on the density of feature-level distribution and the vulnerability of adversarial robustness. The vulnerability of test seeds is indicated by the auxiliary information, which are highly correlated with local robustness. Given a test seed, we then develop a novel genetic algorithm-based local test case generation method, in which two fitness functions work alternatively to control the perceptual quality of detected AEs. Finally, extensive experiments confirm that our holistic approach considering hierarchical distributions is superior to the state-of-the-arts that either disregard any input distribution or only consider a single (non-hierarchical) distribution, in terms of not only detecting imperceptible AEs but also improving the overall robustness of the DL model under testing.  © 2023 Copyright held by the owner/author(s).",adversarial examples detection; Deep learning robustness; distribution-aware testing; natural perturbations; robustness growth; safe AI,Budget control; Feature extraction; Genetic algorithms; Adversarial example detection; Deep learning robustness; Distribution-aware testing; Hierarchical distribution; Learning models; Level distribution; Natural perturbations; Perceptual quality; Robustness growth; Safe AI; Deep learning
Survey of Code Search Based on Deep Learning,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181557563&doi=10.1145%2f3628161&partnerID=40&md5=0776e89a97f1edb9cc94f948ba771e96,"Code writing is repetitive and predictable, inspiring us to develop various code intelligence techniques. This survey focuses on code search, that is, to retrieve code that matches a given natural language query by effectively capturing the semantic similarity between the query and code. Deep learning, being able to extract complex semantics information, has achieved great success in this field. Recently, various deep learning methods, such as graph neural networks and pretraining models, have been applied to code search with significant progress. Deep learning is now the leading paradigm for code search. In this survey, we provide a comprehensive overview of deep learning-based code search. We review the existing deep learning-based code search framework that maps query/code to vectors and measures their similarity. Furthermore, we propose a new taxonomy to illustrate the state-of-the-art deep learning-based code search in a three-step process: query semantics modeling, code semantics modeling, and matching modeling, which involves the deep learning model training. Finally, we suggest potential avenues for future research in this promising field.  © 2023 Copyright held by the owner/author(s).",Code search; code understanding; deep learning; natural language processing; pre-training,Codes (symbols); Deep learning; Graph neural networks; Information retrieval; Learning systems; Natural language processing systems; Code search; Code understanding; Code-writing; Deep learning; Language processing; Natural language processing; Natural languages; Pre-training; Search-based; Semantic modelling; Semantics
Characterizing and DetectingWebAssembly Runtime Bugs,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177693411&doi=10.1145%2f3624743&partnerID=40&md5=c530d2dac3ecc3295e153b500f0b2414,"WebAssembly (abbreviated WASM) has emerged as a promising language of the Web and also been used for a wide spectrum of software applications such as mobile applications and desktop applications. These applications, named WASM applications, commonly run in WASM runtimes. Bugs in WASM runtimes are frequently reported by developers and cause the crash of WASM applications. However, these bugs have not been well studied. To fill in the knowledge gap, we present a systematic study to characterize and detect bugs inWASM runtimes. We first harvest a dataset of 311 real-world bugs from hundreds of related posts on GitHub. Based on the collected high-quality bug reports, we distill 31 bug categories of WASM runtimes and summarize their common fix strategies. Furthermore, we develop a pattern-based bug detection framework to automatically detect bugs inWASM runtimes. We apply the detection framework to seven popularWASM runtimes and successfully uncover 60 bugs that have never been reported previously, among which 13 have been confirmed and 9 have been fixed by runtime developers.  © 2023 Copyright held by the owner/author(s).",WebAssembly; WebAssembly runtime,Application programs; Pattern recognition; Desktop applications; Detection framework; Knowledge gaps; Mobile applications; Runtimes; Software applications; Systematic study; Webassembly; Webassembly runtime; Wide spectrum; Program debugging
ALL: Supporting Experiential Accessibility Education and Inclusive Software Development,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183323681&doi=10.1145%2f3625292&partnerID=40&md5=fc362303794c1c26709b7fd3a352bc97,"Creating accessible software is imperative for making software inclusive for all users. Unfortunately, the topic of accessibility is frequently excluded from computing education, leading to scenarios where students are unaware of either how to develop accessible software or see the need to create it. To address this challenge, we have created a set of educational labs that are systematically designed to not only inform students about fundamental topics in producing accessible software but also demonstrate its importance. Over the previous year, these labs were included in several Computer Science 2 offerings at the Rochester Institute of Technology, comprising a total of 500 student participants. This article discusses instructional observations from these offerings, some of which include the following: (i) many of the research findings from previous efforts remain true with the larger, more diverse evaluation; (ii) our created material and format reduced students' belief that creating accessible software was difficult in relation to the baseline,; (iii) we observed that our created material and format benefited student opinion that creating accessible software is important, and (iv) computing majors may not be uniformly impacted by experiential educational accessibility material.  © 2023 Copyright held by the owner/author(s).",Accessibility education; computing accessibility; computing education,Education computing; Software design; Accessibility education; Computing accessibility; Computing education; Computing majors; Previous year; Rochester institute of technologies; Students
CLFuzz: Vulnerability Detection of Cryptographic Algorithm Implementation via Semantic-aware Fuzzing,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183326977&doi=10.1145%2f3628160&partnerID=40&md5=1b92f71efd069415bff336bee2295022,"Cryptography is a core component ofmany security applications, and flaws hidden in its implementation will affect the functional integrity or, more severely, pose threats to data security. Hence, guaranteeing the correctness of the implementation is important. However, the semantic characteristics (e.g., diverse input data and complex functional transformation) challenge those traditional program validation techniques (e.g., static analysis and dynamic fuzzing). In this article, we propose CLFuzz, a semantic-aware fuzzer for the vulnerability detection of cryptographic algorithm implementation. CLFuzz first extracts the semantic information of targeted algorithms including their cryptographic-specific constraints and function signatures. Based on them, CLFuzz generates high-quality input data adaptively to trigger error-prone situations efficiently. Furthermore, CLFuzz applies innovative logical cross-check that strengthens the logical bug detection ability. We evaluate CLFuzz on the widely used implementations of 54 cryptographic algorithms. It outperforms state-ofthe-art cryptographic fuzzing tools. For example, compared with Cryptofuzz, it achieves a coverage speedup of 3.4× and increases the final coverage by 14.4%. Furthermore, CLFuzz has detected 12 previously unknown implementation bugs in 8 cryptographic algorithms (e.g., CMAC in OpenSSL and Message Digest in Sym-Crypt), most of which are security-critical and have been successfully collected in the national vulnerability database (7 in NVD/CNVD) and is awarded by the Microsoft bounty program (2 for $1,000). © 2023 Association for Computing Machinery. All rights reserved.",Cryptographic algorithm; fuzzing; implementation bug,Cryptography; Input output programs; Metadata; Network security; Program debugging; Semantics; Algorithm implementation; Core components; Cryptographic algorithms; CryptoGraphics; Fuzzing; Implementation bug; Input datas; Security application; Semantic-aware; Vulnerability detection; Static analysis
LibAM: An Area Matching Framework for Detecting Third-Party Libraries in Binaries,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183327144&doi=10.1145%2f3625294&partnerID=40&md5=772c7079a8e934977718f26376373591,"Third-party libraries (TPLs) are extensively utilized by developers to expedite the software development pro- cess and incorporate external functionalities. Nevertheless, insecure TPL reuse can lead to significant security risks. Existing methods, which involve extracting strings or conducting function matching, are employed to determine the presence of TPL code in the target binary. However, these methods often yield unsatisfactory results due to the recurrence of strings and the presence of numerous similar non-homologous functions. Furthermore, the variation in C/C++ binaries across different optimization options and architectures exac- erbates the problem. Additionally, existing approaches struggle to identify specific pieces of reused code in the target binary, complicating the detection of complex reuse relationships and impeding downstream tasks. And, we call this issue the poor interpretability of TPL detection results. In this article, we observe that TPL reuse typically involves not just isolated functions but also areas en- compassing several adjacent functions on the Function Call Graph (FCG). We introduce LibAM, a novel Area Matching framework that connects isolated functions into function areas on FCG and detects TPLs by com- paring the similarity of these function areas, significantly mitigating the impact of different optimization options and architectures. Furthermore, LibAM is the first approach capable of detecting the exact reuse ar- eas on FCG and offering substantial benefits for downstream tasks. To validate our approach, we compile the first TPL detection dataset for C/C++ binaries across various optimization options and architectures. Ex- perimental results demonstrate that LibAM outperforms all existing TPL detection methods and provides interpretable evidence for TPL detection results by identifying exact reuse areas. We also evaluate LibAM's scalability on large-scale, real-world binaries in IoT firmware and generate a list of potential vulnerabilities for these devices. Our experiments indicate that the Area Matching framework performs exceptionally well in the TPL detection task and holds promise for other binary similarity analysis tasks. Last but not least, by analyzing the detection results of IoT firmware, we make several interesting findings, for instance, different target binaries always tend to reuse the same code area of TPL.  © 2023 Copyright held by the owner/author(s).",software component analysis; Static binary analysis; third-party library detection,C++ (programming language); Firmware; Internet of things; Software design; Area matching; Component analysis; Function-call graphs; Optimisations; Reuse; Software component analyse; Software-component; Static binary analysis; Third parties; Third-party library detection; Libraries
KAPE: kNN-based Performance Testing for Deep Code Search,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183325519&doi=10.1145%2f3624735&partnerID=40&md5=1719267d95a58f925b5c97a33ec3de28,"Code search is a common yet important activity of software developers. An efficient code search model can largely facilitate the development process and improve the programming quality. Given the superb performance of learning the contextual representations, deep learning models, especially pre-trained languagemodels, have been widely explored for the code search task. However, studies mainly focus on proposing new architectures for ever-better performance on designed test sets but ignore the performance on unseen test data where only natural language queries are available. The same problem in other domains, e.g., CV and NLP, is usually solved by test input selection that uses a subset of the unseen set to reduce the labeling effort. However, approaches from other domains are not directly applicable and still require labeling effort. In this article, we propose the kNN-based performance testing (KAPE) to efficiently solve the problem without manually matching code snippets to test queries. The main idea is to use semantically similar training data to perform the evaluation. Extensive experiments on six programming language datasets, three state-of-theart pre-trained models, and seven baseline methods demonstrate that KAPE can effectively assess the model performance (e.g., CodeBERT achieves MRR 0.5795 on JavaScript) with a slight difference (e.g., 0.0261).  © 2023 Copyright held by the owner/author(s).",Deep code search; deep learning testing; software testing; test selection,Codes (symbols); Deep learning; Information retrieval; Learning algorithms; Natural language processing systems; Code search; Deep code search; Deep learning testing; Labelings; Performance; Performance testing; Search models; Software developer; Software testings; Test selection; Software testing
Generation-based Differential Fuzzing for Deep Learning Libraries,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183314192&doi=10.1145%2f3628159&partnerID=40&md5=5cf1bdb8be5fe6adf97e18ddbaa6d9f6,"Deep learning (DL) libraries have become the key component in developing and deploying DL-based software nowadays. With the growing popularity of applying DL models in both academia and industry across various domains, any bugs inherent in the DL libraries can potentially cause unexpected server outcomes. As such, there is an urgent demand for improving the software quality ofDL libraries. Although there are some existing approaches specifically designed for testing DL libraries, their focus is usually limited to one specific domain, such as computer vision (CV). It is still not very clear howthe existing approaches perform in detecting bugs of different DL libraries regarding different task domains and to what extent. To bridge this gap, we first conduct an empirical study on four representative and state-of-the-art DL library testing approaches. Our empirical study results reveal that it is hard for existing approaches to generalize to other task domains. We also find that the test inputs generated by these approaches usually lack diversity, with only a few types of bugs. What is worse, the false-positive rate of existing approaches is also high (up to 58%). To address these issues, we propose a guided differential fuzzing approach based on generation, namely, Gandalf. To generate testing inputs across diverse task domains effectively, Gandalf adopts the context-free grammar to ensure validity and utilizes a Deep Q-Network to maximize the diversity. Gandalf also includes 15 metamorphic relations to make it possible for the generated test cases to generalize across different DL libraries. Such a design can decrease the false positives because of the semantic difference for different APIs. We evaluate the effectiveness of Gandalf on nine versions of three representative DL libraries, covering 309 operators from computer vision, natural language processing, and automated speech recognition. The evaluation results demonstrate that Gandalf can effectively and efficiently generate diverse test inputs. Meanwhile, Gandalf successfully detects five categories of bugs with only 3.1% false-positive rates. We report all 49 new unique bugs found during evaluation to the DL libraries' developers, and most of these bugs have been confirmed. Details about our empirical study and evaluation results are available on our project website. © 2023 Copyright held by the owner/author(s).",deep learning libraries; generation-based fuzzing; Software testing,Computer software selection and evaluation; Computer vision; Context free grammars; Inference engines; Learning algorithms; Natural language processing systems; Semantics; Software testing; Speech recognition; Deep learning library; Empirical studies; Evaluation results; False positive rates; Gandalf; Generation-based fuzzing; Learning models; Software testings; Task domain; Test inputs; Deep learning
A Closer Look at the Security Risks in the Rust Ecosystem,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183330140&doi=10.1145%2f3624738&partnerID=40&md5=618482e51d0a69bdce274b652b872402,"Rust is an emerging programming language designed for the development of systems software. To facilitate the reuse of Rust code, crates.io, as a central package registry of the Rust ecosystem, hosts thousands of third-party Rust packages. The openness of crates.io enables the growth of the Rust ecosystem but comes with security risks by severe security advisories. Although Rust guarantees a software program to be safe via programming language features and strict compile-time checking, the unsafe keyword in Rust allows developers to bypass compiler safety checks for certain regions of code. Prior studies empirically investigate the memory safety and concurrency bugs in the Rust ecosystem, as well as the usage of unsafe keywords in practice. Nonetheless, the literature lacks a systematic investigation of the security risks in the Rust ecosystem. In this article, we perform a comprehensive investigation into the security risks present in the Rust ecosystem, asking ""what are the characteristics of the vulnerabilities, what are the characteristics of the vulnerable packages, and how are the vulnerabilities fixed in practice?"". To facilitate the study, we first compile a dataset of 433 vulnerabilities, 300 vulnerable code repositories, and 218 vulnerability fix commits in the Rust ecosystem, spanning over 7 years. With the dataset, we characterize the types, life spans, and evolution of the disclosed vulnerabilities. We then characterize the popularity, categorization, and vulnerability density of the vulnerable Rust packages, as well as their versions and code regions affected by the disclosed vulnerabilities. Finally, we characterize the complexity of vulnerability fixes and localities of corresponding code changes, and inspect how practitioners fix vulnerabilities in Rust packages with various localities. We find that memory safety and concurrency issues account for nearly two thirds of the vulnerabilities in the Rust ecosystem. It takes over 2 years for the vulnerabilities to become publicly disclosed, and one-third of the vulnerabilities have no fixes committed before their disclosure. In terms of vulnerability density, we observe a continuous upward trend at the package level over time, but a decreasing trend at the code level since August 2020. In the vulnerable Rust packages, the vulnerable code tends to be localized at the file level, and contains statistically significantly more unsafe functions and blocks than the rest of the code. More popular packages tend to have more vulnerabilities, while the less popular packages suffer from vulnerabilities for more versions. The vulnerability fix commits tend to be localized to a limited number of lines of code. Developers tend to address vulnerable safe functions by adding safe functions or lines to them, vulnerable unsafe blocks by removing them, and vulnerable unsafe functions by modifying unsafe trait implementations. Based on our findings, we discuss implications, provide recommendations for software practitioners, and outline directions for future research. © 2023 Copyright held by the owner/author(s).",ecosystem; empirical study; Rust; security risks; vulnerability,Computer programming languages; Program compilers; Safety engineering; Empirical studies; Localised; Memory concurrency; Memory safety; Reuse; Rust; Security risks; System softwares; Third parties; Vulnerability; Ecosystems
Acrobats and Safety Nets: Problematizing Large-Scale Agile Software Development,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183320413&doi=10.1145%2f3617169&partnerID=40&md5=d290f3b0fa80fdfb0b356cb5c7ac51d9,"Agile development methods have become a standard in the software industry, including in large-scale projects. These methods share a set of underlying assumptions that distinguish them from more traditional plan-driven approaches. In this article, we adopt Alvesson and Sandberg's problematization approach to challenge three key assumptions that are prevalent in the large-scale agile literature: (1) agile and plan-driven methods are mutually exclusive; (2) self-managing and hierarchically organized teams are mutually exclusive; and (3) agile methods can scale through simple linear composition. Using a longitudinal case study of large-scale agile development, we describe a series of trigger events and episodes whereby the agile approach was tailored to address the needs of the large-scale development context, which was very much at odds with these fundamental assumptions. We develop a set of new underlying assumptions which suggest that agile and plan-driven practices are mutually enabling and necessary for coordination and scaling in large-scale agile projects. We develop nine propositions for large-scale agile projects based on these new alternative underlying assumptions. Finally, we summarize our theoretical contribution in a generic process model of continuously adjusting agile and plan-driven practices in order to accommodate process challenges in largescale agile projects.  ©2023 Copyright held by the owner/author(s).",assumptions; case study; Large-scale agile; literature review; multiteam project management; problematization; requirements engineering; software architecture,Software architecture; Software design; Agile software development; Assumption; Case-studies; Large-scale agile; Large-scales; Literature reviews; Multiteam project management; Problematization; Requirement engineering; Safety nets; Project management
Stress Testing Control Loops in Cyber-physical Systems,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183319274&doi=10.1145%2f3624742&partnerID=40&md5=3bbf9660ff3180c7106b540a894203d0,"Cyber-physical Systems (CPSs) are often safety-critical and deployed in uncertain environments. Identifying scenarios where CPSs do not comply with requirements is fundamental but difficult due to the multidisciplinary nature of CPSs. We investigate the testing of control-based CPSs, where control and software engineers develop the software collaboratively. Control engineers make design assumptions during system development to leverage control theory and obtain guarantees on CPS behaviour. In the implemented system, however, such assumptions are not always satisfied, and their falsification can lead the loss of guarantees. We define stress testing of control-based CPSs as generating tests to falsify such design assumptions. We highlight different types of assumptions, focusing on the use of linearised physics models. To generate stress tests falsifying such assumptions, we leverage control theory to qualitatively characterise the input space of a control-based CPS. We propose a novel test parametrisation for control-based CPSs and use it with the input space characterisation to develop a stress testing approach. We evaluate our approach on three case study systems, including a drone, a continuous-current motor (in five configurations), and an aircraft. Our results show the effectiveness of the proposed testing approach in falsifying the design assumptions and highlighting the causes of assumption violations.  © 2023 Copyright held by the owner/author(s).",control theory; Cyber-physical systems; software testing,Control theory; Crime; Cyber Physical System; Embedded systems; Safety engineering; Control engineers; Control loop; Cybe-physical systems; Cyber-physical systems; Input space; Software testings; Stress Testing; System behaviors; System development; Uncertain environments; Software testing
Understanding the Helpfulness of Stale Bot for Pull-Based Development: An Empirical Study of 20 Large Open-Source Projects,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178935981&doi=10.1145%2f3624739&partnerID=40&md5=6733b10da6b498f966c7063ad6cb5a32,"Pull Requests (PRs) that are neither progressed nor resolved clutter the list of PRs, making it difficult for the maintainers to manage and prioritize unresolved PRs. To automatically track, follow up, and close such inactive PRs, Stale bot was introduced by GitHub. Despite its increasing adoption, there are ongoing debates on whether using Stale bot alleviates or exacerbates the problem of inactive PRs. To better understand if and how Stale bot helps projects in their pull-based development workflow, we perform an empirical study of 20 large and popular open source projects. We find that Stale bot can help deal with a backlog of unresolved PRs, as the projects closed more PRs within the first few months of adoption. Moreover, Stale bot can help improve the efficiency of the PR review process as the projects reviewed PRs that ended up merged and resolved PRs that ended up closed faster after the adoption. However, Stale bot can also negatively affect the contributors, as the projects experienced a considerable decrease in their number of active contributors after the adoption. Therefore, relying solely on Stale bot to deal with inactive PRs may lead to decreased community engagement and an increased probability of contributor abandonment.  © 2023 Copyright held by the owner/author(s).",modern code review; open source software; pull request abandonment; pull-based development; social coding platforms; Software development bots,Botnet; Economic and social effects; Open systems; Software design; Code review; Coding platform; Empirical studies; Modern code review; Open source projects; Open-source softwares; Pull request abandonment; Pull-based development; Social coding platform; Software development bot; Open source software
Automated Mapping of Adaptive App GUIs from Phones to TVs,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183331847&doi=10.1145%2f3631968&partnerID=40&md5=8138e0730608537fdae2cc6e6493f951,"With the increasing interconnection of smart devices, users often desire to adopt the same app on quite different devices for identical tasks, such as watching the same movies on both their smartphones and TVs. However, the significant differences in screen size, aspect ratio, and interaction styles make it challenging to adapt Graphical User Interfaces (GUIs) across these devices. Although there are millions of apps available on Google Play, only a few thousand are designed to support smart TV displays. Existing techniques to map a mobile app GUI to a TV either adopt a responsive design, which struggles to bridge the substantial gap between phone and TV, or use mirror apps for improved video display, which requires hardware support and extra engineering efforts. Instead of developing another app for supporting TVs, we propose a semiautomated approach to generate corresponding adaptive TV GUIs, given the phone GUIs as the input. Based on our empirical study of GUI pairs for TVs and phones in existing apps, we synthesize a list of rules for grouping and classifying phone GUIs, converting them to TV GUIs, and generating dynamic TV layouts and source code for the TV display. Our tool is not only beneficial to developers but also to GUI designers, who can further customize the generated GUIs for their TV app development. An evaluation and user study demonstrate the accuracy of our generated GUIs and the usefulness of our tool.  © 2023 Copyright held by the owner/author(s).",adaptive GUI; cross-screen; Graphic user interface,Aspect ratio; Bridges; Telephone sets; Adaptive graphical user interface; Aspect interactions; Aspect-ratio; Automated mapping; Cross-screen; Graphic user interface; Interaction styles; Screen sizes; Smart devices; Smart phones; Graphical user interfaces
Poracle: Testing Patches under Preservation Conditions to Combat the Overfitting Problem of Program Repair,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183323516&doi=10.1145%2f3625293&partnerID=40&md5=1e1ee38dec84ee2f45199e41603df429,"To date, the users of test-driven program repair tools suffer from the overfitting problem; a generated patch may pass all available tests without being correct. In the existing work, users are treated as merely passive consumers of the tests. However, what if they are willing to modify the test to better assess the patches obtained from a repair tool? In this work, we propose a novel semi-automatic patch-classification methodology named Poracle. Our key contributions are three-fold. First, we design a novel lightweight specification method that reuses the existing test. Specifically, the users extend the existing failing test with a preservation condition-the condition under which the patched and pre-patched versions should produce the same output. Second, we develop a fuzzer that performs differential fuzzing with a test containing a preservation condition. Once we find an input that satisfies a specified preservation condition but produces different outputs between the patched and pre-patched versions, we classify the patch as incorrect with high confidence. We show that our approach is more effective than the four state-of-the-art patch classification approaches. Last, we show through a user study that the users find our semi-automatic patch assessment method more effective and preferable than the manual assessment.  © 2023 Copyright held by the owner/author(s).",Automated program repair; overfitting problem; patch classification; patch validation; preservation condition,Automation; Software testing; Automated program repair; Classification methodologies; Over fitting problem; Patch classification; Patch validation; Preservation condition; Repair tools; Reuse; Semi-automatics; Three folds; Repair
"The Good, the Bad, and the Missing: Neural Code Generation for Machine Learning Tasks",2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183318733&doi=10.1145%2f3630009&partnerID=40&md5=37708c30ca418318c100f279f7080b69,"Machine learning (ML) has been increasingly used in a variety of domains, while solving ML programming tasks poses unique challenges due to the fundamental difference in the nature and the construct of general programming tasks, especially for developers who do not haveML backgrounds. Automatic code generation that produces a code snippet from a natural language description can be a promising technique to accelerate ML programming tasks. In recent years, although many deep learning-based neural code generation models have been proposed with high accuracy, the fact that most of them are mainly evaluated on general programming tasks calls into question their effectiveness and usefulness in ML programming tasks. In this article, we set out to investigate the effectiveness of existing neural code generation models on ML programming tasks. For our analysis, we select six state-of-the-art neural code generation models and evaluate their performance on four widely used ML libraries, with newly created 83K pairs of natural-language described ML programming tasks. Our empirical study reveals some good, bad, and missing aspects of neural code generation models on ML tasks, with a few major ones listed below. (Good) Neural code generation models perform significantly better on ML tasks than on non-ML tasks with an average difference of 10.6 points in BLEU-4 scores. (Bad) More than 80% of the generated code is semantically incorrect. (Bad) Code generation models do not have significance in improving developers' completion time. (Good) The generated code can help developers write correct code by providing developers with clues for using correct APIs. (Missing) The observation from our user study reveals the missing aspects of code generation for ML tasks, e.g., decomposing code generation for divide-and-conquer into API sequence identification and API usage generation.  © 2023 Copyright held by the owner/author(s).",empirical analysis; machine learning tasks; Neural code generation,Application programming interfaces (API); Automatic programming; Codes (symbols); Learning systems; Codegeneration; Empirical analysis; Learning programming; Learning tasks; Machine learning task; Machine-learning; Neural code; Neural code generation; On-machines; Programming tasks; Deep learning
"Aspect-level Information Discrepancies across Heterogeneous Vulnerability Reports: Severity, Types and Detection Methods",2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183314893&doi=10.1145%2f3624734&partnerID=40&md5=8d65ee7ef0e9f26fc4dcb943e1aaa9dc,"Vulnerable third-party libraries pose significant threats to software applications that reuse these libraries. At an industry scale of reuse, manual analysis of third-party library vulnerabilities can be easily overwhelmed by the sheer number of vulnerabilities continually collected from diverse sources for thousands of reused libraries. Our study of four large-scale, actively maintained vulnerability databases (NVD, IBM X-Force, ExploitDB, and Openwall) reveals the wide presence of information discrepancies, in terms of seven vulnerability aspects, i.e., product, version, component, vulnerability type, root cause, attack vector, and impact, between the reports for the same vulnerability from heterogeneous sources. It would be beneficial to integrate and cross-validate multi-source vulnerability information, but it demands automatic aspect extraction and aspect discrepancy detection. In this work, we experimented with a wide range of NLP methods to extract named entities (e.g., product) and free-form phrases (e.g., root cause) from textual vulnerability reports and to detect semantically different aspect mentions between the reports. Our experiments confirm the feasibility of applying NLP methods to automate aspect-level vulnerability analysis and identify the need for domain customization of general NLP methods. Based on our findings, we propose a discrepancy-aware, aspect-level vulnerability knowledge graph and a KG-based web portal that integrates diversified vulnerability key aspect information from heterogeneous vulnerability databases. Our conducted user study proves the usefulness of our web portal. Our study opens the door to new types of vulnerability integration and management, such as vulnerability portraits of a product and explainable prediction of silent vulnerabilities.  © 2023 Copyright held by the owner/author(s).",hetergeneous vulnerability reports; information discrepancy; Vulnerability key aspect,Application programs; Computer software reusability; Knowledge graph; Libraries; Portals; Detection methods; Hetergeneous vulnerability report; Information discrepancy; Reuse; Root cause; Software applications; Third parties; Type methods; Vulnerability database; Vulnerability key aspect; Natural language processing systems
LoGenText-Plus: Improving Neural Machine Translation Based Logging Texts Generation with Syntactic Templates,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177844773&doi=10.1145%2f3624740&partnerID=40&md5=4a0757c9933ac905becea259119d8485,"Developers insert logging statements in the source code to collect important runtime information about software systems. The textual descriptions in logging statements (i.e., logging texts) are printed during system executions and exposed to multiple stakeholders including developers, operators, users, and regulatory authorities. Writing proper logging texts is an important but often challenging task for developers. Prior studies find that developers spend significant efforts modifying their logging texts. However, despite extensive research on automated logging suggestions, research on suggesting logging texts rarely exists. To fill this knowledge gap, we first propose LoGenText (initially reported in our conference paper), an automated approach that uses neural machine translation (NMT) models to generate logging texts by translating the related source code into short textual descriptions. LoGenText takes the preceding source code of a logging text as the input and considers other context information, such as the location of the logging statement, to automatically generate the logging text. LoGenText's evaluation on 10 open source projects indicates that the approach is promising for automatic logging text generation and significantly outperforms the state-of-the-art approach. Furthermore, we extend LoGenText to LoGenText-Plus by incorporating the syntactic templates of the logging texts. Different from LoGenText, LoGenText-Plus decomposes the logging text generation process into two stages. LoGenText-Plus first adopts an NMT model to generate the syntactic template of the target logging text. Then LoGenText-Plus feeds the source code and the generated template as the input to another NMT model for logging text generation. We also evaluate LoGenText-Plus on the same 10 projects and observe that it outperforms LoGenText on 9 of them. According to a human evaluation from developers' perspectives, the logging texts generated by LoGenText-Plus have a higher quality than those generated by LoGenText and the prior baseline approach. By manually examining the generated logging texts, we then identify five aspects that can serve as guidance for writing or generating good logging texts. Our work is an important step toward the automated generation of logging statements, which can potentially save developers' efforts and improve the quality of software logging. Our findings shed light on research opportunities that leverage advances in NMT techniques for automated generation and suggestion of logging statements.  © 2023 Copyright held by the owner/author(s).",logging text; neural machine translation; Software logging,Automation; Computer aided language translation; Computer programming languages; Neural machine translation; Open source software; Quality control; Syntactics; Automated generation; Exposed to; Logging text; Machine translation models; Run-time information; Software logging; Software-systems; Source codes; Text generations; Textual description; Computational linguistics
FQN Inference in Partial Code by Prompt-tuned Language Model of Code,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183317159&doi=10.1145%2f3617174&partnerID=40&md5=6510a691f56ac765875381bb6c21aa4b,"Partial code usually involves non-fully-qualified type names (non-FQNs) and undeclared receiving objects. Resolving the FQNs of these non-FQN types and undeclared receiving objects (referred to as type inference) is the prerequisite to effective search and reuse of partial code. Existing dictionary-lookup based methods build a symbolic knowledge base of API names and code contexts, which involve significant compilation overhead and are sensitive to unseen API names and code context variations. In this article, we propose using a prompt-tuned code masked language model (MLM) as a neural knowledge base for type inference, called POME, which is lightweight and has minimal requirements on code compilation. Unlike the existing symbol name and context matching for type inference, POME infers the FQNs syntax and usage knowledge encapsulated in prompt-tuned code MLM through a colze-style fill-in-blank strategy. POME is integrated as a plug-in into web and integrated development environments (IDE) to assist developers in inferring FQNs in the real world. We systematically evaluate POME on a large amount of source code from GitHub and Stack Overflow, and explore its generalization and hybrid capability. The results validate the effectiveness of the POME design and its applicability for partial code type inference, and they can be easily extended to different programming languages (PL). POME can also be used to generate a PL-hybrid type inference model for providing a one-for-all solution. As the first of its kind, our neural type inference method opens the door to many innovative ways of using partial code. © 2023 Association for Computing Machinery. All rights reserved.",code masked language model; fully qualified names; neural knowledge base; Type inference,Computational linguistics; Code compilation; Code masked language model; Dictionary lookup; Fully qualified name; Language model; Name matching; Neural knowledge base; Reuse; Symbolic knowledge; Type inferences; Knowledge based systems
FormatFuzzer: Effective Fuzzing of Binary File Formats,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183332471&doi=10.1145%2f3628157&partnerID=40&md5=9cf2715a773c8c267c154d22f52c186f,"Effective fuzzing of programs that process structured binary inputs, such as multimedia files, is a challenging task, since those programs expect a very specific input format. Existing fuzzers, however, are mostly formatagnostic, which makes them versatile, but also ineffective when a specific format is required. We present FormatFuzzer, a generator for format-specific fuzzers. FormatFuzzer takes as input a binary template (a format specification used by the 010 Editor) and compiles it into C++ code that acts as parser, mutator, and highly efficient generator of inputs conforming to the rules of the language. The resulting format-specific fuzzer can be used as a standalone producer or mutator in black-box settings, where no guidance from the program is available. In addition, by providing mutable decision seeds, it can be easily integrated with arbitrary format-agnostic fuzzers such as AFL to make them format-aware. In our evaluation on complex formats such as MP4 or ZIP, FormatFuzzer showed to be a highly effective producer of valid inputs that also detected previously unknown memory errors in ffmpeg and timidity.  © 2023 Copyright held by the owner/author(s).",binary files; file format specifications; generator-based fuzzing; grammars; parser generators; Structure-aware fuzzing,C++ (programming language); Computational linguistics; Multimedia systems; Binary file formats; Binary files; Binary inputs; File-format specification; Generator-based fuzzing; Grammar; Multimedia files; Parser generators; Structure-aware; Structure-aware fuzzing; Specifications
Search-Based Software Testing Driven by Automatically Generated and Manually Defined Fitness Functions,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183330610&doi=10.1145%2f3624745&partnerID=40&md5=54396bf3405831654003759dff737484,"Search-based software testing (SBST) typically relies on fitness functions to guide the search exploration toward software failures. There are two main techniques to define fitness functions: (a) automated fitness function computation from the specification of the system requirements, and (b) manual fitness function design. Both techniques have advantages. The former uses information from the system requirements to guide the search toward portions of the input domain more likely to contain failures. The latter uses the engineers' domain knowledge. We propose ATheNA, a novel SBST framework that combines fitness functions automatically generated from requirements specifications and those manually defined by engineers. We design and implement ATheNA-S, an instance of ATheNA that targets Simulink ® models. We evaluate ATheNA-S by considering a large set of models from different domains. Our results show that ATheNA-S generates more failure-revealing test cases than existing baseline tools and that the difference between the runtime performance of ATheNA-S and the baseline tools is not statistically significant. We also assess whether ATheNA-S could generate failure-revealing test cases when applied to two representative case studies: one from the automotive domain and one from the medical domain. Our results show that ATheNA-S successfully revealed a requirement violation in our case studies.  ©2023 Copyright held by the owner/author(s).",CPS; falsification; fitness functions; Testing,Computer software selection and evaluation; Requirements engineering; Search engines; Software testing; Automatically generated; Case-studies; CPS; Falsification; Fitness functions; Function computations; Search-based software testing; Software failure; System requirements; Test case; Specifications
Probabilistic Safe WCET Estimation for Weakly Hard Real-time Systems at Design Stages,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183322573&doi=10.1145%2f3617176&partnerID=40&md5=2c33486eaee0b3bda32ac85dbc4eb50a,"Weakly hard real-time systems can, to some degree, tolerate deadline misses, but their schedulability still needs to be analyzed to ensure their quality of service. Such analysis usually occurs at early design stages to provide implementation guidelines to engineers so they can make better design decisions. Estimating worstcase execution times (WCET) is a key input to schedulability analysis. However, early on during system design, estimating WCET values is challenging, and engineers usually determine them as plausible ranges based on their domain knowledge. Our approach aims at finding restricted, safe WCET sub-ranges given a set of ranges initially estimated by experts in the context of weakly hard real-time systems. To this end, we leverage (1) multi-objective search aiming at maximizing the violation of weakly hard constraints to find worst-case scheduling scenarios and (2) polynomial logistic regression to infer safe WCET ranges with a probabilistic interpretation. We evaluated our approach by applying it to an industrial system in the satellite domain and several realistic synthetic systems. The results indicate that our approach significantly outperforms a baseline relying on random search without learning and estimates safeWCET ranges with a high degree of confidence in practical time (< 23 h).  © 2023 Copyright held by the owner/author(s).",meta-heuristic search; weakly hard real-time systems; Worst-case execution time,Domain Knowledge; Heuristic algorithms; Interactive computer systems; Quality of service; Design decisions; Design stage; Early design stages; Meta-heuristic search; Probabilistics; Quality-of-service; Schedulability; Time estimation; Weakly hard real-time systems; Worst-case execution time; Real time systems
Automated Test Suite Generation for Software Product Lines Based onQuality-Diversity Optimization,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183320034&doi=10.1145%2f3628158&partnerID=40&md5=bc76a06a9eb67e2e9bacf9c4aaafc0dc,"A Software Product Line (SPL) is a set of software products that are built from a variability model. Real-world SPLs typically involve a vast number of valid products, making it impossible to individually test each of them. This arises the need for automated test suite generation, which was previously modeled as either a single-objective or a multi-objective optimization problem considering only objective functions. This article provides a completely different mathematical model by exploiting the benefits of Quality-Diversity (QD) optimization that is composed of not only an objective function (e.g., t-wise coverage or test suite diversity) but also a user-defined behavior space (e.g., the space with test suite size as its dimension). We argue that the new model is more suitable and generic than the two alternatives because it provides at a time a large set of diverse (measured in the behavior space) and high-performing solutions that can ease the decision-making process. We apply MAP-Elites, one of the most popular QD algorithms, to solve the model. The results of the evaluation, on both realistic and artificial SPLs, are promising, withMAP-Elites significantly and substantially outperforming both single- and multi-objective approaches, and also several state-of-the-art SPL testing tools. In summary, this article provides a new and promising perspective on the test suite generation for SPLs.  © 2023 Copyright held by the owner/author(s).",automated test suite generation; Quality-Diversity (QD) optimization; Software Product Line,Automation; Behavioral research; Decision making; Multiobjective optimization; Software design; Automated test; Automated test suite generation; Objective functions; Optimisations; Quality-diversity  optimization; Real-world; Single objective; Software Product Line; Software products; Variability modeling; Software testing
Variable-based Fault Localization via Enhanced Decision Tree,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183324747&doi=10.1145%2f3624741&partnerID=40&md5=e42eb13305c6440710fe6dbab34637eb,"Fault localization, aiming at localizing the root cause of the bug under repair, has been a longstanding research topic. Although many approaches have been proposed in past decades, most of the existing studies work at coarse-grained statement or method levels with very limited insights about how to repair the bug (granularity problem), but few studies target the finer-grained fault localization. In this article, we target the granularity problem and propose a novel finer-grained variable-level fault localization technique. Specifically, the basic idea of our approach is that fault-relevant variables may exhibit different values in failed and passed test runs, and variables that have higher discrimination ability have a larger possibility to be the root causes of the failure. Based on this, we propose a program-dependency-enhanced decision tree model to boost the identification of fault-relevant variables via discriminating failed and passed test cases based on the variable values. To evaluate the effectiveness of our approach, we have implemented it in a tool called VarDT and conducted an extensive study over the Defects4J benchmark. The results show that VarDT outperforms the state-of-the-art fault localization approaches with at least 268.4% improvement in terms of bugs located at Top-1, and the average improvement is 351.3%. Besides, to investigate whether our finer-grained fault localization result can further improve the effectiveness of downstream APR techniques, we have adapted VarDT to the application of patch filtering, where we use the variables located by VarDT to filter incorrect patches. The results denote that VarDT outperforms the state-of-the-art PATCH-SIM and BATS by filtering 14.8% and 181.8% more incorrect patches, respectively, demonstrating the effectiveness of our approach. It also provides a new way of thinking for improving automatic program repair techniques.  © 2023 Copyright held by the owner/author(s).",decision tree; Fault localization; program debugging,Program debugging; Repair; Software testing; Coarse-grained; Decision-tree model; Discrimination ability; Fault localization; Fine grained; Localization technique; Research topics; Root cause; State of the art; Test runs; Decision trees
A Survey of Learning-based Automated Program Repair,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181511778&doi=10.1145%2f3631974&partnerID=40&md5=df2290322ec99ed1a7aacdbdf0cf354f,"Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive opensource code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance. In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. © 2023 Association for Computing Machinery. All rights reserved.",AI and software engineering; Automatic program repair; deep learning; neural machine translation,Codes (symbols); Computational linguistics; Computer aided language translation; Deep learning; Learning systems; Neural machine translation; Program debugging; Program translators; Repair; Software testing; AI and software engineering; Automatic program repair; Automatic programs; Bug-fixing; Deep learning; Learn+; Neural-networks; Repair techniques; Software bug; Software development and maintenances; Software design
Testing Abstractions for Cyber-Physical Control Systems,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180262411&doi=10.1145%2f3617170&partnerID=40&md5=9818bba271eefec8fff136b0c2952f82,"Control systems are ubiquitous and often at the core of Cyber-Physical Systems, like cars and aeroplanes. They are implemented as embedded software that interacts in closed loop with the physical world through sensors and actuators. As a consequence, the software cannot just be tested in isolation. To close the loop in a testing environment and root causing failure generated by different parts of the system, executablemodels are used to abstract specific components. Different testing setups can be implemented by abstracting different elements: The most common ones are model-in-the-loop, software-in-the-loop, hardware-in-the-loop, and real-physics-in-the-loop. In this article, we discuss the properties of these setups and the types of faults they can expose. We develop a comprehensive case study using the Crazyflie, a dronewhose software and hardware are open source. We implement all the most common testing setups and ensure the consistent injection of faults in each of them. We inject faults in the control system and we compare with the nominal performance of the non-faulty software. Our results show the specific capabilities of the different setups in exposing faults. Contrary to intuition and previous literature, we show that the setups do not belong to a strict hierarchy, and they are best designed to maximize the differences across them rather than to be as close as possible to reality.  © 2023 Copyright held by the owner/author(s).",Cyber-physical systems; software testing; X-in-the-loop testing,Abstracting; Control systems; Cyber Physical System; Embedded systems; Hardware-in-the-loop simulation; Open source software; Open systems; Closed-loop; Cybe-physical systems; Cyber physicals; Cyber-physical systems; Physical control systems; Physical world; Sensors and actuators; Software testings; Testing environment; X-in-the-loop testing; Software testing
Automated and Efficient Test-Generation for Grid-Based Multiagent Systems: Comparing Random Input Filtering versus Constraint Solving,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183761518&doi=10.1145%2f3624736&partnerID=40&md5=886ad830508893555b5f532d5f1ba487,"Automatic generation of random test inputs is an approach that can alleviate the challenges of manual test case design. However, random test cases may be ineffective in fault detection and increase testing cost, especially in systems where test execution is resource- and time-consuming. To remedy this, the domain knowledge of test engineers can be exploited to select potentially effective test cases. To this end, test selection constraints suggested by domain experts can be utilized either for filtering randomly generated test inputs or for direct generation of inputs using constraint solvers. In this article, we propose a domain specific language (DSL) for formalizing locality-based test selection constraints of autonomous agents and discuss the impact of test selection filters, specified in our DSL, on randomly generated test cases. We study and compare the performance of filtering and constraint solving approaches in generating selective test cases for different test scenario parameters and discuss the role of these parameters in test generation performance. Through our study, we provide criteria for suitability of the random data filtering approach versus the constraint solving one under the varying size and complexity of our testing problem. We formulate the corresponding research questions and answer them by designing and conducting experiments using QuickCheck for random test data generation with filtering and Z3 for constraint solving. Our observations and statistical analysis indicate that applying filters can significantly improve test efficiency of randomly generated test cases. Furthermore, we observe that test scenario parameters affect the performance of the filtering and constraint solving approaches differently. In particular, our results indicate that the two approaches have complementary strengths: random generation and filteringworks best for large agent numbers and long paths, while its performance degrades in the larger grid sizes and more strict constraints. On the contrary, constraint solving has a robust performance for large grid sizes and strict constraints, while its performance degrades with more agents and long paths.  © 2023 Copyright held by the owner/author(s).",autonomous agents; constraint solving; domain specific languages; grid-based systems; multiagent systems; test input filtering; Test input generation; test selection,Autonomous agents; Codes (symbols); Digital subscriber lines; Domain Knowledge; Fault detection; Logic programming; Problem oriented languages; Constraint Solving; Domains specific languages; Grid-based system; Performance; Test case; Test input filtering; Test input generation; Test inputs; Test selection; Multi agent systems
DifferentiableQuantum Programming with Unbounded Loops,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183689879&doi=10.1145%2f3617178&partnerID=40&md5=764bed01dfb44d5ee736d51d50f91b96,"The emergence of variational quantum applications has led to the development of automatic differentiation techniques in quantum computing. Existing work has formulated differentiable quantum programming with bounded loops, providing a framework for scalable gradient calculation by quantum means for training quantum variational applications. However, promising parameterized quantum applications, e.g., quantum walk and unitary implementation, cannot be trained in the existing framework due to the natural involvement of unbounded loops. To fill in the gap, we provide the first differentiable quantum programming framework with unbounded loops, including a newly designed differentiation rule, code transformation, and their correctness proof. Technically, we introduce a randomized estimator for derivatives to deal with the infinite sum in the differentiation of unbounded loops, whose applicability in classical and probabilistic programming is also discussed. We implement our framework with Python and Q# and demonstrate a reasonable sample efficiency. Through extensive case studies, we showcase an exciting application of our framework in automatically identifying close-to-optimal parameters for several parameterized quantum applications.  © 2023 Copyright held by the owner/author(s).",differentiable programming; quantum machine learning; Quantum programming languages; unbounded loops,Cosine transforms; Learning algorithms; Learning systems; Machine learning; Quantum computers; Variational techniques; Automatic differentiations; Differentiable programming; Machine-learning; Parameterized; Quantum applications; Quantum machine learning; Quantum machines; Quantum programming; Quantum programming languages; Unbounded loop; Python
The Human Side of Fuzzing: Challenges Faced by Developers during Fuzzing Activities,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183708529&doi=10.1145%2f3611668&partnerID=40&md5=e091a4844a26dbb3e43f30334df329dc,"Fuzz testing, also known as fuzzing, is a software testing technique aimed at identifying software vulnerabilities. In recent decades, fuzzing has gained increasing popularity in the research community. However, existing studies led by fuzzing expertsmainly focus on improving the coverage and performance of fuzzing techniques. That is, there is still a gap in empirical knowledge regarding fuzzing, especially about the challenges developers face when they adopt fuzzing. Understanding these challenges can provide valuable insights to both practitioners and researchers on how to further improve fuzzing processes and techniques. We conducted a study to understand the challenges encountered by developers during fuzzing. More specifically, we first manually analyzed 829 randomly sampled fuzzing-related GitHub issues and constructed a taxonomy consisting of 39 types of challenges (22 related to the fuzzing process itself, 17 related to using external fuzzing providers). We then surveyed 106 fuzzing practitioners to verify the validity of our taxonomy and collected feedback on how the fuzzing process can be improved. Our taxonomy, accompanied with representative examples and highlighted implications, can serve as a reference point on how to better adopt fuzzing techniques for practitioners, and indicates potential directions researchers can work on toward better fuzzing approaches and practices.  © 2023 Copyright held by the owner/author(s).",empirical software engineering; Fuzzing; software testing,Computer software selection and evaluation; Taxonomies; Empirical knowledge; Empirical Software Engineering; Fuzz Testing; Fuzzing; Performance; Reference points; Research communities; Software testing techniques; Software testings; Software vulnerabilities; Software testing
LaF: Labeling-free Model Selection for Automated Deep Neural Network Reusing,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180545662&doi=10.1145%2f3611666&partnerID=40&md5=34174866f3fa2c779637a75b2a97b622,"Applying deep learning (DL) to science is a new trend in recent years, which leads DL engineering to become an important problem. Although training data preparation, model architecture design, and model training are the normal processes to build DL models, all of them are complex and costly. Therefore, reusing the open-sourced pre-trained model is a practical way to bypass this hurdle for developers. Given a specific task, developers can collect massive pre-trained deep neural networks from public sources for reusing. However, testing the performance (e.g., accuracy and robustness) of multiple deep neural networks (DNNs) and recommending which model should be used is challenging regarding the scarcity of labeled data and the demand for domain expertise. In this article, we propose a labeling-free (LaF) model selection approach to overcome the limitations of labeling efforts for automated model reusing. The main idea is to statistically learn a Bayesian model to infer the models' specialty only based on predicted labels. We evaluate LaF using nine benchmark datasets, including image, text, and source code, and 165 DNNs, considering both the accuracy and robustness of models. The experimental results demonstrate that LaF outperforms the baseline methods by up to 0.74 and 0.53 on Spearman's correlation and Kendall's τ, respectively.  © 2023 Copyright held by the owner/author(s).",Bayesian model; Deep neural network; labeling-free; model selection,Bayesian networks; Architecture designs; Architecture modeling; Bayesian modelling; Data preparation; Free model; Labeling-free; Labelings; Model Selection; Modeling architecture; Training data; Deep neural networks
Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183787634&doi=10.1145%2f3617593&partnerID=40&md5=807b7be49b1bd8867ebf6bf835a41947,"Recent decades have seen the rise of large-scale Deep Neural Networks (DNNs) to achieve human-competitive performance in a variety of AI tasks. Often consisting of hundreds of million, if not hundreds of billion, parameters, these DNNs are too large to be deployed to or efficiently run on resource-constrained devices such as mobile phones or Internet of Things microcontrollers. Systems relying on large-scale DNNs thus have to call the corresponding model over the network, leading to substantial costs for hosting and running the large-scale remote model, costs which are often charged on a per-use basis. In this article, we propose BiSupervised, a novel architecture, where, before relying on a large remote DNN, a system attempts to make a prediction on a small-scale local model. A DNN supervisor monitors said prediction process and identifies easy inputs for which the local prediction can be trusted. For these inputs, the remote model does not have to be invoked, thus saving costs while only marginally impacting the overall system accuracy. Our architecture furthermore foresees a second supervisor to monitor the remote predictions and identify inputs for which not even these can be trusted, allowing to raise an exception or run a fallback strategy instead. We evaluate the cost savings and the ability to detect incorrectly predicted inputs on four diverse case studies: IMDb movie review sentiment classification, GitHub issue triaging, ImageNet image classification, and SQuADv2 free-text question answering. In all four case studies, we find that BiSupervised allows to reduce cost by at least 30% while maintaining similar system-level prediction performance. In two case studies (IMDb and SQuADv2), we find that BiSupervised even achieves a higher system-level accuracy, at reduced cost, compared to a remoteonly model. Furthermore, measurements taken on our setup indicate a large potential of BiSupervised to reduce average prediction latency.  © 2023 Copyright held by the owner/author(s).",Datasets; gaze detection; neural networks; text tagging,Cost reduction; Forecasting; Large datasets; Network architecture; Supervisory personnel; Text processing; Case-studies; Competitive performance; Dataset; Gaze detection; Large-scales; Neural-networks; Novel architecture; Resourceconstrained devices; System levels; Text tagging; Deep neural networks
Seed Selection for Testing Deep Neural Networks,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177803332&doi=10.1145%2f3607190&partnerID=40&md5=d368a4a4759a833663c3be2c56af5136,"Deep learning (DL) has been applied in many applications. Meanwhile, the quality of DL systems is becoming a big concern. To evaluate the quality of DL systems, a number of DL testing techniques have been proposed. To generate test cases, a set of initial seed inputs are required. Existing testing techniques usually construct seed corpus by randomly selecting inputs from training or test dataset. Till now, there is no study on how initial seed inputs affect the performance of DL testing and how to construct an optimal one. To fill this gap, we conduct the first systematic study to evaluate the impact of seed selection strategies on DL testing. Specifically, considering three popular goals of DL testing (i.e., coverage, failure detection, and robustness), we develop five seed selection strategies, including three based on single-objective optimization (SOO) and two based on multi-objective optimization (MOO). We evaluate these strategies on seven testing tools. Our results demonstrate that the selection of initial seed inputs greatly affects the testing performance. SOO-based selection can construct the best seed corpus that can boost DL testing with respect to the specific testing goal. MOO-based selection strategies can construct seed corpus that achieve balanced improvement on multiple objectives.  © 2023 Copyright held by the owner/author(s).",coverage; Deep learning testing; robustness; seed selection,Deep neural networks; Quality control; Statistical tests; Coverage; Deep learning testing; Multi-objectives optimization; Performance; Robustness; Seed-selection; Single objective optimization; Systematic study; Test case; Testing technique; Multiobjective optimization
Snippet Comment Generation Based on Code Context Expansion,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183757913&doi=10.1145%2f3611664&partnerID=40&md5=5551ab60322476935306972887f69376,"Code commenting plays an important role in program comprehension. Automatic comment generation helps improve software maintenance efficiency. The code comments to annotate a method mainly include header comments and snippet comments. The header comment aims to describe the functionality of the entire method, thereby providing a general comment at the beginning of the method. The snippet comment appears at multiple code segments in the body of a method, where a code segment is called a code snippet. Both of them help developers quickly understand code semantics, thereby improving code readability and code maintainability. However, existing automatic comment generation models mainly focus more on header comments, because there are public datasets to validate the performance. By contrast, it is challenging to collect datasets for snippet comments, because it is difficult to determine their scope. Even worse, code snippets are often too short to capture complete syntax and semantic information. To address this challenge, we propose a novel Snippet Comment Generation approach called SCGen. First, we utilize the context of the code snippet to expand the syntax and semantic information. Specifically, 600,243 snippet code-comment pairs are collected from 959 Java projects. Then, we capture variables from code snippets and extract variable-related statements from the context. After that, we devise an algorithm to parse and traverse abstract syntax tree (AST) information of code snippets and corresponding context. Finally, SCGen generates snippet comments after inputting the source code snippet and corresponding AST information into a.  © 2023 Copyright held by the owner/author(s).",code summarization; contextual information; neural machine translation; Snippet comment generation,Computational linguistics; Computer software maintenance; Semantics; Syntactics; Abstract Syntax Trees; Code segments; Code semantics; Code summarization; Contextual information; Maintenance efficiency; Multiple codes; Program comprehension; Semantics Information; Snippet comment generation; Trees (mathematics)
Testing Causality in Scientific Modelling Software,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183781762&doi=10.1145%2f3607184&partnerID=40&md5=d836c80c12a237426931d9d9a372f79d,"From simulating galaxy formation to viral transmission in a pandemic, scientific models play a pivotal role in developing scientific theories and supporting government policy decisions that affect us all. Given these critical applications, a poor modelling assumption or bug could have far-reaching consequences. However, scientific models possess several properties that make them notoriously difficult to test, including a complex input space, long execution times, and non-determinism, rendering existing testing techniques impractical. In fields such as epidemiology, where researchers seek answers to challenging causal questions, a statistical methodology known as Causal inference has addressed similar problems, enabling the inference of causal conclusions from noisy, biased, and sparse data instead of costly experiments. This article introduces the causal testing framework: a framework that uses causal inference techniques to establish causal effects from existing data, enabling users to conduct software testing activities concerning the effect of a change, such as metamorphic testing, a posteriori. We present three case studies covering real-world scientific models, demon- strating how the causal testing framework can infer metamorphic test outcomes from reused, confounded test data to provide an efficient solution for testing scientific modelling software. © 2023 Association for Computing Machinery. All rights reserved.",causal inference; causal testing; Software testing,Public policy; Causal inferences; Causal testing; Critical applications; Galaxy formations; Modeling softwares; Policy decisions; Scientific modeling; Scientific theories; Software testings; Testing framework; Software testing
Faire: Repairing Fairness of Neural Networks via Neuron Condition Synthesis,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183695933&doi=10.1145%2f3617168&partnerID=40&md5=a0a96dc24ca650377c93949f69f6298f,"Deep Neural Networks (DNNs) have achieved tremendous success in many applications, while it has been demonstrated that DNNs can exhibit some undesirable behaviors on concerns such as robustness, privacy, and other trustworthiness issues. Among them, fairness (i.e., non-discrimination) is one important property, especially when they are applied to some sensitive applications (e.g., finance and employment). However, DNNs easily learn spurious correlations between protected attributes (e.g., age, gender, race) and the classification task and develop discriminatory behaviors if the training data is imbalanced. Such discriminatory decisions in sensitive applications would introduce severe social impacts. To expose potential discrimination problems in DNNs before putting them in use, some testing techniques have been proposed to identify the discriminatory instances (i.e., instances that show defined discrimination1). However, how to repair DNNs.  © 2023 Copyright held by the owner/author(s).",Deep learning repair; fairness; individual discrimination; model interpretation,Classification (of information); Repair; Classification tasks; Condition; Deep learning repair; Fairness; Individual discrimination; Learn+; Model interpretations; Neural-networks; Property; Sensitive application; Deep neural networks
Testing RESTful APIs: A Survey,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176332216&doi=10.1145%2f3617175&partnerID=40&md5=078c384836159308f34eb9475a8964db,"In industry, RESTful APIs are widely used to build modern Cloud Applications. Testing them is challenging, because not only do they rely on network communications, but also they deal with external services like databases. Therefore, there has been a large amount of research sprout in recent years on howto automatically verify this kind of web services. In this article, we present a comprehensive review of the current state-ofthe- art in testing RESTful APIs based on the analysis of 92 scientific articles. These articles were gathered by utilizing search queries formulated around the concept of RESTful API testing on seven popular databases. We eliminated irrelevant articles based on our predefined criteria and conducted a snowballing phase to minimize the possibility of missing any relevant paper. This survey categorizes and summarizes the existing scientific work on testing RESTful APIs and discusses the current challenges in the verification of RESTful APIs. This survey clearly shows an increasing interest among researchers in this field, from 2017 onward. However, there are still a lot of open research challenges to overcome.  © 2023 Copyright held by the owner/author(s).",API; fuzzing; literature review; REST; Survey; test case generation; testing; web service,Websites; 'current; API; Cloud applications; Fuzzing; Large amounts; Literature reviews; Network communications; REST; Test case generation; Webs services; Web services
Automatically Detecting Incompatible Android APIs,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177661817&doi=10.1145%2f3624737&partnerID=40&md5=e74497d032dcbe38c96dc5ceab02b038,"Fragmentation is a serious problem in the Android ecosystem, which is mainly caused by the fast evolution of the system itself and the various system customizations. Many efforts have attempted to mitigate its impact via approaches to automatically pinpointing compatibility issues in Android apps. We conducted a literature review to identify all the currently available approaches to addressing this issue. Within the nine identified approaches, the four issue detection tools and one incompatible API harvesting tool could be successfully executed. We tried to reproduce them based on their original datasets and then empirically compared those approaches against common datasets. Our experimental results show that existing tool capabilities are quite distinct with only a small overlap in the compatibility issues being identified. Moreover, these detection tools commonly detect compatibility issues via two separate steps including incompatible APIs gathering and compatibility issues (induced by the incorrect invocations of the identified incompatible APIs) determination. To help developers better identify compatibility issues in Android apps, we developed a new approach, AndroMevol, to systematically spot incompatible APIs as they play a crucial role in issue detection. AndroMevol was able to pinpoint 397,678 incompatible APIs against the full history of the official Android framework and 52 customized Android frameworks spanning five popular device manufacturers. Our approach could enhance the ability of the state-of-the-art detection tools by identifying many more incompatible APIs that may cause compatibility issues in Android apps and foster more advanced approaches to pinpointing all types of compatibility issues.  © 2023 Copyright held by the owner/author(s).",Android frameworks; compatibility issue; Fragmentation,Inspection; Inspection equipment; Android apps; Android framework; API determination; Compatibility issue; Customisation; Detection tools; Fragmentation; Literature reviews; New approaches; State of the art; Android (operating system)
A First Look at On-device Models in iOS Apps,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175137971&doi=10.1145%2f3617177&partnerID=40&md5=5497ea4c9e905836e7e6886c2f51142f,"Powered by the rising popularity of deep learning techniques on smartphones, on-device deep learning models are being used in vital fields such as finance, social media, and driving assistance. Because of the transparency of the Android platform and the on-device models inside, on-device models on Android smartphones have been proven to be extremely vulnerable. However, due to the challenge in accessing and analyzing iOS app files, despite iOS being a mobile platform as popular as Android, there are no relevant works on on-device models in iOS apps. Since the functionalities of the same app on Android and iOS platforms are similar, the same vulnerabilities may exist on both platforms. In this article, we present the first empirical study about on-device models in iOS apps, including their adoption of deep learning frameworks, structure, functionality, and potential security issues. We study why current developers use different on-device models for one app between iOS and Android. We propose a more general attack against white-box models that does not rely on pre-trained models and a new adversarial attack approach based on our findings to target iOS's gray-box on-device models. Our results show the effectiveness of our approaches. Finally, we successfully exploit the vulnerabilities of on-device models to attack real-world iOS apps.  © 2023 Copyright held by the owner/author(s).",adversarial attack; iOS; iPhone; mobile; On-device models,Deep learning; Learning systems; Mobile security; Security of data; Smartphones; Adversarial attack; Device modelling; First look; IOS; Iphone; Learning models; Learning techniques; Mobile; On-device model; Smart phones; Android (operating system)
GraphPrior: Mutation-based Test Input Prioritization for Graph Neural Networks,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180634937&doi=10.1145%2f3607191&partnerID=40&md5=f4d90b9d7cfccde65d6933ccd3534832,"Graph Neural Networks (GNNs) have achieved promising performance in a variety of practical applications. Similar to traditional DNNs, GNNs could exhibit incorrect behavior that may lead to severe consequences, and thus testing is necessary and crucial. However, labeling all the test inputs for GNNs can be costly and time-consuming, especially when dealing with large and complex graphs, which seriously affects the efficiency of GNN testing. Existing studies have focused on test prioritization for DNNs, which aims to identify and prioritize fault-revealing tests (i.e., test inputs that are more likely to be misclassified) to detect system bugs earlier in a limited time. Although some DNN prioritization approaches have been demonstrated effective, there is a significant problem when applying them to GNNs: They do not take into account the connections (edges) between GNN test inputs (nodes), which play a significant role in GNN inference. In general, DNN test inputs are independent of each other, while GNN test inputs are usually represented as a graph with complex relationships between each test. In this article, we propose GraphPrior (GNN-oriented Test Prioritization), a set of approaches to prioritize test inputs specifically for GNNs via mutation analysis. Inspired by mutation testing in traditional software engineering, in which test suites are evaluated based on the mutants they kill, GraphPrior generates mutated models for GNNs and regards test inputs that kill many mutated models as more likely to be misclassified. Then, GraphPrior leverages the mutation results in two ways, killing-based and feature-based methods. When scoring a test input, the killing-based method considers each mutated model equally important, while feature-based methods learn different importance for each mutated model through ranking models. Finally, GraphPrior ranks all the test inputs based on their scores. We conducted an extensive study based on 604 subjects to evaluate GraphPrior on both natural and adversarial test inputs. The results demonstrate that KMGP, the killing-based GraphPrior approach, outperforms the compared approaches in a majority of cases, with an average improvement of 4.76% ∼49.60% in terms of APFD. Furthermore, the feature-based GraphPrior approach, RFGP, performs the best among all the GraphPrior approaches. On adversarial test inputs, RFGP outperforms the compared approaches across different adversarial attacks, with the average improvement of 2.95% ∼46.69%. © 2023 Copyright held by the owner/author(s).",Graph Neural Networks; Labelling; Mutation; Test Input Prioritization,Complex networks; Program debugging; Software testing; Feature-based method; Graph neural networks; Labelings; Mutated models; Mutation; Network test; Prioritization; Test input prioritization; Test inputs; Test prioritization; Graph neural networks
Towards Causal Analysis of Empirical Software Engineering Data: The Impact of Programming Languages on Coding Competitions,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183740792&doi=10.1145%2f3611667&partnerID=40&md5=0c830852e75226d750f96919b6b80676,"There is abundant observational data in the software engineering domain, whereas running large-scale controlled experiments is often practically impossible. Thus, most empirical studies can only report statistical correlations-instead of potentially more insightful and robust causal relations. To support analyzing purely observational data for causal relations and to assess any differences between purely predictive and causal models of the same data, this article discusses some novel techniques based on structural causal models (such as directed acyclic graphs of causal Bayesian networks). Using these techniques, one can rigorously express, and partially validate, causal hypotheses and then use the causal information to guide the construction of a statistical model that captures genuine causal relations-such that correlation does imply causation. We apply these ideas to analyzing public data about programmer performance in Code Jam, a large worldwide coding contest organized by Google every year. Specifically, we look at the impact of different programming languages on a participant's performance in the contest. While the overall effect associated with programming languages is weak compared to other variables-regardless of whether we consider correlational or causal links-we found considerable differences between a purely associational and a causal analysis of the very same data. The takeaway message is that even an imperfect causal analysis of observational data can help answer the salient research questions more precisely and more robustly than with just purely predictive techniques- where genuine causal effects may be confounded.  © 2023 Copyright held by the owner/author(s).",Causality analysis; empirical software engineering; programming contests; statistical analysis,Bayesian networks; Codes (symbols); Computer programming languages; Software engineering; Causal analysis; Causal modeling; Causal relations; Causality analysis; Empirical Software Engineering; Observational data; Performance; Programming contests; Software engineering data; Software engineering domain; Directed graphs
Horus: Accelerating Kernel Fuzzing through Efficient Host-VM Memory Access Procedures,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183702595&doi=10.1145%2f3611665&partnerID=40&md5=8b582441df4f54ae113c00438b892948,"Kernel fuzzing is an effective technique in operating system vulnerability detection. Fuzzers such as Syzkaller and Moonshine frequently pass highly structured data between fuzzer processes in guest virtual machines and manager processes in the host operating system to synchronize fuzzing-relevant data and information. Since the guest virtual machines' and the host operating system's memory spaces are mutually isolated, fuzzers conduct synchronization operations using mechanisms such as Remote Procedure Calls over TCP/IP networks, incurring significant overheads that negatively impact the fuzzer's efficiency and effectiveness in increasing code coverage and finding vulnerabilities. In this paper, we propose Horus, a kernel fuzzing data transfer mechanism that mitigates the aforementioned data transfer overheads. Horus removes host-VM memory isolation and performs data transfers through copying to and from target memory locations in the guest virtual machine. Horus facilitates such efficient transfers through using fixed stub structures in the guest's memory space, whose addresses, along with the guest's RAM contents, are exposed to the host during the fuzzer's initialization process. When conducting transfers, Horus passes highly-structured non-trivial data between the host and guest instances through copying the data directly to and from the stub structures, reducing the overall overhead significantly compared to that of using a network-based approach. We implemented Horus upon state-of-the-art kernel fuzzers Syzkaller, Moonshine and kAFL and evaluated its effectiveness. For Syzkaller and Moonshine, Horus increased their transfer speeds by 84.5% and 85.8% for non-trivial workloads on average and improved their fuzzing throughputs by 31.07% and 30.62%, respectively. Syzkaller and Moonshine both achieved a coverage speedup of 1.6× through using Horus. For kAFL, Horus improved specifically its Redqueen component's execution speeds by 19.4%.  © 2023 Copyright held by the owner/author(s).",Kernel fuzzing; performance enhancement; security; testing,Data transfer; Network security; Random access storage; Data and information; Kernel fuzzing; Memory access; Memory space; Non-trivial; Operating system vulnerability; Performance enhancements; Security; Structured data; Vulnerability detection; Virtual machine
Adonis: Practical and Efficient Control Flow Recovery through OS-level Traces,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177741342&doi=10.1145%2f3607187&partnerID=40&md5=117e8e86dbc76a1f6c7050415bfbc49a,"Control flow recovery is critical to promise the software quality, especially for large-scale software in production environment. However, the efficiency of most current control flow recovery techniques is compromised due to their runtime overheads along with deployment and development costs. To tackle this problem, we propose a novel solution, Adonis, which harnesses Operating System (OS)-level traces, such as dynamic library calls and system call traces, to efficiently and safely recover control flows in practice. Adonis operates in two steps: It first identifies the call-sites of trace entries, and then it executes a pairwise symbolic execution to recover valid execution paths. This technique has several advantages. First, Adonis does not require the insertion of any probes into existing applications, thereby minimizing runtime cost. Second, given that OSlevel traces are hardware-independent, Adonis can be implemented across various hardware configurations without the need for hardware-specific engineering efforts, thus reducing deployment cost. Third, as Adonis is fully automated and does not depend on manually created logs, it circumvents additional development cost. We conducted an evaluation of Adonis on representative desktop applications and real-world IoT applications. Adonis can faithfully recover the control flow with 86.8% recall and 81.7% precision. Compared to the stateof- the-art log-based approach, Adonis can not only cover all the execution paths recovered but also recover 74.9% of statements that cannot be covered. In addition, the runtime cost of Adonis is 18.3× lower than the instrument-based approach; the analysis time and storage cost (indicative of the deployment cost) of Adonis is 50× smaller and 443× smaller than the hardware-based approach, respectively. To facilitate future replication and extension of this work, we have made the code and data publicly available. © 2023 Association for Computing Machinery. All rights reserved.",Control flow recovery; os-level traces; reverse engineering,Computer software selection and evaluation; Cost benefit analysis; Cost engineering; Digital storage; Quality control; Reverse engineering; Control flow recovery; Control-flow; Deployment costs; Development costs; Efficient control; Execution paths; Os-level trace; Practical controls; Runtime costs; System levels; Recovery
StubCoder: Automated Generation and Repair of Stub Code for Mock Objects,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183752948&doi=10.1145%2f3617171&partnerID=40&md5=9b8a52b5ad62315af9baac21f9fb07cb,"Mocking is an essential unit testing technique for isolating the class under test from its dependencies. Developers often leverage mocking frameworks to develop stub code that specifies the behaviors of mock objects. However, developing and maintaining stub code is labor-intensive and error-prone. In this article, we present StubCoder to automatically generate and repair stub code for regression testing. StubCoder implements a novel evolutionary algorithm that synthesizes test-passing stub code guided by the runtime behavior of test cases. We evaluated our proposed approach on 59 test cases from 13 open source projects. Our evaluation results show that StubCoder can effectively generate stub code for incomplete test cases without stub code and repair obsolete test cases with broken stub code.  © 2023 Copyright held by the owner/author(s).",evolutionary computation; genetic programming; mocking; program analysis; Software testing; test generation and repair,Genetic algorithms; Genetic programming; Open source software; Open systems; Repair; Automated generation; Mock objects; Mocking; Program analysis; Software testings; Test case; Test generations; Test repair; Testing technique; Unit testing; Software testing
Programming by Example Made Easy,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183781962&doi=10.1145%2f3607185&partnerID=40&md5=541114a086824c5926653e9c999dc3bf,"Programming by example (PBE) is an emerging programming paradigm that automatically synthesizes programs specified by user-provided input-output examples. Despite the convenience for end-users, implementing PBE tools often requires strong expertise in programming language and synthesis algorithms. Such a level of knowledge is uncommon among software developers. It greatly limits the broad adoption of PBE by the industry. To facilitate the adoption of PBE techniques, we propose a PBE framework called Bee, which leverages an ""entity-action"" model based on relational tables to ease PBE development for a wide but restrained range of domains. Implementing PBE tools with Bee only requires adapting domain-specific data entities and user actions to tables, with no need to design a domain-specific language or an efficient synthesis algorithm. The synthesis algorithm of Bee exploits bidirectional searching and constraint-solving techniques to address the challenge of value computation nested in table transformation. We evaluated Bee's effectiveness on 64 PBE tasks from three different domains and usability with a human study of 12 participants. Evaluation results show that Bee is easier to learn and use than the state-of-the-art PBE framework, and the bidirectional algorithm achieves comparable performance to domain-specifically optimized synthesizers. © 2023 Association for Computing Machinery. All rights reserved.",Program synthesis; programming by example,Computational efficiency; Action modeling; End-users; Input-output; Model-based OPC; Program synthesis; Programming by Example; Programming paradigms; Relational tables; Software developer; Synthesis algorithms; Problem oriented languages
What Constitutes the Deployment and Runtime Configuration System? An Empirical Study on OpenStack Projects,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178660657&doi=10.1145%2f3607186&partnerID=40&md5=f3c3f1b0cd84f315e1111f6f945f4913,"Modern software systems are designed to be deployed in different configured environments (e.g., permissions, virtual resources, network connections) and adapted at runtime to different situations (e.g., memory limits, enabling/disabling features, database credentials). Such a configuration during the deployment and runtime of a software system is implemented via a set of configuration files, which together constitute what we refer to as a ""configuration system."" Recent research efforts investigated the evolution and maintenance of configuration files. However, they merely focused on a limited part of the configuration system (e.g., specific infrastructure configuration files or Dockerfiles), and their results do not generalize to the whole configuration system. To cope with such a limitation, we aim to better capture and understand what files constitute a configuration system. To do so, we leverage an open card sort technique to qualitatively study 1,756 configuration files from OpenStack, a large and widely studied open source software ecosystem. Our investigation reveals the existence of nine types of configuration files, which cover the creation of the infrastructure on top of which OpenStack will be deployed, along with other types of configuration files used to customize OpenStack after its deployment. These configuration files are interconnected while being used at different deployment stages. For instance, we observe specific configuration files used during the deployment stage to create other configuration files that are used in the runtime stage. We also observe that identifying and classifying these types of files is not straightforward, as five out of the nine types can be written in similar programming languages (e.g., Python and Bash) as regular source code files. We also found that the same file extensions (e.g., Yaml) can be used for different configuration types, making it difficult to identify and classify configuration files. Thus, we first leverage a machine learning model to identify configuration from non-configuration files, which achieved a median area under the curve (AUC) of 0.91, a median Brier score of 0.12, a median precision of 0.86, and a median recall of 0.83. Thereafter, we leverage a multi-class classification model to classify configuration files based on the nine configuration types. Our multi-class classification model achieved a median weighted AUC of 0.92, a median Brier score of 0.04, a median weighted precision of 0.84, and a median weighted recall of 0.82. Our analysis also shows that with only 100 labeled configuration and non-configuration files, our model reached a median AUC higher than 0.69. Furthermore, our configuration model requires a minimum of 100 configuration files to reach a median weighted AUC higher than 0.75. © 2023 Association for Computing Machinery. All rights reserved.",Configuration files; files classification; infrastructure-as-code; machine learning models; OpenStack,Classifiers; Codes (symbols); Learning systems; Machine learning; Open source software; Open systems; Platform as a Service (PaaS); Python; Areas under the curves; Configuration files; Configuration system; File classification; Infrastructure-as-code; Machine learning models; Multi-class classification; Openstack; Runtimes; Software-systems; Classification (of information)
Finding Near-optimal Configurations in Colossal Spaces with Statistical Guarantees,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176099656&doi=10.1145%2f3611663&partnerID=40&md5=d42f6ad30815e4c855c3efd341ca8b86,"A Software Product Line (SPL) is a family of similar programs. Each program is defined by a unique set of features, called a configuration, that satisfies all feature constraints. ""What configuration achieves the best performance for a given workload?""is the SPL Optimization (SPLO) challenge. SPLO is daunting: just 80 unconstrained features yield 1024 unique configurations, which equals the estimated number of stars in the universe. We explain (a) how uniform random sampling and random search algorithms solve SPLO more efficiently and accurately than current machine-learned performance models and (b) how to compute statistical guarantees on the quality of a returned configuration; i.e., it is within x% of optimal with y% confidence.  © 2023 Copyright held by the owner/author(s).",configuration optimization; machine learning; order statistics; product spaces; random search; Software product lines; uniform random sampling,Computer software; Optimization; Sampling; Software design; Configuration optimization; Machine-learning; Optimisations; Order-statistics; Product space; Random sampling; Random searches; Software Product Line; Statistical guarantee; Uniform random sampling; Machine learning
On the Caching Schemes to Speed Up Program Reduction,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180554479&doi=10.1145%2f3617172&partnerID=40&md5=df2671e3648c353e99657403ff94b0b2,"Program reduction is a highly practical, widely demanded technique to help debug language tools, such as compilers, interpreters and debuggers. Given a program P that exhibits a property Ψ, conceptually, program reduction iteratively applies various program transformations to generate a vast number of variants from P by deleting certain tokens and returns the minimal variant preserving Ψ as the result. A program reduction process inevitably generates duplicate variants, and the number of them can be significant. Our study reveals that on average 61.8% and 24.3% of the generated variants in two representative program reducers HDD and Perses, respectively, are duplicates. Checking them against Ψ is thus redundant and unnecessary, which wastes time and computation resources. Although it seems that simply caching the generated variants can avoid redundant property tests, such a trivial method is impractical in the real world due to the significant memory footprint. Therefore, a memory-efficient caching scheme for program reduction is in great demand. This study is the first effort to conduct a systematic, extensive analysis of memory-efficient caching schemes for program reduction. We first propose to use two well-known compression methods, ZIP and SHA, to compress the generated variants before they are stored in the cache. Furthermore, our keen understanding on the program reduction process motivates us to propose a novel, domain-specific, both memory and computation-efficient caching scheme, Refreshable Compact Caching (RCC). Our key insight is two-fold: 1 by leveraging the correlation between variants and the original program P, we losslessly encode each variant into an equivalent, compact, canonical representation; 2 periodically, stale cache entries, which will never be accessed, are timely removed to minimize the memory footprint over time. Our extensive evaluation on 31 real-world C compiler bugs demonstrates that caching schemes help avoid issuing redundant queries by 61.8% and 24.3% in HDD and Perses, respectively; correspondingly, the runtime.  © 2023 Copyright held by the owner/author(s).",debugging; delta debugging; Program reduction,C (programming language); Cache memory; Program compilers; Program debugging; Program interpreters; % reductions; Caching scheme; Debugging; Delta debugging; Memory efficient; Memory footprint; Program reduction; Real-world; Reduction process; Speed up; Iterative methods
Framework for SQL Error Message Design: A Data-Driven Approach,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175016261&doi=10.1145%2f3607180&partnerID=40&md5=e386f4e25f5367c056c90d11a527932e,"Software developers use a significant amount of time reading and interpreting error messages. However, error messages have often been based on either anecdotal evidence or expert opinion, disregarding novices, who arguably are the ones who benefit the most from effective error messages. Furthermore, the usability aspects of Structured Query Language (SQL) error messages have not received much scientific attention. In this mixed-methods study, we coded a total of 128 error messages from eight database management systems (DBMS), and using data from 311 participants, analysed 4,796 queries using regression analysis to find out if and how acknowledged error message qualities explain SQL syntax error fixing success rates. Additionally, we performed a conventional content analysis on 1,505 suggestions on how to improve SQL error messages, and based on the analysis, formulated a framework consisting of nine guidelines for SQL error message design. The results indicate that general error message qualities do not necessarily explain query fixing success in the context of SQL syntax errors and that even some novel NewSQL systems fail to account for basic error message design guidelines. The error message design framework and examples of its practical applications shown in this study are applicable in educational contexts as well as by DBMS vendors in understanding novice perspectives in error message design. © 2023 Association for Computing Machinery. All rights reserved.",compiler; database management system; error message; human factor; human-computer interaction; readability; SQL; Structured Query Language; usability,Errors; Human engineering; Information management; Quality control; Query languages; Query processing; Regression analysis; Syntactics; Compiler; Data-driven approach; Error messages; Language syntax; Readability; Software developer; Structured Query Language; Syntax errors; Usability; Human computer interaction
An Interleaving Guided Metamorphic Testing Approach for Concurrent Programs,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183716168&doi=10.1145%2f3607182&partnerID=40&md5=bfea4eaa7f28e87a01a0f46ffe52bea3,"Concurrent programs are normally composed of multiple concurrent threads sharing memory space. These threads are often interleaved, which may lead to some non-determinism in execution results, even for the same program input. This poses huge challenges to the testing of concurrent programs, especially on the test result verification-that is, the prevalent existence of the oracle problem. In this article, we investigate the application of metamorphic testing (MT), a mainstream technique to address the oracle problem, into the testing of concurrent programs. Based on the unique features of interleaved executions in concurrent programming, we propose an extended notion of metamorphic relations, the core part of MT, which are particularly designed for the testing of concurrent programs. A comprehensive testing approach, namely ConMT, is thus developed and a tool is built to automate its implementation on concurrent programs written in Java. Empirical studies have been conducted to evaluate the performance of ConMT, and the experimental results show that in addition to addressing the oracle problem, ConMT outperforms the baseline traditional testing techniques with respect to a higher degree of automation, better bug detection capability, and shorter testing time. It is clear that ConMT can significantly improve the cost-effectiveness for the testing of concurrent programs and thus advances the state of the art in the field. The study also brings novelty into MT, hence promoting the fundamental research of software testing.  © 2020 Copyright held by the owner/author(s).",Concurrent programming; data race; interleaving executions; metamorphic testing,Application programs; Computer programming; Cost effectiveness; Concurrent programming; Concurrent threads; Concurrents programs; Data races; Interleaving execution; Interleavings; Memory space; Metamorphic testing; Oracle problem; Sharing memory; Software testing
Asteria-Pro: Enhancing Deep Learning-based Binary Code Similarity Detection by Incorporating Domain Knowledge,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174796567&doi=10.1145%2f3604611&partnerID=40&md5=d282d76d4191e77b41c0d634d3a3f503,"Widespread code reuse allows vulnerabilities to proliferate among a vast variety of firmware. There is an urgent need to detect these vulnerable codes effectively and efficiently. By measuring code similarities, AIbased binary code similarity detection is applied to detecting vulnerable code at scale. Existing studies have proposed various function features to capture the commonality for similarity detection. Nevertheless, the significant code syntactic variability induced by the diversity of IoT hardware architectures diminishes the accuracy of binary code similarity detection. In our earlier study and the tool Asteria, we adopted a Tree- LSTM network to summarize function semantics as function commonality, and the evaluation result indicates an advanced performance. However, it still has utility concerns due to excessive time costs and inadequate precision while searching for large-scale firmware bugs. To this end, we propose a novel deep learning-enhancement architecture by incorporating domain knowledge-based pre-filtration and re-ranking modules, and we develop a prototype named Asteria-Pro based on Asteria. The pre-filtration module eliminates dissimilar functions, thus reducing the subsequent deep learning-model calculations. The re-ranking module boosts the rankings of vulnerable functions among candidates generated by the deep learning model. Our evaluation indicates that the pre-filtration module cuts the calculation time by 96.9%, and the re-ranking module improves MRR and Recall by 23.71% and 36.4%, respectively. By incorporating these modules, Asteria-Pro outperforms existing state-of-the-art approaches in the bug search task by a significant margin. Furthermore, our evaluation shows that embedding baseline methods with pre-filtration and re-ranking modules significantly improves their precision. We conduct a large-scale real-world firmware bug search, and Asteria-Pro manages to detect 1,482 vulnerable functions with a high precision 91.65%. © 2023 Association for Computing Machinery. All rights reserved.",abstract syntactic tree; Binary code similarity detection; graph neural network; pre-fitering; re-ranking,Domain Knowledge; Firmware; Graph neural networks; Long short-term memory; Network architecture; Network coding; Syntactics; Trees (mathematics); Abstract syntactic tree; Binary code similarity detection; Code similarities; Domain knowledge; Graph neural networks; Large-scales; Pre-fitering; Re-ranking; Similarity detection; Syntactic trees; Semantics
Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks - RCR Report,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183687047&doi=10.1145%2f3617594&partnerID=40&md5=889acd5a07b6cf6d06b0b627303cc9a0,"This is the Replicated Computational Results (RCR) Report for our TOSEM paper ""Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks"", where we propose a novel client-server architecture allowing to leverage the high accuracy of huge neural networks running on remote servers while reducing the economical and latency costs typically coming from using such models. As part of this RCR, we provide a replication package, which allows the full replication of all our results and is specifically designed to facilitate reuse.  © 2023 Copyright held by the owner/author(s).",artifact; Neural networks; replication,Client server computer systems; Deep neural networks; Artifact; Client-server architectures; Computational results; Economical Costs; High-accuracy; Large-scales; Latency costs; Neural-networks; Remote servers; Replication; Supervisory personnel
TopicAns: Topic-informed Architecture for Answer Recommendation on Technical Q&A Site,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183714895&doi=10.1145%2f3607189&partnerID=40&md5=6753c0020321737bfddf87760b9dc5a3,"Technical Q&A sites, such as Stack Over flow and Ask Ubuntu, have been widely utilized by software engineers to seek support for development challenges. However, not all the raised questions get instant feedback, and the retrieved answers can vary in quality. The users can hardly avoid spending much time before solving their problems. Prior studies propose approaches to automatically recommend answers for the question posts on technical Q&A sites. However, the lengthiness and the lack of background knowledge issues limit the performance of answer recommendation on these sites. The irrelevant sentences in the posts may introduce noise to the semantics learning and prevent neural models from capturing the gist of texts. The lexical gap between question and answer posts further misleads current models to make failure recommendations. From this end, we propose a novel neural network named TopicAns for answer selection on technical Q&A sites. TopicAns aims at learning high-quality representations for the posts in Q&A sites with a neural topic model and a pre-trained model. This involves three main steps: (1) generating topic-aware representations of Q&A posts with the neural topic model, (2) incorporating the corpus-level knowledge from the neural topic model to enhance the deep representations generated by the pre-trained language model, and (3) determining the most suitable answer for a given query based on the topic-aware representation and the deep representation. Moreover, we propose a two-stage training technique to improve the stability of our model. We conduct comprehensive experiments on four benchmark datasets to verify our proposed TopicAns's effectiveness. Experiment results suggest that TopicAns consistently outperforms state-of-the-art techniques by over 30% in terms of Precision@1.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",answer recommendation; neural networks; Stack overflow,Learning systems; Query processing; Semantics; Answer recommendation; Background knowledge; Current modeling; Neural modelling; Neural-networks; Over-flow; Performance; Semantic learning; Stack overflow; Topic Modeling; Neural networks
API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177745260&doi=10.1145%2f3607188&partnerID=40&md5=982c1c81720850b83bd4586905a059a5,"Extraction of Application Programming Interfaces (APIs) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., API recommendation). However, existing approaches are rule based and sequence labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, this article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERJE), an API entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, AERJE builds a multi-task architecture that extracts API entities and relations from unstructured text using dynamic prompts. We systematically evaluate AERJE on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that AERJE achieves high accuracy and discrimination ability in API entity-relation joint extraction, even with zero or few-shot fine-tuning.  © 2020 Copyright held by the owner/author(s).",API entity; API relation; dynamic prompt; joint extraction,Application programming interfaces (API); Application programs; Computational linguistics; Data mining; Semantics; Application programming interface entity; Application programming interface relation; Applications programming interfaces; Dynamic prompt; Joint extraction; Language model; Semantic relations; Stack overflow; Unstructured texts; Extraction
A First Look at Dark Mode in Real-world Android Apps,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183777975&doi=10.1145%2f3604607&partnerID=40&md5=c3ab7bb3a6125378203a6653d84873e3,"Android apps often have a ""dark mode""option used in low-light situations, for those who find the conventional color palette problematic, or because of personal preferences. Typically developers add a dark mode option for their apps with different backgrounds, text, and sometimes iconic forms. We wanted to understand the actual provision of this dark mode in real-world Android apps through an empirical study of posts from Stack Overflow and real-world Android app analysis. Using these approaches, we identified the aspects of dark mode that developers implemented as well as the key difficulties they experienced in implementing it. We performed a quantitative analysis using open-coding of more than 300 discussion threads to create a taxonomy regarding the aspects discussed by developers with respect to dark mode in Android. Our quantitative analysis of over 6,000 Android apps highlights which dark mode features are typically provided in Android apps and which aspects developers care about during dark mode design. We also examined four app development support tools to see how well they aid Android app development for dark mode. From our analysis, we distilled some key lessons to guide further research and actions in aiding developers with supporting users who require such assistive features. For example, developers should be aware of the potential risks in using unsuitable dark mode design schema and researchers should take dark mode features into consideration when developing app development support tools.  © 2020 Copyright held by the owner/author(s).",accessibility; Android; dark mode; Graphical user interface,Graphical user interfaces; Accessibility; Android; Android apps; Dark modes; Development support; First look; Low light; Mode features; Real-world; Support tool; Android (operating system)
NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing - RCR Report,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174711105&doi=10.1145%2f3580599&partnerID=40&md5=29328941e05f39103b98f35083654c6b,"We provide artifacts to reproduce the evaluation results of our article: ""NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing"". The provided artifacts can be downloaded from https://zenodo.org/record/7134490. It includes 14 docker containers, several scripts for execution and analysis, one additional proof for the crash results, and six related documents for the running of experiments. We claim for all three badges, i.e., Available, Functional, and Reusable. This report gives instructions on how to reproduce the answers which mainly involve basic operations on the Ubuntu operating system. © 2023 Copyright held by the owner/author(s).",fuzzing; Network service; vulnerability discovery,Basic operation; Evaluation results; Fuzzing; Networks services; Vulnerability discovery; Network security
TestSGD: Interpretable Testing of Neural Networks against Subtle Group Discrimination,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167662405&doi=10.1145%2f3591869&partnerID=40&md5=4ac033c1ed94aa496dc58d27ee1ea981,"Discrimination has been shown in many machine learning applications, which calls for sufficient fairness testing before their deployment in ethic-relevant domains. One widely concerning type of discrimination, testing against group discrimination, mostly hidden, is much less studied, compared with identifying individual discrimination. In this work, we propose TestSGD, an interpretable testing approach that systematically identifies and measures hidden (which we call ""subtle"") group discrimination of a neural network characterized by conditions over combinations of the sensitive attributes. Specifically, given a neural network, TestSGD first automatically generates an interpretable rule set that categorizes the input space into two groups. Alongside, TestSGD also provides an estimated group discrimination score based on sampling the input space to measure the degree of the identified subtle group discrimination, which is guaranteed to be accurate up to an error bound. We evaluate TestSGD on multiple neural network models trained on popular datasets including both structured data and text data. The experiment results show that TestSGD is effective and efficient in identifying and measuring such subtle group discrimination that has never been revealed before. Furthermore, we show that the testing results of TestSGD can be used to mitigate such discrimination through retraining with negligible accuracy drop. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fairness; fairness improvement; fairness testing; machine learning,Learning systems; Condition; Fairness; Fairness improvement; Fairness testing; Input space; Interpretable rules; Machine learning applications; Machine-learning; Neural-networks; Sensitive attribute; Machine learning
Incorporating Signal Awareness in Source Code Modeling: An Application to Vulnerability Detection,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175296686&doi=10.1145%2f3597202&partnerID=40&md5=412506514efa93e5984a2dfc5041bcb4,"AI models of code have made significant progress over the past few years. However, many models are actually not learning task-relevant source code features. Instead, they often fit non-relevant but correlated data, leading to a lack of robustness and generalizability, and limiting the subsequent practical use of such models. In this work, we focus on improving the model quality through signal awareness, i.e., learning the relevant signals in the input for making predictions. We do so by leveraging the heterogeneity of code samples in terms of their signal-to-noise content. We perform an end-to-end exploration of model signal awareness, comprising: (i) uncovering the reliance of AI models of code on task-irrelevant signals, via prediction-preserving input minimization; (ii) improving models' signal awareness by incorporating the notion of code complexity during model training, via curriculum learning; (iii) improving models' signal awareness by generating simplified signal-preserving programs and augmenting them to the training dataset; and (iv) presenting a novel interpretation of the model learning behavior from the perspective of the dataset, using its code complexity distribution. We propose a new metric to measure model signal awareness, Signal-aware Recall, which captures how much of the model's performance is attributable to task-relevant signal learning. Using a software vulnerability detection use-case, our model probing approach uncovers a significant lack of signal awareness in the models, across three different neural network architectures and three datasets. Signal-aware Recall is observed to be in the sub-50s for models with traditional Recall in the high 90s, suggesting that the models are presumably picking up a lot of noise or dataset nuances while learning their logic. With our code-complexity-aware model learning enhancement techniques, we are able to assist the models toward more task-relevant learning, recording up-to 4.8× improvement in model signal awareness. Finally, we employ our model learning introspection approach to uncover the aspects of source code where the model is facing difficulty, and we analyze how our learning enhancement techniques alleviate it. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesMachine learning; curriculum learning; data augmentation; explainability; neural networks; reliability; signal awareness,Codes (symbols); Complex networks; Computer programming languages; Learning systems; Network architecture; Network coding; Neural networks; Signal to noise ratio; Additional key word and phrasesmachine learning; Curriculum learning; Data augmentation; Explainability; Key words; Model signals; Neural-networks; Signal awareness; Source codes; Task relevant; Curricula
Semantic-Enriched Code Knowledge Graph to Reveal Unknowns in Smart Contract Code Reuse,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174717051&doi=10.1145%2f3597206&partnerID=40&md5=d055db42d69c2c4119f4277d8749ea34,"Programmers who work with smart contract development often encounter challenges in reusing code from repositories. This is due to the presence of two unknowns that can lead to non-functional and functional failures. These unknowns are implicit collaborations between functions and subtle differences among similar functions. Current code mining methods can extract syntax and semantic knowledge (known knowledge), but they cannot uncover these unknowns due to a significant gap between the known and the unknown. To address this issue, we formulate knowledge acquisition as a knowledge deduction task and propose an analytic flow that uses the function clone as a bridge to gradually deduce the known knowledge into the problem-solving knowledge that can reveal the unknowns. This flow comprises five methods: clone detection, co-occurrence probability calculation, function usage frequency accumulation, description propagation, and control flow graph annotation. This provides a systematic and coherent approach to knowledge deduction. We then structure all of the knowledge into a semantic-enriched code Knowledge Graph (KG) and integrate this KG into two software engineering tasks: code recommendation and crowd-scaled coding practice checking. As a proof of concept, we apply our approach to 5,140 smart contract files available on Etherscan.io and confirm high accuracy of our KG construction steps. In our experiments, our code KG effectively improved code recommendation accuracy by 6% to 45%, increased diversity by 61% to 102%, and enhanced NDCG by 1% to 21%. Furthermore, compared to traditional analysis tools and the debugging-with-the-crowd method, our KG improved time efficiency by 30 to 380 seconds, vulnerability determination accuracy by 20% to 33%, and vulnerability fixing accuracy by 24% to 40% for novice developers who identified and fixed vulnerable smart contract functions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code knowledge graph; code recommendation; crowd-scale coding practice checking; knowledge deduction; Smart contract,Cloning; Codes (symbols); Flow graphs; Knowledge graph; Semantics; Software engineering; Code knowledge graph; Code recommendation; Code reuse; Crowd-scale coding practice checking; Current codes; Functional failure; Knowledge deduction; Knowledge graphs; Mining methods; Non-functional; Smart contract
CodeEditor: Learning to Edit Source Code with Pre-trained Models,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173041533&doi=10.1145%2f3597207&partnerID=40&md5=c66bc3cff4dc802b27e6f757f0787fa5,"Developers often perform repetitive code editing activities (up to 70%) for various reasons (e.g., code refactoring) during software development. Many deep learning (DL) models have been proposed to automate code editing by learning from the code editing history. Among DL-based models, pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.In this article, we propose a novel pre-training task specialized in code editing and present an effective pre-trained code editing model named CodeEditor. Compared to previous code infilling tasks, our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the corresponding ground truth, to learn edit patterns. We conduct experiments on four code editing datasets and evaluate the pre-trained CodeEditor in three settings (i.e., fine-tuning, few-shot, and zero-shot). (1) In the fine-tuning setting, we train the pre-trained CodeEditor with four datasets and evaluate it on the test data. CodeEditor outperforms the SOTA baselines by 15%, 25.5%, 9.4%, and 26.6% on four datasets. (2) In the few-shot setting, we train the pre-trained CodeEditor with limited data and evaluate it on the test data. CodeEditor substantially performs better than all baselines, even outperforming baselines that are fine-tuned with all data. (3) In the zero-shot setting, we evaluate the pre-trained CodeEditor on the test data without training. CodeEditor correctly edits 1,113 programs, while the SOTA baselines cannot work. The results show that the superiority of our pre-training task and the pre-trained CodeEditor is more effective in automatic code editing. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSource code editing; deep learning; Pre-training,Codes (symbols); Learning systems; Modeling languages; Natural language processing systems; Software design; Zero-shot learning; Additional key word and phrasessource code editing; Automatic codes; Deep learning; Fine tuning; Ground truth; Infilling; Key words; Pre-training; State of the art; Test data; Deep learning
Optimization Techniques for Model Checking Leads-to Properties in a Stratified Way,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166390327&doi=10.1145%2f3604610&partnerID=40&md5=ee131b4cbdcc080ba491a1f66f28cf44,"We devised the L+1-layer divide & conquer approach to leads-to model checking (L+1-DCA2L2MC) and its parallel version, and developed sequential and parallel tools for L+1-DCA2L2MC. In a temporal logic called UNITY, designed by Chandy and Misra, the leads-to temporal connective plays an important role and many case studies have been conducted in UNITY, demonstrating that many systems requirements can be expressed as leads-to properties. Hence, it is worth dedicating to these properties. Counterexample generation is one of the main tasks in the L+1-DCA2L2MC technique that can be optimized to improve its running performance. This article proposes a technique to find all counterexamples at once in model checking with a new model checker. Furthermore, layer configuration selection is essential to make the best use of the L+1-DCA2L2MC technique. This work also proposes an approach to finding a good layer configuration for the technique with an analysis tool. Some experiments are conducted to demonstrate the power and usefulness of the two optimization techniques, respectively. Moreover, our sequential and parallel tools are compared with SPIN and LTSmin model checkers, showing a promising way to mitigate the state space explosion and improve the running performance of model checking when dealing with large state spaces. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLeads-to properties; master-worker model; Maude; parallel model checking; state space explosion,Optimization; Additional key word and phraseslead-to property; Key words; Master/worker models; Maude; Models checking; Optimization techniques; Parallel model checking; Performance; Property; State-space explosion; Model checking
Toward Understanding Deep Learning Framework Bugs,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174693783&doi=10.1145%2f3587155&partnerID=40&md5=ea5f428c3affdba759da6afd8e2bd32f,"DL frameworks are the basis of constructing all DL programs and models, and thus their bugs could lead to the unexpected behaviors of any DL program or model relying on them. Such a wide effect demonstrates the necessity and importance of guaranteeing DL frameworks' quality. Understanding the characteristics of DL framework bugs is a fundamental step for this quality assurance task, facilitating designing effective bug detection and debugging approaches. Hence, in this work, we conduct the most large-scale study on 1,000 bugs from four popular and diverse DL frameworks (i.e., TensorFlow, PyTorch, MXNet, and DL4J). By analyzing the root causes and symptoms of DL framework bugs associated with five components decomposed from DL frameworks, as well as measuring test coverage achieved by three state-of-the-art testing techniques, we obtain 12 major findings for the comprehensive understanding of DL framework bugs and the current status of existing DL framework testing practice, and then provide a series of actionable guidelines for better DL framework bug detection and debugging. Finally, based on the guidelines, we design and implement a prototype DL-framework testing tool, called TenFuzz, which is evaluated to be effective and finds three unknown bugs on the latest TensorFlow framework in a preliminary study, indicating the significance of our guidelines. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bug analysis; Deep learning frameworks; deep learning testing; empirical study,Deep learning; Program debugging; Well testing; Bug analyse; Bug detection; Deep learning framework; Deep learning testing; Empirical studies; Large-scale studies; Learning frameworks; Root cause; State of the art; Test-coverage; Quality assurance
NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174741390&doi=10.1145%2f3580598&partnerID=40&md5=8dced16bffcea6a3a449930e85e09ad4,"As an essential component responsible for communication, network services are security critical, thus, it is vital to find their vulnerabilities. Fuzzing is currently one of the most popular software vulnerability discovery techniques, widely adopted due to its high efficiency and low false positives. However, existing coverage-guided fuzzers mainly aim at stateless local applications, leaving stateful network services underexplored. Recently, some fuzzers targeting network services have been proposed but have certain limitations, for example, insufficient or inaccurate state representation and low testing efficiency.In this article, we propose a new fuzzing solution NSFuzz for stateful network services. We studied typical implementations of network service programs to determine how they represent states and interact with clients. Accordingly, we propose (1) a program variable-based state representation scheme and (2) an efficient interaction synchronization mechanism to improve fuzzing efficiency. We implemented a prototype of NSFuzz, which uses static analysis and annotation application programming interfaces (APIs) to identify synchronization points and state variables within the services. It then achieves fast I/O synchronization and accurate service state tracing to carry out efficient state-aware fuzzing via lightweight compile-time instrumentation. The evaluation results show that compared with other network service fuzzers, including AFLnet and StateAFL, our solution NSFuzz could infer a more accurate state model during fuzzing and improve fuzzing throughput by up to 200×. In addition, NSFuzz could improve code coverage by up to 25% and trigger more crashes in less time. We also performed a fuzzing campaign to find new bugs in the latest version of the target services; 8 zero-day vulnerabilities have been found by NSFuzz. © 2023 Copyright held by the owner/author(s).",fuzzing; Network service; vulnerability discovery,Application programming interfaces (API); Static analysis; Synchronization; Zero-day attack; Communications networks; False positive; Fuzzing; High-low; Higher efficiency; Networks services; Security-critical; Software vulnerabilities; State representation; Vulnerability discovery; Efficiency
Predicting the Change Impact of Resolving Defects by Leveraging the Topics of Issue Reports in Open Source Software Systems,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174709801&doi=10.1145%2f3593802&partnerID=40&md5=618827d32115a09fb06b1fa6f2ee6fb9,"Upon receiving a new issue report, practitioners start by investigating the defect type, the potential fixing effort needed to resolve the defect and the change impact. Moreover, issue reports contain valuable information, such as, the title, description and severity, and researchers leverage the topics of issue reports as a collective metric portraying similar characteristics of a defect. Nonetheless, none of the existing studies leverage the defect topic, i.e., a semantic cluster of defects of the same nature, such as Performance, GUI, and Database, to estimate the change impact that represents the amount of change needed in terms of code churn and the number of files changed. To this end, in this article, we conduct an empirical study on 298,548 issue reports belonging to three large-scale open-source systems, i.e., Mozilla, Apache, and Eclipse, to estimate the change impact in terms of code churn or the number of files changed while leveraging the topics of issue reports. First, we adopt the Embedded Topic Model (ETM), a state-of-the-art topic modelling algorithm, to identify the topics. Second, we investigate the feasibility of predicting the change impact using the identified topics and other information extracted from the issue reports by building eight prediction models that classify issue reports requiring small or large change impact along two dimensions, i.e., the code churn size and the number of files changed. Our results suggest that XGBoost is the best-performing algorithm for predicting the change impact, with an AUC of 0.84, 0.76, and 0.73 for the code churn and 0.82, 0.71, and 0.73 for the number of files changed metric for Mozilla, Apache, and Eclipse, respectively. Our results also demonstrate that the topics of issue reports improve the recall of the prediction model by up to 45%. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",amount of change; change impact analysis; code churn; defect fixing; fixing effort; Issue reports; topics of issue reports,Classification (of information); Forecasting; Open source software; Open systems; Semantics; Amount of change; Change impact analysis; Change impacts; Code churn; Defect fixing; Fixing effort; Issue report; Mozilla; Prediction modelling; Topic of issue report; Defects
XCoS: Explainable Code Search Based on Query Scoping and Knowledge Graph,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170857127&doi=10.1145%2f3593800&partnerID=40&md5=5fa60db54e4cfc75b07d6aff46bb7d8d,"When searching code, developers may express additional constraints (e.g., functional constraints and nonfunctional constraints) on the implementations of desired functionalities in the queries. Existing code search tools treat the queries as a whole and ignore the different implications of different parts of the queries. Moreover, these tools usually return a ranked list of candidate code snippets without any explanations. Therefore, the developers often find it hard to choose the desired results and build confidence on them. In this article, we conduct a developer survey to better understand and address these issues and induct some insights from the survey results. Based on the insights, we propose XCoS, an explainable code search approach based on query scoping and knowledge graph. XCoS extracts a background knowledge graph from general knowledge bases like Wikidata and Wikipedia. Given a code search query, XCoS identifies different parts (i.e., functionalities, functional constraints, nonfunctional constraints) from it and use the expressions of functionalities and functional constraints to search the codebase. It then links both the query and the candidate code snippets to the concepts in the background knowledge graph and generates explanations based on the association paths between these two parts of concepts together with relevant descriptions. XCoS uses an interactive user interface that allows the user to better understand the associations between candidate code snippets and the query from different aspects and choose the desired results. Our evaluation shows that the quality of the extracted background knowledge and the concept linkings in codebase is generally high. Furthermore, the generated explanations are considered complete, concise, and readable, and the approach can help developers find the desired code snippets more accurately and confidently. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Code search; concept; explainability; knowledge,Codes (symbols); Data mining; User interfaces; Background knowledge; Code developers; Code search; Concept; Explainability; Functional constraints; Knowledge; Knowledge graphs; Query scoping; Search-based; Knowledge graph
Fair Enough: Searching for Sufficient Measures of Fairness,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169004751&doi=10.1145%2f3585006&partnerID=40&md5=68fb5326920be83c82c2600712ea6b9a,"Testing machine learning software for ethical bias has become a pressing current concern. In response, recent research has proposed a plethora of new fairness metrics, for example, the dozens of fairness metrics in the IBM AIF360 toolkit. This raises the question: How can any fairness tool satisfy such a diverse range of goals? While we cannot completely simplify the task of fairness testing, we can certainly reduce the problem. This article shows that many of those fairness metrics effectively measure the same thing. Based on experiments using seven real-world datasets, we find that (a) 26 classification metrics can be clustered into seven groups and (b) four dataset metrics can be clustered into three groups. Further, each reduced set may actually predict different things. Hence, it is no longer necessary (or even possible) to satisfy all fairness metrics. In summary, to simplify the fairness testing problem, we recommend the following steps: (1) determine what type of fairness is desirable (and we offer a handful of such types), then (2) lookup those types in our clusters, and then (3) just test for one item per cluster.For the purpose of reproducibility, our scripts and data are available at https://github.com/Repoanon ymous/Fairness_Metrics. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clustering; empirical analysis; fairness metrics; Software fairness; theoretical analysis,Learning systems; Software testing; 'current; Clusterings; Empirical analysis; Fairness metric; Machine learning software; Pressung; Recent researches; Software fairness; Testing machine; Theoretical analyse; Classification (of information)
JavaScript SBST Heuristics to Enable Effective Fuzzing of NodeJS Web APIs,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172667189&doi=10.1145%2f3593801&partnerID=40&md5=9c8de370cc6a659ec0641447c72b9fb4,"JavaScript is one of the most popular programming languages. However, its dynamic nature poses several challenges to automated testing techniques. In this paper, we propose an approach and open-source tool support to enable white-box testing of JavaScript applications using Search-Based Software Testing (SBST) techniques. We provide an automated approach to collect search-based heuristics like the common Branch Distance and to enable Testability Transformations. To empirically evaluate our results, we integrated our technique into the EvoMaster test generation tool, and carried out analyses on the automated system testing of RESTful and GraphQL APIs. Experiments on eight Web APIs running on NodeJS show that our technique leads to significantly better results than existing black-box and grey-box testing tools, in terms of code coverage and fault detection. © 2023 Copyright held by the owner/author(s).",Babel; fuzzer; JavaScript instrumentation; NodeJS; SBST; white-box test generation,Application programs; Automation; Black-box testing; High level languages; Open source software; Babel; Dynamic nature; Fuzzer; Javascript; Javascript instrumentation; Nodejs; Search-based software testing; Test generations; White box; White-box test generation; Fault detection
Revisiting the Identification of the Co-evolution of Production and Test Code,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174814351&doi=10.1145%2f3607183&partnerID=40&md5=23ce22ebc2f1638dd6ea6fa5da61d861,"Many software processes advocate that the test code should co-evolve with the production code. Prior work usually studies such co-evolution based on production-test co-evolution samples mined from software repositories. A production-test co-evolution sample refers to a pair of a test code change and a production code change where the test code change triggers or is triggered by the production code change. The quality of the mined samples is critical to the reliability of research conclusions. Existing studies mined production-test co-evolution samples based on the following assumption: if a test class and its associated production class change together in one commit, or a test class changes immediately after the changes of the associated production class within a short time interval, this change pair should be a production-test co-evolution sample. However, the validity of this assumption has never been investigated.To fill this gap, we present an empirical study, investigating the reasons for test code updates occurring after the associated production code changes, and revealing the pervasive existence of noise in the production-test co-evolution samples identified based on the aforementioned assumption by existing works. We define a taxonomy of such noise, including six categories (i.e., adaptive maintenance, perfective maintenance, corrective maintenance, indirectly related production code update, indirectly related test code update, and other reasons). Guided by the empirical findings, we propose CHOSEN (an identifiCation metHod Of production-teSt co-EvolutioN) based on a two-stage strategy. CHOSEN takes a test code change and its associated production code change as input, aiming to determine whether the production-test change pair is a production-test co-evolution sample. Such identified samples are the basis of or are useful for various downstream tasks. We conduct a series of experiments to evaluate our method. Results show that (1) CHOSEN achieves an AUC of 0.931 and an F1-score of 0.928, significantly outperforming existing identification methods, and (2) CHOSEN can help researchers and practitioners draw more accurate conclusions on studies related to the co-evolution of production and test code. For the task of Just-In-Time (JIT) obsolete test code detection, which can help detect whether a piece of test code should be updated when developers modify the production code, the test set constructed by CHOSEN can help measure the detection method's performance more accurately, only leading to 0.76% of average error compared with ground truth. In addition, the dataset constructed by CHOSEN can be used to train a better obsolete test code detection model, of which the average improvements on accuracy, precision, recall, and F1-score are 12.00%, 17.35%, 8.75%, and 13.50% respectively.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEmpirical software engineering; mining software repositories; software evolution; software testing,Codes (symbols); Computer software maintenance; Corrective maintenance; Just in time production; Additional key word and phrasesempirical software engineering; Co-evolution; Key words; Mining software; Mining software repository; Production test; Software Evolution; Software repositories; Software testings; Test code; Software testing
Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174705020&doi=10.1145%2f3597204&partnerID=40&md5=60642448bf5eb4a62f623a162a4a3d89,"Deep learning (DL) has become a key component of modern software. In the ""big model""era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers' issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers' issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",distributed training; Empirical study; software engineering,Cell proliferation; Deep learning; Large dataset; Learning systems; Life cycle; Program processors; Software testing; Distributed training; Empirical studies; Engineering perspective; Fault symptoms; Large datasets; Learning models; Learning software; Rich features; Software life cycles; Training process; Program debugging
Automatic Core-Developer Identification on GitHub: A Validation Study,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173822353&doi=10.1145%2f3593803&partnerID=40&md5=3f4cd54e0f085750e73f0cc3fbc6512f,"Many open-source software projects are self-organized and do not maintain official lists with information on developer roles. So, knowing which developers take core and maintainer roles is, despite being relevant, often tacit knowledge. We propose a method to automatically identify core developers based on role permissions of privileged events triggered in GitHub issues and pull requests. In an empirical study on 25/GitHub projects, (1) we validate the set of automatically identified core developers with a sample of project-reported developer lists, and (2) we use our set of identified core developers to assess the accuracy of state-of-the-art unsupervised developer classification methods. Our results indicate that the set of core developers, which we extracted from privileged issue events, is sound and the accuracy of state-of-the-art unsupervised classification methods depends mainly on the data source (commit data versus issue data) rather than the network-construction method (directed versus undirected, etc.). In perspective, our results shall guide research and practice to choose appropriate unsupervised classification methods, and our method can help create reliable ground-truth data for training supervised classification methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",developer classification; developer networks; Open-source software projects,Open source software; Open systems; Classification methods; Core developers; Developer classification; Developer network; Open source software projects; Self-organised; State of the art; Tacit knowledge; Unsupervised classification; Validation study; Classification (of information)
UniLoc: Unified Fault Localization of Continuous Integration Failures,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174708654&doi=10.1145%2f3593799&partnerID=40&md5=d091bae691ff5f8d529409aaba514c9c,"Continuous integration (CI) practices encourage developers to frequently integrate code into a shared repository. Each integration is validated by automatic build and testing such that errors are revealed as early as possible. When CI failures or integration errors are reported, existing techniques are insufficient to automatically locate the root causes for two reasons. First, a CI failure may be triggered by faults in source code and/or build scripts, whereas current approaches consider only source code. Second, a tentative integration can fail because of build failures and/or test failures, whereas existing tools focus on test failures only. This article presents UniLoc, the first unified technique to localize faults in both source code and build scripts given a CI failure log, without assuming the failure's location (source code or build scripts) and nature (a test failure or not). Adopting the information retrieval (IR) strategy, UniLoc locates buggy files by treating source code and build scripts as documents to search and by considering build logs as search queries. However, instead of naïvely applying an off-the-shelf IR technique to these software artifacts, for more accurate fault localization, UniLoc applies various domain-specific heuristics to optimize the search queries, search space, and ranking formulas. To evaluate UniLoc, we gathered 700 CI failure fixes in 72 open source projects that are built with Gradle. UniLoc could effectively locate bugs with the average mean reciprocal rank value as 0.49, mean average precision value as 0.36, and normalized discounted cumulative gain value as 0.54. UniLoc outperformed the state-of-the-art IR-based tool BLUiR and Locus. UniLoc has the potential to help developers diagnose root causes for CI failures more accurately and efficiently. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CI failures; Fault localization; information retrieval (IR),Codes (symbols); Computer programming languages; Failure (mechanical); Integration; Integration testing; Open source software; Continuous integration failure; Continuous integrations; Fault localization; Information retrieval; Integration error; Root cause; Search queries; Shared repositories; Source codes; Test failure; Information retrieval
PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168641672&doi=10.1145%2f3591870&partnerID=40&md5=43621e43fe2a26159e8b5f228c95a828,"In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g., 67.1% on ImageNet for 2%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8% on ImageNet). The clean accuracy is the same as vanilla ViT models. Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes by simply changing the masking strategy. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial patch; certified accuracy; deep learning; neural networks; robustness certification; vision transformer,Accident prevention; Convolutional neural networks; Deep neural networks; Electric transformer testing; Learning systems; Adversarial patch; Certified accuracy; Classical neural networks; Deep learning; Exhaustive testing; Neural-networks; Performance; Record-breaking performance; Robustness certification; Vision transformer; Cameras
What Quality Aspects Influence the Adoption of Docker Images?,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174680157&doi=10.1145%2f3603111&partnerID=40&md5=97d727c8f5a364da7c8993f1205e0610,"Docker is a containerization technology that allows developers to ship software applications along with their dependencies in Docker images. Developers can extend existing images using them as base images when writing Dockerfiles. However, a lot of alternative functionally equivalent base images are available. Although many studies define and evaluate quality features that can be extracted from Docker artifacts, the criteria on which developers choose a base image over another remain unclear.In this article, we aim to fill this gap. First, we conduct a literature review through which we define a taxonomy of quality features, identifying two main groups: configuration-related features (i.e., mainly related to the Dockerfile and image build process), and externally observable features (i.e., what the Docker image users can observe). Second, we ran an empirical study considering the developers' preference for 2,441 Docker images in 1,911 open source software projects. We want to understand how the externally observable features influence the developers' preferences, and how they are related to the configuration-related features. Our results pave the way to the definition of a reliable quality measure for Docker artifacts, along with tools that support developers for a quality-aware development of them. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEmpirical software engineering; container virtualization; Docker; software maintenance,Application programs; Computer software maintenance; Open source software; Open systems; Virtualization; Additional key word and phrasesempirical software engineering; Base images; Container virtualization; Docker; Key words; Literature reviews; Quality aspects; Quality features; Software applications; Virtualizations; Containers
"Tiny, Always-on, and Fragile: Bias Propagation through Design Choices in On-device Machine Learning Workflows",2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167808648&doi=10.1145%2f3591867&partnerID=40&md5=2355da2d7ef050ff19337fc42b0d069e,"Billions of distributed, heterogeneous, and resource constrained IoT devices deploy on-device machine learning (ML) for private, fast, and offline inference on personal data. On-device ML is highly context dependent and sensitive to user, usage, hardware, and environment attributes. This sensitivity and the propensity toward bias in ML makes it important to study bias in on-device settings. Our study is one of the first investigations of bias in this emerging domain and lays important foundations for building fairer on-device ML. We apply a software engineering lens, investigating the propagation of bias through design choices in on-device ML workflows. We first identify reliability bias as a source of unfairness and propose a measure to quantify it. We then conduct empirical experiments for a keyword spotting task to show how complex and interacting technical design choices amplify and propagate reliability bias. Our results validate that design choices made during model training, like the sample rate and input feature type, and choices made to optimize models, like light-weight architectures, the pruning learning rate, and pruning sparsity, can result in disparate predictive performance across male and female groups. Based on our findings, we suggest low effort strategies for engineers to mitigate bias in on-device ML. © 2023 Copyright held by the owner/author(s).",audio keyword spotting; Bias; design choices; embedded machine learning; fairness; on-device machine learning; personal data,Software engineering; Audio keyword spotting; Audio keywords; Bias; Design choice; Embedded machine learning; Embedded machines; Fairness; Keyword spotting; Machine-learning; On-device machine learning; Machine learning
An Accurate Identifier Renaming Prediction and Suggestion Approach,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174698076&doi=10.1145%2f3603109&partnerID=40&md5=e87f5c760ab95ddd6745ae628651336a,"Identifiers play an important role in helping developers analyze and comprehend source code. However, many identifiers exist that are inconsistent with the corresponding code conventions or semantic functions, leading to flawed identifiers. Hence, identifiers need to be renamed regularly. Even though researchers have proposed several approaches to identify identifiers that need renaming and further suggest correct identifiers for them, these approaches only focus on a single or a limited number of granularities of identifiers without universally considering all the granularities and suggest a series of sub-tokens for composing identifiers without completely generating new identifiers. In this article, we propose a novel identifier renaming prediction and suggestion approach. Specifically, given a set of training source code, we first extract all the identifiers in multiple granularities. Then, we design and extract five groups of features from identifiers to capture inherent properties of identifiers themselves and the relationships between identifiers and code conventions, as well as other related code entities, enclosing files, and change history. By parsing the change history of identifiers, we can figure out whether specific identifiers have been renamed or not. These identifier features and their renaming history are used to train a Random Forest classifier, which can be further used to predict whether a given new identifier needs to be renamed or not. Subsequently, for the identifiers that need renaming, we extract all the related code entities and their renaming change history. Based on the intuition that identifiers are co-evolved as their relevant code entities with similar patterns and renaming sequences, we could suggest and recommend a series of new identifiers for those identifiers. We conduct extensive experiments to validate our approach in both the Java projects and the Android projects. Experimental results demonstrate that our approach could identify identifiers that need renaming with an average F-measure of more than 89%, which outperforms the state-of-the-art approach by 8.30% in the Java projects and 21.38% in the Android projects. In addition, our approach achieves a Hit@10 of 48.58% and 40.97% in the Java and Android projects in suggesting correct identifiers and outperforms the state-of-the-art approach by 29.62% and 15.75%, respectively. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesIdentifier renaming; code refactoring; mining code repository; source code analysis,Android (operating system); Java programming language; Semantics; Additional key word and phrasesidentifier renaming; Change history; Code conventions; Code re-factoring; Key words; Mining code repository; Mining codes; Source code analysis; Source codes; State-of-the-art approach; Forecasting
Exploring the Impact of Code Clones on Deep Learning Software,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174721879&doi=10.1145%2f3607181&partnerID=40&md5=924b38311f852e7a4103f74fe68f2b20,"Deep learning (DL) is a really active topic in recent years. Code cloning is a common code implementation that could negatively impact software maintenance. For DL software, developers rely heavily on frameworks to implement DL features. Meanwhile, to guarantee efficiency, developers often reuse the steps and configuration settings for building DL models. These may bring code copy-pastes or reuses inducing code clones. However, there is little work exploring code clones' impact on DL software. In this article, we conduct an empirical study and show that: (1) code clones are prevalent in DL projects, about 16.3% of code fragments encounter clones, which is almost twice larger than the traditional projects; (2) 75.6% of DL projects contain co-changed clones, meaning changes are propagated among cloned fragments, which can bring maintenance difficulties; (3) Percentage of the clones and Number of clone lines are associated with the emergence of co-changes; (4) the prevalence of Code clones varies in DL projects with different frameworks, but the difference is not significant; (5) Type 1 co-changed clones often spread over different folders, but Types 2 and 3 co-changed clones mainly occur within the same files or folders; (6) 57.1% of all co-changed clones are involved in bugs. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDeep learning software; co-changed clone; code clone,Codes (symbols); Computer software maintenance; Deep learning; Additional key word and phrasesdeep learning software; Co-changed clone; Code clone; Code cloning; Key words; Learning models; Learning projects; Learning software; Reuse; Software developer; Cloning
Open Problems in Fuzzing RESTful APIs: A Comparison of Tools,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172432949&doi=10.1145%2f3597205&partnerID=40&md5=066cb3b9c053f8357a280f52d6656fa0,"RESTful APIs are a type of web service that are widely used in industry. In the past few years, a lot of effort in the research community has been spent in designing novel techniques to automatically fuzz those APIs to find faults in them. Many real faults were automatically found in a large variety of RESTful APIs. However, usually the analyzed fuzzers treat the APIs as black-box, and no analysis of what is actually covered in these systems is done. Therefore, although these fuzzers are clearly useful for practitioners, we do not know their current limitations and actual effectiveness. Solving this is a necessary step to be able to design better, more efficient, and effective techniques. To address this issue, in this article we compare seven state-of-the-art fuzzers on 18 open source - 1 industrial and 1 artificial - RESTful APIs. We then analyze the source code for which parts of these APIs the fuzzers fail to generate tests. This analysis points to clear limitations of these current fuzzers, listing concrete follow-up challenges for the research community. © 2023 Copyright held by the owner/author(s).",Automated test generation; comparison; fuzzing; REST; SBST,Open source software; Open systems; Automated test generations; Black boxes; Comparison; Current limitation; Fuzzing; Novel techniques; Research communities; REST; SBST; Webs services; Web services
DeepPatch: Maintaining Deep Learning Model Programs to Retain Standard Accuracy with Substantial Robustness Improvement,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174698225&doi=10.1145%2f3604609&partnerID=40&md5=6ee971865e1d0b73ec7592635709c08b,"Maintaining a deep learning (DL) model by making the model substantially more robust through retraining with plenty of adversarial examples of non-trivial perturbation strength often reduces the model's standard accuracy. Many existing model repair or maintenance techniques sacrifice standard accuracy to produce a large gain in robustness or vice versa. This article proposes DeepPatch, a novel technique to maintain filter-intensive DL models. To the best of our knowledge, DeepPatch is the first work to address the challenge of standard accuracy retention while substantially improving the robustness of DL models with plenty of adversarial examples of non-trivial and diverse perturbation strengths. Rather than following the conventional wisdom to generalize all the components of a DL model over the union set of clean and adversarial samples, DeepPatch formulates a novel division of labor method to adaptively activate a subset of its inserted processing units to process individual samples. Its produced model can generate the original or replacement feature maps in each forward pass of the patched model, making the patched model carry an intrinsic property of behaving like the model under maintenance on demand. The overall experimental results show that DeepPatch successfully retains the standard accuracy of all pretrained models while improving the robustness accuracy substantially. However, the models produced by the peer techniques suffer from either large standard accuracy loss or small robustness improvement compared with the models under maintenance, rendering them unsuitable in general to replace the latter. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",accuracy recovery; maintenance; Model testing,Deep learning; Learning systems; Accuracy recovery; Division of labor; Learning models; Model maintenance; Model programs; Model repair; Model testing; Non-trivial; Novel techniques; Perturbation strength; Repair
Pre-implementation Method Name Prediction for Object-oriented Programming,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174671970&doi=10.1145%2f3597203&partnerID=40&md5=0e3a27f07b3e088259984d5a256eebf1,"Method naming is a challenging development task in object-oriented programming. In recent years, several research efforts have been undertaken to provide automated tool support for assisting developers in this task. In general, literature approaches assume the availability of method implementation to infer its name. Methods, however, are usually named before their implementations. In this work, we fill the gap in the literature about method name prediction by developing an approach that predicts the names of all methods to be implemented within a class. Our work considers the class name as the input: The overall intuition is that classes with semantically similar names tend to provide similar functionalities, and hence similar method names. We first conduct a large-scale empirical analysis on 258K+ classes from real-world projects to validate our hypotheses. Then, we propose a hybrid big code-driven approach, Mario, to predict method names based on the class name: We combine a deep learning model with heuristics summarized from code analysis. Extensive experiments on 22K+ classes yielded promising results: compared to the state-of-the-art code2seq model (which leverages method implementation data), our approach achieves comparable results in terms of F-score at token-level prediction; our approach, additionally, outperforms code2seq in prediction at the name level. We further show that our approach significantly outperforms several other baselines. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Method name prediction; naming convention,Codes (symbols); Deep learning; Heuristic methods; Object oriented programming; Automated tool support; Development tasks; Empirical analysis; Large-scales; Method implementations; Method name prediction; Naming convention; Objectoriented programming (OOP); Real world projects; Research efforts; Forecasting
Towards Practical Binary Code Similarity Detection: Vulnerability Verification via Patch Semantic Analysis,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174732311&doi=10.1145%2f3604608&partnerID=40&md5=6556f1fe99f415a31c0b437e62d9757b,"Vulnerability is a major threat to software security. It has been proven that binary code similarity detection approaches are efficient to search for recurring vulnerabilities introduced by code sharing in binary software. However, these approaches suffer from high false-positive rates (FPRs) since they usually take the patched functions as vulnerable, and they usually do not work well when binaries are compiled with different compilation settings. To this end, we propose an approach, named Robin, to confirm recurring vulnerabilities by filtering out patched functions. Robin is powered by a lightweight symbolic execution to solve the set of function inputs that can lead to the vulnerability-related code. It then executes the target functions with the same inputs to capture the vulnerable or patched behaviors for patched function filtration. Experimental results show that Robin achieves high accuracy for patch detection across different compilers and compiler optimization levels respectively on 287 real-world vulnerabilities of 10 different software. Based on accurate patch detection, Robin significantly reduces the false-positive rate of state-of-the-art vulnerability detection tools (by 94.3% on average), making them more practical. Robin additionally detects 12 new potentially vulnerable functions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPatch detection; malicious function input; under constrained symbolic execution; vulnerability detection,Model checking; Program compilers; Semantics; Additional key word and phrasespatch detection; Code similarities; False positive rates; Key words; Malicious function input; Similarity detection; Symbolic execution; Under constrained symbolic execution; Under-constrained; Vulnerability detection; Binary codes
Dependency Update Strategies and Package Characteristics,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174739622&doi=10.1145%2f3603110&partnerID=40&md5=2e85c7537b282d2b8a2d29d3c57ffd90,"Managing project dependencies is a key maintenance issue in software development. Developers need to choose an update strategy that allows them to receive important updates and fixes while protecting them from breaking changes. Semantic Versioning was proposed to address this dilemma, but many have opted for more restrictive or permissive alternatives. This empirical study explores the association between package characteristics and the dependency update strategy selected by its dependents to understand how developers select and change their update strategies. We study over 112,000 Node Package Manager (npm) packages and use 19 characteristics to build a prediction model that identifies the common dependency update strategy for each package. Our model achieves a minimum improvement of 72% over the baselines and is much better aligned with community decisions than the npm default strategy. We investigate how different package characteristics can influence the predicted update strategy and find that dependent count, age, and release status to be the highest influencing features. We complement the work with qualitative analyses of 160 packages to investigate the evolution of update strategies. While the common update strategy remains consistent for many packages, certain events such as the release of the 1.0.0 version or breaking changes influence the selected update strategy over time. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDependency update strategy; dependency management; npm; software ecosystems,Software design; Additional key word and phrasesdependency update strategy; Breakings; Dependency management; Empirical studies; Key words; Managing programmes; Node package manager; Prediction modelling; Software ecosystems; Versioning; Semantics
A Systematic Review of Automated Query Reformulations in Source Code Search,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174683041&doi=10.1145%2f3607179&partnerID=40&md5=ba772d4511138bd87abfb652641d2543,"Fixing software bugs and adding new features are two of the major maintenance tasks. Software bugs and features are reported as change requests. Developers consult these requests and often choose a few keywords from them as an ad hoc query. Then they execute the query with a search engine to find the exact locations within software code that need to be changed. Unfortunately, even experienced developers often fail to choose appropriate queries, which leads to costly trials and errors during a code search. Over the years, many studies have attempted to reformulate the ad hoc queries from developers to support them. In this systematic literature review, we carefully select 70 primary studies on query reformulations from 2,970 candidate studies, perform an in-depth qualitative analysis (e.g., Grounded Theory), and then answer seven research questions with major findings. First, to date, eight major methodologies (e.g., term weighting, term co-occurrence analysis, thesaurus lookup) have been adopted to reformulate queries. Second, the existing studies suffer from several major limitations (e.g., lack of generalizability, the vocabulary mismatch problem, subjective bias) that might prevent their wide adoption. Finally, we discuss the best practices and future opportunities to advance the state of research in search query reformulations. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesConcept location; automated query reformulation; bug localization; Internet-scale code search; machine learning; query quality analysis; systematic literature review; term weighting,Program debugging; Quality control; Search engines; Additional key word and phrasesconcept location; Automated query reformulation; Bug localizations; Code search; Internet-scale code search; Key words; Machine-learning; Query quality analyse; Query reformulation; Systematic literature review; Term weighting; Machine learning
An Empirical Study on GitHub Pull Requests' Reactions,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174696500&doi=10.1145%2f3597208&partnerID=40&md5=213ab4e7ef0db3952162df79db0bfa1c,"The pull request mechanism is commonly used to propose source code modifications and get feedback from the community before merging them into a software repository. On GitHub, practitioners can provide feedback on a pull request by either commenting on the pull request or simply reacting to it using a set of pre-defined GitHub reactions, i.e., ""Thumbs-up"", ""Laugh"", ""Hooray"", ""Heart"", ""Rocket"", ""Thumbs-down"", ""Confused"", and ""Eyes"". While a large number of prior studies investigated how to improve different software engineering activities (e.g., code review and integration) by investigating the feedback on pull requests, they focused only on pull requests' comments as a source of feedback. However, the GitHub reactions, according to our preliminary study, contain feedback that is not manifested within the comments of pull requests. In fact, our preliminary analysis of six popular projects shows that a median of 100% of the practitioners who reacted to a pull request did not leave any comment suggesting that reactions can be a unique source of feedback to further improve the code review and integration process.To help future studies better leverage reactions as a feedback mechanism, we conduct an empirical study to understand the usage of GitHub reactions and understand their promises and limitations. We investigate in this article how reactions are used, when and who use them on what types of pull requests, and for what purposes. Our study considers a quantitative analysis on a set of 380 k reactions on 63 k pull requests of six popular open-source projects on GitHub and three qualitative analyses on a total number of 989 reactions from the same six projects. We find that the most common used GitHub reactions are the positive ones (i.e., ""Thumbs-up"", ""Hooray"", ""Heart"", ""Rocket"", and ""Laugh""). We observe that reactors use positive reactions to express positive attitude (e.g., approval, appreciation, and excitement) on the proposed changes in pull requests. A median of just 1.95% of the used reactions are negative ones, which are used by reactors who disagree with the proposed changes for six reasons, such as feature modifications that might have more downsides than upsides or the use of the wrong approach to address certain problems. Most (a median of 78.40%) reactions on a pull request come before the closing of the corresponding pull requests. Interestingly, we observe that non-contributors (i.e., outsiders who potentially are the ""end-users""of the software) are also active on reacting to pull requests. On top of that, we observe that core contributors, peripheral contributors, casual contributors and outsiders have different behaviors when reacting to pull requests. For instance, most core contributors react in the early stages of a pull request, while peripheral contributors, casual contributors and outsiders react around the closing time or, in some cases, after a pull request is merged. Contributors tend to react to the pull request's source code, while outsiders are more concerned about the impact of the pull request on the end-user experience. Our findings shed light on common patterns of GitHub reactions usage on pull requests and provide taxonomies about the intention of reactors, which can inspire future studies better leverage pull requests' reactions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",feedback; GitHub reactions; pull requests; software collaboration,Codes (symbols); Open systems; Rockets; Code review; Empirical studies; Engineering activities; Github reaction; Preliminary analysis; Pull request; Review process; Software collaboration; Software repositories; Source code modification; Open source software
COMET: Coverage-guided Model Generation For Deep Learning Library Testing,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168772146&doi=10.1145%2f3583566&partnerID=40&md5=baade4460deef2ab4b32932f2c2140e3,"Recent deep learning (DL) applications are mostly built on top of DL libraries. The quality assurance of these libraries is critical to the dependable deployment of DL applications. Techniques have been proposed to generate various DL models and apply them to test these libraries. However, their test effectiveness is constrained by the diversity of layer API calls in their generated DL models. Our study reveals that these techniques can cover at most 34.1% layer inputs, 25.9% layer parameter values, and 15.6% layer sequences. As a result, we find that many bugs arising from specific layer API calls (i.e., specific layer inputs, parameter values, or layer sequences) can be missed by existing techniques.Because of this limitation, we propose COMET to effectively generate DL models with diverse layer API calls for DL library testing. COMET: (1) designs a set of mutation operators and a coverage-based search algorithm to diversify layer inputs, layer parameter values, and layer sequences in DL models. (2) proposes a model synthesis method to boost the test efficiency without compromising the layer API call diversity. Our evaluation result shows that COMET outperforms baselines by covering twice as many layer inputs (69.7% vs. 34.1%), layer parameter values (50.2% vs. 25.9%), and layer sequences (39.0% vs. 15.6%) as those by the state-of-the-art. Moreover, COMET covers 3.4% more library branches than those by existing techniques. Finally, COMET detects 32 new bugs in the latest version of eight popular DL libraries, including TensorFlow and MXNet, with 21 of them confirmed by DL library developers and seven of those confirmed bugs have been fixed by developers. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning testing; library testing; model diversity; model generation,Deep learning; Learning systems; Libraries; Parameter estimation; API calls; Deep learning testing; Input parameter; Layer parameters; Layer sequence; Learning models; Library testing; Model diversity; Model generation; Test effectiveness; Quality assurance
A Hypothesis Testing-based Framework for Software Cross-modal Retrieval in Heterogeneous Semantic Spaces,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168757821&doi=10.1145%2f3591868&partnerID=40&md5=79246364384c0359c24385dcf47df599,"Software cross-modal retrieval is a popular yet challenging direction, such as bug localization and code search. Previous studies generally map natural language texts and codes into a homogeneous semantic space for similarity measurement. However, it is not easy to accurately capture their similar semantics in a homogeneous semantic space due to the semantic gap. Therefore, we propose to map the multi-modal data into heterogeneous semantic spaces to capture their unique semantics. Specifically, we propose a novel software cross-modal retrieval framework named Deep Hypothesis Testing (DeepHT). In DeepHT, to capture the unique semantics of the code's control flow structure, all control flow paths (CFPs) in the control flow graph are mapped to a CFP sample set in the sample space. Meanwhile, the text is mapped to a CFP correlation distribution in the distribution space to model its correlation with different CFPs. The matching score is calculated according to how well the sample set obeys the distribution using hypothesis testing. The experimental results on two text-to-code retrieval tasks (i.e., bug localization and code search) and two code-to-text retrieval tasks (i.e., vulnerability knowledge retrieval and historical patch retrieval) show that DeepHT outperforms the baseline methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; hypothesis testing; Software cross-modal retrieval,Codes (symbols); Data flow analysis; Deep learning; Flow graphs; Information retrieval; Modal analysis; Software testing; Well testing; Bug localizations; Code search; Control-flow; Cross-modal; Deep learning; Flow path; Hypothesis testing; Sample sets; Semantic Space; Software cross-modal retrieval; Semantics
DatAFLow: Toward a Data-flow-guided Fuzzer,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168759447&doi=10.1145%2f3587159&partnerID=40&md5=65e301e96b9585b24abb7f87a4420330,"This Replicating Computational Report (RCR) describes (a) our datAFLow fuzzer and (b) how to replicate the results in ""datAFLow: Toward a Data-Flow-Guided Fuzzer.""Our primary artifact is the datAFLow fuzzer. Unlike traditional coverage-guided greybox fuzzers - which use control-flow coverage to drive program exploration - datAFLow uses data-flow coverage to drive exploration. This is achieved through a set of LLVM-based analyses and transformations. In addition to datAFLow, we also provide a set of tools, scripts, and patches for (a) statically analyzing data flows in a target program, (b) compiling a target program with the datAFLow instrumentation, (c) evaluating datAFLow on the Magma benchmark suite, and (d) evaluating datAFLow on the DDFuzz dataset. datAFLow is available at https://github.com/HexHive/datAFLow.  © 2023 Copyright held by the owner/author(s).",coverage; data flow; Fuzzing,C (programming language); Data flow analysis; Data transfer; Benchmark suites; Control-flow; Coverage; Dataflow; Fuzzing; Grey-box; Digital storage
ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168682506&doi=10.1145%2f3585005&partnerID=40&md5=0014f7a9c92cb5fd02571b5e168c3b94,"Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level).We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e., block localization, where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing. Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neurons' status and weights' gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair, and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art DNN repair techniques.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; DNN repair; neural architecture search,Multilayer neural networks; Network architecture; Network layers; Repair; Spectrum analysis; Applications domains; Deep learning; Deep neural network repair; Industrial tasks; Localisation; Network repairs; Neural architecture search; Neural architectures; Overfitting; Real-world; Deep neural networks
Actor-Driven Decomposition of Microservices through Multi-level Scalability Assessment,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162191840&doi=10.1145%2f3583563&partnerID=40&md5=d86e9e5bcf5b5bd90772cb75b3666aca,"The microservices architectural style has gained widespread acceptance. However, designing applications according to this style is still challenging. Common difficulties concern finding clear boundaries that guide decomposition while ensuring performance and scalability. With the aim of providing software architects and engineers with a systematic methodology, we introduce a novel actor-driven decomposition strategy to complement the domain-driven design and overcome some of its limitations by reaching a finer modularization yet enforcing performance and scalability improvements. The methodology uses a multi-level scalability assessment framework that supports decision-making over iterative steps. At each iteration, architecture alternatives are quantitatively evaluated at multiple granularity levels. The assessment helps architects to understand the extent to which architecture alternatives increase or decrease performance and scalability. We applied the methodology to drive further decomposition of the core microservices of a real data-intensive smart mobility application and an existing open-source benchmark in the e-commerce domain. The results of an in-depth evaluation show that the approach can effectively support engineers in (i) decomposing monoliths or coarse-grained microservices into more scalable microservices and (ii) comparing among alternative architectures to guide decision-making for their deployment in modern infrastructures that orchestrate lightweight virtualized execution units.  © 2023 Copyright held by the owner/author(s).",architectural patterns; decomposition process; Microservices; performance analysis; scalability assessment,Benchmarking; Decision making; Digital storage; Iterative methods; Modular construction; Open source software; Software architecture; Architectural pattern; Architectural style; Decisions makings; Decomposition process; Microservice; Multilevels; Performance and scalabilities; Performances analysis; Scalability assessment; Software architects; Scalability
Finding Deviated Behaviors of the Compressed DNN Models for Image Classifications,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167681382&doi=10.1145%2f3583564&partnerID=40&md5=c664540c14d36afae55f00b98f12a2c5,"Model compression can significantly reduce the sizes of deep neural network (DNN) models and thus facilitate the dissemination of sophisticated, sizable DNN models, especially for deployment on mobile or embedded devices. However, the prediction results of compressed models may deviate from those of their original models. To help developers thoroughly understand the impact of model compression, it is essential to test these models to find those deviated behaviors before dissemination. However, this is a non-trivial task, because the architectures and gradients of compressed models are usually not available.To this end, we propose Dflare, a novel, search-based, black-box testing technique to automatically find triggering inputs that result in deviated behaviors in image classification tasks. Dflare iteratively applies a series of mutation operations to a given seed image until a triggering input is found. For better efficacy and efficiency, Dflare models the search problem as Markov Chains and leverages the Metropolis-Hasting algorithm to guide the selection of mutation operators in each iteration. Further, Dflare utilizes a novel fitness function to prioritize the mutated inputs that either cause large differences between two models' outputs or trigger previously unobserved models' probability vectors. We evaluated Dflare on 21 compressed models for image classification tasks with three datasets. The results show that Dflare not only constantly outperforms the baseline in terms of efficacy but also significantly improves the efficiency: Dflare is 17.84×-446.06× as fast as the baseline in terms of time; the number of queries required by Dflare to find one triggering input is only 0.186-1.937% of those issued by the baseline. We also demonstrated that the triggering inputs found by Dflare can be used to repair up to 48.48% deviated behaviors in image classification tasks and further decrease the effectiveness of Dflare on the repaired models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",image classification models; model compression; Model dissemination; neural networks,Black-box testing; Classification (of information); Deep neural networks; Efficiency; Iterative methods; Markov processes; Neural network models; Classification models; Classification tasks; Embedded device; Image classification model; Images classification; Model compression; Model dissemination; Neural network model; Neural-networks; Original model; Image classification
A Survey on Automated Driving System Testing: Landscapes and Trends,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167536644&doi=10.1145%2f3579642&partnerID=40&md5=b05c32e0d8b2a6bb541bb29a64592545,"Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ADS testing; module-level testing; system security; system-level testing,Deceleration; Automated driving system testing; Automated driving systems; Different domains; Module-level testing; Perception planning; Planning and control; System level testing; System security; System testing; Testing technique; Software testing
The Influence of Human Aspects on Requirements Engineering-related Activities: Software Practitioners' Perspective,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147411017&doi=10.1145%2f3546943&partnerID=40&md5=b83248d90930f7c817746777785d3ceb,"Requirements Engineering (RE)-related activities require high collaboration between various roles in software engineering (SE), such as requirements engineers, stakeholders, developers, and so on. Their demographics, views, understanding of technologies, working styles, communication and collaboration capabilities make RE highly human-dependent. Identifying how ""human aspects""- such as motivation, domain knowledge, communication skills, personality, emotions, culture, and so on - might impact RE-related activities would help us improve RE and SE in general. This study aims at better understanding current industry perspectives on the influence of human aspects on RE-related activities, specifically focusing on motivation and personality, by targeting software practitioners involved in RE-related activities. Our findings indicate that software practitioners consider motivation, domain knowledge, attitude, communication skills and personality as highly important human aspects when involved in RE-related activities. A set of factors were identified as software practitioners' key motivational factors when involved in RE-related activities, along with important personality characteristics to have when involved in RE. We also identified factors that made individuals less effective when involved in RE-related activities and obtained some feedback on measuring individuals' performance when involved in RE. The findings from our study suggest various areas needing more investigation, and we summarise a set of key recommendations for further research.  © 2023 Association for Computing Machinery.",Human aspects; requirements engineering; software engineering,Domain Knowledge; Motivation; Software engineering; Collaboration capabilities; Communication and collaborations; Communication capabilities; Communication skills; Domain knowledge; Human aspects; Knowledge communication; Requirement engineering; Software practitioners; Working styles; Requirements engineering
Assessing the Early Bird Heuristic (for Predicting Project Quality),2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168758982&doi=10.1145%2f3583565&partnerID=40&md5=537c22c18a377dbedd9a29ba81e8acd6,"Before researchers rush to reason across all available data or try complex methods, perhaps it is prudent to first check for simpler alternatives. Specifically, if the historical data has the most information in some small region, then perhaps a model learned from that region would suffice for the rest of the project.To support this claim, we offer a case study with 240 projects, where we find that the information in those projects ""clumps""towards the earliest parts of the project. A quality prediction model learned from just the first 150 commits works as well, or better than state-of-the-art alternatives. Using just this ""early bird""data, we can build models very quickly and very early in the project life cycle. Moreover, using this early bird method, we have shown that a simple model (with just a few features) generalizes to hundreds of projects.Based on this experience, we doubt that prior work on generalizing quality models may have needlessly complicated an inherently simple process. Further, prior work that focused on later-life cycle data needs to be revisited, since their conclusions were drawn from relatively uninformative regions.Replication note: All our data and scripts are available here: https://github.com/snaraya7/early-bird.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data-lite; defects; early; Quality prediction,Birds; Forecasting; Case-studies; Complex methods; Data-lite; Early; Historical data; Project quality; Quality prediction; Quality prediction models; Simple++; Small region; Life cycle
Rise of the Planet of Serverless Computing: A Systematic Review,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150002365&doi=10.1145%2f3579643&partnerID=40&md5=a5a2d7985c6aceab5fc59ec6f3c9cf0e,"Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This article provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this article covers 164 articles on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, and so on. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",literature view; Serverless computing,Computation theory; Program debugging; Application logic; Cloud-computing; Computing paradigm; Error prones; Infrastructure managements; Literature view; Research efforts; Serverless computing; Software applications; Systematic Review; Application programs
SEAL: Integrating Program Analysis and Repository Mining,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168771367&doi=10.1145%2f3585008&partnerID=40&md5=2953f7aa73119624a5dcc36628dd6e5e,"Software projects are complex technical and organizational systems involving large numbers of artifacts and developers. To understand and tame software complexity, a wide variety of program analysis techniques have been developed for bug detection, program comprehension, verification, and more. At the same time, repository mining techniques aim at obtaining insights into the inner socio-technical workings of software projects at a larger scale. While both program analysis and repository mining have been successful on their own, they are largely isolated, which leaves considerable potential for synergies untapped. We present SEAL, the first integrated approach that combines low-level program analysis with high-level repository information. SEAL maps repository information, mined from the development history of a project, onto a low-level intermediate program representation, making it available for state-of-the-art program analysis. SEAL's integrated approach allows us to efficiently address software engineering problems that span multiple levels of abstraction, from low-level data flow to high-level organizational information. To demonstrate its merits and practicality, we use SEAL to determine which code changes modify central parts of a given software project, how authors interact (indirectly) with each other through code, and we demonstrate that putting static analysis' results into a socio-technical context improves their expressiveness and interpretability.  © 2023 Copyright held by the owner/author(s).",socio-technical software analytics; software repository mining; Static program analysis,Codes (symbols); Integrated control; Verification; Complex technical systems; Integrated approach; Organizational system; Program analysis; Repository mining; Socio-technical software analytic; Sociotechnical; Software project; Software repository mining; Static program analysis; Static analysis
FaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149357562&doi=10.1145%2f3585007&partnerID=40&md5=453e9e6d8d4c5fc471ca5c44ed5254f4,"Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency.In this article, we propose an approach, namely, FaaSLight, to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform- and language-agnostic. In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95%, 28.78% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05% (19.21% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25× improvement in reducing the average total response latency.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cold start; optional function elimination; performance optimization; Serverless computing,Loading; Application level; Code-loading; Cold-start; General applications; Latency optimizations; Optional function elimination; Performance optimizations; Serverless computing; Services applications; Total response; High level languages
A Comparative Study on Method Comment and Inline Comment,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168775324&doi=10.1145%2f3582570&partnerID=40&md5=67d0de01c4878ab8c2572cc245178b82,"Code comments are one of the important documents to help developers review and comprehend source code. In recent studies, researchers have proposed many deep learning models to generate the method header comments (i.e., method comment), which have achieved encouraging results. The comments in the method, which is called inline comment, are also important for program comprehension. Unfortunately, they have not received enough attention in automatic generation when comparing with the method comments. In this paper, we compare and analyze the similarities and differences between the method comments and the inline comments. By applying the existing models of generating method comments to the inline comment generation, we find that these existing models perform worse on the task of inline comment generation. We then further explore the possible reasons and obtain a number of new observations. For example, we find that there are a lot of templates (i.e., comments with the same or similar structures) in the method comment dataset, which makes the models perform better. Some terms were thought to be important (e.g., API calls) in the comment generation by previous study does not significantly affect the quality of the generated comments, which seems counter-intuitive. Our findings may give some implications for building the approaches of method comment or inline comment generation in the future.  © 2023 Copyright held by the owner/author(s).",Code comment; comment generation; comparative study; inline comment; method comment,Codes (symbols); Automatic Generation; Code comment; Comment generation; Comparatives studies; Compare and analyze; Inline comment; Learning models; Method comment; Program comprehension; Source codes; Deep learning
Securing the Ethereum from Smart Ponzi Schemes: Identification Using Static Features,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159164922&doi=10.1145%2f3571847&partnerID=40&md5=4c8244cd3146bdba9f7c0d64f7525f80,"Malware detection approaches have been extensively studied for traditional software systems. However, the development of blockchain technology has promoted the birth of a new type of software system-decentralized applications. Composed of smart contracts, a type of application that implements the Ponzi scheme logic (called smart Ponzi schemes) has caused irreversible loss and hindered the development of blockchain technology. These smart contracts generally had a short life but involved a large amount of money. Whereas identification of these Ponzi schemes before causing financial loss has been significantly important, existing methods suffer from three main deficiencies, i.e., the insufficient dataset, the reliance on the transaction records, and the low accuracy. In this study, we first build a larger dataset. Then, a large number of features from multiple views, including bytecode, semantic, and developers, are extracted. These features are independent of the transaction records. Furthermore, we leveraged machine learning methods to build our identification model, i.e., Multi-view Cascade Ensemble model (MulCas). The experiment results show that MulCas can achieve higher performance and robustness in the scope of our dataset. Most importantly, the proposed method can identify smart Ponzi scheme at the creation time.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Blockchain; Ethereum; malware detection; Ponzi schemes,Application programs; Ethereum; Learning systems; Losses; Malware; Semantics; Smart contract; Block-chain; Detection approach; Financial loss; Irreversible loss; Large amounts; Malware detection; Ponzi scheme; Software-systems; Static features; Transaction records; Blockchain
Automated Identification of Toxic Code Reviews Using ToxiCR,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165620488&doi=10.1145%2f3583562&partnerID=40&md5=36bd54856265dcdada647601e4a83a48,"Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR, a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8% accuracy and an 88.9% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trained models, evaluation results, and source code publicly, which is available at https://github.com/WSU-SEAL/ToxiCR.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code review; Natural Language Processing; sentiment analysis; tool development; Toxicity,Codes (symbols); Computer software selection and evaluation; Large dataset; Learning algorithms; Open source software; Open systems; Software design; Supervised learning; Toxicity; Automated identification; Code review; Free and open source softwares; Language processing; Natural language processing; Natural languages; Pre-processing step; Sentiment analysis; Tool development; Vectorization techniques; Sentiment analysis
Open Source License Inconsistencies on GitHub,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168769851&doi=10.1145%2f3571852&partnerID=40&md5=42eef3adb5780cbd340744c819f4cea9,"Almost all software, open or closed, builds on open source software and therefore needs to comply with the license obligations of the open source code. Not knowing which licenses to comply with poses a legal danger to anyone using open source software. This article investigates the extent of inconsistencies between licenses declared by an open source project at the top level of the repository and the licenses found in the code. We analyzed a sample of 1,000 open source GitHub repositories. We find that about half of the repositories did not fully declare all licenses found in the code. Of these, approximately 10% represented a permissive vs. copyleft license mismatch. Furthermore, existing tools cannot fully identify licences. We conclude that users of open source code should not just look at the declared licenses of the open source code they intend to use, but rather examine the software to understand its actual licenses.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",license conflicts; License management,Codes (symbols); Copyrights; Open systems; Copyleft; License conflict; License management; Open source license; Open source projects; Open-source; Open-source code; Open-source softwares; Open source software
What's (Not) Working in Programmer User Studies?,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168770320&doi=10.1145%2f3587157&partnerID=40&md5=111cffb80929095b894686689b82de13,"A key goal of software engineering research is to improve the environments, tools, languages, and techniques programmers use to efficiently create quality software. Successfully designing these tools and demonstrating their effectiveness involves engaging with tool users - software engineers. Researchers often want to conduct user studies of software engineers to collect direct evidence. However, running user studies can be difficult, and researchers may lack solution strategies to overcome the barriers, so they may avoid user studies. To understand the challenges researchers face when conducting programmer user studies, we interviewed 26 researchers. Based on the analysis of interview data, we contribute (i) a taxonomy of 18 barriers researchers encounter; (ii) 23 solution strategies some researchers use to address 8 of the 18 barriers in their own studies; and (iii) 4 design ideas, which we adapted from the behavioral science community, that may lower 8 additional barriers. To validate the design ideas, we held an in-person all-day focus group with 16 researchers.  © 2023 Copyright held by the owner/author(s).",Empirical software engineering; experiments; human participants; human subjects; meta study; research methodology; user study,Software engineering; Design ideas; Empirical Software Engineering; Human participant; Human subjects; Meta-study; Quality software; Research methodologies; Software engineering research; Solution strategy; User study; Behavioral research
Extraction of Phrase-based Concepts in Vulnerability Descriptions through Unsupervised Labeling,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168771254&doi=10.1145%2f3579638&partnerID=40&md5=e7d9b6304fb7df764bb98f69935c2ab2,"Software vulnerabilities, once disclosed, can be documented in vulnerability databases, which have great potential to advance vulnerability analysis and security research. People describe the key characteristics of software vulnerabilities in natural language mixed with domain-specific names and concepts. This textual nature poses a significant challenge for the automatic analysis of vulnerability knowledge embedded in text. Automatic extraction of key vulnerability aspects is highly desirable but demands significant effort to manually label data for model training. In this article, we propose unsupervised methods to label and extract important vulnerability concepts in textual vulnerability descriptions (TVDs). We focus on six types of phrase-based vulnerability concepts (vulnerability type, vulnerable component, root cause, attacker type, impact, and attack vector) as they are much more difficult to label and extract than name- or number-based entities (i.e., vendor, product, and version). Our approach is based on a key observation that the same-type of phrases, no matter how they differ in sentence structures and phrase expressions, usually share syntactically similar paths in the sentence parsing trees. Specifically, we present a source-target neural architecture that learns the Part-of-Speech (POS) tagging to identify a token's functional role within TVDs, where the source neural model is trained to capture common features found in the TVD corpus, and the target model is trained to identify linguistically malformed words specific to the security domain. Our evaluation confirms that the proposed tagger outperforms (4.45%-5.98%) the taggers designed on natural language notions and identifies a broad set of TVDs and natural language contents. Then, based on the key observations, we propose two path representations (absolute paths and relative paths) and use an auto-encoder to encode such syntactic similarities. To address the discrete nature of our paths, we enhance the traditional Variational Auto-encoder (VAE) with Gumble-Max trick for categorical data distribution and thus create a Categorical VAE (CaVAE). In the latent space of absolute and relative paths, we further apply unsupervised clustering techniques to generate clusters of the same-type of concepts. Our evaluation confirms the effectiveness of our CaVAE, which achieves a small (85.85) log-likelihood for encoding path representations and the accuracy (83%-89%) of vulnerability concepts in the resulting clusters. The resulting clusters accurately label six types of vulnerability concepts from a TVD corpus in an unsupervised way. Furthermore, these labeled vulnerability concepts can be mapped back to the corresponding phrases in the original TVDs, which produce labels of six types of vulnerability concepts. The resulting labeled TVDs can be used to train concept extraction models for other TVD corpora. In this work, we present two concept extraction methods (concept classification and sequence labeling model) to demonstrate the utility of the unsupervisedly labeled concepts. Our study shows that models trained with our unsupervisedly labeled vulnerability concepts outperform (3.9%-5.14%) those trained with the two manually labeled TVD datasets from previous work due to the consistent boundary and typing by our unsupervised labeling method.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clustering and concept labeling; phrase-based vulnerability concepts; supervised concept extraction; Textual vulnerability descriptions; unsupervised representation learning,Cluster analysis; Computational linguistics; Encoding (symbols); Extraction; Signal encoding; Clustering and concept labeling; Clusterings; Concept extraction; Labelings; Natural languages; Phrase-based vulnerability concept; Supervised concept extraction; Textual vulnerability description; Unsupervised representation learning; Vulnerability description; Syntactics
Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168769189&doi=10.1145%2f3582571&partnerID=40&md5=e8e41047164d5d11c33b56539e158777,"Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",anomaly detection; curriculum learning; Cyber-physical system; deep learning; digital twin,Anomaly detection; Curricula; Cybersecurity; Deep learning; E-learning; Embedded systems; Learning systems; Anomaly detection; Anomaly detection methods; Curriculum learning; Cybe-physical systems; Cyber-physical systems; Deep learning; Human learning; Learning models; Learning process; Real-time data; Cyber Physical System
Estimating Software Functional Size via Machine Learning,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168762092&doi=10.1145%2f3582575&partnerID=40&md5=7c16d112e7e67ce7425e9c1b5ffd5bdb,"Measuring software functional size via standard Function Points Analysis (FPA) requires the availability of fully specified requirements and specific competencies. Most of the time, the need to measure software functional size occurs well in advance with respect to these ideal conditions, under the lack of complete information or skilled experts. To work around the constraints of the official measurement process, several estimation methods for FPA have been proposed and are commonly used. Among these, the International Function Points User Group (IFPUG) has adopted the ""High-level FPA""method (also known as the NESMA method). This method avoids weighting each data and transaction function by using fixed weights instead. Applying High-level FPA, or similar estimation methods, is faster and easier than carrying out the official measurement process but inevitably yields an approximation in the measures. In this article, we contribute to the problem of estimating software functional size measures by using machine learning. To the best of our knowledge, machine learning methods were never applied to the early estimation of software functional size. Our goal is to understand whether machine learning techniques yield estimates of FPA measures that are more accurate than those obtained with High-level FPA or similar methods. An empirical study on a large dataset of functional size predictors was carried out to train and test three of the most popular and robust machine learning methods, namely Random Forests, Support Vector Regression , and Neural Networks. A systematic experimental phase, with cycles of dataset filtering and splitting, parameter tuning, and model training and validation, is presented. The estimation accuracy of the obtained models was then evaluated and compared to that of fixed-weight models (e.g., High-level FPA) and linear regression models, also using a second dataset as the test set. We found that Support Vector Regression yields quite accurate estimation models. However, the obtained level of accuracy does not appear significantly better with respect to High-level FPA or to models built via ordinary least squares regression. Noticeably, fairly good accuracy levels were obtained by models that do not even require discerning among different types of transactions and data.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",early size estimation; Function Point Analysis; Function Points; functional size measurement; High-level FPA; machine learning estimation; NESMA Estimated; Neural Networks; Random Forests; SFP; SiFP; simple function points; Support Vector Regression,Forestry; Large dataset; Logistic regression; Machine learning; Random forests; Early size estimation; Function point; Function point analysis; Functional Size Measurements; High-level function point analyse; High-level functions; Machine learning estimation; Machine-learning; NESMA estimated; Neural-networks; Random forests; SFP; Simple function point; Simple++; Size estimation; Support vector regressions; Statistical tests
Challenges of Working from Home in Software Development During Covid-19 Lockdowns,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168771783&doi=10.1145%2f3579636&partnerID=40&md5=f3c90026f32bf19b9d4f63e0ac4a51da,"The COVID-19 pandemic in 2020/2021/2022 and the resulting lockdowns forced many companies to switch to working from home, swiftly, on a large scale, and without preparation. This situation created unique challenges for software development, where individual software professionals had to shift instantly from working together at a physical venue to working remotely from home. Our research questions focus on the challenges of software professionals who work from home due to the COVID-19 pandemic, which we studied empirically at a German bank. We conducted a case study employing a mixed methods approach. We aimed to cover both the breadth of challenges via a quantitative survey, as well as a deeper understanding of these challenges via the follow-up qualitative analysis of 15 semi-structured interviews. In this article, we present the key impediments employees faced during the crisis, as well as their similarities and differences to the known challenges in distributed software development (DSD). We also analyze the employees' job satisfaction and how the identified challenges impact job satisfaction. In our study, we focus on challenges in communication, collaboration, tooling, and management. The findings of the study provide insights into this emerging topic of high industry relevance. At the same time, the study contributes to the existing academic research on work from home and on the COVID-19 pandemic aftermath.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",corona crisis; coronavirus; COVID-19; Distributed software development; DSD; lockdown; mixed methods; open source; remote work; work from home,Job satisfaction; Locks (fasteners); Open source software; Open systems; Software design; Corona crisis; Coronaviruses; Distributed software development; Large-scales; Lockdown; Mixed method; Open-source; Remote work; Work from home; Coronavirus; COVID-19
White-Box Fuzzing RPC-Based APIs with EvoMaster: An Industrial Case Study,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168760170&doi=10.1145%2f3585009&partnerID=40&md5=515eed6c3e2860ccf63888706f90cf92,"Remote Procedure Call (RPC) is a communication protocol to support client-server interactions among services over a network. RPC is widely applied in industry for building large-scale distributed systems, such as Microservices. Modern RPC frameworks include, for example, Thrift, gRPC, SOFARPC, and Dubbo. Testing such systems using RPC communications is very challenging, due to the complexity of distributed systems and various RPC frameworks the system could employ. To the best of our knowledge, there does not exist any tool or solution that could enable automated testing of modern RPC-based services. To fill this gap, in this article we propose the first approach in the literature, together with an open source tool, for fuzzing modern RPC-based APIs. The approach is in the context of white-box testing with search-based techniques. To tackle schema extraction of various RPC frameworks, we formulate a RPC schema specification along with a parser that allows the extraction from source code of any JVM RPC-based APIs. Then, with the extracted schema we employ a search to produce tests by maximizing white-box heuristics and newly defined heuristics specific to the RPC domain. We built our approach as an extension to an open source fuzzer (i.e., EvoMaster), and the approach has been integrated into a real industrial pipeline that could be applied to a real industrial development process for fuzzing RPC-based APIs. To assess our novel approach, we conducted an empirical study with two artificial and four industrial web services selected by our industrial partner. In addition, to further demonstrate its effectiveness and application in industrial settings, we report results of employing our tool for fuzzing another 50 industrial APIs autonomously conducted by our industrial partner in their testing processes. Results show that our novel approach is capable of enabling automated test case generation for industrial RPC-based APIs (i.e., 2 artificial and 54 industrial). We also compared with a simple gray-box technique and existing manually written tests. Our white-box solution achieves significant improvements on code coverage. Regarding fault detection, by conducting a careful review with our industrial partner of the tests generated by our novel approach in the selected four industrial APIs, a total of 41 real faults were identified, which have now been fixed. Another 8,377 detected faults are currently under investigation.  © 2023 Copyright held by the owner/author(s).",fuzzing; gRPC; Mircoservices; RPC; SBST; test generation; Thrift,Application programming interfaces (API); Codes (symbols); Fault detection; Open source software; Open systems; Pipelines; Web services; Fuzzing; GRPC; Industrial case study; Industrial partners; Mircoservice; Remote Procedure Call; SBST; Test generations; Thrift; White box; Extraction
QuoTe: Quality-oriented Testing for Deep Learning Systems,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168765739&doi=10.1145%2f3582573&partnerID=40&md5=708a15d7c9b5c1290b59777ad78df10d,"Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testing - that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Quality-oriented Testing). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model quality - that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; fairness; robustness; testing,Deep learning; Image enhancement; Quality assurance; Well testing; Deep learning; Engineering techniques; Fairness; Guided search; Model properties; Modeling quality; Property; Robustness; Test case; Testing framework; Learning systems
Toward Interpretable Graph Tensor Convolution Neural Network for Code Semantics Embedding,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168769136&doi=10.1145%2f3582574&partnerID=40&md5=ba7fac921dc538ffe1e6a93800dcd2d5,"Intelligent deep learning-based models have made significant progress for automated source code semantics embedding, and current research works mainly leverage natural language-based methods and graph-based methods. However, natural language-based methods do not capture the rich semantic structural information of source code, and graph-based methods do not utilize rich distant information of source code due to the high cost of message-passing steps.In this article, we propose a novel interpretable model, called graph tensor convolution neural network (GTCN), to generate accurate code embedding, which is capable of comprehensively capturing the distant information of code sequences and rich code semantics structural information. First, we propose to utilize a high-dimensional tensor to integrate various heterogeneous code graphs with node sequence features, such as control flow, data flow. Second, inspired by the current advantages of graph-based deep learning and efficient tensor computations, we propose a novel interpretable graph tensor convolution neural network for learning accurate code semantic embedding from the code graph tensor. Finally, we evaluate three popular applications on the GTCN model: variable misuse detection, source code prediction, and vulnerability detection. Compared with current state-of-the-art methods, our model achieves higher scores with respect to the top-1 accuracy while costing less training time.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code embedding; graph neural network; Tensor computation,Codes (symbols); Convolution; Data flow analysis; Deep learning; Flow graphs; Graph neural networks; Graph structures; Graphic methods; Knowledge graph; Message passing; Network coding; Semantic Web; Semantics; Tensors; 'current; Code embedding; Code semantics; Convolution neural network; Embeddings; Graph neural networks; Natural languages; Semantic embedding; Source codes; Tensor computation; Embeddings
"Code-line-level Bugginess Identification: How Far have We Come, and How Far have We Yet to Go?",2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164279616&doi=10.1145%2f3582572&partnerID=40&md5=61925753e401ac7ecd1702ee67c8713c,"Background. Code-line-level bugginess identification (CLBI) is a vital technique that can facilitate developers to identify buggy lines without expending a large amount of human effort. Most of the existing studies tried to mine the characteristics of source codes to train supervised prediction models, which have been reported to be able to discriminate buggy code lines amongst others in a target program.Problem. However, several simple and clear code characteristics, such as complexity of code lines, have been disregarded in the current literature. Such characteristics can be acquired and applied easily in an unsupervised way to conduct more accurate CLBI, which also can decrease the application cost of existing CLBI approaches by a large margin.Objective. We aim at investigating the status quo in the field of CLBI from the perspective of (1) how far we have really come in the literature, and (2) how far we have yet to go in the industry, by analyzing the performance of state-of-the-art (SOTA) CLBI approaches and tools, respectively.Method. We propose a simple heuristic baseline solution GLANCE (aiminG at controL- ANd ComplEx-statements) with three implementations (i.e., GLANCE-MD, GLANCE-EA, and GLANCE-LR). GLANCE is a two-stage CLBI framework: first, use a simple model to predict the potentially defective files; second, leverage simple code characteristics to identify buggy code lines in the predicted defective files. We use GLANCE as the baseline to investigate the effectiveness of the SOTA CLBI approaches, including natural language processing (NLP) based, model interpretation techniques (MIT) based, and popular static analysis tools (SAT).Result. Based on 19 open-source projects with 142 different releases, the experimental results show that GLANCE framework has a prediction performance comparable or even superior to the existing SOTA CLBI approaches and tools in terms of 8 different performance indicators.Conclusion. The results caution us that, if the identification performance is the goal, the real progress in CLBI is not being achieved as it might have been envisaged in the literature and there is still a long way to go to really promote the effectiveness of static analysis tools in industry. In addition, we suggest using GLANCE as a baseline in future studies to demonstrate the usefulness of any newly proposed CLBI approach. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bugginess; Code line; defect prediction; quality assurance; static analysis tool,Codes (symbols); Computer software selection and evaluation; Defects; Forecasting; Natural language processing systems; Open source software; Quality assurance; Quality control; Analysis tools; Bugginess; Code line; Defect prediction; Identification approach; Identification tools; Performance; Simple++; State of the art; Static analyse tool; Static analysis
Katana: Dual Slicing Based Context for Learning Bug Fixes,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164276900&doi=10.1145%2f3579640&partnerID=40&md5=c54a689cff3312f75559e560f7e731e1,"Contextual information plays a vital role for software developers when understanding and fixing a bug. Consequently, deep learning based program repair techniques leverage context for bug fixes. However, existing techniques treat context in an arbitrary manner, by extracting code in close proximity of the buggy statement within the enclosing file, class, or method, without any analysis to find actual relations with the bug. To reduce noise, they use a predefined maximum limit on the number of tokens to be used as context. We present a program slicing based approach, in which instead of arbitrarily including code as context, we analyze statements that have a control or data dependency on the buggy statement. We propose a novel concept called dual slicing, which leverages the context of both buggy and fixed versions of the code to capture relevant repair ingredients. We present our technique and tool called Katana, the first to apply slicing-based context for a program repair task. The results show that Katana effectively preserves sufficient information for a model to choose contextual information while reducing noise. We compare against four recent state-of-the-art context-aware program repair techniques. Our results show that Katana fixes between 1.5 and 3.7 times more bugs than existing techniques. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contextual information; deep learning; graph neural networks; program repair; Program slicing,Codes (symbols); Deep neural networks; Repair; Bug fixes; Close proximity; Contextual information; Data dependencies; Deep learning; Graph neural networks; Program repair; Program slicing; Repair techniques; Software developer; Graph neural networks
SafeDrop: Detecting Memory Deallocation Bugs of Rust Programs via Static Data-flow Analysis,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164301111&doi=10.1145%2f3542948&partnerID=40&md5=0e0c3ea26210aab3be643abe00100e44,"Rust is an emerging programming language that aims to prevent memory-safety bugs. However, the current design of Rust also brings side effects, which may increase the risk of memory-safety issues. In particular, it employs ownership-based resource management and enforces automatic deallocation of unused resources without using the garbage collector. It may therefore falsely deallocate reclaimed memory and lead to use-after-free or double-free issues. In this article, we study the problem of invalid memory deallocation and propose SafeDrop, a static path-sensitive data-flow analysis approach to detect such bugs. Our approach analyzes each function of a Rust crate iteratively in a flow-sensitive and field-sensitive way. It leverages a modified Tarjan algorithm to achieve scalable path-sensitive analysis and a cache-based strategy for efficient inter-procedural analysis. We have implemented our approach and integrated it into the Rust compiler. Experiment results show that the approach can successfully detect all such bugs in our experiments with a limited number of false positives and incurs a very small overhead compared to the original compilation time. © 2023 Association for Computing Machinery.",data-flow analysis; meet over path; path sensitivity; Rust,Data transfer; Iterative methods; Program debugging; Safety engineering; Sensitive data; 'current; Data-flow analysis; Meet over path; Memory de-allocation; Memory safety; Path sensitivity; Rust; Safety issues; Side effect; Static data-flow analysis; Data flow analysis
Security Misconfigurations in Open Source Kubernetes Manifests: An Empirical Study,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164297169&doi=10.1145%2f3579639&partnerID=40&md5=144a0030b77d8a84791b7b639e578cff,"Context: Kubernetes has emerged as the de-facto tool for automated container orchestration. Business and government organizations are increasingly adopting Kubernetes for automated software deployments. Kubernetes is being used to provision applications in a wide range of domains, such as time series forecasting, edge computing, and high-performance computing. Due to such a pervasive presence, Kubernetes-related security misconfigurations can cause large-scale security breaches. Thus, a systematic analysis of security misconfigurations in Kubernetes manifests, i.e., configuration files used for Kubernetes, can help practitioners secure their Kubernetes clusters.Objective: The goal of this paper is to help practitioners secure their Kubernetes clusters by identifying security misconfigurations that occur in Kubernetes manifests.Methodology: We conduct an empirical study with 2,039 Kubernetes manifests mined from 92 open-source software repositories to systematically characterize security misconfigurations in Kubernetes manifests. We also construct a static analysis tool called Security Linter for Kubernetes Manifests (SLI-KUBE) to quantify the frequency of the identified security misconfigurations.Results: In all, we identify 11 categories of security misconfigurations, such as absent resource limit, absent securityContext, and activation of hostIPC. Specifically, we identify 1,051 security misconfigurations in 2,039 manifests. We also observe the identified security misconfigurations affect entities that perform mesh-related load balancing, as well as provision pods and stateful applications. Furthermore, practitioners agreed to fix 60% of 10 misconfigurations reported by us.Conclusion: Our empirical study shows Kubernetes manifests to include security misconfigurations, which necessitates security-focused code reviews and application of static analysis when Kubernetes manifests are developed. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Configuration; container orchestration; devops; devsecops; empirical study; Kubernetes; misconfiguration; security,Containers; Open source software; Open systems; Business organizations; Configuration; Container orchestration; Devops; Devsecop; Empirical studies; Kubernetes; Misconfigurations; Open-source; Security; Static analysis
A Comprehensive Empirical Study of Bias Mitigation Methods for Machine Learning Classifiers,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164291118&doi=10.1145%2f3583561&partnerID=40&md5=4164002ae082454b6e167eb73c46f33d,"Software bias is an increasingly important operational concern for software engineers. We present a large-scale, comprehensive empirical study of 17 representative bias mitigation methods for Machine Learning (ML) classifiers, evaluated with 11 ML performance metrics (e.g., accuracy), 4 fairness metrics, and 20 types of fairness-performance tradeoff assessment, applied to 8 widely-adopted software decision tasks. The empirical coverage is much more comprehensive, covering the largest numbers of bias mitigation methods, evaluation metrics, and fairness-performance tradeoff measures compared to previous work on this important software property. We find that (1) the bias mitigation methods significantly decrease ML performance in 53% of the studied scenarios (ranging between 42%∼66% according to different ML performance metrics); (2) the bias mitigation methods significantly improve fairness measured by the 4 used metrics in 46% of all the scenarios (ranging between 24%∼59% according to different fairness metrics); (3) the bias mitigation methods even lead to decrease in both fairness and ML performance in 25% of the scenarios; (4) the effectiveness of the bias mitigation methods depends on tasks, models, the choice of protected attributes, and the set of metrics used to assess fairness and ML performance; (5) there is no bias mitigation method that can achieve the best tradeoff in all the scenarios. The best method that we find outperforms other methods in 30% of the scenarios. Researchers and practitioners need to choose the bias mitigation method best suited to their intended application scenario(s). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bias mitigation; fairness-performance trade-off; Machine Learning,Economic and social effects; Bias mitigation; Empirical studies; Fairness performance; Fairness-performance trade-off; Learning classifiers; Learning performance; Machine-learning; Mitigation methods; Performance metrices; Performance tradeoff; Machine learning
Client-Specific Upgrade Compatibility Checking via Knowledge-Guided Discovery,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162086093&doi=10.1145%2f3582569&partnerID=40&md5=99f5656392e602e6a0825fb32ce6f1d6,"Modern software systems are complex, and they heavily rely on external libraries developed by different teams and organizations. Such systems suffer from higher instability due to incompatibility issues caused by library upgrades. In this article, we address the problem by investigating the impact of a library upgrade on the behaviors of its clients. We developed CompCheck, an automated upgrade compatibility checking framework that generates incompatibility-revealing tests based on previous examples. CompCheck first establishes an offline knowledge base of incompatibility issues by mining from open source projects and their upgrades. It then discovers incompatibilities for a specific client project, by searching for similar library usages in the knowledge base and generating tests to reveal the problems. We evaluated CompCheck on 202 call sites of 37 open source projects and the results show that CompCheck successfully revealed incompatibility issues on 76 call sites, 72.7% and 94.9% more than two existing techniques, confirming CompCheck's applicability and effectiveness. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",compatibility; Software upgrade; test generation,Open source software; Software testing; Client specific; Compatibility; Offline; Open source projects; Software upgrades; Software-systems; Test generations; Knowledge based systems
Simulator-based Explanation and Debugging of Hazard-triggering Events in DNN-based Safety-critical Systems,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148277620&doi=10.1145%2f3569935&partnerID=40&md5=25d62f52344fe32670311f45bf5322f5,"When Deep Neural Networks (DNNs) are used in safety-critical systems, engineers should determine the safety risks associated with failures (i.e., erroneous outputs) observed during testing. For DNNs processing images, engineers visually inspect all failure-inducing images to determine common characteristics among them. Such characteristics correspond to hazard-triggering events (e.g., low illumination) that are essential inputs for safety analysis. Though informative, such activity is expensive and error prone.To support such safety analysis practices, we propose Simulator-based Explanations for DNN failurEs (SEDE), a technique that generates readable descriptions for commonalities in failure-inducing, real-world images and improves the DNN through effective retraining. SEDE leverages the availability of simulators, which are commonly used for cyber-physical systems. It relies on genetic algorithms to drive simulators toward the generation of images that are similar to failure-inducing, real-world images in the test set; it then employs rule learning algorithms to derive expressions that capture commonalities in terms of simulator parameter values. The derived expressions are then used to generate additional images to retrain and improve the DNN.With DNNs performing in-car sensing tasks, SEDE successfully characterized hazard-triggering events leading to a DNN accuracy drop. Also, SEDE enabled retraining leading to significant improvements in DNN accuracy, up to 18 percentage points. © 2023 Association for Computing Machinery.",DNN debugging; DNN explanation; DNN functional safety analysis; explainable AI; heatmaps,Embedded systems; Genetic algorithms; Hazards; Image enhancement; Program debugging; Safety testing; Deep neural network debugging; Deep neural network explanation; Deep neural network functional safety analyse; Explainable AI; Functional Safety; Heatmaps; Network debugging; Real-world image; Safety analysis; Safety critical systems; Deep neural networks
sem2vec: Semantics-aware Assembly Tracelet Embedding,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164238887&doi=10.1145%2f3569933&partnerID=40&md5=68ad74e6986c3d3ddd45584146b8e5a2,"Binary code similarity is the foundation of many security and software engineering applications. Recent works leverage deep neural networks (DNN) to learn a numeric vector representation (namely, embeddings) of assembly functions, enabling similarity analysis in the numeric space. However, existing DNN-based techniques capture syntactic-, control flow-, or data flow-level information of assembly code, which is too coarse-grained to represent program functionality. These methods can suffer from low robustness to challenging settings such as compiler optimizations and obfuscations.We present sem2vec, a binary code embedding framework that learns from semantics. Given the control-flow graph (CFG), 34 pages. of an assembly function, we divide it into tracelets, denoting continuous and short execution traces that are reachable from the function entry point. We use symbolic execution to extract symbolic constraints and other auxiliary information on each tracelet. We then train masked language models to compute embeddings of symbolic execution outputs. Last, we use graph neural networks, to aggregate tracelet embeddings into the CFG-level embedding for a function. Our evaluation shows that sem2vec extracts high-quality embedding and is robust against different compilers, optimizations, architectures, and popular obfuscation methods including virtualization obfuscation. We further augment a vulnerability search application with embeddings computed by sem2vec and demonstrate a significant improvement in vulnerability search accuracy. © 2023 Association for Computing Machinery.",binary code similarity; embedding; graph neural network; Symbolic execution,Application programs; Binary codes; Data flow analysis; Deep neural networks; Flow graphs; Graph neural networks; Model checking; Quality control; Semantics; Vector spaces; Assembly functions; Binary code similarity; Code similarities; Compiler optimizations; Control-flow graphs; Embeddings; Graph neural networks; Learn+; Semantic-aware; Symbolic execution; Embeddings
Duplicate Bug Report Detection: How Far Are We?,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162633999&doi=10.1145%2f3576042&partnerID=40&md5=fa974b5bbf9e8b066b8f62a484b1e5f5,"Many Duplicate Bug Report Detection (DBRD) techniques have been proposed in the research literature. The industry uses some other techniques. Unfortunately, there is insufficient comparison among them, and it is unclear how far we have been. This work fills this gap by comparing the aforementioned techniques. To compare them, we first need a benchmark that can estimate how a tool would perform if applied in a realistic setting today. Thus, we first investigated potential biases that affect the fair comparison of the accuracy of DBRD techniques. Our experiments suggest that data age and issue tracking system (ITS) choice cause a significant difference. Based on these findings, we prepared a new benchmark. We then used it to evaluate DBRD techniques to estimate better how far we have been. Surprisingly, a simpler technique outperforms recently proposed sophisticated techniques on most projects in our benchmark. In addition, we compared the DBRD techniques proposed in research with those used in Mozilla and VSCode. Surprisingly, we observe that a simple technique already adopted in practice can achieve comparable results as a recently proposed research tool. Our study gives reflections on the current state of DBRD, and we share our insights to benefit future DBRD research. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bug reports; deep learning; duplicate bug report detection; empirical study,Bug reports; Deep learning; Duplicate bug report detection; Duplicate bug reports; Empirical studies; Industry use; Issue Tracking; Mozilla; Simple++; Tracking system; Deep learning
HybridCISave: A Combined Build and Test Selection Approach in Continuous Integration,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164243498&doi=10.1145%2f3576038&partnerID=40&md5=749617640692c8840bae00de92aa4b7f,"Continuous Integration (CI) is a popular practice in modern software engineering. Unfortunately, it is also a high-cost practice - Google and Mozilla estimate their CI systems in millions of dollars. To reduce the computational cost in CI, researchers developed approaches to selectively execute builds or tests that are likely to fail (and skip those likely to pass). In this article, we present a novel hybrid technique (HybridCISave) to improve on the limitations of existing techniques: to provide higher cost savings and higher safety. To provide higher cost savings, HybridCISave combines techniques to predict and skip executions of both full builds that are predicted to pass and partial ones (only the tests in them predicted to pass). To provide higher safety, HybridCISave combines the predictions of multiple techniques to obtain stronger certainty before it decides to skip a build or test. We evaluated HybridCISave by comparing its effectiveness with the existing build selection techniques over 100 projects and found that it provided higher cost savings at the highest safety. We also evaluated each design decision in HybridCISave and found that skipping both full and partial builds increased its cost savings and that combining multiple test selection techniques made it safer. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",build selection; Continuous Integration; Software maintenance; test selection,Computer software maintenance; Cost benefit analysis; Integration; Safety engineering; Build selection; Continuous integrations; Cost saving; Google+; High costs; High safety; Integration systems; Mozilla; Selection techniques; Test selection; Software testing
Structured Theorem for Quantum Programs and its Applications,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164277007&doi=10.1145%2f3587154&partnerID=40&md5=49f9ae40452531386bb091d44f897b85,"This article proves a structured program theorem for flowchart quantum programs. The theorem states that any flowchart quantum program is equivalent to a single quantum program that repeatedly executes a quantum measurement and a subprogram, so long as the measurement outcome is true. Moreover, their expected runtime, variance, and general moments are the same. This theorem simplifies the quantum program's verification significantly.-We derive an analytical characterization of the termination problem for quantum programs in polynomial time. Our procedure is more efficient and accurate with much simpler techniques than the analysis of this problem, as described in [29].-We compute the expected runtime analytically and exactly for quantum programs in polynomial time. This result improves the methods based on the weakest precondition calculus for the question recently developed in [31, 34].-We show that a single loop rule is a relatively complete Hoare logic for quantum programs after applying our structured theorem. Although using fewer rules, our method verifies a broader class of quantum programs, compared with the results in [45] and [56]. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",flowchart language; Quantum programming; structure programming; while-language,Flowcharting; Polynomial approximation; Quantum theory; Flowchart language; ITS applications; Polynomial-time; Program applications; Quantum measurement; Quantum programming; Runtimes; Single quantum; Structure programming; While-language; Application programs
Testing Feedforward Neural Networks Training Programs,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145565103&doi=10.1145%2f3529318&partnerID=40&md5=1ba18c51f53057cf02c6a747c3ee7a88,"At present, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. In this article, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design TheDeepChecker, an end-to-end property-based debugging approach for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we conduct a case study to assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs and compare its performance to that of the Amazon SageMaker Debugger (SMD). Results show that TheDeepChecker's on-execution validation of DNN-based program's properties through three sequential phases (pre-, on-, and post-fitting) succeeds in revealing several coding bugs and system misconfigurations errors early on and at a low cost. Moreover, our property-based approach outperforms the SMD's offline rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of additional DL bugs. © 2023 Association for Computing Machinery.",Neural networks; property-based debugging; training programs,Aircraft accidents; Errors; Feedforward neural networks; Neural network models; Program debugging; Software testing; Training aircraft; Aircraft collision avoidance systems; Car collisions; Neural network model; Neural networks trainings; Neural-networks; Performance; Property-based; Property-based debugging; Safety critical systems; Training program; Deep neural networks
I Depended on You and You Broke Me: An Empirical Study of Manifesting Breaking Changes in Client Packages,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164244277&doi=10.1145%2f3576037&partnerID=40&md5=987c8b00fa4d2c978d5cc401bb1be6b5,"Complex software systems have a network of dependencies. Developers often configure package managers (e.g., npm) to automatically update dependencies with each publication of new releases containing bug fixes and new features. When a dependency release introduces backward-incompatible changes, commonly known as breaking changes, dependent packages may not build anymore. This may indirectly impact downstream packages, but the impact of breaking changes and how dependent packages recover from these breaking changes remain unclear. To close this gap, we investigated the manifestation of breaking changes in the npm ecosystem, focusing on cases where packages' builds are impacted by breaking changes from their dependencies. We measured the extent to which breaking changes affect dependent packages. Our analyses show that around 12% of the dependent packages and 14% of their releases were impacted by a breaking change during updates of non-major releases of their dependencies. We observed that, from all of the manifesting breaking changes, 44% were introduced in both minor and patch releases, which in principle should be backward compatible. Clients recovered themselves from these breaking changes in half of the cases, most frequently by upgrading or downgrading the provider's version without changing the versioning configuration in the package manager. We expect that these results help developers understand the potential impact of such changes and recover from them. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Breaking changes; change impact; dependency management; npm; Semantic Version,Recovery; Breaking change; Breakings; Bug fixes; Change impacts; Complex software systems; Dependency management; Down-stream; Empirical studies; Npm; Semantic version; Semantics
DAISY: Dynamic-Analysis-Induced Source Discovery for Sensitive Data,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164244675&doi=10.1145%2f3569936&partnerID=40&md5=848e8afa8ac01548912f47a40c15be7e,"Mobile apps are widely used and often process users' sensitive data. Many taint analysis tools have been applied to analyze sensitive information flows and report data leaks in apps. These tools require a list of sources (where sensitive data is accessed) as input, and researchers have constructed such lists within the Android platform by identifying Android API methods that allow access to sensitive data. However, app developers may also define methods or use third-party library's methods for accessing data. It is difficult to collect such source methods, because they are unique to the apps, and there are a large number of third-party libraries available on the market that evolve over time. To address this problem, we propose DAISY, a Dynamic-Analysis-Induced Source discoverY approach for identifying methods that return sensitive information from apps and third-party libraries. Trained on an automatically labeled dataset of methods and their calling context, DAISY identifies sensitive methods in unseen apps. We evaluated DAISY on real-world apps, and the results show that DAISY can achieve an overall precision of 77.9% when reporting the most confident results. Most of the identified sources and leaks cannot be detected by existing technologies. © 2023 Association for Computing Machinery.",mobile application; natural language processing; Privacy leak,Android (operating system); Libraries; Natural language processing systems; Dynamics analysis; Language processing; Mobile app; Mobile applications; Natural language processing; Natural languages; Privacy leak; Sensitive datas; Sensitive informations; Third parties; Sensitive data
Video Game Bad Smells: What They Are and How Developers Perceive Them,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164270844&doi=10.1145%2f3563214&partnerID=40&md5=18be7076c873875bab83c898f74ce3d5,"Video games represent a substantial and increasing share of the software market. However, their development is particularly challenging as it requires multi-faceted knowledge, which is not consolidated in computer science education yet. This article aims at defining a catalog of bad smells related to video game development. To achieve this goal, we mined discussions on general-purpose and video game-specific forums. After querying such a forum, we adopted an open coding strategy on a statistically significant sample of 572 discussions, stratified over different forums. As a result, we obtained a catalog of 28 bad smells, organized into five categories, covering problems related to game design and logic, physics, animation, rendering, or multiplayer. Then, we assessed the perceived relevance of such bad smells by surveying 76 game development professionals. The survey respondents agreed with the identified bad smells but also provided us with further insights about the discussed smells. Upon reporting results, we discuss bad smell examples, their consequences, as well as possible mitigation/fixing strategies and trade-offs to be pursued by developers. The catalog can be used not only as a guideline for developers and educators but also can pave the way toward better automated tool support for video game developers. © 2023 Association for Computing Machinery.",bad smells; empirical study; Q&A forums; Video games,Commerce; Economic and social effects; Education computing; Game design; Human computer interaction; Interactive computer graphics; Software design; Bad smells; Coding strategy; Computer Science Education; Covering problems; Empirical studies; Game design; Q&A forum; Software markets; Video game development; Video-games; Animation
Making Sense of the Unknown: How Managers Make Cyber Security Decisions,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164216769&doi=10.1145%2f3548682&partnerID=40&md5=970923513fb488d4e46f038d9fe5f373,"Managers rarely have deep knowledge of cyber security and yet are expected to make decisions with cyber security implications for software-based systems. We investigate the decision-making conversations of seven teams of senior managers from the same organisation as they complete the Decisions & Disruptions cyber security exercise. We use grounded theory to situate our analysis of their decision-making and help us explore how these complex socio-cognitive interactions occur. We have developed a goal-model (using iStar 2.0) of the teams' dialogue that illustrates what cyber security goals teams identify and how they operationalise their decisions to reach these goals. We complement this with our model of cyber security reasoning that describes how these teams make their decisions, showing how each team members' experience, intuition, and understanding affects the team's overall shared reasoning and decision-making. Our findings show how managers with little cyber security expertise are able to use logic and traditional risk management thinking to make cyber security decisions. Despite their lack of cyber security-specific training, they demonstrate reasoning that closely resembles the decision-making approaches espoused in cyber security-specific standards (e.g., NIST/ISO). Our work demonstrates how organisations and practitioners can enrich goal modelling to capture not only what security goals an organisation has (and how they can operationalise them) but also how and why these goals have been identified. Ultimately, non-cyber security experts can develop their cyber security model based on their current context (and update it when new requirements appear or new incidents happen), whilst capturing their reasoning at every stage. © 2023 Copyright held by the owner/author(s).",Cyber security decision-making; cyber security risk analysis; goal modelling,Decision making; Decision theory; Human resource management; Risk analysis; Risk assessment; Risk management; Cybe security decision-making; Cybe security risk analyse; Cyber security; Deep knowledge; Goal models; Security decision makings; Security goals; Security risk analysis; Cybersecurity
On the Discoverability of npm Vulnerabilities in Node.js Projects,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164240458&doi=10.1145%2f3571848&partnerID=40&md5=45398f37670e68546c168133b48c40f6,"The reliance on vulnerable dependencies is a major threat to software systems. Dependency vulnerabilities are common and remain undisclosed for years. However, once the vulnerability is discovered and publicly known to the community, the risk of exploitation reaches its peak, and developers have to work fast to remediate the problem. While there has been a lot of research to characterize vulnerabilities in software ecosystems, none have explored the problem taking the discoverability into account. Therefore, we perform a large-scale empirical study examining 6,546 Node.js applications. We define three discoverability levels based on vulnerabilities lifecycle (undisclosed, reported, and public). We find that although the majority of the affected applications (99.42%) depend on undisclosed vulnerable packages, 206 (4.63%) applications were exposed to dependencies with public vulnerabilities. The major culprit for the applications being affected by public vulnerabilities is the lack of dependency updates; in 90.8% of the cases, a fix is available but not patched by application maintainers. Moreover, we find that applications remain affected by public vulnerabilities for a long time (103 days). Finally, we devise DepReveal, a tool that supports our discoverability analysis approach, to help developers better understand vulnerabilities in their application dependencies and plan their project maintenance. © 2023 Association for Computing Machinery.",dependency vulnerabilities; Open source software; software ecosystems; software packages,Ecosystems; Life cycle; Open systems; Analysis approach; Dependency vulnerability; Empirical studies; Exposed to; Large-scales; Open-source softwares; Project maintenance; Software ecosystems; Software-systems; Open source software
Modern Code Reviews - Survey of Literature and Practice,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163852501&doi=10.1145%2f3585004&partnerID=40&md5=ca72efe7435d9cbe7b231095b40fa810,"Background: Modern Code Review (MCR) is a lightweight alternative to traditional code inspections. While secondary studies on MCR exist, it is uanknown whether the research community has targeted themes that practitioners consider important.Objectives: The objectives are to provide an overview of MCR research, analyze the practitioners' opinions on the importance of MCR research, investigate the alignment between research and practice, and propose future MCR research avenues.Method: We conducted a systematic mapping study to survey state of the art until and including 2021, employed the Q-Methodology to analyze the practitioners' perception of the relevance of MCR research, and analyzed the primary studies' research impact.Results: We analyzed 244 primary studies, resulting in five themes. As a result of the 1,300 survey data points, we found that the respondents are positive about research investigating the impact of MCR on product quality and MCR process properties. In contrast, they are negative about human factor- and support systems-related research.Conclusion: These results indicate a misalignment between the state of the art and the themes deemed important by most survey respondents. Researchers should focus on solutions that can improve the state of MCR practice. We provide an MCR research agenda that can potentially increase the impact of MCR research. © 2023 Copyright held by the owner/author(s).",literature survey; Modern code review; practitioner survey,Code inspections; Code review; Literature survey; Modern code review; Practitioner surveys; Research analysis; Research communities; Research impacts; State of the art; Systematic mapping studies; Codes (symbols)
1-to-1 or 1-to-n? Investigating the Effect of Function Inlining on Binary Similarity Analysis,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141387196&doi=10.1145%2f3561385&partnerID=40&md5=05f97bf29928692f2e060a64a5679df3,"Binary similarity analysis is critical to many code-reuse-related issues, where function matching is its fundamental task. ""1-to-1""mechanism has been applied in most binary similarity analysis works, in which one function in a binary file is matched against one function in a source file or binary file. However, we discover that the function mapping is a more complex problem of ""1-to-n""(one binary function matches multiple source functions or binary functions) or even ""n-to-n""(multiple binary functions match multiple binary functions) due to the existence of function inlining, different from traditional understanding. In this article, we investigate the effect of function inlining on binary similarity analysis. We carry out three studies to investigate the extent of function inlining, the performance of existing works under function inlining, and the effectiveness of existing inlining-simulation strategies. Firstly, a scalable and lightweight identification method is designed to recover function inlining in binaries. 88 projects (compiled in 288 versions and resulting in 32,460,156 binary functions) are collected and analyzed to construct four inlining-oriented datasets for four security tasks in the software supply chain, including code search, OSS (Open Source Software) reuse detection, vulnerability detection, and patch presence test. Datasets reveal that the proportion of function inlining ranges from 30-40% when using O3 and sometimes can reach nearly 70%. Then, we evaluate four existing works on our dataset. Results show most existing works neglect inlining and use the ""1-to-1""mechanism. The mismatches cause a 30% loss in performance during code search and a 40% loss during vulnerability detection. Moreover, most inlined functions would be ignored during OSS reuse detection and patch presence test, thus leaving these functions risky. Finally, we analyze two inlining-simulation strategies on our dataset. It is shown that they miss nearly 40% of the inlined functions, and there is still a large space for promotion. By precisely recovering when function inlining happens, we discover that inlining is usually cumulative when optimization increases. Thus, conditional inlining and incremental inlining are recommended to design a low-cost and high-coverage inlining-simulation strategy. © 2023 Association for Computing Machinery.",1-to-1; 1-to-n; Binary similarity analysis; function inlining,Computer software reusability; Open systems; Software testing; Supply chains; 1-to-1; 1-to-n; Binary files; Binary functions; Binary similarity analyse; Function inlining; Inlining; Performance; Similarity analysis; Simulation strategies; Open source software
The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164237362&doi=10.1145%2f3576039&partnerID=40&md5=c5699fa7328a6c7f276d39f9de22523e,"A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state-of-the-art explores research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower Leopard (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based learned embeddings associated with XGBoost achieves an AUC value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. By combining deep learned embeddings and engineered features, Panther (the upgraded version of Leopard implemented in this work) outperforms Leopard with higher scores in terms of AUC, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable ML technique, SHAP, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction. © 2023 Copyright held by the owner/author(s).",distributed representation learning; embeddings; explanation; features combination; machine learning; patch correctness; Program repair,Classification (of information); Codes (symbols); Embeddings; Forecasting; Software testing; Distributed representation; Distributed representation learning; Dynamic information; Embeddings; Explanation; Feature combination; Machine-learning; Patch correctness; Program repair; State of the art; Machine learning
Arachne: Search-Based Repair of Deep Neural Networks,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164284701&doi=10.1145%2f3563210&partnerID=40&md5=8951e2fd73eb55d2de367e4f07aaa00d,"The rapid and widespread adoption of Deep Neural Networks (DNNs) has called for ways to test their behaviour, and many testing approaches have successfully revealed misbehaviour of DNNs. However, it is relatively unclear what one can do to correct such behaviour after revelation, as retraining involves costly data collection and does not guarantee to fix the underlying issue. This article introduces Arachne, a novel program repair technique for DNNs, which directly repairs DNNs using their input-output pairs as a specification. Arachne localises neural weights on which it can generate effective patches and uses differential evolution to optimise the localised weights and correct the misbehaviour. An empirical study using different benchmarks shows that Arachne can fix specific misclassifications of a DNN without reducing general accuracy significantly. On average, patches generated by Arachne generalise to 61.3% of unseen misbehaviour, whereas those by a state-of-the-art DNN repair technique generalise only to 10.2% and sometimes to none while taking tens of times more than Arachne. We also show that Arachne can address fairness issues by debiasing a gender classification model. Finally, we successfully apply Arachne to a text sentiment model to show that it generalises beyond convolutional neural networks. © 2023 Association for Computing Machinery.",Automatic program repair; deep learning,Evolutionary algorithms; Optimization; Repair; Automatic program repair; Automatic programs; Data collection; Deep learning; Differential Evolution; Input-output; Misbehaviour; Neural weights; Repair techniques; Search-based; Deep neural networks
IFDS-based Context Debloating for Object-Sensitive Pointer Analysis,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164284427&doi=10.1145%2f3579641&partnerID=40&md5=8bb4edf9d0cb8163966e34d3ba06a3eb,"Object-sensitive pointer analysis, which separates the calling contexts of a method by its receiver objects, is known to achieve highly useful precision for object-oriented languages such as Java. Despite recent advances, all object-sensitive pointer analysis algorithms still suffer from the scalability problem due to the combinatorial explosion of contexts in large programs. In this article, we introduce a new approach, Conch, that can be applied to debloat contexts for all object-sensitive pointer analysis algorithms, thereby improving significantly their efficiency while incurring a negligible loss of precision. Our key insight is to approximate a recently proposed set of two necessary conditions for an object in a program to be context-sensitive, i.e., context-dependent (whose precise verification is undecidable) with a set of three linearly verifiable conditions in terms of the number of edges in the pointer assignment graph (PAG) representation of the program. These three linearly verifiable conditions, which turn out to be almost always necessary in practice, are synthesized from three key observations regarding context-dependability for the objects created and used in real-world object-oriented programs. To develop a practical implementation for Conch, we introduce an IFDS-based algorithm for reasoning about object reachability in the PAG of a program, which runs linearly in terms of the number of edges in the PAG. By debloating contexts for three representative object-sensitive pointer analysis algorithms, which are applied to a set of representative Java programs, Conch can speed up these three baseline algorithms substantially at only a negligible loss of precision (less than 0.1%) with respect to several commonly used precision metrics. In addition, Conch also improves their scalability by enabling them to analyze substantially more programs to completion than before (under a time budget of 12 hours). Conch has been open-sourced (http://www.cse.unsw.edu.au/∼corg/tools/conch), opening up new opportunities for other researchers and practitioners to further improve this research. To demonstrate this, we introduce one extension of Conch to accelerate further the three baselines without losing any precision, providing further insights on extending Conch to make precision-efficiency tradeoffs in future research. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",context debloating; IFDS; object sensitivity; Pointer analysis,Budget control; Computer software; Java programming language; Object oriented programming; Scalability; Analysis algorithms; Calling contexts; Combinatorial explosion; Condition; Context debloating; IFDS; Object sensitivity; Object-oriented languages; Pointer analysis; Scalability problems; Efficiency
Reliable Fix Patterns Inferred from Static Checkers for Automated Program Repair,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164234765&doi=10.1145%2f3579637&partnerID=40&md5=b0229f8fd6bb7b7e43596b65a221c635,"Fix pattern-based patch generation is a promising direction in automated program repair (APR). Notably, it has been demonstrated to produce more acceptable and correct patches than the patches obtained with mutation operators through genetic programming. The performance of pattern-based APR systems, however, depends on the fix ingredients mined from fix changes in development histories. Unfortunately, collecting a reliable set of bug fixes in repositories can be challenging. In this article, we propose investigating the possibility in an APR scenario of leveraging fix patterns inferred from code changes that address violations detected by static analysis tools. To that end, we build a fix pattern-based APR tool, Avatar, which exploits fix patterns of static analysis violations as ingredients for the patch generation of repairing semantic bugs. Evaluated on four benchmarks (i.e., Defects4J, Bugs.jar, BEARS, and QuixBugs), Avatar presents the potential feasibility of fixing semantic bugs with the fix patterns inferred from the patches for fixing static analysis violations and can correctly fix 26 semantic bugs when Avatar is implemented with the normal program repair pipeline. We also find that Avatar achieves performance metrics that are comparable to that of the closely related approaches in the literature. Compared with CoCoNut, Avatar can fix 18 new bugs in Defects4J and 3 new bugs in QuixBugs. When compared with HDRepair, JAID, and SketchFix, Avatar can newly fix 14 Defects4J bugs. In terms of the number of correctly fixed bugs, Avatar is also comparable to the program repair tools with the normal fault localization setting and presents better performance than most program repair tools. These results imply that Avatar is complementary to current program repair approaches. We further uncover that Avatar can present different bug-fixing performances when it is configured with different fault localization tools, and the stack trace information from the failed executions of test cases can be exploited to improve the bug-fixing performance of Avatar by fixing more bugs with fewer generated patch candidates. Overall, our study highlights the relevance of static bug-finding tools as indirect contributors of fix ingredients for addressing code defects identified with functional test cases (i.e., dynamic information). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated program repair; fix pattern; static analysis,Automation; Defects; Genetic algorithms; Genetic programming; Program debugging; Repair; Semantics; Automated program repair; Bug-fixing; Development history; Fault localization; Fix pattern; Mutation operators; Performance; Repair system; Repair tools; Test case; Static analysis
Hierarchical and Hybrid Organizational Structures in Open-source Software Projects: A Longitudinal Study,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235722&doi=10.1145%2f3569949&partnerID=40&md5=c28e72484dbcd1cb33a5832cca2bad7a,"Despite the absence of a formal process and a central command-and-control structure, developer organization in open-source software (OSS) projects are far from being a purely random process. Prior work indicates that, over time, highly successful OSS projects develop a hybrid organizational structure that comprises a hierarchical part and a non-hierarchical part. This suggests that hierarchical organization is not necessarily a global organizing principle and that a fundamentally different principle is at play below the lowest positions in the hierarchy. Given the vast proportion of developers are in the non-hierarchical part, we seek to understand the interplay between these two fundamentally differently organized groups, how this hybrid structure evolves, and the trajectory individual developers take through these structures over the course of their participation. We conducted a longitudinal study of the full histories of 20 popular OSS projects, modeling their organizational structures as networks of developers connected by communication ties and characterizing developers' positions in terms of hierarchical (sub)structures in these networks. We observed a number of notable trends and patterns in the subject projects: (1) hierarchy is a pervasive structural feature of developer networks of OSS projects; (2) OSS projects tend to form hybrid organizational structures, consisting of a hierarchical and a non-hierarchical part; and (3) the positional trajectory of a developer starts loosely connected in the non-hierarchical part and then tightly integrate into the hierarchical part, which is associated with the acquisition of experience (tenure), in addition to coordination and coding activities. Our study (a) provides a methodological basis for further investigations of hierarchy formation, (b) suggests a number of hypotheses on prevalent organizational patterns and trends in OSS projects to be addressed in further work, and (c) may ultimately guide the governance of organizational structures. © 2023 Association for Computing Machinery.",developer networks; hierarchy; Open-source software projects; organizational structure,Open source software; Open systems; Command and control; Control structure; Developer network; Hierarchical organizations; Hierarchy; Hybrid structure; Longitudinal study; Open source software projects; Organizational structures; Project modelling; Random processes
DeltaDroid: Dynamic Delivery Testing in Android,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162861065&doi=10.1145%2f3563213&partnerID=40&md5=7447e6791acd44221cad22024d8b5913,"Android is a highly fragmented platform with a diverse set of devices and users. To support the deployment of apps in such a heterogeneous setting, Android has introduced dynamic delivery - a new model of software deployment in which optional, device- or user-specific functionalities of an app, called Dynamic Feature Modules (DFMs), can be installed, as needed, after the app's initial installation. This model of app deployment, however, has exacerbated the challenges of properly testing Android apps. In this article, we first describe the results of an extensive study in which we formalized a defect model representing the various conditions under which DFM installations may fail. We then present DeltaDroid - a tool aimed at assisting the developers with validating dynamic delivery behavior in their apps by augmenting their existing test suite. Our experimental evaluation using real-world apps corroborates DeltaDroid's ability to detect many crashes and unexpected behaviors that the existing automated testing tools cannot reveal. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Android applications; dynamic delivery; Software testing; test augmentation,Ability testing; Android (operating system); Application programs; Android applications; Android apps; Condition; Defect model; Dynamic delivery; Dynamic features; Module installation; Software deployment; Software testings; Test augmentation; Software testing
Uncertainty-Aware Robustness Assessment of Industrial Elevator Systems,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164242017&doi=10.1145%2f3576041&partnerID=40&md5=df544046049021f98967999cbc69d40a,"Industrial elevator systems are commonly used software systems in our daily lives, which operate in uncertain environments such as unpredictable passenger traffic, uncertain passenger attributes and behaviors, and hardware delays. Understanding and assessing the robustness of such systems under various uncertainties enable system designers to reason about uncertainties, especially those leading to low system robustness, and consequently improve their designs and implementations in terms of handling uncertainties. To this end, we present a comprehensive empirical study conducted with industrial elevator systems provided by our industrial partner Orona, which focuses on assessing the robustness of a dispatcher - that is, a software component responsible for elevators' optimal scheduling. In total, we studied 90 industrial dispatchers in our empirical study. Based on the experience gained from the study, we derived an uncertainty-aware robustness assessment method (named UncerRobua) comprising a set of guidelines on how to conduct the robustness assessment and a newly proposed ranking algorithm, for supporting the robustness assessment of industrial elevator systems against uncertainties. © 2023 Association for Computing Machinery.",empirical study; Uncertainty-aware robustness assessment,Traffic surveys; Uncertainty analysis; Daily lives; Elevator systems; Empirical studies; Passenger traffic; Robustness assessment; Software-systems; System designers; Uncertain environments; Uncertainty; Uncertainty-aware robustness assessment; Elevators
What Is the Intended Usage Context of This Model? An Exploratory Study of Pre-Trained Models on Various Model Repositories,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162059219&doi=10.1145%2f3569934&partnerID=40&md5=3a37d5c805cc99dedee5472ecbafa816,"There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card's and FactSheet's guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task's type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSoftware engineering for artificial intelligence; model contract; model reuse; pre-trained models,Artificial intelligence; Codes (symbols); Contracts; Knowledge management; Learning systems; Additional key word and phrasessoftware engineering for artificial intelligence; Condition; Exploratory studies; Key words; Model contract; Model repositories; Model reuse; Pre-trained model; Pre/post conditions; Reuse; Computer software reusability
Precise Quantitative Analysis of Binarized Neural Networks: A BDD-based Approach,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162069611&doi=10.1145%2f3563212&partnerID=40&md5=23c360bb174e4059c9bc8c6332d45223,"As a new programming paradigm, neural-network-based machine learning has expanded its application to many real-world problems. Due to the black-box nature of neural networks, verifying and explaining their behavior are becoming increasingly important, especially when they are deployed in safety-critical applications. Existing verification work mostly focuses on qualitative verification, which asks whether there exists an input (in a specified region) for a neural network such that a property (e.g., local robustness) is violated. However, in many practical applications, such an (adversarial) input almost surely exists, which makes a qualitative answer less meaningful. In this work, we study a more interesting yet more challenging problem, i.e., quantitative verification of neural networks, which asks how often a property is satisfied or violated. We target binarized neural networks (BNNs), the 1-bit quantization of general neural networks. BNNs have attracted increasing attention in deep learning recently, as they can drastically reduce memory storage and execution time with bit-wise operations, which is crucial in recourse-constrained scenarios, e.g., embedded devices for Internet of Things. Toward quantitative verification of BNNs, we propose a novel algorithmic approach for encoding BNNs as Binary Decision Diagrams (BDDs), a widely studied model in formal verification and knowledge representation. By exploiting the internal structure of the BNNs, our encoding translates the input-output relation of blocks in BNNs to cardinality constraints, which are then encoded by BDDs. Based on the new BDD encoding, we develop a quantitative verification framework for BNNs where precise and comprehensive analysis of BNNs can be performed. To improve the scalability of BDD encoding, we also investigate parallelization strategies at various levels. We demonstrate applications of our framework by providing quantitative robustness verification and interpretability for BNNs. An extensive experimental evaluation confirms the effectiveness and efficiency of our approach.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesBinarized neural networks; binary decision diagrams; formal verification; interpretability; robustness,Binary decision diagrams; Boolean functions; Encoding (symbols); Formal verification; Knowledge representation; Safety engineering; Signal encoding; Additional key word and phrasesbinarized neural network; Encodings; Interpretability; Key words; Network-based; Neural-networks; Programming paradigms; Property; Quantitative verification; Robustness; Deep learning
Blindspots in Python and Java APIs Result in Vulnerable Code,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162006557&doi=10.1145%2f3571850&partnerID=40&md5=664207983c675f670e8f37585c577475,"Blindspots in APIs can cause software engineers to introduce vulnerabilities, but such blindspots are, unfortunately, common. We study the effect APIs with blindspots have on developers in two languages by replicating a 109-developer, 24-Java-API controlled experiment. Our replication applies to Python and involves 129 new developers and 22 new APIs. We find that using APIs with blindspots statistically significantly reduces the developers' ability to correctly reason about the APIs in both languages, but that the effect is more pronounced for Python. Interestingly, for Java, the effect increased with complexity of the code relying on the API, whereas for Python, the opposite was true. This suggests that Python developers are less likely to notice potential for vulnerabilities in complex code than in simple code, whereas Java developers are more likely to recognize the extra complexity and apply more care, but are more careless with simple code. Whether the developers considered API uses to be more difficult, less clear, and less familiar did not have an effect on their ability to correctly reason about them. Developers with better long-term memory recall were more likely to correctly reason about APIs with blindspots, but short-term memory, processing speed, episodic memory, and memory span had no effect. Surprisingly, professional experience and expertise did not improve the developers' ability to reason about APIs with blindspots across both languages, with long-term professionals with many years of experience making mistakes as often as relative novices. Finally, personality traits did not significantly affect the Python developers' ability to reason about APIs with blindspots, but less extroverted and more open developers were better at reasoning about Java APIs with blindspots. Overall, our findings suggest that blindspots in APIs are a serious problem across languages, and that experience and education alone do not overcome that problem, suggesting that tools are needed to help developers recognize blindspots in APIs as they write code that uses those APIs.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSoftware vulnerabilities; API blindspots; APIs; Java; Python,Application programming interfaces (API); Codes (symbols); High level languages; Java programming language; Additional key word and phrasessoftware vulnerability; API; API blindspot; Complex codes; Controlled experiment; Java; Java API; Java developers; Key words; Simple++; Python
Simulating Software Evolution to Evaluate the Reliability of Early Decision-making among Design Alternatives toward Maintainability,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161992893&doi=10.1145%2f3569931&partnerID=40&md5=5964a610fef89929340dd67638f5c7dd,"Critical decisions among design altern seventh atives with regards to maintainability arise early in the software design cycle. Existing comparison models relayed on the structural evolution of the used design patterns are suitable to support such decisions. However, their effectiveness on predicting maintenance effort is usually verified on a limited number of case studies under heterogeneous metrics. In this article, a multi-variable simulation model for validating the decision-making reliability of the derived formal comparison models for the significant designing problem of recursive hierarchies of part-whole aggregations, proposed in our prior work, is introduced. In the absence of a strict validation, the simulation model has been thoroughly calibrated concerning its decision-making precision based on empirical distributions from time-series analysis, approximating the highly uncertain nature of actual maintenance process. The decision reliability of the formal models has been statistically validated on a sample of 1,000 instances of design attributes representing the entire design space of the problem. Despite the limited accuracy of measurements, the results show that the models demonstrate an increasing reliability in a long-term perspective, even under assumptions of high variability. Thus, the modeling theory discussed in our prior work delivers reliable models that significantly reduce decision-risk and relevant maintenance cost. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesStatistical validation; design pattern; maintainability quality attribute requirement; software evolution,Decision theory; Maintainability; Quality control; Software design; Software reliability; Time series analysis; Additional key word and phrasesstatistical validation; Comparison models; Decisions makings; Design Patterns; Early decision; Key words; Maintainability quality attribute requirement; Quality attributes; Simulation model; Software Evolution; Decision making
Is My Transaction Done Yet? An Empirical Study of Transaction Processing Times in the Ethereum Blockchain Platform,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162054235&doi=10.1145%2f3549542&partnerID=40&md5=bcaae5bf3edb2ead7cb51aebff738192,"Ethereum is one of the most popular platforms for the development of blockchain-powered applications. These applications are known as ÐApps. When engineering ÐApps, developers need to translate requests captured in the front-end of their application into one or more smart contract transactions. Developers need to pay for these transactions and, the more they pay (i.e., the higher the gas price), the faster the transaction is likely to be processed. Developing cost-effective ÐApps is far from trivial, as developers need to optimize the balance between cost (transaction fees) and user experience (transaction processing times). Online services have been developed to provide transaction issuers (e.g., ÐApp developers) with an estimate of how long transactions will take to be processed given a certain gas price. These estimation services are crucial in the Ethereum domain and several popular wallets such as Metamask rely on them. However, despite their key role, their accuracy has not been empirically investigated so far. In this article, we quantify the transaction processing times in Ethereum, investigate the relationship between processing times and gas prices, and determine the accuracy of state-of-the-practice estimation services. Our results indicate that transactions are processed in a median of 57 seconds and that 90% of the transactions are processed within 8 minutes. We also show that higher gas prices result in faster transaction processing times with diminishing returns. In particular, we observe no practical difference in processing time between expensive and very expensive transactions. With regards to the accuracy of processing time estimation services, we observe that they are equivalent. However, when stratifying transactions by gas prices, we observe that Etherscan's Gas Tracker is the most accurate estimation service for the very cheap and cheap transactions. EthGasStation's Gas Price API, in turn, is the most accurate estimation service for regular, expensive, and very expensive transactions. In a post-hoc study, we design a simple linear regression model with only one feature that outperforms the Gas Tracker for very cheap and cheap transactions and that performs as accurately as the EthGasStation model for the remaining categories. Based on our findings, ÐApp developers can make more informed decisions concerning the choice of the gas price of their application-issued transactions.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTransaction processing time; blockchain; decentralized applications (DApps); Ethereum,Cost effectiveness; Ethereum; Gases; Regression analysis; Accurate estimation; Additional key word and phrasestransaction processing time; Block-chain; Decentralised; Decentralized application; Empirical studies; Gas price; Key words; Processing time; Transaction processing; Blockchain
Automated Identification and Qualitative Characterization of Safety Concerns Reported in UAV Software Platforms,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162030421&doi=10.1145%2f3564821&partnerID=40&md5=3d431dbf63bbaa557c21787b126649e7,"Unmanned Aerial Vehicles (UAVs) are nowadays used in a variety of applications. Given the cyber-physical nature of UAVs, software defects in these systems can cause issues with safety-critical implications. An important aspect of the lifecycle of UAV software is to minimize the possibility of harming humans or damaging properties through a continuous process of hazard identification and safety risk management. Specifically, safety-related concerns typically emerge during the operation of UAV systems, reported by end-users and developers in the form of issue reports and pull requests. However, popular UAV systems daily receive tens or hundreds of reports of varying types and quality. To help developers timely identify and triage safety-critical UAV issues, we (i) experiment with automated approaches (previously used for issue classification) for detecting the safety-related matters appearing in the titles and descriptions of issues and pull requests reported in UAV platforms and (ii) propose a categorization of the main hazards and accidents discussed in such issues. Our results (i) show that shallow machine learning (ML)-based approaches can identify safety-related sentences with precision, recall, and F-measure values of about 80%; and (ii) provide a categorization and description of the relationships between safety issue hazards and accidents. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesUnmanned aerial vehicles; empirical study; issue management; machine learning; safety issues,Aircraft accidents; Aircraft detection; Antennas; Hazards; Life cycle; Risk management; Unmanned aerial vehicles (UAV); Additional key word and phrasesunmanned aerial vehicle; Aerial vehicle; Automated identification; Empirical studies; Issue managements; Key words; Machine-learning; Safety issues; Safety-Related; Unmanned aerial vehicle systems; Machine learning
SLR: From Saltzer and Schroeder to 2021...47 Years of Research on the Development and Validation of Security API Recommendations,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162042025&doi=10.1145%2f3561383&partnerID=40&md5=d3a1a3687f111ca4b8e14abc43d12c3e,"Producing secure software is challenging. The poor usability of security Application Programming Interfaces (APIs) makes this even harder. Many recommendations have been proposed to support developers by improving the usability of cryptography libraries - rooted in wider best practice guidance in software engineering and API design. In this SLR, we systematize knowledge regarding these recommendations. We identify and analyze 65 papers, offering 883 recommendations. Through thematic analysis, we identify seven core ways to improve usability of APIs. Most of the recommendations focus on helping API developers to construct and structure their code and make it more usable and easier for programmers to understand. There is less focus, however, on documentation, writing requirements, code quality assessment, and the impact of organizational software development practices. By tracing and analyzing paper ancestry, we map how this knowledge becomes validated and translated over time. We find that very few API usability recommendations are empirically validated, and that recommendations specific to usable security APIs lag even further behind.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAPI; recommendations; security; SLR; usability,Application programming interfaces (API); Codes (symbols); Usability engineering; Additional key word and phrasesapi; Applications programming interfaces; Best practices; Key words; Recommendation; Secure software; Security; Security application; SLR; Usability; Software design
Refactoring in Computational Notebooks,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162022234&doi=10.1145%2f3576036&partnerID=40&md5=9af7b911d54c2c4694c24713c342573d,"Due to the exploratory nature of computational notebook development, a notebook can be extensively evolved even though it is small, potentially incurring substantial technical debt. Indeed, in interview studies notebook authors have attested to performing ongoing tidying and big cleanups. However, many notebook authors are not trained as software developers, and environments like JupyterLab possess few features to aid notebook maintenance. As software refactoring is traditionally a critical tool for reducing technical debt, we sought to better understand the unique and growing ecology of computational notebooks by investigating the refactoring of public Jupyter notebooks. We randomly selected 15,000 Jupyter notebooks hosted on GitHub and studied 200 with meaningful commit histories. We found that notebook authors do refactor, favoring a few basic classic refactorings as well as those involving the notebook cell construct. Those with a computing background refactored differently than others, but not more so. Exploration-focused notebooks had a unique refactoring profile compared to more exposition-focused notebooks. Authors more often refactored their code as they went along, rather than deferring maintenance to big cleanups. These findings point to refactoring being intrinsic to notebook development.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesComputational notebooks; end-user programming; refactoring,Additional key word and phrasescomputational notebook; End-user programming; End-users; Interview study; Key words; Refactorings; Software developer; Software environments; Software refactoring; Technical debts; Computer programming
I Know What You Are Searching for: Code Snippet Recommendation from Stack Overflow Posts,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137092769&doi=10.1145%2f3550150&partnerID=40&md5=ad626aca0e05e499ce8587b255edfe2f,"Stack Overflow has been heavily used by software developers to seek programming-related information. More and more developers use Community Question and Answer forums, such as Stack Overflow, to search for code examples of how to accomplish a certain coding task. This is often considered to be more efficient than working from source documentation, tutorials, or full worked examples. However, due to the complexity of these online Question and Answer forums and the very large volume of information they contain, developers can be overwhelmed by the sheer volume of available information. This makes it hard to find and/or even be aware of the most relevant code examples to meet their needs. To alleviate this issue, in this work, we present a query-driven code recommendation tool, named Que2Code, that identifies the best code snippets for a user query from Stack Overflow posts. Our approach has two main stages: (i) semantically equivalent question retrieval and (ii) best code snippet recommendation. During the first stage, for a given query question formulated by a developer, we first generate paraphrase questions for the input query as a way of query boosting and then retrieve the relevant Stack Overflow posted questions based on these generated questions. In the second stage, we collect all of the code snippets within questions retrieved in the first stage and develop a novel scheme to rank code snippet candidates from Stack Overflow posts via pairwise comparisons. To evaluate the performance of our proposed model, we conduct a large-scale experiment to evaluate the effectiveness of the semantically equivalent question retrieval task and best code snippet recommendation task separately on Python and Java datasets in Stack Overflow. We also perform a human study to measure how real-world developers perceive the results generated by our model. Both the automatic and human evaluation results demonstrate the promising performance of our model, and we have released our code and data to assist other researchers. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesCode Search; Duplicate questions; paraphrase mining; Stack Overflow,Python; Query processing; Additional key word and phrasescode search; Duplicate question; Key words; Large volumes; Paraphrase mining; Performance; Software developer; Stack overflow; User query; Worked examples; Large dataset
Patching Locking Bugs Statically with Crayons,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162003734&doi=10.1145%2f3548684&partnerID=40&md5=9e40194b48875babf3dc3987de018fe5,"The Linux Kernel is a world-class operating system controlling most of our computing infrastructure: mobile devices, Internet routers and services, and most of the supercomputers. Linux is also an example of low-level software with no comprehensive regression test suite (for good reasons). The kernel's tremendous societal importance imposes strict stability and correctness requirements. These properties make Linux a challenging and relevant target for static automated program repair (APR). Over the past decade, a significant progress has been made in dynamic APR. However, dynamic APR techniques do not translate naturally to systems without tests. We present a static APR technique addressing sequential locking API misuse bugs in the Linux Kernel. We attack the key challenge of static APR, namely, the lack of detailed program specification, by combining static analysis with machine learning to complement the information presented by the static analyzer. In experiments on historical real-world bugs in the kernel, we were able to automatically re-produce or propose equivalent patches in 85% of the human-made patches, and automatically rank them among the top three candidates for 64% of the cases and among the top five for 74%. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesAutomated repair; api misuse; static program repair,Linux; Locks (fasteners); Program debugging; Repair; Software testing; Supercomputers; Additional key word and phrasesautomated repair; Api misuse; Computing infrastructures; Internet-services; Key words; Linux kernel; Repair techniques; Static program; Static program repair; World class; Static analysis
Security Responses in Software Development,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162071717&doi=10.1145%2f3563211&partnerID=40&md5=36ca67a0c746355916ef5c35204a8e10,"The pressure on software developers to produce secure software has never been greater. But what does security look like in environments that do not produce security-critical software? In answer to this question, this multi-sited ethnographic study characterizes security episodes and identifies five typical behaviors in software development. Using theory drawn from information security and motivation research in software engineering, this article characterizes key ways in which individual developers form security responses to meet the demands of particular circumstances, providing a framework managers and teams can use to recognize, understand, and alter security activity in their environments.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSecurity; developers; software engineering,Human resource management; Security of data; Additional key word and phrasessecurity; Critical software; Developer; Ethnographic study; Key words; Secure software; Security activities; Security-critical; Software developer; Software design
Storage State Analysis and Extraction of Ethereum Blockchain Smart Contracts,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162055590&doi=10.1145%2f3548683&partnerID=40&md5=a48a96119e3616e5eb66552c35a63342,"In migrating and upgrading an Ethereum smart contract, it is necessary to transfer both the code as well as the stored data. Various methods attempt to migrate or upgrade a smart contract, but they are mostly manual, error-prone, and applicable only before deployment. Further, they have challenges in extracting the storage state of complex mapping data structures along with their keys. In this work, we present Smartmuv as an automatic source-code-based static analysis tool to analyze and extract the state from the storage-trie of smart contracts. Based on the abstract syntax tree and the control flow graphs of the Solidity source code, the tool analyzes each state variable including mapping types along the inheritance hierarchy. It also provides the upgrade algorithm that initializes the extracted state in the constructor of new smart contract. Smartmuv safely approximates the origin of the keys used in the mapping to extract values and has been able to extract the mapping state of 23,673 smart contracts with 95.7% overall precision. Moreover, we also validate the Smartmuv's extracted state with the third-party tool Etherscan.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesBlockchain; compiler; Smartmuv; Solidity; source code; state extraction; upgrade,Blockchain; Codes (symbols); Digital storage; Ethereum; Extraction; Flow graphs; Mapping; Static analysis; Trees (mathematics); Additional key word and phrasesblockchain; Compiler; Key words; Smartmuv; Solidity; Source codes; State analysis; State extraction; Storage state; Upgrade; Smart contract
Do Performance Aspirations Matter for Guiding Software Configuration Tuning? An Empirical Investigation under Dual Performance Objectives,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146836774&doi=10.1145%2f3571853&partnerID=40&md5=5a2a51385b2d8ef0d90f59141698a63c,"Configurable software systems can be tuned for better performance. Leveraging on some Pareto optimizers, recent work has shifted from tuning for a single, time-related performance objective to two intrinsically different objectives that assess distinct performance aspects of the system, each with varying aspirations to be satisfied, e.g., ""the latency is less than 10s""while ""the memory usage is no more than 1GB"". Before we design better optimizers, a crucial engineering decision to make therein is how to handle the performance requirements with clear aspirations in the tuning process. For this, the community takes two alternative optimization models: either quantifying and incorporating the aspirations into the search objectives that guide the tuning, or not considering the aspirations during the search but purely using them in the later decision-making process only. However, despite being a crucial decision that determines how an optimizer can be designed and tailored, there is a rather limited understanding of which optimization model should be chosen under what particular circumstance, and why.In this article, we seek to close this gap. Firstly, we do that through a review of over 426 articles in the literature and 14 real-world requirements datasets, from which we summarize four performance requirement patterns that quantify the aspirations in the configuration tuning. Drawing on these, we then conduct a comprehensive empirical study that covers 15 combinations of the state-of-the-art performance requirement patterns, four types of aspiration space, three Pareto optimizers, and eight real-world systems/environments, leading to 1,296 cases of investigation. Our findings reveal that (1) the realism of aspirations is the key factor that determines whether they should be used to guide the tuning; (2) the given patterns and the position of the realistic aspirations in the objective landscape are less important for the choice, but they do matter to the extents of improvement; (3) the available tuning budget can also influence the choice for unrealistic aspirations but it is insignificant under realistic ones. To promote open science practice, we make our code and dataset publicly available at: https://github.com/ideas-labo/aspiration-study.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSearch-Based Software Engineering; multi-objective optimization; performance aspiration; performance requirement; software configuration tuning,Budget control; Decision making; Pareto principle; Software engineering; Additional key word and phrasessearch-based software engineering; Key words; Multi-objectives optimization; Optimizers; Performance; Performance aspiration; Performance objective; Performance requirements; Software configuration; Software configuration tuning; Multiobjective optimization
Black-box Safety Analysis and Retraining of DNNs based on Feature Extraction and Clustering,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162041066&doi=10.1145%2f3550271&partnerID=40&md5=cf80b7f1f510219f8c7f68893d7742ab,"Deep neural networks (DNNs) have demonstrated superior performance over classical machine learning to support many features in safety-critical systems. Although DNNs are now widely used in such systems (e.g., self driving cars), there is limited progress regarding automated support for functional safety analysis in DNN-based systems. For example, the identification of root causes of errors, to enable both risk analysis and DNN retraining, remains an open problem. In this article, we propose SAFE, a black-box approach to automatically characterize the root causes of DNN errors. SAFE relies on a transfer learning model pre-trained on ImageNet to extract the features from error-inducing images. It then applies a density-based clustering algorithm to detect arbitrary shaped clusters of images modeling plausible causes of error. Last, clusters are used to effectively retrain and improve the DNN. The black-box nature of SAFE is motivated by our objective not to require changes or even access to the DNN internals to facilitate adoption. Experimental results show the superior ability of SAFE in identifying different root causes of DNN errors based on case studies in the automotive domain. It also yields significant improvements in DNN accuracy after retraining, while saving significant execution time and memory when compared to alternatives. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesDNN explanation; clustering; DNN debugging; DNN functional safety analysis; transfer learning,Clustering algorithms; Errors; Learning systems; Risk analysis; Risk assessment; Safety engineering; Additional key word and phrasesdnn explanation; Clusterings; Deep neural network debugging; Deep neural network functional safety analyse; Functional Safety; Key words; Network debugging; Root cause; Safety analysis; Transfer learning; Deep neural networks
Seeing the Whole Elephant: Systematically Understanding and Uncovering Evaluation Biases in Automated Program Repair,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161982187&doi=10.1145%2f3561382&partnerID=40&md5=c5af7c7df149502ee029204053597e14,"Evaluation is the foundation of automated program repair (APR), as it provides empirical evidence on strengths and weaknesses of APR techniques. However, the reliability of such evaluation is often threatened by various introduced biases. Consequently, bias exploration, which uncovers biases in the APR evaluation, has become a pivotal activity and performed since the early years when pioneer APR techniques were proposed. Unfortunately, there is still no methodology to support a systematic comprehension and discovery of evaluation biases in APR, which impedes the mitigation of such biases and threatens the evaluation of APR techniques.In this work, we propose to systematically understand existing evaluation biases by rigorously conducting the first systematic literature review on existing known biases and systematically uncover new biases by building a taxonomy that categorizes evaluation biases. As a result, we identify 17 investigated biases and uncover a new bias in the usage of patch validation strategies. To validate this new bias, we devise and implement an executable framework APRConfig, based on which we evaluate three typical patch validation strategies with four representative heuristic-based and constraint-based APR techniques on three bug datasets. Overall, this article distills 13 findings for bias understanding, discovery, and validation. The systematic exploration we performed and the open source executable framework we proposed in this article provide new insights as well as an infrastructure for future exploration and mitigation of biases in APR evaluation.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesAutomated program repair; bias study; empirical evaluation,Open source software; Petroleum reservoir evaluation; Additional key word and phrasesautomated program repair; Bias studies; Constraint-based; Empirical evaluations; Executables; Key words; Repair techniques; Systematic exploration; Systematic literature review; Validation strategies; Repair
Anchor: Fast and Precise Value-flow Analysis for Containers via Memory Orientation,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162023553&doi=10.1145%2f3565800&partnerID=40&md5=f52d9f7cd2493d7002924243d0084dc1,"Containers are ubiquitous data structures that support a variety of manipulations on the elements, inducing the indirect value flows in the program. Tracking value flows through containers is stunningly difficult, because it depends on container memory layouts, which are expensive to be discovered.This work presents a fast and precise value-flow analysis framework called Anchor for the programs using containers. We introduce the notion of anchored containers and propose the memory orientation analysis to construct a precise value-flow graph. Specifically, we establish a combined domain to identify anchored containers and apply strong updates to container memory layouts. Anchor finally conducts a demand-driven reachability analysis in the value-flow graph for a client. Experiments show that it removes 17.1% spurious statements from thin slices and discovers 20 null pointer exceptions with 9.1% as its false-positive ratio, while the smashing-based analysis reports 66.7% false positives. Anchor scales to millions of lines of code and checks the program with around 5.12 MLoC within 5 hours.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesAbstract interpretation; data structure analysis; value-flow analysis,Data structures; Flow graphs; Graphic methods; Value engineering; Additional key word and phrasesabstract interpretation; Data structure analyse; False positive; Flow-graphs; Key words; Memory layout; Structure analysis; Ubiquitous data; Value flow; Value flow analysis; Containers
Input Distribution Coverage: Measuring Feature Interaction Adequacy in Neural Network Testing,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162030589&doi=10.1145%2f3576040&partnerID=40&md5=586e1ad29cc664ebc0c7d7bc84e43418,"Testing deep neural networks (DNNs) has garnered great interest in the recent years due to their use in many applications. Black-box test adequacy measures are useful for guiding the testing process in covering the input domain. However, the absence of input specifications makes it challenging to apply black-box test adequacy measures in DNN testing. The Input Distribution Coverage (IDC) framework addresses this challenge by using a variational autoencoder to learn a low dimensional latent representation of the input distribution, and then using that latent space as a coverage domain for testing. IDC applies combinatorial interaction testing on a partitioning of the latent space to measure test adequacy. Empirical evaluation demonstrates that IDC is cost-effective, capable of detecting feature diversity in test inputs, and more sensitive than prior work to test inputs generated using different DNN test generation methods. The findings demonstrate that IDC overcomes several limitations of white-box DNN coverage approaches by discounting coverage from unrealistic inputs and enabling the calculation of test adequacy metrics that capture the feature diversity present in the input space of DNNs.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSoftware testing; deep neural networks; generative models; test coverage,Black-box testing; Cost effectiveness; Additional key word and phrasessoftware testing; Black box test; Feature interactions; Generative model; Input distributions; Key words; Neural-networks; Test adequacies; Test inputs; Test-coverage; Deep neural networks
Continuous Integration and Delivery Practices for Cyber-Physical Systems: An Interview-Based Study,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162002833&doi=10.1145%2f3571854&partnerID=40&md5=8c5f68470ca18aba99495f5eff0de260,"Continuous Integration and Delivery (CI/CD) practices have shown several benefits for software development and operations, such as faster release cycles and early discovery of defects. For Cyber-Physical System (CPS) development, CI/CD can help achieving required goals, such as high dependability, yet it may be challenging to apply. This article empirically investigates challenges, barriers, and their mitigation occurring when applying CI/CD practices to develop CPSs in 10 organizations working in eight different domains. The study has been conducted through semi-structured interviews, by applying an open card sorting procedure together with a member-checking survey within the same organizations, and by validating the results through a further survey involving 55 professional developers. The study reveals several peculiarities in the application of CI/CD to CPSs. These include the need for (i) combining continuous and periodic builds while balancing the use of Hardware-in-the-Loop and simulators, (ii) coping with difficulties in software deployment (iii) accounting for simulators and Hardware-in-the-Loop differing in their behavior, and (vi) combining hardware/software expertise in the development team. Our findings open the road toward recommenders aimed at supporting the setting and evolution of CI/CD pipelines, as well as university curricula requiring interdisciplinarity, such as knowledge about hardware, software, and their interplay.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesContinuous Integration and Delivery; Cyber-Physical Systems; empirical software engineering,Balancing; Embedded systems; Hardware-in-the-loop simulation; Integration; Software design; Synthetic apertures; Additional key word and phrasescontinuous integration and delivery; Continuous integrations; Cybe-physical systems; Cyber-physical systems; Development and operations; Empirical Software Engineering; Hardware in the loops; Hardware/software; Key words; Release cycles; Cyber Physical System
Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162013722&doi=10.1145%2f3560264&partnerID=40&md5=d5106d2506239588b72d9ca4cc54b0b6,"With the development of decentralized networks, smart contracts, especially those for ERC tokens, are attracting more and more Dapp users to implement their applications. There are some functions in ERC token contracts that only a specific group of accounts could invoke. Among those functions, some even can influence other accounts or the whole system without prior notice or permission. These functions are referred to as contract backdoors. Once exploited by an attacker, they can cause property losses and harm users' privacy.In this work, we propose Pied-Piper, a hybrid analysis method that integrates datalog analysis and directed fuzzing to detect backdoor threats in Ethereum ERC token contracts. First, datalog analysis is applied to abstract the data structures and identification rules related to the threats for preliminary static detection. Then, directed fuzzing is applied to eliminate false positives caused by the static analysis. We first evaluated Pied-Piper on 200 smart contracts, which are injected with different types of backdoors. It reported all problems without false positives, and none of the injected problems was missed. Then, we applied Pied-Piper on 13,484 real token contracts deployed on Ethereum. Pied-Piper reported 189 confirmed problems, four of which have been assigned unique CVE ids while others are still in the review process. Each contract takes 8.03 seconds for datalog analysis on average, and the fuzzing engine can eliminate the false positives within one minute.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSmart contract; backdoor detection; datalog analysis; directed fuzzing,Ethereum; Static analysis; Additional key word and phrasessmart contract; Backdoor detections; Backdoors; Datalog; Datalog analyse; Decentralized networks; Directed fuzzing; False positive; Key words; Property loss; Smart contract
How the Quality of Maintenance Tasks is Affected by Criteria for Selecting Engineers for Collaboration,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161972397&doi=10.1145%2f3561384&partnerID=40&md5=72344b04afb551cf16745fad3fc7a74e,"In industry, software projects might span over decades, with many engineers joining or leaving the company over time. In these circumstances, no single engineer has all of the knowledge when maintenance tasks such as Traceability Link Recovery (TLR), Bug Localization (BL), and Feature Location (FL) are performed. Thus, collaboration has the potential to boost the quality of maintenance tasks since the solution advanced by one engineer might be enhanced with contributions from other engineers. However, assembling a team of software engineers to collaborate may not be as intuitive as we might think. In the context of a worldwide industrial supplier of railway solutions, this work evaluates how the quality of TLR, BL, and FL is affected by the criteria for selecting engineers for collaboration. The criteria for collaboration are based on engineers' profile information to select the set of search queries that are involved in the maintenance task. Collaboration is achieved by applying automatic query reformulation, and the location relies on an evolutionary algorithm. Our work uncovers how software engineers who might be seen as not being relevant in the collaboration can lead to significantly better results. A focus group confirmed the relevance of the findings.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesCollaborative software engineering; model-driven engineering; search-based software engineering,Computer software maintenance; Additional key word and phrasescollaborative software engineering; Bug localizations; Feature location; Key words; Link recoveries; Maintenance tasks; Model-driven Engineering; Search-based; Search-based software engineering; Traceability links; Engineers
A Theory of Scrum Team Effectiveness,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161980325&doi=10.1145%2f3571849&partnerID=40&md5=823cebc7e38c8a1d3fdf9697c4ee4944,"Scrum teams are at the heart of the Scrum framework. Nevertheless, an integrated and systemic theory that can explain what makes some Scrum teams more effective than others is still missing. To address this gap, we performed a 7-year-long mixed-methods investigation composed of two main phases. First, we induced a theoretical model from 13 exploratory field studies. Our model proposes that the effectiveness of Scrum teams depends on five high-level factors (responsiveness, stakeholder concern, continuous improvement, team autonomy, and management support) and 13 lower-level factors. In the second phase of our study, we validated our model with a covariance-based structural equation modeling analysis using data from about 5,000 developers and 2,000 Scrum teams that we gathered with a custom-built survey. Results suggest a very good fit of the empirical data in our theoretical model (CFI = 0.959, RMSEA = 0.038, SRMR = 0.035). Accordingly, this research allowed us to (1) propose and validate a generalizable theory for effective Scrum teams and (2) formulate clear recommendations for how organizations can better support Scrum teams.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesAgile; case studies; Scrum; structural equation modeling; teams,Additional key word and phrasesagile; Case-studies; Key words; Mixed method; Scra; Still missing; Structural equation models; Team; Team effectiveness; Theoretical modeling; Human resource management
Similarity-based Web Element Localization for Robust Test Automation,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159145393&doi=10.1145%2f3571855&partnerID=40&md5=3e20c6f16e9c69dfbea6e814f5ff11d4,"Non-robust (fragile) test execution is a commonly reported challenge in GUI-based test automation, despite much research and several proposed solutions. A test script needs to be resilient to (minor) changes in the tested application but, at the same time, fail when detecting potential issues that require investigation. Test script fragility is a multi-faceted problem. However, one crucial challenge is how to reliably identify and locate the correct target web elements when the website evolves between releases or otherwise fail and report an issue. This article proposes and evaluates a novel approach called similarity-based web element localization (Similo), which leverages information from multiple web element locator parameters to identify a target element using a weighted similarity score. This experimental study compares Similo to a baseline approach for web element localization. To get an extensive empirical basis, we target 48 of the most popular websites on the Internet in our evaluation. Robustness is considered by counting the number of web elements found in a recent website version compared to how many of these existed in an older version. Results of the experiment show that Similo outperforms the baseline; it failed to locate the correct target web element in 91 out of 801 considered cases (i.e., 11%) compared to 214 failed cases (i.e., 27%) for the baseline approach. The time efficiency of Similo was also considered, where the average time to locate a web element was determined to be 4 milliseconds. However, since the cost of web interactions (e.g., a click) is typically on the order of hundreds of milliseconds, the additional computational demands of Similo can be considered negligible. This study presents evidence that quantifying the similarity between multiple attributes of web elements when trying to locate them, as in our proposed Similo approach, is beneficial. With acceptable efficiency, Similo gives significantly higher effectiveness (i.e., robustness) than the baseline web element localization approach.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesGUI testing; test automation; test case robustness; web element locators; XPath locators,Automation; Efficiency; Additional key word and phrasesgui testing; Key words; Localisation; Robust tests; Test Automation; Test case; Test case robustness; Test scripts; Web element locator; Xpath locator; Websites
Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161990099&doi=10.1145%2f3550270&partnerID=40&md5=f1f17ad44fe6f1c6f2699a05406c94d5,"Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS's final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS's parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSoftware testing; autonomous driving; coverage criteria; mutation analysis,Cost functions; Software testing; Uncertainty analysis; Additional key word and phrasessoftware testing; Autonomous driving; Coverage criteria; Driving systems; Key words; Mutation analysis; Parameterized; Rules based systems; Simulated environment; Uncertainty; Autonomous vehicles
Route: Roads Not Taken in UI Testing,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162831948&doi=10.1145%2f3571851&partnerID=40&md5=cf0266964b457df61a0ee5cd5285b44c,"Core features (functionalities) of an app can often be accessed and invoked in several ways, i.e., through alternative sequences of user-interface (UI) interactions. Given the manual effort of writing tests, developers often only consider the typical way of invoking features when creating the tests (i.e., the ""sunny day scenario""). However, the alternative ways of invoking a feature are as likely to be faulty. These faults would go undetected without proper tests. To reduce the manual effort of creating UI tests and help developers more thoroughly examine the features of apps, we present Route, an automated tool for feature-based UI test augmentation for Android apps. Route first takes a UI test and the app under test as input. It then applies novel heuristics to find additional high-quality UI tests, consisting of both inputs and assertions, that verify the same feature as the original test in alternative ways. Application of Route on several dozen tests for popular apps on Google Play shows that for 96% of the existing tests, Route was able to generate at least one alternative test. Moreover, the fault detection effectiveness of augmented test suites in our experiments showed substantial improvements of up to 39% over the original test suites. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",GUI test augmentation; mobile testing; test amplification; test reuse,Fault detection; Software testing; Automated tools; Core features; GUI test augmentation; Interface interaction; Interface testings; Mobile testing; Reuse; Sunny days; Test amplifications; Test reuse; User interfaces
Influential Global and Local Contexts Guided Trace Representation for Fault Localization,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161984151&doi=10.1145%2f3576043&partnerID=40&md5=b5aa64aaca47595569cb9e69becb2529,"Trace data is critical for fault localization (FL) to analyze suspicious statements potentially responsible for a failure. However, existing trace representation meets its bottleneck mainly in two aspects: (1) the trace information of a statement is restricted to a local context (i.e., a test case) without the consideration of a global context (i.e., all test cases of a test suite); (2) it just uses the goccurrence' for representation without strong FL semantics. Thus, we propose UNITE: an inflUential coNtext-GuIded Trace rEpresentation, representing the trace from both global and local contexts with influential semantics for FL. UNITE embodies and implements two key ideas: (1) UNITE leverages the widely used weighting capability from local and global contexts of information retrieval to reflect how important a statement (a word) is to a test case (a document) in all test cases of a test suite (a collection), where a test case (a document) and all test cases of a test suite (a collection) represent local and global contexts respectively; (2) UNITE further elaborates the trace representation from goccurrence' (weak semantics) to ginfluence' (strong semantics) by combing program dependencies. The large-scale experiments on 12 FL techniques and 20 programs show that UNITE significantly improves FL effectiveness.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesFault localization; program dependence; statement weighting; suspiciousness; trace representation,Software testing; Additional key word and phrasesfault localization; Fault localization; Global context; Key words; Localisation; Program dependence; Statement weighting; Suspiciousness; Test case; Trace representations; Semantics
Exploring Better Black-Box Test Case Prioritization via Log Analysis,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143859058&doi=10.1145%2f3569932&partnerID=40&md5=59033c83efcb735d2bb8ae45cd5fb6b0,"Test case prioritization (TCP) has been widely studied in regression testing, which aims to optimize the execution order of test cases so as to detect more faults earlier. TCP has been divided into white-box test case prioritization (WTCP) and black-box test case prioritization (BTCP). WTCP can achieve better prioritization effectiveness by utilizing source code information, but is not applicable in many practical scenarios (where source code is unavailable, e.g., outsourced testing). BTCP has the benefit of not relying on source code information, but tends to be less effective than WTCP. That is, both WTCP and BTCP suffer from limitations in the practical use.To improve the practicability of TCP, we aim to explore better BTCP, significantly bridging the effectiveness gap between BTCP and WTCP. In this work, instead of statically analyzing test cases themselves in existing BTCP techniques, we conduct the first study to explore whether this goal can be achieved via log analysis. Specifically, we propose to mine test logs produced during test execution to more sufficiently reflect test behaviors, and design a new BTCP framework (called LogTCP), including log pre-processing, log representation, and test case prioritization components. Based on the LogTCP framework, we instantiate seven log-based BTCP techniques by combining different log representation strategies with different prioritization strategies.We conduct an empirical study to explore the effectiveness of LogTCP. Based on 10 diverse open-source Java projects from GitHub, we compared LogTCP with three representative BTCP techniques and four representative WTCP techniques. Our results show that all of our LogTCP techniques largely perform better than all the BTCP techniques in average fault detection, to the extent that they become competitive to the WTCP techniques. That demonstrates the great potential of logs in practical TCP.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTest case prioritization; log analysis; regression testing,Black-box testing; Codes (symbols); Open source software; Transmission control protocol; Additional key word and phrasestest case prioritization; Black box test; Key words; Log analysis; Prioritization; Prioritization techniques; Regression testing; Test case; Test case prioritization; White box; Fault detection
Coverage-Based Debloating for Java Bytecode,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147732395&doi=10.1145%2f3546948&partnerID=40&md5=fce2bf799494a2bb31342c8083cca2fd,"Software bloat is code that is packaged in an application but is actually not necessary to run the application. The presence of software bloat is an issue for security, performance, and for maintenance. In this article, we introduce a novel technique for debloating, which we call coverage-based debloating. We implement the technique for one single language: Java bytecode. We leverage a combination of state-of-the-art Java bytecode coverage tools to precisely capture what parts of a project and its dependencies are used when running with a specific workload. Then, we automatically remove the parts that are not covered, in order to generate a debloated version of the project. We succeed to debloat 211 library versions from a dataset of 94 unique open-source Java libraries. The debloated versions are syntactically correct and preserve their original behaviour according to the workload. Our results indicate that 68.3% of the libraries' bytecode and 20.3% of their total dependencies can be removed through coverage-based debloating.For the first time in the literature on software debloating, we assess the utility of debloated libraries with respect to client applications that reuse them. We select 988 client projects that either have a direct reference to the debloated library in their source code or which test suite covers at least one class of the libraries that we debloat. Our results show that 81.5% of the clients, with at least one test that uses the library, successfully compile and pass their test suite when the original library is replaced by its debloated version.  © 2023 Association for Computing Machinery.",bytecode; code coverage; program specialization; Software bloat; software maintenance,Application programs; Computer software maintenance; Computer software reusability; Java programming language; Open source software; Bytecodes; Code coverage; Java byte codes; Java library; Novel techniques; Open-source; Program specialization; Security performance; Software bloat; State of the art; Libraries
A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153762223&doi=10.1145%2f3533021&partnerID=40&md5=f72b329810db9e53b390a46e0666a69c,"Users frequently interact with software systems through data entry forms. However, form filling is time-consuming and error-prone. Although several techniques have been proposed to auto-complete or pre-fill fields in the forms, they provide limited support to help users fill categorical fields, i.e., fields that require users to choose the right value among a large set of options.In this article, we propose LAFF, a learning-based automated approach for filling categorical fields in data entry forms. LAFF first builds Bayesian Network models by learning field dependencies from a set of historical input instances, representing the values of the fields that have been filled in the past. To improve its learning ability, LAFF uses local modeling to effectively mine the local dependencies of fields in a cluster of input instances. During the form filling phase, LAFF uses such models to predict possible values of a target field, based on the values in the already-filled fields of the form and their dependencies; the predicted values (endorsed based on field dependencies and prediction confidence) are then provided to the end-user as a list of suggestions.We evaluated LAFF by assessing its effectiveness and efficiency in form filling on two datasets, one of them proprietary from the banking domain. Experimental results show that LAFF is able to provide accurate suggestions with a Mean Reciprocal Rank value above 0.73. Furthermore, LAFF is efficient, requiring at most 317 ms per suggestion.  © 2023 Association for Computing Machinery.",data entry forms; Form filling; machine learning; software data quality; user interfaces,Bayesian networks; Filling; User profile; Automated approach; Bayesian network models; Data entry form; Error prones; Field dependencies; Form filling; Machine learning approaches; Machine-learning; Software data quality; Software-systems; Machine learning
Assessing the Alignment between the Information Needs of Developers and the Documentation of Programming Languages: A Case Study on Rust,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153732825&doi=10.1145%2f3546945&partnerID=40&md5=797e36559e3c6bd001b5c4169136726c,"Programming language documentation refers to the set of technical documents that provide application developers with a description of the high-level concepts of a language (e.g., manuals, tutorials, and API references). Such documentation is essential to support application developers in effectively using a programming language. One of the challenges faced by documenters (i.e., personnel that design and produce documentation for a programming language) is to ensure that documentation has relevant information that aligns with the concrete needs of developers, defined as the missing knowledge that developers acquire via voluntary search. In this article, we present an automated approach to support documenters in evaluating the differences and similarities between the concrete information need of developers and the current state of documentation (a problem that we refer to as the topical alignment of a programming language documentation). Our approach leverages semi-supervised topic modelling that uses domain knowledge to guide the derivation of topics. We initially train a baseline topic model from a set of Rust-related Q&A posts. We then use this baseline model to determine the distribution of topic probabilities of each document of the official Rust documentation. Afterwards, we assess the similarities and differences between the topics of the Q&A posts and the official documentation. Our results show a relatively high level of topical alignment in Rust documentation. Still, information about specific topics is scarce in both the Q&A websites and the documentation, particularly related topics with programming niches such as network, game, and database development. For other topics (e.g., related topics with language features such as structs, patterns and matchings, and foreign function interface), information is only available on Q&A websites while lacking in the official documentation. Finally, we discuss implications for programming language documenters, particularly how to leverage our approach to prioritize topics that should be added to the documentation.  © 2023 Association for Computing Machinery.",Documentation; domain knowledge; programming languages; Q&A websites; Rust; RustForum; Stack Overflow; topic models,Alignment; Application programming interfaces (API); Concretes; Domain Knowledge; High level languages; Probability distributions; Application developers; Case-studies; Documentation; Domain knowledge; Q&A website; Rust; Rustforum; Stack overflow; Technical documents; Topic Modeling; Websites
Fuzzing Configurations of Program Options - RCR Report,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153764659&doi=10.1145%2f3580601&partnerID=40&md5=43aeee915d1425e56f045e1408b12b5c,"This artifact contains the source code and instructions to reproduce the evaluation results of the article ""Fuzzing Configurations of Program Options.""The source code includes the configuration grammars for six target programs, the scripts to generate configuration stubs, and the scripts to post-process fuzzing results. The README of the artifact includes the steps to prepare the experimental environment on a clean Ubuntu machine and step-by-step commands to reproduce the evaluation experiments. A VirtualBox image with ConfigFuzz properly set up is also included.  © 2023 Copyright held by the owner/author(s).",command-line option configurations; Fuzzing,Command line; Command-line option configuration; Configuration grammars; Evaluation experiments; Evaluation results; Experimental environment; Fuzzing; Post process; Source codes
EDITORIAL: Announcing Six TOSEM Issues Per Year,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152690079&doi=10.1145%2f3583569&partnerID=40&md5=aead8d1a7c2fb6668b640249b5a30492,[No abstract available],,
Dissecting American Fuzzy Lop - A FuzzBench Evaluation - RCR Report,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153845124&doi=10.1145%2f3580600&partnerID=40&md5=470212162f7910fdc4b32a860f178d08,"This report describes the artifacts of the ""Dissecting American Fuzzy Lop - A FuzzBench Evaluation""paper. The artifacts are available online at https://github.com/eurecom-s3/dissecting_afl and archived at https://doi.org/10.6084/m9.figshare.21401280. American Fuzzy Lop (AFL) consists of the produced code, the setup to run the experiments in FuzzBench, and the generated reports. We claim the Functional badge as the patches to AFL are easy to enable and the experiments are easy to run thanks to the FuzzBench service, but the evaluations are self-contained and the modifications to AFL are as is. For the purpose of reproducing the experiments, no particular skills are needed as the process is straightforward and described in https://google.github.io/fuzzbench/getting-started/adding-a-new-fuzzer/#requesting-an-experiment. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",afl; fuzzbench; fuzzing,Afl; Fuzzbench; Fuzzing; HTTP
Single and Multi-objective Test Cases Prioritization for Self-driving Cars in Virtual Environments,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153733332&doi=10.1145%2f3533818&partnerID=40&md5=92e733676c0d01b6bb77fa2fd6df78e4,"Testing with simulation environments helps to identify critical failing scenarios for self-driving cars (SDCs). Simulation-based tests are safer than in-field operational tests and allow detecting software defects before deployment. However, these tests are very expensive and are too many to be run frequently within limited time constraints.In this article, we investigate test case prioritization techniques to increase the ability to detect SDC regression faults with virtual tests earlier. Our approach, called SDC-Prioritizer, prioritizes virtual tests for SDCs according to static features of the roads we designed to be used within the driving scenarios. These features can be collected without running the tests, which means that they do not require past execution results. We introduce two evolutionary approaches to prioritize the test cases using diversity metrics (black-box heuristics) computed on these static features. These two approaches, called SO-SDC-Prioritizer and MO-SDC-Prioritizer, use single-objective and multi-objective genetic algorithms (GA), respectively, to find trade-offs between executing the less expensive tests and the most diverse test cases earlier.Our empirical study conducted in the SDC domain shows that MO-SDC-Prioritizer significantly (P- value <=0.1e-10) improves the ability to detect safety-critical failures at the same level of execution time compared to baselines: random and greedy-based test case orderings. Besides, our study indicates that multi-objective meta-heuristics outperform single-objective approaches when prioritizing simulation-based tests for SDCs.MO-SDC-Prioritizer prioritizes test cases with a large improvement in fault detection while its overhead (up to 0.45% of the test execution cost) is negligible.  © 2023 Copyright held by the owner/author(s).",Autonomous systems; software simulation; test case prioritization,Autonomous vehicles; Economic and social effects; Fault detection; Safety engineering; Software testing; Virtual reality; Autonomous system; In-field; Multi-objective tests; Simulation environment; Single objective; Software simulation; Static features; Test case; Test case prioritization; Virtual tests; Genetic algorithms
On Proving the Correctness of Refactoring Class Diagrams of MDE Metamodels,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143706230&doi=10.1145%2f3549541&partnerID=40&md5=f414245152100382de8420fdb60e8bea,"Model Driven Engineering (MDE) is a general-purpose engineering methodology to elevate system design, maintenance, and analysis to corresponding activities on models. Models (graphical and/or textual) of a target application are automatically transformed into source code, performance models, Promela files (for model checking), and so on for system analysis and construction.Models are instances of metamodels. One form an MDE metamodel can take is a [class diagram, constraints] pair: the class diagram defines all object diagrams that could be metamodel instances; object constraint language (OCL) constraints eliminate semantically undesirable instances.A metamodel refactoring is an invertible semantics-preserving co-transformation, i.e., it transforms both a metamodel and its models without losing data. This article addresses a subproblem of metamodel refactoring: how to prove the correctness of refactorings of class diagrams without OCL constraints using the Coq Proof Assistant.  © 2023 Association for Computing Machinery.",Class diagram refactorings; Coq; object diagram refactorings,Metadata; Model checking; Semantics; Class diagram refactoring; Class diagrams; Coq; Meta model; Model-driven Engineering; Object Constraint Language; Object diagram refactoring; Object diagrams; Refactorings; Systems analysis
Estimating Probabilistic Safe WCET Ranges of Real-Time Systems at Design Stages,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153701965&doi=10.1145%2f3546941&partnerID=40&md5=a66d49542d2974b3690b266ded9eb52f,"Estimating worst-case execution time (WCET) is an important activity at early design stages of real-time systems. Based on WCET estimates, engineers make design and implementation decisions to ensure that task executions always complete before their specified deadlines. However, in practice, engineers often cannot provide precise point WCET estimates and prefer to provide plausible WCET ranges. Given a set of real-time tasks with such ranges, we provide an automated technique to determine for what WCET values the system is likely to meet its deadlines and, hence, operate safely with a probabilistic guarantee. Our approach combines a search algorithm for generating worst-case scheduling scenarios with polynomial logistic regression for inferring probabilistic safe WCET ranges. We evaluated our approach by applying it to three industrial systems from different domains and several synthetic systems. Our approach efficiently and accurately estimates probabilistic safe WCET ranges within which deadlines are likely to be satisfied with a high degree of confidence.  © 2023 Association for Computing Machinery.",machine learning; meta-heuristic search; Schedulability analysis; search-based software engineering; worst-case execution time,Heuristic algorithms; Interactive computer systems; Machine learning; Software engineering; Design stage; Machine-learning; Meta-heuristic search; Probabilistics; Real - Time system; Schedulability analysis; Search-based; Search-based software engineering; Time range; Worst-case execution time; Real time systems
"Testing, Validation, and Verification of Robotic and Autonomous Systems: A Systematic Review",2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153787218&doi=10.1145%2f3542945&partnerID=40&md5=1aedd02538d5aa4f52266ca11614e9e6,"We perform a systematic literature review on testing, validation, and verification of robotic and autonomous systems (RAS). The scope of this review covers peer-reviewed research papers proposing, improving, or evaluating testing techniques, processes, or tools that address the system-level qualities of RAS. Our survey is performed based on a rigorous methodology structured in three phases. First, we made use of a set of 26 seed papers (selected by domain experts) and the SERP-TEST taxonomy to design our search query and (domain-specific) taxonomy. Second, we conducted a search in three academic search engines and applied our inclusion and exclusion criteria to the results. Respectively, we made use of related work and domain specialists (50 academics and 15 industry experts) to validate and refine the search query. As a result, we encountered 10,735 studies, out of which 195 were included, reviewed, and coded. Our objective is to answer four research questions, pertaining to (1) the type of models, (2) measures for system performance and testing adequacy, (3) tools and their availability, and (4) evidence of applicability, particularly in industrial contexts. We analyse the results of our coding to identify strengths and gaps in the domain and present recommendations to researchers and practitioners. Our findings show that variants of temporal logics are most widely used for modelling requirements and properties, while variants of state-machines and transition systems are used widely for modelling system behaviour. Other common models concern epistemic logics for specifying requirements and belief-desire-intention models for specifying system behaviour. Apart from time and epistemics, other aspects captured in models concern probabilities (e.g., for modelling uncertainty) and continuous trajectories (e.g., for modelling vehicle dynamics and kinematics). Many papers lack any rigorous measure of efficiency, effectiveness, or adequacy for their proposed techniques, processes, or tools. Among those that provide a measure of efficiency, effectiveness, or adequacy, the majority use domain-agnostic generic measures such as number of failures, size of state-space, or verification time were most used. There is a trend in addressing the research gap in this respect by developing domain-specific notions of performance and adequacy. Defining widely accepted rigorous measures of performance and adequacy for each domain is an identified research gap. In terms of tools, the most widely used tools are well-established model-checkers such as Prism and Uppaal, as well as simulation tools such as Gazebo; Matlab/Simulink is another widely used toolset in this domain. Overall, there is very limited evidence of industrial applicability in the papers published in this domain. There is even a gap considering consolidated benchmarks for various types of autonomous systems.  © 2023 Copyright held by the owner/author(s).",autonomous systems; literature survey; robotics; testing; Verification and validation,Efficiency; Industrial research; Search engines; Taxonomies; Uncertainty analysis; Autonomous system; Domain specific; Literature survey; Research gaps; Search queries; System behaviors; Systematic literature review; Systematic Review; Validation and verification; Verification-and-validation; Robotics
Nudge: Accelerating Overdue Pull Requests toward Completion,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143061311&doi=10.1145%2f3544791&partnerID=40&md5=f8b18a4a1a2d66b7b8ea1da0bf87e926,"Pull requests are a key part of the collaborative software development and code review process today. However, pull requests can also slow down the software development process when the reviewer(s) or the author do not actively engage with the pull request. In this work, we design an end-to-end service, Nudge, for accelerating overdue pull requests toward completion by reminding the author or the reviewer(s) to engage with their overdue pull requests. First, we use models based on effort estimation and machine learning to predict the completion time for a given pull request. Second, we use activity detection to filter out pull requests that may be overdue but for which sufficient action is taking place nonetheless. Last, we use actor identification to understand who the blocker of the pull request is and nudge the appropriate actor (author or reviewer(s)). The key novelty of Nudge is that it succeeds in reducing pull request resolution time, while ensuring that developers perceive the notifications sent as useful, at the scale of thousands of repositories. In a randomized trial on 147 repositories in use at Microsoft, Nudge was able to reduce pull request resolution time by 60% for 8,500 pull requests, when compared to overdue pull requests for which Nudge did not send a notification. Furthermore, developers receiving Nudge notifications resolved 73% of these notifications as positive. We observed similar results when scaling up the deployment of Nudge to 8,000 repositories at Microsoft, for which Nudge sent 210,000 notifications during a full year. This demonstrates Nudge's ability to scale to thousands of repositories. Last, our qualitative analysis of a selection of Nudge notifications indicates areas for future research, such as taking dependencies among pull requests and developer availability into account.  © 2023 Association for Computing Machinery.",distributed software development; merge conflict; pull request; Pull-based software development,Code review; Collaborative software development; Distributed software development; Key parts; Merge conflict; MicroSoft; Pull request; Pull-based software development; Resolution time; Software codes; Software design
Aide-mémoire: Improving a Project's Collective Memory via Pull Request-Issue Links,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153699689&doi=10.1145%2f3542937&partnerID=40&md5=51805fa6ff7f8c0d9210267213b49eda,"Links between pull request and the issues they address document and accelerate the development of a software project but are often omitted. We present a new tool, Aide-mémoire, to suggest such links when a developer submits a pull request or closes an issue, smoothly integrating into existing workflows. In contrast to previous state-of-the-art approaches that repair related commit histories, Aide-mémoire is designed for continuous, real-time, and long-term use, employing Mondrian forest to adapt over a project's lifetime and continuously improve traceability. Aide-mémoire is tailored for two specific instances of the general traceability problem - namely, commit to issue and pull request to issue links, with a focus on the latter - and exploits data inherent to these two problems to outperform tools for general purpose link recovery. Our approach is online, language-agnostic, and scalable. We evaluate over a corpus of 213 projects and six programming languages, achieving a mean average precision of 0.95. Adopting Aide-mémoire is both efficient and effective: A programmer need only evaluate a single suggested link 94% of the time, and 16% of all discovered links were originally missed by developers.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",link inference; missing link; Traceability,Collective memory; Commit history; Link inference; Missing link; Mondrian; Real- time; Software project; State-of-the-art approach; Traceability; Work-flows
"Graded Refinement, Retrenchment, and Simulation",2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156161675&doi=10.1145%2f3534116&partnerID=40&md5=c666b36002b5c5ba53ba7dcacea74ede,"Refinement of formal system models towards implementation has been a mainstay of system development since the inception of formal and Correct by Construction approaches to system development. However, pure refinement approaches do not always deal fluently with all desirable system requirements. This prompted the development of alternatives and generalizations, such as retrenchment. The crucial concept of simulation is key to judging the quality of the conformance between abstract and more concrete system models. Reformulations of these theoretical approaches are reprised and are embedded in a graded framework. The added flexibility this offers is intended to deal more effectively with the needs of applications in which the relationship between different levels of abstraction is not straightforward, and in which behavior can oscillate between conforming quite closely to an idealized abstraction and deviating quite far from it. The framework developed is confronted with an intentionally demanding case study: a model active control system for the protection of buildings during earthquakes. This offers many challenges: it is hybrid/cyber-physical; it has to respond to rather unpredictable inputs; and it has to straddle the gap between continuous behavior and discretized/quantized/numerical implementation. © 2023 Association for Computing Machinery.",PhrasesRefinement; retrenchment; simulation,Concrete system; Construction approaches; Correct-by-construction; Formal system models; Generalisation; Phrasesrefinement; Retrenchment; Simulation; System development; System requirements; Abstracting
Dissecting American Fuzzy Lop: A FuzzBench Evaluation,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153789424&doi=10.1145%2f3580596&partnerID=40&md5=8962841035d553647b9ba86ea423f55b,"AFL is one of the most used and extended fuzzers, adopted by industry and academic researchers alike. Although the community agrees on AFL's effectiveness at discovering new vulnerabilities and its outstanding usability, many of its internal design choices remain untested to date. Security practitioners often clone the project ""as-is""and use it as a starting point to develop new techniques, usually taking everything under the hood for granted. Instead, we believe that a careful analysis of the different parameters could help modern fuzzers improve their performance and explain how each choice can affect the outcome of security testing, either negatively or positively. The goal of this work is to provide a comprehensive understanding of the internal mechanisms of AFL by performing experiments and by comparing different metrics used to evaluate fuzzers. This can help to show the effectiveness of some techniques and to clarify which aspects are instead outdated. To perform our study, we performed nine unique experiments that we carried out on the popular Fuzzbench platform. Each test focuses on a different aspect of AFL, ranging from its mutation approach to the feedback encoding scheme and its scheduling methodologies. Our findings show that each design choice affects different factors of AFL. Some of these are positively correlated with the number of detected bugs or the coverage of the target application, whereas other features are related to usability and reliability. Most important, we believe that the outcome of our experiments indicates which parts of AFL we should preserve in the design of modern fuzzers.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AFL; FuzzBench; Fuzzing,AFL; Feedback encoding schemes; Fuzzbench; Fuzzing; Internal design; Performance; Security practitioners; Security testing; Target application; Program debugging
Toward More Efficient Statistical Debugging with Abstraction Refinement,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153764239&doi=10.1145%2f3544790&partnerID=40&md5=4abedf847913cf42c3aa490f6aba75ad,"Debugging is known to be a notoriously painstaking and time-consuming task. As one major family of automated debugging, statistical debugging approaches have been well investigated over the past decade, which collect failing and passing executions and apply statistical techniques to identify discriminative elements as potential bug causes. Most of the existing approaches instrument the entire program to produce execution profiles for debugging, thus incurring hefty instrumentation and analysis cost. However, as in fact a major part of the program code is error-free, full-scale program instrumentation is wasteful and unnecessary. This article presents a systematic abstraction refinement-based pruning technique for statistical debugging. Our technique only needs to instrument and analyze the code partially. While guided by a mathematically rigorous analysis, our technique is guaranteed to produce the same debugging results as an exhaustive analysis in deterministic settings. With the help of the effective and safe pruning, our technique greatly saves the cost of failure diagnosis without sacrificing any debugging capability. We apply this technique to two different statistical debugging scenarios: in-house and production-run statistical debugging. The comprehensive evaluations validate that our technique can significantly improve the efficiency of statistical debugging in both scenarios, while without jeopardizing the debugging capability.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",abstraction refinement; fault localization; selective instrumentation; Statistical debugging,Abstracting; Codes (symbols); Cost benefit analysis; Program debugging; Abstraction-refinement; Analysis costs; Automated debugging; Fault localization; Program code; Program instrumentations; Selective instrumentation; Statistical debugging; Statistical techniques; Time-consuming tasks; Statistics
Efficient and Effective Feature Space Exploration for Testing Deep Learning Systems,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153773586&doi=10.1145%2f3544792&partnerID=40&md5=9a2b2bbb1a8408bbb070ca9dedf2ced4,"Assessing the quality of Deep Learning (DL) systems is crucial, as they are increasingly adopted in safety-critical domains. Researchers have proposed several input generation techniques for DL systems. While such techniques can expose failures, they do not explain which features of the test inputs influenced the system's (mis-) behaviour. DeepHyperion was the first test generator to overcome this limitation by exploring the DL systems' feature space at large. In this article, we propose DeepHyperion-CS, a test generator for DL systems that enhances DeepHyperion by promoting the inputs that contributed more to feature space exploration during the previous search iterations. We performed an empirical study involving two different test subjects (i.e., a digit classifier and a lane-keeping system for self-driving cars). Our results proved that the contribution-based guidance implemented within DeepHyperion-CS outperforms state-of-the-art tools and significantly improves the efficiency and the effectiveness of DeepHyperion. DeepHyperion-CS exposed significantly more misbehaviours for five out of six feature combinations and was up to 65% more efficient than DeepHyperion in finding misbehaviour-inducing inputs and exploring the feature space. DeepHyperion-CS was useful for expanding the datasets used to train the DL systems, populating up to 200% more feature map cells than the original training set.  © 2023 Association for Computing Machinery.",Deep Learning; search based software engineering; self-driving cars; Software testing,Autonomous vehicles; Deep learning; Learning systems; Safety engineering; Space research; Deep learning; Feature space; Generation techniques; Misbehaviour; Safety-critical domain; Search based software engineering; Search-based; Software testings; Space explorations; Test inputs; Software testing
HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153889738&doi=10.1145%2f3528100&partnerID=40&md5=de52d42855541d278e134f052768f38c,"Modern software systems are usually highly configurable, providing users with customized functionality through various configuration options. Understanding how system performance varies with different option combinations is important to determine optimal configurations that meet specific requirements. Due to the complex interactions among multiple options and the high cost of performance measurement under a huge configuration space, it is challenging to study how different configurations influence the system performance. To address these challenges, we propose HINNPerf, a novel hierarchical interaction neural network for performance prediction of configurable systems. HINNPerf employs the embedding method and hierarchic network blocks to model the complicated interplay between configuration options, which improves the prediction accuracy of the method. In addition, we devise a hierarchical regularization strategy to enhance the model robustness. Empirical results on 10 real-world configurable systems show that our method statistically significantly outperforms state-of-the-art approaches by achieving average 22.67% improvement in prediction accuracy. In addition, combined with the Integrated Gradients method, the designed hierarchical architecture provides some insights about the interaction complexity and the significance of configuration options, which might help users and developers better understand how the configurable system works and efficiently identify significant options affecting the performance. © 2023 Association for Computing Machinery.",deep neural network; highly configurable systems; machine learning; Software performance prediction,Complex networks; Forecasting; Hierarchical systems; Configurable systems; Configuration options; Hierarchical interactions; Highly configurable system; Machine-learning; Neural-networks; Performance prediction; Software performance; Software performance prediction; Systems performance; Deep neural networks
deGraphCS: Embedding Variable-based Flow Graph for Neural Code Search,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153750091&doi=10.1145%2f3546066&partnerID=40&md5=bd8b4bfceb84f7efe993d499261fbcd2,"With the rapid increase of public code repositories, developers maintain a great desire to retrieve precise code snippets by using natural language. Despite existing deep learning-based approaches that provide end-to-end solutions (i.e., accept natural language as queries and show related code fragments), the performance of code search in the large-scale repositories is still low in accuracy because of the code representation (e.g., AST) and modeling (e.g., directly fusing features in the attention stage). In this paper, we propose a novel learnable deep Graph for Code Search (called deGraphCS) to transfer source code into variable-based flow graphs based on an intermediate representation technique, which can model code semantics more precisely than directly processing the code as text or using the syntax tree representation. Furthermore, we propose a graph optimization mechanism to refine the code representation and apply an improved gated graph neural network to model variable-based flow graphs. To evaluate the effectiveness of deGraphCS, we collect a large-scale dataset from GitHub containing 41,152 code snippets written in the C language and reproduce several typical deep code search methods for comparison. The experimental results show that deGraphCS can achieve state-of-the-art performance and accurately retrieve code snippets satisfying the needs of the users.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code search; deep learning; graph neural networks; Intermediate representation,Deep neural networks; Flow graphs; Graphic methods; Large dataset; Modeling languages; Natural language processing systems; Semantics; Trees (mathematics); Code representation; Code search; Deep learning; Embeddings; Flow-graphs; Graph neural networks; Intermediate representations; Learning-based approach; Natural languages; Neural code; Graph neural networks
On the Significance of Category Prediction for Code-Comment Synchronization,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156099950&doi=10.1145%2f3534117&partnerID=40&md5=5410478dac9d3609f8122ad2496f37bd,"Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED), Relative Edit Distance (RED), BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance. © 2023 Association for Computing Machinery.",category classification; comment synchronization; deep learning; heuristic rules,Codes (symbols); Electric circuit breakers; Forecasting; Learning systems; Long short-term memory; Semantics; Category Classification; Code changes; Comment synchronization; Deep learning; Edit distance; Ensemble learning; Heuristic rules; Inconsistent samples; Semantic modelling; Synchronization performance; Synchronization
Hippodrome: Data Race Repair Using Static Analysis Summaries,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153735422&doi=10.1145%2f3546942&partnerID=40&md5=c8d4e643711ecc12024454c8c70b5363,"Implementing bug-free concurrent programs is a challenging task in modern software development. State-of-the-art static analyses find hundreds of concurrency bugs in production code, scaling to large codebases. Yet, fixing these bugs in constantly changing codebases represents a daunting effort for programmers, particularly because a fix in the concurrent code can introduce other bugs in a subtle way. In this work, we show how to harness compositional static analysis for concurrency bug detection, to enable a new Automated Program Repair (APR) technique for data races in large concurrent Java codebases. The key innovation of our work is an algorithm that translates procedure summaries inferred by the analysis tool for the purpose of bug reporting into small local patches that fix concurrency bugs (without introducing new ones). This synergy makes it possible to extend the virtues of compositional static concurrency analysis to APR, making our approach effective (it can detect and fix many more bugs than existing tools for data race repair), scalable (it takes seconds to analyze and suggest fixes for sizeable codebases), and usable (generally, it does not require annotations from the users and can perform continuous automated repair). Our study, conducted on popular open-source projects, has confirmed that our tool automatically produces concurrency fixes similar to those proposed by the developers in the past.  © 2023 Association for Computing Machinery.",Concurrency; program repair; static analysis,Open source software; Program debugging; Repair; Software design; Bug detection; Bug-free; Code scaling; Concurrency; Concurrency bugs; Concurrents programs; Data races; Program repair; Repair techniques; State of the art; Static analysis
Evaluating Surprise Adequacy for Deep Learning System Testing,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153749279&doi=10.1145%2f3546947&partnerID=40&md5=d3a21036604664e827eeaf27fda0b528,"The rapid adoption of Deep Learning (DL) systems in safety critical domains such as medical imaging and autonomous driving urgently calls for ways to test their correctness and robustness. Borrowing from the concept of test adequacy in traditional software testing, existing work on testing of DL systems initially investigated DL systems from structural point of view, leading to a number of coverage metrics. Our lack of understanding of the internal mechanism of Deep Neural Networks (DNNs), however, means that coverage metrics defined on the Boolean dichotomy of coverage are hard to intuitively interpret and understand. We propose the degree of out-of-distribution-ness of a given input as its adequacy for testing: the more surprising a given input is to the DNN under test, the more likely the system will show unexpected behavior for the input. We develop the concept of surprise into a test adequacy criterion, called Surprise Adequacy (SA). Intuitively, SA measures the difference in the behavior of the DNN for the given input and its behavior for the training data. We posit that a good test input should be sufficiently, but not overtly, surprising compared to the training dataset. This article evaluates SA using a range of DL systems from simple image classifiers to autonomous driving car platforms, as well as both small and large data benchmarks ranging from MNIST to ImageNet. The results show that the SA value of an input can be a reliable predictor of the correctness of the mode behavior. We also show that SA can be used to detect adversarial examples, and also be efficiently computed against large training dataset such as ImageNet using sampling.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning systems; Test adequacy,Autonomous vehicles; Large dataset; Learning systems; Medical imaging; Safety engineering; Software testing; Statistical tests; Autonomous driving; Coverage metrics; Deep learning system; Safety-critical domain; Software testings; System testing; Test adequacies; Test adequacy criteria; Training data; Training dataset; Deep neural networks
DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153786327&doi=10.1145%2f3546946&partnerID=40&md5=ad9bb0c4afb19016107249178b86a9b1,"The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE. We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE's overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23% more frequently than the original DIRE model.  © 2023 Copyright held by the owner/author(s).",data provenance; decompilation; Machine learning,Quality control; Data provenance; Decompilation; Decompilers; Domain specific; Executables; ITS data; Machine learning models; Machine-learning; Performance; Training data; Machine learning
Demystifying Hidden Sensitive Operations in Android Apps,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153857869&doi=10.1145%2f3574158&partnerID=40&md5=207a42a73b8829afceb3ecc7eb54cc0b,"Security of Android devices is now paramount, given their wide adoption among consumers. As researchers develop tools for statically or dynamically detecting suspicious apps, malware writers regularly update their attack mechanisms to hide malicious behavior implementation. This poses two problems to current research techniques: static analysis approaches, given their over-approximations, can report an overwhelming number of false alarms, while dynamic approaches will miss those behaviors that are hidden through evasion techniques. We propose in this work a static approach specifically targeted at highlighting hidden sensitive operations (HSOs), mainly sensitive data flows. The prototype version of HiSenDroid has been evaluated on a large-scale dataset of thousands of malware and goodware samples on which it successfully revealed anti-analysis code snippets aiming at evading detection by dynamic analysis. We further experimentally show that, with FlowDroid, some of the hidden sensitive behaviors would eventually lead to private data leaks. Those leaks would have been hard to spot either manually among the large number of false positives reported by the state-of-the-art static analyzers, or by dynamic tools. Overall, by putting the light on hidden sensitive operations, HiSenDroid helps security analysts in validating potentially sensitive data operations, which would be previously unnoticed. © 2023 Association for Computing Machinery.",Android application; hidden sensitive operations; privacy leak; program analysis,Android (operating system); Android malware; Application programs; Large dataset; Mobile security; Static analysis; Android applications; Android apps; Attack mechanism; Behavior implementation; Hidden sensitive operation; Malicious behavior; Malware writers; Privacy leak; Program analysis; Sensitive datas; Sensitive data
Suboptimal Comments in Java Projects: From Independent Comment Changes to Commenting Practices,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153703956&doi=10.1145%2f3546949&partnerID=40&md5=4b2ec491e9286ac044005f68f2ce9539,"High-quality source code comments are valuable for software development and maintenance, however, code often contains low-quality comments or lacks them altogether. We name such source code comments as suboptimal comments. Such suboptimal comments create challenges in code comprehension and maintenance. Despite substantial research on low-quality source code comments, empirical knowledge about commenting practices that produce suboptimal comments and reasons that lead to suboptimal comments are lacking. We help bridge this knowledge gap by investigating (1) independent comment changes (ICCs) - comment changes committed independently of code changes - which likely address suboptimal comments, (2) commenting guidelines, and (3) comment-checking tools and comment-generating tools, which are often employed to help commenting practice - especially to prevent suboptimal comments. We collect 24M+ comment changes from 4,392 open-source GitHub Java repositories and find that ICCs widely exist. The ICC ratio - proportion of ICCs among all comment changes - is ∼15.5%, with 98.7% of the repositories having ICC. Our thematic analysis of 3,533 randomly sampled ICCs provides a three-dimensional taxonomy for what is changed (four comment categories and 13 subcategories), how it changed (six commenting activity categories), and what factors are associated with the change (three factors). We investigate 600 repositories to understand the prevalence, content, impact, and violations of commenting guidelines. We find that only 15.5% of the 600 sampled repositories have any commenting guidelines. We provide the first taxonomy for elements in commenting guidelines: where and what to comment are particularly important. The repositories without such guidelines have a statistically significantly higher ICC ratio, indicating the negative impact of the lack of commenting guidelines. However, commenting guidelines are not strictly followed: 85.5% of checked repositories have violations. We also systematically study how developers use two kinds of tools, comment-checking tools and comment-generating tools, in the 4,392 repositories. We find that the use of Javadoc tool is negatively correlated with the ICC ratio, while the use of Checkstyle has no statistically significant correlation; the use of comment-generating tools leads to a higher ICC ratio. To conclude, we reveal issues and challenges in current commenting practice, which help understand how suboptimal comments are introduced. We propose potential research directions on comment location prediction, comment generation, and comment quality assessment; suggest how developers can formulate commenting guidelines and enforce rules with tools; and recommend how to enhance current comment-checking and comment-generating tools.  © 2023 Association for Computing Machinery.",Code comments; coding guidelines; software documentation; software evolution,Codes (symbols); Computer software maintenance; Java programming language; Open source software; Software design; 'current; Change ratio; Code comment; Coding guideline; High quality source; Low qualities; Software development and maintenances; Software documentation; Software Evolution; Source code comments; Taxonomies
iBiR: Bug-report-driven Fault Injection,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153751800&doi=10.1145%2f3542946&partnerID=40&md5=3667939a32cd751b403cc0716a7f389a,"Much research on software engineering relies on experimental studies based on fault injection. Fault injection, however, is not often relevant to emulate real-world software faults since it ""blindly""injects large numbers of faults. It remains indeed challenging to inject few but realistic faults that target a particular functionality in a program. In this work, we introduce iBiR , a fault injection tool that addresses this challenge by exploring change patterns associated to user-reported faults. To inject realistic faults, we create mutants by re-targeting a bug-report-driven automated program repair system, i.e., reversing its code transformation templates. iBiR is further appealing in practice since it requires deep knowledge of neither code nor tests, just of the program's relevant bug reports. Thus, our approach focuses the fault injection on the feature targeted by the bug report. We assess iBiR by considering the Defects4J dataset. Experimental results show that our approach outperforms the fault injection performed by traditional mutation testing in terms of semantic similarity with the original bug, when applied at either system or class levels of granularity, and provides better, statistically significant estimations of test effectiveness (fault detection). Additionally, when injecting 100 faults, iBiR injects faults that couple with the real ones in around 36% of the cases, while mutation testing achieves less than 4%.  © 2023 Copyright held by the owner/author(s).",bug reports; Fault injection; information retrieval; mutation,Cosine transforms; Semantics; Software testing; Bug reports; Change patterns; Code transformation; Deep knowledge; Fault injection; Mutation; Mutation testing; Real-world; Repair system; Software fault; Fault detection
Dealing with Belief Uncertainty in Domain Models,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153791867&doi=10.1145%2f3542947&partnerID=40&md5=b8d034751b2e42369a2b80c310b11bfa,"There are numerous domains in which information systems need to deal with uncertain information. These uncertainties may originate from different reasons such as vagueness, imprecision, incompleteness, or inconsistencies, and in many cases, they cannot be neglected. In this article, we are interested in representing and processing uncertain information in domain models, considering the stakeholders' beliefs (opinions). We show how to associate beliefs to model elements and how to propagate and operate with their associated uncertainty so that domain experts can individually reason about their models enriched with their personal opinions. In addition, we address the challenge of combining the opinions of different domain experts on the same model elements, with the goal to come up with informed collective decisions. We provide different strategies and a methodology to optimally merge individual opinions.  © 2023 Association for Computing Machinery.",belief; belief fusion; consensus; decision-making; domain models; Information systems; software; subjective logic; uncertainty; vagueness,Information systems; Information use; Belief; Belief fusion; Consensus; Decisions makings; Domain model; Software; Subjective Logic; Uncertain informations; Uncertainty; Vagueness; Decision making
A Characterization Study of Merge Conflicts in Java Projects,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141987629&doi=10.1145%2f3546944&partnerID=40&md5=ddc80624f4b7cdfa0e9db376d73ab9a4,"In collaborative software development, programmers create software branches to add features and fix bugs tentatively, and then merge branches to integrate edits. When edits from different branches textually overlap (i.e., textual conflicts) or lead to compilation and runtime errors (i.e., build and test conflicts), it is challenging for developers to remove such conflicts. Prior work proposed tools to detect and solve conflicts. They investigate how conflicts relate to code smells and the software development process. However, many questions are still not fully investigated, such as what types of conflicts exist in real-world applications and how developers or tools handle them. For this article, we used automated textual merge, compilation, and testing to reveal three types of conflicts in 208 open-source repositories: textual conflicts, build conflicts (i.e., conflicts causing build errors), and test conflicts (i.e., conflicts triggering test failures). We manually inspected 538 conflicts and their resolutions to characterize merge conflicts from different angles. Our analysis revealed three interesting phenomena. First, higher-order conflicts (i.e., build and test conflicts) are harder to detect and resolve, while existing tools mainly focus on textual conflicts. Second, developers manually resolved most higher-order conflicts by applying similar edits to multiple program locations; their conflict resolutions share common editing patterns implying great opportunities for future tool design. Third, developers resolved 64% of true textual conflicts by keeping complete edits from either a left or right branch. Unlike prior studies, our research for the first time thoroughly characterizes three types of conflicts, with a special focus on higher-order conflicts and limitations of existing tool design. Our work will shed light on future research of software merge.  © 2023 Association for Computing Machinery.",conflict detection; conflict resolution; Empirical; software merge,Java programming language; Program debugging; Software design; Characterization studies; Collaborative software development; Conflict detection; Conflict Resolution; Empirical; High-order; Higher-order; Run-time errors; Software merge; Tool designs; Open source software
Towards Learning Generalizable Code Embeddings Using Task-agnostic Graph Convolutional Networks,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153731394&doi=10.1145%2f3542944&partnerID=40&md5=5579536a2ca56d11845228acae273057,"Code embeddings have seen increasing applications in software engineering (SE) research and practice recently. Despite the advances in embedding techniques applied in SE research, one of the main challenges is their generalizability. A recent study finds that code embeddings may not be readily leveraged for the downstream tasks that the embeddings are not particularly trained for. Therefore, in this article, we propose GraphCodeVec, which represents the source code as graphs and leverages the Graph Convolutional Networks to learn more generalizable code embeddings in a task-agnostic manner. The edges in the graph representation are automatically constructed from the paths in the abstract syntax trees, and the nodes from the tokens in the source code. To evaluate the effectiveness of GraphCodeVec , we consider three downstream benchmark tasks (i.e., code comment generation, code authorship identification, and code clones detection) that are used in a prior benchmarking of code embeddings and add three new downstream tasks (i.e., source code classification, logging statements prediction, and software defect prediction), resulting in a total of six downstream tasks that are considered in our evaluation. For each downstream task, we apply the embeddings learned by GraphCodeVec and the embeddings learned from four baseline approaches and compare their respective performance. We find that GraphCodeVec outperforms all the baselines in five out of the six downstream tasks, and its performance is relatively stable across different tasks and datasets. In addition, we perform ablation experiments to understand the impacts of the training context (i.e., the graph context extracted from the abstract syntax trees) and the training model (i.e., the Graph Convolutional Networks) on the effectiveness of the generated embeddings. The results show that both the graph context and the Graph Convolutional Networks can benefit GraphCodeVec in producing high-quality embeddings for the downstream tasks, while the improvement by Graph Convolutional Networks is more robust across different downstream tasks and datasets. Our findings suggest that future research and practice may consider using graph-based deep learning methods to capture the structural information of the source code for SE tasks.  © 2023 Association for Computing Machinery.",code embeddings; Machine learning; neural network; source code representation,Abstracting; Application programs; Convolution; Deep learning; Graphic methods; Network coding; Semantics; Syntactics; Trees (mathematics); Abstract Syntax Trees; Code embedding; Convolutional networks; Down-stream; Embeddings; Machine-learning; Neural-networks; Software engineering research; Source code representations; Source codes; Embeddings
Coverage-directed Differential Testing of X.509 Certificate Validation in SSL/TLS Implementations,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152604401&doi=10.1145%2f3510416&partnerID=40&md5=b369a42398ad8bd2a8be2eec8233d80f,"Secure Sockets Layer (SSL) and Transport Security (TLS) are two secure protocols for creating secure connections over the Internet. X.509 certificate validation is important for security and needs to be performed before an SSL/TLS connection is established. Some advanced testing techniques, such as frankencert, have revealed, through randomly mutating Internet accessible certificates, that there exist unexpected, sometimes critical, validation differences among different SSL/TLS implementations. Despite these efforts, X.509 certificate validation still needs to be thoroughly tested as this work shows. This article tackles this challenge by proposing transcert, a coverage-directed technique to much more effectively test real-world certificate validation code. Our core insight is to (1) leverage easily accessible Internet certificates as seed certificates and (2) use code coverage to direct certificate mutation toward generating a set of diverse certificates. The generated certificates are then used to reveal discrepancies, thus potential flaws, among different certificate validation implementations. We implement transcert and evaluate it against frankencert, NEZHA, and RFCcert (three advanced fuzzing techniques) on five widely used SSL/TLS implementations. The evaluation results clearly show the strengths of transcert: During 10,000 iterations, transcert reveals 71 unique validation differences, 12×, 1.4×, and 7× as many as those revealed by frankencert, NEZHA, and RFCcert, respectively; it also supplements RFCcert in conformance testing of the SSL/TLS implementations against 120 validation rules, 85 of which are exclusively covered by transcert-generated certificates. We identify 17 root causes of validation differences, all of which have been confirmed and 11 have never been reported previously. The transcert-generated X.509 certificates also reveal that the primary goal of certificate chain validation is stated ambiguously in the widely adopted public key infrastructure standard RFC 5280.  © 2023 Association for Computing Machinery.",certificate validation; certification mutation; Coverage transfer graph; differential testing,Public key cryptography; Certificate validations; Certification mutation; Coverage transfer graph; Differential testing; Real-world; Secure protocols; Secure sockets layers; Testing technique; Transport security; X.509 certificates; Directed graphs
Consent Verification Monitoring,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152593921&doi=10.1145%2f3490754&partnerID=40&md5=ca56ee9d4391105fd7eb1c338dc63668,"Advances in personalization of digital services are driven by low-cost data collection and processing, in addition to the wide variety of third-party frameworks for authentication, storage, and marketing. New privacy regulations, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act, increasingly require organizations to explicitly state their data practices in privacy policies. When data practices change, a new version of the policy is released. This can occur a few times a year, when data collection or processing requirements are rapidly changing. Consent evolution raises specific challenges to ensuring GDPR compliance. We propose a formal consent framework to support organizations, data users, and data subjects in their understanding of policy evolution under a consent regime that supports both the retroactive and non-retroactive granting and withdrawal of consent. The contributions include (i) a formal framework to reason about data collection and access under multiple consent granting and revocation scenarios, (ii) a scripting language that implements the consent framework for encoding and executing different scenarios, (iii) five consent evolution use cases that illustrate how organizations would evolve their policies using this framework, and (iv) a scalability evaluation of the reasoning framework. The framework models are used to verify when user consent prevents or detects unauthorized data collection and access. The framework can be integrated into a runtime architecture to monitor policy violations as data practices evolve in real time. The framework was evaluated using the five use cases and a simulation to measure the framework scalability. The simulation results show that the approach is computationally scalable for use in runtime consent monitoring under a standard model of data collection and access and practice and policy evolution.  © 2023 Association for Computing Machinery.",analysis; consent; consent revocation; description logic; evolution; formal framework; GDPR; logs; Privacy; retroactivity; verification,Consumer protection; Data acquisition; Data description; Data privacy; Scalability; Analyse; Consent; Consent revocation; Description logic; Evolution; Formal framework; General data protection regulations; Log; Privacy; Retroactivity; Digital storage
"Microservice Security Metrics for Secure Communication, Identity Management, and Observability",2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152594050&doi=10.1145%2f3532183&partnerID=40&md5=3924407fc8b5c344aa516b15cba966f7,"Microservice architectures are increasingly being used to develop application systems. Despite many guidelines and best practices being published, architecting microservice systems for security is challenging. Reasons are the size and complexity of microservice systems, their polyglot nature, and the demand for the continuous evolution of these systems. In this context, to manually validate that security architecture tactics are employed as intended throughout the system is a time-consuming and error-prone task. In this article, we present an approach to avoid such manual validation before each continuous evolution step in a microservice system, which we demonstrate using three widely used categories of security tactics: secure communication, identity management, and observability. Our approach is based on a review of existing security guidelines, the gray literature, and the scientific literature, from which we derived Architectural Design Decisions (ADDs) with the found security tactics as decision options. In our approach, we propose novel detectors to detect these decision options automatically and formally defined metrics to measure the conformance of a system to the different options of the ADDs. We apply the approach to a case study data set of 10 open source microservice systems, plus another 20 variants of these systems, for which we manually inspected the source code for security tactics. We demonstrate and assess the validity and appropriateness of our metrics by performing an assessment of their conformance to the ADDs in our systems' dataset through statistical methods.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesMicroservice architecture; microservice security; software architecture detectors; software architecture metrics,Observability; Open source software; Open systems; Secure communication; Additional key word and phrasesmicroservice architecture; Application systems; Architectural design decisions; Best practices; Identity management; Key words; Microservice security; Security metrics; Software architecture detector; Software architecture metric; Software architecture
Some Seeds Are Strong: Seeding Strategies for Search-based Test Case Selection,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145884657&doi=10.1145%2f3532182&partnerID=40&md5=638ca34230d641d80410d39d5e6b1cba,"The time it takes software systems to be tested is usually long. Search-based test selection has been a widely investigated technique to optimize the testing process. In this article, we propose a set of seeding strategies for the test case selection problem that generates the initial population of Pareto-based multi-objective algorithms, with the goals of (1) helping to find an overall better set of solutions and (2) enhancing the convergence of the algorithms. The seeding strategies were integrated with four state-of-the-art multi-objective search algorithms and applied into two contexts where regression-testing is paramount: (1) Simulation-based testing of Cyber-physical Systems and (2) Continuous Integration. For the first context, we evaluated our approach by using six fitness function combinations and six independent case studies, whereas in the second context, we derived a total of six fitness function combinations and employed four case studies. Our evaluation suggests that some of the proposed seeding strategies are indeed helpful for solving the multi-objective test case selection problem. Specifically, the proposed seeding strategies provided a higher convergence of the algorithms towards optimal solutions in 96% of the studied scenarios and an overall cost-effectiveness with a standard search budget in 85% of the studied scenarios.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTest case selection; regression testing; search-based software testing,Budget control; Embedded systems; Integration testing; Additional key word and phrasestest case selection; Case selections; Fitness functions; Key words; Regression testing; Search-based; Search-based software testing; Seeding strategies; Selection problems; Test case selection; Cost effectiveness
Parametric Timed Pattern Matching,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152603893&doi=10.1145%2f3517194&partnerID=40&md5=e0034cebaa7165f6906c13250358f304,"Given a log and a specification, timed pattern matching aims at exhibiting for which start and end dates a specification holds on that log. For example, ""a given action is always followed by another action before a given deadline"". This problem has strong connections with monitoring real-time systems. We address here timed pattern matching in the presence of an uncertain specification, i.e., that may contain timing parameters (e.g., the deadline can be uncertain or unknown). We want to know for which start and end dates, and for what values of the timing parameters, a property holds. For instance, we look for the minimum or maximum deadline (together with the corresponding start and end dates) for which the property holds. We propose two frameworks for parametric timed pattern matching. The first one is based on parametric timed model checking. In contrast to most parametric timed problems, the solution is effectively computable. The second one is a dedicated method; not only we largely improve the efficiency compared to the first method, but we further propose optimizations with skipping. Our experiment results suggest that our algorithms, especially the second one, are efficient and practically relevant.  © 2023 Association for Computing Machinery.",Monitoring; parametric timed automata; real-time systems,Interactive computer systems; Model checking; Pattern matching; Specifications; Uncertainty analysis; Optimisations; Parametric timed automata; Pattern-matching; Property; Real - Time system; Timed model checking; Timing parameters; Real time systems
LiDetector: License Incompatibility Detection for Open Source Software,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152604068&doi=10.1145%2f3518994&partnerID=40&md5=31ef68077935a75050b859e20b14bd76,"Open-source software (OSS) licenses dictate the conditions, which should be followed to reuse, distribute, and modify software. Apart from widely-used licenses such as the MIT License, developers are also allowed to customize their own licenses (called custom license), whose descriptions are more flexible. The presence of such various licenses imposes challenges to understand licenses and their compatibility. To avoid financial and legal risks, it is essential to ensure license compatibility when integrating third-party packages or reusing code accompanied with licenses. In this work, we propose LiDetector, an effective tool that extracts and interprets OSS licenses (including both official licenses and custom licenses), and detects license incompatibility among these licenses. Specifically, LiDetector introduces a learning-based method to automatically identify meaningful license terms from an arbitrary license, and employs Probabilistic Context-Free Grammar (PCFG) to infer rights and obligations for incompatibility detection. Experiments demonstrate that LiDetector outperforms existing methods with 93.28% precision for term identification, and 91.09% accuracy for right and obligation inference, and can effectively detect incompatibility with 10.06% FP rate and 2.56% FN rate. Furthermore, with LiDetector, our large-scale empirical study on 1,846 projects reveals that 72.91% of the projects are suffering from license incompatibility, including popular ones such as the MIT License and the Apache License. We highlighted lessons learned from perspectives of different stakeholders and made all related data and the replication package publicly available to facilitate follow-up research.  © 2023 Association for Computing Machinery.",incompatibility detection; license; Open source software,Computer software reusability; Context free grammars; Open systems; Condition; Effective tool; Financial risks; Incompatibility detection; Legal risks; License; Open-source softwares; Reuse; Software license; Third parties; Open source software
Scanner++: Enhanced Vulnerability Detection of Web Applications with Attack Intent Synchronization,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152596343&doi=10.1145%2f3517036&partnerID=40&md5=870c00c9b27de0e3031d0909ab08466b,"Scanners are commonly applied for detecting vulnerabilities in web applications. Various scanners with different strategies are widely in use, but their performance is challenged by the increasing diversity of target applications that have more complex attack surfaces (i.e., website paths) and covert vulnerabilities that can only be exploited by more sophisticated attack vectors (i.e., payloads). In this paper, we propose Scanner++, a framework that improves web vulnerability detection of existing scanners through combining their capabilities with attack intent synchronization. We design Scanner++ as a proxy-based architecture while using a package-based intent synchronization approach. Scanner++ first uses a purification mechanism to aggregate and refine attack intents, consisting of attack surfaces and attack vectors extracted from the base scanners' request packets. Then, Scanner++ uses a runtime intent synchronization mechanism to select relevant attack intents according to the scanners' detection spots to guide their scanning process. Consequently, base scanners can expand their attack surfaces, generate more diverse attack vectors and achieve better vulnerability detection performance.For evaluation, we implemented and integrated Scanner++ together with four widely used scanners, BurpSuite, AWVS, Arachni, and ZAP, testing it on ten benchmark web applications and three well-tested real-world web applications of a critical financial platform from our industry partner. Working under the Scanner++ framework helps BurpSuite, AWVS, Arachni, and ZAP cover 15.26%, 37.14%, 59.21%, 68.54% more pages, construct 12.95×, 1.13×, 15.03×, 52.66× more attack packets, and discover 77, 55, 77, 176 more bugs, respectively. Furthermore, Scanner++ detected eight serious previously unknown vulnerabilities on real-world applications, while the base scanners only found three of them.  © 2023 Association for Computing Machinery.",attack intent; scanner; synchronization; Web security,Benchmarking; Program debugging; Well testing; Attack intent; Attack vector; Performance; Purification mechanisms; Scanner; Target application; Vulnerability detection; WEB application; Web applications; WEB security; Synchronization
The Co-evolution of the WordPress Platform and Its Plugins,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152590800&doi=10.1145%2f3533700&partnerID=40&md5=3362753b8f065f757ad8141a42e83577,"One can extend the features of a software system by installing a set of additional components called plugins. WordPress, as a typical example of such plugin-based software ecosystems, is used by millions of websites and has a large number (i.e., 54,777) of available plugins. These plugin-based software ecosystems are different from traditional ecosystems (e.g., NPM dependencies) in the sense that there is high coupling between a platform and its plugins compared to traditional ecosystems for which components might not necessarily depend on each other (e.g., NPM libraries do not depend on a specific version of NPM or a specific version of a client software system). The high coupling between a plugin and its platform and other plugins causes incompatibility issues that occur during the co-evolution of a plugin and its platform as well as other plugins. In fact, incompatibility issues represent a major challenge when upgrading WordPress or its plugins. According to our study of the top 500 most-released WordPress plugins, we observe that incompatibility issues represent the third major cause for bad releases, which are rapidly (within the next 24 hours) fixed via urgent releases. Thirty-two percent of these incompatibilities are between a plugin and WordPress while 19% are between peer plugins. In this article, we study how plugins co-evolve with the underlying platform as well as other plugins, in an effort to understand the practices that are related support such co-evolution and reduce incompatibility issues. In particular, we investigate how plugins support the latest available versions of WordPress, as well as how plugins are related to each other, and how they co-evolve. We observe that a plugin's support of new versions of WordPress with a large amount of code change is risky, as the releases that declare such support have a higher chance to be followed by an urgent release compared to ordinary releases. Although plugins support the latest WordPress version, plugin developers omit important changes such as deleting the use of removed WordPress APIs, which are removed a median of 873 days after the APIs have been removed from the source code of WordPress. Plugins introduce new releases that are made according to a median of five other plugins, which we refer to as peer-triggered releases. A median of 20% of the peer-triggered releases are urgent releases that fix problems in their previous releases. The most common goal of peer-triggered releases is the fixing of incompatibility issues that a plugin detects as late as after a median of 36 days since the last release of another plugin. Our work sheds light on the co-evolution of WordPress plugins with their platform as well as peer plugins in an effort to uncover the practices of plugin evolution, so WordPress can accordingly design approaches to avoid incompatibility issues.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesPlugin-based ecosystems; incompatibility issues; plugins co-evolution,Computer software; Websites; Additional key word and phrasesplugin-based ecosystem; Co-evolution; Incompatibility issue; Key words; Plug-ins; Plugin co-evolution; Software ecosystems; Software-systems; Triggered release; Wordpress; Ecosystems
Semantics Foundation for Cyber-physical Systems Using Higher-order UTP,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152590644&doi=10.1145%2f3517192&partnerID=40&md5=bcb98593fb603352ffba74e8c2e4fc98,"Model-based design has become the predominant approach to the design of hybrid and cyber-physical systems (CPSs). It advocates the use of mathematically founded models to capture heterogeneous digital and analog behaviours from domain-specific formalisms, allowing all engineering tasks of verification, code synthesis, and validation to be performed within a single semantic body. Guaranteeing the consistency among the different views and heterogeneous models of a system at different levels of abstraction, however, poses significant challenges. To address these issues, Hoare and He's Unifying Theories of Programming (UTP) proposes a calculus to capture domain-specific programming and modelling paradigms into a unified semantic framework. Our goal is to extend UTP to form a semantic foundation for CPS design. Higher-order UTP (HUTP) is a conservative extension to Hoare and He's theory that supports the specification of discrete, real-time, and continuous dynamics, concurrency and communication, and higher-order quantification. Within HUTP, we define a calculus of normal hybrid designs to model, analyse, compose, refine, and verify heterogeneous hybrid system models. In addition, we define respective formal semantics for Hybrid Communicating Sequential Processes and Simulink using HUTP.  © 2023 Association for Computing Machinery.",CPS; model-based design; semantic model; UTP,Embedded systems; Formal methods; Hybrid systems; Semantics; Analog behavior; Cybe-physical systems; Cyber-physical systems; Domain specific; High-order; Higher-order; Model-based design; Semantic foundation; Semantic modelling; Unifying Theories of Programming; Cyber Physical System
Preference-wise Testing of Android Apps via Test Amplification,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149426449&doi=10.1145%2f3511804&partnerID=40&md5=05bd07c3711d4e8443721af18db6b4df,"Preferences, the setting options provided by Android, are an essential part of Android apps. Preferences allow users to change app features and behaviors dynamically, and therefore their impacts need to be considered when testing the apps. Unfortunately, few test cases explicitly specify the assignments of valid values to the preferences, or configurations, under which they should be executed, and few existing mobile testing tools take the impact of preferences into account or provide help to testers in identifying and setting up the configurations for running the tests. This article presents the Prefest approach to effective testing of Android apps with preferences. Given an Android app and a set of test cases for the app, Prefest amplifies the test cases with a small number of configurations to exercise more behaviors and detect more bugs that are related to preferences. In an experimental evaluation conducted on real-world Android apps, amplified test cases produced by Prefest from automatically generated test cases covered significantly more code of the apps and detected seven real bugs, and the tool's test amplification time was at the same order of magnitude as the running time of the input test cases. Prefest's effectiveness and efficiency in amplifying programmer-written test cases was comparable with that in amplifying automatically generated test cases.  © 2023 Association for Computing Machinery.",Android apps; Android testing; preference-wise testing,Automation; Program debugging; Android apps; Android testing; Automatically generated; Effective testing; Experimental evaluation; Mobile testing; Preference-wise testing; Test amplifications; Test case; Testing tools; Android (operating system)
Automated Identification of Uniqueness in JUnit Tests,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152594957&doi=10.1145%2f3533313&partnerID=40&md5=76daa00b59583f35d7558d5c3fccb605,"In the context of testing, descriptive test names are desirable because they document the purpose of tests and facilitate comprehension tasks during maintenance. Unfortunately, prior work has shown that tests often do not have descriptive names. To address this limitation, techniques have been developed to automatically generate descriptive names. However, they often generated names that are invalid or do not meet developer approval. To help address these limitations, we present a novel approach to extract the attributes of a given test that make it unique among its siblings. Because such attributes often serve as the basis for descriptive names, identifying them is an important first step towards improving test name generation approaches. To evaluate the approach, we created a prototype implementation for JUnit tests and compared its output with human judgment. The results of the evaluation demonstrate that the attributes identified by the approach are consistent with human judgment and are likely to be useful for future name generation techniques.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesUnit testing; formal concept analysis,Additional key word and phrasesunit testing; Automated identification; Comprehension tasks; Descriptive names; Formal concepts analysis; Generation techniques; Human judgments; Key words; Prototype implementations; Formal concept analysis
On Wasted Contributions: Understanding the Dynamics of Contributor-Abandoned Pull Requests-A Mixed-Methods Study of 10 Large Open-Source Projects,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152596575&doi=10.1145%2f3530785&partnerID=40&md5=896b8b459ca14396a5a38f14d575e276,"Pull-based development has enabled numerous volunteers to contribute to open-source projects with fewer barriers. Nevertheless, a considerable amount of pull requests (PRs) with valid contributions are abandoned by their contributors, wasting the effort and time put in by both the contributors and maintainers. To better understand the underlying dynamics of contributor-abandoned PRs, we conduct a mixed-methods study using both quantitative and qualitative methods. We curate a dataset consisting of 265,325 PRs including 4,450 abandoned ones from ten popular and mature GitHub projects and measure 16 features characterizing PRs, contributors, review processes, and projects. Using statistical and machine learning techniques, we find that complex PRs, novice contributors, and lengthy reviews have a higher probability of abandonment and the rate of PR abandonment fluctuates alongside the projects' maturity or workload. To identify why contributors abandon their PRs, we also manually examine a random sample of 354 abandoned PRs. We observe that the most frequent abandonment reasons are related to the obstacles faced by contributors, followed by the hurdles imposed by maintainers during the review process. Finally, we survey the top core maintainers of the studied projects to understand their perspectives on dealing with PR abandonment and on our findings.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSocio-technical factors; mixed-methods research; modern code review; open-source software; pull-based development; social coding platforms,Codes (symbols); Economic and social effects; Learning systems; Open systems; Additional key word and phrasessocio-technical factor; Code review; Coding platform; Key words; Mixed-methods research; Modern code review; Open-source softwares; Pull-based development; Social coding platform; Technical factors; Open source software
TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152591316&doi=10.1145%2f3560263&partnerID=40&md5=c733e3ad23e337dbc5c074edf2b9384a,"Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware, a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7% precision. TokenAware with optimizations merely incurs 4% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesEthereum; bookkeeping recognition; smart contract; token,Blockchain; Additional key word and phrasesethereum; Behavior detection; Block-chain; Bookkeeping recognition; Bytecodes; Key words; Malicious behavior; Optimisations; Token; Type information; Smart contract
"Bash in the Wild: Language Usage, Code Smells, and Bugs",2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152598064&doi=10.1145%2f3517193&partnerID=40&md5=cf2744364ebdb8322387c1708f56cb80,"The Bourne-again shell (Bash) is a prevalent scripting language for orchestrating shell commands and managing resources in Unix-like environments. It is one of the mainstream shell dialects that is available on most GNU Linux systems. However, the unique syntax and semantics of Bash could easily lead to unintended behaviors if carelessly used. Prior studies primarily focused on improving the reliability of Bash scripts or facilitating writing Bash scripts; there is yet no empirical study on the characteristics of Bash programs written in reality, e.g., frequently used language features, common code smells, and bugs. In this article, we perform a large-scale empirical study of Bash usage, based on analyses over one million open source Bash scripts found in Github repositories. We identify and discuss which features and utilities of Bash are most often used. Using static analysis, we find that Bash scripts are often error-prone, and the error-proneness has a moderately positive correlation with the size of the scripts. We also find that the most common problem areas concern quoting, resource management, command options, permissions, and error handling. We envision that these findings can be beneficial for learning Bash and future research that aims to improve shell and command-line productivity and reliability.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bash; bugs; code smells; Empirical studies; language features; shell scripts,Codes (symbols); Computer operating systems; Errors; Odors; Open source software; Program debugging; Semantics; Shells (structures); Bash; Bug; Code smell; Empirical studies; GNU-Linux; Language features; Managing resources; Scripting languages; Shell command; Shell script; Static analysis
APIRO: A Framework for Automated Security Tools API Recommendation,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152604475&doi=10.1145%2f3512768&partnerID=40&md5=5c2823438c55cd062b66b5c5ea8aae95,"Security Orchestration, Automation, and Response (SOAR) platforms integrate and orchestrate a wide variety of security tools to accelerate the operational activities of Security Operation Center (SOC). Integration of security tools in a SOAR platform is mostly done manually using APIs, plugins, and scripts. SOC teams need to navigate through API calls of different security tools to find a suitable API to define or update an incident response action. Analyzing various types of API documentation with diverse API format and presentation structure involves significant challenges such as data availability, data heterogeneity, and semantic variation for automatic identification of security tool APIs specific to a particular task. Given these challenges can have negative impact on SOC team's ability to handle security incident effectively and efficiently, we consider it important to devise suitable automated support solutions to address these challenges. We propose a novel learning-based framework for automated security tool API Recommendation for security Orchestration, automation, and response, APIRO. To mitigate data availability constraint, APIRO enriches security tool API description by applying a wide variety of data augmentation techniques. To learn data heterogeneity of the security tools and semantic variation in API descriptions, APIRO consists of an API-specific word embedding model and a Convolutional Neural Network (CNN) model that are used for prediction of top three relevant APIs for a task. We experimentally demonstrate the effectiveness of APIRO in recommending APIs for different tasks using three security tools and 36 augmentation techniques. Our experimental results demonstrate the feasibility of APIRO for achieving 91.9% Top-1 Accuracy. Compared to the state-of-the-art baseline, APIRO is 26.93%, 23.03%, and 20.87% improved in terms of Top-1, Top-2, and Top-3 Accuracy and outperforms the baseline by 23.7% in terms of Mean Reciprocal Rank (MRR).  © 2023 Association for Computing Machinery.",API Recommendation; Incident Response Plan; Security Operation Center; Security Orchestration; security tool API; SOAR,"Semantics; API recommendation; Augmentation techniques; Data availability; Data heterogeneity; Incident response plans; Security operation center; Security orchestration; Security orchestration, automation, and response; Security tool API; Security tools; Automation"
The Weights Can Be Harmful: Pareto Search versus Weighted Search in Multi-objective Search-based Software Engineering,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152604993&doi=10.1145%2f3514233&partnerID=40&md5=fd13abe456997ed1338061b0138e4027,"In presence of multiple objectives to be optimized in Search-Based Software Engineering (SBSE), Pareto search has been commonly adopted. It searches for a good approximation of the problem's Pareto-optimal solutions, from which the stakeholders choose the most preferred solution according to their preferences. However, when clear preferences of the stakeholders (e.g., a set of weights that reflect relative importance between objectives) are available prior to the search, weighted search is believed to be the first choice, since it simplifies the search via converting the original multi-objective problem into a single-objective one and enables the search to focus on what only the stakeholders are interested in.This article questions such a ""weighted search first""belief. We show that the weights can, in fact, be harmful to the search process even in the presence of clear preferences. Specifically, we conduct a large-scale empirical study that consists of 38 systems/projects from three representative SBSE problems, together with two types of search budget and nine sets of weights, leading to 604 cases of comparisons. Our key finding is that weighted search reaches a certain level of solution quality by consuming relatively less resources at the early stage of the search; however, Pareto search is significantly better than its weighted counterpart the majority of the time (up to 77% of the cases), as long as we allow a sufficient, but not unrealistic search budget. This is a beneficial result, as it discovers a potentially new ""rule-of-thumb""for the SBSE community: Even when clear preferences are available, it is recommended to always consider Pareto search by default for multi-objective SBSE problems, provided that solution quality is more important. Weighted search, in contrast, should only be preferred when the resource/search budget is limited, especially for expensive SBSE problems. This, together with other findings and actionable suggestions in the article, allows us to codify pragmatic and comprehensive guidance on choosing weighted and Pareto search for SBSE under the circumstance that clear preferences are available. All code and data can be accessed at https://github.com/ideas-labo/pareto-vs-weight-for-sbse.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive systems; configurable systems; multi-objective optimization; pareto optimization; quality evaluation; quality indicator; Search-based software engineering; self-adaptive systems; user preference,Adaptive systems; Budget control; Pareto principle; Software engineering; Configurable systems; Engineering problems; Multi-objectives optimization; Pareto-optimization; Quality evaluation; Quality indicators; Search-based; Search-based software engineering; Self-adaptive system; User's preferences; Multiobjective optimization
Defining a Knowledge Graph Development Process Through a Systematic Review,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138203254&doi=10.1145%2f3522586&partnerID=40&md5=a9a50efa6a0e46f42b67e273bf4bdc2d,"Knowledge graphs are widely used in industry and studied within the academic community. However, the models applied in the development of knowledge graphs vary. Analysing and providing a synthesis of the commonly used approaches to knowledge graph development would provide researchers and practitioners a better understanding of the overall process and methods involved. Hence, this article aims at defining the overall process of knowledge graph development and its key constituent steps. For this purpose, a systematic review and a conceptual analysis of the literature was conducted. The resulting process was compared to case studies to evaluate its applicability. The proposed process suggests a unified approach and provides guidance for both researchers and practitioners when constructing and managing knowledge graphs.  © 2023 Copyright held by the owner/author(s).",development process semantic network; information integration; knowledge graph construction; Knowledge graphs,Air navigation; Graphic methods; Semantic Web; Semantics; Development process; Development process semantic network; Graph construction; Information integration; Knowledge graph construction; Knowledge graphs; Process semantics; Semantics networks; Systematic Review; Knowledge graph
Combatting Energy Issues for Mobile Applications,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146923784&doi=10.1145%2f3527851&partnerID=40&md5=d38907bd0a1db47745e8e4282f9a0a71,"Energy efficiency is an important criterion to judge the quality of mobile apps, but one third of our arbitrarily sampled apps suffer from energy issues that can quickly drain battery power. To understand these issues, we conduct an empirical study on 36 well-maintained apps such as Chrome and Firefox, whose issue tracking systems are publicly accessible. Our study involves issue causes, manifestation, fixing efforts, detection techniques, reasons of no-fixes, and debugging techniques. Inspired by the empirical study, we propose a novel testing framework for detecting energy issues in real-world mobile apps. Our framework examines apps with well-designed input sequences and runtime context. We develop leading edge technologies, e.g., pre-designing input sequences with potential energy overuse and tuning tests on-the-fly, to achieve high efficacy in detecting energy issues. A large-scale evaluation shows that 90.4% of the detected issues in our experiments were previously unknown to developers. On average, these issues can double the energy consumption of the test cases where the issues were detected. And our test achieves a low number of false positives. Finally, we show how our test reports can help developers fix the issues.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesMobile applications; android; energy bugs; energy issues,Android (operating system); Energy efficiency; Energy utilization; Program debugging; Software testing; Additional key word and phrasesmobile application; Android; Empirical studies; Energy; Energy bug; Energy issues; Input sequence; Key words; Mobile app; Mobile applications; Potential energy
Fold2Vec: Towards a Statement-Based Representation of Code for Code Comprehension,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152605859&doi=10.1145%2f3514232&partnerID=40&md5=784ed33b7e8feda931e42a777267bfca,"We introduce a novel approach to source code representation to be used in combination with neural networks. Such a representation is designed to permit the production of a continuous vector for each code statement. In particular, we present how the representation is produced in the case of Java source code. We test our representation for three tasks: code summarization, statement separation, and code search. We compare with the state-of-the-art non-autoregressive and end-to-end models for these tasks. We conclude that all tasks benefit from the proposed representation to boost their performance in terms of F1-score, accuracy, and mean reciprocal rank, respectively. Moreover, we show how models trained on code summarization and models trained on statement separation can be combined to address methods with tangled responsibilities, meaning that these models can be used to detect code misconduct.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Big code; intent identification; learning representations; method name suggestion,Big code; Code comprehension; Code search; Intent identification; Java source codes; Learning representation; Method name suggestion; Neural-networks; Source code representations; State of the art
An In-depth Study of Java Deserialization Remote-Code Execution Exploits and Vulnerabilities,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152601021&doi=10.1145%2f3554732&partnerID=40&md5=0f607b99a534faeb7b5023c76d52dc2b,"Nowadays, an increasing number of applications use deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities are so critical that they are in OWASP's list of top 10 security risks for web applications. This is mainly caused by faults in the development process of applications and by flaws in their dependencies, i.e., flaws in the libraries used by these applications. No previous work has studied deserialization attacks in-depth: How are they performed? How are weaknesses introduced and patched? And for how long are vulnerabilities present in the codebase? To yield a deeper understanding of this important kind of vulnerability, we perform two main analyses: one on attack gadgets, i.e., exploitable pieces of code, present in Java libraries, and one on vulnerabilities present in Java applications. For the first analysis, we conduct an exploratory large-scale study by running 256515 experiments in which we vary the versions of libraries for each of the 19 publicly available exploits. Such attacks rely on a combination of gadgets present in one or multiple Java libraries. A gadget is a method which is using objects or fields that can be attacker-controlled. Our goal is to precisely identify library versions containing gadgets and to understand how gadgets have been introduced and how they have been patched. We observe that the modification of one innocent-looking detail in a class - such as making it public - can already introduce a gadget. Furthermore, we noticed that among the studied libraries, 37.5% are not patched, leaving gadgets available for future attacks.For the second analysis, we manually analyze 104 deserialization vulnerabilities CVEs to understand how vulnerabilities are introduced and patched in real-life Java applications. Results indicate that the vulnerabilities are not always completely patched or that a workaround solution is proposed. With a workaround solution, applications are still vulnerable since the code itself is unchanged.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSerialization; deserialization; gadget; remote code execution RCE; vulnerabilities,Codes (symbols); Java programming language; Security of data; Additional key word and phrasesserialization; Code execution; Deserialization; Gadget; Key words; Remote code; Remote code execution remote code execution; Vulnerability; Libraries
"A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning",2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152600056&doi=10.1145%2f3511805&partnerID=40&md5=1738f496a11c3ad268cedcd41825b9a5,"Most software companies have extensive test suites and re-run parts of them continuously to ensure that recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This article analyses two decades of TCP research and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to system under test code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes.  © 2023 Association for Computing Machinery.",industry; machine learning; Regression testing; taxonomy; test case prioritisation,Cost benefit analysis; Software testing; Transmission control protocol; Adverse effect; Classifieds; Industry needs; Machine-learning; Re-runs; Regression testing; Software company; Systems under tests; Test case; Test case prioritization; Machine learning
Code Structure-Guided Transformer for Source Code Summarization,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152605091&doi=10.1145%2f3522674&partnerID=40&md5=1bdc137646f3fa8ab9e753e42179ae6f,"Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality.  © 2023 Association for Computing Machinery.",code structure; Code summary; multi-head attention; Transformer,Benchmarking; Codes (symbols); Data flow analysis; Learning systems; Syntactics; Code structure; Code summary; Learning techniques; Multi-head attention; Performance; Sequence models; Source codes; Structure information; Task domain; Transformer; Deep learning
Feedback-Directed Metamorphic Testing,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152603512&doi=10.1145%2f3533314&partnerID=40&md5=a0dd0a3b58aa7e5eb3567fab5149877f,"Over the past decade, metamorphic testing has gained rapidly increasing attention from both academia and industry, particularly thanks to its high efficacy on revealing real-life software faults in a wide variety of application domains. On the basis of a set of metamorphic relations among multiple software inputs and their expected outputs, metamorphic testing not only provides a test case generation strategy by constructing new (or follow-up) test cases from some original (or source) test cases, but also a test result verification mechanism through checking the relationship between the outputs of source and follow-up test cases. Many efforts have been made to further improve the cost-effectiveness of metamorphic testing from different perspectives. Some studies attempted to identify ""good""metamorphic relations, while other studies were focused on applying effective test case generation strategies especially for source test cases. In this article, we propose improving the cost-effectiveness of metamorphic testing by leveraging the feedback information obtained in the test execution process. Consequently, we develop a new approach, namely feedback-directed metamorphic testing, which makes use of test execution information to dynamically adjust the selection of metamorphic relations and selection of source test cases. We conduct an empirical study to evaluate the proposed approach based on four laboratory programs, one GNU program, and one industry program. The empirical results show that feedback-directed metamorphic testing can use fewer test cases and take less time than the traditional metamorphic testing for detecting the same number of faults. It is clearly demonstrated that the use of feedback information about test execution does help enhance the cost-effectiveness of metamorphic testing. Our work provides a new perspective to improve the efficacy and applicability of metamorphic testing as well as many other software testing techniques.  © 2023 Association for Computing Machinery.",adaptive partition testing; Additional Key Words and PhrasesMetamorphic testing; feedback control; metamorphic relation; random testing; test execution,Application programs; Feedback control; Information use; Open source software; Software testing; Verification; Well testing; Adaptive partition testing; Adaptive partitions; Additional key word and phrasesmetamorphic testing; Key words; Metamorphic relations; Metamorphic testing; Partition testing; Random testing; Test case; Test execution; Cost effectiveness
Mutation Testing in Evolving Systems: Studying the Relevance of Mutants to Code Evolution,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152596004&doi=10.1145%2f3530786&partnerID=40&md5=f6264ac9345ba49fe0db2b48328ee945,"Context:When software evolves, opportunities for introducing faults appear. Therefore, it is important to test the evolved program behaviors during each evolution cycle. However, while software evolves, its complexity is also evolving, introducing challenges to the testing process. To deal with this issue, testing techniques should be adapted to target the effect of the program changes instead of the entire program functionality. To this end, commit-aware mutation testing, a powerful testing technique, has been proposed. Unfortunately, commit-aware mutation testing is challenging due to the complex program semantics involved. Hence, it is pertinent to understand the characteristics, predictability, and potential of the technique.Objective: We conduct an exploratory study to investigate the properties of commit-relevant mutants, i.e., the test elements of commit-aware mutation testing, by proposing a general definition and an experimental approach to identify them. We thus aim at investigating the prevalence, location, and comparative advantages of commit-aware mutation testing over time (i.e., the program evolution). We also investigate the predictive power of several commit-related features in identifying and selecting commit-relevant mutants to understand the essential properties for its best-effort application case.Method: Our commit-relevant definition relies on the notion of observational slicing, approximated by higher-order mutation. Specifically, our approach utilizes the impact of mutants, effects of one mutant on another in capturing and analyzing the implicit interactions between the changed and unchanged code parts. The study analyses millions of mutants (over 10 million), 288 commits, five (5) different open-source software projects involving over 68,213 CPU days of computation and sets a ground truth where we perform our analysis.Results: Our analysis shows that commit-relevant mutants are located mainly outside of program commit change (81%), suggesting a limitation in previous work. We also note that effective selection of commit-relevant mutants has the potential of reducing the number of mutants by up to 93%. In addition, we demonstrate that commit relevant mutation testing is significantly more effective and efficient than state-of-the-art baselines, i.e., random mutant selection and analysis of only mutants within the program change. In our analysis of the predictive power of mutants and commit-related features (e.g., number of mutants within a change, mutant type, and commit size) in predicting commit-relevant mutants, we found that most proxy features do not reliably predict commit-relevant mutants.Conclusion: This empirical study highlights the properties of commit-relevant mutants and demonstrates the importance of identifying and selecting commit-relevant mutants when testing evolving software systems.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSoftware testing; continuous integration; evolving-systems; mutation testing,Codes (symbols); Integration; Integration testing; Open source software; Open systems; Verification; Additional key word and phrasessoftware testing; Continuous integrations; Evolving systems; Key words; Mutation testing; Predictive power; Program behavior; Property; Testing process; Testing technique; Semantics
ActivFORMS: A Formally Founded Model-based Approach to Engineer Self-adaptive Systems,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152593093&doi=10.1145%2f3522585&partnerID=40&md5=032bbf4b0ccc75fd24f1c2e4c5b0afbb,"Self-adaptation equips a computing system with a feedback loop that enables it to deal with change caused by uncertainties during operation, such as changing availability of resources and fluctuating workloads. To ensure that the system complies with the adaptation goals, recent research suggests the use of formal techniques at runtime. Yet, existing approaches have three limitations that affect their practical applicability: (i) they ignore correctness of the behavior of the feedback loop, (ii) they rely on exhaustive verification at runtime to select adaptation options to realize the adaptation goals, which is time- and resource-demanding, and (iii) they provide limited or no support for changing adaptation goals at runtime. To tackle these shortcomings, we present ActivFORMS (Active FORmal Models for Self-adaptation). ActivFORMS contributes an end-to-end approach for engineering self-adaptive systems, spanning four main stages of the life cycle of a feedback loop: design, deployment, runtime adaptation, and evolution. We also present ActivFORMS-ta, a tool-supported instance of ActivFORMS that leverages timed automata models and statistical model checking at runtime. We validate the research results using an IoT application for building security monitoring that is deployed in Leuven. The experimental results demonstrate that ActivFORMS supports correctness of the behavior of the feedback loop, achieves the adaptation goals in an efficient way, and supports changing adaptation goals at runtime.  © 2023 Copyright held by the owner/author(s).",executable models; formal techniques; Internet of Things; MAPE-K; Self-adaptation; statistical model checking,Adaptive systems; Feedback; Formal methods; Life cycle; Model checking; Executable modeling; Feedback loops; Formal modeling; Formal techniques; Mape; MAPE-K; Runtimes; Self- adaptations; Self-adaptive system; Statistical model checking; Internet of things
There's no Such Thing as a Free Lunch: Lessons Learned from Exploring the Overhead Introduced by the Greenkeeper Dependency Bot in Npm,2023,ACM Transactions on Software Engineering and Methodology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152592558&doi=10.1145%2f3522587&partnerID=40&md5=b6fb400497f3b9f136765b1aee657b16,"Dependency management bots are increasingly being used to support the software development process, for example, to automatically update a dependency when a new version is available. Yet, human intervention is often required to either accept or reject any action or recommendation the bot creates. In this article, our objective is to study the extent to which dependency management bots create additional, and sometimes unnecessary, work for their users. To accomplish this, we analyze 93,196 issue reports opened by Greenkeeper, a popular dependency management bot used in open source software projects in the npm ecosystem. We find that Greenkeeper is responsible for half of all issues reported in client projects, inducing a significant amount of overhead that must be addressed by clients, since many of these issues were created as a result of Greenkeeper taking incorrect action on a dependency update (i.e., false alarms). Reverting a broken dependency update to an older version, which is a potential solution that requires the least overhead and is automatically attempted by Greenkeeper, turns out to not be an effective mechanism. Finally, we observe that 56% of the commits referenced by Greenkeeper issue reports only change the client's dependency specification file to resolve the issue. Based on our findings, we argue that dependency management bots should (i) be configurable to allow clients to reduce the amount of generated activity by the bots, (ii) take into consideration more sources of information than only the pass/fail status of the client's build pipeline to help eliminate false alarms, and (iii) provide more effective incentives to encourage clients to resolve dependency issues.  © 2023 Association for Computing Machinery.",Dependency management; greenkeeper; mining software repositories; overhead; software bots,Botnet; Errors; Open systems; Software design; Dependency management; Falsealarms; Greenkeeper; Human intervention; Mining software; Mining software repository; Open source software projects; Overhead; Software development process; Software repositories; Open source software
