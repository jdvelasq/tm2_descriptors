Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Analysis and solution of CSS-sprite packing problem,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953325162&doi=10.1145%2f2818377&partnerID=40&md5=62dce5aae6e46547f4e505ff555d1ebf,"A CSS-sprite packing problem is considered in this article. CSS-sprite is a technique of combining many pictures of a web page into one image for the purpose of reducing network transfer time. The CSS-sprite packing problem is formulated here as an optimization challenge. The significance of geometric packing, image compression and communication performance is discussed. A mathematical model for constructing multiple sprites and optimization of load time is proposed. The impact of PNG-sprite aspect ratio on file size is studied experimentally. Benchmarking of real user web browsers communication performance covers latency, bandwidth, number of concurrent channels as well as speedup from parallel download. Existing software for building CSS-sprites is reviewed. A novel method, called Spritepack, is proposed and evaluated. Spritepack outperforms current software. © 2015 ACM.",CSS image sprites; Heuristics; Image compression; JPEG; Load time reduction; PNG; Rectangle packing; Web engineering; Web optimization,Aspect ratio; Benchmarking; Electronic guidance systems; Image compression; Optimization; Websites; CSS image sprites; Heuristics; JPEG; PNG; Rectangle packing; Time reduction; Web engineering; Web browsers
Diversionary comments under blog posts,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942744945&doi=10.1145%2f2789211&partnerID=40&md5=58396a2f9746b03c85d1b10143c9e276,"There has been a recent swell of interest in the analysis of blog comments. However, much of the work focuses on detecting comment spam in the blogsphere. An important issue that has been neglected so far is the identification of diversionary comments. Diversionary comments are defined as comments that divert the topic from the original post. A possible purpose is to distract readers from the original topic and draw attention to a new topic. We categorize diversionary comments into five types based on our observations and propose an effective framework to identify and flag them. To the best of our knowledge, the problem of detecting diversionary comments has not been studied so far. We solve the problem in two different ways: (i) rank all comments in descending order of being diversionary and (ii) consider it as a classification problem. Our evaluation on 4,179 comments under 40 different blog posts from Digg and Reddit shows that the proposed method achieves the high mean average precision of 91.9% when the problem is considered as a ranking problem and 84.9% of F-measure as a classification problem. Sensitivity analysis indicates that the effectiveness of the method is stable under different parameter settings. © 2015 ACM.",Classification; Coreference resolution; Diversionary comments; Extraction from Wikipedia; Hierarchical Dirichlet process; Latent Dirichlet allocation; Ranking; Spam; Topic model,Classification (of information); Sensitivity analysis; Statistics; Co-reference resolutions; Diversionary comments; Hierarchical Dirichlet process; Latent Dirichlet allocation; Ranking; Spam; Topic Modeling; Wikipedia; Blogs
Fona: Quantitative metric to measure focus navigation on rich internet applications,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942801559&doi=10.1145%2f2812812&partnerID=40&md5=5ebac8caa1c579f56a3084ec802970b1,"The Web 2.0 brought new requirements to the architecture of web systems. Web applications' interfaces are becoming more and more interactive. However, these changes are severely impacting how disabled users interact through assistive technologies with the web. In order to deploy an accessible web application, developers can use WAI-ARIA to design an accessible web application, which manually implements focus and keyboard navigation mechanisms. This article presents a quantitative metric, named Fona, which measures how the Focus Navigation WAI-ARIA requirement has been implemented on the web. Fona counts JavaScript mouse event listeners, HTML elements with role attributes, and TabIndex attributes in the DOM structure of webpages. Fona's evaluation approach provides a narrow analysis of one single accessibility requirement. But it enables monitoring this accessibility requirement in a large number of webpages. This monitoring activity might be used to give insights about how Focus Navigation and ARIA requirements have been considered by web development teams. Fona is validated comparing the results of a set of WAI-ARIA conformant implementations and a set of web pages formed by Alexa's 349 top most popular websites. The analysis of Fona's value for Alexa's websites highlights that many websites still lack the implementation of Focus Navigation through their JavaScript interactive content. © 2015 ACM.",ARIA; Focus navigation; Web accessibility,High level languages; Mammals; Navigation; Web services; Accessibility requirements; ARIA; Assistive technology; Evaluation approach; Interactive contents; Monitoring activities; Rich Internet Applications; Web accessibility; Websites
Improving researcher homepage classification with unlabeled data,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946055462&doi=10.1145%2f2767135&partnerID=40&md5=5da805ddaa7f8050cd38d08d507d08c2,"A classifier that determines if a webpage is relevant to a specified set of topics comprises a key component for focused crawling. Can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant pages in the face of changing content on the Web? We investigate this question in the context of identifying researcher homepages. We show experimentally that classifiers trained on existing datasets of academic homepages underperform on ""non-homepages"" present on current-day academic websites. As an alternative to obtaining labeled datasets to retrain classifiers for the new content, in this article we ask the following question: ""How can we effectively use the unlabeled data readily available from academic websites to improve researcher homepage classification?"" We design novel URL-based features and use them in conjunction with content-based features for representing homepages. Within the co-training framework, these sets of features can be treated as complementary views enabling us to effectively use unlabeled data and obtain remarkable improvements in homepage identification on the current-day academic websites. We also propose a novel technique for ""learning a conforming pair of classifiers"" that mimics co-training. Our algorithm seeks to minimize a loss (objective) function quantifying the difference in predictions from the two views afforded by co-training. We argue that this loss formulation provides insights for understanding co-training and can be used even in the absence of a validation dataset. Our next set of findings pertains to the evaluation of other state-of-the-art techniques for classifying homepages. First, we apply feature selection (FS) and feature hashing (FH) techniques independently and in conjunction with co-training to academic homepages. FS is a well-known technique for removing redundant and unnecessary features from the data representation, whereas FH is a technique that uses hash functions for efficient encoding of features. We show that FS can be effectively combined with co-training to obtain further improvements in identifying homepages. However, using hashed feature representations, a performance degradation is observed possibly due to feature collisions. Finally, we evaluate other semisupervised algorithms for homepage classification. We show that although several algorithms are effective in using information from the unlabeled instances, co-training that explicitly harnesses the feature split in the underlying instances outperforms approaches that combine content and URL features into a single view. © 2015 ACM 1559-1131/2015/10-ART17 $15.00.",Co-training; Conforming classifiers; Researcher homepage classification; Unlabeled data,Function evaluation; Hash functions; Websites; Co-training; Content-based features; Feature representation; Homepage; Performance degradation; Semi-supervised algorithm; State-of-the-art techniques; Unlabeled data; Classification (of information)
Estimating clustering coefficients and size of social networks via random walk,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942778903&doi=10.1145%2f2790304&partnerID=40&md5=dfca57b9a88e8854b9b09488ac82c33b,"This work addresses the problem of estimating social network measures. Specifically, the measures at hand are the network average and global clustering coefficients and the number of registered users. The algorithms at hand (1) assume no prior knowledge about the network and (2) access the network using only the publicly available interface. More precisely, this work provides (a) a unified approach for clustering coefficients estimation and (b) a new network size estimator. The unified approach for the clustering coefficients yields the first external access algorithm for estimating the global clustering coefficient. The new network size estimator offers improved accuracy compared to prior art estimators. Our approach is to view a social network as an undirected graph and use the public interface to retrieve a random walk. To estimate the clustering coefficient, the connectivity of each node in the random walk sequence is tested in turn. We show that the error drops exponentially in the number of random walk steps. For the network size estimation we offer a generalized view of prior art estimators that in turn yields an improved estimator. All algorithms are validated on several publicly available social network datasets. © 2015 ACM.",Clustering coefficient; Estimation; Sampling; Social network,Clustering algorithms; Estimation; Random processes; Sampling; Social networking (online); Clustering coefficient; Global clustering; Network measures; Network size; Prior knowledge; Random Walk; Undirected graph; Unified approach; Arts computing
Detection of political manipulation in online communities through measures of effort and collaboration,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932620171&doi=10.1145%2f2767134&partnerID=40&md5=375ecb0b5f05a38c955b4576f00b78d5,"Online social media allow users to interact with one another by sharing opinions, and these opinions have a critical impact on the way readers think and behave. Accordingly, an increasing number of manipulators deliberately spread messages to influence the public, often in an organized manner. In particular, political manipulation-manipulation of opponents to win political advantage-can result in serious consequences: antigovernment riots can break out, leading to candidates' defeat in an election. A few approaches have been proposed to detect such manipulation based on the level of social interaction (i.e., manipulators actively post opinions but infrequently befriend and reply to other users). However, several studies have shown that the interactions can be forged at a low cost and thus may not be effective measures of manipulation. To go one step further, we collect a dataset for real, large-scale political manipulation, which consists of opinions found on Internet forums. These opinions are divided into manipulators and nonmanipulators. Using this collection, we demonstrate that manipulators inevitably work hard, in teams, to quickly influence a large audience. With this in mind, it could be said that a high level of collaborative efforts strongly indicates manipulation. For example, a group of manipulators may jointly post numerous opinions with a consistent theme and selectively recommend the same, well-organized opinion to promote its rank. We show that the effort measures, when combined with a supervised learning algorithm, successfully identify greater than 95% of the manipulators. We believe that the proposed method will help system administrators to accurately detect manipulators in disguise, significantly decreasing the intensity of manipulation. © 2015 ACM.",Machine learning; Online social media; Opinion manipulation; Political manipulation,Artificial intelligence; Learning algorithms; Learning systems; Manipulators; Social sciences; Effective measures; Help systems; Internet forums; On-line communities; Online social medias; Opinion manipulation; Political manipulation; Social interactions; Social networking (online)
A supervised learning approach to protect client authentication on the web,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932631843&doi=10.1145%2f2754933&partnerID=40&md5=e1db7b0977edc884d9eabb0becf8c435,"Browser-based defenses have recently been advocated as an effective mechanism to protect potentially insecure web applications against the threats of session hijacking, fixation, and related attacks. In existing approaches, all such defenses ultimately rely on client-side heuristics to automatically detect cookies containing session information, to then protect them against theft or otherwise unintended use. While clearly crucial to the effectiveness of the resulting defense mechanisms, these heuristics have not, as yet, undergone any rigorous assessment of their adequacy. In this article, we conduct the first such formal assessment, based on a ground truth of 2,464 cookies we collect from 215 popular websites of the Alexa ranking. To obtain the ground truth, we devise a semiautomatic procedure that draws on the novel notion of authentication token, which we introduce to capture multiple web authentication schemes. We test existing browser-based defenses in the literature against our ground truth, unveiling several pitfalls both in the heuristics adopted and in the methods used to assess them. We then propose a new detection method based on supervised learning, where our ground truth is used to train a set of binary classifiers, and report on experimental evidence that our method outperforms existing proposals. Interestingly, the resulting classifiers, together with our hands-on experience in the construction of the ground truth, provide new insight on how web authentication is actually implemented in practice. © 2015 ACM.",Authentication cookies; Classification; Web security,Classification (of information); Heuristic methods; Social networking (online); Supervised learning; Authentication token; Client authentication; Effective mechanisms; Experimental evidence; Semi-automatic procedures; Supervised learning approaches; Web authentication; WEB security; Authentication
Reliable and resilient trust management in distributed service provision networks,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932627100&doi=10.1145%2f2754934&partnerID=40&md5=f48e227e45d8bd57a0dcbc473778d5d5,"Distributed service networks are popular platforms for service providers to offer services to consumers and for service consumers to acquire services from unknown parties. eBay and Amazon are two well-known examples of enabling and hosting such service networks to connect service providers to service consumers. Trust management is a critical component for scaling such distributed service networks to a large and growing number of participants. In this article, we present ServiceTrust++, a feedback quality-sensitive and attack resilient trust management scheme for empowering distributed service networks with effective trust management capability. Compared with existing trust models, ServiceTrust++ has several novel features. First, we present six attack models to capture both independent and colluding attacks with malicious cliques, malicious spies, and malicious camouflages. Second, we aggregate the feedback ratings based on the variances of participants' feedback behaviors and incorporate feedback similarity as weight into the local trust algorithm. Third, we compute the global trust of a participant by employing conditional trust propagation based on the feedback similarity threshold. This allows ServiceTrust++ to control and prevent malicious spies and malicious camouflage peers from boosting their global trust scores by manipulating the feedback ratings of good peers and by taking advantage of the uniform trust propagation. Finally, we systematically combine a trust-decaying strategy with a threshold value-based conditional trust propagation to further strengthen the robustness of our global trust computation against sophisticated malicious feedback. Experimental evaluation with both simulation-based networks and real network dataset Epinion show that ServiceTrust++ is highly resilient against all six attack models and highly effective compared to EigenTrust, the most popular and representative trust propagation model to date. © 2015 ACM.",Attack resilience; Distributed service network; Feedback rating quality; Reliability; Trust,Computer networks; Reliability; Attack resiliences; Critical component; Distributed service; Experimental evaluation; Rating quality; Similarity threshold; Trust; Trust computations; Internet
Sampling content from online social networks: Comparing random vs. expert sampling of the Twitter stream,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931270016&doi=10.1145%2f2743023&partnerID=40&md5=04ba217a14b10f5a6091a3a1f69c6932,"Analysis of content streams gathered from social networking sites such as Twitter has several applications ranging from content search and recommendation, news detection to business analytics. However, processing large amounts of data generated on these sites in real-time poses a difficult challenge. To cope with the data deluge, analytics companies and researchers are increasingly resorting to sampling. In this article, we investigate the crucial question of how to sample content streams generated by users in online social networks. The traditional method is to randomly sample all the data. For example, most studies using Twitter data today rely on the 1% and 10% randomly sampled streams of tweets that are provided by Twitter. In this paper, we analyze a different sampling methodology, one where content is gathered only from a relatively small sample (<1%) of the user population, namely, the expert users. Over the duration of a month, we gathered tweets from over 500,000 Twitter users who are identified as experts on a diverse set of topics, and compared the resulting expert sampled tweets with the 1% randomly sampled tweets provided publicly by Twitter. We compared the sampled datasets along several dimensions, including the popularity, topical diversity, trustworthiness, and timeliness of the information contained within them, and on the sentiment/opinion expressed on specific topics. Our analysis reveals several important differences in data obtained through the different sampling methodologies, which have serious implications for applications such as topical search, trustworthy content recommendations, breaking news detection, and opinion mining. © 2015 ACM.",Random sampling; Sampling content streams; Sampling from experts; Twitter; Twitter lists,Data mining; Importance sampling; Websites; Business analytics; Content recommendations; Large amounts of data; On-line social networks; Random sampling; Social networking sites; Twitter; Twitter lists; Social networking (online)
A UI-centric approach for the End-User Development of multidevice mashups,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932646098&doi=10.1145%2f2735632&partnerID=40&md5=3b7ceef7f26876f2726b94ad9f5a8fa9,"In recent years, models, composition paradigms, and tools for mashup development have been proposed to support the integration of information sources, services and APIs available on the Web. The challenge is to provide a gate to a ""programmable Web,"" where end users are allowed to construct easily composite applications that merge content and functions so as to satisfy the long tail of their specific needs. The approaches proposed so far do not fully accommodate this vision. This article, therefore, proposes a mashup development framework that is oriented toward the End-User Development. Given the fundamental role of user interfaces (UIs) as a medium easily understandable by the end users, the proposed approach is characterized by UI-centric models able to support a WYSIWYG (What You See Is What You Get) specification of data integration and service orchestration. It, therefore, contributes to the definition of adequate abstractions that, by hiding the technology and implementation complexity, can be adopted by the end users in a kind of ""democratic"" paradigm for mashup development. This article also shows how model-to-code generative techniques translate models into application schemas, which in turn guide the dynamic instantiation of the composite applications at runtime. This is achieved through lightweight execution environments that can be deployed on the Web and on mobile devices to support the pervasive use of the created applications. © 2015 ACM.",Data fusion; End-User Development; Mashups; Model-driven mashup development; Multidevice mashups; Personal information management; Web interfaces,Data fusion; Data integration; Information management; Mobile devices; Social networking (online); User interfaces; End user development; Mash-up; Mashups; Personal information management; Web interface; Human computer interaction
Integrating transactions into BPEL service compositions: An aspect-based approach,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930935860&doi=10.1145%2f2757288&partnerID=40&md5=6ac01d353956fd7059bba12a5fe7ef3f,"The concept of software as a service has been increasingly adopted to develop distributed applications. Ensuring the reliability of loosely coupled compositions is a challenging task because of the open, dynamic, and independent nature of composable services; this is especially true when the execution of a service-based process relies on independent but correlated services. Transactions are the prototypical case of compositions spanning across multiple services and needing properties to be valid throughout the whole execution. Although transaction protocols and service composition languages have been proposed inthe past decade,atrue viable and effective solution is still missing. In this article, we propose a systematic aspect-based approach to integrating transactions into service compositions, taking into account the well-known protocols: Web Service Transaction and Business Process Execution Language (BPEL). In our approach, transaction policies are first defined as a set of aspects. They are then converted to standard BPEL elements. Finally, these transaction-related elements and the original BPEL process are weaved together, resulting in a transactional executable BPEL process. At runtime, transaction management is the responsibility of a middleware, which implements the coordination framework and transaction protocols followed by the transactional BPEL process and transaction-aware Web services. To automate the proposed approach, we developed a supporting platform called Salan to aid the tasks of defining, validating, and weaving aspect-based transaction policies, and of deploying the transactional BPEL processes. By means of a case study, we demonstrate the proposed approach and evaluate the performance of the supporting platform. Experimental results show that this approach is effective in producing reliable business processes while reducing the need for direct human involvement. © 2015 ACM.",Aspect-oriented programming; Business process execution language; Transaction management; Web services,Application programs; Aspect oriented programming; Computational linguistics; Internet protocols; Middleware; Social networking (online); Software as a service (SaaS); Websites; Business Process Execution Language; Composable services; Coordination frameworks; Distributed applications; Service compositions; Supporting platform; Transaction management; Transaction protocol; Web services
Should we use the sample? Analyzing datasets sampled from Twitter's stream API,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930692789&doi=10.1145%2f2746366&partnerID=40&md5=24248c782151ec9da5424a9b158e2d4c,"Researchers have begun studying content obtained from microblogging services such as Twitter to address a variety of technological, social, and commercial research questions. The large number of Twitter users and even larger volume of tweets often make it impractical to collect and maintain a complete record of activity; therefore, most research and some commercial software applications rely on samples, often relatively small samples, of Twitter data. For the most part, sample sizes have been based on availability and practical considerations. Relatively little attention has been paid to how well these samples represent the underlying stream of Twitter data. To fill this gap, this article performs a comparative analysis on samples obtained from two of Twitter's streaming APIs with a more complete Twitter dataset to gain an in-depth understanding of the nature of Twitter data samples and their potential for use in various data mining tasks. © 2015 ACM.",Data mining; Sample; Twitter aPi,Application programs; Data streams; Sampling; Social networking (online); Commercial software; Comparative analysis; Data mining tasks; In-depth understanding; Micro-blogging services; Research questions; Small samples; Twitter aPi; Data mining
Editorial,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930162402&doi=10.1145%2f2755995&partnerID=40&md5=413ab840381b0410f32948860ceb86d3,[No abstract available],,
"Modeling, enacting, and integrating custom crowdsourcing processes",2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930157557&doi=10.1145%2f2746353&partnerID=40&md5=297cb5b1f0c44a07908fda862cde796f,"Crowdsourcing (CS) is the outsourcing of a unit of work to a crowd of people via an open call for contributions. Thanks to the availability of online CS platforms, such as Amazon Mechanical Turk or CrowdFlower, the practice has experienced a tremendous growth over the past few years and demonstrated its viability in a variety of fields, such as data collection and analysis or human computation. Yet it is also increasingly struggling with the inherent limitations of these platforms: each platform has its own logic of how to crowdsource work (e.g., marketplace or contest), there is only very little support for structured work (work that requires the coordination of multiple tasks), and it is hard to integrate crowdsourced tasks into stateof-the-art business process management (BPM) or information systems. We attack these three shortcomings by (1) developing a flexible CS platform (we call it Crowd Computer, or CC) that allows one to program custom CS logics for individual and structured tasks, (2) devising a BPMN-based modeling language that allows one to program CC intuitively, (3) equipping the language with a dedicated visual editor, and (4) implementing CC on top of standard BPM technology that can easily be integrated into existing software and processes. We demonstrate the effectiveness of the approach with a case study on the crowd-based mining of mashup model patterns. © 2015 ACM 1559-1131/2015/05-ART7 15.00.",BPMN4Crowd; Crowd Computer; Crowdsourcing; Processes; Tactics,Computation theory; Computer simulation languages; Crowdsourcing; Enterprise resource management; Information management; Processing; Visual languages; Amazon mechanical turks; BPMN4Crowd; Business process management; Data collection; Human computation; Inherent limitations; State of the art; Tactics; Modeling languages
A formal account of the open provenance model,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930158710&doi=10.1145%2f2734116&partnerID=40&md5=56923885a08673e9e8a1d99e150b9a1b,"On the Web, where resources such as documents and data are published, shared, transformed, and republished, provenance is a crucial piece of metadata that would allow users to place their trust in the resources they access. The open provenance model (OPM) is a community data model for provenance that is designed to facilitate the meaningful interchange of provenance information between systems. Underpinning OPM is a notion of directed graph, where nodes represent data products and processes involved in past computations and edges represent dependencies between them; it is complemented by graphical inference rules allowing new dependencies to be derived. Until now, however, the OPM model was a purely syntactical endeavor. The present article extends OPM graphs with an explicit distinction between precise and imprecise edges. Then a formal semantics for the thus enriched OPM graphs is proposed, by viewing OPM graphs as temporal theories on the temporal events represented in the graph. The original OPM inference rules are scrutinized in view of the semantics and found to be sound but incomplete. An extended set of graphical rules is provided and proved to be complete for inference. The article concludes with applications of the formal semantics to inferencing in OPM graphs, operators on OPM graphs, and a formal notion of refinement among OPM graphs. © 2015 ACM 1559-1131/2015/05-ART10 15.00.",Provenance; Temporal reasoning; World wide web,Data description; Directed graphs; Electronic document exchange; Formal methods; Graphic methods; Information management; Semantics; World Wide Web; Data products; Formal Semantics; Inference rules; Open Provenance Model (OPM); Provenance; Provenance models; Temporal reasoning; Temporal theory; Graph structures
"The augmented web: Rationales, opportunities, and challenges on browser-side transcoding",2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930170972&doi=10.1145%2f2735633&partnerID=40&md5=7ca1bac35720cc8f813efdb4c7bcea93,"Today's web personalization technologies use approaches like user categorization, configuration, and customization but do not fully support individualized requirements. As a significant portion of our social and working interactions are migrating to the web, we can expect an increase in these kinds of minority requirements. Browser-side transcoding holds the promise of facilitating this aim by opening personalization to third parties through web augmentation (WA), realized in terms of extensions and userscripts. WA is to the web what augmented reality is to the physical world: to layer relevant content/layout/navigation over the existing web to improve the user experience. From this perspective, WA is not as powerful as web personalization since its scope is limited to the surface of the web. However, it permits this surface to be tuned by developers other than the sites' webmasters. This opens up the web to third parties who might come up with imaginative ways of adapting the web surface for their own purposes. Its success is backed up by millions of downloads. This work looks at this phenomenon, delving into the ""what,"" the ""why,"" and the ""what for"" of WA, and surveys the challenges ahead for WA to thrive. To this end, we appraise the most downloaded 45 WA extensions for Mozilla Firefox and Google Chrome as well as conduct a systematic literature review to identify what quality issues received the most attention in the literature. The aim is to raise awareness about WA as a key enabler of the personal web and point out research directions. © 2015 ACM 1559-1131/2015/05-ART8 15.00.",Adaptation; Javascript; Personalization; Transcoding,Augmented reality; Social networking (online); World Wide Web; Adaptation; Javascript; Mozilla firefox; Personalizations; Systematic literature review; Transcoding; Web augmentation; Web personalization; Web browsers
Characterizing web censorship worldwide: Another look at the opennet initiative data,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921919197&doi=10.1145%2f2700339&partnerID=40&md5=ef7deeca7b0711712242b8ea548e7703,"In this study, we take another look at 5 years of web censorship data gathered by the OpenNet Initiative in 77 countries using user-based testing with locally relevant content. Prior to our work, this data had been analyzed with little automation, focusing on what content had been blocked, rather than how blocking was carried out. In this study, we use more rigorous automation to obtain a longitudinal, global view of the technical means used for web censorship. We also identify blocking that had been missed in prior analyses. Our results point to considerable variability in the technologies used for web censorship, across countries, time, and types of content, and even across ISPs in the same country. In addition to characterizing web censorship in countries that, thus far, have eluded technical analysis, we also discuss the implications of our observations on the design of future network measurement platforms and circumvention technologies.",,Social networking (online); Future networks; Global view; Technical analysis; Web Design
Elastic personalized nonfunctional attribute preference and trade-off based service selection,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921925345&doi=10.1145%2f2697389&partnerID=40&md5=33ff7e7c0f999ce00b51427798a7762c,"For service users to get the best service that meet their requirements, they prefer to personalize their nonfunctional attributes, such as reliability and price. However, the personalization makes it challenging for service providers to completely meet users' preferences, because they have to deal with conflicting nonfunctional attributes when selecting services for users. With this in mind, users may sometimes want to explicitly specify their trade-offs among nonfunctional attributes to make their preferences known to service providers. In this article, we present a novel service selection method based on fuzzy logic that considers users' personalized preferences and their trade-offs on nonfunctional attributes during service selection. The method allows users to represent their elastic nonfunctional requirements and associated importance using linguistic terms to specify their personalized trade-off strategies. We present examples showing how the service selection framework is used and a prototype with real-world airline services to evaluate the proposed framework's application. -c 2015 ACM 1559-1131/2015/01-ART1 15.00.",,Air transportation; Fuzzy logic; Linguistics; Airline services; Linguistic terms; Non-functional requirements; Personalizations; Real-world; Selecting services; Service provider; Service selection; Economic and social effects
ReputationPro: The efficient approaches to contextual transaction trust computation in e-commerce environments,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921914402&doi=10.1145%2f2697390&partnerID=40&md5=77f7eab3951bfab4d038a7b005ba5694,"In e-commerce environments, the trustworthiness of a seller is utterly important to potential buyers, especially when a seller is not known to them. Most existing trust evaluation models compute a single value to reflect the general trustworthiness of a seller without taking any transaction context information into account. With such a result as the indication of reputation, a buyer may be easily deceived by a malicious seller in a transaction where the notorious value imbalance problem is involved-in other words, a malicious seller accumulates a high-level reputation by selling cheap products and then deceives buyers by inducing them to purchase more expensive products. In this article, we first present a trust vector consisting of three values for contextual transaction trust (CTT). In the computation of CTT values, three identified important context dimensions, including Product Category, Transaction Amount, and Transaction Time, are taken into account. In the meantime, the computation of each CTT value is based on both past transactions and the forthcoming transaction. In particular, with different parameters specified by a buyer regarding context dimensions, different sets of CTT values can be calculated. As a result, all of these trust values can outline the reputation profile of a seller that indicates the dynamic trustworthiness of a seller in different products, product categories, price ranges, time periods, and any necessary combination of them. We name this new model ReputationPro. Nevertheless, in ReputationPro, the computation of reputation profile requires new data structures for appropriately indexing the precomputation of aggregates over large-scale ratings and transaction data in three context dimensions, as well as novel algorithms for promptly answering buyers' CTT queries. In addition, storing precomputed aggregation results consumes a large volume of space, particularly for a system with millions of sellers. Therefore, reducing storage space for aggregation results is also a great demand. To solve these challenging problems, we first propose a new index scheme CMK-tree by extending the two-dimensional K-D-B-tree that indexes spatial data to support efficient computation of CTT values. Then, we further extend the CMK-tree and propose a CMK-treeRS approach to reducing the storage space allocated to each seller. The two approaches are not only applicable to three context dimensions that are either linear or hierarchical but also take into account the characteristics of the transaction-time model-that is, transaction data is inserted in chronological order. Moreover, the proposed data structures can index each specific product traded in a time period to compute the trustworthiness of a seller in selling a product. Finally, the experimental results illustrate that the CMK-tree is superior in efficiency of computing CTT values to all three existing approaches in the literature. In particular, while answering a buyer's CTT queries for each brand-based product category, the CMK-tree has almost linear query performance. In addition, with significantly reduced storage space, the CMK-treeRS approach can further improve the efficiency in computing CTT values. Therefore, our proposed ReputationPro model is scalable to large-scale e-commerce Web sites in terms of efficiency and storage space consumption.",,Algorithms; Commerce; Data structures; Digital storage; Efficiency; Electronic commerce; Sales; Chronological order; Chronological order; Context information; Context information; Contextual transaction trusts; Efficient computation; Expensive products; Pre-computed aggregation; Product categories; Storage space consumption; Trees (mathematics)
Nautilod: A Formal language for the web of data graph,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921920911&doi=10.1145%2f2697393&partnerID=40&md5=97471116ad5825bdf8566b0f4d0be7be,"The Web of Linked Data is a huge graph of distributed and interlinked datasources fueled by structured information. This new environment calls for formal languages and tools to automatize navigation across datasources (nodes in such graph) and enable semantic-Aware and Web-scale search mechanisms. In this article we introduce a declarative navigational language for theWeb of Linked Data graph called NAUTILOD. NAUTILOD enables one to specify datasources via the intertwining of navigation and querying capabilities. It also features a mechanism to specify actions (e.g., send notification messages) that obtain their parameters from datasources reached during the navigation. We provide a formalization of the NAUTILOD semantics, which captures both nodes and fragments of the Web of Linked Data. We present algorithms to implement such semantics and study their computational complexity. We discuss an implementation of the features of NAUTILOD in a tool called swget, which exploits current Web technologies and protocols. We report on the evaluation of swget and its comparison with related work. Finally, we show the usefulness of capturing Web fragments by providing examples in different knowledge domains.",,Codes (symbols); Computational linguistics; Data handling; Formal languages; Internet protocols; Navigation; Semantics; Social networking (online); World Wide Web; Knowledge domains; Related works; Search mechanism; Semantic-aware; Structured information; Web fragment; Web of datum; Web technologies; Semantic Web
Active learning for web search ranking via noise injection,2015,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921913723&doi=10.1145%2f2697391&partnerID=40&md5=a02012ea66598e2685dcccd7b1aa4e90,"Learning to rank has become increasingly important for many information retrieval applications. To reduce the labeling cost at training data preparation, many active sampling algorithms have been proposed. In this article, we propose a novel active learning-for-ranking strategy called ranking-based sensitivity sampling (RSS), which is tailored for Gradient Boosting Decision Tree (GBDT), a machine-learned ranking method widely used in practice by major commercial search engines for ranking. We leverage the property of GBDT that samples close to the decision boundary tend to be sensitive to perturbations and design the active learning strategy accordingly. We further theoretically analyze the proposed strategy by exploring the connection between the sensitivity used for sample selection and model regularization to provide a potentially theoretical guarantee w.r.t. the generalization capability. Considering that the performance metrics of ranking overweight the top-ranked items, item rank is incorporated into the selection function. In addition, we generalize the proposed technique to several other base learners to show its potential applicability in a wide variety of applications. Substantial experimental results on both the benchmark dataset and a real-world dataset have demonstrated that our proposed active learning strategy is highly effective in selecting the most informative examples.",,Artificial intelligence; Decision trees; Learning systems; Search engines; Active learning strategies; Benchmark datasets; Generalization capability; Performance metrics; Retrieval applications; Selection function; Theoretical guarantees; Web search rankings; World Wide Web
Constructing and comparing user mobility profiles,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910107455&doi=10.1145%2f2637483&partnerID=40&md5=d73a352225e633833cb8f4c31a64a7a4,"Nowadays, the accumulation of people's whereabouts due to location-based applications has made it possible to construct their mobility profiles. This access to users' mobility profiles subsequently brings benefits back to location-based applications. For instance, in on-line social networks, friends can be recommended not only based on the similarity between their registered information, for instance, hobbies and professions but also referring to the similarity between their mobility profiles. In this article, we propose a new approach to construct and compare users' mobility profiles. First, we improve and apply frequent sequential pattern mining technologies to extract the sequences of places that a user frequently visits and use them to model his mobility profile. Second, we present a new method to calculate the similarity between two users using their mobility profiles. More specifically, we identify the weaknesses of a similarity metric in the literature, and propose a new one which not only fixes the weaknesses but also provides more precise and effective similarity estimation. Third, we consider the semantics of spatiotemporal information contained in user mobility profiles and add them into the calculation of user similarity. It enables us to measure users' similarity from different perspectives. Two specific types of semantics are explored in this article: location semantics and temporal semantics. Last, we validate our approach by applying it to two real-life datasets collected by Microsoft Research Asia and Yonsei University, respectively. The results show that our approach outperforms the existing works from several aspects. © 2014 ACM.",Mobility profiles; Pattern mining; Recommendation systems; Similarity measurement; Spatio-temporal trajectories,Data mining; Location; Mobile telecommunication systems; Recommender systems; Frequent sequential patterns; Location-based applications; Mobility profiles; On-line social networks; Pattern mining; Similarity measurements; Spatio-temporal trajectories; Spatiotemporal information; Semantics
EXIP: A framework for embedded Web development,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910151277&doi=10.1145%2f2665068&partnerID=40&md5=c412b4787c0278b0803ce360d1d24b5c,"Developing and deploying Web applications on networked embedded devices is often seen as a way to reduce the development cost and time to market for new target platforms. However, the size of the messages and the processing requirements of today's Web protocols, such as HTTP and XML, are challenging for the most resource-constrained class of devices that could also benefit from Web connectivity. New Web protocols using binary representations have been proposed for addressing this issue. Constrained Application Protocol (CoAP) reduces the bandwidth and processing requirements compared to HTTP while preserving the core concepts of the Web architecture. Similarly, Efficient XML Interchange (EXI) format has been standardized for reducing the size and processing time for XML structured information. Nevertheless, the adoption of these technologies is lagging behind due to lack of support from Web browsers and current Web development toolkits. Motivated by these problems, this article presents the design and implementation techniques for the EXIP framework for embedded Web development. The framework consists of a highly efficient EXI processor, a tool for EXI data binding based on templates, and a CoAP/EXI/XHTML Web page engine. A prototype implementation of the EXI processor is herein presented and evaluated. It can be applied to Web browsers or thin server platforms using XHTML and Web services for supporting human-machine interactions in the Internet of Things. This article contains four major results: (1) theoretical and practical evaluation of the use of binary protocols for embedded Web programming; (2) a novel method for generation of EXI grammars based on XML Schema definitions; (3) an algorithm for grammar concatenation that produces normalized EXI grammars directly, and hence reduces the number of iterations during grammar generation; (4) an algorithm for efficient representation of possible deviations from the XML schema.",CoAP; Data formats; Data processing; Embedded systems; EXI; Information exchange; Internet of Things; Web of things; XHTML; XML,Bins; Data handling; Data processing; Embedded systems; HTTP; Hypertext systems; Internet of things; Internet protocols; Network architecture; Web crawler; Web services; Websites; XML; CoAP; Constrained Application Protocol (CoAP); Data formats; Design and implementations; Information exchanges; Networked embedded devices; Prototype implementations; XHTML; Web browsers
Sentiment-focused Web crawling,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910131543&doi=10.1145%2f2644821&partnerID=40&md5=0b1f463e469f1997168e39942f446a7c,"Sentiments and opinions expressed in Web pages towards objects, entities, and products constitute an important portion of the textual content available in the Web. In the last decade, the analysis of such content has gained importance due to its high potential for monetization. Despite the vast interest in sentiment analysis, somewhat surprisingly, the discovery of sentimental or opinionated Web content is mostly ignored. This work aims to fill this gap and addresses the problem of quickly discovering and fetching the sentimental content present in the Web. To this end, we design a sentiment-focused Web crawling framework. In particular, we propose different sentiment-focused Web crawling strategies that prioritize discovered URLs based on their predicted sentiment scores. Through simulations, these strategies are shown to achieve considerable performance improvement over general-purpose Web crawling strategies in discovery of sentimental Web content.",Focused Web crawling; Sentiment analysis,Sentiment analysis; Websites; Focused web crawling; High potential; Sentiment scores; Textual content; Web content; Web Crawling; Web crawler
Content bias in online health search,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910123085&doi=10.1145%2f2663355&partnerID=40&md5=eaa9fa54080b8de223b56fa9b6662cbf,"Search engines help people answer consequential questions. Biases in retrieved and indexed content (e.g., skew toward erroneous outcomes that represent deviations from reality), coupled with searchers' biases in how they examine and interpret search results, can lead people to incorrect answers. In this article, we seek to better understand biases in search and retrieval, and in particular those affecting the accuracy of content in search results, including the search engine index, features used for ranking, and the formulation of search queries. Focusing on the important domain of online health search, this research broadens previous work on biases in search to examine the role of search systems in contributing to biases. To assess bias, we focus on questions about medical interventions and employ reliable ground truth data from authoritative medical sources. In the course of our study, we utilize large-scale log analysis using data from a popular Web search engine, deep probes of result lists on that search engine, and crowdsourced human judgments of search result captions and landing pages. Our findings reveal bias in results, amplifying searchers' existing biases that appear evident in their search activity. We also highlight significant bias in indexed content and show that specific ranking signals and specific query terms support bias. Both of these can degrade result accuracy and increase skewness in search results. Our analysis has implications for bias mitigation strategies in online search systems, and we offer recommendations for search providers based on our findings. © 2014 ACM 1559-1131/2014/10-ART25 $15.00.",Content biases; Health search,Health; Online systems; Safety devices; Content biases; Ground truth data; Human judgments; Medical intervention; Mitigation strategy; Search activity; Search and retrieval; Search engine indices; Search engines
Merging query results from local search engines for georeferenced objects,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910140909&doi=10.1145%2f2656344&partnerID=40&md5=6cd6e03b146cf45af8c3b4ca565c6c79,"The emergence of numerous online sources about local services presents a need for more automatic yet accurate data integration techniques. Local services are georeferenced objects and can be queried by their locations on a map, for instance, neighborhoods. Typical local service queries (e.g., ""French Restaurant in The Loop"") include not only information about ""what"" (""French Restaurant"") a user is searching for (such as cuisine) but also ""where"" information, such as neighborhood (""The Loop""). In this article, we address three key problems: query translation, result merging and ranking. Most local search engines provide a (hierarchical) organization of (large) cities into neighborhoods. A neighborhood in one local search engine may correspond to sets of neighborhoods in other local search engines. These make the query translation challenging. To provide an integrated access to the query results returned by the local search engines, we need to combine the results into a single list of results. Our contributions include: (1) An integration algorithm for neighborhoods. (2) A very effective business listing resolution algorithm. (3) A ranking algorithm that takes into consideration the user criteria, user ratings and rankings. We have created a prototype system, Yumi, over local search engines in the restaurant domain. The restaurant domain is a representative case study for the local services. We conducted a comprehensive experimental study to evaluate Yumi. A prototype version of Yumi is available online. © 2014 ACM.",Geospatial query processing; Ranking; Web database integration,Merging; Query processing; Search engines; Geo-spatial; Integration algorithm; Integration techniques; Local search engines; Ranking; Representative case; Resolution algorithms; Web database; Data integration
Using interaction data to explain difficulty navigating online,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910134742&doi=10.1145%2f2656343&partnerID=40&md5=7e392e7393e463f628dc9348bd8a3e1a,"A user's behaviour when browsing a Web site contains clues to that user's experience. It is possible to record some of these behaviours automatically, and extract signals that indicate a user is having trouble finding information. This allows for Web site analytics based on user experiences, not just page impressions. A series of experiments identified user browsing behaviours-such as time taken and amount of scrolling up a page-which predict navigation difficulty and which can be recorded with minimal or no changes to existing sites or browsers. In turn, patterns of page views correlate with these signals and these patterns can help Web authors understand where and why their sites are hard to navigate. A new software tool, ""LATTE,"" automates this analysis and makes it available to Web authors in the context of the site itself.",Browsing; Logfile analysis; Navigation; Web analytics,Navigation; Signal processing; Web crawler; Websites; Browsing; Browsing behaviour; Log-file analysis; User experience; Web analytics; Behavioral research
A model-based approach for crawling Rich Internet Applications,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904101973&doi=10.1145%2f2626371&partnerID=40&md5=e67f33d80b013d34aa1f009166f11247,"New Web technologies, like AJAX, result in more responsive and interactive Web applications, sometimes called Rich Internet Applications (RIAs). Crawling techniques developed for traditionalWeb applications are not sufficient for crawling RIAs. The inability to crawl RIAs is a problem that needs to be addressed for at least making RIAs searchable and testable. We present a new methodology, called ""model-based crawling"", that can be used as a basis to design efficient crawling strategies for RIAs. We illustrate model-based crawling with a sample strategy, called the ""hypercube strategy"". The performances of our model-based crawling strategies are compared against existing standard crawling strategies, including breadth-first, depth-first, and a greedy strategy. Experimental results show that our model-based crawling approach is significantly more efficient than these standard strategies. © 2014 ACM.",AJAX; Crawling; DOM; Dynamic analysis; Modeling; Rich Internet Applications,Computer networks; Dynamic analysis; Internet; Models; AJAX; Crawling; Crawling strategy; DOM; Greedy strategies; Interactive web applications; Model based approach; Rich Internet Applications; Web services
Propagating both trust and distrust with target differentiation for combating link-based Web spam,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904107872&doi=10.1145%2f2628440&partnerID=40&md5=0599144fe5e5626d3b5269ecbfae9e11,"Semi-automatic anti-spam algorithms propagate either trust through links from a good seed set (e.g., TrustRank) or distrust through inverse links from a bad seed set (e.g., Anti-TrustRank) to the entire Web. These kinds of algorithms have shown their powers in combating link-based Web spam since they integrate both human judgement and machine intelligence. Nevertheless, there is still much space for improvement. One issue of most existing trust/distust propagation algorithms is that only trust or distrust is propagated and only a good seed set or a bad seed set is used. According to Wu et al. [2006a], a combined usage of both trust and distrust propagation can lead to better results, and an effective framework is needed to realize this insight. Another more serious issue of existing algorithms is that trust or distrust is propagated in nondifferential ways, that is, a page propagates its trust or distrust score uniformly to its neighbors, without considering whether each neighbor should be trusted or distrusted. Such kinds of blind propagating schemes are inconsistent with the original intention of trust/distrust propagation. However, it seems impossible to implement differential propagation if only trust or distrust is propagated. In this article, we take the view that each Web page has both a trustworthy side and an untrustworthy side, and we thusly assign two scores to eachWeb page: T-Rank, scoring the trustworthiness of the page, and D-Rank, scoring the untrustworthiness of the page.We then propose an integrated framework that propagates both trust and distrust. In the framework, the propagation of T-Rank/D-Rank is penalized by the target's current D-Rank/T-Rank. In other words, the propagation of T-Rank/D-Rank is decided by the target's current (generalized) probability of being trustworthy/ untrustworthy; thus a page propagates more trust/distrust to a trustworthy/untrustworthy neighbor than to an untrustworthy/trustworthy neighbor. In this way, propagating both trust and distrust with target differentiation is implemented. We use T-Rank scores to realize spam demotion and D-Rank scores to accomplish spam detection. The proposed Trust-DistrustRank (TDR) algorithm regresses to TrustRank and Anti-TrustRank when the penalty factor is set to 1 and 0, respectively. Thus TDR could be seen as a combinatorial generalization of both TrustRank and Anti-TrustRank. TDR not only makes full use of both trust and distrust propagation, but also overcomes the disadvantages of both TrustRank and Anti-TrustRank. Experimental results on benchmark datasets show that TDR outperforms other semi-automatic anti-spam algorithms for both spam demotion and spam detection tasks under various criteria. © 2014 ACM.",Distrust propagation; Target differentiation; Trust propagation; Web spam,Artificial intelligence; Internet; Supervisory personnel; Benchmark datasets; Distrust propagation; Integrated frameworks; Machine intelligence; Propagation algorithm; Semi-automatics; Trust propagation; Web spam; Algorithms
"Conceptual development of custom, domain-specific mashup platforms",2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904124014&doi=10.1145%2f2628439&partnerID=40&md5=2e4f793de37f171fc5a97ab622b6911b,"Despite the common claim by mashup platforms that they enable end-users to develop their own software, in practice end-users still don't develop their own mashups, as the highly technical or inexistent user bases of today's mashup platforms testify. The key shortcoming of current platforms is their general-purpose nature, that privileges expressive power over intuitiveness. In our prior work, we have demonstrated that a domainspecific mashup approach, which privileges intuitiveness over expressive power, has much more potential to enable end-user development (EUD). The problem is that developing mashup platforms - domain-specific or not - is complex and time consuming. In addition, domain-specific mashup platforms by their very nature target only a small user basis, that is, the experts of the target domain, which makes their development not sustainable if it is not adequately supported and automated. With this article, we aim to make the development of custom, domain-specific mashup platforms costeffective. We describe a mashup tool development kit (MDK) that is able to automatically generate a mashup platform (comprising custom mashup and component description languages and design-time and runtime environments) from a conceptual design and to provision it as a service. We equip the kit with a dedicated development methodology and demonstrate the applicability and viability of the approach with the help of two case studies. © 2014 ACM.",Conceptual development; Domain-specific mashups; Mashup platforms as a service; Mashup tools/platforms; Mashups; Metadesign,Computer networks; Internet; Conceptual development; Mashup platforms; Mashup tools; Mashups; Meta-design; Conceptual design
Foundations of trust and distrust in networks: Extended structural balance theory,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904117391&doi=10.1145%2f2628438&partnerID=40&md5=281f1a3562e318b4ed44730a41934939,"Modeling trust in very large social networks is a hard problem due to the highly noisy nature of these networks that span trust relationships from many different contexts, based on judgments of reliability, dependability, and competence. Furthermore, relationships in these networks vary in their level of strength. In this article, we introduce a novel extension of structural balance theory as a foundational theory of trust and distrust in networks. Our theory preserves the distinctions between trust and distrust as suggested in the literature, but also incorporates the notion of relationship strength that can be expressed as either discrete categorical values, as pairwise comparisons, or as metric distances. Our model is novel, has sound social and psychological basis, and captures the classical balance theory as a special case. We then propose a convergence model, describing how an imbalanced network evolves towards new balance, and formulate the convergence problem of a social network as a Metric Multidimensional Scaling (MDS) optimization problem. Finally, we show how the convergence model can be used to predict edge signs in social networks and justify our theory through extensive experiments on real datasets. © 2014 ACM.",Distrust; Social networks; Structural balance; Trust,Optimization; Social networking (online); Convergence problems; Distrust; Metric multidimensional scaling; Optimization problems; Pair-wise comparison; Structural balance; Trust; Trust relationship; Algorithms
Analyzing and mining comments and comment ratings on the social web,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904122751&doi=10.1145%2f2628441&partnerID=40&md5=72fd73e004baf98df6f9b580575e9c93,"An analysis of the social video sharing platform YouTube and the news aggregator Yahoo! News reveals the presence of vast amounts of community feedback through comments for published videos and news stories, as well as through metaratings for these comments. This article presents an in-depth study of commenting and comment rating behavior on a sample of more than 10 million user comments on YouTube and Yahoo! News. In this study, comment ratings are considered first-class citizens. Their dependencies with textual content, thread structure of comments, and associated content (e.g., videos and their metadata) are analyzed to obtain a comprehensive understanding of the community commenting behavior. Furthermore, this article explores the applicability of machine learning and data mining to detect acceptance of comments by the community, comments likely to trigger discussions, controversial and polarizing content, and users exhibiting offensive commenting behavior. Results from this study have potential application in guiding the design of community-oriented online discussion platforms. 2014 Copyright held by the Owner/Author.",Comment ratings; Community feedback; Yahoo! News; YouTube,Artificial intelligence; In-depth study; News aggregators; Online discussions; Social videos; Social webs; Textual content; Yahoo! News; YouTube; Social networking (online)
"Ten years of Rich Internet applications: A systematic mapping study, and beyond",2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904132619&doi=10.1145%2f2626369&partnerID=40&md5=12fc486fa51bc234f8470db6f69fc85c,"BACKGROUND. The term Rich Internet Applications (RIAs) is generally associated with Web applications that provide the features and functionality of traditional desktop applications. Ten years after the introduction of the term, an ample amount of research has been carried out to study various aspects of RIAs. It has thus become essential to summarize this research and provide an adequate overview. OBJECTIVE. The objective of our study is to assemble, classify, and analyze all RIA research performed in the scientific community, thus providing a consolidated overview thereof, and to identify well-established topics, trends, and open research issues. Additionally, we provide a qualitative discussion of the most interesting findings. This work therefore serves as a reference work for beginning and established RIA researchers alike, as well as for industrial actors that need an introduction in the field, or seek pointers to (a specific subset of) the state-of-the-art. METHOD. A systematic mapping study is performed in order to identify all RIA-related publications, define a classification scheme, and categorize, analyze, and discuss the identified research according to it. RESULTS. Our source identification phase resulted in 133 relevant, peer-reviewed publications, published between 2002 and 2011 in awide variety of venues. They were subsequently classified according to four facets: development activity, research topic, contribution type, and research type. Pie, stacked bar, and bubble charts were used to depict and analyze the results. A deeper analysis is provided for the most interesting and/or remarkable results. CONCLUSION. Analysis of the results shows that, although the RIA term was coined in 2002, the first RIA-related research appeared in 2004. From 2007 there was a significant increase in research activity, peaking in 2009 and decreasing to pre-2009 levels afterwards. All development phases are covered in the identified research, with emphasis on ""design"" (33%) and ""implementation"" (29%). The majority of research proposes a ""method"" (44%), followed by ""model"" (22%), ""methodology"" (18%), and ""tools"" (16%); no publications in the category ""metrics"" were found. The preponderant research topic is ""models, methods and methodologies"" (23%) and, to a lesser extent, ""usability and accessibility"" and ""user interface"" (11% each). On the other hand, the topic ""localization, internationalization and multilinguality"" received no attention at all, and topics such as ""deep Web"" (under 1%), ""business processing"", ""usage analysis"", ""data management"", ""quality and metrics"" (all under 2%), ""semantics"", and ""performance"" (slightly above 2%) received very little attention. Finally, there is a large majority of ""solution proposals"" (66%), few ""evaluation research"" (14%), and even fewer ""validation"" (6%), although the latter have been increasing in recent years. © 2014 ACM.",Rich Internet applications; Systematic mapping study,Information management; Mapping; Publishing; Quality control; Semantics; User interfaces; World Wide Web; Business processing; Classification scheme; Desktop applications; Development activity; Rich Internet Applications; Scientific community; Source identification; Systematic mapping studies; Research
Incremental text indexing for fast disk-based search,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904126861&doi=10.1145%2f2560800&partnerID=40&md5=7e63b028c1c53a6762b0c9f2ee5d3d83,"Real-time search requires to incrementally ingest content updates and almost immediately make them searchable while serving search queries at low latency. This is currently feasible for datasets of moderate size by fully maintaining the index in the main memory of multiple machines. Instead, disk-based methods for incremental index maintenance substantially increase search latency with the index fragmented across multiple disk locations. For the support of fast search over disk-based storage, we take a fresh look at incremental text indexing in the context of current architectural features. We introduce a greedy method called Selective Range Flush (SRF) to contiguously organize the index over disk blocks and dynamically update it at low cost. We show that SRF requires substantial experimental effort to tune specific parameters for performance efficiency. Subsequently, we propose the Unified Range Flush (URF) method, which is conceptually simpler than SRF, achieves similar or better performance with fewer parameters and less tuning, and is amenable to I/O complexity analysis.We implement interesting variations of the two methods in the Proteus prototype search engine that we developed and do extensive experiments with three different Web datasets of size up to 1TB. Across different systems, we show that our methods offer search latency that matches or reduces up to half the lowest achieved by existing disk-based methods. In comparison to an existing method of comparable search latency on the same system, our methods reduce by a factor of 2.0-2.4 the I/O part of build time and by 21-24% the total build time. © 2014 ACM.",Inverted files; Online maintenance; Performance evaluation; Prototype implementation; Search engines,Indexing (of information); Search engines; Architectural features; Better performance; Inverted files; Online maintenance; Performance efficiency; Performance evaluation; Prototype implementations; Real-time searches; Disks (machine components)
How to improve your search engine ranking: Myths and reality,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897564510&doi=10.1145%2f2579990&partnerID=40&md5=7f8f9db58532d0ca642e3b2d0c058e5e,"Search engines have greatly influenced the way people access information on the Internet, as such engines provide the preferred entry point to billions of pages on the Web. Therefore, highly ranked Web pages generally have higher visibility to people and pushing the ranking higher has become the top priority for Web masters. As a matter of fact, Search Engine Optimization (SEO) has became a sizeable business that attempts to improve their clients' ranking. Still, the lack of ways to validate SEO's methods has created numerous myths and fallacies associated with ranking algorithms. In this article, we focus on two ranking algorithms, Google's and Bing's, and design, implement, and evaluate a ranking system to systematically validate assumptions others have made about these popular ranking algorithms. We demonstrate that linear learning models, coupled with a recursive partitioning ranking scheme, are capable of predicting ranking results with high accuracy. As an example, we manage to correctly predict 7 out of the top 10 pages for 78% of evaluated keywords. Moreover, for content-only ranking, our system can correctly predict 9 or more pages out of the top 10 ones for 77% of search terms. We show how our ranking system can be used to reveal the relative importance of ranking features in a search engine's ranking function, provide guidelines for SEOs and Web masters to optimize their Web pages, validate or disprove new ranking features, and evaluate search engine ranking results for possible ranking bias. © 2014 ACM.",Learning; Ranking algorithm; Search engine; Search engine optimization,Algorithms; Forecasting; Regression analysis; Websites; Learning; Learning models; Ranking algorithm; Ranking functions; Ranking system; Recursive Partitioning; Search engine optimizations; Search terms; Search engines
Neighbor selection and weighting in user-based collaborative filtering: A performance prediction approach,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897524239&doi=10.1145%2f2579993&partnerID=40&md5=763fa1d8ef26a1f090b8eef7cda14ee6,"User-based collaborative filtering systems suggest interesting items to a user relying on similar-minded people called neighbors. The selection and weighting of these neighbors characterize the different recommendation approaches. While standard strategies perform a neighbor selection based on user similarities, trust-aware recommendation algorithms rely on other aspects indicative of user trust and reliability. In this article we restate the trust-aware recommendation problem, generalizing it in terms of performance prediction techniques, whose goal is to predict the performance of an information retrieval system in response to a particular query. We investigate how to adopt the preceding generalization to define a unified framework where we conduct an objective analysis of the effectiveness (predictive power) of neighbor scoring functions. The proposed framework enables discriminating whether recommendation performance improvements are caused by the used neighbor scoring functions or by the ways these functions are used in the recommendation computation. We evaluated our approach with several state-of-the-art and novel neighbor scoring functions on three publicly available datasets. By empirically comparing four neighbor quality metrics and thirteen performance predictors, we found strong predictive power for some of the predictors with respect to certain metrics. This result was then validated by checking the final performance of recommendation strategies where predictors are used for selecting and/or weighting user neighbors. As a result, we have found that, by measuring the predictive power of neighbor performance predictors, we are able to anticipate which predictors are going to perform better in neighbor-scoring-powered versions of a user-based collaborative filtering algorithm. © 2014 ACM.",Neighbor selection; Neighbor weighting; Performance prediction; Recommender systems; Trust; User-based collaborative filtering,Forecasting; Recommender systems; Collaborative filtering algorithms; Collaborative filtering systems; Neighbor selection; Neighbor weighting; Performance prediction; Recommendation algorithms; Recommendation performance; Trust; Collaborative filtering
Textual and content-based search in repositories of Web application models,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897519832&doi=10.1145%2f2579991&partnerID=40&md5=e43e6580c5d5798d98b71306c43ad8a2,"Model-driven engineering relies on collections of models, which are the primary artifacts for software development. To enable knowledge sharing and reuse, models need to be managed within repositories, where they can be retrieved upon users' queries. This article examines two different techniques for indexing and searching model repositories, with a focus on Web development projects encoded in a domain-specific language. Keyword-based and content-based search (also known as query-by-example) are contrasted with respect to the architecture of the system, the processing of models and queries, and the way in which metamodel knowledge can be exploited to improve search. A thorough experimental evaluation is conducted to examine what parameter configurations lead to better accuracy and to offer an insight in what queries are addressed best by each system. © 2014 ACM.",Domain-specific language; Information retrieval; Search; Web application,Applications; Information retrieval; Problem oriented languages; World Wide Web; Content-based search; Domain specific languages; Experimental evaluation; Knowledge-sharing; Model repositories; Model-driven Engineering; Search; WEB application; Software design
Analysis of search and browsing behavior of young users on the Web,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897550174&doi=10.1145%2f2555595&partnerID=40&md5=f7929c8e969b5c5e7854517ae1614912,"The Internet is increasingly used by young children for all kinds of purposes. Nonetheless, there are not many resources especially designed for children on the Internet and most of the content online is designed for grown-up users. This situation is problematic if we consider the large differences between young users and adults since their topic interests, computer skills, and language capabilities evolve rapidly during childhood. There is little research aimed at exploring and measuring the difficulties that children encounter on the Internet when searching for information and browsing for content. In the first part of this work, we employed query logs from a commercial search engine to quantify the difficulties children of different ages encounter on the Internet and to characterize the topics that they search for. We employed query metrics (e.g., the fraction of queries posed in natural language), session metrics (e.g., the fraction of abandoned sessions), and click activity (e.g., the fraction of ad clicks). The search logs were also used to retrace stages of child development. Concretely, we looked for changes in interests (e.g., the distribution of topics searched) and language development (e.g., the readability of the content accessed and the vocabulary size). In the second part of this work, we employed toolbar logs from a commercial search engine to characterize the browsing behavior of young users, particularly to understand the activities on the Internet that trigger search. We quantified the proportion of browsing and search activity in the toolbar sessions and we estimated the likelihood of a user to carry out search on the Web vertical and multimedia verticals (i.e., videos and images) given that the previous event is another search event or a browsing event. We observed that these metrics clearly demonstrate an increased level of confusion and unsuccessful search sessions among children. We also found a clear relation between the reading level of the clicked pages and characteristics of the users such as age and educational attainment. In terms of browsing behavior, children were found to start their activities on the Internet with a search engine (instead of directly browsing content) more often than adults. We also observed a significantly larger amount of browsing activity for the case of teenager users. Interestingly we also found that if children visit knowledge-relatedWeb sites (i.e., information-dense pages such as Wikipedia articles), they subsequently do moreWeb searches than adults. Additionally, children and especially teenagers were found to have a greater tendency to engage in multimedia search, which calls to improve the aggregation of multimedia results into the current search result pages. © 2014 ACM.",Adults; Browsing behavior; Children; Query logs; Search behavior; Session analysis; Toolbar logs; Topic classification; Web search; Yahoo! Answers; Yahoo! Search; Young adults,Online searching; Search engines; Websites; Adults; Browsing behavior; Children; Query logs; Search behavior; Session analysis; Toolbars; Topic Classification; Web searches; Yahoo! Answers; Yahoo! Search; Young adults; Internet
Efficient multiview maintenance under insertion in huge social networks,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897555033&doi=10.1145%2f2541290&partnerID=40&md5=48a295430e555295ff66b20728bd735e,"Applications to monitor various aspects of social networks are becoming increasingly popular. For instance, marketers want to look for semantic patterns relating to the content of tweets and Facebook posts relating to their products. Law enforcement agencies want to track behaviors involving potential criminals on the Internet by looking for certain patterns of behavior. Music companies want to track patterns of spread of illegal music. These applications allow multiple users to specify patterns of interest and monitor them in real time as new data gets added to the Web or to a social network. In this article we develop the concept of social network view servers in which all of these types of applications can be simultaneously monitored. The patterns of interest are expressed as views over an underlying graph or social network database. We show that a given set of views can be compiled in multiple possible ways to take advantage of common substructures and define the concept of an optimal merge. Though finding an optimal merge is shown to be NP-hard, we develop the AddView to find very good merges quickly. We develop a very fast MultiView algorithm that scalably and efficiently maintains multiple subgraph views when insertions are made to the social network database. We show that our algorithm is correct, study its complexity, and experimentally demonstrate that our algorithm can scalably handle updates to hundreds of views on 6 real-world social network databases with up to 540M edges. © 2014 ACM.",Graph matching; Social network databases; View maintenance,Algorithms; Crime; Database systems; Maintenance; Optimization; Pattern matching; Semantics; Graph matchings; Law-enforcement agencies; Multiple user; Network database; Semantic pattern; Track behavior; Underlying graphs; View maintenance; Social networking (online)
Leveraging social feedback to verify online identity claims,2014,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897518946&doi=10.1145%2f2543711&partnerID=40&md5=61a646de8bf30ad4ec4deb82ad9b5b54,"Anonymity is one of the main virtues of the Internet, as it protects privacy and enables users to express opinions more freely. However, anonymity hinders the assessment of the veracity of assertions that online users make about their identity attributes, such as age or profession. We propose FaceTrust, a system that uses online social networks to provide lightweight identity credentials while preserving a user's anonymity. FaceTrust employs a ""game with a purpose"" design to elicit the opinions of the friends of a user about the user's self-claimed identity attributes, and uses attack-resistant trust inference to assign veracity scores to identity attribute assertions. FaceTrust provides credentials, which a user can use to corroborate his assertions. We evaluate our proposal using a live Facebook deployment and simulations on a crawled social graph. The results show that our veracity scores are strongly correlated with the ground truth, even when dishonest users make up a large fraction of the social network and employ the Sybil attack. © 2014 ACM.",Anonymity; Online credential; Online identity; Online social network; Privacy,Data privacy; Online systems; Anonymity; Attack resistants; Game with a purpose; On-line social networks; Online credential; Online identity; Social feedbacks; Trust inferences; Social networking (online)
Second chance: A hybrid approach for dynamic result caching and prefetching in search engines,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891808022&doi=10.1145%2f2536777&partnerID=40&md5=7d030903dd743f658ab8a35ae490b365,"Web search engines are known to cache the results of previously issued queries. The stored results typically contain the document summaries and some data that is used to construct the final search result page returned to the user. An alternative strategy is to store in the cache only the result document IDs, which take much less space, allowing results of more queries to be cached. These two strategies lead to an interesting trade-off between the hit rate and the average query response latency. In this work, in order to exploit this trade-off, we propose a hybrid result caching strategy where a dynamic result cache is split into two sections: an HTML cache and a docID cache. Moreover, using a realistic cost model, we evaluate the performance of different result prefetching strategies for the proposed hybrid cache and the baseline HTML-only cache. Finally, we propose a machine learning approach to predict singleton queries, which occur only once in the query stream. We show that when the proposed hybrid result caching strategy is coupled with the singleton query predictor, the hit rate is further improved. © 2013 ACM.",Dynamic result caching; Result prefetching; Web search engines,HTML; Information retrieval; Websites; Caching and prefetching; Caching strategy; Cost modeling; Hybrid approach; Hybrid caches; Machine learning approaches; Prefetching; Query response; Search engines
"Analyzing, detecting, and exploiting sentiment in web queries",2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891753621&doi=10.1145%2f2535525&partnerID=40&md5=d8eecf59d587639420ab1e35e498b8d8,"The Web contains an increasing amount of biased and opinionated documents on politics, products, and polarizing events. In this article, we present an indepth analysis of Web search queries for controversial topics, focusing on query sentiment. To this end, we conduct extensive user assessments and discriminative term analyses, as well as a sentiment analysis using the SentiWordNet thesaurus, a lexical resource containing sentiment annotations. Furthermore, in order to detect the sentiment expressed in queries, we build different classifiers based on query texts, query result titles, and snippets. We demonstrate the virtue of query sentiment detection in two different use cases. First, we define a query recommendation scenario that employs sentiment detection of results to recommend additional queries for polarized queries issued by search engine users. The second application scenario is controversial topic discovery, where query sentiment classifiers are employed to discover previously unknown topics that trigger both highly positive and negative opinions among the users of a search engine. For both use cases, the results of our evaluations on real-world data are promising and show the viability and potential of query sentiment analysis in practical scenarios. © 2013 ACM.",Opinionated queries; Sentiment analysis; Web search,Data mining; Search engines; Application scenario; Controversial topics; Lexical resources; Opinionated queries; Query recommendations; Sentiment analysis; Web search queries; Web searches; World Wide Web
Control-flow patterns for decentralized RESTful service composition,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891776533&doi=10.1145%2f2535911&partnerID=40&md5=2bfe45b23a830ea301617d2de7ee5407,"The REST architectural style has attracted a lot of interest from industry due to the nonfunctional properties it contributes to Web-based solutions. SOAP/WSDL-based services, on the other hand, provide tools and methodologies that allow the design and development of software supporting complex service arrangements, enabling complex business processes which make use of well-known control-flow patterns. It is not clear if and how such patterns should be modeled, considering RESTful Web services that comply with the statelessness, uniform interface and hypermedia constraints. In this article, we analyze a set of fundamental control-flow patterns in the context of stateless compositions of RESTful services. We propose a means of enabling their implementation using the HTTP protocol and discuss the impact of our design choices according to key REST architectural principles. We hope to shed new light on the design of basic building blocks for RESTful business processes. © 2013 ACM.",Business processes; Control flow; Control-flow patterns; REST; Service composition; Web services,Design; Hypertext systems; Quality of service; Web services; Websites; Business Process; Control flows; Control-flow; REST; Service compositions; Flow patterns
Form-based web service composition for domain experts,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891778555&doi=10.1145%2f2542168&partnerID=40&md5=136d7bf24ce717b954a2dee48b19fdbc,"In many cases, it is not cost effective to automate business processes which affect a small number of people and/or change frequently.We present a novel approach for enabling domain experts to model and deploy such processes from their respective domain as Web service compositions. The approach builds on user-editable service, naming and representing Web services as forms. On this basis, the approach provides a visual composition language with a targeted restriction of control-flow expressivity, process simulation, automated process verification mechanisms, and code generation for executing orchestrations. A Web-based service composition prototype implements this approach, including a WS-BPEL code generator. A small lab user study with 14 participants showed promising results for the usability of the system, even for nontechnical domain experts. © 2013 ACM.",Automatic code generation; Business process modelling; End-user programming; SaaS; Web service composition,Automatic programming; Automation; Network components; Program compilers; Quality of service; Visual languages; Websites; Automatic code generations; Business process modelling; End user programming; SaaS; Web service composition; Web services
UsageQoS: Estimating the QoS of web services through online user communities,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891817982&doi=10.1145%2f2532635&partnerID=40&md5=e67d5629fd3a0b642047f3bd8806f6a3,"Services are an indispensable component in cloud computing. Web services are particularly important. As an increasing number of Web services provides equivalent functions, one common issue faced by users is the selection of the most appropriate one based on quality. This article presents a conceptual framework that characterizes the quality of Web services, an algorithm that quantifies them, and a system architecture that ranks Web services by using the proposed algorithm. In particular, the algorithm, called UsageQoS that computes the scores of quality of service (QoS) of Web services within a community, makes use of the usage frequencies of Web services. The frequencies are defined as the numbers of times invoked by other services in a given time period. The UsageQoS algorithm is able to optionally take user ratings as its initial input. The proposed approach has been validated by extensively experimenting on several datasets, including two real datasets. The results of the experiments have demonstrated that our approach is capable of estimating QoS parameters of Web services, regardless of whether user ratings are available or not. © 2013 ACM.",Algorithms; Design; Measurement,Algorithms; Design; Measurements; Web services; Websites; Conceptual frameworks; Equivalent functions; Online users; QoS parameters; Quality of web services; Real data sets; System architectures; Time-periods; Quality of service
Improving contextual advertising by adopting collaborative filtering,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885673794&doi=10.1145%2f2516633.2516635&partnerID=40&md5=08fcd98012410697d4db0c43fc688320,"Contextual advertising can be viewed as an information filtering task aimed at selecting suitable ads to be suggested to the final ""use"", that is, the Web page in hand. Starting from this insight, in this article we propose a novel system, which adopts a collaborative filtering approach to perform contextual advertising. In particular, given a Web page, the system relies on collaborative filtering to classify the page content and to suggest suitable ads accordingly. Useful information is extracted from ""inlinks"", that is, similar pages that link to the Web page in hand. In so doing, collaborative filtering is used in a content-based setting, giving rise to a hybrid contextual advertising system. After being implemented, the system has been experimented with about 15000 Web pages extracted from the Open Directory Project. Comparative experiments with a content-based system have been performed. The corresponding results highlight that the proposed system performs better. A suitable case study is also provided to enable the reader to better understand how the system works and its effectiveness. © 2013 ACM.",Ads; Bag of words; Classification feature; Collaborative filtering; Contextual advertising; Inlinks; Online advertising; Recommender sytems; Text categorization; Text summarization; Web,Collaborative filtering; Text processing; Websites; Ads; Bag of words; Classification features; Contextual advertisings; Inlinks; Online advertising; Recommender sytems; Text categorization; Text summarization; Web; Marketing
A feature-word-topic model for image annotation and retrieval,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885642555&doi=10.1145%2f2516633.2516634&partnerID=40&md5=38af221b248d68697d2900448c5456d5,"Image annotation is a process of finding appropriate semantic labels for images in order to obtain a more convenient way for indexing and searching images on the Web. This article proposes a novel method for image annotation based on combining feature-word distributions, which map from visual space to word space, and word-topic distributions, which form a structure to capture label relationships for annotation. We refer to this type of model as Feature-Word-Topic models. The introduction of topics allows us to efficiently take word associations, such as {ocean, fish, coral} or {desert, sand, cactus}, into account for image annotation. Unlike previous topic-based methods, we do not consider topics as joint distributions of words and visual features, but as distributions of words only. Feature-word distributions are utilized to define weights in computation of topic distributions for annotation. By doing so, topic models in text mining can be applied directly in our method. Our Feature-word-topic model, which exploits Gaussian Mixtures for feature-word distributions, and probabilistic Latent Semantic Analysis (pLSA) for word-topic distributions, shows that our method is able to obtain promising results in image annotation and retrieval. © 2013 ACM.",Gaussian mixtures; Image annotation; Image retrieval; Multi-instance multilabel learning; Probabilistic Latent Semantic Analysis (pLSA); Topic models,Data mining; Image analysis; Semantic Web; Semantics; Gaussian mixtures; Image annotation; Multi-label learning; Probabilistic latent semantic analysis; Topic model; Image retrieval
Semantic content-based recommendation of software services using context,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885660280&doi=10.1145%2f2516633.2516639&partnerID=40&md5=a32f837b8922860ab74d4c623b24a88d,"The current proliferation of software services means users should be supported when selecting one service out of the many which meet their needs. Recommender Systems provide such support for selecting products and conventional services, yet their direct application to software services is not straightforward, because of the current scarcity of available user feedback, and the need to fine-tune software services to the context of intended use. In this article, we address these issues by proposing a semantic content-based recommendation approach that analyzes the context of intended service use to provide effective recommendations in conditions of scarce user feedback. The article ends with two experiments based on a realistic set of semantic services. The first experiment demonstrates how the proposed semantic content-based approach can produce effective recommendations using semantic reasoning over service specifications by comparing it with three other approaches. The second experiment demonstrates the effectiveness of the proposed context analysis mechanism by comparing the performance of both context-aware and plain versions of our semantic content-based approach, benchmarked against user-performed selection informed by context. © 2013 ACM.",Context; Recommender systems; Semantic content-based approach; Semantic Web services; Service descriptions,Application programs; Experiments; Recommender systems; Semantic Web; Content-based approach; Content-based recommendation; Context; Conventional services; Semantic reasoning; Service description; Service specifications; Software services; Semantics
The parallel path framework for entity discovery on the Web,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885653219&doi=10.1145%2f2516633.2516638&partnerID=40&md5=ddeb86b9c9a19e1936b924821e63e0ce,"It has been a dream of the database and Web communities to reconcile the unstructured nature of the World Wide Web with the neat, structured schemas of the database paradigm. Even though databases are currently used to generate Web content in some sites, the schemas of these databases are rarely consistent across a domain. This makes the comparison and aggregation of information from different domains difficult. We aim to make an important step towards resolving this disparity by using the structural and relational information on the Web to (1) extract Web lists, (2) find entity-pages, (3) map entity-pages to a database, and (4) extract attributes of the entities. Specifically, given a Web site and an entity-page (e.g., university department and faculty member home page) we seek to find all of the entity-pages of the same type (e.g., all faculty members in the department), as well as attributes of the specific entities (e.g., their phone numbers, email addresses, office numbers). To do this, we propose aWeb structureminingmethod which grows parallel paths through the Web graph and DOM trees and propagates relevant attribute information forward. We show that by utilizing these parallel paths we can efficiently discover entity-pages and attributes. Finally, we demonstrate the accuracy of our method with a large case study. © 2013 ACM.",Entity pages; Parallel paths; Semi-structured data; Web structure mining,Trees (mathematics); Websites; Attribute information; Different domains; E-mail address; Entity pages; Faculty members; Parallel path; Semi structured data; Web structure mining; Database systems
"A bottom-up, knowledge-aware approach to integrating and querying web data services",2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887493184&doi=10.1145%2f2493536&partnerID=40&md5=d04b12c32ddd4a6cf16bf78fcb53e70f,"As a wealth of data services is becoming available on the Web, building and querying Web applications that effectively integrate their content is increasingly important. However, schema integration and ontology matching with the aim of registering data services often requires a knowledge-intensive, tedious, and errorprone manual process. We tackle this issue by presenting a bottom-up, semi-automatic service registration process that refers to an external knowledge base and uses simple text processing techniques in order to minimize and possibly avoid the contribution of domain experts in the annotation of data services. The first by-product of this process is a representation of the domain of data services as an entity-relationship diagram, whose entities are named after concepts of the external knowledge base matching service terminology rather than being manually created to accommodate an application-specific ontology. Second, a three-layer annotation of service semantics (service interfaces, access patterns, servicemarts) describing how services ""play""with such domain elements is also automatically constructed at registration time. When evaluated against heterogeneous existing data services and with a synthetic service dataset constructed using Google Fusion Tables, the approach yields good results in terms of data representation accuracy. We subsequently demonstrate that natural language processing methods can be used to decompose and match simple queries to the data services represented in three layers according to the preceding methodology with satisfactory results. We show how semantic annotations are used at query time to convert the user's request into an executable logical query. Globally, our findings show that the proposed registration method is effective in creating a uniform semantic representation of data services, suitable for building Web applications and answering search queries. © 2013 ACM.",Design,Design; Knowledge based systems; Natural language processing systems; Ontology; Semantics; Text processing; World Wide Web; Data representations; Entity relationship diagrams; NAtural language processing; Processing technique; Registration methods; Semantic annotations; Semantic representation; Service registration; Applications
Understanding latent interactions in online social networks,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887450479&doi=10.1145%2f2517040&partnerID=40&md5=af4c08923c1efc4cba869f23c7efb814,"Popular online social networks (OSNs) like Facebook and Twitter are changing the way users communicate and interact with the Internet. A deep understanding of user interactions in OSNs can provide important insights into questions of human social behavior and into the design of social platforms and applications. However, recent studies have shown that amajority of user interactions on OSNs are latent interactions, that is, passive actions, such as profile browsing, that cannot be observed by traditional measurement techniques. In this article, we seek a deeper understanding of both active and latent user interactions in OSNs. For quantifiable data on latent user interactions, we perform a detailed measurement study on Renren, the largest OSN in China with more than 220 million users to date. All friendship links in Renren are public, allowing us to exhaustively crawl a connected graph component of 42 million users and 1.66 billion social links in 2009. Renren also keeps detailed, publicly viewable visitor logs for each user profile. We capture detailed histories of profile visits over a period of 90 days for users in the Peking University Renren network and use statistics of profile visits to study issues of user profile popularity, reciprocity of profile visits, and the impact of content updates on user popularity. We find that latent interactions are much more prevalent and frequent than active events, are nonreciprocal in nature, and that profile popularity is correlated with page views of content rather than with quantity of content updates. Finally, we construct latent interaction graphs as models of user browsing behavior and compare their structural properties, evolution, community structure, and mixing times against those of both active interaction graphs and social graphs. © 2013 ACM.",Human Factors; Measurement; Performance,Human engineering; Measurements; Social networking (online); Community structures; Interaction graphs; On-line social networks; Online social networks (OSNs); Peking University; Performance; Traditional measurement techniques; User browsing behaviors; Behavioral research
Semantic contextual advertising based on the open directory project,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887481113&doi=10.1145%2f2529995.2529997&partnerID=40&md5=9b8b72cb4dd95f69ad83c7357dee9a0f,"Contextual advertising seeks to place relevant textual ads within the content of generic webpages. In this article, we explore a novel semantic approach to contextual advertising. This consists of three tasks: (1) building a well-organized hierarchical taxonomy of topics, (2) developing a robust classifier for effectively finding the topics of pages and ads, and (3) ranking ads based on the topical relevance to pages. First, we heuristically build our own taxonomy of topics from the Open Directory Project (ODP). Second, we investigate how to increase classification accuracy by taking the unique characteristics of the ODP into account. Last, we measure the topical relevance of ads by applying a link analysis technique to the similarity graph carefully derived from our taxonomy. Experiments show that our classification method improves the performance of Ma-F1 by as much as 25.7% over the baseline classifier. In addition, our ranking method enhances the relevance of ads substantially, up to 10% in terms of precision at k, compared to a representative strategy. © 2013 ACM.",Algorithms; Experimentation; Performance,Algorithms; Experiments; Semantics; Taxonomies; Classification accuracy; Classification methods; Contextual advertisings; Experimentation; Hierarchical taxonomy; Open directory projects; Performance; Semantic approach; Marketing
A vlHMM approach to context-aware search,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887493949&doi=10.1145%2f2490255&partnerID=40&md5=bd3152238d1534816e4d2ebe53d227c8,"Capturing the context of a user's query from the previous queries and clicks in the same session leads to a better understanding of the user's information need. A context-aware approach to document reranking, URL recommendation, and query suggestion may substantially improve users' search experience. In this article, we propose a general approach to context-aware search by learning a variable length hidden Markov model (vlHMM) from search sessions extracted from log data. While the mathematical model is powerful, the huge amounts of log data present great challenges. We develop several distributed learning techniques to learn a very large vlHMM under the map-reduce framework. Moreover, we construct feature vectors for each state of the vlHMM model to handle users' novel queries not covered by the training data. We test our approach on a raw dataset consisting of 1.9 billion queries, 2.9 billion clicks, and 1.2 billion search sessions before filtering, and evaluate the effectiveness of the vlHMM learned from the real data on three search applications: document reranking, query suggestion, and URL recommendation. The experiment results validate the effectiveness of vlHMM in the applications of document reranking, URL recommendation, and query suggestion. © 2013 ACM.",Algorithms; Experimentation,Algorithms; Data processing; Experiments; Hidden Markov models; Information retrieval systems; Mathematical models; Statistical tests; Context-aware approaches; Distributed learning; Experimentation; Feature vectors; Query suggestion; Search application; Search sessions; Variable length; Query processing
Robust detection of semi-structured web records using a DOM structure-knowledge-driven model,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887502073&doi=10.1145%2f2508434&partnerID=40&md5=e33a77834ee492e8c62d8b873bd6e943,"Web data record extraction aims at extracting a set of similar object records from a single webpage. These records have similar attributes or fields and are presented with a regular format in a coherent region of the page. To tackle this problem, most existing works analyze the DOM tree of an input page. One major limitation of these methods is that the lack of a global view in detecting data records from an input page results in a myopic decision. Their brute-force searching manner in detecting various types of records degrades the flexibility and robustness.We propose a Structure-Knowledge-OrientedGlobal Analysis (Skoga) framework which can perform robust detection of different-kinds of data records and record regions. The major component of the Skoga framework is a DOM structure-knowledge-driven detection model which can conduct a global analysis on the DOM structure to achieve effective detection. The DOM structure knowledge consists of background knowledge as well as statistical knowledge capturing different characteristics of data records and record regions, as exhibited in the DOM structure. The background knowledge encodes the semantics of labels indicating general constituents of data records and regions. The statistical knowledge is represented by some carefully designed features that capture different characteristics of a single node or a node group in the DOM. The feature weights are determined using a development dataset via a parameter estimation algorithm based on a structured output support vector machine. An optimization method based on the divide-and-conquer principle is developed making use of the DOM structure knowledge to quantitatively infer and recognize appropriate records and regions for a page. Extensive experiments have been conducted on four datasets. The experimental results demonstrate that our framework achieves higher accuracy compared with state-of-the-art methods. © 2013 ACM.",Algorithms; Design; Performance,Algorithms; Computer networks; Design; Internet; Back-ground knowledge; Brute-force searching; Divide-and-conquer principle; Optimization method; Parameter estimation algorithm; Performance; State-of-the-art methods; Statistical knowledge; Semantics
A term-based inverted index partitioning model for efficient distributed query processing,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885622332&doi=10.1145%2f2516633.2516637&partnerID=40&md5=2bcf7aecdd1b62d43d776fbaff58d3a4,"In a shared-nothing, distributed text retrieval system, queries are processed over an inverted index that is partitioned among a number of index servers. In practice, the index is either document-based or term-based partitioned. This choice is made depending on the properties of the underlying hardware infrastructure, query traffic distribution, and some performance and availability constraints. In query processing on retrieval systems that adopt a term-based index partitioning strategy, the high communication overhead due to the transfer of large amounts of data from the index servers forms a major performance bottleneck, deteriorating the scalability of the entire distributed retrieval system. In this work, to alleviate this problem, we propose a novel inverted index partitioning model that relies on hypergraph partitioning. In the proposed model, concurrently accessed index entries are assigned to the same index servers, based on the inverted index access patterns extracted from the past query logs. The model aims tominimize the communication overhead that will be incurred by future queries while maintaining the computational load balance among the index servers. We evaluate the performance of the proposed model through extensive experiments using a real-life text collection and a search query sample. Our results show that considerable performance gains can be achieved relative to the term-based index partitioning strategies previously proposed in literature. In most cases, however, the performance remains inferior to that attained by document-based partitioning. © 2013 ACM.",Distributed query processing; Hypergraph partitioning; Term-based index partitioning; Web search engine,Communication; Query processing; Search engines; Availability constraints; Communication overheads; Distributed query processing; Hypergraph partitioning; Index partitioning; Inverted index partitioning; Performance bottlenecks; Traffic distributions; Information retrieval
Captions and biases in diagnostic search,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887497586&doi=10.1145%2f2486040&partnerID=40&md5=4d17275ca02297f208b1df4f22e76bb2,"People frequently turn to the Web with the goal of diagnosing medical symptoms. Studies have shown that diagnostic search can often lead to anxiety about the possibility that symptoms are explained by the presence of rare, serious medical disorders, rather than far more common benign syndromes. We study the influence of the appearance of potentially-alarming content, such as severe illnesses or serious treatment options associated with the queried for symptoms, in captions comprising titles, snippets, and URLs. We explore whether users are drawn to results with potentially-alarming caption content, and if so, the implications of such attraction for the design of search engines. We specifically study the influence of the content of search result captions shown in response to symptom searches on search-result click-through behavior. We show that users are significantly more likely to examine and click on captions containing potentially-alarming medical terminology such as ""heart attack"" or ""medical emergency"" independent of result rank position and well-known positional biases in users' search examination behaviors. The findings provide insights about the possible effects of displaying implicit correlates of searchers' goals in search-result captions, such as unexpressed concerns and fears. As an illustration of the potential utility of these results, we developed and evaluated an enhanced click prediction model that incorporates potentially-alarming caption features and show that it significantly outperforms models that ignore caption content. Beyond providing additional understanding of the effects ofWeb content on medical concerns, the methods and findings have implications for search engine design. As part of our discussion on the implications of this research, we propose procedures for generating more representative captions that may be less likely to cause alarm, as well as methods for learning to more appropriately rank search results from logged search behavior, for examples, by also considering the presence of potentially-alarming content in the captions that motivate observed clicks and down-weighting clicks seemingly driven by searchers' health anxieties. © 2013 ACM.",Experimentation; Human Factors,Human engineering; Machine design; Search engines; Experimentation; Heart attack; Medical disorder; Medical emergency; Medical terminologies; Potential utility; Prediction model; Search behavior; Diagnosis
Virtual private social networks and a Facebook implementation,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885627507&doi=10.1145%2f2516633.2516636&partnerID=40&md5=d2edf6c5a73f85d2153ada5cbd8ad7f1,"The popularity of Social Networking Sites (SNS) is growing rapidly, with the largest sites serving hundreds of millions of users and their private information. The privacy settings of these SNSs do not allow the user to avoid sharing some information (e.g., name and profile picture) with all the other users. Also, no matter the privacy settings, this information is always shared with the SNS (that could sell this information or be hacked). To mitigate these threats, we recently introduced the concept of Virtual Private Social Networks (VPSNs). In this work we propose the first complete architecture and implementation of VPSNs for Facebook. In particular, we address an important problem left unexplored in our previous research-that is the automatic propagation of updated profiles to all the members of the same VPSN. Furthermore, we made an in-depth study on performance and implemented several optimization to reduce the impact of VPSN on user experience. The proposed solution is lightweight, completely distributed, does not depend on the collaboration from Facebook, does not have a central point of failure, it offers (with some limitations) the same functionality as Facebook, and apart from some simple settings, the solution is almost transparent to the user. Thorough experiments, with an extended set of parameters, we have confirmed the feasibility of the proposal and have shown a very limited time-overhead experienced by the user while browsing Facebook pages. © 2013 ACM.",Facebook privacy; Internet and privacy; Social networking sites; Virtual private social networks,Computer networks; Internet; Central point; Facebook; Facebook pages; In-depth study; Privacy Settings; Private information; Social networking sites; User experience; Social networking (online)
Web browsing behavior analysis and interactive hypervideo,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887436108&doi=10.1145%2f2529995.2529996&partnerID=40&md5=90276f1c8745410c7712790417459dad,"Processing data on any sort of user interaction is well known to be cumbersome and mostly time consuming. In order to assist researchers in easily inspecting fine-grained browsing data, current tools usually display user interactions as mouse cursor tracks, a video-like visualization scheme. However, to date, traditional online video inspection has not explored the full capabilities of hypermedia and interactive techniques. In response to this need, we have developed SMT2ε, a Web-based tracking system for analyzing browsing behavior using feature-rich hypervideo visualizations. We compare our system to related work in academia and the industry, showing that ours features unprecedented visualization capabilities. We also show that SMT2ε efficiently captures browsing data and is perceived by users to be both helpful and usable. A series of prediction experiments illustrate that raw cursor data are accessible and can be easily handled, providing evidence that the data can be used to construct and verify research hypotheses. Considering its limitations, it is our hope that SMT2ε will assist researchers, usability practitioners, and other professionals interested in understanding how users browse the Web. © 2013 ACM.",Design; Human Factors; Performance,Data handling; Design; Human engineering; Research; Visualization; Web browsers; Browsing behavior; Browsing behavior analysis; Interactive techniques; Mouse cursor; Performance; Related works; Tracking system; User interaction; Data visualization
A measurement study of insecure JavaScript practices on the Web,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879906311&doi=10.1145%2f2460383.2460386&partnerID=40&md5=7902f9606b2a1f994f21207ac7474971,"JavaScript is an interpreted programming language most often used for enhancing webpage interactivity and functionality. It has powerful capabilities to interact with webpage documents and browser windows, however, it has also opened the door for many browser-based security attacks. Insecure engineering practices of using JavaScript may not directly lead to security breaches, but they can create new attack vectors and greatly increase the risks of browser-based attacks. In this article, we present the first measurement study on insecure practices of using JavaScript on the Web. Our focus is on the insecure practices of JavaScript inclusion and dynamic generation, and we examine their severity and nature on 6,805 unique websites. Our measurement results reveal that insecure JavaScript practices are common at various websites: (1) at least 66.4% of the measured websites manifest the insecure practices of including JavaScript files from external domains into the top-level documents of their webpages; (2) over 44.4% of the measured websites use the dangerous eval() function to dynamically generate and execute JavaScript code on their webpages; and (3) in JavaScript dynamic generation, using the document.write() method and the innerHTML property is much more popular than using the relatively secure technique of creating script elements via DOM methods. Our analysis indicates that safe alternatives to these insecure practices exist in common cases and ought to be adopted by website developers and administrators for reducing potential security risks. © 2013 ACM.",AST tree matching; Execution-based measurement; JavaScript; Same origin policy; Security; Web engineering,Interactive computer systems; Websites; Javascript; Same-origin policy; Security; Tree-matching; Web engineering; High level languages
Enhancing the trust-based recommendation process with explicit distrust,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879924036&doi=10.1145%2f2460383.2460385&partnerID=40&md5=7855a8624fe165538ed26b0be91d2ebc,"When aWeb application with a built-in recommender offers a social networking component which enables its users to form a trust network, it can generate more personalized recommendations by combining user ratings with information from the trust network. These are the so-called trust-enhanced recommendation systems. While research on the incorporation of trust for recommendations is thriving, the potential of explicitly stated distrust remains almost unexplored. In this article, we introduce a distrust-enhanced recommendation algorithm which has its roots in Golbeck's trust-based weighted mean. Through experiments on a set of reviews from Epinions.com, we show that our new algorithm outperforms its standard trust-only counterpart with respect to accuracy, thereby demonstrating the positive effect that explicit distrust can have on trustbased recommendations. © 2013 ACM.",Distrust; Recommender systems; Social networks; Trust metrics,Recommender systems; Social networking (online); Distrust; Its standards; Personalized recommendation; Recommendation algorithms; Trust metrics; Trust networks; Trust-based recommendations; Weighted mean; Algorithms
Progress on website accessibility?,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880250012&doi=10.1145%2f2435215.2435217&partnerID=40&md5=64404d2692e934b63a390b5d038596cf,"Over 100 top-traffic and government websites from the United States and United Kingdom were examined for evidence of changes on accessibility indicators over the 14-year period from 1999 to 2012, the longest period studied to date. Automated analyses of WCAG 2.0 Level A Success Criteria found high percentages of violations overall. Unlike more circumscribed studies, however, these sites exhibited improvements over the years on a number of accessibility indicators, with government sites being less likely than topsites to have accessibility violations. Examination of the causes of success and failure suggests that improving accessibility may be due, in part, to changes in website technologies and coding practices rather than a focus on accessibility per se. © 2013 ACM.",Disability; Web accessibility,Computer networks; Internet; Automated analysis; Disability; Government websites; United kingdom; Web accessibility; Website accessibility; Websites
A language for end-user Web augmentation: Caring for producers and consumers alike,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879994457&doi=10.1145%2f2460383.2460388&partnerID=40&md5=2d68c8b9485c0dfd27bf49e854da06d9,"Web augmentation is to the Web what augmented reality is to the physical world: layering relevant content/ layout/navigation over the existing Web to customize the user experience. This is achieved through JavaScript (JS) using browser weavers (e.g., Greasemonkey). To date, over 43 million of downloads of Greasemonkey scripts ground the vitality of this movement. However, Web augmentation is hindered by being programming intensive and prone to malware. This prevents end-users from participating as both producers and consumers of scripts: producers need to know JS, consumers need to trust JS. This article aims at promoting end-user participation in oth roles. The vision is for end-users to prosume (the act of simultaneously caring for producing and consuming) scripts as easily as they currently prosume their pictures or videos. Encouraging production requires more ""natural"" and abstract constructs. Promoting consumption calls for augmentation scripts to be easier to understand, share, and trust upon. To this end, we explore the use of Domain-Specific Languages (DSLs) by introducing Sticklet. Sticklet is an internal DSL on JS, where JS generality is reduced for the sake of learnability and reliability. Specifically, Web augmentation is conceived as fixing in existing web sites (i.e., the wall) HTML fragments extracted from either other sites or Web services (i.e., the stickers). Sticklet targets hobby programmers as producers, and computer literates as consumers. From a producer perspective, benefits are threefold. As a restricted grammar on top of JS, Sticklet expressions are domain oriented and more declarative than their JS counterparts, hence speeding up development. As syntactically correct JS expressions, Sticklet scripts can be installed as traditional scripts and hence, programmers can continue using existing JS tools. As declarative expressions, they are easier to maintain, and amenable for optimization. From a consumer perspective, domain specificity brings understandability (due to declarativeness), reliability (due to built-in security), and ""consumability"" (i.e., installation/enactment/sharing of Sticklet expressions are tuned to the shortage of time and skills of the targetaudience). Preliminary evaluations indicate that 77% of the subjects were able to develop new Sticklet scripts in less than thirty minutes while 84% were able to consume these scripts in less than ten minutes. Sticklet is available to download as a Mozilla add-on. © 2013 ACM.",Domain-specific language; End-user programming; Greasemonkey; JavaScript; Web augmentation,Abstracting; Augmented reality; Computer programming; Graphical user interfaces; Problem oriented languages; Web services; Websites; Domain specific languages; End user programming; Greasemonkey; Javascript; Web augmentation; Web browsers
Coordinating the web of services for a smart home,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879928714&doi=10.1145%2f2460383.2460389&partnerID=40&md5=2e7fd86ee6220d89118855407480fd3c,"Domotics, concerned with the realization of intelligent home environments, is a novel field which can highly benefit from solutions inspired by service-oriented principles to enhance the convenience and security of modern home residents. In this work, we present an architecture for a smart home, starting from the lower device interconnectivity level up to the higher application layers that undertake the load of complex functionalities and provide a number of services to end-users. We claim that in order for smart homes to exhibit a genuinely intelligent behavior, the ability to compute compositions of individual devices automatically and dynamically is paramount. To this end, we incorporate into the architecture a composition component that employs artificial intelligence domain-independent planning to generate compositions at runtime, in a constantly evolving environment. We have implemented a fully working prototype that realizes such an architecture, and have evaluated it both in terms of performance as well as from the end-user point of view. The results of the evaluation show that the service-oriented architectural design and the support for dynamic compositions is quite efficient from the technical point of view, and that the system succeeds in satisfying the expectations and objectives of the users. © 2013 ACM.",Internet of things; Service composition; Service-oriented architecture,Architecture; Artificial intelligence; Automation; Information services; Service oriented architecture (SOA); Web services; Application layers; Domain-independent planning; Dynamic composition; Individual devices; Intelligent behavior; Internet of Things (IOT); Number of services; Service compositions; Intelligent buildings
A test-based security certification scheme for Web services,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879910325&doi=10.1145%2f2460383.2460384&partnerID=40&md5=c42413b19f86f746a5f70b0a0df3f00a,"The Service-Oriented Architecture (SOA) paradigm is giving rise to a new generation of applications built by dynamically composing loosely coupled autonomous services. Clients (i.e., software agents acting on behalf of human users or service providers) implementing such complex applications typically search and integrate services on the basis of their functional requirements and of their trust in the service suppliers. A major issue in this scenario relates to the definition of an assurance technique allowing clients to select services on the basis of their nonfunctional requirements and increasing their confidence that the selected services will satisfy such requirements. In this article, we first present an assurance solution that focuses on security and supports a test-based security certification scheme for Web services. The certification scheme is driven by the security properties to be certified and relies upon a formal definition of the service model. The evidence supporting a certified property is computed using a model-based testing approach that, starting from the service model, automatically generates the test cases to be used in the service certification. We also define a set of indexes and metrics that evaluate the assurance level and the quality of the certification process. Finally, we present our evaluation toolkit and experimental results obtained applying our certificationsolution to a financial service implementing the Interactive Financial eXchange (IFX) standard. © 2013 ACM.",Model-based testing; Security certification; Service-Oriented Architecture; Symbolic transition systems; Web services,Information services; Service oriented architecture (SOA); Websites; Certification process; Complex applications; Functional requirement; Model based testing; Model-based testing approaches; Non-functional requirements; Security certification; Symbolic Transition Systems; Web services
Assessing relevance and trust of the deep web sources and results based on inter-source agreement,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879994707&doi=10.1145%2f2460383.2460390&partnerID=40&md5=5080e319c87561e9c36565d88de74905,"Deep web search engines face the formidable challenge of retrieving high-quality results from the vast collection of searchable databases. Deep web search is a two-step process of selecting the high-quality sources and ranking the results from the selected sources. Though there are existing methods for both the steps, they assess the relevance of the sources and the results using the query-result similarity. When applied to the deep web these methods have two deficiencies. First is that they are agnostic to the correctness (trustworthiness) of the results. Second, the query-based relevance does not consider the importance of the results and sources. These two considerations are essential for the deep web and open collections in general. Since a number of deep web sources provide answers to any query, we conjuncture that the agreements between these answers are helpful in assessing the importance and the trustworthiness of the sources and the results. For assessing source quality, we compute the agreement between the sources as the agreement of the answers returned. While computing the agreement, we also measure and compensate for the possible collusion between the sources. This adjusted agreement is modeled as a graph with sources at the vertices. On this agreement graph, a quality score of a source, that we call SourceRank, is calculated as the stationary visit probability of a random walk. For ranking results, we analyze the second-order agreement between the results. Further extending SourceRank to multidomain search, we propose a source ranking sensitive to the query domains. Multiple domain-specific rankings of a source are computed, and these ranks are combined for the final ranking. We perform extensive evaluations on online and hundreds of Google Base sources spanning across domains. The proposed result and source rankings are implemented in the deep web search engine Factal.We demonstrate that the agreement analysis tracks source corruption. Further, our relevance evaluations show that our methods improve precision significantly over Google Base and the other baseline methods. The result ranking and the domain-specific source ranking are evaluated separately. © 2013 ACM.",Agreement analysis; Database integration; Deep web integration; Deep web search; Source rank; Web database search; Web trust,Information retrieval; Query processing; Websites; Database integration; Deep web; Source rank; Web database; Web trust; Search engines
HTML automatic table layout,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880237886&doi=10.1145%2f2435215.2435219&partnerID=40&md5=14f7c4829e3ec3050b1c96732d28467b,"Automatic layout of tables is required in online applications because of the need to tailor the layout to the viewport width, choice of font, and dynamic content. However, if the table contains text, minimizing the height of the table for a fixed maximum width is NP-hard. Thus, more efficient heuristic algorithms are required. We evaluate the HTML table layout recommendation and find that while it generally produces quite compact layout it is brittle and can lead to quite uncompact layout. We present an alternate heuristic algorithm. It uses a greedy strategy that starts from the widest reasonable layout and repeatedly chooses to narrow the column for which narrowing leads to the least increase in table height. The algorithm is simple, fast enough to be used in online applications, and gives significantly more compact layout than is obtained with HTML's recommended table layout algorithm. © 2013 ACM.",Automatic table layout; Constrained optimization; Electronic publishing,Constrained optimization; Electronic publishing; Heuristic algorithms; Automatic layout; Dynamic content; Greedy strategies; HTML tables; NP-hard; On-line applications; Table layouts; HTML
A comprehensive study of techniques for URL-based web page language classification,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880239938&doi=10.1145%2f2435215.2435218&partnerID=40&md5=9a2c4ddb8d8ca5dfa725b005923f065d,"Given only the URL of a Web page, can we identify its language? In this article we examine this question. URL-based language classification is useful when the content of the Web page is not available or downloading the content is a waste of bandwidth and time. We built URL-based language classifiers for English, German, French, Spanish, and Italian by applying a variety of algorithms and features. As algorithms we used machine learning algorithms which are widely applied for text classification and state-of-art algorithms for language identification of text. As features we used words, various sized n-grams, and custom-made features (our novel feature set). We compared our approaches with two baseline methods, namely classification by country code top-level domains and classification by IP addresses of the hosting Web servers. We trained and tested our classifiers in a 10-fold cross-validation setup on a dataset obtained from the Open Directory Project and from querying a commercial search engine. We obtained the lowest F1-measure for English (94) and the highest F1-measure for German (98) with the best performing classifiers. We also evaluated the performance of our methods: (i) on a set of Web pages written in Adobe Flash and (ii) as part of a language-focused crawler. In the first case, the content of the Web page is hard to extract and in the second page downloading pages of the ""wrong"" language constitutes a waste of bandwidth. In both settings the best classifiers have a high accuracy with an F1-measure between 95 (for English) and 98 (for Italian) for the Adobe Flash pages and a precision between 90 (for Italian) and 97 (for French) for the language-focused crawler. © 2013 ACM.",Document and text processing; Language classification; URL; Web page classification,Bandwidth; Classification (of information); Learning algorithms; Search engines; Text processing; Websites; 10-fold cross-validation; Document and text processing; Language identification; Open directory projects; Text classification; URL; Web page classification; Web page languages; Information retrieval systems
Measuring the visual complexities of web pages,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880201938&doi=10.1145%2f2435215.2435216&partnerID=40&md5=7ddedf99bbd6e5f67c9d551c94d5c0f3,"Visual complexities (VisComs) of Web pages significantly affect user experience, and automatic evaluation can facilitate a large number of Web-based applications. The construction of a model for measuring the Vis- Coms of Web pages requires the extraction of typical features and learning based on labeled Web pages. However, as far as the authors are aware, little headway has been made on measuring VisCom in Web mining and machine learning. The present article provides a new approach combining Web mining techniques and machine learning algorithms for measuring the VisComs of Web pages. The structure of a Web page is first analyzed, and the layout is then extracted. Using a Web page as a semistructured image, three classes of features are extracted to construct a feature vector. The feature vector is fed into a learned measuring function to calculate the VisCom of the page. In the proposed approach of the present study, the type of the measuring function and its learning depend on the quantification strategy for VisCom. Aside from using a category and a score to represent VisCom as existing work, this study presents a new strategy utilizing a distribution to quantify the VisCom of a Web page. Empirical evaluation suggests the effectiveness of the proposed approach in terms of both features and learning algorithms. © 2013 ACM.",Feature; Learning; Quantification; Visual complexity (VisCom); Web mining,Intelligent systems; Learning algorithms; Learning systems; Feature; Learning; Quantification; Visual complexity; Web Mining; Websites
Understanding Query Interfaces by Statistical Parsing,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181588039&doi=10.1145%2f2460383.2460387&partnerID=40&md5=d047ccf094d4d96514b690d5a96ccd07,"Users submit queries to an online database via its query interface. Query interface parsing, which is important for many applications, understands the query capabilities of a query interface. Since most query interfaces are organized hierarchically, we present a novel query interface parsing method, StatParser (Statistical Parser), to automatically extract the hierarchical query capabilities of query interfaces. StatParser automatically learns from a set of parsed query interfaces and parses new query interfaces. StatParser starts from a small grammar and enhances the grammar with a set of probabilities learned from parsed query interfaces under the maximum-entropy principle. Given a new query interface, the probability-enhanced grammar identifies the parse tree with the largest global probability to be the query capabilities of the query interface. Experimental results show that StatParser very accurately extracts the query capabilities and can effectively overcome the problems of existing query interface parsers. © 2013, ACM. All rights reserved.",Algorithms; Experimentation Query Interface; Maximum Entropy; Performance,Entropy; Formal languages; Query languages; Query processing; Hierarchical queries; Learn+; Maximum-entropy; Methods:statistical; Online database; Parsing methods; Query capabilities; Query interfaces; Statistical parser; Statistical parsing; Probability
Efficient time-stamped event sequence anonymi,2013,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891773277&doi=10.1145%2f2532643&partnerID=40&md5=0eac155d233facd353bf99629bac8315,"With the rapid growth of applications which generate timestamped sequences (click streams, GPS trajectories, RFID sequences), sequence anonymization has become an important problem, in that should such data be published or shared. Existing trajectory anonymization techniques disregard the importance of time or the sensitivity of events. This article is the first, to our knowledge, thorough study on time-stamped event sequence anonymization. We propose a novel and tunable generalization framework tailored to event sequences. We generalize time stamps using time intervals and events using a taxonomy which models the domain semantics. We consider two scenarios: (i) sharing the data with a single receiver (the SSR setting), where the receiver's background knowledge is confined to a set of time stamps and time generalization suffices, and (ii) sharing the data with colluding receivers (the SCR setting), where time generalization should be combined with event generalization. For both cases, we propose appropriate anonymization methods that prevent both user identification and event prediction. To achieve computational efficiency and scalability, we propose optimization techniques for both cases using a utility-based index, compact summaries, fast to compute bounds for utility, and a novel taxonomy-aware distance function. Extensive experiments confirm the effectiveness of our approach compared with state of the art, in terms of information loss, range query distortion, and preserving temporal causality patterns. Furthermore, our experiments demonstrate efficiency and scalability on large-scale real and synthetic datasets.© 2013 ACM.",Anonymity; Privacy-preserving data publishing; Time and URL generalization; User browsing history,Computational efficiency; Data privacy; Efficiency; Scalability; Semantics; Taxonomies; Anonymity; Back-ground knowledge; Browsing history; Distance functions; Optimization techniques; Privacy Preserving Data Publishing; Time and URL generalization; User identification; Graphical user interfaces
Exploiting external collections for query expansion,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870673325&doi=10.1145%2f2382616.2382621&partnerID=40&md5=e37b03b71631fc89c59dad1d7a1ec1a6,"A persisting challenge in the field of information retrieval is the vocabulary mismatch between a user's information need and the relevant documents. One way of addressing this issue is to apply query modeling: to add terms to the original query and reweigh the terms. In social media, where documents usually contain creative and noisy language (e.g., spelling and grammatical errors), query modeling proves difficult. To address this, attempts to use external sources for query modeling have been made and seem to be successful. In this article we propose a general generative query expansion model that uses external document collections for term generation: the External Expansion Model (EEM). The main rationale behind our model is our hypothesis that each query requires its own mixture of external collections for expansion and that an expansion model should account for this. For some queries we expect, for example, a news collection to be most beneficial, while for other queries we could benefit more by selecting terms from a general encyclopedia. EEM allows for query-dependent weighing of the external collections. We put our model to the test on the task of blog post retrieval and we use four external collections in our experiments: (i) a news collection, (ii) a Web collection, (iii) Wikipedia, and (iv) a blog post collection. Experiments show that EEM outperforms query expansion on the individual collections, as well as the Mixture of Relevance Models that was previously proposed by Diaz and Metzler [2006]. Extensive analysis of the results shows that our naive approach to estimating query-dependent collection importance works reasonably well and that, when we use ""oracle"" settings, we see the full potential of our model. We also find that the query-dependent collection importance has more impact on retrieval performance than the independent collection importance (i.e., a collection prior). © 2012 ACM.",Blog post retrieval; External expansion; Query modeling,Blogs; Expansion; Experiments; Blog post retrieval; Document collection; External sources; Grammatical errors; Information need; Query expansion; Query expansion models; Relevance models; Relevant documents; Retrieval performance; Social media; Web collections; Wikipedia; Information retrieval
Beyond social graphs: User interactions in online Social networks and their implications,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870675357&doi=10.1145%2f2382616.2382620&partnerID=40&md5=6ecc452c7792726fce9a30bafc6768d7,"Social networks are popular platforms for interaction, communication, and collaboration between friends. Researchers have recently proposed an emerging class of applications that leverage relationships from social networks to improve security and performance in applications such as email, Web browsing, and overlay routing. While these applications often cite social network connectivity statistics to support their designs, researchers in psychology and sociology have repeatedly cast doubt on the practice of inferring meaningful relationships from social network connections alone. This leads to the question: ""Are social links valid indicators of real user interaction? If not, then how can we quantify these factors to form a more accurate model for evaluating socially enhanced applications?"" In this article, we address this question through a detailed study of user interactions in the Facebook social network. We propose the use of ""interaction graphs"" to impart meaning to online social links by quantifying user interactions. We analyze interaction graphs derived from Facebook user traces and show that they exhibit significantly lower levels of the ""small-world"" properties present in their social graph counterparts. This means that these graphs have fewer ""supernodes"" with extremely high degree, and overall graph diameter increases significantly as a result. To quantify the impact of our observations, we use both types of graphs to validate several well-known social-based applications that rely on graph properties to infuse new functionality into Internet applications, including Reliable Email (RE), SybilGuard, and the weighted cascade influence maximization algorithm. The results reveal new insights into each of these systems, and confirm our hypothesis that to obtain realistic and accurate results, ongoing research on social network applications studies of social applications should use real indicators of user interactions in lieu of social graphs. © 2012 ACM.",Facebook; Interaction graphs; Social networks,Electronic mail; Graphic methods; Internet; Research; Facebook; Graph diameter; Graph properties; Influence maximizations; Interaction graphs; Internet application; Online social networks; Overlay routing; Popular platform; Security and performance; Small worlds; Social graphs; Social Networks; Social-based; Supernodes; User interaction; User trace; Social networking (online)
Cache-based query processing for search engines,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870699322&doi=10.1145%2f2382616.2382617&partnerID=40&md5=b352b6de11b91530793d99506093c898,"In practice, a search engine may fail to serve a query due to various reasons such as hardware/network failures, excessive query load, lack of matching documents, or service contract limitations (e.g., the query rate limits for third-party users of a search service). In this kind of scenarios, where the backend search system is unable to generate answers to queries, approximate answers can be generated by exploiting the previously computed query results available in the result cache of the search engine.In this work, we propose two alternative strategies to implement this cache-based query processing idea. The first strategy aggregates the results of similar queries that are previously cached in order to create synthetic results for new queries. The second strategy forms an inverted index over the textual information (i.e., query terms and result snippets) present in the result cache and uses this index to answer new queries. Both approaches achieve reasonable result qualities compared to processing queries with an inverted index built on the collection. © 2012 ACM.",Query view; Result aggregation; Result caching; System availability; Web search engine,Search engines; Inverted indices; Query results; Query terms; Query view; Rate limit; Result caching; Search services; Search system; Service contract; System availability; Textual information; Query processing
A methodology for SIP and SOAP integration using application-specific protocol conversion,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870688341&doi=10.1145%2f2382616.2382618&partnerID=40&md5=73b5b21f11d58f1484da9733c0e9cdaa,"In recent years, the ubiquitous demands for cross-protocol application access are driving the need for deeper integration between SIP and SOAP. In this article we present a novel methodology for integrating these two protocols. Through an analysis of properties of SIP and SOAP we show that integration between these protocols should be based on application-specific converters. We describe a generic SIP/SOAP gateway that implements message handling and network and storage management while relying on application-specific converters to define session management and message mapping for a specific set of SIP and SOAP communication nodes. In order to ease development of these converters, we introduce an XML-based domain-specific language for describing application-specific conversion processes. We show how conversion processes can be easily specified in the language using message sequence diagrams of the desired interaction. We evaluate the presented methodology through performance analysis of the developed prototype gateway and high-level comparison with other solutions. © 2012 ACM.",Design concepts; Middleware; Protocol conversion; SIP; SOAP; Specialized application languages,Integration; Internet protocols; Middleware; Problem oriented languages; Soaps (detergents); Communication nodes; Conversion process; Design concept; Domain specific languages; Message handling; Message sequences; Novel methodology; Performance analysis; Protocol conversion; Session management; SIP; Specialized application languages; Storage management; Gateways (computer networks)
Workload characterization and performance implications of large-scale blog servers,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870696631&doi=10.1145%2f2382616.2382619&partnerID=40&md5=7c6e5e607223c63b104c982c5df3472a,"With the ever-increasing popularity of Social Network Services (SNSs), an understanding of the characteristics of these services and their effects on the behavior of their host servers is critical. However, there has been a lack of research on the workload characterization of servers running SNS applications such as blog services. To fill this void, we empirically characterized real-world Web server logs collected from one of the largest South Korean blog hosting sites for 12 consecutive days. The logs consist of more than 96 million HTTP requests and 4.7TB of network traffic. Our analysis reveals the following: (i) The transfer size of nonmultimedia files and blog articles can be modeled using a truncated Pareto distribution and a log-normal distribution, respectively; (ii) user access for blog articles does not show temporal locality, but is strongly biased towards those posted with image or audio files. We additionally discuss the potential performance improvement through clustering of small files on a blog page into contiguous disk blocks, which benefits from the observed file access patterns. Trace-driven simulations show that, on average, the suggested approach achieves 60.6% better system throughput and reduces the processing time for file access by 30.8% compared to the best performance of the Ext4 filesystem. © 2012 ACM.",Filesystems; Measurement; Modeling; Social Network Services; Workload characterization,Characterization; Measurements; Models; Social networking (online); Audio files; Disk block; File access; Filesystem; Filesystems; Host servers; Log-normal distribution; Network traffic; Pareto distributions; Performance improvements; Processing time; Small files; Social network services; System throughput; Temporal locality; Trace driven simulation; User access; Web server logs; Workload characterization; Blogs
Navigating tomorrow's web: From searching and browsing to visual exploration,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870168732&doi=10.1145%2f2344416.2344420&partnerID=40&md5=b6d2cb7b90e1afd289880a62b0814f1c,"We propose a new way of navigating the Web using interactive information visualizations, and present encouraging results from a large-scale Web study of a visual exploration system. While the Web has become an immense, diverse information space, it has also evolved into a powerful software platform.We believe that the established interaction techniques of searching and browsing do not sufficiently utilize these advances, since information seekers have to transform their information needs into specific, text-based search queries resulting in mostly text-based lists of resources. In contrast, we foresee a new type of information seeking that is high-level and more engaging, by providing the information seeker with interactive visualizations that give graphical overviews and enable query formulation. Building on recent work on faceted navigation, information visualization, and exploratory search, we conceptualize this type of information navigation as visual exploration and evaluate a prototype Web-based system that implements it. We discuss the results of a large-scale, mixed-method Web study that provides a better understanding of the potential benefits of visual exploration on the Web, and its particular performance challenges. © 2012 ACM.",Exploratory search; Faceted navigation; Information retrieval; Information visualization; Visual information seeking; World wide web,Information analysis; Information retrieval; Information systems; Information use; Navigation; World Wide Web; Exploratory search; Information need; Information seeking; Information spaces; Information visualization; Interaction techniques; Interactive information visualization; Interactive visualizations; Potential benefits; Query formulation; Search queries; Visual exploration; Visual information seeking; Web-based system; Search engines
Extracting information networks from the blogosphere,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870175941&doi=10.1145%2f2344416.2344418&partnerID=40&md5=ccece81d7215fd93f21549355320d273,"We study the problem of automatically extracting information networks formed by recognizable entities as well as relations among them from social media sites. Our approach consists of using state-of-the-art natural language processing tools to identify entities and extract sentences that relate such entities, followed by using text-clustering algorithms to identify the relations within the information network. We propose a new term-weighting scheme that significantly improves on the state-of-the-art in the task of relation extraction, both when used in conjunction with the standard tf idf scheme and also when used as a pruning filter. We describe an effective method for identifying benchmarks for open information extraction that relies on a curated online database that is comparable to the hand-crafted evaluation datasets in the literature. From this benchmark, we derive a much larger dataset which mimics realistic conditions for the task of open information extraction. We report on extensive experiments on both datasets, which not only shed light on the accuracy levels achieved by state-of-the-art open information extraction tools, but also on how to tune such tools for better results. © 2012 ACM.",Clustering; Domain frequency; Named entities; Open information extraction; Relation extraction,Clustering algorithms; Information services; Clustering; Domain frequency; Information Extraction; Named entities; Relation extraction; Natural language processing systems
A model-driven methodology to the content layout problem in web applications,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870197804&doi=10.1145%2f2344416.2344417&partnerID=40&md5=ebabc2fe7bdc33210de2111a25a42714,"This article presents a model-driven approach for the design of the layout in a complex Web application, where large amounts of data are accessed. The aim of this work is to reduce, as much as possible, repetitive tasks and to factor out common aspects into different kinds of rules that can be reused across different applications. In particular, exploiting the conceptual elements of the typical models used for the design of a Web application, it defines presentation and layout rules at different levels of abstraction and granularity. A procedure for the automatic layout of the content of a page is proposed and evaluated, and the layout of advanced Web applications is discussed. © 2012 ACM.",Automatic contents layout; Graphical visualization and rendering; Web applications design,Design; Automatic content; Automatic layout; Conceptual elements; Graphical visualization; Large amounts of data; Layout problems; Levels of abstraction; Model driven approach; Model-driven methodology; Repetitive task; Typical model; WEB application; World Wide Web
FoXtrot: Distributed structural and value XML filtering,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870157184&doi=10.1145%2f2344416.2344419&partnerID=40&md5=a365609698e388cceda0cb0107dde58a,"Publish/subscribe systems have emerged in recent years as a promising paradigm for offering various popular notification services. In this context, many XML filtering systems have been proposed to efficiently identify XML data that matches user interests expressed as queries in an XML query language like XPath. However, in order to offer XML filtering functionality on an Internet-scale, we need to deploy such a service in a distributed environment, avoiding bottlenecks that can deteriorate performance. In this work, we design and implement FoXtrot, a system for filtering XML data that combines the strengths of automata for efficient filtering and distributed hash tables for building a fully distributed system. Apart from structural-matching, performed using automata, we also discuss different methods for evaluating value-based predicates. We perform an extensive experimental evaluation of our system, FoXtrot, on a local cluster and on the PlanetLab network and demonstrate that it can index millions of user queries, achieving a high indexing and filtering throughput. At the same time, FoXtrot exhibits very good load-balancing properties and improves its performance as we increase the size of the network. © 2012 ACM.",Automata; Distributed hash tables; Load-balancing; XML filtering,Automata theory; Automata; Distributed environments; Distributed Hash Table; Distributed systems; Experimental evaluation; Load-Balancing; Local cluster; Notification Service; PlanetLab; Publish/Subscribe system; User interests; User query; Value-based predicates; XML data; XML filtering; XML query language; XML
Integrating trust management and access control in data-intensive web applications,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863623874&doi=10.1145%2f2180861.2180863&partnerID=40&md5=87829c68cd86a4eef498d9494db248c5,"The widespread diffusion of Web-based services provided by public and private organizations emphasizes the need for a flexible solution for protecting the information accessible through Web applications. A promising approach is represented by credential-based access control and trust management. However, although much research has been done and several proposals exist, a clear obstacle to the realization of their benefits in data-intensive Web applications is represented by the lack of adequate support in the DBMSs. As a matter of fact, DBMSs are often responsible for the management of most of the information that is accessed using a Web browser or a Web service invocation. In this article, we aim at eliminating this gap, and present an approach integrating trust management with the access control of the DBMS. We propose a trust model with a SQL syntax and illustrate an algorithm for the efficient verification of a delegation path for certificates. Our solution nicely complements current trust management proposals allowing the efficient realization of the services of an advanced trust management model within current relational DBMSs. An important benefit of our approach lies in its potential for a robust end-to-end design of security for personal data in Web scenario, where vulnerabilities of Web applications cannot be used to violate the protection of the data residing on the database server. We also illustrate the implementation of our approach within an open-source DBMS discussing design choices and performance impact. © 2012 ACM.",Access control; Relational databases; Trust management,Security of data; Web crawler; Web services; Websites; Credential based access control; Data-intensive web applications; Performance impact; Private organizations; Relational Database; Trust management; Trust management model; Web service invocation; Access control
Friendship prediction and homophily in social media,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863608196&doi=10.1145%2f2180861.2180866&partnerID=40&md5=8beebae1b9440acc9ca5790aa82079aa,"Social media have attracted considerable attention because their open-ended nature allows users to create lightweight semantic scaffolding to organize and share content. To date, the interplay of the social and topical components of social media has been only partially explored. Here, we study the presence of homophily in three systems that combine tagging social media with online social networks. We find a substantial level of topical similarity among users who are close to each other in the social network. We introduce a null model that preserves user activity while removing local correlations, allowing us to disentangle the actual local similarity between users from statistical effects due to the assortative mixing of user activity and centrality in the social network. This analysis suggests that users with similar interests are more likely to be friends, and therefore topical similarity measures among users based solely on their annotation metadata should be predictive of social links. We test this hypothesis on several datasets, confirming that social networks constructed from topical similarity capture actual friendship accurately. When combined with topological features, topical similarity achieves a link prediction accuracy of about 92%. © 2012 ACM.",Collaborative tagging; Folksonomies; Homophily; Link prediction; Maximum information path; Social media; Social network; Topical similarity,Forecasting; Metadata; Scaffolds; Semantics; Collaborative tagging; Folksonomies; Homophily; Link prediction; Maximum information path; Social media; Social Networks; Topical similarity; User interfaces
Editorial,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863607452&doi=10.1145%2f2180861.2180862&partnerID=40&md5=31b75b7d5233a4b926ad6307f95f7371,[No abstract available],,
Modellus: Automated modeling of complex internet data center applications,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863616687&doi=10.1145%2f2180861.2180865&partnerID=40&md5=1eec4ede7d0e22b6d6d3a6c671c00609,"The rising complexity of distributed server applications in Internet data centers has made the tasks of modeling and analyzing their behavior increasingly difficult. This article presents Modellus, a novel system for automated modeling of complex web-based data center applications using methods from queuing theory, data mining, and machine learning. Modellus uses queuing theory and statistical methods to automatically derive models to predict the resource usage of an application and the workload it triggers; these models can be composed to capture multiple dependencies between interacting applications. Model accuracy is maintained by fast, distributed testing, automated relearning of models when they change, and methods to bound prediction errors in composite models. We have implemented a prototype of Modellus, deployed it on a data center testbed, and evaluated its efficacy for modeling and analysis of several distributed multitier web applications. Our results show that this feature-based modeling technique is able to make predictions across several data center tiers, and maintain predictive accuracy (typically 95% or better) in the face of significant shifts in workload composition; we also demonstrate practical applications of the Modellus system to prediction and provisioning of real-world data center applications. © 2012 ACM.",Internet applications; Workload and performance modeling,Automation; Forecasting; Queueing theory; Automated modeling; Bound prediction; Composite models; Data centers; Distributed servers; Distributed testing; Feature based modeling; Internet application; Internet data centers; Model accuracy; Modeling and analysis; Multi-tier; Multiple dependencies; Performance Modeling; Predictive accuracy; Queuing theory; Real world data; Resource usage; WEB application; Internet
A hybrid approach for efficient web service composition with end-to-end QoS constraints,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863617430&doi=10.1145%2f2180861.2180864&partnerID=40&md5=b5cd36312f7d54c5ebce27ca4f49cff2,"Dynamic selection of Web services at runtime is important for building flexible and loosely-coupled serviceoriented applications. An abstract description of the required services is provided at design-time, and matching service offers are located at runtime. With the growing number of Web services that provide the same functionality but differ in quality parameters (e.g., availability, response time), a decision needs to be made on which services should be selected such that the user's end-to-end QoS requirements are satisfied. Although very efficient, local selection strategy fails short in handling global QoS requirements. Solutions based on global optimization, on the other hand, can handle global constraints, but their poor performance renders them inappropriate for applications with dynamic and realtime requirements. In this article we address this problem and propose a hybrid solution that combines global optimization with local selection techniques to benefit from the advantages of both worlds. The proposed solution consists of two steps: first, we use mixed integer programming (MIP) to find the optimal decomposition of global QoS constraints into local constraints. Second, we use distributed local selection to find the best Web services that satisfy these local constraints. The results of experimental evaluation indicate that our approach significantly outperforms existing solutions in terms of computation time while achieving close-to-optimal results. © 2012 ACM.",Optimization; QoS; Service composition; Web services,Abstracting; Global optimization; Optimization; Web services; Websites; Computation time; Dynamic selection; End-to-end QoS; Experimental evaluation; Global constraints; Hybrid approach; Hybrid solution; Local constraints; Mixed integer programming (MIP); Optimal decomposition; Poor performance; QoS constraints; QoS requirements; Quality parameters; Real time requirement; Runtimes; Selection techniques; Service compositions; Service Oriented; Web service composition; Quality of service
Crawling AJAX-based web applications through dynamic analysis of user interface state changes,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859409079&doi=10.1145%2f2109205.2109208&partnerID=40&md5=30ece2ad473c2d92c90ecd5d0e2b8fa3,"Using JAVASCRIPT and dynamic DOM manipulation on the client side of Web applications is becoming a widespread approach for achieving rich interactivity and responsiveness in modern Web applications. At the same time, such techniques-collectively known as AJAX-shatter the concept of webpages with unique URLs, on which traditional Web crawlers are based. This article describes a novel technique for crawling AJAX-based applications through automatic dynamic analysis of user-interface-state changes in Web browsers. Our algorithm scans the DOM tree, spots candidate elements that are capable of changing the state, fires events on those candidate elements, and incrementally infers a state machine that models the various navigational paths and states within an AJAX application. This inferred model can be used in program comprehension and in analysis and testing of dynamic Web states, for instance, or for generating a static version of the application. In this article, we discuss our sequential and concurrent AJAX crawling algorithms. We present our open source tool called CRAWLJAX, which implements the concepts and algorithms discussed in this article. Additionally, we report a number of empirical studies in which we apply our approach to a number of open-source and industrialWeb applications and elaborate on the obtained results. © 2012 ACM 1559-1131/2012/03-ART3 $10.00.",Algorithms; Design; Experimentation,Algorithms; Design; User interfaces; World Wide Web; Analysis and testing; Automatic dynamic analysis; DOM tree; Empirical studies; Experimentation; Interactivity; Javascript; Navigational paths; Novel techniques; Open source tools; Open-source; Program comprehension; State machine; WEB application; Web crawlers; Java programming language
ClickRank: Learning session-context models to enrich web search ranking,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859395361&doi=10.1145%2f2109205.2109206&partnerID=40&md5=db4021bb421fe491e57506c33789b01d,"User browsing information, particularly non-search-related activity, reveals important contextual information on the preferences and intents of Web users. In this article, we demonstrate the importance of mining general Web user behavior data to improve ranking and other Web-search experience, with an emphasis on analyzing individual user sessions for creating aggregate models. In this context, we introduce Click- Rank, an efficient, scalable algorithm for estimating Webpage and Website importance from general Web user-behavior data. We lay out the theoretical foundation of ClickRank based on an intentional surfer model and discuss its properties. We quantitatively evaluate its effectiveness regarding the problem of Web-search ranking, showing that it contributes significantly to retrieval performance as a novel Web-search feature. We demonstrate that the results produced by ClickRank for Web-search ranking are highly competitive with those produced by other approaches, yet achieved at better scalability and substantially lower computational costs. Finally, we discuss novel applications of ClickRank in providing enriched user Web-search experience, highlighting the usefulness of our approach for nonranking tasks. © 2012 ACM 1559-1131/2012/03-ART1 $10.00.",Algorithms; Experimentation; Theory,Algorithms; Websites; Aggregate model; Computational costs; Contextual information; Experimentation; Lay-out; Novel applications; Retrieval performance; Scalable algorithms; Theoretical foundations; Theory; User sessions; Web search rankings; Web user behaviors; Web users; Web-page; Information retrieval
Identifying web spam with the wisdom of the crowds,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859400554&doi=10.1145%2f2109205.2109207&partnerID=40&md5=7bef64e5231ab374ec49487b41dc1c6c,"Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam-detection techniques are usually designed for specific, known types ofWeb spam and are incapable of dealing with newly appearing spam types efficiently. With user-behavior analyses from Web access logs, a spam page-detection algorithm is proposed based on a learning scheme. The main contributions are the following. (1) User-visiting patterns of spam pages are studied, and a number of user-behavior features are proposed for separating Web spam pages from ordinary pages. (2) A novel spam-detection framework is proposed that can detect various kinds of Web spam, including newly appearing ones, with the help of the user-behavior analysis. Experiments on large-scale practical Web access log data show the effectiveness of the proposed features and the detection framework. © 2012 ACM 1559-1131/2012/03-ART2 $10.00.",Experimentation; Human Factors; Measurement,Experiments; Human engineering; Measurements; Search engines; World Wide Web; Detection framework; Experimentation; Learning schemes; Web access; Web access logs; Behavioral research
Quality and leniency in online collaborative rating systems,2012,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859412794&doi=10.1145%2f2109205.2109209&partnerID=40&md5=213d4fcc373a3454ec1a06fd2908eb60,"The emerging trend of social information processing has resulted in Web users' increased reliance on usergenerated content contributed by others for information searching and decision making. Rating scores, a form of user-generated content contributed by reviewers in online rating systems, allow users to leverage others' opinions in the evaluation of objects. In this article, we focus on the problem of summarizing the rating scores given to an object into an overall score that reflects the object's quality. We observe that the existing approaches for summarizing scores largely ignores the effect of reviewers exercising different standards in assigning scores. Instead of treating all reviewers as equals, our approach models the leniency of reviewers, which refers to the tendency of a reviewer to assign higher scores than other coreviewers. Our approach is underlined by two insights: (1) The leniency of a reviewer depends not only on how the reviewer rates objects, but also on how other reviewers rate those objects and (2) The leniency of a reviewer and the quality of rated objects are mutually dependent. We develop the leniency-aware quality, or LQ model, which solves leniency and quality simultaneously. We introduce both an exact and a ranked solution to the model. Experiments on real-life and synthetic datasets show that LQ is more effective than comparable approaches. LQ is also shown to perform consistently better under different parameter settings. © 2012 ACM 1559-1131/2012/03-ART4 $10.00.",Algorithms; Experimentation; Human Factors,Algorithms; Data processing; Experiments; Human engineering; Online systems; Emerging trends; Experimentation; Information searching; Online rating systems; Parameter setting; Rating system; Social information processing; Synthetic datasets; User-generated content; Web users; Rating
Efficient search engine measurements,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455127332&doi=10.1145%2f2019643.2019645&partnerID=40&md5=8f1c29396f918990cc6f8d10877e3584,"We address the problem of externally measuring aggregate functions over documents indexed by search engines, like corpus size, index freshness, and density of duplicates in the corpus. State of the art estimators for such quantities [Bar-Yossef and Gurevich 2008b; Broder et al. 2006] are biased due to inaccurate approximation of the so called ""document degrees"". In addition, the estimators in Bar-Yossef and Gurevich [2008b] are quite costly, due to their reliance on rejection sampling. We present new estimators that are able to overcome the bias introduced by approximate degrees. Our estimators are based on a careful implementation of an approximate importance sampling procedure. Comprehensive theoretical and empirical analysis of the estimators demonstrates that they have essentially no bias even in situations where document degrees are poorly approximated. By avoiding the costly rejection sampling approach, our new importance sampling estimators are significantly more efficient than the estimators proposed in Bar-Yossef and Gurevich [2008b]. Furthermore, building on an idea from Broder et al. [2006], we discuss Rao-Blackwellization as a generic method for reducing variance in search engine estimators. We show that Rao-Blackwellizing our estimators results in performance improvements, without compromising accuracy. © 2011 ACM 1559-1131/2011/10-ART18 $10.00.",Corpus size estimation; Evaluation; Sampling; Search engines,Information retrieval; Search engines; Aggregate function; Corpus size; Empirical analysis; Evaluation; Generic method; Importance sampling; Index freshness; Performance improvements; Rao-Blackwellization; State of the art; Estimation
A practical architecture for an anycast CDN,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455168630&doi=10.1145%2f2019643.2019644&partnerID=40&md5=56569f9baa942d8c65e16d344671e9e6,"IP Anycast has many attractive features for any service that involve the replication of multiple instances across the Internet. IP Anycast allows multiple instances of the same service to be ""naturally"" discovered, and requests for this service to be delivered to the closest instance. However, while briefly considered as an enabler for content delivery networks (CDNs) when they first emerged, IP Anycast was deemed infeasible in that environment. The main reasons for this decision were the lack of load awareness of IP Anycast and unwanted side effects of Internet routing changes on the IP Anycast mechanism. In this article we re-evaluate IP Anycast for CDNs by proposing a load-aware IP Anycast CDN architecture. Our architecture is prompted by recent developments in route control technology, as well as better understanding of the behavior of IP Anycast in operational settings. Our architecture makes use of route control mechanisms to take server and network load into account to realize load-aware Anycast. We show that the resulting redirection requirements can be formulated as a Generalized Assignment Problem and present practical algorithms that address these requirements while at the same time limiting connection disruptions that plague regular IP Anycast. We evaluate our algorithms through trace based simulation using traces obtained from a production CDN network. © 2011 ACM 1559-1131/2011/10-ART17 $10.00.",Anycast; Autonomous system; Content delivery networks; Load balancing; Routing,Algorithms; Computer simulation; Computer supported cooperative work; Internet; Internet protocols; Telecommunication networks; Anycast; Autonomous systems; Content delivery networks; Load-Balancing; Routing; Network architecture
Camera brand congruence and camera model propagation in the flickr social graph,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455127323&doi=10.1145%2f2019643.2019647&partnerID=40&md5=073d3566b53fa4d066433939e0d0092c,"Given that my friends on Flickr use cameras of brand X, am I more likely to also use a camera of brand X? Given that one of these friends changes her brand, am I likely to do the same? Do new camera models pop up uniformly in the friendship graph? Or do early adopters then ""convert"" their friends? Which factors influence the conversion probability of a user? These are the kind of questions addressed in this work. Direct applications involve personalized advertising in social networks. For our study, we crawled a complete connected component of the Flickr friendship graph with a total of 67M edges and 3.9M users. 1.2M of these users had at least one public photograph with valid model metadata, which allowed us to assign camera brands and models to users and time slots. Similarly, we used, where provided in a user's profile, information about a user's geographic location and the groups joined on Flickr. Concerning brand congruence, our main findings are the following. First, a pair of friends on Flickr has a higher probability of being congruent, that is, using the same brand, compared to two random users (27% vs. 19%). Second, the degree of congruence goes up for pairs of friends (i) in the same country (29%), (ii) who both only have very few friends (30%), and (iii) with a very high cliqueness (38%). Third, given that a user changes her camera model between March-May 2007 and March-May 2008, high cliqueness friends are more likely than random users to do the same (54% vs. 48%). Fourth, users using high-end cameras are far more loyal to their brand than users using point-and-shoot cameras, with a probability of staying with the same brand of 60% vs 33%, given that a new camera is bought. Fifth, these ""expert"" users' brand congruence reaches 66% for high cliqueness friends. All these differences are statistically significant at 1%. As for the propagation of new models in the friendship graph, we observe the following. First, the growth of connected components of users converted to a particular, new camera model differs distinctly from random growth. Second, the decline of dissemination of a particularmodel is close to random decline. This illustrates that users influence their friends to change to a particular new model, rather than from a particular old model. Third, having many converted friends increases the probability of the user to convert herself. Here differences between friends from the same or from different countries are more pronounced for point-andshoot than for digital single-lens reflex users. Fourth, there was again a distinct difference between arbitrary friends and high cliqueness friends in terms of prediction quality for conversion. © 2011 ACM 1559-1131/2011/10-ART20 $10.00.",Brand congruence; Brand loyalty; Flickr; Social networks,Metadata; Photography; Probability; Social networking (online); Brand congruence; Brand loyalty; Brand X; Camera model; Connected component; Conversion probability; Flickr; Friendship graphs; Geographic location; New model; Prediction quality; Random growth; Random users; Social graphs; Social Networks; Time slots; Cameras
"Characterizing organizational use of web-based services: Methodology, challenges, observations, and insights",2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053352734&doi=10.1145%2f2019643.2019646&partnerID=40&md5=0d213dd83a329ea00790c6febe72adf6,"Today's Web provides many different functionalities, including communication, entertainment, social networking, and information retrieval. In this article, we analyze traces of HTTP activity from a large enterprise and from a large university to identify and characterize Web-based service usage. Our work provides an initial methodology for the analysis of Web-based services. While it is nontrivial to identify the classes, instances, and providers for each transaction, our results show that most of the traffic comes from a small subset of providers, which can be classified manually. Furthermore, we assess both qualitatively and quantitatively how the Web has evolved over the past decade, and discuss the implications of these changes. © 2011 ACM 1559-1131/2011/10-ART19 $10.00.",Organizational use; Web-based services; Workload characterization,HTTP; Information retrieval; User interfaces; Websites; Organizational use; Web-based service; Workload characterization; Web services
ACCONV - An access control model for conversational Web services,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051928978&doi=10.1145%2f1993053.1993055&partnerID=40&md5=8100f23d5979757f7d5af6872ca48728,"With organizations increasingly depending on Web services to build complex applications, security and privacy concerns including the protection of access control policies are becoming a serious issue. Ideally, service providers would like to make sure that clients have knowledge of only portions of the access control policy relevant to their interactions to the extent to which they are entrusted by the Web service and without restricting the client's choices in terms of which operations to execute. We propose ACCONV, a novel model for access control in Web services that is suitable when interactions between the client and the Web service are conversational and long-running. The conversation-based access control model proposed in this article allows service providers to limit how much knowledge clients have about the credentials specified in their access policies. This is achieved while reducing the number of times credentials are asked from clients and minimizing the risk that clients drop out of a conversation with the Web service before reaching a final state due to the lack of necessary credentials. Clients are requested to provide credentials, and hence are entrusted with part of the Web service access control policies, only for some specific granted conversations which are decided based on: (1) a level of trust that the Web service provider has vis- à-vis the client, (2) the operation that the client is about to invoke, and (3) meaningful conversations which represent conversations that lead to a final state from the current one. We have implemented the proposed approach in a software prototype and conducted extensive experiments to show its effectiveness. © 2011 ACM.",Access control; Conversations; Web services,Access control; Security systems; Software prototyping; User interfaces; Access control models; Access control policies; Access policies; Complex applications; Conversations; Drop-out; Final state; Security and privacy; Service access; Service provider; Software prototypes; Web service providers; Web services
On computing deltas of RDF/S knowledge bases,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051932644&doi=10.1145%2f1993053.1993056&partnerID=40&md5=3d11b9356dc2c180789f6ff9e3be0c80,"The ability to compute the differences that exist between two RDF/S Knowledge Bases (KB) is an important step to cope with the evolving nature of the SemanticWeb (SW). In particular, RDF/S deltas can be employed to reduce the amount of data that need to be exchanged and managed over the network in order to build SW synchronization and versioning services. By considering deltas as sets of change operations, in this article we introduce various RDF/S differential functions which take into account inferred knowledge from an RDF/S knowledge base. We first study their correctness in transforming a source to a target RDF/S knowledge base in conjunction with the semantics of the employed change operations (i.e., with or without side-effects on inferred knowledge). Then we formally analyze desired properties of RDF/S deltas such as size minimality, semantic identity, redundancy elimination, reversibility, and composability, as well as identify those RDF/S differential functions that satisfy them. Subsequently, we experimentally evaluate the computing time and size of the produced deltas over real and synthetic RDF/S knowledge bases. © 2011 ACM.",Delta; RDF; Semantic Web,Knowledge based systems; Semantic Web; Semantics; User interfaces; Composability; Computing time; Delta; Differential functions; Knowledge base; Knowledge basis; Minimality; RDF; Redundancy elimination; Side effect; Versioning; Delta functions
A comprehensive study of features and algorithms for URL-based topic classification,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051944589&doi=10.1145%2f1993053.1993057&partnerID=40&md5=8940bee5904790a1451ec8ce5af8c3e6,"Given only the URL of a Web page, can we identify its topic? We study this problem in detail by exploring a large number of different feature sets and algorithms on several datasets. We also show that the inherent overlap between topics and the sparsity of the information in URLs makes this a very challenging problem. Web page classification without a page's content is desirable when the content is not available at all, when a classification is needed before obtaining the content, or when classification speed is of utmost importance. For our experiments we used five different corpora comprising a total of about 3 million (URL, classification) pairs. We evaluated several techniques for feature generation and classification algorithms. The individual binary classifiers were then combined via boosting intometabinary classifiers. We achieve typical F-measure values between 80 and 85, and a typical precision of around 86. The precision can be pushed further over 90 while maintaining a typical level of recall between 30 and 40. © 2011 ACM.",ODP; Topic classification; URL,User interfaces; World Wide Web; Binary classifiers; Classification algorithm; Comprehensive studies; Data sets; F-measure; Feature generation; Feature sets; ODP; Topic classification; URL; Web page; Web page classification; Algorithms
A clustering-driven LDAP framework,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051943409&doi=10.1145%2f1993053.1993054&partnerID=40&md5=1e79edc6deaa9dadcc77c75367dcd378,"LDAP directories have proliferated as the appropriate storage framework for various and heterogeneous data sources, operating under a wide range of applications and services. Due to the increased amount and heterogeneity of the LDAP data, there is a requirement for appropriate data organization schemes. The LPAIR & LMERGE (LP-LM) algorithm, presented in this article, is a hierarchical agglomerative structurebased clustering algorithm which can be used for the LDAP directory information tree definition. A thorough study of the algorithm's performance is provided, which designates its efficiency. Moreover, the Relative Link as an alternative merging criterion is proposed, since as indicated by the experimentation, it can result in more balanced clusters. Finally, the LP and LM Query Engine is presented, which considering the clustering-based LDAP data organization, results in the enhancement of the LDAP server's performance. © 2011 ACM.",Clustering; DIT organization; LDAP services; Merging criteria; Query and retrieval engine,Lagrange multipliers; Merging; Search engines; Trees (mathematics); Clustering; Data organization; Directory information trees; DIT organization; Heterogeneous data sources; Its efficiencies; LDAP servers; LDAP services; Query engines; Retrieval engines; Structure-based; Clustering algorithms
Building mashups by demonstration,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051915843&doi=10.1145%2f1993053.1993058&partnerID=40&md5=1fb87837f439751c67893f67d26babc1,"The latest generation of WWW tools and services enables Web users to generate applications that combine content from multiple sources. This type of Web application is referred to as a mashup. Many of the tools for constructing mashups rely on a widget paradigm, where users must select, customize, and connect widgets to build the desired application. While this approach does not require programming, the users must still understand programming concepts to successfully create a mashup. As a result, they are put off by the time, effort, and expertise needed to build a mashup. In this article, we describe our programming-bydemonstration approach to building mashup by example. Instead of requiring a user to select and customize a set of widgets, the user simply demonstrates the integration task by example. Our approach addresses the problems of extracting data from Web sources, cleaning and modeling the extracted data, and integrating the data across sources. We implemented these ideas in a system called Karma, and evaluated Karma on a set of 23 users. The results show that, compared to other mashup construction tools, Karma allows more of the users to successfully build mashups and makes it possible to build these mashups significantly faster compared to using a widget-based approach. © 2011 ACM.",Information integration; Mashups; Programming by demonstration,World Wide Web; Information integration; Mashups; Multiple source; Programming by demonstration; Programming concepts; WEB application; Web sources; Web users; User interfaces
Topic distillation with query-dependent link connections and page characteristics,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052046390&doi=10.1145%2f1961659.1961660&partnerID=40&md5=d8074ad2670de2d1244fdbf6c10976d1,"Searchers on the Web often aim to find key resources about a topic. Finding such results is called topic distillation. Previous research has shown that the use of sources of evidence such as page indegree and URL structure can significantly improve search performance on interconnected collections such as the Web, beyond the use of simple term distribution statistics. This article presents a new approach to improve topic distillation by exploring the use of external sources of evidence: link structure, including query dependent indegree and outdegree; and web page characteristics, such as the density of anchor links. Our experiments with the TREC .GOV collection, an 18GB crawl of the US .gov domain from 2002, show that using such evidence can significantly improve search effectiveness, with combinations of evidence leading to significant performance gains over both full-text and anchor-text baselines. Moreover, we demonstrate that, at a different scope level, both local query-dependent outdegree and query-dependent indegree out-performed their global query-independent counterparts; and at the same scope level, outdegree out-performed indegree. Adding query-dependent indegree or page characteristics to query-dependent outdegree could have a small, but not significant, improvement. © 2011 ACM.",Citation and link analysis; Connectivity; Indegree; Outdegree; PageRank; Topic distillation; Web information retrieval,Distillation; User interfaces; Websites; Connectivity; In-Degree; Link analysis; Outdegree; PageRank; Topic distillation; Web information retrieval; Information retrieval
Designing and implementing the OP and OP2 web browsers,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052072472&doi=10.1145%2f1961659.1961665&partnerID=40&md5=592428561b181d34e988300a22c94a6b,"Current web browsers are plagued with vulnerabilities, providing hackers with easy access to computer systems via browser-based attacks. Browser security efforts that retrofit existing browsers have had limited success because the design of modern browsers is fundamentally flawed. To enable more secure web browsing, we design and implement a new browser, called the OP web browser, that attempts to improve the state-of-the-art in browser security. We combine operating system design principles with formal methods to design a more secure web browser by drawing on the expertise of both communities. Our design philosophy is to partition the browser into smaller subsystems and make all communication between subsystems simple and explicit. At the core of our design is a small browser kernel that manages the browser subsystems and interposes on all communications between them to enforce our new browser security features. To show the utility of our browser architecture, we design and implement three novel security features. First, we develop flexible security policies that allow us to include browser plugins within our security framework. Second, we use formal methods to prove useful security properties including user interface invariants and browser security policy. Third, we design and implement a browser-level information-flow tracking system to enable post-mortem analysis of browser-based attacks. In addition to presenting the OP browser architecture, we discuss the design and implementation of a second version of OP, OP2, that includes features from other secure web browser designs to improve on the overall security and performance of OP. To evaluate our design, we implemented OP2 and tested both performance, memory, and filesystem impact while browsing popular pages. We show that the additional security features in OP and OP2 introduce minimal overhead. © 2011 ACM.",Browser plugin; Formal verification; OP browser; Security; Web browsing,Computer crime; Design; Formal methods; Network security; Personal computing; Retrofitting; Security systems; Systems analysis; User interfaces; World Wide Web; Formal verifications; OP browser; Plug-ins; Security; Web browsing; Web browsers
Characterizing Web-based video sharing workloads,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052058388&doi=10.1145%2f1961659.1961662&partnerID=40&md5=ea87c33668d4f899d460fe30a6bb266f,"Video sharing services that allow ordinary Web users to upload video clips of their choice and watch video clips uploaded by others have recently become very popular. This article identifies invariants in video sharing workloads, through comparison of the workload characteristics of four popular video sharing services. Our traces contain metadata on approximately 1.8 million videos which together have been viewed approximately 6 billion times. Using these traces, we study the similarities and differences in use of several Web 2.0 features such as ratings, comments, favorites, and propensity of uploading content. In general, we find that active contribution, such as video uploading and rating of videos, is much less prevalent than passive use. While uploaders in general are skewed with respect to the number of videos they upload, the fraction of multi-time uploaders is found to differ by a factor of two between two of the sites. The distributions of lifetime measures of video popularity are found to have heavy-tailed forms that are similar across the four sites. Finally, we consider implications for system design of the identified invariants. To gain further insight into caching in video sharing systems, and the relevance to caching of lifetime popularity measures, we gathered an additional dataset tracking views to a set of approximately 1.3 million videos from one of the services, over a twelve-week period. We find that lifetime popularity measures have some relevance for large cache (hot set) sizes (i.e., a hot set defined according to one of these measures is indeed relatively ""hot""), but that this relevance substantially decreases as cache size decreases, owing to churn in video popularity. © 2011 ACM.",Power law; Social interaction; User-generated content; Video sharing; Workload characterization,Metadata; Systems analysis; Video cameras; World Wide Web; Power law; Social interactions; User-generated content; Video sharing; Workload characterization; User interfaces
Cost-aware strategies for query result caching in Web search engines,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052055790&doi=10.1145%2f1961659.1961663&partnerID=40&md5=8f4a3c58ff4635109b3da616ac724f00,"Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time. © 2011 ACM.",Query result caching; Web search engines,Costs; Information retrieval; Search engines; User interfaces; Cache Miss; Caching policy; Data sets; Processing costs; Query costs; Query execution time; Query logs; Query results; Simulation result; Static and dynamic; Total costs; World Wide Web
A survey of requirements specification in model-driven development of web applications,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052053136&doi=10.1145%2f1961659.1961664&partnerID=40&md5=f7525e6ce12adf64719f19295576e35c,"Model-driven development has become more and more important in the last few years. In the context of web application development, many web Engineering methods that propose model-driven development processes have appeared. However, earlier stages of these processes are seldom considered and few of these methods rigorously face the problems of specifying web application requirements and translating them into the proper conceptual model. However, it is widely recognized that requirements engineering activities are essential to obtain quality software products. This article surveys Model-driven web engineering methods in a comparative study and analyzes the techniques proposed for specifying functional, data and navigational requirements as well as the mechanisms provided for automatically translating these requirements into conceptual models. Our main goal is to provide a critical view of the support that is provided by these methods for handling web application requirements in order to show their current limitations and strengths. © 2011 ACM.",Model-driven development; Requirements engineering; Survey; Web applications; Web engineering,Models; Requirements engineering; Surveys; World Wide Web; Comparative studies; Conceptual model; Current limitation; Model driven development; Model-driven; Quality software; Requirements specifications; WEB application; Web application development; Web engineering; User interfaces
Host-based P2P flow identification and use in real-time,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052074576&doi=10.1145%2f1961659.1961661&partnerID=40&md5=35dbf2ba67b4e95e8a0a28786db58400,"Data identification and classification is a key task for any Internet Service Provider (ISP) or network administrator. As port fluctuation and encryption become more common in P2P applications wishing to avoid identification, new strategies must be developed to detect and classify their flows. This article introduces a method of separating P2P and standard web traffic that can be applied as part of an offline data analysis process, based on the activity of the hosts on the network. Heuristics are analyzed and a classification system proposed that focuses on classifying those ""long"" flows that transfer most of the bytes across a network. The accuracy of the system is then tested using real network traffic from a core Internet router showing misclassification rates as low as 0.54% of flows in some cases. We expand on this proposed strategy to investigate its relevance to real-time, early classification problems. New proposals are made and the results of real-time experiments are compared to those obtained in the offline analysis. It is shown that classification accuracies in the real-time strategy are similar to those achieved in offline analysis with a large portion of the total web and P2P flows correctly identified. © 2011 ACM.",Classification; Host based; P2P,Data reduction; Internet service providers; Telecommunication networks; User interfaces; World Wide Web; Classification accuracy; Classification system; Core Internet; Data identification; Host-based; Misclassification rates; New strategy; Off-line analysis; Offline data; OR-networks; P2P; P2P applications; Real networks; Real time strategies; Real-time experiment; Web traffic; Peer to peer networks
Recommending friends and locations based on individual location history,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650617680&doi=10.1145%2f1921591.1921596&partnerID=40&md5=3a61344f707eefbc177d8e4c27d3db35,"The increasing availability of location-acquisition technologies (GPS, GSM networks, etc.) enables people to log the location histories with spatio-temporal data. Such real-world location histories imply, to some extent, users' interests in places, and bring us opportunities to understand the correlation between users and locations. In this article, we move towards this direction and report on a personalized friend and location recommender for the geographical information systems (GIS) on theWeb. First, in this recommender system, a particular individual's visits to a geospatial region in the real world are used as their implicit ratings on that region. Second, we measure the similarity between users in terms of their location histories and recommend to each user a group of potential friends in a GIS community. Third, we estimate an individual's interests in a set of unvisited regions by involving his/her location history and those of other users. Some unvisited locations that might match their tastes can be recommended to the individual. A framework, referred to as a hierarchical-graph-based similarity measurement (HGSM), is proposed to uniformly model each individual's location history, and effectively measure the similarity among users. In this framework, we take into account three factors: 1) the sequence property of people's outdoor movements, 2) the visited popularity of a geospatial region, and 3) the hierarchical property of geographic spaces. Further, we incorporated a content-based method into a user-based collaborative filtering algorithm, which uses HGSM as the user similarity measure, to estimate the rating of a user on an item. We evaluated this recommender system based on the GPS data collected by 75 subjects over a period of 1 year in the real world. As a result, HGSM outperforms related similarity measures, namely similarity-by-count, cosine similarity, and Pearson similarity measures. Moreover, beyond the item-based CF method and random recommendations, our system provides users with more attractive locations and better user experiences of recommendation. © 2011 ACM.",Collaborative filtering; GeoLife; GPS trajectories; Location history; Recommender system; Spatio-temporal data mining; User similarity,Data mining; Geographic information systems; Signal filtering and prediction; Collaborative filtering; GeoLife; GPS trajectories; Spatio-temporal data mining; User similarity; Recommender systems
Introduction to special issue on recommender systems,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951859636&doi=10.1145%2f1921591.1921592&partnerID=40&md5=96bf19b124b006a18ca3ba0ba802e433,[No abstract available],,
A specialized search assistant for learning objects,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455130030&doi=10.1145%2f2019643.2019648&partnerID=40&md5=ec9b9616f25d611e93c8bd3d4f0d64fc,"The Web holds a great quantity of material that can be used to enhance classroom instruction. However, it is not easy to retrieve this material with the search engines currently available. This study produced a specialized search assistant based on Google that significantly increases the number of instances in which teachers find the desired learning objects as compared to using this popular public search engine directly. Success in finding learning objects by study participants went from 80% using Google alone to 96% when using our search assistant in one scenario and, in another scenario, from a 40% success rate with Google alone to 66% with our assistant. This specialized search assistant implements features such as bilingual search and term suggestion which were requested by teacher participants to help improve their searches. Study participants evaluated the specialized search assistant and found it significantly easier to use and more useful than the popular search engine for the purpose of finding learning objects. © 2011 ACM 1559-1131/2011/10-ART21 $10.00.",Bilingual search; Search assistant; Web search interfaces; WWW,Search engines; World Wide Web; Bilingual search; Classroom instruction; Learning objects; Search assistant; Term suggestion; Web search interfaces; Teaching
Using external aggregate ratings for improving individual recommendations,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951873230&doi=10.1145%2f1921591.1921594&partnerID=40&md5=f65408548a48e9c3c351bcd14c8ce03b,"This article describes an approach for incorporating externally specified aggregate ratings information into certain types of recommender systems, including two types of collaborating filtering and a hierarchical linear regression model. First, we present a framework for incorporating aggregate rating information and apply this framework to the aforementioned individual rating models. Then we formally show that this additional aggregate rating information provides more accurate recommendations of individual items to individual users. Further, we experimentally confirm this theoretical finding by demonstrating on several datasets that the aggregate rating information indeed leads to better predictions of unknown ratings. We also propose scalable methods for incorporating this aggregate information and test our approaches on large datasets. Finally, we demonstrate that the aggregate rating information can also be used as a solution to the cold start problem of recommender systems. © 2011 ACM.",Aggregate ratings; Cold-start problem; Collaborative filtering; Hierarchical linearmodels; Predictive models; Recommender systems,Aggregates; Distributed computer systems; Hierarchical systems; Linear regression; Predictive control systems; Rating; Scalability; Signal filtering and prediction; Cold start problems; Collaborative filtering; Data sets; Hierarchical linearmodels; Large datasets; Linear regression models; Predictive models; Rating information; Rating model; Scalable methods; Recommender systems
Automatic tag recommendation algorithms for social recommender systems,2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951867458&doi=10.1145%2f1921591.1921595&partnerID=40&md5=1de648653607870d6e0373fc9ef93bbf,"The emergence of Web 2.0 and the consequent success of social network Web sites such as Del.icio.us and Flickr introduce us to a new concept called social bookmarking, or tagging. Tagging is the action of connecting a relevant user-defined keyword to a document, image, or video, which helps the user to better organize and share their collections of interesting stuff. With the rapid growth of Web 2.0, tagged data is becoming more and more abundant on the social network Web sites. An interesting problem is how to automate the process of making tag recommendations to users when a new resource becomes available. In this article, we address the issue of tag recommendation from a machine learning perspective. From our empirical observation of two large-scale datasets, we first argue that the user-centered approach for tag recommendation is not very effective in practice. Consequently, we propose two novel document-centered approaches that are capable ofmaking effective and efficient tag recommendations in real scenarios. The first, graph-based, method represents the tagged data in two bipartite graphs, (document, tag) and (document, word), then finds document topics by leveraging graph partitioning algorithms. The second, prototypebased, method aims at finding the most representative documents within the data collections and advocates a sparse multiclass Gaussian process classifier for efficient document classification. For both methods, tags are ranked within each topic cluster/class by a novel ranking method. Recommendations are performed by first classifying a new document into one or more topic clusters/classes, and then selecting the most relevant tags from those clusters/classes as machine-recommended tags. Experiments on real-world data from Del.icio.us, CiteULike, and BibSonomy examine the quality of tag recommendation as well as the efficiency of our recommendation algorithms. The results suggest that our document-centered models can substantially improve the performance of tag recommendations when compared to the user-centered methods, as well as topic models LDA and SVM classifiers. © 2011 ACM.",Gaussian processes; Graph partitioning; Mixture model; Multi-label classification; Prototype selection; Tagging system,Algorithms; Classifiers; Gaussian distribution; Gaussian noise (electronic); Graph theory; Models; Recommender systems; Thesauri; User interfaces; World Wide Web; Gaussian Processes; Graph Partitioning; Mixture model; Multi-label; Prototype selection; Tagging system; Information retrieval systems
"Comparison of collaborative filtering algorithms: Limitations of current techniques and proposals for scalable, high-performance recommender systems",2011,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951865393&doi=10.1145%2f1921591.1921593&partnerID=40&md5=7fc564cd293207c8006d5044089b8049,"The technique of collaborative filtering is especially successful in generating personalized recommendations. More than a decade of research has resulted in numerous algorithms, although no comparison of the different strategies has been made. In fact, a universally accepted way of evaluating a collaborative filtering algorithm does not exist yet. In this work, we compare different techniques found in the literature, and we study the characteristics of each one, highlighting their principal strengths and weaknesses. Several experiments have been performed, using the most popular metrics and algorithms. Moreover, two new metrics designed to measure the precision on good items have been proposed. The results have revealed the weaknesses of many algorithms in extracting information from user profiles especially under sparsity conditions. We have also confirmed the good results of SVD-based techniques already reported by other authors. As an alternative, we present a new approach based on the interpretation of the tendencies or differences between users and items. Despite its extraordinary simplicity, in our experiments, it obtained noticeably better results than more complex algorithms. In fact, in the cases analyzed, its results are at least equivalent to those of the best approaches studied. Under sparsity conditions, there is more than a 20% improvement in accuracy over the traditional user-based algorithms, while maintaining over 90% coverage. Moreover, it is much more efficient computationally than any other algorithm, making it especially adequate for large amounts of data. © 2011 ACM.",Collaborative filtering; Recommender systems,Algorithms; Experiments; Signal filtering and prediction; Collaborative filtering; Collaborative filtering algorithms; Complex algorithms; Current techniques; Extracting information; Large amounts of data; New approaches; Other algorithms; Personalized recommendation; User profile; Recommender systems
Learning deterministic regular expressions for the inference of schemas from XML data,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958075803&doi=10.1145%2f1841909.1841911&partnerID=40&md5=11b4dbfebdeef08b98fd8e0f7809d894,"Inferring an appropriate DTD or XML Schema Definition (XSD) for a given collection of XML documents essentially reduces to learning deterministic regular expressions from sets of positive example words. Unfortunately, there is no algorithm capable of learning the complete class of deterministic regular expressions from positive examples only, as we will show. The regular expressions occurring in practical DTDs and XSDs, however, are such that every alphabet symbol occurs only a small number of times. As such, in practice it suffices to learn the subclass of deterministic regular expressions in which each alphabet symbol occurs at most k times, for some small k. We refer to such expressions as k-occurrence regular expressions (k-OREs for short). Motivated by this observation, we provide a probabilistic algorithm that learns k-OREs for increasing values of k, and selects the deterministic one that best describes the sample based on a Minimum Description Length argument. The effectiveness of the method is empirically validated both on real world and synthetic data. Furthermore, the method is shown to be conservative over the simpler classes of expressions considered in previous work. © 2010 ACM.",Regular expressions; Schema inference; XML,Alphabet symbols; Minimum description length; Positive examples; Probabilistic algorithm; Regular expressions; Schema inference; Schemas; Synthetic data; XML data; Xml schema definitions; XML
Mining historic query trails to label long and rare search engine queries,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958031519&doi=10.1145%2f1841909.1841912&partnerID=40&md5=30d147d9361746dbb4c70097607751cc,"Web search engines can perform poorly for long queries (i.e., those containing four or more terms), in part because of their high level of query specificity. The automatic assignment of labels to long queries can capture aspects of a user's search intent that may not be apparent from the terms in the query. This affords search result matching or reranking based on queries and labels rather than the query text alone. Query labels can be derived from interaction logs generated from many users' search result clicks or from query trails comprising the chain of URLs visited following query submission. However, since long queries are typically rare, they are difficult to label in this way because little or no historic log data exists for them. A subset of these queries may be amenable to labeling by detecting similarities between parts of a long and rare query and the queries which appear in logs. In this article, we present the comparison of four similarity algorithms for the automatic assignment of Open Directory Project category labels to long and rare queries, based solely on matching against similar satisfied query trails extracted from log data. Our findings show that although the similarity-matching algorithms we investigated have tradeoffs in terms of coverage and accuracy, one algorithm that bases similarity on a popular search result ranking function (effectively regarding potentially-similar queries as ""documents"") outperforms the others. We find that it is possible to correctly predict the top label better than one in five times, even when no past query trail exactly matches the long and rare query. We show that these labels can be used to reorder top-ranked search results leading to a significant improvement in retrieval performance over baselines that do not utilize query labeling, but instead rank results using content-matching or click-through logs. The outcomes of our research have implications for search providers attempting to provide users with highly-relevant search results for long queries. © 2010 ACM.",Long queries; Query labeling,Algorithms; Information retrieval; Natural language processing systems; Search engines; Automatic assignment; Click-through; Log data; Long queries; Open directory projects; Query labeling; Query submission; Ranking functions; Re-ranking; Retrieval performance; Search results; Similarity algorithm; Similarity-matching; Web search engines; World Wide Web
Fast and compact web graph representations,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958035555&doi=10.1145%2f1841909.1841913&partnerID=40&md5=63bd9927c8d4dd2edbc3fbef7811e75a,"Compressed graph representations, in particular for Web graphs, have become an attractive research topic because of their applications in the manipulation of huge graphs in main memory. The state of the art is well represented by the WebGraph project, where advantage is taken of several particular properties of Web graphs to offer a trade-off between space and access time. In this paper we show that the same properties can be exploited with a different and elegant technique that builds on grammar-based compression. In particular, we focus on Re-Pair and on Ziv-Lempel compression, which, although cannot reach the best compression ratios of WebGraph, achievemuch faster navigation of the graph when both are tuned to use the same space. Moreover, the technique adapts well to run on secondary memory and in distributed scenarios. As a byproduct, we introduce an approximate Re-Pair version that works efficiently with severely limited main memory. © 2010 ACM.",Compression; Data structures; Web graph,Data structures; Access time; Compression; Compression ratios; Grammar-based compression; Graph representation; Main memory; Research topics; Secondary memories; State of the art; Web graphs; Data compression
AjaxScope: A platform for remotely monitoring the client-side behavior of Web 2.0 applications,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958044669&doi=10.1145%2f1841909.1841910&partnerID=40&md5=77f0f2e580556e507a0b16c8120b724e,"The rise of the software-as-a-service paradigm has led to the development of a new breed of sophisticated, interactive applications often called Web 2.0. While Web applications have become larger and more complex, Web application developers today have little visibility into the end-toend behavior of their systems. This article presents AjaxScope, a dynamic instrumentation platform that enables cross-user monitoring and just-in-time control of Web application behavior on end-user desktops. AjaxScope is a proxy that performs on-the-fly parsing and instrumentation of JavaScript code as it is sent to users' browsers. AjaxScope provides facilities for distributed and adaptive instrumentation in order to reduce the client-side overhead, while giving fine-grained visibility into the code-level behavior of Web applications. We present a variety of policies demonstrating the power of AjaxScope, ranging from simple error reporting and performance profiling to more complex memory leak detection and optimization analyses. We also apply our prototype to analyze the behavior of over 90 Web 2.0 applications and sites that use significant amounts of JavaScript. © 2010 ACM.",Applications; Software instrumentation; Software monitoring,Error detection; High level languages; Instruments; Visibility; Dynamic instrumentation; End users; Interactive applications; Javascript; Just-in-time control; On-the-fly; Optimization analysis; Software instrumentation; Software monitoring; Software-as-a-Service; Web 2.0; Web 2.0 applications; WEB application; World Wide Web
Relating reputation and money in online markets,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958069662&doi=10.1145%2f1841909.1841914&partnerID=40&md5=209dc43926b0f42fd7bb7b3d42a87e2f,"Reputation in online economic systems is typically quantified using counters that specify positive and negative feedback from past transactions and/or some form of transaction network analysis that aims to quantify the likelihood that a network user will commit a fraudulent transaction. These approaches can be deceiving to honest users from numerous perspectives. We take a radically different approach with the goal of guaranteeing to a buyer that a fraudulent seller cannot disappear from the system with profit following a set of fabricated transactions that total a certain monetary limit. Even in the case of stolen identity, such an adversary cannot produce illegal profit unless a buyer decides to pay over the suggested limit. © 2010 ACM.",Internet; Online markets; Reputation management; Social networks,Commerce; Electric network analysis; Feedback; Internet; Online systems; Profitability; Economic system; Fraudulent transactions; Negative feedback; Network analysis; Network users; Online markets; Reputation management; Social Networks; Social networking (online)
Exploring XML Web collections with DescribeX,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955011770&doi=10.1145%2f1806916.1806920&partnerID=40&md5=770155152c98695b7a193cc772d4d3e2,"As Web applications mature and evolve, the nature of the semistructured data that drives these applications also changes. An important trend is the need for increased flexibility in the structure of Web documents. Hence, applications cannot rely solely on schemas to provide the complex knowledge neededtovisualize, use, query and manage documents. Even when XMLWeb documents are valid with regard to a schema, the actual structure of such documents may exhibit significant variations across collections for several reasons: the schema may be very lax (e.g., RSS feeds), the schema may be large and different subsets of it may be used in different documents (e.g., industry standards like UBL), or open content models may allow arbitrary schemas to be mixed (e.g., RSS extensions like those used for podcasting). For these reasons, many applications that incorporate XPath queries to process a large Web document collection require an understanding of the actual structure present in the collection, and not just the schema. To support modern Web applications, we introduce DescribeX, a powerful framework that is capable of describing complex XML summaries of Web collections. DescribeX supports the construction of heterogenous summaries that can be declaratively defined and refined by means of axis path regular expression (AxPREs). AxPREs provide the flexibility necessary for declaratively defining complex mappings between instance nodes (in the documents) and summary nodes. These mappings are capable of expressing order and cardinality, among other properties, which can significantly help in the understanding of the structure of large collections of XML documents and enhance the performance of Web applications over these collections. DescribeX captures most summary proposals in the literature by providing (for the first time) a common declarative definition for them. Experimental results demonstrate the scalability of DescribeX summary operations (summary creation, as well as refinement and stabilization, two key enablers for tailoring summaries) on multi-gigabyte Web collections. © 2010 ACM.",Semistructured data; Structural summaries; XML; XPath,Digital storage; Mapping; Increased flexibility; Industry standards; Other properties; Regular expressions; Semi structured data; Structural summary; Web document collection; XPath; XML
Discovery of latent subcommunities in a blog's readership,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955004313&doi=10.1145%2f1806916.1806921&partnerID=40&md5=4945ee17616f1af777809622be1e9522,"The blogosphere has grown to be a mainstream forum of social interaction as well as a commercially attractive source of information and influence. Tools are needed to better understand how communities that adhere to individual blogs are constituted in order to facilitate new personal, socially-focused browsing paradigms, and understand how blog content is consumed, which is of interest to blog authors, big media, and search. We present a novel approach to blog subcommunity characterization by modeling individual blog readers using mixtures of an extension to the LDA family that jointly models phrases and time, Ngram Topic over Time (NTOT), and cluster with a number of similarity measures using Affinity Propagation. We experiment with two datasets: a small set of blogs whose authors provide feedback, and a set of popular, highly commented blogs, which provide indicators of algorithm scalability and interpretability without prior knowledge of a given blog. The results offer useful insight to the blog authors about their commenting community, and are observed to offer an integrated perspective on the topics of discussion and members engaged in those discussions for unfamiliar blogs. Our approach also holds promise as a component of solutions to related problems, such as online entity resolution and role discovery. © 2010 ACM.",Affinity Propagation; Blog; Topic models; Web communities,Internet; Affinity propagation; Blogospheres; Data sets; Interpretability; Prior knowledge; Similarity measure; Social interactions; Topic model; Web community; Blogs
Modeling Web quality using a probabilistic approach: An empirical validation,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954977647&doi=10.1145%2f1806916.1806918&partnerID=40&md5=d71ebbb9b8aed13483a74f8513bbd26f,"Web-based applications are software systems that continuously evolve to meet users' needs and to adapt to new technologies. Assuring their quality is then a difficult, but essential task. In fact, a large number of factors can affect their quality. Considering these factors and their interaction involves managing uncertainty and subjectivity inherent to this kind of applications. In this article, we present a probabilistic approach for building Web quality models and the associated assessment method. The proposed approach is based on Bayesian Networks. A model is built following a four-step process consisting in collecting quality characteristics, refining them, building a model structure, and deriving the model parameters. The feasibility of the approach is illustrated on the important quality characteristic of Navigability design. To validate the produced model, we conducted an experimental study with 20 subjects and 40 web pages. The results obtained show that the scores given by the used model are strongly correlated with navigability as perceived and experienced by the users. © 2010 ACM.",Bayesian Networks; Navigability design; Probabilistic approach; Quality evaluation; Web applications,Application programs; Websites; Empirical validation; Managing uncertainty; Number of factors; Probabilistic approaches; Quality characteristic; Quality evaluation; WEB application; Web-based applications; Bayesian networks
A large-scale study on map search logs,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954997015&doi=10.1145%2f1806916.1806917&partnerID=40&md5=1fc950e874e6df6a50b6c9b694e1e12f,"Map search engines, such as Google Maps, Yahoo! Maps, and Microsoft Live Maps, allow users to explicitly specify a target geographic location, either in keywords or on the map, and to search businesses, people, and other information of that location. In this article, we report a first study on a million-entry map search log. We identify three key attributes of a map search record-the keyword query, the target location and the user location, and examine the characteristics of these three dimensions separately as well as the associations between them. Comparing our results with those previously reported on logs of general search engines and mobile search engines, including those for geographic queries, we discover the following unique features of map search: (1) People use longer queries and modify queries more frequently in a session than in general search and mobile search; People view fewer result pages per query than in general search; (2) The popular query topics in map search are different from those in general search and mobile search; (3) The target locations in a session change within 50 kilometers for almost 80% of the sessions; (4) Queries, search target locations and user locations (both at the city level) all follow the power law distribution; (5) One third of queries are issued for target locations within 50 kilometers from the user locations; (6) The distribution of a query over target locations appears to follow the geographic location of the queried entity. © 2010 ACM.",Local search; Log analysis; Map search; Query categorization; Search interface; User behavior,Behavioral research; Search engines; Local search; Log analysis; Query categorization; Search interfaces; User behaviors; Location
Privacy-preserving query log mining for business confidentiality protection,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955019109&doi=10.1145%2f1806916.1806919&partnerID=40&md5=c9aef80fe7e94d0aa257da8c584e80a7,"We introduce the concern of confidentiality protection of business information for the publication of search engine query logs and derived data. We study business confidentiality, as the protection of nonpublic data from institutions, such as companies and people in the public eye. In particular, we relate this concern to the involuntary exposure of confidential Web site information, and we transfer this problem into the field of privacy-preserving data mining. We characterize the possible adversaries interested in disclosing Web site confidential data and the attack strategies that they could use. These attacks are based on different vulnerabilities found in query log for which we present several anonymization heuristics to prevent them. We perform an experimental evaluation to estimate the remaining utility of the log after the application of our anonymization techniques. Our experimental results show that a query log can be anonymized against these specific attacks while retaining a significant volume of useful data. © 2010 ACM.",Privacy preservation; Queries; Query log publication; Web sites,Eye protection; Information retrieval; Search engines; Websites; Business information; Experimental evaluation; Privacy preservation; Privacy preserving; Privacy preserving data mining; Queries; Query logs; Query-log minings; Data mining
Reporting incentives and biases in online review forums,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951896342&doi=10.1145%2f1734200.1734202&partnerID=40&md5=ff5bc9201defbbdf15fc14370fdada6c,"Online reviews have become increasingly popular as a way to judge the quality of various products and services. However, recent work demonstrates that the absence of reporting incentives leads to a biased set of reviews that may not reflect the true quality. In this paper, we investigate underlying factors that influence users when reporting feedback. In particular, we study both reporting incentives and reporting biases observed in a widely used review forum, the Tripadvisor Web site. We consider three sources of information: first, the numerical ratings left by the user for different aspects of quality; second, the textual comment accompanying a review; third, the patterns in the time sequence of reports. We first show that groups of users who discuss a certain feature at length are more likely to agree in their ratings. Second, we show that users are more motivated to give feedback when they perceive a greater risk involved in a transaction. Third, a user's rating partly reflects the difference between true quality and prior expectation of quality, as inferred from previous reviews. We finally observe that because of these biases, when averaging review scores there are strong differences between the mean and the median. We speculate that the median may be a better way to summarize the ratings. © 2010 ACM.",Online reviews; Reputation mechanisms,Numerical rating; Products and services; Reporting bias; Reputation mechanism; Reputation mechanisms; Time sequences; Underlying factors
Engineering rich internet applications with a model-driven approach,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951889375&doi=10.1145%2f1734200.1734204&partnerID=40&md5=a81d79f432716e8c59931f03d9c9d740,"Rich Internet Applications (RIAs) have introduced powerful novel functionalities into the Web architecture, borrowed from client-server and desktop applications. The resulting platforms allow designers to improve the user's experience, by exploiting client-side data and computation, bidirectional client-server communication, synchronous and asynchronous events, and rich interface widgets. However, the rapid evolution of RIA technologies challenges the Model-Driven Development methodologies that have been successfully applied in the past decade to traditional Web solutions. This paper illustrates an evolutionary approach for incorporating a wealth of RIA features into an existing Web engineering methodology and notation. The experience demonstrates that it is possible to model RIA application requirements at a high-level using a platform-independent notation, and generate the client-side and server-side code automatically. The resulting approach is evaluated in terms of expressive power, ease of use, and implementability. © 2010 ACM.",Information interfaces and presentation; Information storage and retrieval; Model-driven development; Rich Internet applications; Web engineering,Application requirements; Asynchronous event; Client server; Client-server communication; Desktop applications; Ease of use; Evolutionary approach; Expressive power; Implementability; Information interfaces; Information storage and retrieval; Model driven approach; Model driven development; Rapid evolution; Rich Internet Applications; Web architecture; Web engineering; Web solutions; Internet
Optimal distance bounds for fast search on compressed time-series query logs,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951928248&doi=10.1145%2f1734200.1734203&partnerID=40&md5=db96f0475b9eec3942efaaed1eaf772a,"Consider a database of time-series, where each datapoint in the series records the total number of users who asked for a specific query at an internet search engine. Storage and analysis of such logs can be very beneficial for a search company from multiple perspectives. First, from a data organization perspective, because query Weblogs capture important trends and statistics, they can help enhance and optimize the search experience (keyword recommendation, discovery of news events). Second, Weblog data can provide an important polling mechanism for the microeconomic aspects of a search engine, since they can facilitate and promote the advertising facet of the search engine (understand what users request and when they request it). Due to the sheer amount of time-series Weblogs, manipulation of the logs in a compressed form is an impeding necessity for fast data processing and compact storage requirements. Here, we explicate how to compute the lower and upper distance bounds on the time-series logs when working directly on their compressed form. Optimal distance estimation means tighter bounds, leading to better candidate selection/elimination and ultimately faster search performance. Our derivation of the optimal distance bounds is based on the careful analysis of the problem using optimization principles. The experimental evaluation suggests a clear performance advantage of the proposed method, compared to previous compression/search techniques. The presented method results in a 10 - 30% improvement on distance estimations, which in turn leads to 25 - 80% improvement on the search performance. © 2010 ACM.",,Data processing; Information retrieval; Search engines; World Wide Web; Data organization; Distance bound; Distance estimation; Experimental evaluation; Fast search; Internet search engine; Multiple perspectives; Optimization principle; Polling mechanism; Query logs; Search performance; Storage requirements; Weblogs; Optimization
Ads-portal domains: Identification and measurements,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951933528&doi=10.1145%2f1734200.1734201&partnerID=40&md5=e62c445c6ba0851dd560d74c654e7b09,"An ads-portal domain refers to a Web domain that shows only advertisements, served by a third-party advertisement syndication service, in the form of ads listing. We develop a machine-learning-based classifier to identify ads-portal domains, which has 96% accuracy. We use this classifier to measure the prevalence of ads-portal domains on the Internet. Surprisingly, 28.3/25% of the (two-level) *.com/*.net web domains are ads-portal domains. Also, 41/39.8% of *.com/ *.net ads-portal domains are typos of well-known domains, also known as typo-squatting domains. In addition, we use the classifier along with DNS trace files to estimate how often Internet users visit ads-portal domains. It turns out that ∼5% of the two-level *.com, *.net, *.org, *.biz and *.info web domains on the traces are ads-portal domains and ∼50% of these accessed ads-portal domains are typos. These numbers show that ads-portal domains and typo-squatting ads-portal domains are prevalent on the Internet and successful in attracting many visits. Our classifier represents a step towards better categorizing the web documents. It can also be helpful to search engines ranking algorithms, helpful in identifying web spams that redirects to ads-portal domains, and used to discourage access to typo-squatting ads-portal domains. © 2010 ACM.",Ads-portal; Advertisement syndication; Data mining; Parked domain; Parking service; Web characterization,Data mining; Learning systems; Search engines; Advertisement syndication; Internet users; Parked domain; Ranking algorithm; Third parties; Trace files; Web characterization; Web document; Portals
Declarative specification and verification of service choreographiess,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249104125&doi=10.1145%2f1658373.1658376&partnerID=40&md5=a933018baaf7d6042dbd1896051e345d,"Service-oriented computing, an emerging paradigm for architecting and implementing business collaborations within and across organizational boundaries, is currently of interest to both software vendors and scientists. While the technologies for implementing and interconnecting basic services are reaching a good level of maturity, modeling service interaction from a global viewpoint, that is, representing service choreographies, is still an open challenge. The main problem is that, although declarativeness has been identified as a key feature, several proposed approaches specify choreographies by focusing on procedural aspects, leading to over-constrained and over-specified models. To overcome these limits, we propose to adopt DecSerFlow, a truly declarative language, to model choreographies. Thanks to its declarative nature, DecSerFlow semantics can be given in terms of logic-based languages. In particular, we present how DecSerFlow can be mapped onto Linear Temporal Logic and onto Abductive Logic Programming. We show how the mappings onto both formalisms can be concretely exploited to address the enactment of DecSerFlow models, to enrich its expressiveness and to perform a variety of different verification tasks. We illustrate the advantages of using a declarative language in conjunction with logic-based semantics by applying our approach to a running example. © 2010 ACM.",Abductive logic programming; Compliance verification; Conformance checking; Declarative modeling; Interoperability; Linear temporal logic; Monitoring; Reasoning; Service choreographies,Computer software selection and evaluation; Interoperability; Linguistics; Logic programming; Query languages; Semantic Web; Semantics; Abductive logic programming; Business collaboration; Compliance verification; Conformance checking; Declarative Languages; Key feature; Linear temporal logic; Logic-based languages; Logic-based semantics; Organizational boundaries; Over-constrained; Procedural aspects; Service interaction; Service oriented computing; Software vendors; Specification and verification; Verification task; Temporal logic
A distributed service-oriented architecture for business process execution,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249114623&doi=10.1145%2f1658373.1658375&partnerID=40&md5=f011519a865fef5d1c0764aa643b10a6,"The Business Process Execution Language (BPEL) standardizes the development of composite enterprise applications that make use of software components exposed as Web services. BPEL processes are currently executed by a centralized orchestration engine, in which issues such as scalability, platform heterogeneity, and division across administrative domains can be difficult to manage. We propose a distributed agent-based orchestration engine in which several lightweight agents execute a portion of the original business process and collaborate in order to execute the complete process. The complete set of standard BPEL activities are supported, and the transformations of several BPEL activities to the agent-based architecture are described. Evaluations of an implementation of this architecture demonstrate that agent-based execution scales better than a non-distributed approach, with at least 70% and 120% improvements in process execution time, and throughput, respectively, even with a large number of concurrent process instances. In addition, the distributed architecture successfully executes large processes that are shown to be infeasible to execute with a nondistributed engine. © 2010 ACM.",BPEL; Business process; Complex event processing (CEP); Distributed orchestration; Enterprise service bus (ESB); Event processing; Publish/subscribe; Service-oriented architecture (SOA); Workflow management,Information services; Management; Web services; Work simplification; Business Process; Complex event processing; Complex event processing (CEP); Enterprise service bus; Publish/subscribe; Workflow managements; Service oriented architecture (SOA)
Understanding transportation modes based on GPS data for web applications,2010,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249092316&doi=10.1145%2f1658373.1658374&partnerID=40&md5=db4bdcf98d40a119b96730624fc29b07,"User mobility has given rise to a variety of Web applications, in which the global positioning system (GPS) plays many important roles in bridging between these applications and end users. As a kind of human behavior, transportation modes, such as walking and driving, can provide pervasive computing systems with more contextual information and enrich a user's mobility with informative knowledge. In this article, we report on an approach based on supervised learning to automatically infer users' transportation modes, including driving, walking, taking a bus and riding a bike, from raw GPS logs. Our approach consists of three parts: a change point-based segmentation method, an inference model and a graph-based post-processing algorithm. First, we propose a change point-based segmentation method to partition each GPS trajectory into separate segments of different transportation modes. Second, from each segment, we identify a set of sophisticated features, which are not affected by differing traffic conditions (e.g., a person's direction when in a car is constrained more by the road than any change in traffic conditions). Later, these features are fed to a generative inference model to classify the segments of different modes. Third, we conduct graph-based postprocessing to further improve the inference performance. This postprocessing algorithm considers both the commonsense constraints of the real world and typical user behaviors based on locations in a probabilistic manner. The advantages of our method over the related works include three aspects. (1) Our approach can effectively segment trajectories containing multiple transportation modes. (2) Our work mined the location constraints from user-generated GPS logs, while being independent of additional sensor data and map information like road networks and bus stops. (3) The model learned from the dataset of some users can be applied to infer GPS data from others. Using the GPS logs collected by 65 people over a period of 10 months, we evaluated our approach via a set of experiments. As a result, based on the change-point-based segmentation method and Decision Tree-based inference model, we achieved prediction accuracy greater than 71 percent. Further, using the graph-based post-processing algorithm, the performance attained a 4-percent enhancement. © 2010 ACM.",GeoLife; GPS trajectory; Spatial data mining; Transportation modes; Ubiquitous computing; Understanding user behavior; User mobility,Behavioral research; Decision trees; Human computer interaction; Inference engines; Location; Motor transportation; Roads and streets; Trajectories; Ubiquitous computing; World Wide Web; Spatial data mining; Transportation mode; Understanding user behavior; User behaviors; User mobility; Global positioning system
Trust and Nuanced Profile Similarity in Online Social Networks,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009714348&doi=10.1145%2f1594173.1594174&partnerID=40&md5=e4fbb498421c3f9d68dfa1de0d8092bb,"Online social networks, where users maintain lists of friends and express their preferences for items like movies, music, or books, are very popular. The Web-based nature of this information makes it ideal for use in a variety of intelligent systems that can take advantage of the users' social and personal data. For those systems to be effective, however, it is important to understand the relationship between social and personal preferences. In this work we investigate features of profile similarity and how those relate to the way users determine trust. Through a controlled study, we isolate several profile features beyond overall similarity that affect how much subjects trust hypothetical users. We then use data from FilmTrust, a real social network where users rate movies, and show that the profile features discovered in the experiment allow us to more accurately predict trust than when using only overall similarity. In this article, we present these experimental results and discuss the potential implications for using trust in user interfaces. © 2009, ACM. All rights reserved.",Human Factors; recommender systems; Social networks; trust,
Search-as-a-Service: Outsourced Search over Outsourced Storage,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925452156&doi=10.1145%2f1594173.1594175&partnerID=40&md5=85c9bf08fabf4d7f7ce485e89c02034f,"With fast-paced growth of digital data and exploding storage management costs, enterprises are looking for new ways to effectively manage their data. One such cost-effective paradigm is the cloud storage model also referred to as Storage-as-a-Service, in which enterprises outsource their storage to a storage service provider (SSP) by storing data (usually encrypted) at a remote SSPmanaged site and accessing it over a high speed network. Along with storage capacity used, the SSP often charges clients on the amount of data that is accessed from the SSP site. Thus, it is in the interest of the client enterprise to download only relevant content. This makes search over outsourced storage an important capability. Searching over encrypted outsourced storage, however, is a complex challenge. Each enterprise has different access privileges for different users and this access control needs to be preserved during search (for example, ensuring that a user cannot search through data that is inaccessible from the filesystem due to its permissions). Secondly, the search mechanism has to preserve confidentiality from the SSP and indices can not be stored in plain text. In this article, we present a new filesystem search technique that integrates access control and indexing/search mechanisms into a unified framework to support access control aware search. Our approach performs indexing within the trusted enterprise domain and uses a novel access control barrel (ACB) primitive to encapsulate access control within these indices. The indices are then systematically encrypted and shipped to the SSP for hosting. Unlike existing enterprise search techniques, our approach is resilient to various common attacks that leak private information. Additionally, to the best of our knowledge, our approach is a first such technique that allows search indices to be hosted at the SSP site, thus effectively providing search-as-a-service. This does not require the client enterprise to fully trust the SSP for data confidentiality. We describe the architecture and implementation of our approach and a detailed experimental analysis comparing with other approaches. © 2009, ACM. All rights reserved.",access control aware search; cloud search; cloud storage; Management; Search-as-a-Service; Security,
Emergence of Consensus and Shared Vocabularies in Collaborative Tagging Systems,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983400544&doi=10.1145%2f1594173.1594176&partnerID=40&md5=601be4860b1bec5da91a269663c09723,"This article uses data from the social bookmarking site del.icio.us to empirically examine the dynamics of collaborative tagging systems and to study how coherent categorization schemes emerge from unsupervised tagging by individual users. First, we study the formation of stable distributions in tagging systems, seen as an implicit form of “consensus” reached by the users of the system around the tags that best describe a resource. We show that final tag frequencies for most resources converge to power law distributions and we propose an empirical method to examine the dynamics of the convergence process, based on the Kullback-Leibler divergence measure. The convergence analysis is performed for both the most utilized tags at the top of tag distributions and the so-called long tail. Second, we study the information structures that emerge from collaborative tagging, namely tag correlation (or folksonomy) graphs.We show how community-based network techniques can be used to extract simple tag vocabularies from the tag correlation graphs by partitioning them into subsets of related tags. Furthermore, we also show, for a specialized domain, that shared vocabularies produced by collaborative tagging are richer than the vocabularies which can be extracted from large-scale query logs provided by a major search engine. © 2009, ACM. All rights reserved.",Algorithms; Collaborative tagging; community identification algorithms; complex systems; emergent semantics; graphical models; Human Factors; knowledge extraction; Measurement; power laws; search engines,
A framework for QoS-based Web service contracting,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549140050&doi=10.1145%2f1541822.1541825&partnerID=40&md5=d5f07b4189087d7df9b65fda98c48b40,"The extensive adoption of Web service-based applications in dynamic business scenarios, such as on-demand computing or highly reconfigurable virtual enterprises, advocates for methods and tools for the management of Web service nonfunctional aspects, such as Quality of Service (QoS). Concerning contracts on Web service QoS, the literature has mostly focused on the contract definition and on mechanisms for contract enactment, such as the monitoring of the satisfaction of negotiated QoS guarantees. In this context, this article proposes a framework for the automation of the Web service contract specification and establishment. An extensible model for defining both domain-dependent and domain-independent Web service QoS dimensions and a method for the automation of the contract establishment phase are proposed. We describe a matchmaking algorithm for the ranking of functionally equivalent services, which orders services on the basis of their ability to fulfill the service requestor requirements, while maintaining the price below a specified budget. We also provide an algorithm for the configuration of the negotiable part of the QoS Service-Level Agreement (SLA), which is used to configure the agreement with the top-ranked service identified in the matchmaking phase. Experimental results show that, in a utility theory perspective, the contract establishment phase leads to efficient outcomes. We envision two advanced application scenarios for the Web service contracting framework proposed in this article. First, it can be used to enhance Web services self-healing properties in reaction to QoS-related service failures; second, it can be exploited in process optimization for the online reconfiguration of candidate Web services QoS SLAs. © 2009 ACM.",Matchmaking; Negotiation; QoS; Service selection; SLA; Web service,Forestry; Ocean currents; Optimization; Virtual corporation; Web services; Matchmaking; Negotiation; QoS; Service selection; SLA; Quality of service
Cookies: A deployment study and the testing implications,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549085219&doi=10.1145%2f1541822.1541824&partnerID=40&md5=544d176f92a00a433cb462c6a5a39f7a,"The results of an extensive investigation of cookie deployment amongst 100,000 Internet sites are presented. Cookie deployment is found to be approaching universal levels and hence there exists an associated need for relevant Web and software engineering processes, specifically testing strategies which actively consider cookies. The semi-automated investigation demonstrates that over two-thirds of the sites studied deploy cookies. The investigation specifically examines the use of first-party, third-party, sessional, and persistent cookies within Web-based applications, identifying the presence of a P3P policy and dynamic Web technologies as major predictors of cookie usage. The results are juxtaposed with the lack of testing strategies present in the literature. A number of real-world examples, including two case studies are presented, further accentuating the need for comprehensive testing strategies for Web-based applications. The use of antirandom test case generation is explored with respect to the testing issues discussed. Finally, a number of seeding vectors are presented, providing a basis for testing cookies within Web-based applications. © 2009 ACM.",Cookies; Internet browser; Software testing; Web engineering; Web technologies,Computer software selection and evaluation; Software engineering; Software testing; Web services; Comprehensive testing; Cookies; Internet browser; Real-world; Semi-automated; Software engineering process; Test case generation; Testing strategies; Web engineering; Web technologies; Web-based applications; Internet
Unified publication and discovery of semantic web services,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549126876&doi=10.1145%2f1541822.1541826&partnerID=40&md5=457b208c08c1d399c4b4bbf1c1fbeb33,"The challenge of publishing and discovering Web services has recently received lots of attention. Various solutions to this problem have been proposed which, apart from their offered advantages, suffer the following disadvantages: (i) most of them are syntactic-based, leading to poor precision and recall, (ii) they are not scalable to large numbers of services, and (iii) they are incompatible, thus yielding in cumbersome service publication and discovery. This article presents the principles, the functionality, and the design of PYRAMID-S which addresses these disadvantages by providing a scalable framework for unified publication and discovery of semantically enhanced services over heterogeneous registries. PYRAMID-S uses a hybrid peer-to-peer topology to organize Web service registries based on domains. In such a topology, each Registry retains its autonomy, meaning that it can use the publication and discovery mechanisms as well as the ontology of its choice. The viability of this approach is demonstrated through the implementation and experimental analysis of a prototype. © 2009 ACM.",Evaluation; PYRAMID-S; Scalable; Semantic Web services; Unified; Web service discovery; Web service publication,Ontology; Semantic Web; Semantics; Topology; Web services; Evaluation; PYRAMID-S; Scalable; Unified; Web service discovery; Publishing
IRLbot: Scaling to 6 billion pages and beyond,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549090722&doi=10.1145%2f1541822.1541823&partnerID=40&md5=e057dc0ec614b8a456ebabc2a27413c7,"This article shares our experience in designing a Web crawler that can download billions of pages using a single-server implementation and models its performance. We first show that current crawling algorithms cannot effectively cope with the sheer volume of URLs generated in large crawls, highly branching spam, legitimate multimillion-page blog sites, and infinite loops created by server-side scripts. We then offer a set of techniques for dealing with these issues and test their performance in an implementation we call IRLbot. In our recent experiment that lasted 41 days, IRLbot running on a single server successfully crawled 6.3 billion valid HTML pages (7.6 billion connection requests) and sustained an average download rate of 319 mb/s (1,789 pages/s). Unlike our prior experiments with algorithms proposed in related work, this version of IRLbot did not experience any bottlenecks and successfully handled content from over 117 million hosts, parsed out 394 billion links, and discovered a subset of the Web graph with 41 billion unique nodes. © 2009 ACM.",Crawling; IRLbot; Large scale,Internet; Markup languages; Crawling; HTML pages; IRLbot; Large scale; Single server; Web crawlers; Web graphs; Electronic equipment manufacture
Classifying search queries using the Web as a source of knowledge,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349742467&doi=10.1145%2f1513876.1513877&partnerID=40&md5=cf89b86315bb98bf1879c38086914ec0,"We propose a methodology for building a robust query classification system that can identify thousands of query classes, while dealing in real time with the query volume of a commercial Web search engine. We use a pseudo relevance feedback technique: given a query, we determine its topic by classifying the Web search results retrieved by the query. Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregate account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience. © 2009 ACM.",Pseudo relevance feedback; Query classification; Web search,Feedback; Image retrieval; Search engines; Classification accuracy; Empirical evaluations; Machine-learning; Online ads; Pseudo relevance feedback; Query class; Query classification; Real time; Search queries; User experience; Web search; Web search engines; Web searches; World Wide Web
Extraction and classification of dense implicit communities in the Web graph,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349754411&doi=10.1145%2f1513876.1513879&partnerID=40&md5=34e66183f91f79e8983f47cd2c8c4e54,"The World Wide Web (WWW) is rapidly becoming important for society as a medium for sharing data, information, and services, and there is a growing interest in tools for understanding collective behavior and emerging phenomena in the WWW. In this article we focus on the problem of searching and classifying communities in the Web. Loosely speaking a community is a group of pages related to a common interest. More formally, communities have been associated in the computer science literature with the existence of a locally dense subgraph of the Web graph (where Web pages are nodes and hyperlinks are arcs of the Web graph). The core of our contribution is a new scalable algorithm for finding relatively dense subgraphs in massive graphs. We apply our algorithm on Web graphs built on three publicly available large crawls of the Web (with raw sizes up to 120M nodes and 1G arcs). The effectiveness of our algorithm in finding dense subgraphs is demonstrated experimentally by embedding artificial communities in the Web graph and counting how many of these are blindly found. Effectiveness increases with the size and density of the communities: it is close to 100% for communities of thirty nodes or more (even at low density). It is still about 80% even for communities of twenty nodes with density over 50% of the arcs present. At the lower extremes the algorithm catches 35% of dense communities made of ten nodes. We also develop some sufficient conditions for the detection of a community under some local graph models and not-too-restrictive hypotheses. We complete our Community Watch system by clustering the communities found in the Web graph into homogeneous groups by topic and labeling each group by representative keywords. © 2009 ACM.",Communities; Detection of dense subgraph; Web graph,Hypertext systems; World Wide Web; Collective behavior; Communities; Community IS; Detection of dense subgraph; Graph model; Homogeneous group; Hyperlinks; Low density; Massive graph; Representative keywords; Scalable algorithms; Subgraphs; Sufficient conditions; Web graph; Web graphs; Web page; Algorithms
A large-scale empirical study of P3P privacy policies: Stated actions vs. legal obligations,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349754413&doi=10.1145%2f1513876.1513878&partnerID=40&md5=77168c5bf3a37a8a01e2682ca11dfd7c,"Numerous studies over the past ten years have shown that concern for personal privacy is a major impediment to the growth of e-commerce. These concerns are so serious that most if not all consumer watchdog groups have called for some form of privacy protection for Internet users. In response, many nations around the world, including all European Union nations, Canada, Japan, and Australia, have enacted national legislation establishing mandatory safeguards for personal privacy. However, recent evidence indicates that Web sites might not be adhering to the requirements of this legislation. The goal of this study is to examine the posted privacy policies of Web sites, and compare these statements to the legal mandates under which the Web sites operate. We harvested all available P3P (Platform for Privacy Preferences Protocol) documents from the 100,000 most popular Web sites (over 3,000 full policies, and another 3,000 compact policies). This allows us to undertake an automated analysis of adherence to legal mandates on Web sites that most impact the average Internet user. Our findings show that Web sites generally do not even claim to follow all the privacy-protection mandates in their legal jurisdiction (we do not examine actual practice, only posted policies). Furthermore, this general statement appears to be true for every jurisdiction with privacy laws and any significant number of P3P policies, including European Union nations, Canada, Australia, and Web sites in the USA Safe Harbor program. © 2009 ACM.",Electronic commerce; Legislation and enforcement; P3P; Privacy protection,Electronic commerce; Internet; World Wide Web; Australia; Automated analysis; E-Commerce; Empirical studies; European Union; Internet users; Legal obligations; Legislation and enforcement; National legislation; P3P; Personal privacy; Platform for privacy preferences; Privacy law; Privacy policies; Privacy protection; Internet protocols
Browsing on small displays by transforming Web pages into hierarchically structured subpages,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58849124220&doi=10.1145%2f1462148.1462152&partnerID=40&md5=5069bd2f51f17daa3dc90e5989d745eb,"We propose a new Web page transformation method to facilitate Web browsing on handheld devices such as Personal Digital Assistants (PDAs). In our approach, an original Web page that does not fit on the screen is transformed into a set of subpages, each of which fits on the screen. This transformation is done through slicing the original page into page blocks iteratively, with several factors considered. These factors include the size of the screen, the size of each page block, the number of blocks in each transformed page, the depth of the tree hierarchy that the transformed pages form, as well as the semantic coherence between blocks. We call the tree hierarchy of the transformed pages an SP-tree. In an SP-tree, an internal node consists of a textually enhanced thumbnail image with hyperlinks, and a leaf node is a block extracted from a subpage of the original Web page. We adaptively adjust the fanout and the height of the SP-tree so that each thumbnail image is clear enough for users to read, while at the same time, the number of clicks needed to reach a leaf page is few. Through this transformation algorithm, we preserve the contextual information in the original Web page and reduce scrolling. We have implemented this transformation module on a proxy server and have conducted usability studies on its performance. Our system achieved a shorter task completion time compared with that of transformations from the Opera browser in nine of ten tasks. The average improvement on familiar pages was 44%. The average improvement on unfamiliar pages was 37%. Subjective responses were positive. © 2009 ACM.",Proxy; Slicing tree; Small displays; Thumbnails; Web browsing; Web page adaptation,Digital devices; Fourier transforms; Hand held computers; Hypertext systems; Image processing; Information theory; Personal digital assistants; User interfaces; Proxy; Slicing tree; Small displays; Thumbnails; Web browsing; Web page adaptation; World Wide Web
Methods for extracting place semantics from Flickr tags,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58849125785&doi=10.1145%2f1462148.1462149&partnerID=40&md5=b250333c3d47ad26e525a9c52a193a16,"We describe an approach for extracting semantics for tags, unstructured text-labels assigned to resources on the Web, based on each tag's usage patterns. In particular, we focus on the problem of extracting place semantics for tags that are assigned to photos on Flickr, a popular-photo sharing Web site that supports location (latitude/longitude) metadata for photos. We propose the adaptation of two baseline methods, inspired by well-known burst-analysis techniques, for the task; we also describe two novel methods, TagMaps and scale-structure identification. We evaluate the methods on a subset of Flickr data. We show that our scale-structure identification method outperforms existing techniques and that a hybrid approach generates further improvements (achieving 85% precision at 81% recall). The approach and methods described in this work can be used in other domains such as geo-annotated Web pages, where text terms can be extracted and associated with usage patterns. © 2009 ACM.",Places; Semantics; Tagging systems; Tags,Metadata; Semantics; Structure (composition); Thesauri; Websites; World Wide Web; Analysis techniques; Baseline methods; Hybrid approaches; Novel methods; Photo sharing; Places; Structure identifications; Tagging systems; Tags; Usage patterns; Web pages; Information theory
Protecting browsers from DNS rebinding attacks,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58849088038&doi=10.1145%2f1462148.1462150&partnerID=40&md5=876a71d12ad1fc9a50b027fe944b87a4,"DNS rebinding attacks subvert the same-origin policy of browsers, converting them into open network proxies. Using DNS rebinding, an attacker can circumvent organizational and personal firewalls, send spam email, and defraud pay-per-click advertisers. We evaluate the cost effectiveness of mounting DNS rebinding attacks, finding that an attacker requires less than $100 to hijack 100,000 IP addresses. We analyze defenses to DNS rebinding attacks, including improvements to the classic DNS pinning, and recommend changes to browser plug-ins, firewalls, and Web servers. Our defenses have been adopted by plug-in vendors and by a number of open-source firewall implementations. © 2009 ACM.",Click fraud; DNS; Firewall; Same-origin policy; Spam,Computer system firewalls; Internet; Internet protocols; Spamming; Click fraud; DNS; Firewall; Same-origin policy; Spam; Servers
Do not crawl in the DUST: Different URLs with similar text,2009,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58849118940&doi=10.1145%2f1462148.1462151&partnerID=40&md5=f9294cf81c4f5858de67b2ee1803988d,"We consider the problem of DUST: Different URLs with Similar Text. Such duplicate URLs are prevalent in Web sites, as Web server software often uses aliases and redirections, and dynamically generates the same page from various different URL requests. We present a novel algorithm, DustBuster, for uncovering DUST; that is, for discovering rules that transform a given URL to others that are likely to have similar content. DustBuster mines DUST effectively from previous crawl logs or Web server logs, without/examining page contents. Verifying these rules via sampling requires fetching few actual Web pages. Search engines can benefit from information about DUST to increase the effectiveness of crawling, reduce indexing overhead, and improve the quality of popularity statistics such as PageRank. © 2009 ACM.",Antialiasing; Crawling; Duplicate detection; Search engines; URL normalization,Anti-aliasing; Dust; Fourier transforms; Information retrieval; Internet; Search engines; Web services; Websites; Crawling; Duplicate detection; Novel algorithms; Page ranks; URL normalization; Web pages; Web server logs; Web servers; World Wide Web
Introduction to special issue on query log analysis: Technology and ethics,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149104020&doi=10.1145%2f1409220.1409221&partnerID=40&md5=2b824dd34bc08c186ac6e4ef36e8735e,[No abstract available],,
Combating spam in tagging systems: An evaluation,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149105065&doi=10.1145%2f1409220.1409225&partnerID=40&md5=01c80fe03027fa70e34349750fca367e,"Tagging systems allow users to interactively annotate a pool of shared resources using descriptive strings called tags. Tags are used to guide users to interesting resources and help them build communities that share their expertise and resources. As tagging systems are gaining in popularity, they become more susceptible to tag spam: misleading tags that are generated in order to increase the visibility of some resources or simply to confuse users. Our goal is to understand this problem better. In particular, we are interested in answers to questions such as: How many malicious users can a tagging system tolerate before results significantly degrade What types of tagging systems are more vulnerable to malicious attacks What would be the effort and the impact of employing a trusted moderator to find bad postings Can a system automatically protect itself from spam, for instance, by exploiting user tag patterns In a quest for answers to these questions, we introduce a framework for modeling tagging systems and user tagging behavior. We also describe a method for ranking documents matching a tag based on taggers' reliability. Using our framework, we study the behavior of existing approaches under malicious attacks and the impact of a moderator and our ranking method. © 2008 ACM.",Bookmarking systems; Tag spam; Tagging; Tagging models,Internet; Moderators; Spamming; Bookmarking systems; Malicious attacks; Malicious users; Ranking methods; Shared resources; Tag spam; Tagging; Tagging models; Tagging systems; Thesauri
Learning about the world through long-term query logs,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149100303&doi=10.1145%2f1409220.1409224&partnerID=40&md5=57219ddc71d089c5d591a1efb6e7dda5,"In this article, we demonstrate the value of long-term query logs. Most work on query logs to date considers only short-term (within-session) query information. In contrast, we show that long-term query logs can be used to learn about the world we live in. There are many applications of this that lead not only to improving the search engine for its users, but also potentially to advances in other disciplines such as medicine, sociology, economics, and more. In this article, we will show how long-term query logs can be used for these purposes, and that their potential is severely reduced if the logs are limited to short time horizons. We show that query effects are long-lasting, provide valuable information, and might be used to automatically make medical discoveries, build concept hierarchies, and generally learn about the sociological behavior of users. We believe these applications are only the beginning of what can be done with the information contained in long-term query logs, and see this work as a step toward unlocking their potential. © 2008 ACM.",Data mining; Knowledge discovery; Query logs; User behavior,Behavioral research; Decision support systems; Information management; Search engines; Concept hierarchies; Knowledge discovery; Query informations; Query logs; Short times; User behavior; Knowledge based systems
A survey of query log privacy-enhancing techniques from a policy perspective,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149118992&doi=10.1145%2f1409220.1409222&partnerID=40&md5=ff668ddd2f7d67beb30b2883eef5440f,"As popular search engines face the sometimes conflicting interests of protecting privacy while retaining query logs for a variety of uses, numerous technical measures have been suggested to both enhance privacy and preserve at least a portion of the utility of query logs. This article seeks to assess seven of these techniques against three sets of criteria: (1) how well the technique protects privacy, (2) how well the technique preserves the utility of the query logs, and (3) how well the technique might be implemented as a user control. A user control is defined as a mechanism that allows individual Internet users to choose to have the technique applied to their own query logs. © 2008 ACM.",Log; Policy; Privacy; Query; Search,Search engines; Log; Policy; Privacy; Query; Search; World Wide Web
Design trade-offs for search engine caching,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149095218&doi=10.1145%2f1409220.1409223&partnerID=40&md5=32b555d42e5ddc12b1772252049f186b,"In this article we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year, we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log influence the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. © 2008 ACM.",Caching; Query logs; Web search,Commerce; Computer software; Information retrieval; Search engines; Caching; Caching systems; Design trades; Designing efficient; Dynamic caching; Hit rates; New algorithms; Query answers; Query logs; Query results; Web search; Web search engines; World Wide Web
Models and framework for supporting runtime decisions in Web-based systems,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249144372&doi=10.1145%2f1377488.1377491&partnerID=40&md5=8f40fbd43bfebfc813073292c0528e95,"Efficient management of distributed Web-based systems requires several mechanisms that decide on request dispatching, load balance, admission control, request redirection. The algorithms behind these mechanisms typically make fast decisions on the basis of the load conditions of the system resources. The architecture complexity and workloads characterizing most Web-based services make it extremely difficult to deduce a representative view of a resource load from collected measures that show extreme variability even at different time scales. Hence, any decision based on instantaneous or average views of the system load may lead to useless or even wrong actions. As an alternative, we propose a two-phase strategy that first aims to obtain a representative view of the load trend from measured system values and then applies this representation to support runtime decision systems. We consider two classical problems behind decisions: how to detect significant and nontransient load changes of a system resource and how to predict its future load behavior. The two-phase strategy is based on stochastic functions that are characterized by a computational complexity that is compatible with runtime decisions. We describe, test, and tune the two-phase strategy by considering as a first example a multitier Web-based system that is subject to different classes of realistic and synthetic workloads. Also, we integrate the proposed strategy into a framework that we validate by applying it to support runtime decisions in a cluster Web system and in a locally distributed Network Intrusion Detection System. © 2008 ACM.",Distributed systems; Load change detection; Load prediction; Load representation; World Wide Web,Arsenic compounds; Computational complexity; Computer crime; Computer systems; Control systems; Internet; Lead; Mechanisms; Network architecture; Security of data; Sensors; Two phase flow; Web services; Websites; World Wide Web; Admission control (AC); Classical problems; Decision systems; Different time scales; Distributed networks; Load balancing; Load behavior; Load conditions; Load trend; Measured system; Request dispatching; Run time; Stochastic functions; Synthetic workloads; System loads; System resources; Web based systems; Web systems; Web-based services; Intrusion detection
Mitigating application-level denial of service attacks on Web servers: A client-transparent approach,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249148481&doi=10.1145%2f1377488.1377489&partnerID=40&md5=af74cc53463432bf97b98e799e5f8a28,"Recently, we have seen increasing numbers of denial of service (DoS) attacks against online services and Web applications either for extortion reasons or for impairing and even disabling the competition. These DoS attacks have increasingly targeted the application level. Application-level DoS attacks emulate the same request syntax and network-level traffic characteristics as those of legitimate clients, thereby making the attacks much harder to detect and counter. Moreover, such attacks often target bottleneck resources such as disk bandwidth, database bandwidth, and CPU resources. In this article, we propose handling DoS attacks by using a twofold mechanism. First, we perform admission control to limit the number of concurrent clients served by the online service. Admission control is based on port hiding that renders the online service invisible to unauthorized clients by hiding the port number on which the service accepts incoming requests. Second, we perform congestion control on admitted clients to allocate more resources to good clients. Congestion control is achieved by adaptively setting a client's priority level in response to the client's requests in a way that can incorporate application-level semantics. We present a detailed evaluation of the proposed solution using two sample applications: Apache HTTPD and the TPCW benchmark (running on Apache Tomcat and IBM DB2). Our experiments show that the proposed solution incurs low performance overhead and is resilient to DoS attacks. © 2008 ACM.",Client transparency; DoS Attacks; Game theory; Web servers,Computer crime; Concurrency control; Information science; Information theory; Internet; Ports and harbors; Security of data; Servers; Telecommunication systems; Traffic congestion; Transmission control protocol; World Wide Web; Admission control (AC); Bottleneck resources; Congestion control; CPU resources; Denial of service (DoS) attacks; Disk bandwidth; DOS attacks; Incoming requests; On-line services; Port numbers; Traffic characteristics; WEB applications; Web servers; Benchmarking
Leveraging popular destinations to enhance Web search interaction,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249148884&doi=10.1145%2f1377488.1377490&partnerID=40&md5=b064a2dfb2f403e1b87668ec9c22b95a,"This article presents a novel Web search interaction feature that for a given query provides links to Web sites frequently visited by other users with similar information needs. These popular destinations complement traditional search results, allowing direct navigation to authoritative resources for the query topic. Destinations are identified using the history of the search and browsing behavior of many users over an extended time period, and their collective behavior provides a basis for computing source authority. They are drawn from the end of users' postquery browse trails where users may cease searching once they find relevant information. We describe a user study that compared the suggestion of destinations with the previously proposed suggestion of related queries as well as with traditional, unaided Web search. Results show that search enhanced by query suggestions outperforms other systems in terms of subject perceptions and search effectiveness for fact-finding search tasks. However, search enhanced by destination suggestions performs best for exploratory tasks with its best performance obtained from mining past user behavior at query-level granularity. We discuss the implications of these and other findings from our study for the design of search systems that utilize user behavior, in particular, user browse trails and popular destinations. © 2008 ACM.",Enhanced Web search; Search destinations; User studies,Behavioral research; Flow interactions; Information retrieval; Websites; Collective behavior; Computing sources; Fact-finding; Information needs; Query provides; Relevant information; Search results; Search systems; Search tasks; Time periods; User behaviors; User studies; Web searches; World Wide Web
Correctness-aware high-level functional matching approaches for semantic Web services,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43749093194&doi=10.1145%2f1346337.1346240&partnerID=40&md5=8c4475cec1af6c478c014af75dfb6f32,"Service matching approaches trade precision for recall, creating the need for users to choose the correct services, which obviously is a major obstacle for automating the service discovery and aggregation processes. Our approach to overcome this problem, is to eliminate the appearance of false positives by returning only the correct services. As different users have different semantics for what is correct, we argue that the correctness of the matching results must be determined according to the achievement of users' goals: that only services achieving users' goals are considered correct. To determine such correctness, we argue that the matching process should be based primarily on the high-level functional specifications (namely goals, achievement contexts, and external behaviors). In this article, we propose models, data structures, algorithms, and theorems required to correctly match such specifications. We propose a model called G+, to capture such specifications, for both services and users, in a machine-understandable format. We propose a data structure, called a Concepts Substitutability Graph (CSG), to capture the substitution semantics of application domain concepts in a context-based manner, in order to determine the semantic-preserving mapping transformations required to match different G+ models. We also propose a behavior matching approach that is able to match states in an m-to-n manner, such that behavior models with different numbers of state transitions can be matched. Finally, we show how services are matched and aggregated according to their G+ models. Results of supporting experiments demonstrate the advantages of the proposed service matching approaches. © 2008 ACM.",High-level functional matching; Semantic Web services; Service aggregation,Algorithms; Data structures; Problem solving; Semantic Web; Semantics; User interfaces; High-level functional matching; Semantic Web services; Service aggregation; Web services
An environment for flexible advanced compensations of Web service transactions,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43749117481&doi=10.1145%2f1346337.1346242&partnerID=40&md5=c0ef1fd5016fa4d730a530dab354fcca,"Business to business integration has recently been performed by employing Web service environments. Moreover, such environments are being provided by major players on the technology markets. Those environments are based on open specifications for transaction coordination. When a failure in such an environment occurs, a compensation can be initiated to recover from the failure. However, current environments have only limited capabilities for compensations, and are usually based on backward recovery. In this article, we introduce an environment to deal with advanced compensations based on forward recovery principles. We extend the existing Web service transaction coordination architecture and infrastructure in order to support flexible compensation operations. We use a contract-based approach, which allows the specification of permitted compensations at runtime. We introduce abstract service and adapter components, which allow us to separate the compensation logic from the coordination logic. In this way, we can easily plug in or plug out different compensation strategies based on a specification language defined on top of basic compensation activities and complex compensation types. Experiments with our approach and environment show that such an approach to compensation is feasible and beneficial. Additionally, we introduce a cost-benefit model to evaluate the proposed environment based on net value analysis. The evaluation shows in which circumstances the environment is economical. © 2008 ACM.",Compensations; Forward-recovery; Transactions; Web services,Computer architecture; Error compensation; Formal logic; Specifications; Compensation activities; Compensation logic; Forward-recovery; Transactions; Web services
Automatic annotation of Web services based on workflow definitions,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43749083658&doi=10.1145%2f1346337.1346239&partnerID=40&md5=d7673162e2470c6afe0a83fe9a9ddb8b,"Semantic annotations of web services can support the effective and efficient discovery of services, and guide their composition into workflows. At present, however, the practical utility of such annotations is limited by the small number of service annotations available for general use. Manual annotation of services is a time consuming and thus expensive task, so some means are required by which services can be automatically (or semi-automatically) annotated. In this paper, we show how information can be inferred about the semantics of operation parameters based on their connections to other (annotated) operation parameters within tried-and-tested workflows. Because the data links in the workflows do not necessarily contain every possible connection of compatible parameters, we can infer only constraints on the semantics of parameters. We show that despite their imprecise nature these so-called loose annotations are still of value in supporting the manual annotation task, inspecting workflows and discovering services. We also show that derived annotations for already annotated parameters are useful. By comparing existing and newly derived annotations of operation parameters, we can support the detection of errors in existing annotations, the ontology used for annotation and in workflows. The derivation mechanism has been implemented, and its practical applicability for inferring new annotations has been established through an experimental evaluation. The usefulness of the derived annotations is also demonstrated. © 2008 ACM.",Automatic annotation; Ontologies; Semantic annotations; Semantic web services; Workflows,Information analysis; Ontology; Parameter estimation; Semantic Web; Telecommunication links; Automatic annotation; Semantic annotations; Semantic web services; Workflows; Web services
Introduction to special issue on service oriented computing (SOC),2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43749118646&doi=10.1145%2f1346337.1346338&partnerID=40&md5=7e70a01b9d7a89a5455147a78a1ad873,[No abstract available],,
Supporting the dynamic evolution of Web service protocols in service-oriented architectures,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43749124807&doi=10.1145%2f1346337.1346241&partnerID=40&md5=d3c2330a0a7876dc2e182b806f8d3717,"In service-oriented architectures, everything is a service and everyone is a service provider. Web services (or simply services) are loosely coupled software components that are published, discovered, and invoked across the Web. As the use of Web service grows, in order to correctly interact with them, it is important to understand the business protocols that provide clients with the information on how to interact with services. In dynamic Web service environments, service providers need to constantly adapt their business protocols for reflecting the restrictions and requirements proposed by new applications, new business strategies, and new laws, or for fixing problems found in the protocol definition. However, the effective management of such a protocol evolution raises critical problems: one of the most critical issues is how to handle instances running under the old protocol when it has been changed. Simple solutions, such as aborting them or allowing them to continue to run according to the old protocol, can be considered, but they are inapplicable for many reasons (for example, the loss of work already done and the critical nature of work). In this article, we present a framework that supports service managers in managing the business protocol evolution by providing several features, such as a variety of protocol change impact analyses automatically determining which ongoing instances can be migrated to the new version of protocol, and data mining techniques inferring interaction patterns used for classifying ongoing instances migrateable to the new protocol. To support the protocol evolution process, we have also developed database-backed GUI tools on top of our existing system. The proposed approach and tools can help service managers in managing the evolution of ongoing instances when the business protocols of services with which they are interacting have changed. © 2008 ACM.",Business protocols; Change impact analysis; Decision trees; Dynamic evolution; Ongoing instances; Web services,Computer architecture; Computer software; Data mining; Decision trees; Web services; Business protocols; Change impact analysis; Dynamic evolution; Ongoing instances; Network protocols
Introduction to special section on adversarial issues in Web search,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40949112746&doi=10.1145%2f1326561.1326562&partnerID=40&md5=2730444ce7f7b689ee442ecfdef795e2,[No abstract available],,
Link analysis for Web spam detection,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40949116672&doi=10.1145%2f1326561.1326563&partnerID=40&md5=a76e436bd4a4bc4da346d3028bb33f1b,"We propose link-based techniques for automatic detection of Web spam, a term referring to pages which use deceptive techniques to obtain undeservedly high scores in search engines. The use of Web spam is widespread and difficult to solve, mostly due to the large size of the Web which means that, in practice, many algorithms are infeasible. We perform a statistical analysis of a large collection of Web pages. In particular, we compute statistics of the links in the vicinity of every Web page applying rank propagation and probabilistic counting over the entire Web graph in a scalable way. These statistical features are used to build Web spam classifiers which only consider the link structure of the Web, regardless of page contents. We then present a study of the performance of each of the classifiers alone, as well as their combined performance, by testing them over a large collection of Web link spam. After tenfold cross-validation, our best classifiers have a performance comparable to that of state-of-the-art spam classifiers that use content attributes, but are orthogonal to content-based methods.",Adversarial information retrieval; Link analysis,Algorithms; Information retrieval; Search engines; Statistical methods; World Wide Web; Link analysis; Web graph; Web link spam; Web spam; Intrusion detection
Tracking Web spam with HTML style similarities,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40949104348&doi=10.1145%2f1326561.1326564&partnerID=40&md5=0b4eb403f1d3ce4a8badc005cc0e18e1,"Automatically generated content is ubiquitous in the web: dynamic sites built using the three-tier paradigm are good examples (e.g., commercial sites, blogs and other sites edited using web authoring software), as well as less legitimate spamdexing attempts (e.g., link farms, faked directories). Those pages built using the same generating method (template or script) share a common look and feel that is not easily detected by common text classification methods, but is more related to stylometry. In this work we study and compare several HTML style similarity measures based on both textual and extra-textual features in HTML source code. We also propose a flexible algorithm to cluster a large collection of documents according to these measures. Since the proposed algorithm is based on locality sensitive hashing (LSH), we first review this technique. We then describe how to use the HTML style similarity clusters to pinpoint dubious pages and enhance the quality of spam classifiers. We present an evaluation of our algorithm on the WEBSPAM-UK2006 dataset.",Clustering; Document similarity; Search engine spam; Stylometry; Templates identification,Algorithms; HTML; Search engines; Template matching; Document similarity; Locality sensitive hashing (LSH); Search engine spam; Stylometry; Templates identification; World Wide Web
Detecting splogs via temporal dynamics using self-similarity analysis,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40949163854&doi=10.1145%2f1326561.1326565&partnerID=40&md5=fa211da74a26242c6a365859c8cd69e4,"This article addresses the problem of spam blog (splog) detection using temporal and structural regularity of content, post time and links. Splogs are undesirable blogs meant to attract search engine traffic, used solely for promoting affiliate sites. Blogs represent popular online media, and splogs not only degrade the quality of search engine results, but also waste network resources. The splog detection problem is made difficult due to the lack of stable content descriptors. We have developed a new technique for detecting splogs, based on the observation that a blog is a dynamic, growing sequence of entries (or posts) rather than a collection of individual pages. In our approach, splogs are recognized by their temporal characteristics and content. There are three key ideas in our splog detection framework. (a) We represent the blog temporal dynamics using self-similarity matrices defined on the histogram intersection similarity measure of the time, content, and link attributes of posts, to investigate the temporal changes of the post sequence. (b) We study the blog temporal characteristics using a visual representation derived from the self-similarity measures. The visual signature reveals correlation between attributes and posts, depending on the type of blogs (normal blogs and splogs). (c) We propose two types of novel temporal features to capture the splog temporal characteristics. In our splog detector, these novel features are combined with content based features. We extract a content based feature vector from blog home pages as well as from different parts of the blog. The dimensionality of the feature vector is reduced by Fisher linear discriminant analysis. We have tested an SVM-based splog detector using proposed features on real world datasets, with appreciable results (90% accuracy).",Blogs; Regularity; Self-similarity; Spam; Splog detection; Temporal dynamics; Topology,Discriminant analysis; Online systems; Search engines; Support vector machines; Datasets; Feature vector; Fisher linear discriminant analysis; Splog detection; Intrusion detection
Adaptive quality of service management for enterprise services,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40949138410&doi=10.1145%2f1326561.1326569&partnerID=40&md5=6602ebf51ff7983d513d1d811c61611e,"In the past, enterprise resource planning systems were designed as monolithic software systems running on centralized mainframes. Today, these systems are (re-)designed as a repository of enterprise services that are distributed throughout the available computing infrastructure. These service oriented architectures (SOAs) require advanced automatic and adaptive management concepts in order to achieve a high quality of service level in terms of, for example, availability, responsiveness, and throughput. The adaptive management has to allocate service instances to computing resources, adapt the resource allocation to unforeseen load fluctuations, and intelligently schedule individual requests to guarantee negotiated service level agreements (SLAs). Our AutoGlobe platform provides such a comprehensive adaptive service management comprising - -static service-to-server allocation based on automatically detected service utilization patterns, - -adaptive service management based on a fuzzy controller that remedies exceptional situations by automatically initiating, for example, service migration, service replication (scale-out), and - -adaptive scheduling of individual service requests that prioritizes requests depending on the current degree of service level conformance. All three complementary control components are described in detail, and their effectiveness is analyzed by means of realistic business application scenarios.",Fuzzy controller; Quality of service; Workload characterization,Control systems; Fuzzy control; Quality of service; Resource allocation; Systems analysis; Fuzzy controller; Monolithic software systems; Resource planning systems; Service oriented architectures (SOAs); Adaptive systems
Framework for Web service query algebra and optimization,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40949146968&doi=10.1145%2f1326561.1326567&partnerID=40&md5=c38de4d944bc1e5c354dad331b6fb820,We present a query algebra that supports optimized access of Web services through service-oriented queries. The service query algebra is defined based on a formal service model that provides a high-level abstraction of Web services across an application domain. The algebra defines a set of algebraic operators. Algebraic service queries can be formulated using these operators. This allows users to query their desired services based on both functionality and quality. We provide the implementation of each algebraic operator. This enables the generation of Service Execution Plans (SEPs) that can be used by users to directly access services. We present an optimization algorithm by extending the Dynamic Programming (DP) approach to efficiently select the SEPs with the best user-desired quality. The experimental study validates the proposed algorithm by demonstrating significant performance improvement compared with the traditional DP approach.,Query optimization; Service oriented computing; Service query; Web service,Algorithms; Mathematical operators; Optimization; Quality of service; Query processing; Query optimization; Service oriented computing; Service query; Web services
Not quite the average: An empirical study of Web use,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40949097728&doi=10.1145%2f1326561.1326566&partnerID=40&md5=e86581f53a734d975b7500c687fcbb94,"In the past decade, the World Wide Web has been subject to dramatic changes. Web sites have evolved from static information resources to dynamic and interactive applications that are used for a broad scope of activities on a daily basis. To examine the consequences of these changes on user behavior, we conducted a long-term client-side Web usage study with twenty-five participants. This report presents results of this study and compares the user behavior with previous long-term browser usage studies, which range in age from seven to thirteen years. Based on the empirical data and the interview results, various implications for the interface design of browsers and Web sites are discussed. A major finding is the decreasing prominence of backtracking in Web navigation. This can largely be attributed to the increasing importance of dynamic, service-oriented Web sites. Users do not navigate on these sites searching for information, but rather interact with an online application to complete certain tasks. Furthermore, the usage of multiple windows and tabs has partly replaced back button usage, posing new challenges for user orientation and backtracking. We found that Web browsing is a rapid activity even for pages with substantial content, which calls for page designs that allow for cursory reading. Click maps provide additional information on how users interact with the Web on page level. Finally, substantial differences were observed between users, and characteristic usage patterns for different types of Web sites emphasize the need for more adaptive and customizable Web browsers.",Browser interfaces; Hypertext; Navigation; Usability; User study; Web; Web browsing; Web design; WWW,Hypertext systems; Interactive computer systems; Online systems; User interfaces; Web browsers; Browser interfaces; Static information resources; Web design; World Wide Web
Discovering global network communities based on local centralities,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40949107741&doi=10.1145%2f1326561.1326570&partnerID=40&md5=54c693f35b49977cb86263cf51677b8d,"One of the central problems in studying and understanding complex networks, such as online social networks or World Wide Web, is to discover hidden, either physically (e.g., interactions or hyperlinks) or logically (e.g., profiles or semantics) well-defined topological structures. From a practical point of view, a good example of such structures would be so-called network communities. Earlier studies have introduced various formulations as well as methods for the problem of identifying or extracting communities. While each of them has pros and cons as far as the effectiveness and efficiency are concerned, almost none of them has explicitly dealt with the potential relationship between the global topological property of a network and the local property of individual nodes. In order to study this problem, this paper presents a new algorithm, called ICS, which aims to discover natural network communities by inferring from the local information of nodes inherently hidden in networks based on a new centrality, that is, clustering centrality, which is a generalization of eigenvector centrality. As compared with existing methods, our method runs efficiently with a good clustering performance. Additionally, it is insensitive to its built-in parameters and prior knowledge.",Centrality; Community mining; Complex network; Graph theory; World Wide Web,Algorithms; Graph theory; Online systems; Semantics; World Wide Web; Community mining; Global networks; Online social networks; Topological structures; Problem solving
Scalable semantic analytics on social networks for addressing the problem of conflict of interest detection,2008,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40949096910&doi=10.1145%2f1326561.1326568&partnerID=40&md5=dca4052e35975ca41145442250be1671,"In this article, we demonstrate the applicability of semantic techniques for detection of Conflict of Interest (COI). We explain the common challenges involved in building scalable Semantic Web applications, in particular those addressing connecting-the-dots problems. We describe in detail the challenges involved in two important aspects on building Semantic Web applications, namely, data acquisition and entity disambiguation (or reference reconciliation). We extend upon our previous work where we integrated the collaborative network of a subset of DBLP researchers with persons in a Friend-of-a-Friend social network (FOAF). Our method finds the connections between people, measures collaboration strength, and includes heuristics that use friendship/affiliation information to provide an estimate of potential COI in a peer-review scenario. Evaluations are presented by measuring what could have been the COI between accepted papers in various conference tracks and their respective program committee members. The experimental results demonstrate that scalability can be achieved by using a dataset of over 3 million entities (all bibliographic data from DBLP and a large collection of FOAF documents).",Conflict of interest; Data fusion; DBLP; Entity disambiguation; Ontologies; Peer review process; RDF; Semantic analytics; Semantic associations; Semantic Web; Social networks; SwetoDblp,Data fusion; Data structures; Ontology; Problem solving; Semantic Web; Entity disambiguation; Peer review process; Semantic analytics; Social networks; Semantics
Decoding the structure of the WWW: A comparative analysis of Web crawls,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548765501&doi=10.1145%2f1255438.1255442&partnerID=40&md5=9ed25bfc026bf1671b77deadc45df1ec,"The understanding of the immense and intricate topological structure of the World Wide Web (WWW) is a major scientific and technological challenge. This has been recently tackled by characterizing the properties of its representative graphs, in which vertices and directed edges are identified with Web pages and hyperlinks, respectively. Data gathered in large-scale crawls have been analyzed by several groups resulting in a general picture of the WWW that encompasses many of the complex properties typical of rapidly evolving networks. In this article, we report a detailed statistical analysis of the topological properties of four different WWW graphs obtained with different crawlers. We find that, despite the very large size of the samples, the statistical measures characterizing these graphs differ quantitatively, and in some cases qualitatively, depending on the domain analyzed and the crawl used for gathering the data. This spurs the issue of the presence of sampling biases and structural differences of Web crawls that might induce properties not representative of the actual global underlying graph. In short, the stability of the widely accepted statistical description of the Web is called into question. In order to provide a more accurate characterization of the Web graph, we study statistical measures beyond the degree distribution, such as degree-degree correlation functions or the statistics of reciprocal connections. The latter appears to enclose the relevant correlations of the WWW graph and carry most of the topological information of the Web. The analysis of this quantity is also of major interest in relation to the navigability and searchability of the Web. © 2007 ACM.",Crawler biases; Statistical analysis; Web graph structure; Web measurement,Function evaluation; Statistical methods; Topology; World Wide Web; Crawler biases; Web graph structure; Web measurement; Decoding
Model-directed Web transactions under constrained modalities,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34848906255&doi=10.1145%2f1281480.1281482&partnerID=40&md5=0cf2e7264e40c1bacdd8a3c81ff84c14,"Online transactions (e.g., buying a book on the Web) typically involve a number of steps spanning several pages. Conducting such transactions under constrained interaction modalities as exemplified by small screen handhelds or interactive speech interfaces - -the primary mode of communication for visually impaired individuals - -is a strenuous, fatigue-inducing activity. But usually one needs to browse only a small fragment of a Web page to perform a transactional step such as a form fillout, selecting an item from a search results list, and so on. We exploit this observation to develop an automata-based process model that delivers only the relevant page fragments at each transactional step, thereby reducing information overload on such narrow interaction bandwidths. We realize this model by coupling techniques from content analysis of Web documents, automata learning and statistical classification. The process model and associated techniques have been incorporated into Guide-O, a prototype system that facilitates online transactions using speech/keyboard interface (Guide-O-Speech), or with limited-display size handhelds (Guide-O-Mobile). Performance of Guide-O and its user experience are reported. © 2007 ACM.",Assistive device; Content adaption; Web transaction,Bandwidth; Communication systems; Learning systems; Mathematical models; Speech analysis; Statistical methods; User interfaces; Websites; Assistive devices; Content adaption; Keyboard interfaces; Web transaction; Web services
Cache architecture for on-demand streaming on the Web,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34848922111&doi=10.1145%2f1281480.1281483&partnerID=40&md5=597aa08754cd2524aeac0d2c1c4e4982,"On-demand streaming from a remote server through best-effort Internet poses several challenges because of network losses and variable delays. The primary technique used to improve the quality of distributed content service is replication. In the context of the Internet, Web caching is the traditional mechanism that is used. In this article we develop a new staged delivery model for a distributed architecture in which video is streamed from remote servers to edge caches where the video is buffered and then streamed to the client through a last-mile connection. The model uses a novel revolving indexed cache buffer management mechanism at the edge cache and employs selective retransmissions of lost packets between the remote and edge cache for a best-effort recovery of the losses. The new Web cache buffer management scheme includes a dynamic adjustment of cache buffer parameters based on network conditions. In addition, performance of buffer management and retransmission policies at the edge cache is modeled and assessed using a probabilistic analysis of the streaming process as well as system simulations. The influence of different endogenous control parameters on the quality of stream received by the client is studied. Calibration curves on the QoS metrics for different network conditions have been obtained using simulations. Edge cache management can be done using these calibration curves. ISPs can make use of calibration curves to set the values of the endogenous control parameters for specific QoS in real-time streaming operations based on network conditions. A methodology to benchmark transmission characteristics using real-time traffic data is developed to enable effective decision making on edge cache buffer allocation and management strategies. © 2007 ACM.",Buffering; Edge cache; On-demand streaming; Quality of service; Selective retransmissions; Web caching,Cache memory; Computer simulation; Decision making; Mathematical models; Quality of service; Servers; Video streaming; Buffering; On-demand streaming; Selective retransmissions; Web caching; Web services
Modeling process-driven and service-oriented architectures using patterns and pattern primitives,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34848864554&doi=10.1145%2f1281480.1281484&partnerID=40&md5=3b8c150a9a9027ce9ecb9890d4515268,"Service-oriented architectures are increasingly used in the context of business processes. However, the proven practices for process-oriented integration of services are not well documented yet. In addition, modeling approaches for the integration of processes and services are neither mature nor do they exactly reflect the proven practices. In this article, we propose a pattern language for process-oriented integration of services to describe the proven practices. Our main contribution is a modeling concept based on pattern primitives for these patterns. A pattern primitive is a fundamental, precisely specified modeling element that represents a pattern. We present a catalog of pattern primitives that are precisely modeled using OCL constraints and map these primitives to the patterns in the pattern language of process-oriented integration of services. We also present a model validation tool that we have developed to support modeling the process-oriented integration of services, and an industrial case study in which we have applied our results. © 2007 ACM.",Middleware; Service-oriented architecture; Software patterns,Formal languages; Mathematical models; Software architecture; Pattern languages; Process-oriented integration; Service-oriented architecture; Software patterns; Middleware
"Scouts, promoters, and connectors: The roles of ratings in nearest-neighbor collaborative filtering",2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548718744&doi=10.1145%2f1255438.1255440&partnerID=40&md5=357b2f0a1548fcac06ff8ce261857143,"Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors. The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce. We present a characterization of nearest-neighbor collaborative filtering that allows us to disaggregate global recommender performance measures into contributions made by each individual rating. In particular, we formulate three roles - -scouts, promoters, and connectors - -that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected, respectively. These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole. For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling. We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community. © 2007 ACM.",Collaborative filtering; Connectors; Neighborhoods; Promoters; Recommender systems; Scouts; User-based and item-based algorithms,Computer supported cooperative work; Image quality; Tracking (position); Collaborative filtering; Recommender systems; User-based and item-based algorithms; Web services
BrowserShield: Vulnerability-driven filtering of dynamic HTML,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34848907433&doi=10.1145%2f1281480.1281481&partnerID=40&md5=971d99dd02f08149f6c055df2c81ff82,"Vulnerability-driven filtering of network data can offer a fast and easy-to-deploy alternative or intermediary to software patching, as exemplified in Shield [Wang et al. 2004]. In this article we take Shield's vision to a new domain, inspecting and cleansing not just static content, but also dynamic content. The dynamic content we target is the dynamic HTML in Web pages, which have become a popular vector for attacks. The key challenge in filtering dynamic HTML is that it is undecidable to statically determine whether an embedded script will exploit the browser at runtime. We avoid this undecidability problem by rewriting web pages and any embedded scripts into safe equivalents, inserting checks so that the filtering is done at runtime. The rewritten pages contain logic for recursively applying runtime checks to dynamically generated or modified web content, based on known vulnerabilities. We have built and evaluated BrowserShield, a general framework that performs this dynamic instrumentation of embedded scripts, and that admits policies for customized runtime actions like vulnerability-driven filtering. We also explore other applications on top of BrowserShield. © 2007 ACM.",Code rewriting; JavaScript; Vulnerability; Web browser,Embedded systems; Java programming language; Public policy; Web browsers; Websites; Code rewriting; Dynamic content; JavaScript; Vulnerability; HTML
Visualizing tags over time,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548734181&doi=10.1145%2f1255438.1255439&partnerID=40&md5=e8f8a2999065445970f4eb393a291944,"We consider the problem of visualizing the evolution of tags within the Flickr (flickr.com) online image sharing community. Any user of the Flickr service may append a tag to any photo in the system. Over the past year, users have on average added over a million tags each week. Understanding the evolution of these tags over time is therefore a challenging task. We present a new approach based on a characterization of the most interesting tags associated with a sliding interval of time. An animation provided via Flash in a Web browser allows the user to observe and interact with the interesting tags as they evolve over time. New algorithms and data structures are required to support the efficient generation of this visualization. We combine a novel solution to an interval covering problem with extensions to previous work on score aggregation in order to create an efficient backend system capable of producing visualizations at arbitrary scales on this large dataset in real time. © 2007 ACM.",Flickr; Interval covering; Tags; Temporal evolution; Visualization,Animation; Problem solving; Web browsers; Web services; Interval covering; Temporal evolution; Image analysis
The effects of proxy bidding and minimum bid increments within eBay auctions,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250204457&doi=10.1145%2f1255438.1255441&partnerID=40&md5=08f416b9fa7cc64dc9142d99ec37ab45,"We present a mathematical model of the eBay auction protocol and perform a detailed analysis of the effects that the eBay proxy bidding system and the minimum bid increment have on the auction properties. We first consider the revenue of the auction, and we show analytically that when two bidders with independent private valuations use the eBay proxy bidding system there exists an optimal value for the minimum bid increment at which the auctioneer's revenue is maximized. We then consider the sequential way in which bids are placed within the auction, and we show analytically that independent of assumptions regarding the bidders' valuation distribution or bidding strategy the number of visible bids placed is related to the logarithm of the number of potential bidders. Thus, in many cases, it is only a minority of the potential bidders that are able to submit bids and are visible in the auction bid history (despite the fact that the other hidden bidders are still effectively competing for the item). Furthermore, we show through simulation that the minimum bid increment also introduces an inefficiency to the auction, whereby a bidder who enters the auction late may find that its valuation is insufficient to allow them to advance the current bid by the minimum bid increment despite them actually having the highest valuation for the item. Finally, we use these results to consider appropriate strategies for bidders within real world eBay auctions. We show that while last-minute bidding (sniping) is an effective strategy against bidders engaging in incremental bidding (and against those with common values), in general, delaying bidding is disadvantageous even if delayed bids are sure to be received before the auction closes. Thus, when several bidders submit last-minute bids, we show that rather than seeking to bid as late as possible, a bidder should try to be the first sniper to bid (i.e., it should snipe before the snipers). © 2007 ACM.",Bid increment; Electronic commerce; Online auctions; Proxy bidding; Sniping,Competition; Computer simulation; Mathematical models; Optimization; Bid increment; Online auctions; Proxy bidding; Electronic commerce
Efficient algorithms for Web services selection with end-to-end QoS constraints,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248525624&doi=10.1145%2f1232722.1232728&partnerID=40&md5=2a64aa452b5f641556bd3dc5b2e5db8a,"Service-Oriented Architecture (SOA) provides a flexible framework for service composition. Using standard-based protocols (such as SOAP and WSDL), composite services can be constructed by integrating atomic services developed independently. Algorithms are needed to select service components with various QoS levels according to some application-dependent performance requirements. We design a broker-based architecture to facilitate the selection of QoS-based services. The objective of service selection is to maximize an application-specific utility function under the end-to-end QoS constraints. The problem is modeled in two ways: the combinatorial model and the graph model. The combinatorial model defines the problem as a multidimension multichoice 0-1 knapsack problem (MMKP). The graph model defines the problem as a multiconstraint optimal path (MCOP) problem. Efficient heuristic algorithms for service processes of different composition structures are presented in this article and their performances are studied by simulations. We also compare the pros and cons between the two models. © 2007 ACM.",End-to-end QoS; Service composition; Service Oriented Architecture (SOA); Service selection; Web services,Algorithms; Computer architecture; Constraint theory; Mathematical models; Quality of service; Requirements engineering; Efficient algorithms; Service composition; Service Oriented Architecture (SOA); Service selection; Web services
The comparative effectiveness of sponsored and nonsponsored links for Web e-commerce queries,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248599033&doi=10.1145%2f1232722.1232725&partnerID=40&md5=9971ba67a9f845e40fffb7be07c410d7,"The predominant business model for Web search engines is sponsored search, which generates billions in yearly revenue. But are sponsored links providing online consumers with relevant choices for products and services? We address this and related issues by investigating the relevance of sponsored and nonsponsored links for e-commerce queries on the major search engines. The results show that average relevance ratings for sponsored and nonsponsored links are practically the same, although the relevance ratings for sponsored links are statistically higher. We used 108 ecommerce queries and 8,256 retrieved links for these queries from three major Web search engines: Yahoo!, Google, and MSN. In addition to relevance measures, we qualitatively analyzed the e-commerce queries, deriving five categorizations of underlying information needs. Product-specific queries are the most prevalent (48%). Title (62%) and summary (33%) are the primary basis for evaluating sponsored links with URL a distant third (2%). To gauge the effectiveness of sponsored search campaigns, we analyzed the sponsored links from various viewpoints. It appears that links from organizations with large sponsored search campaigns are more relevant than the average sponsored link. We discuss the implications for Web search engines and sponsored search as a long-term business model and as a mechanism for finding relevant information for searchers. © 2007 ACM.",e-commerce searching; Sponsored links; Sponsored results; Sponsored search; Web search engines; Web searching,Electronic commerce; Information analysis; Mathematical models; Online systems; Queueing networks; Search engines; E-commerce searching; Sponsored links; Sponsored results; Sponsored search; Web searching; Telecommunication links
Analytic modeling of multitier Internet applications,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248588783&doi=10.1145%2f1232722.1232724&partnerID=40&md5=af68c7ad369f052ea66ce18774b57a82,"Since many Internet applications employ a multitier architecture, in this article, we focus on the problem of analytically modeling the behavior of such applications. We present a model based on a network of queues where the queues represent different tiers of the application. Our model is sufficiently general to capture (i) the behavior of tiers with significantly different performance characteristics and (ii) application idiosyncrasies such as session-based workloads, tier replication, load imbalances across replicas, and caching at intermediate tiers. We validate our model using real multitier applications running on a Linux server cluster. Our experiments indicate that our model faithfully captures the performance of these applications for a number of workloads and configurations. Furthermore, our model successfully handles a comprehensive range of resource utilization - -from 0 to near saturation for the CPU - -for two separate tiers. For a variety of scenarios, including those with caching at one of the application tiers, the average response times predicted by our model were within the 95% confidence intervals of the observed average response times. Our experiments also demonstrate the utility of the model for dynamic capacity provisioning, performance prediction, bottleneck identification, and session policing. In one scenario, where the request arrival rate increased from less than 1500 to nearly 4200 requests/minute, a dynamic provisioning technique employing our model was able to maintain response time targets by increasing the capacity of two of the tiers by factors of 2 and 3.5, respectively. © 2007 ACM.",Analytical model; Dynamic provisioning; Hosting platform; Internet service; Mean-value analysis; Performance prediction; Policing; Queuing theory; Session; Tier,Computer architecture; Computer operating systems; Dynamic programming; Mathematical models; Queueing theory; Telecommunication services; Analytical models; Dynamic provisioning; Hosting platforms; Internet services; Mean-value analysis; Performance prediction; Internet
ACM Transactions on the Web: Introduction,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248584101&doi=10.1145%2f1232722.1232723&partnerID=40&md5=b5afbaf87d7d21e0e6813b6d430a3a39,[No abstract available],,
The dynamics of viral marketing,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248550159&doi=10.1145%2f1232722.1232727&partnerID=40&md5=f1acdaa2a3a301ea8bfd8f0bf9d0c520,"We present an analysis of a person-to-person recommendation network, consisting of 4 million people who made 16 million recommendations on half a million products. We observe the propagation of recommendations and the cascade sizes, which we explain by a simple stochastic model. We analyze how user behavior varies within user communities defined by a recommendation network. Product purchases follow a long tail where a significant share of purchases belongs to rarely sold items. We establish how the recommendation network grows over time and how effective it is from the viewpoint of the sender and receiver of the recommendations. While on average recommendations are not very effective at inducing purchases and do not spread very far, we present a model that successfully identifies communities, product, and pricing categories for which viral marketing seems to be very effective. © 2007 ACM.",e-commerce; Long tail; Network analysis; Recommender systems; Viral marketing; Word-of-mouth,Artificial intelligence; Marketing; Stochastic models; Telecommunication networks; User interfaces; Wave propagation; Recommender systems; User communities; Viral marketing; Electronic commerce
Mobile information access: A study of emerging search behavior on the mobile Internet,2007,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248590834&doi=10.1145%2f1232722.1232726&partnerID=40&md5=609ed1f9e5d90158f354e5834fdf7141,"It is likely that mobile phones will soon come to rival more traditional devices as the primary platform for information access. Consequently, it is important to understand the emerging information access behavior of mobile Internet (MI) users especially in relation to their use of mobile handsets for information browsing and query-based search. In this article, we describe the results of a recent analysis of the MI habits of more than 600,000 European MI users, with a particular emphasis on the emerging interest in mobile search. We consider a range of factors including whether there are key differences between browsing and search behavior on the MI compared to the Web. We highlight how browsing continues to dominate mobile information access, but go on to show how search is becoming an increasingly popular information access alternative especially in relation to certain types of mobile handsets and information needs. Moreover, we show that sessions involving search tend to be longer and more data-rich than those that do not involve search. We also look at the type of queries used during mobile search and the way that these queries tend to be modified during the course of a mobile search session. Finally we examine the overlap among mobile search queries and the different topics mobile users are interested in. © 2007 ACM.",Log analysis; Mobile browsing; Mobile Internet; Mobile search,Internet; Mobile telecommunication systems; Query languages; Search engines; User interfaces; Web browsers; Log analysis; Mobile browsing; Mobile Internet (MI); Mobile search; Electronic data interchange
Pre-trained Language Model-based Retrieval and Ranking for Web Search,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149444233&doi=10.1145%2f3568681&partnerID=40&md5=51d3de435384fde57847ae112bdb442a,"Pre-trained language representation models (PLMs) such as BERT and Enhanced Representation through kNowledge IntEgration (ERNIE) have been integral to achieving recent improvements on various downstream tasks, including information retrieval. However, it is nontrivial to directly utilize these models for the large-scale web search due to the following challenging issues: (1) the prohibitively expensive computations of massive neural PLMs, especially for long texts in the web document, prohibit their deployments in the web search system that demands extremely low latency; (2) the discrepancy between existing task-agnostic pre-training objectives and the ad hoc retrieval scenarios that demand comprehensive relevance modeling is another main barrier for improving the online retrieval and ranking effectiveness; and (3) to create a significant impact on real-world applications, it also calls for practical solutions to seamlessly interweave the resultant PLM and other components into a cooperative system to serve web-scale data. Accordingly, we contribute a series of successfully applied techniques in tackling these exposed issues in this work when deploying the state-of-the-art Chinese pre-trained language model, i.e., ERNIE, in the online search engine system. We first present novel practices to perform expressive PLM-based semantic retrieval with a flexible poly-interaction scheme and cost-efficiently contextualize and rank web documents with a cheap yet powerful Pyramid-ERNIE architecture. We then endow innovative pre-training and fine-tuning paradigms to explicitly incentivize the query-document relevance modeling in PLM-based retrieval and ranking with the large-scale noisy and biased post-click behavioral data. We also introduce a series of effective strategies to seamlessly interwoven the designed PLM-based models with other conventional components into a cooperative system. Extensive offline and online experimental results show that our proposed techniques are crucial to achieving more effective search performance. We also provide a thorough analysis of our methodology and experimental results.  © 2022 Association for Computing Machinery.",Pre-trained language model; ranking; web retrieval,Computational linguistics; Information retrieval; Semantics; Websites; Knowledge integration; Language model; Large-scales; Model-based OPC; Pre-trained language model; Ranking; Representation model; Web document; Web retrieval; Web searches; Search engines
Keywords-enhanced Deep Reinforcement Learning Model for Travel Recommendation,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149165549&doi=10.1145%2f3570959&partnerID=40&md5=6b8034762fd5d9310c301b20e8435540,"Tourism is an important industry and a popular entertainment activity involving billions of visitors per annum. One challenging problem tourists face is identifying satisfactory products from vast tourism information. Most of travel recommendation methods regard the recommendation procedure as a static process and only focus on immediate rewards. Meanwhile, they often infer user intensions from click behaviors and ignore the informative keywords of the clicked products. To this end, in this article, we present a Keywords-enhanced Deep Reinforcement Learning model (KDRL) framework. Specifically, we formalize travel recommendation as a Markov Decision Process and implement it upon the Actor-Critic framework. It integrates keyword information into the reinforcement learning-(RL) based recommendation framework by devising novel state representation and reward function and learns the travel recommendation and keywords generation simultaneously. To the best of our knowledge, this is the first time that keywords are explicitly discussed and used in RL-based travel recommendations. Extensive experiments are performed on the real-world datasets and the results clearly show the superior performance of KDRL compared with the baseline methods.  © 2022 Association for Computing Machinery.",attention mechanism; deep neural network; Recommender system; reinforcement learning,Deep neural networks; Learning algorithms; Learning systems; Markov processes; Recommender systems; Actor critic; Attention mechanisms; Learn+; Markov Decision Processes; Modelling framework; Recommendation methods; Reinforcement learning models; Reinforcement learnings; Reward function; State representation; Reinforcement learning
Classification of Layout vs. Relational Tables on the Web: Machine Learning with Rendered Pages,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149438189&doi=10.1145%2f3555349&partnerID=40&md5=5f86ec2ced7b59c5f0dc5dc49612603f,"Table mining on the web is an open problem, and none of the previously proposed techniques provides a complete solution. Most research focuses on the structure of the HTML document, but because of the nature and structure of the web, it is still a challenging problem to detect relational tables. Web Content Accessibility Guidelines (WCAG) also cover a wide range of recommendations for making tables accessible, but our previous work shows that these recommendations are also not followed; therefore, tables are still inaccessible to disabled people and automated processing. We propose a new approach to table mining by not looking at the HTML structure, but rather, the rendered pages by the browser. The first task in table mining on the web is to classify relational vs. layout tables, and here, we propose two alternative approaches for that task. We first introduce our dataset, which includes 725 web pages with 9,957 extracted tables. Our first approach extracts features from a page after being rendered by the browser, then applies several machine learning algorithms in classifying the layout vs. relational tables. The best result is with Random Forest with the accuracy of 97.2% (F1-score: 0.955) with 10-fold cross-validation. Our second approach classifies tables using images taken from the same sources using Convolutional Neural Network (CNN), which gives an accuracy of 95% (F1-score: 0.95). Our work here shows that the web's true essence comes after it goes through a browser and using the rendered pages and tables, the classification is more accurate compared to literature and paves the way in making the tables more accessible. © 2022 Association for Computing Machinery.",information extraction; table accessibility; table classification; Table mining,Convolutional neural networks; HTML; Learning algorithms; Machine learning; Rendering (computer graphics); Websites; Complete solutions; F1 scores; Information extraction; Machine-learning; Relational tables; Research focus; Table accessibility; Table classifications; Table mining; Web machines; Classification (of information)
Double Attention Convolutional Neural Network for Sequential Recommendation,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146424086&doi=10.1145%2f3555350&partnerID=40&md5=3da674585c0f88d21857ee81529c38db,"The explosive growth of e-commerce and online service has led to the development of recommender system. Aiming to provide a list of items to meet a user's personalized need by analyzing his/her interaction1 history, recommender system has been widely studied in academic and industrial communities. Different from conventional recommender systems, sequential recommender systems attempt to capture the pattern of users' sequential behaviors and the evolution of users' preferences. Most of the existing sequential recommendation models only focus on user interaction sequence, but neglect item interaction sequence. An item interaction sequence also contains rich contextual information for capturing the item's dynamic characteristic, since an item's dynamic characteristic can be reflected by the users who interact with it in a period. Furthermore, existing dual sequential models use the same method to handle the user interaction sequence and item interaction sequence, and do not consider their different characteristics. Hence, we propose a novel Double Attention Convolution Neural Network (DACNN), which incorporates user interaction sequence and item interaction sequence into an integrated neural network framework. DACNN leverages the strength of attention mechanism to capture the temporary suitability and adopts CNN to extract local sequential features. Experimental evaluations on the real datasets show that DACNN outperforms the baseline approaches.  © 2022 Association for Computing Machinery.",Additional Key Words and PhrasesRecommender system; deep learning; neural network; sequential prediction,Convolution; Convolutional neural networks; Deep neural networks; Electronic commerce; Additional key word and phrasesrecommende system; Convolution neural network; Convolutional neural network; Deep learning; Dynamics characteristic; Explosive growth; Key words; Neural-networks; Sequential prediction; User interaction; Recommender systems
BanditProp: Bandit Selection of Review Properties for Effective Recommendation,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146422809&doi=10.1145%2f3532859&partnerID=40&md5=ee814246e656faaaa2458656a4d7b838,"Many recent recommendation systems leverage the large quantity of reviews placed by users on items. However, it is both challenging and important to accurately measure the usefulness of such reviews for effective recommendation. In particular, users have been shown to exhibit distinct preferences over different types of reviews (e.g., preferring longer versus shorter or recent versus old reviews), indicating that users might differ in their viewpoints on what makes the reviews useful. Yet, there have been limited studies that account for the personalised usefulness of reviews when estimating the users' preferences. In this article, we propose a novel neural model, called BanditProp, which addresses this gap in the literature. It first models reviews according to both their content and associated properties (e.g., length, sentiment and recency). Thereafter, it constructs a multi-task learning (MTL) framework to model the reviews' content encoded with various properties.In such an MTL framework, each task corresponds to producing recommendations focusing on an individual property. Next, we address the selection of the features from reviews with different review properties as a bandit problem using multinomial rewards. We propose a neural contextual bandit algorithm (i.e., ConvBandit) and examine its effectiveness in comparison to eight existing bandit algorithms in addressing the bandit problem. Our extensive experiments on two well-known Amazon and Yelp datasets show that BanditProp can significantly outperform one classic and six existing state-of-the-art recommendation baselines. Moreover, BanditProp using ConvBanditconsistently outperforms the use of other bandit algorithms over the two used datasets. In particular, we experimentally demonstrate the effectiveness of our proposed customised multinomial rewards in comparison to binary rewards, when addressing our bandit problem.  © 2022 Association for Computing Machinery.",bandit search; Recommendation systems; review property; user behaviour modelling,Behavioral research; Learning systems; Linearization; Statistics; User profile; Bandit problems; Bandit search; Learning frameworks; Multinomials; Multitask learning; Neural modelling; Property; Review property; User behavior modeling; User's preferences; Recommender systems
Mutexion: Mutually Exclusive Compression System for Mitigating Compression Side-Channel Attacks,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146420510&doi=10.1145%2f3532850&partnerID=40&md5=e52b0f81641e29ad78996992085823a5,"To enhance the performance of web services, web servers often compress data to be delivered. Unfortunately, the data compression technique has also introduced a side effect called compression side-channel attacks (CSCA). CSCA allows eavesdroppers to unveil secret strings included in the encrypted traffic by observing the length of data. A promising defense technique called Debreach was recently proposed to mitigate CSCA by excluding all secret data in a web page during the compression process. Although Debreach has proven to be safe against CSCA and outperforms other approaches, the exclusion of all secret data from compression eventually resulted in a decreased compression efficiency. In this paper, we present a highly efficient CSCA mitigation system called ""Mutexion""(Mutually exclusive compression) which allows us to fully take advantage of compression over an entire web page, including secret data. The key idea behind Mutexion is to fully take advantage of all the matching subsequences within a web page except only for those between secret data and user-controlled data (potentially controlled by an attacker) during the compression process. This approach of Mutexion effectively prevents side-channel leaks of secret data under CSCA misusing user-controlled data in a web page while minimizing the degradation in compression efficiency. It is required for our compressor to trace both secret data and user-controlled data in its compression process of web pages. To meet this requirement, we provide techniques to enable automated annotation of secret and user-controlled data in web pages. We implemented Mutexion as a fully working system to test live web pages and evaluated its performance with respect to security and compression efficiency. Our evaluation results demonstrated that Mutexion effectively prevents CSCA and also achieves almost the same compression ratio as the original zlib, which is vulnerable to CSCA, with a slight increase (0.032 milliseconds (7.9%) on average) in execution time.  © 2022 Association for Computing Machinery.",compression; side-channel attack; Web security,Data compression; Efficiency; Side channel attack; Web services; Compression; Compression efficiency; Compression process; Compression system; Performance; Secret data; Side-channel attacks; WEB security; Web-page; Webs services; Websites
Spotting Flares: The Vital Signs of the Viral Spread of Tweets Made During Communal Incidents,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146422132&doi=10.1145%2f3550357&partnerID=40&md5=15ff45bf98505c7b7043c63344b89c62,"With the increasing use of Twitter for encouraging users to instigate violent behavior with hate and racial content, it becomes necessary to investigate the uniqueness in the dynamics of the spread of tweets made during violent communal incidents and the challenges they pose in early identification of potential viral content. In this article, we study the spread of the tweets made during several violent communal incidents along four major dimensions - the underlying follower network of the users, their structural and engagement characteristics, the cascades, and the cognitive aspects of the content, each of which plays a vital role in the spread of content. Using large public and collected data, we compare these features with tweets related to other subjects from several major domains, such as non-violent political events, celebrities, and technology, that contribute to a large fraction of the viral content over Twitter. We discover that while the spread of cascades and the users involved may provide strong early evidence of the viral content for several domains, the early phases of the spread of viral tweets related to violent communal incidents are characterized by cascades with protracted growth involving fringe or low-importance users, which would possibly make early prediction difficult. Our findings indicate that an interplay of certain network and cascade properties, together with the cognitive characteristics of tweets and the behavioral patterns of the engaging users, may provide stronger early indicators of the virality of this content.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",causality analysis; communal incidents; Social media; social network analysis; Twitter; virality prediction,Causality analysis; Cognitive aspects; Communal incident; Social media; Social Network Analysis; Twitter; Violent behavior; Viral spread; Virality prediction; Vital sign; Social networking (online)
JSAnalyzer: A Web Developer Tool for Simplifying Mobile Web Pages through Non-critical JavaScript Elimination,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141438031&doi=10.1145%2f3550358&partnerID=40&md5=b117ff4c016040899662cac3f3aebdbc,"The amount of JavaScript used in web pages has substantially grown in the past decade, leading to large and complex pages that are computationally intensive for handheld mobile devices. Due to the increasing usage of these devices to access today's web, and to accommodate the needs of a large number of mobile web users who solely rely on low-end devices, we propose ""JSAnalyzer,""an easy-to-use tool that enables web developers to quickly optimize JavaScript usage in their pages and to generate simpler versions of these pages for mobile web users. JSAnalyzer is motivated by the widespread use of non-critical JavaScript elements, i.e., those that have negligible (if any) impact on the page's visual content and interactive functionality. JSAnalyzer allows the developer to selectively enable or disable JavaScript elements in any given page while visually observing their impact on the page to (1) accurately identify any non-critical JavaScript elements and (2) create a simplified page with these elements removed. Our quantitative evaluation shows that, given a low-end mobile phone, JSAnalyzer achieves an increase of nearly 90% in Google's lighthouse performance score while reducing the page load time by 30%. A qualitative study of 22 users shows that the lighter pages produced by JSAnalyzer maintain more than 90% visual similarity compared to the original pages. Moreover, JSAnalyzer was evaluated by 69 developers, showing that it scores nearly 90% in terms of usefulness and usability while retaining the page's content and functionality. Finally, we show that JSAnalyzer outperforms state-of-the-art solutions in terms of timing speedups and resource savings.  © 2022 Association for Computing Machinery.",Mobile web; non-critical Javascript; user experience,High level languages; Mobile telecommunication systems; End-devices; Handheld/mobile devices; Javascript; Mobile web; Mobile web pages; Non-critical javascript; Users' experiences; Web developers; Web users; Web-page; Websites
Optimisation Techniques for Flexible SPARQL Queries,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146420558&doi=10.1145%2f3532855&partnerID=40&md5=a64515da28442df59d85135b1cb0a1fc,"Resource Description Framework datasets can be queried using the SPARQL language but are often irregularly structured and incomplete, which may make precise query formulation hard for users. The SPARQLAR language extends SPARQL 1.1 with two operators - APPROX and RELAX - to allow flexible querying over property paths. These operators encapsulate different dimensions of query flexibility, namely, approximation and generalisation, and they allow users to query complex, heterogeneous knowledge graphs without needing to know precisely how the data is structured. Earlier work has described the syntax, semantics, and complexity of SPARQLAR, has demonstrated its practical feasibility, but has also highlighted the need for improving the speed of query evaluation. In the present article, we focus on the design of two optimisation techniques targeted at speeding up the execution of SPARQLAR queries and on their empirical evaluation on three knowledge graphs: LUBM, DBpedia, and YAGO. We show that applying these optimisations can result in substantial improvements in the execution times of longer-running queries (sometimes by one or more orders of magnitude) without incurring significant performance penalties for fast queries.  © 2022 Association for Computing Machinery.",path queries; query approximation; query relaxation; SPARQL 1.1,Knowledge graph; Query processing; Flexible querying; Knowledge graphs; Optimization techniques; Path queries; Property; Query approximations; Query formulation; Query relaxation; Resources description frameworks; SPARQL 1.1; Semantics
The Internet with Privacy Policies: Measuring the Web Upon Consent,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141031421&doi=10.1145%2f3555352&partnerID=40&md5=9b58721befea6b82404c83c044231f2b,"To protect user privacy, legislators have regulated the use of tracking technologies, mandating the acquisition of users' consent before collecting data. As a result, websites started showing more and more consent management modules-i.e., Consent Banners-the visitors have to interact with to access the website content. Since these banners change the content the browser loads, they challenge web measurement collection, primarily to monitor the extent of tracking technologies, but also to measure web performance. If not correctly handled, Consent Banners prevent crawlers from observing the actual content of the websites.In this paper, we present a comprehensive measurement campaign focusing on popular websites in Europe and the US, visiting both landing and internal pages from different countries around the world. We engineer Priv-Accept, a Web crawler able to accept the Consent Banners, as most users would do in practice. It lets us compare how webpages change before and after accepting such policies, if present. Our results show that all measurements performed ignoring the Consent Banners offer a biased and partial view of the Web. After accepting the privacy policies, web tracking is far more pervasive, and webpages are larger and slower to load. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Consent Banner; crawling; GDPR; Web measurements,Web crawler; Consent banner; Consent managements; Crawling; GDPR; Privacy policies; Tracking technology; User privacy; Web measurements; Web site contents; Web-page; Websites
Personalized Visualization Recommendation,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139176236&doi=10.1145%2f3538703&partnerID=40&md5=62f0b42797f1f681a829b8cdd1fb5212,"Visualization recommendation work has focused solely on scoring visualizations based on the underlying dataset, and not the actual user and their past visualization feedback. These systems recommend the same visualizations for every user, despite that the underlying user interests, intent, and visualization preferences are likely to be fundamentally different, yet vitally important. In this work, we formally introduce the problem of personalized visualization recommendation and present a generic learning framework for solving it. In particular, we focus on recommending visualizations personalized for each individual user based on their past visualization interactions (e.g., viewed, clicked, manually created) along with the data from those visualizations. More importantly, the framework can learn from visualizations relevant to other users, even if the visualizations are generated from completely different datasets. Experiments demonstrate the effectiveness of the approach as it leads to higher quality visualization recommendations tailored to the specific user intent and preferences. To support research on this new problem, we release our user-centric visualization corpus consisting of 17.4k users exploring 94k datasets with 2.3 million attributes and 32k user-generated visualizations. © 2022 Association for Computing Machinery.",automated visualization design; data attribute recommendation; dataset recommendation; deep learning; machine learning; personalized visualization design recommendation; personalized visualization recommendation systems; user modeling; user personalization; Visualization recommendation,Data visualization; Deep learning; Learning systems; Recommender systems; User profile; Automated visualization; Automated visualization design; Data attribute recommendation; Data attributes; Dataset recommendation; Deep learning; Design recommendations; Machine-learning; Personalizations; Personalized visualization design recommendation; Personalized visualization recommendation system; User Modelling; User personalization; Visualization designs; Visualization recommendation; Visualization
SpotSpam: Intention Analysis-driven SMS Spam Detection Using BERT Embeddings,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137003730&doi=10.1145%2f3538491&partnerID=40&md5=dffb99ce493b5d3c79f2dbbf1e683e5f,"Short Message Service (SMS) is one of the widely used mobile applications for global communication for personal and business purposes. Its widespread use for customer interaction, business updates, and reminders has made it a billion-dollar industry in ""Text Marketing.""Along with valid SMS, a tsunami of spam messages also pop up that serve various purposes for the sender and the majority of them are fraudulent. Filtering spam SMS in an accurate manner is a crucial and challenging task that will benefit human lives both mentally and economically. Some of the challenges in the filtering of spam SMS include less number of characters, texts in informal languages, lack of public SMS spam corpus, and so on. Focusing solely on the textual features of the SMS is a major handicap of the existing methods, as it lacks in dynamically adapting to the increasing number of new keywords and jargon. In this article, we develop an intention-based approach of SMS spam filtering that efficiently handles dynamic keywords by focusing on the semantics of the words. We capture both semantic and textual features of the short-text messages based on 13 pre-defined intention labels. Moreover, the contextual embeddings of the texts are generated using various pre-trained NLP (Natural Language Processing) models. Finally, intention scores are computed for the pre-defined labels and a bunch of supervised learning classifiers are employed for filtering as spam or ham. Our approaches are evaluated on the SMS Spam Collection [24] benchmark dataset, and extensive experimentation shows interesting results. Our model did remarkably well with an accuracy of 98.07%, Precision and Recall of ∼0.97, which is better than few of the existing state-of-the-art alternatives. Though the accuracy of our approach is not the best among other existing approaches, the model is highly stable due to its emphasis on extracting the contextual features from the text through intention labels. © 2022 Association for Computing Machinery.",accuracy; BERT (Bidirectional Encoder Representations from Transformers); Classification; ham; intention analysis; SMS; spam; text mining,Natural language processing systems; Semantics; Signal encoding; Text messaging; Text processing; Accuracy; Bidirectional encoder representation from transformer; Embeddings; Ham; Intention analyse; Short message services; Spam; Spam detection; Text-mining; Textual features; Embeddings
An Extended Ultimatum Game for Multi-Party Access Control in Social Networks,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141077755&doi=10.1145%2f3555351&partnerID=40&md5=35d08ae2cebe89db8040f03108553629,"In this article, we aim to answer an important set of questions about the potential longitudinal effects of repeated sharing and privacy settings decisions over jointly managed content among users in a social network.We model user interactions through a repeated game in a network graph. We present a variation of the one-shot Ultimatum Game, wherein individuals interact with peers to make a decision on a piece of shared content. The outcome of this game is either success or failure, wherein success implies that a satisfactory decision for all parties is made and failure instead implies that the parties could not reach an agreement. Our proposed game is grounded in empirical data about individual decisions in repeated pairwise negotiations about jointly managed content in a social network. We consider both a ""continuous""privacy model as well the ""discrete""case of a model wherein privacy values are to be chosen among a fixed set of options. We formally demonstrate that over time, the system converges toward a ""fair""state, wherein each individual's preferences are accounted for. Our discrete model is validated by way of a user study, where participants are asked to propose privacy settings for own shared content from a small, discrete set of options. © 2022 Association for Computing Machinery.",Datasets; gaze detection; neural networks; text tagging,Social networking (online); User interfaces; Dataset; Gaze detection; Longitudinal effect; Neural-networks; Privacy Settings; Set of questions; Shared contents; Text tagging; Ultimatum game; User interaction; Access control
"Fake News Propagation: A Review of Epidemic Models, Datasets, and Insights",2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137044472&doi=10.1145%2f3522756&partnerID=40&md5=e8b21882e6a4ceaffc13fa7889698bcc,"Fake news propagation is a complex phenomenon influenced by a multitude of factors whose identification and impact assessment is challenging. Although many models have been proposed in the literature, the one capturing all the properties of a real fake-news propagation phenomenon is inevitably still missing. Modern propagation models, mainly inspired by old epidemiological models, attempt to approximate the fake-news propagation phenomena by blending psychological factors, social relations, and user behavior.This work provides an in-depth analysis of the current state of fake-news propagation models supported by real-world datasets. We highlighted similarities and differences in the modeling approaches, wrapping up the main research trends. Propagation models, transitions, network topologies, and performance metrics have been identified and discussed in detail. The thorough analysis we provided in this article, coupled with the highlighted research hints, have a high potential to pave the way for future research in the area. © 2022 Association for Computing Machinery.",epidemiological models; Fake news; fake news propagation; social networks,Behavioral research; Blending; Fake detection; Social networking (online); Epidemic modeling; Epidemiological modeling; Fake news; Fake news propagation; Impact assessments; Propagation models; Property; Psychological factors; Social network; Still missing; Network topology
Queryable Compression on Time-evolving Web and Social Networks with Streaming,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131130211&doi=10.1145%2f3495012&partnerID=40&md5=3b7f16e0e966b1d56eaa7a530e1a3742,"Time-evolving web and social network graphs are modeled as a set of pages/individuals (nodes) and their arcs (links/relationships) that change over time. Due to their popularity, they have become increasingly massive in terms of their number of nodes, arcs, and lifetimes. However, these graphs are extremely sparse throughout their lifetimes. For example, it is estimated that Facebook has over a billion vertices, yet at any point in time, it has far less than 0.001% of all possible relationships. The space required to store these large sparse graphs may not fit in most main memories using underlying representations such as a series of adjacency matrices or adjacency lists.We propose building a compressed data structure that has a compressed binary tree corresponding to each row of each adjacency matrix of the time-evolving graph. We do not explicitly construct the adjacency matrix, and our algorithms take the time-evolving arc list representation as input for its construction. Our compressed structure allows for directed and undirected graphs, faster arc and neighborhood queries, as well as the ability for arcs and frames to be added and removed directly from the compressed structure (streaming operations). We use publicly available network data sets such as Flickr, Yahoo!, and Wikipedia in our experiments and show that our new technique performs as well or better than our benchmarks on all datasets in terms of compression size and other vital metrics.  © 2021 Association for Computing Machinery.",Binary tree; Graph compression; Graphs; Online social networks; Queries; Streaming; Temporal; Time-evolving,Binary trees; Directed graphs; Graphic methods; Matrix algebra; Trees (mathematics); Undirected graphs; Adjacency matrix; Change-over time; Facebook; Graph compressions; Link relationships; Networks/graphs; Query; Streaming; Temporal; Time-evolving; Social networking (online)
My Tweets Bring All the Traits to the Yard: Predicting Personality and Relational Traits in Online Social Networks,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131120158&doi=10.1145%2f3523749&partnerID=40&md5=f11a139c4da58bb79e5648cf649304f3,"Users in Online Social Networks (OSNs,) leave traces that reflect their personality characteristics. The study of these traces is important for several fields, such as social science, psychology, marketing, and others. Despite a marked increase in research on personality prediction based on online behavior, the focus has been heavily on individual personality traits, and by doing so, largely neglects relational facets of personality. This study aims to address this gap by providing a prediction model for holistic personality profiling in OSNs that includes socio-relational traits (attachment orientations) in combination with standard personality traits. Specifically, we first designed a feature engineering methodology that extracts a wide range of features (accounting for behavior, language, and emotions) from the OSN accounts of users. Subsequently, we designed a machine learning model that predicts trait scores of users based on the extracted features. The proposed model architecture is inspired by characteristics embedded in psychology; i.e, it utilizes interrelations among personality facets and leads to increased accuracy in comparison with other state-of-the-art approaches. To demonstrate the usefulness of this approach, we applied our model on two datasets, namely regular OSN users and opinion leaders on social media, and contrast both samples' psychological profiles. Our findings demonstrate that the two groups can be clearly separated by focusing on both Big Five personality traits and attachment orientations. The presented research provides a promising avenue for future research on OSN user characterization and classification.  © 2022 Copyright held by the owner/author(s).",Machine learning; Online behavior; Personality prediction; Social networks; User profiling,Contrast media; E-learning; Forecasting; Machine learning; User profile; Engineering methodology; Feature engineerings; Online behaviours; Personality characteristic; Personality predictions; Personality traits; Prediction modelling; Prediction-based; Social network; User's profiling; Social networking (online)
Toward Fair Recommendation in Two-sided Platforms,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131116759&doi=10.1145%2f3503624&partnerID=40&md5=5ef4b67b4c84aa76d109830fb3dfa58c,"Many online platforms today (such as Amazon, Netflix, Spotify, LinkedIn, and AirBnB) can be thought of as two-sided markets with producers and customers of goods and services. Traditionally, recommendation services in these platforms have focused on maximizing customer satisfaction by tailoring the results according to the personalized preferences of individual customers. However, our investigation reinforces the fact that such customer-centric design of these services may lead to unfair distribution of exposure to the producers, which may adversely impact their well-being. However, a pure producer-centric design might become unfair to the customers. As more and more people are depending on such platforms to earn a living, it is important to ensure fairness to both producers and customers. In this work, by mapping a fair personalized recommendation problem to a constrained version of the problem of fairly allocating indivisible goods, we propose to provide fairness guarantees for both sides. Formally, our proposed FairRec algorithm guarantees Maxi-Min Share of exposure for the producers, and Envy-Free up to One Item fairness for the customers. Extensive evaluations over multiple real-world datasets show the effectiveness of FairRec in ensuring two-sided fairness while incurring a marginal loss in overall recommendation quality. Finally, we present a modification of FairRec (named as FairRecPlus) that at the cost of additional computation time, improves the recommendation performance for the customers, while maintaining the same fairness guarantees.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Envy-freeness; Fair allocation; Fair recommendation; Maximin share; Multi-stakeholder recommendation; Two-sided markets,Commerce; Customer satisfaction; Database systems; Knowledge management; Envy-freeness; Fair allocation; Fair recommendation; Fairness guarantee; Maximin; Maximin share; Multi-stakeholder; Multi-stakeholder recommendation; Two-sided markets; Two-sided platforms; Sales
Measuring International Online Human Values with Word Embeddings,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131145898&doi=10.1145%2f3501306&partnerID=40&md5=a0202350ec89f5ba32ef2f07c5e64648,"As the Internet grows in number of users and in the diversity of services, it becomes more influential on peoples lives. It has the potential of constructing or modifying the opinion, the mental perception, and the values of individuals. What is being created and published online is a reflection of people's values and beliefs. As a global platform, the Internet is a great source of information for researching the online culture of many different countries. In this work we develop a methodology for measuring data from textual online sources using word embedding models, to create a country-based online human values index that captures cultural traits and values worldwide. Our methodology is applied with a dataset of 1.7 billion tweets, and then we identify their location among 59 countries. We create a list of 22 Online Values Inquiries (OVI), each one capturing different questions from the World Values Survey, related to several values such as religion, science, and abortion. We observe that our methodology is indeed capable of capturing human values online for different counties and different topics. We also show that some online values are highly correlated (up to c = 0.69, p < 0.05) with the corresponding offline values, especially religion-related ones. Our method is generic, and we believe it is useful for social sciences specialists, such as demographers and sociologists, that can use their domain knowledge and expertise to create their own Online Values Inquiries, allowing them to analyze human values in the online environment.  © 2021 Association for Computing Machinery.",Countries; Culture; Internet; Online social networks; Values; Word embeddings,Domain Knowledge; Social networking (online); Country; Culture; Embeddings; Human values; Measuring data; Online sources; Sources of informations; Value; Values and beliefs; Word embedding; Embeddings
A Large-scale Empirical Analysis of Ransomware Activities in Bitcoin,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130471406&doi=10.1145%2f3494557&partnerID=40&md5=370c2055bdd66e02b7dfb2cb9f31b419,"Exploiting the anonymous mechanism of Bitcoin, ransomware activities demanding ransom in bitcoins have become rampant in recent years. Several existing studies quantify the impact of ransomware activities, mostly focusing on the amount of ransom. However, victims' reactions in Bitcoin that can well reflect the impact of ransomware activities are somehow largely neglected. Besides, existing studies track ransom transfers at the Bitcoin address level, making it difficult for them to uncover the patterns of ransom transfers from a macro perspective beyond Bitcoin addresses.In this article, we conduct a large-scale analysis of ransom payments, ransom transfers, and victim migrations in Bitcoin from 2012 to 2021. First, we develop a fine-grained address clustering method to cluster Bitcoin addresses into users, which enables us to identify more addresses controlled by ransomware criminals. Second, motivated by the fact that Bitcoin activities and their participants already formed stable industries, such as Darknet and Miner, we train a multi-label classification model to identify the industry identifiers of users. Third, we identify ransom payment transactions and then quantify the amount of ransom and the number of victims in 63 ransomware activities. Finally, after we analyze the trajectories of ransom transferred across different industries and track victims' migrations across industries, we find out that to obscure the purposes of their transfer trajectories, most ransomware criminals (e.g., operators of Locky and Wannacry) prefer to spread ransom into multiple industries instead of utilizing the services of Bitcoin mixers. Compared with other industries, Investment is highly resilient to ransomware activities in the sense that the number of users in Investment remains relatively stable. Moreover, we also observe that a few victims become active in the Darknet after paying ransom. Our findings in this work can help authorities deeply understand ransomware activities in Bitcoin. While our study focuses on ransomware, our methods are potentially applicable to other cybercriminal activities that have similarly adopted bitcoins as their payments.  © 2021 Association for Computing Machinery.",Bitcoin transactions; Clustering; Ransomware,Bitcoin; Cluster analysis; Crime; Malware; Bitcoin transaction; Clustering methods; Clusterings; Darknets; Empirical analysis; Fine grained; Large-scale analysis; Large-scales; Payment transactions; Well reflects; Classification (of information)
A Large-scale Empirical Analysis of Browser Fingerprints Properties for Web Authentication,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123992709&doi=10.1145%2f3478026&partnerID=40&md5=963f25b7be816121dc99682781249cbb,"Modern browsers give access to several attributes that can be collected to form a browser fingerprint. Although browser fingerprints have primarily been studied as a web tracking tool, they can contribute to improve the current state of web security by augmenting web authentication mechanisms. In this article, we investigate the adequacy of browser fingerprints for web authentication. We make the link between the digital fingerprints that distinguish browsers, and the biological fingerprints that distinguish Humans, to evaluate browser fingerprints according to properties inspired by biometric authentication factors. These properties include their distinctiveness, their stability through time, their collection time, their size, and the accuracy of a simple verification mechanism. We assess these properties on a large-scale dataset of 4,145,408 fingerprints composed of 216 attributes and collected from 1,989,365 browsers. We show that, by time-partitioning our dataset, more than 81.3% of our fingerprints are shared by a single browser. Although browser fingerprints are known to evolve, an average of 91% of the attributes of our fingerprints stay identical between two observations, even when separated by nearly six months. About their performance, we show that our fingerprints weigh a dozen of kilobytes and take a few seconds to collect. Finally, by processing a simple verification mechanism, we show that it achieves an equal error rate of 0.61%. We enrich our results with the analysis of the correlation between the attributes and their contribution to the evaluated properties. We conclude that our browser fingerprints carry the promise to strengthen web authentication mechanisms. © 2021 Association for Computing Machinery.",Browser fingerprinting; multi-factor authentication; web authentication,Large dataset; 'current; Authentication mechanisms; Browser fingerprinting; Browser fingerprints; Empirical analysis; Large-scales; Multi-factor authentication; Property; Simple++; Web authentication; Authentication
Factorizing Historical User Actions for Next-Day Purchase Prediction,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124012699&doi=10.1145%2f3468227&partnerID=40&md5=25fa94669683cb7306838cb9471c119e,"It is common practice for many large e-commerce operators to analyze daily logged transaction data to predict customer purchase behavior, which may potentially lead to more effective recommendations and increased sales. Traditional recommendation techniques based on collaborative filtering, although having gained success in video and music recommendation, are not sufficient to fully leverage the diverse information contained in the implicit user behavior on e-commerce platforms. In this article, we analyze user action records in the Alibaba Mobile Recommendation dataset from the Alibaba Tianchi Data Lab, as well as the Retailrocket recommender system dataset from the Retail Rocket website. To estimate the probability that a user will purchase a certain item tomorrow, we propose a new model called Time-decayed Multifaceted Factorizing Personalized Markov Chains (Time-decayed Multifaceted-FPMC), taking into account multiple types of user historical actions not only limited to past purchases but also including various behaviors such as clicks, collects and add-to-carts. Our model also considers the time-decay effect of the influence of past actions. To learn the parameters in the proposed model, we further propose a unified framework named Bayesian Sparse Factorization Machines. It generalizes the theory of traditional Factorization Machines to a more flexible learning structure and trains the Time-decayed Multifaceted-FPMC with the Markov Chain Monte Carlo method. Extensive evaluations based on multiple real-world datasets demonstrate that our proposed approaches significantly outperform various existing purchase recommendation algorithms. © 2021 Association for Computing Machinery.",factorization machine; factorizing personalized Markov chains; Markov chain Monte Carlo; matrix factorization; Online purchase prediction; recommendation systems,Behavioral research; Collaborative filtering; Data mining; Electronic commerce; Factorization; Markov processes; Monte Carlo methods; Online systems; Recommender systems; Sales; E- commerces; Factorization machines; Factorizing personalized markov chain; Markov chain Monte Carlo; Markov Chain Monte-Carlo; Matrix factorizations; Online purchase prediction; Recommendation techniques; Transaction data; User action; Forecasting
How Do Home Computer Users Browse the Web?,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123990131&doi=10.1145%2f3473343&partnerID=40&md5=3cf67097bf0e695055398df452898888,"With the ubiquity of web tracking, information on how people navigate the internet is abundantly collected yet, due to its proprietary nature, rarely distributed. As a result, our understanding of user browsing primarily derives from small-scale studies conducted more than a decade ago. To provide an broader updated perspective, we analyze data from 257 participants who consented to have their home computer and browsing behavior monitored through the Security Behavior Observatory. Compared to previous work, we find a substantial increase in tabbed browsing and demonstrate the need to include tab information for accurate web measurements. Our results confirm that user browsing is highly centralized, with 50% of internet use spent on 1% of visited websites. However, we also find that users spend a disproportionate amount of time on low-visited websites, areas with a greater likelihood of containing risky content. We then identify the primary gateways to these sites and discuss implications for future research. © 2021 Copyright held by the owner/author(s).",measurement; personal computers; user behavior; Web browsing,Behavioral research; Web browsers; Websites; Browsing behaviour; Centralised; Computer users; Internet use; Small scale; Tabbed browsing; User behaviors; Web browsing; Web measurements; Personal computers
On the Aggression Diffusion Modeling and Minimization in Twitter,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124004918&doi=10.1145%2f3486218&partnerID=40&md5=3a7a47a73112f9f15bb8af9e04cad4d9,"Aggression in online social networks has been studied mostly from the perspective of machine learning, which detects such behavior in a static context. However, the way aggression diffuses in the network has received little attention as it embeds modeling challenges. In fact, modeling how aggression propagates from one user to another is an important research topic, since it can enable effective aggression monitoring, especially in media platforms, which up to now apply simplistic user blocking techniques.In this article, we address aggression propagation modeling and minimization in Twitter, since it is a popular microblogging platform at which aggression had several onsets. We propose various methods building on two well-known diffusion models, Independent Cascade (IC) and Linear Threshold (LT), to study the aggression evolution in the social network. We experimentally investigate how well each method can model aggression propagation using real Twitter data, while varying parameters, such as seed users selection, graph edge weighting, users' activation timing, and so on. It is found that the best performing strategies are the ones to select seed users with a degree-based approach, weigh user edges based on their social circles' overlaps, and activate users according to their aggression levels. We further employ the best performing models to predict which ordinary real users could become aggressive (and vice versa) in the future, and achieve up to AUC = 0.89 in this prediction task. Finally, we investigate aggression minimization by launching competitive cascades to ""inform""and ""heal""aggressors. We show that IC and LT models can be used in aggression minimization, providing less intrusive alternatives to the blocking techniques currently employed by Twitter. © 2021 Association for Computing Machinery.",aggression minimization; aggression modeling; cascades; immunization; information diffusion; Social networks,Diffusion; Integrated circuits; User profile; Aggression minimization; Aggression modeling; Blocking technique; Cascade; Diffusion model; Embed model; Facts modeling; Information diffusion; Minimisation; Social network; Social networking (online)
Context-aware Distance Measures for Dynamic Networks,2022,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124026384&doi=10.1145%2f3476228&partnerID=40&md5=e95a3b26b8079ea220baac13d3d9eccc,"Dynamic networks are widely used in the social, physical, and biological sciences as a concise mathematical representation of the evolving interactions in dynamic complex systems. Measuring distances between network snapshots is important for analyzing and understanding evolution processes of dynamic systems. To the best of our knowledge, however, existing network distance measures are designed for static networks. Therefore, when measuring the distance between any two snapshots in dynamic networks, valuable context structure information existing in other snapshots is ignored. To guide the construction of context-aware distance measures, we propose a context-aware distance paradigm, which introduces context information to enrich the connotation of the general definition of network distance measures. A Context-aware Spectral Distance (CSD) is then given as an instance of the paradigm by constructing a context-aware spectral representation to replace the core component of traditional Spectral Distance (SD). In a node-aligned dynamic network, the context effectively helps CSD gain mainly advantages over SD as follows: (1) CSD is not affected by isospectral problems; (2) CSD satisfies all the requirements of a metric, while SD cannot; and (3) CSD is computationally efficient. In order to process large-scale networks, we develop a kCSD that computes top-k eigenvalues to further reduce the computational complexity of CSD. Although kCSD is a pseudo-metric, it retains most of the advantages of CSD. Experimental results in two practical applications, i.e., event detection and network clustering in dynamic networks, show that our context-aware spectral distance performs better than traditional spectral distance in terms of accuracy, stability, and computational efficiency. In addition, context-aware spectral distance outperforms other baseline methods. © 2021 Association for Computing Machinery.",Dynamic networks; matrix perturbation; network distance; spectral distance,Biology; Clustering algorithms; Complex networks; Eigenvalues and eigenfunctions; Biological science; Context-Aware; Distance measure; Dynamic complex system; Dynamic network; Mathematical representations; Matrix perturbation; Network distance; Physical science; Spectral distances; Computational efficiency
Sequential Learning-based IaaS Composition,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171694962&doi=10.1145%2f3452332&partnerID=40&md5=cd84a1df666423e8d79a566a7ae5b7a6,We propose a novel Infrastructure-as-a-Service composition framework that selects an optimal set of consumer requests according to the provider's qualitative preferences on long-term service provisions. Decision variables are included in the temporal conditional preference networks to represent qualitative preferences for both short-term and long-term consumers. The global preference ranking of a set of requests is computed using a k-d tree indexing-based temporal similarity measure approach. We propose an extended three-dimensional Q-learning approach to maximize the global preference ranking. We design the on-policy-based sequential selection learning approach that applies the length of request to accept or reject requests in a composition. The proposed on-policy-based learning method reuses historical experiences or policies of sequential optimization using an agglomerative clustering approach. Experimental results prove the feasibility of the proposed framework. © 2021 Association for Computing Machinery.,agglomerative clustering; IaaS composition; policy reuse Q-learning; sequential optimization; temporal CP-nets,Cluster analysis; Learning systems; Reinforcement learning; Agglomerative clustering; CP-nets; Iaa composition; Policy reuse Q-learning; Policy-based; Preference ranking; Q-learning; Reuse; Sequential optimization; Temporal CP-net; Infrastructure as a service (IaaS)
Studying and Understanding Characteristics of Post-Syncing Practice and Goal in Social Network Sites,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110511758&doi=10.1145%2f3457986&partnerID=40&md5=05cda692fb02120f2a612d4a04501cf2,"Many popular social network sites (SNSs) provide the post-syncing functionality, which allows users to synchronize posts automatically among different SNSs. Nowadays there exists divergence on this functionality from the view of sink SNS. The key to solving this problem is to understand the characteristics of users' post-syncing practice and goals and evaluate whether they are consistent with an SNS's norms, cultures, and goals. However, studying and understanding the characteristics of post-syncing practice and goal are challenging tasks as a result of the difficulty of data sampling and the complexity of post-syncing behavior. In this article, we focus on investigating this question by quantitative analysis in combination with qualitative analysis. In the quantitative study, by utilizing 211,233 synced-posts sampled from Weibo, we aim to investigate characteristics of post-syncing from three perspectives: user, content, and goal. The results suggest that post-syncing plays an important role in exhibiting one's current activities, creations, and skills as well as advertisements but involves a risk of exhibiting personal sensitive profiles. To understand the results, we present an interview-based qualitative study based on thematic analysis. It indicates that the publicity, urgency, and remarkableness of contents and differences of social affordances and social circles between sink SNS and source SNS as well as the one-time consent of post-syncing authentication jointly account for the major role of post-syncing. Based on these results, we propose insights for post-syncing functionality's adoption, design, and promotion.  © 2021 Association for Computing Machinery.",content sharing; post-syncing; Social network sites; Weibo,Computer networks; Internet; Affordances; Data sampling; Qualitative analysis; Qualitative study; Quantitative study; Social circles; Social Network Sites; Thematic analysis; Social networking (online)
Sampling Graphlets of Multiplex Networks: A Restricted Random Walk Approach,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110422773&doi=10.1145%2f3456291&partnerID=40&md5=732b85d2e39e0a8f370fc18a77171df9,"Graphlets are induced subgraph patterns that are crucial to the understanding of the structure and function of a large network. A lot of effort has been devoted to calculating graphlet statistics where random walk-based approaches are commonly used to access restricted graphs through the available application programming interfaces (APIs). However, most of them merely consider individual networks while overlooking the strong coupling between different networks. In this article, we estimate the graphlet concentration in multiplex networks with real-world applications. An inter-layer edge connects two nodes in different layers if they actually belong to the same node. The access to a multiplex network is restrictive in the sense that the upper layer allows random walk sampling, whereas the nodes of lower layers can be accessed only through the inter-layer edges and only support random node or edge sampling. To cope with this new challenge, we define a suit of two-layer graphlets and propose novel random walk sampling algorithms to estimate the proportion of all the three-node graphlets. An analytical bound on the sampling steps is proved to guarantee the convergence of our unbiased estimator. We further generalize our algorithm to explore the tradeoff between the estimated accuracy of different graphlets when the sample budget is split into different layers. Experimental evaluation on real-world and synthetic multiplex networks demonstrates the accuracy and high efficiency of our unbiased estimators.  © 2021 Association for Computing Machinery.",graph sampling; Graphlets; multiplex network; random walk; unbiased estimation,Application programming interfaces (API); Budget control; Analytical bounds; Different layers; Experimental evaluation; Individual network; Induced subgraphs; Multiplex networks; Sampling algorithm; Unbiased estimator; Random processes
Categorizing Sexism and Misogyny through Neural Approaches,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110474136&doi=10.1145%2f3457189&partnerID=40&md5=b44ac8864c70800a6eb5b0ae4d021ec7,"Sexism, an injustice that subjects women and girls to enormous suffering, manifests in blatant as well as subtle ways. In the wake of growing documentation of experiences of sexism on the web, the automatic categorization of accounts of sexism has the potential to assist social scientists and policymakers in studying and thereby countering sexism. The existing work on sexism classification has certain limitations in terms of the categories of sexism used and/or whether they can co-occur. To the best of our knowledge, this is the first work on the multi-label classification of sexism of any kind(s).1 We also consider the related task of misogyny classification. While sexism classification is performed on textual accounts describing sexism suffered or observed, misogyny classification is carried out on tweets perpetrating misogyny. We devise a novel neural framework for classifying sexism and misogyny that can combine text representations obtained using models such as Bidirectional Encoder Representations from Transformers with distributional and linguistic word embeddings using a flexible architecture involving recurrent components and optional convolutional ones. Further, we leverage unlabeled accounts of sexism to infuse domain-specific elements into our framework. To evaluate the versatility of our neural approach for tasks pertaining to sexism and misogyny, we experiment with adapting it for misogyny identification. For categorizing sexism, we investigate multiple loss functions and problem transformation techniques to address the multi-label problem formulation. We develop an ensemble approach using a proposed multi-label classification model with potentially overlapping subsets of the category set. Proposed methods outperform several deep-learning as well as traditional machine learning baselines for all three tasks.  © 2021 Association for Computing Machinery.",machine learning; misogyny classification; misogyny detection; multi-label classification; neural networks; Sexism classification; text classification,Recurrent neural networks; Automatic categorization; Ensemble approaches; Flexible architectures; Multi label classification; Multi-label problems; Problem transformations; Recurrent components; Text representation; Classification (of information)
Identifying and Evaluating Anomalous Structural Change-based Nodes in Generalized Dynamic Social Networks,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110430153&doi=10.1145%2f3457906&partnerID=40&md5=87120379e76fc59561737f9a141a72ce,"Recently, dynamic social network research has attracted a great amount of attention, especially in the area of anomaly analysis that analyzes the anomalous change in the evolution of dynamic social networks. However, most of the current research focused on anomaly analysis of the macro representation of dynamic social networks and failed to analyze the nodes that have anomalous structural changes at a micro level. To identify and evaluate anomalous structural change-based nodes in generalized dynamic social networks that only have limited structural information, this research considers undirected and unweighted graphs and develops a multiple-neighbor superposition similarity method (), which mainly consists of a multiple-neighbor range algorithm () and a superposition similarity fluctuation algorithm (). introduces observation nodes, characterizes the structural similarities of nodes within multiple-neighbor ranges, and proposes a new multiple-neighbor similarity index on the basis of extensional similarity indices. Subsequently, maximally reflects the structural change of each node, using a new superposition similarity fluctuation index from the perspective of diverse multiple-neighbor similarities. As a result, based on and , not only identifies anomalous structural change-based nodes by detecting the anomalous structural changes of nodes but also evaluates their anomalous degrees by quantifying these changes. Results obtained by comparing with state-of-the-art methods via extensive experiments show that can accurately identify anomalous structural change-based nodes and evaluate their anomalous degrees well.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Anomalous structural change-based node; generalized dynamic social network; structural similarity,Graph algorithms; Anomaly analysis; Dynamic social networks; Micro level; Similarity indices; State-of-the-art methods; Structural information; Structural similarity; Unweighted graphs; Graph theory
Who Has the Last Word? Understanding How to Sample Online Discussions,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126351552&doi=10.1145%2f3452936&partnerID=40&md5=ee555bb51f938fb76cf6b40c6f6a3cc4,"In online debates, as in offline ones, individual utterances or arguments support or attack each other, leading to some subset of arguments (potentially from different sides of the debate) being considered more relevant than others. However, online conversations are much larger in scale than offline ones, with often hundreds of thousands of users weighing in, collaboratively forming large trees of comments by starting from an original post and replying to each other. In large discussions, readers are often forced to sample a subset of the arguments being put forth. Since such sampling is rarely done in a principled manner, users may not read all the relevant arguments to get a full picture of the debate from a sample. This article is interested in answering the question of how users should sample online conversations to selectively favour the currently justified or accepted positions in the debate. We apply techniques from argumentation theory and complex networks to build a model that predicts the probabilities of the normatively justified arguments given their location in idealised online discussions of comments and replies, which we represent as trees. Our model shows that the proportion of replies that are supportive, the distribution of the number of replies that comments receive, and the locations of comments that do not receive replies (i.e., the ""leaves""of the reply tree) all determine the probability that a comment is a justified argument given its location. We show that when the distribution of the number of replies is homogeneous along the tree length, for acrimonious discussions (with more attacking comments than supportive ones), the distribution of justified arguments depends on the parity of the tree level, which is the distance from the root expressed as number of edges. In supportive discussions, which have more supportive comments than attacks, the probability of having justified comments increases as one moves away from the root. For discussion trees that have a non-homogeneous in-degree distribution, for supportive discussions we observe the same behaviour as before, while for acrimonious discussions we cannot observe the same parity-based distribution. This is verified with data obtained from the online debating platform Kialo. By predicting the locations of the justified arguments in reply trees, we can therefore suggest which arguments readers should sample, to grasp the currently accepted opinions in such discussions. Our models have important implications for the design of future online debating platforms. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Argumentation theory; graph sampling; Kialo; online discussions; probabilistic analysis,Complex networks; Probability distributions; Social networking (online); Argumentation theory; As numbers; Graph samplings; In-degree distribution; Kialo; Non-homogeneous; Offline; Online discussions; Probabilistic analysis; Tree level; Location
Cookie Banners and Privacy Policies: Measuring the Impact of the GDPR on the Web,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110434456&doi=10.1145%2f3466722&partnerID=40&md5=fed64a3ba26606594d371d4fc0814977,"The General Data Protection Regulation (GDPR) is in effect since May of 2018. As one of the most comprehensive pieces of legislation concerning privacy, it sparked a lot of discussion on the effect it would have on users and providers of online services in particular, due to the large amount of personal data processed in this context. Almost three years later, we are interested in revisiting this question to summarize the impact this new regulation has had on actors in the World Wide Web. Using Scopus, we obtain a vast corpus of academic work to survey studies related to changes on websites since and around the time the GDPR went into force. Our findings show that the emphasis on privacy increased w.r.t. online services, but plenty potential for improvements remains. Although online services are on average more transparent regarding data processing practices in their public data policies, a majority of these policies still either lack information required by the GDPR (e.g., contact information for users to file privacy inquiries) or do not provide this information in a user-friendly form. Additionally, we summarize that online services more often provide means for their users to opt out of data processing, but regularly obstruct convenient access to such means through unnecessarily complex and sometimes illegitimate interface design. Our survey further details that this situation contradicts the preferences expressed by users both verbally and through their actions, and researchers have proposed multiple approaches to facilitate GDPR-conform data processing without negatively impacting the user experience. Thus, we compiled reoccurring points of criticism by privacy researchers and data protection authorities into a list of four guidelines for service providers to consider.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cookies; fingerprinting; GDPR; privacy; privacy legislation; web,Information use; Laws and legislation; Surveys; User experience; Academic work; General data protection regulations; Interface designs; Large amounts; On-line service; Privacy policies; Service provider; User friendly; Privacy by design
Semantic Table Retrieval Using Keyword and Table Queries,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128575245&doi=10.1145%2f3441690&partnerID=40&md5=f6ca29131552bf1a46ac785191af9f54,"Tables on the Web contain a vast amount of knowledge in a structured form. To tap into this valuable resource, we address the problem of table retrieval: answering an information need with a ranked list of tables. We investigate this problem in two different variants, based on how the information need is expressed: as a keyword query or as an existing table (""query-by-table""). The main novel contribution of this work is a semantic table retrieval framework for matching information needs (keyword or table queries) against tables. Specifically, we (i) represent queries and tables in multiple semantic spaces (both discrete sparse and continuous dense vector representations) and (ii) introduce various similarity measures for matching those semantic representations. We consider all possible combinations of semantic representations and similarity measures and use these as features in a supervised learning model. Using two purpose-built test collections based on Wikipedia tables, we demonstrate significant and substantial improvements over state-of-the-art baselines. © 2021 Association for Computing Machinery.",table retrieval; Table search,Information retrieval; Semantic Web; Semantics; Vector spaces; Keyword queries; Matchings; Retrieval frameworks; Semantic representation; Semantic Space; Semantic tables; Similarity measure; Table retrieval; Table search; Vector representations; Taps
"Dynamic, Incremental, and Continuous Detection of Cyberbullying in Online Social Media",2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171695534&doi=10.1145%2f3448014&partnerID=40&md5=c8065b1eecac46098432b754f93e55f6,"The potentially detrimental effects of cyberbullying have led to the development of numerous automated, data-driven approaches, with emphasis on classification accuracy. Cyberbullying, as a form of abusive online behavior, although not well-defined, is a repetitive process, i.e., a sequence of aggressive messages sent from a bully to a victim over a period of time with the intent to harm the victim. Existing work has focused on harassment (i.e., using profanity to classify toxic comments independently) as an indicator of cyberbullying, disregarding the repetitive nature of this harassing process. However, raising a cyberbullying alert immediately after an aggressive comment is detected can lead to a high number of false positives. At the same time, two key practical challenges remain unaddressed: (i) detection timeliness, which is necessary to support victims as early as possible, and (ii) scalability to the staggering rates at which content is generated in online social networks. In this work, we introduce CONcISE, a novel approach for timely and accurate Cyberbullying detectiON in online social media SEssions. CONcISE is a two-stage online approach designed to reduce the time to raise a cyberbullying alert by sequentially examining comments as they become available over time, and minimizing the number of feature evaluations necessary for a decision to be made for each comment. Extensive experiments on a real-world Instagram dataset with users and comments demonstrate the effectiveness, scalability, and timeliness of our approach and its benefits over existing methods. Additional experiments using a Twitter dataset offer evidence in support of the potential generalizability of CONcISE to other social media platforms. © 2021 Association for Computing Machinery.",Classification; cyberharassment; optimization; sequential selection; social networks,Computer crime; Social networking (online); Automated data; Continuous detections; Cyber bullying; Cyber-harassment; Dynamic detection; Dynamic incremental; Online social medias; Optimisations; Sequential selection; Social network; Scalability
Welcome Message from the New Editor-in-Chief,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174074585&doi=10.1145%2f3456294&partnerID=40&md5=f128e5b710103aefc32214c5f0c5b751,[No abstract available],,
Review Summary Generation in Online Systems: Frameworks for Supervised and Unsupervised Scenarios,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108456543&doi=10.1145%2f3448015&partnerID=40&md5=a08d204818f07a7870551085f32cf690,"In online systems, including e-commerce platforms, many users resort to the reviews or comments generated by previous consumers for decision making, while their time is limited to deal with many reviews. Therefore, a review summary, which contains all important features in user-generated reviews, is expected. In this article, we study ""how to generate a comprehensive review summary from a large number of user-generated reviews.""This can be implemented by text summarization, which mainly has two types of extractive and abstractive approaches. Both of these approaches can deal with both supervised and unsupervised scenarios, but the former may generate redundant and incoherent summaries, while the latter can avoid redundancy but usually can only deal with short sequences. Moreover, both approaches may neglect the sentiment information. To address the above issues, we propose comprehensive Review Summary Generation frameworks to deal with the supervised and unsupervised scenarios. We design two different preprocess models of re-ranking and selecting to identify the important sentences while keeping users' sentiment in the original reviews. These sentences can be further used to generate review summaries with text summarization methods. Experimental results in seven real-world datasets (Idebate, Rotten Tomatoes Amazon, Yelp, and three unlabelled product review datasets in Amazon) demonstrate that our work performs well in review summary generation. Moreover, the re-ranking and selecting models show different characteristics. © 2021 Association for Computing Machinery.",review summary generation; supervised and unsupervised scenarios; text summarization; User-generated review,Decision making; Text processing; Commerce platforms; E- commerces; Re-ranking; Review summary generation; Summary generation; Supervised and unsupervised scenario; System framework; Text Summarisation; User-generated; User-generated review; Online systems
"An integrated approach for improving brand consistency of web content: Modeling, analysis, and recommendation",2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106972129&doi=10.1145%2f3450445&partnerID=40&md5=f48a4902e8aa26f1ab458e05847116f2,"A consumer-dependent (business-to-consumer) organization tends to present itself as possessing a set of human qualities, which is termed the brand personality of the company. The perception is impressed upon the consumer through the content, be it in the form of advertisement, blogs, or magazines, produced by the organization. A consistent brand will generate trust and retain customers over time as they develop an affinity toward regularity and common patterns. However, maintaining a consistent messaging tone for a brand has become more challenging with the virtual explosion in the amount of content that needs to be authored and pushed to the Internet to maintain an edge in the era of digital marketing. To understand the depth of the problem, we collect around 300K web page content from around 650 companies. We develop trait-specific classification models by considering the linguistic features of the content. The classifier automatically identifies the web articles that are not consistent with the mission and vision of a company and further helps us to discover the conditions under which the consistency cannot be maintained. To address the brand inconsistency issue, we then develop a sentence ranking system that outputs the top three sentences that need to be changed for making a web article more consistent with the company’s brand personality. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Brand personality; Online reputation management; Sentence ranking; Text classification,Linguistics; Websites; Business to Consumer; Classification models; Digital marketing; Integrated approach; Linguistic features; Ranking system; Web content; Classification (of information)
A two phases self-healing framework for service-oriented systems,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106972873&doi=10.1145%2f3450443&partnerID=40&md5=b4b8132cab73189799a4767c88c2bcba,"Service-Oriented Architectures (SOA) enable the automatic creation of business applications from independently developed and deployed Web services. As Web services are inherently a priori unknown, how to deliver reliable Web services compositions is a significant and challenging problem. Services involved in an SOA often do not operate under a single processing environment and need to communicate using different protocols over a network. Under such conditions, designing a fault management system that is both efficient and extensible is a challenging task. In this article, we propose SFSS, a self-healing framework for SOA fault management. SFSS is predicting, identifying, and solving faults in SOAs. In SFSS, we identified a set of high-level exception handling strategies based on the QoS performances of different component services and the preferences articled by the service consumers. Multiple recovery plans are generated and evaluated according to the performance of the selected component services, and then we execute the best recovery plan. We assess the overall user dependence (i.e., the service is independent of other services) using the generated plan and the available invocation information of the component services. Due to the experiment results, the given technique enhances the service selection quality by choosing the services that have the highest score and betters the overall system performance. The experiment results indicate the applicability of SFSS and show improved performance in comparison to similar approaches. © 2021 Association for Computing Machinery.",Exception handling; Fault management; QoS; Selection tree; Self-healing; Semantic attribute; Similarity module; SOA,Failure analysis; Information services; Quality of service; Self-healing materials; Service oriented architecture (SOA); Websites; Automatic creations; Business applications; Different protocols; Exception handling; Fault management system; Processing environments; Service Oriented Systems; Web services composition; Web services
Utilizing web trackers for sybil defense,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106985389&doi=10.1145%2f3450444&partnerID=40&md5=ad2440d076e426c56537ff080dfd5421,"User tracking has become ubiquitous practice on the Web, allowing services to recommend behaviorally targeted content to users. In this article, we design Alibi, a system that utilizes such readily available personalized content, generated by recommendation engines in real time, as a means to tame Sybil attacks. In particular, by using ads and other tracker-generated recommendations as implicit user “certificates,” Alibi is capable of creating meta-profiles that allow for rapid and inexpensive validation of users’ uniqueness, thereby enabling an Internet-wide Sybil defense service. We demonstrate the feasibility of such a system, exploring the aggregate behavior of recommendation engines on the Web and demonstrating the richness of the meta-profile space defined by such inputs. We further explore the fundamental properties of such meta-profiles, i.e., their construction, uniqueness, persistence, and resilience to attacks. By conducting a user study, we show that the user meta-profiles are robust and show important scaling effects. We demonstrate that utilizing even a moderate number of popular Web sites empowers Alibi to tame large-scale Sybil attacks. © 2021 Association for Computing Machinery.",Recommendation engines; Sybil attacks; User tracking,Engines; Network security; Recommender systems; Aggregate behavior; Defense services; Fundamental properties; Personalized content; Scaling effects; Sybil attack; User study; User tracking; Computer crime
Exploring weather data to predict activity attendance in event-based social network: From the organizer’s view,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105489312&doi=10.1145%2f3440134&partnerID=40&md5=0a97372f1eec63b2647bd78fa7808d1c,"Event-based social networks (EBSNs) connect online and offline lives. They allow online users with similar interests to get together in real life. Attendance prediction for activities in EBSNs has attracted a lot of attention and several factors have been studied. However, the prediction accuracy is not very good for some special activities, such as outdoor activities. Moreover, a very important factor, the weather, has not been well exploited. In this work, we strive to understand how the weather factor impacts activity attendance, and we explore it to improve attendance prediction from the organizer’s view. First, we classify activities into two categories: the outdoor and the indoor activities. We study the different ways that weather factors may impact these two kinds of activities. We also introduce a new factor of event duration. By integrating the above factors with user interest and user-event distance, we build a model of attendance prediction with the weather named GBT-W, based on the Gradient Boosting Tree. Furthermore, we develop a platform to help event organizers estimate the possible number of activity attendance with different settings (e.g., different weather, location) to effectively plan their events. We conduct extensive experiments, and the results show that our method has a better prediction performance on both the outdoor and the indoor activities, which validates the reasonability of considering weather and duration. © 2021 Association for Computing Machinery.",Attendance prediction; Event classification; Event-based social networks; Weather factors,Social networking (online); Gradient boosting; Indoor activities; Outdoor activities; Prediction accuracy; Prediction performance; Similar Interests; User interests; Weather factors; Weather forecasting
Cross-site prediction on social influence for cold-start users in online social networks,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106982897&doi=10.1145%2f3409108&partnerID=40&md5=2420187b641d796fdd139fa294d63e2a,"Online social networks (OSNs) have become a commodity in our daily life. As an important concept in sociology and viral marketing, the study of social influence has received a lot of attentions in academia. Most of the existing proposals work well on dominant OSNs, such as Twitter, since these sites are mature and many users have generated a large amount of data for the calculation of social influence. Unfortunately, cold-start users on emerging OSNs generate much less activity data, which makes it challenging to identify potential influential users among them. In this work, we propose a practical solution to predict whether a cold-start user will become an influential user on an emerging OSN, by opportunistically leveraging the user’s information on dominant OSNs. A supervised machine learning-based approach is adopted, transferring the knowledge of both the descriptive information and dynamic activities on dominant OSNs. Descriptive features are extracted from the public data on a user’s homepage. In particular, to extract useful information from the fine-grained dynamic activities that cannot be represented by the statistical indices, we use deep learning technologies to deal with the sequential activity data. Using the real data of millions of users collected from Twitter (a dominant OSN) and Medium (an emerging OSN), we evaluate the performance of our proposed framework to predict prospective influential users. Our system achieves a high prediction performance based on different social influence definitions. © 2021 Association for Computing Machinery.",,Data mining; Deep learning; Economic and social effects; Forecasting; Sociology; Supervised learning; Descriptive information; On-line social networks; Online social networks (OSNs); Practical solutions; Prediction performance; Sequential activities; Statistical indices; Supervised machine learning; Social networking (online)
CBPCS: A cache-block-based service process caching strategy to accelerate the execution of service processes,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099085607&doi=10.1145%2f3411494&partnerID=40&md5=9699afb40b57341d1f0ecc3a384d229e,"With the development of cloud computing and the advent of the Web 2.0 era, composing a set of Web services as a service process is becoming a common practice to provide more functional services. However, a service process involves multiple service invocations over the network, which incurs a huge time cost and could become a bottleneck to performance. To accelerate its execution, we propose an engine-side cache-block-based service process caching strategy (CBPCS). It is based on, and derives its advantages from, three key ideas. First, the invocation of Web services embodies semantics, which enables the application of semantic-based caching. Second, cache blocks are identified from a service process, and each block is equipped with a separate cache so that the time overhead of service invocation and caching can be minimized. Third, a replacement strategy is introduced taking into account time and space factors to manage the space allocation for a process with multiple caches. The algorithms and methods used in CBPCS are introduced in detail. Moreover, how CBPCS can be applied to multiple service process models is also investigated. Finally, CBPCS is validated via comparison experiments, which shows the considerable improvements of CBPCS over other strategies.  © 2020 ACM.",cache replacement; process caching; semantic caching; Service process; web service,Semantics; Websites; Cache blocks; Caching strategy; Functional services; Multiple services; Replacement strategy; Service invocation; Service process; Space allocation; Web services
Investigating and modeling theweb elements' visual feature influence on free-viewing attention,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099082379&doi=10.1145%2f3409474&partnerID=40&md5=d9c3a7c2f14aaf432eb7074750ac5e29,"User attentional analyses on web elements help in synthesis and rendering of webpages. However, majority of the existing analyses are limited in incorporating the intrinsic visual features of text and images. This study aimed to analyze the influence of elements' visual features (font-size, font-family, color, etc., for text; and brightness, color, intensity, etc., for images) besides their position on users' free-viewing visual attention. The investigation includes: (i) user's position-based attention allocation on text and image web elements, (ii) identification of informative visual features with respect to the attention, (iii) performance of informative visual features in predicting the ordinal visual attention (fixation-indices). Towards the study, an eye-tracking experiment was conducted with 42 participants on 36 real-world webpages. The analyses revealed: (i) Though users predominantly allocate the initial attention to MiddleCenter}, MiddleLeft, TopCenter, TopLeft regions, the elements in Right and Bottom regions are not completely ignored; (ii) Space-related (column-gap, line-height, padding) and font Size-related (font-size, font-weight) intrinsic text features, and Mid-level Color Histogram intrinsic image features are informative, while position and size are informative for both the types; (iii) the informative visual features predict the ordinal visual attention on an element with 90% average accuracy and 70% micro-F1 score. Our approach finds applications in element-granular web-designing and user attention prediction.  © 2020 ACM.",eye-tracking; intrinsic visual features; support vector machine (SVM); visual attention; Web elements,Behavioral research; Color; Eye tracking; Forecasting; Websites; Bottom regions; Color histogram; Intrinsic images; Text feature; User attention; Visual Attention; Visual feature; Web designing; Image analysis
Decoupled Variational Embedding for Signed Directed Networks,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099109802&doi=10.1145%2f3408298&partnerID=40&md5=448b72df7a00779f316046b43e31894b,"Node representation learning for signed directed networks has received considerable attention in many real-world applications such as link sign prediction, node classification, and node recommendation. The challenge lies in how to adequately encode the complex topological information of the networks. Recent studies mainly focus on preserving the first-order network topology that indicates the closeness relationships of nodes. However, these methods generally fail to capture the high-order topology that indicates the local structures of nodes and serves as an essential characteristic of the network topology. In addition, for the first-order topology, the additional value of non-existent links is largely ignored. In this article, we propose to learn more representative node embeddings by simultaneously capturing the first-order and high-order topology in signed directed networks. In particular, we reformulate the representation learning problem on signed directed networks from a variational auto-encoding perspective and further develop a decoupled variational embedding (DVE) method. DVE leverages a specially designed auto-encoder structure to capture both the first-order and high-order topology of signed directed networks, and thus learns more representative node embeddings. Extensive experiments are conducted on three widely used real-world datasets. Comprehensive results on both link sign prediction and node recommendation task demonstrate the effectiveness of DVE. Qualitative results and analysis are also given to provide a better understanding of DVE.  © 2020 ACM.",Decoupled variational embedding; graph convolution; network embedding; signed directed networks,Directed graphs; Embeddings; Encoding (symbols); Learning systems; Signal encoding; Closeness relationship; Directed network; Essential characteristic; Learning problem; Local structure; Network topology; Real-world datasets; Topological information; Topology
A Structured and Linguistic Approach to Understanding Recovery and Relapse in AA,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099093800&doi=10.1145%2f3423208&partnerID=40&md5=dc0502da6c1115ead8f8e0872821373f,"Alcoholism, also known as Alcohol Use Disorder (AUD), is a serious problem affecting millions of people worldwide. Recovery from AUD is known to be challenging and often leads to relapse at various points after enrolling in a rehabilitation program such as Alcoholics Anonymous (AA). In this work, we present a structured and linguistic approach using hinge-loss Markov random fields (HL-MRFs) to understand recovery and relapse from AUD using social media data. We evaluate our models on AA-attending users extracted from: (i) the Twitter social network and predict recovery at two different points - 90 days and 1 year after the user joins AA, respectively, and (ii) the Reddit AA recovery forums and predict whether the participating user is currently sober. The two datasets present two facets of the same underlying problem of understanding recovery and relapse in AUD users. We flesh out different characteristics in both these datasets: (i) In the Twitter dataset, we focus on the social aspect of the users and the relationship with recovery and relapse, and (ii) in the Reddit dataset, we focus on modeling the linguistic topics and dependency structure to understand users' recovery journey. We design a unified modeling framework using HL-MRFs that takes the different characteristics of both these platforms into account. Our experiments reveal that our structured and linguistic approach is helpful in predicting recovery in users in both these datasets. We perform extensive quantitative analysis of different groups of features and dependencies among them in both datasets. The interpretable and intuitive nature of our models and analysis is helpful in making meaningful predictions and can potentially be helpful in identifying and preventing relapse early.  © 2020 ACM.",alcoholics anonymous; modeling recovery from alcoholism; probabilistic graphical models; Social media analysis,Forecasting; Linguistics; Markov processes; Social aspects; Social networking (online); Structural frames; Dependency structures; Linguistic approach; Markov Random Fields; Rehabilitation programs; Social media datum; Twitter social networks; Unified Modeling; Recovery
Attributed Collaboration Network Embedding for Academic Relationship Mining,2021,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097502912&doi=10.1145%2f3409736&partnerID=40&md5=77784dd0c87cbd7afcd78efe389c9f39,"Finding both efficient and effective quantitative representations for scholars in scientific digital libraries has been a focal point of research. The unprecedented amounts of scholarly datasets, combined with contemporary machine learning and big data techniques, have enabled intelligent and automatic profiling of scholars from this vast and ever-increasing pool of scholarly data. Meanwhile, recent advance in network embedding techniques enables us to mitigate the challenges of large scale and sparsity of academic collaboration networks. In real-world academic social networks, scholars are accompanied with various attributes or features, such as co-authorship and publication records, which result in attributed collaboration networks. It has been observed that both network topology and scholar attributes are important in academic relationship mining. However, previous studies mainly focus on network topology, whereas scholar attributes are overlooked. Moreover, the influence of different scholar attributes are unclear. To bridge this gap, in this work, we present a novel framework of Attributed Collaboration Network Embedding (ACNE) for academic relationship mining. ACNE extracts four types of scholar attributes based on the proposed scholar profiling model, including demographics, research, influence, and sociability. ACNE can learn a low-dimensional representation of scholars considering both scholar attributes and network topology simultaneously. We demonstrate the effectiveness and potentials of ACNE in academic relationship mining by performing collaborator recommendation on two real-world datasets and the contribution and importance of each scholar attribute on scientific collaborator recommendation is investigated. Our work may shed light on academic relationship mining by taking advantage of attributed collaboration network embedding.  © 2020 ACM.",academic information retrieval; graph learning; Network embedding; scientific collaboration,Digital libraries; Large dataset; Topology; Co-authorships; Collaboration network; Focal points; In networks; Low-dimensional representation; Network topology; Real-world datasets; Scientific digital libraries; Embeddings
Early Detection of Social Media Hoaxes at Scale,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091049181&doi=10.1145%2f3407194&partnerID=40&md5=1db76a9faefa89debec732e04873d95b,"The unmoderated nature of social media enables the diffusion of hoaxes, which in turn jeopardises the credibility of information gathered from social media platforms. Existing research on automated detection of hoaxes has the limitation of using relatively small datasets, owing to the difficulty of getting labelled data. This, in turn, has limited research exploring early detection of hoaxes as well as exploring other factors such as the effect of the size of the training data or the use of sliding windows. To mitigate this problem, we introduce a semi-automated method that leverages the Wikidata knowledge base to build large-scale datasets for veracity classification, focusing on celebrity death reports. This enables us to create a dataset with 4,007 reports including over 13M tweets, 15% of which are fake. Experiments using class-specific representations of word embeddings show that we can achieve F1 scores nearing 72% within 10 minutes of the first tweet being posted when we expand the size of the training data following our semi-automated means. Our dataset represents a realistic scenario with a real distribution of true, commemorative, and false stories, which we release for further use as a benchmark in future research.  © 2020 ACM.",Datasets; hoax detection; social media; veracity classification,Automation; Knowledge based systems; Large dataset; Social networking (online); Automated detection; Automated methods; Knowledge base; Large-scale datasets; Real distribution; Realistic scenario; Small data set; Social media platforms; Classification (of information)
Emotions behind Drive-by Download Propagation on Twitter,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091020219&doi=10.1145%2f3408894&partnerID=40&md5=3d43f7c7fc1776731b0500bbe3fc2e44,"Twitter has emerged as one of the most popular platforms to get updates on entertainment and current events. However, due to its 280-character restriction and automatic shortening of URLs, it is continuously targeted by cybercriminals to carry out drive-by download attacks, where a user's system is infected by merely visiting a Web page. Popular events that attract a large number of users are used by cybercriminals to infect and propagate malware by using popular hashtags and creating misleading tweets to lure users to malicious Web pages. A drive-by download attack is carried out by obfuscating a malicious URL in an enticing tweet and used as clickbait to lure users to a malicious Web page. In this article, we answer the following two questions: Why are certain malicious tweets retweeted more than others? Do emotions reflecting in a tweet drive virality? We gathered tweets from seven different sporting events over 3 years and identified those tweets that were used to carry to out a drive-by download attack. From the malicious (N = 105, 642) and benign (N = 169, 178) data sample identified, we built models to predict information flow size and survival. We define size as the number of retweets of an original tweet, and survival as the duration of the original tweet's presence in the study window. We selected the zero-truncated negative binomial (ZTNB) regression method for our analysis based on the distribution exhibited by our dependent size measure and the comparison of results with other predictive models. We used the Cox regression technique to model the survival of information flows as it estimates proportional hazard rates for independent measures. Our results show that both social and content factors are statistically significant for the size and survival of information flows for both malicious and benign tweets. In the benign data sample, positive emotions and positive sentiment reflected in the tweet significantly predict size and survival. In contrast, for the malicious data sample, negative emotions, especially fear, are associated with both size and survival of information flows.  © 2020 ACM.",Cyber security; cybercrime; drive-by download; machine learning; malware,Digital storage; Predictive analytics; Regression analysis; Social networking (online); Websites; Drive-by-download; Information flows; Malicious web pages; Negative binomial; Positive emotions; Predictive models; Proportional hazards; Regression method; Malware
Cold-start Point-of-interest Recommendation through Crowdsourcing,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091045680&doi=10.1145%2f3407182&partnerID=40&md5=0310042bec951f9164b98e8dcbf8f8d8,"Recommender system is a popular tool that aims to provide personalized suggestions to user about items, products, services, and so on. Recommender system has effectively been used in online social networks, especially the location-based social networks for providing suggestions for interesting places known as POIs (points-of-interest). Popular recommender systems explore historical data to learn users' preferences and, subsequently, they recommend locations to an active user. This strategy faces a major problem when a new POI or business evolves in a city. New business has no historical user experience data. Thus, a recommender system fails to gather enough knowledge about the new businesses, resulting in ignoring them during recommendations. This scenario is popularly known as a cold-start POI problem. Users never get recommendations of the new businesses in a city even though they can be relevant to a user. Also, from a business owner's perspective, such a recommendation strategy does not help its reachability among users. Therefore, it is important for a recommender system to remain updated with new businesses in a city and ensure that all relevant POIs are recommended to a user irrespective of their lifetime. A POI recommendation approach is proposed in this work that can effectively handle the new businesses, or the cold-start POI problem, in a city. We crowdsource descriptions of cold-start POIs from various online social networks. The reviews of users are exploited here to learn the inherent features at the existing POIs and the new crowdsourced POIs. Finally, the proposed approach recommends top-K POIs consisting of the existing and new POIs. We perform experiments on the real-world Yelp dataset, which is one of the largest available data resources containing details on a wide range of businesses, users, and reviews. The proposed approach is compared with four existing POI recommendation approaches. The obtained results show that our approach outperforms others in handling cold-start POIs.  © 2020 ACM.",clustering; crowdsourcing; Recommender systems; Yelp network,Crowdsourcing; Social networking (online); User experience; Business owners; Data resources; Historical data; Location-based social networks; On-line social networks; Points of interest; Reachability; Recommendation strategies; Recommender systems
"Analyzing Genetic Testing Discourse on the Web through the Lens of Twitter, Reddit, and 4chan",2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091045860&doi=10.1145%2f3404994&partnerID=40&md5=b3c01aca71b184004a5f390d736ebe3b,"Recent progress in genomics has enabled the emergence of a flourishing market for direct-to-consumer (DTC) genetic testing. Companies like 23andMe and AncestryDNA provide affordable health, genealogy, and ancestry reports, and have already tested tens of millions of customers. Consequently, news, experiences, and views on genetic testing are increasingly shared and discussed on social media. At the same time, far-right groups have also taken an interest in genetic testing, using them to attack minorities and prove their genetic ""purity.""In this article, we set to study the genetic testing discourse on a number of mainstream and fringe Web communities. We do so in two steps. First, we conduct an exploratory, large-scale analysis of the genetic testing discourse on a mainstream social network such as Twitter. We find that the genetic testing discourse is fueled by accounts that appear to be interested in digital health and technology. However, we also identify tweets with highly racist connotations. This motivates us to explore the connection between genetic testing and racism on platforms with a reputation for toxicity, namely, Reddit and 4chan, where we find that discussions around genetic testing often include highly toxic language expressed through hateful and racist comments. In particular, on 4chan's politically incorrect board (/pol/), content from genetic testing conversations involves several alt-right personalities and openly anti-semitic rhetoric, often conveyed through memes.  © 2020 ACM.",4chan; ancestry testing; Genetic testing; Internet measurement; Reddit; Twitter,Computer networks; Internet; Genetic testing; Genomics; Large-scale analysis; Recent progress; Social media; Through the lens; Web community; Social networking (online)
Dynamic Offloading of Web Application Execution Using Snapshot,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091029106&doi=10.1145%2f3402124&partnerID=40&md5=b2f15baf58c683d3cc43fa55749bc8d5,"Mobile web platforms are facing new demands for emerging applications, such as machine learning or augmented reality, which require significant computing powers beyond that of current mobile hardware. Computation offloading can accelerate these apps by offloading the computation-intensive parts of an app from a client to a powerful server. Unfortunately, previous studies of offloading in the field of web apps have a limitation for the offloading target code or require complex user annotations, hindering the widespread use of offloading in web apps. This article proposes a novel offloading system for web apps, which can simplify the offloading process by sending and receiving the execution state of a running web app in the form of another web app called the snapshot. Since running the snapshot restores the whole app state and continues the execution from the point where it was saved, we can offload regular web app computations that affect the DOM state as well as the JavaScript state, and we do not have to pre-install the app binary at the server. Moreover, the snapshot does not require any annotations to be captured, making computation offloading more transparent to app developers. We qualitatively compared the proposed system with previous approaches in terms of programming difficulty and the scope of offloadable codes. In addition, we implemented the proposed system based on a WebKit browser and evaluated the offloading performance with five computation-intensive web apps. Our system achieved significant speedup (from 1.7 to approximately 9.0) in all of the apps, compared to local execution, which proves the feasibility of the proposed approach.  © 2020 ACM.",computation offloading; Mobile computing; web application,Augmented reality; Computation intensives; Computation offloading; Computing power; Emerging applications; Mobile hardware; Offloading system; User annotations; WEB application; Codes (symbols)
SMINT: Toward interpretable and robust model sharing for deep neural networks,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092721616&doi=10.1145%2f3381833&partnerID=40&md5=90d5da9c98d4dbb2115f5cb4d352c547,"Sharing a pre-trained machine learning model, particularly a deep neural network via prediction APIs, is becoming a common practice on machine learning as a service (MLaaS) platforms nowadays. Although deep neural networks (DNN) have shown remarkable successes in many tasks, they are also criticized for the lack of interpretability and transparency. Interpreting a shared DNN model faces two additional challenges compared with interpreting a general model. (1) Limited training data can be disclosed to users. (2) The internal structure of the models may not be available. These two challenges impede the application of most existing interpretability approaches, such as saliency maps or influence functions, for DNN models. Case-based reasoning methods have been used for interpreting decisions; however, how to select and organize the data points under the constraints of shared DNN models is not discussed. Moreover, simply providing cases as explanations may not be sufficient for supporting instance level interpretability. Meanwhile, existing interpretation methods for DNN models generally lack the means to evaluate the reliability of the interpretation. In this article, we propose a framework named Shared Model INTerpreter (SMINT) to address the above limitations. We propose a new data structure called a boundary graph to organize training points to mimic the predictions of DNN models. We integrate local features, such as saliency maps and interpretable input masks, into the data structure to help users to infer the model decision boundaries. We show that the boundary graph is able to address the reliability issues in many local interpretation methods. We further design an algorithm named hidden-layer aware p-test to measure the reliability of the interpretations. Our experiments show that SMINT is able to achieve above 99% fidelity to corresponding DNN models on both MNIST and ImageNet by sharing only a tiny fraction of training data to make these models interpretable. The human pilot study demonstrates that SMINT provides better interpretability compared with existing methods. Moreover, we demonstrate that SMINT is able to assist model tuning for better performance on different user data.  © 2020 ACM.",decision boundary; Deep neural networks; interpretability; model sharing,Case based reasoning; Data structures; Deep learning; Deep neural networks; Platform as a Service (PaaS); Predictive analytics; Reliability; Influence functions; Internal structure; Interpretability; Interpretation methods; Limited training data; Local interpretation; Machine learning models; Modeling decisions; Neural networks
QoS-aware Automatic Web Service Composition with Multiple Objectives,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092706584&doi=10.1145%2f3389147&partnerID=40&md5=370377c50c92ebcd154fea7ff9be6b27,"Automatic web service composition has received a significant research attention in service-oriented computing over decades of research. With increasing number of web services, providing an end-to-end Quality of Service (QoS) guarantee in responding to user queries is becoming an important concern. Multiple QoS parameters (e.g., response time, latency, throughput, reliability, availability, success rate) are associated with a service, thereby, service composition with a large number of candidate services is a challenging multi-objective optimization problem. In this article, we study the multi-constrained multi-objective QoS-aware web service composition problem and propose three different approaches to solve the same, one optimal, based on Pareto front construction, and two others based on heuristically traversing the solution space. We compare the performance of the heuristics against the optimal and show the effectiveness of our proposals over other classical approaches for the same problem setting, with experiments on WSC-2009 and ICEBE-2005 datasets.  © 2020 ACM.",multi-objective; Pareto optimal; Quality of Service (QoS); Service composition,Multiobjective optimization; Quality of service; Websites; Automatic web service compositions; Classical approach; End-to-end quality of service; Multi-objective optimization problem; Multiple-objectives; Service compositions; Service oriented computing; Web service composition; Web services
Roaming through the Castle Tunnels,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092692763&doi=10.1145%2f3395050&partnerID=40&md5=eaea5f96cf98cd537766bd833f38fad8,"Smartphone applications (a.k.a., apps) have become indispensable in our everyday life and work. In practice, accomplishing a task on smartphones may require the user to navigate among various apps. Unlike Web pages that are inherently interconnected through hyperlinks, apps are usually isolated building blocks, and the lack of direct links between apps has compromised the efficiency of task completion and user experience. In this article, we present the first in-depth empirical study of page-level access behaviors of smartphone users based on a comprehensive dataset collected through an extensive user study. We propose a model to distinguish informational pages and transitional pages, based on which we can extract page-level inter-app navigation. Surprisingly, the transitional pages account for quite substantial time cost and manual actions when navigating from the current informational page to the desirable informational page. We reveal that developing ""tunnels""between ""isolated""apps under specific usage scenarios has a huge potential to reduce the cost of navigation. Our analysis provides some practical implications on how to improve app-navigation experience from both the operating system's perspective and the developer's<?brk?> perspective.  © 2020 ACM.",empirical study; inter-app navigation; Mobile apps; user experience,Hypertext systems; Navigation; Smartphones; Websites; Direct links; Empirical studies; Hyperlinks; Isolated buildings; Smart-phone applications; Time cost; Usage scenarios; User study; User experience
On Scalability of Association-rule-based Recommendation,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092687963&doi=10.1145%2f3398202&partnerID=40&md5=1761f6b70f41b1d8755734ef5fef9507,"The association-rule-based approach is one of the most common technologies for building recommender systems and it has been extensively adopted for commercial use. A variety of techniques, mainly including eligible rule selection and multiple rules combination, have been developed to create effective recommendation. Unfortunately, little attention has been paid to the scalability concern of rule-based recommendation methods. However, the computational complexity of rule-based methods shall increase drastically with the growth of both online customers and rules, which are usually several millions in typical e-commerce platforms. Moreover, the dynamic change of users' actions requires rule-based methods make recommendations in nearly real-time, which further highlights the scalability issue of rule-based recommender systems. In this article, we present a distributed framework that can scale different association-rule-based recommendation methods in a unified way. Specifically, based on the summarization of existing rule-based approaches, a generic tree-type structure is defined to store separate kinds of patterns, and an efficient algorithm is designed for mining eligible patterns along with computing recommendation scores. To handle the ever-increasing number of online customers, a distributed framework is proposed, where two load-balanced strategies for partitioning tree are put forward to fit sparse and dense data, respectively. Extensive experiments on five real-life data sets demonstrate that the efficiency of association-rule-based recommender systems can be significantly improved by the proposed framework.  © 2020 ACM.",association rule; distributed computing; frequent pattern; load balanced partitioning; Recommender system,Association rules; Electronic commerce; Forestry; Recommender systems; Scalability; Trees (mathematics); Distributed framework; Dynamic changes; Online customers; Real life datasets; Recommendation methods; Rule-based approach; Rule-based method; Scalability issue; Real time systems
PatternRank+NN: A Ranking Framework bringing user behaviors into entity set expansion fromweb searchqueries,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092719276&doi=10.1145%2f3386042&partnerID=40&md5=be6998ada5f90ada1acf9f972ad0e24a,"We propose a ranking framework, called PatternRank+NN, for expanding a set of seed entities of a particular class (i.e., entity set expansion) from Web search queries. PatternRank+NN consists of two parts: PatternRank and NN. Unlike the traditional methods, PatternRank brings user behaviors into entity set expansion from Web search queries. PatternRank is a Markov chain which simulates the Web search query process of users on the graph model for Web search query log, and ranks the features of the class. The features in the front rank are used to generate candidate entities of the class. NN, a ranking strategy called Nearest Neighbor, ranks these candidate entities such that the set of seed entities can be expanded from the candidate entities in the front rank. Our experiments demonstrate the superior performance of PatternRank+NN in comparison with the state-of-the-art methods.  © 2020 ACM.",Entity set expansion; Markov chain; Nearest Neighbor; User behavior,Behavioral research; Markov chains; Websites; Graph model; Nearest neighbors; Ranking strategy; Set expansions; State-of-the-art methods; User behaviors; Web search queries; Information retrieval
Image Privacy Prediction Using Deep Neural Networks,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090842678&doi=10.1145%2f3386082&partnerID=40&md5=45bbef8372c961bde81b43b5b008ede3,"Images today are increasingly shared online on social networking sites such as Facebook, Flickr, and Instagram. Image sharing occurs not only within a group of friends but also more and more outside a user's social circles for purposes of social discovery. Despite that current social networking sites allow users to change their privacy preferences, this is often a cumbersome task for the vast majority of users on the Web, who face difficulties in assigning and managing privacy settings. When these privacy settings are used inappropriately, online image sharing can potentially lead to unwanted disclosures and privacy violations. Thus, automatically predicting images' privacy to warn users about private or sensitive content before uploading these images on social networking sites has become a necessity in our current interconnected world. In this article, we explore learning models to automatically predict appropriate images' privacy as private or public using carefully identified image-specific features. We study deep visual semantic features that are derived from various layers of Convolutional Neural Networks (CNNs) as well as textual features such as user tags and deep tags generated from deep CNNs. Particularly, we extract deep (visual and tag) features from four pre-trained CNN architectures for object recognition, i.e., AlexNet, GoogLeNet, VGG-16, and ResNet, and compare their performance for image privacy prediction. The results of our experiments obtained on a Flickr dataset of 32,000 images show that ResNet yeilds the best results for this task among all four networks. We also fine-tune the pre-trained CNN architectures on our privacy dataset and compare their performance with the models trained on pre-trained features. The results show that even though the overall performance obtained using the fine-tuned networks is comparable to that of pre-trained networks, the fine-tuned networks provide an improved performance for the private class. The results also show that the learning models trained on features extracted from ResNet outperform the state-of-the-art models for image privacy prediction. We further investigate the combination of user tags and deep tags derived from CNN architectures using two settings: (1) Support Vector Machines trained on the bag-of-tags features and (2) text-based CNN. We compare these models with the models trained on ResNet visual features and show that, even though the models trained on the visual features perform better than those trained on the tag features, the combination of deep visual features with image tags shows improvements in performance over the individual feature sets. We also compare our models with prior privacy prediction approaches and show that for private class, we achieve an improvement of ≈ 10% over prior CNN-based privacy prediction approaches. Our code, features, and the dataset used in experiments are available at https://github.com/ashwinitonge/deepprivate.git.  © 2020 ACM.",deep learning; image analysis; image privacy prediction; Social networks,Architecture; Convolutional neural networks; Deep learning; Deep neural networks; E-learning; Forecasting; HTTP; Image enhancement; Learning systems; Multilayer neural networks; Network architecture; Object recognition; Semantics; Support vector machines; Individual features; Privacy preferences; Privacy Settings; Privacy violation; Social networking sites; State of the art; Textual features; Visual semantics; Social networking (online)
Browser Fingerprinting: A Survey,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097271856&doi=10.1145%2f3386040&partnerID=40&md5=0e8eba76e34fc61b0c89a2f8d3c5df87,"With this article, we survey the research performed in the domain of browser fingerprinting, while providing an accessible entry point to newcomers in the field. We explain how this technique works and where it stems from. We analyze the related work in detail to understand the composition of modern fingerprints and see how this technique is currently used online. We systematize existing defense solutions into different categories and detail the current challenges yet to overcome.  © 2020 ACM.",Browser fingerprinting; user privacy; web tracking,Computer networks; Internet; Defense solutions; Entry point; Related works; Surveys
Topic-aware Web Service Representation Learning,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087782732&doi=10.1145%2f3386041&partnerID=40&md5=58ceb14626cb408d47ef3d089e15a8a3,"The advent of Service-Oriented Architecture (SOA) has brought a fundamental shift in the way in which distributed applications are implemented. An overwhelming number of Web-based services (e.g., APIs and Mashups) have leveraged this shift and furthered development. Applications designed with SOA principles are typically characterized by frequent dependencies with one another in the form of heterogeneous networks, i.e., annotation relations between tags and services, and composition relations between Mashups and APIs. Although prior work has shown the utility gained by exploring these networks, their analysis is still in its infancy. This article develops an approach to learning representations of the Web service network, which seeks to embed Web services in low-dimensional continuous vectors with preserved information of the network structure, functional tags, and service descriptions, such that services with similar functional properties and network structures are mapped together in the learned latent space. We first propose a topic generative model for constructing two topic distribution networks (Mashup-Topic and API-Topic) from the service content. Then, we present an efficient optimization process to derive low-dimensional vector representations of Web services from a tri-layer bipartite network with the Mashup-Topic and API-Topic networks on two ends and the Mashup-API composition network in the middle. Experiments on real-word datasets have verified that our approach is effective to learn robust low-rank service representations, i.e., 25% F1-measure gain over the state-of-the-art in Web service recommendation task.  © 2020 ACM.",Mashups; network embedding; probabilistic topic model; service representation; Web services,Application programming interfaces (API); Heterogeneous networks; Information services; Learning to rank; Network layers; Service oriented architecture (SOA); Vector spaces; Websites; Bipartite network; Distributed applications; Functional properties; Generative model; Network structures; Service description; Web service recommendations; Web-based service; Web services
An experimental study of automatic detection and measurement of counterfeit in brand search results,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079774149&doi=10.1145%2f3378443&partnerID=40&md5=f33e00540ea9bd6706907e6961c460d8,"Brand search results are poisoned by fake ecommerce websites that infringe on the trademark rights of legitimate holders. In this article, we study how to tackle and measure this problem automatically. We present a pipeline with two machine learning stages that can detect the ecommerce websites present in the list of brand search results and distinguish between legitimate and fake ecommerce websites. For each classification task, we identify and extract suitable learning features and study their relative importance. Through a prototype system termed RI.SI.CO., we show that this approach is feasible, fast, and more accurate than both existing systems for trustworthiness assessment and non-expert humans. We next introduce two complementary metrics for evaluating the counterfeit incidence in brand search results: namely, a chart-based and a single-value measure. They allow us to analyze and compare counterfeit at various levels, including single brands within a specific sector as well as whole sectors. Experimenting with two luxury goods sectors, we report a number of interesting findings about how the main search parameters (e.g., search engine, query type, number of search results seen) affect counterfeiting and how this activity changes with time. On the whole, our research offers new insights and some very practical and useful means of analyzing and measuring counterfeit in brand search results, thus increasing awareness of and knowledge about this phenomenon and enabling targeted anti-counterfeiting actions. © 2020 Association for Computing Machinery.",Cybercrime measurement; Online counterfeit goods; Spam detection in web search results; Trustworthiness assessment of eshops; Website classification,Electronic commerce; Search engines; Websites; Anti-counterfeiting; Automatic Detection; Classification tasks; Cybercrime; E-commerce websites; E-shops; Online counterfeit goods; Web search results; Marketing
Time-aspect-sentiment recommendation models based on novel similarity measure methods,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079798335&doi=10.1145%2f3375548&partnerID=40&md5=74f7d8e437e581cc64238d4d9685ce68,"The explosive growth of e-commerce has led to the development of the recommendation system. The recommendation system aims to provide a set of items that meet users' personalized needs through analyzing users' consumption records. However, the timeliness of purchasing data and the implicity of feedback data pose severe challenges for the existing recommendation methods. To alleviate these challenges, we exploit the user's consumption records from the perspectives of user and item, by modeling the data on both item and user level, where the item-level value reflects the grade of item, and the user-level value reflects the user's purchase intention. In this article, we collect the description information and the reviews of the items from public websites, then adopt sentiment analysis techniques to model the similarities on user level and item level, respectively. In particular, we extend the traditional latent factor model and propose two novel methods-Item Level Similarity Matrix Factorization (ILMF) and User Level Similarity Matrix Factorization (ULMF)-by introducing two novel similarity measure methods. In ILMF and ULMF, the consistency between latent factors and explicit aspects is naturally incorporated into learning latent factors of the users and items, such that we can predict the users' preferences on different items more accurately. Moreover, we propose Item-User Level Similarity Matrix Factorization (IULMF), which combines these two methods to study their contributions on the final performance. Experimental evaluations on the real datasets show that our methods outperform the baseline approaches in terms of both the precision and NDCG. © 2020 Association for Computing Machinery.",Aspect; Matrix factorization; Recommendation system; Sentiment analysis; Time,Factorization; Matrix algebra; Sentiment analysis; Analysis techniques; Aspect; Description information; Experimental evaluation; Latent factor models; Matrix factorizations; Recommendation methods; Time; Recommender systems
An outsourcing model for alert analysis in a cybersecurity operations center,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077813662&doi=10.1145%2f3372498&partnerID=40&md5=d5a211b72a475337ddc9806b8b272b12,"A typical Cybersecurity Operations Center (CSOC) is a service organization. It hires and trains analysts, whose task is to perform analysis of alerts that were generated while monitoring the client's networks. Due to ever-increasing financial and infrastructure burden on a CSOC driven by the rapidly growing demand for security services, it would become prohibitively expensive to continually expand the size of a CSOC to meet the demands in the future. An alternative solution is to outsource the alert analysis process to on-demand analysts, to provide scalable CSOC service to its clients with features, such as (1) higher throughput, (2) higher quality, and (3) more economical service than the current in-house service. The current outsourcing model is not cost effective and an exact optimization model is computationally inefficient. This article presents a novel two-step sequential mixed integer programming optimization method that is used in the development of a new decision-support business model for outsourcing the alert analysis process. It is demonstrated that through this model, a CSOC can effectively deliver its alert management services with the above-mentioned features. Results indicate that the model is scalable, computationally viable, real-time implementable, and can deliver CSOC services that meet the service-level agreement (SLA) between the CSOC and its client. In addition, the article provides valuable insights into the cost of operating the new business process outsourcing model for cybersecurity services. © 2020 Association for Computing Machinery.",Alert analysis; CSOC; On-demand analysts; Optimization; Outsourcing,Cost effectiveness; Decision support systems; Integer programming; Optimization; Outsourcing; Alert analysis; Business process outsourcing; CSOC; Mixed integer programming; On demands; Optimization modeling; Service Level Agreements; Service organizations; Quality control
“The best of both worlds!”: Integration of web page and eye tracking data driven approaches for automatic AOI detection,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077818832&doi=10.1145%2f3372497&partnerID=40&md5=e502112b9817f64bc2e569277e2d1fb5,"Web pages are composed of different kinds of elements (menus, adverts, etc.). Segmenting pages into their elements has long been important in understanding how people experience those pages and in making those experiences “better.” Many approaches have been proposed that relate the resultant elements with the underlying source code; however, they do not consider users' interactions. Another group of approaches analyses eye movements of users to discover areas that interest or attract them (i.e., areas of interest or AOIs). Although these approaches consider how users interact with web pages, they do not relate AOIs with the underlying source code. We propose a novel approach that integrates web page and eye tracking data driven approaches for automatic AOI detection. This approach segments an entire web page into its AOIs by considering users' interactions and relates AOIs with the underlying source code. Based on the Adjusted Rand Index measure, our approach provides the most similar segmentation to the ground-truth segmentation compared to its individual components. © 2020 Association for Computing Machinery.",Region of interest; ROI; Segment; Visual block; Visual element; Web page segmentation,Codes (symbols); Computer programming languages; Eye movements; Image segmentation; Websites; Region of interest; Segment; Visual block; Visual elements; Web page segmentation; Eye tracking
A survey of figurative language and its computational detection in online social networks,2020,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079502619&doi=10.1145%2f3375547&partnerID=40&md5=5d11288967537e076df32fe8f91c3150,"The frequent usage of figurative language on online social networks, especially on Twitter, has the potential to mislead traditional sentiment analysis and recommender systems. Due to the extensive use of slangs, bashes, flames, and non-literal texts, tweets are a great source of figurative language, such as sarcasm, irony, metaphor, simile, hyperbole, humor, and satire. Starting with a brief introduction of figurative language and its various categories, this article presents an in-depth survey of the state-of-the-art techniques for computational detection of seven different figurative language categories, mainly on Twitter. For each figurative language category, we present details about the characterizing features, datasets, and state-of-the-art computational detection approaches. Finally, we discuss open challenges and future directions of research for each figurative language category. © 2020 Association for Computing Machinery.",Figurative language; Humor recognition; Hyperbole detection; Irony detection; Metaphor detection; Sarcasm detection; Satire detection; Simile detection; Social network analysis,Online systems; Sentiment analysis; Surveys; Computational detection; Depth surveys; Figurative language; Humor recognition; Non-literal; On-line social networks; State of the art; State-of-the-art techniques; Social networking (online)
Combining URL and HTML features for entity discovery in the web,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077788046&doi=10.1145%2f3365574&partnerID=40&md5=ef0330c32f6ba0975add1b6ae04cd114,"The web is a large repository of entity-pages. An entity-page is a page that publishes data representing an entity of a particular type, for example, a page that describes a driver on a website about a car racing championship. The attribute values published in the entity-pages can be used for many data-driven companies, such as insurers, retailers, and search engines. In this article, we define a novel method, called SSUP, which discovers the entity-pages on the websites. The novelty of our method is that it combines URL and HTML features in a way that allows the URL terms to have different weights depending on their capacity to distinguish entity-pages from other pages, and thus the efficacy of the entity-page discovery task is increased. SSUP determines the similarity thresholds on each website without human intervention. We carried out experiments on a dataset with different real-world websites and a wide range of entity types. SSUP achieved a 95% rate of precision and 85% recall rate. Our method was compared with two state-of-the-art methods and outperformed them with a precision gain between 51% and 66%. © 2019 Association for Computing Machinery.",Crawler; Entity-pages; URL and HTML features; Web structure mining,HTML; Websites; Attribute values; Crawler; Entity-pages; Entity-types; Human intervention; Recall rate; Similarity threshold; Web structure mining; Search engines
Long-term measurement and analysis of the free proxy ecosystem,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077794418&doi=10.1145%2f3360695&partnerID=40&md5=7fed8e740b62304bbf847d449157c021,"Free web proxies promise anonymity and censorship circumvention at no cost. Several websites publish lists of free proxies organized by country, anonymity level, and performance. These lists index hundreds of thousands of hosts discovered via automated tools and crowd-sourcing. A complex free proxy ecosystem has been forming over the years, of which very little is known. In this article, we shed light on this ecosystem via a distributed measurement platform that leverages both active and passive measurements. Active measurements are carried out by an infrastructure we name ProxyTorrent, which discovers free proxies, assesses their performance, and detects potential malicious activities. Passive measurements focus on proxy performance and usage in the wild, and are accomplished by means of a Chrome extension named Ciao. ProxyTorrent has been running since January 2017, monitoring up to 230K free proxies. Ciao was launched in March 2017 and has thus far served roughly 9.7K users and generated 14TB of traffic. Our analysis shows that less than 2% of the proxies announced on the Web indeed proxy traffic on behalf of users; further, only half of these proxies have decent performance and can be used reliably. Every day, around 5%–10% of the active proxies exhibit malicious behaviors, e.g., advertisement injection, TLS interception, and cryptojacking, and these proxies are also the ones providing the best performance. Through the analysis of more than 14TB of proxied traffic, we show that web browsing is the primary user activity. Geo-blocking avoidance—allegedly a popular use case for free web proxies—accounts for 30% or less of the traffic, and it mostly involves countries hosting popular geo-blocked content. © 2019 Copyright held by the owner/author(s).",Network measurements; Proxies; Web protocol security; Web security and privacy,Ecosystems; Network security; Distributed measurements; Long-term measurements; Network measurement; Passive measurements; Primary user activities; Proxies; Web protocols; WEB security; Network protocols
Efficient pairwise penetrating-rank similarity retrieval,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077787394&doi=10.1145%2f3368616&partnerID=40&md5=6fbb8dace9573d153b9af3dc49157205,"Many web applications demand a measure of similarity between two entities, such as collaborative filtering, web document ranking, linkage prediction, and anomaly detection. P-Rank (Penetrating-Rank) has been accepted as a promising graph-based similarity measure, as it provides a comprehensive way of encoding both incoming and outgoing links into assessment. However, the existing method to compute P-Rank is iterative in nature and rather cost-inhibitive. Moreover, the accuracy estimate and stability issues for P-Rank computation have not been addressed. In this article, we consider the optimization techniques for P-Rank search that encompasses its accuracy, stability, and computational efficiency. (1) The accuracy estimation is provided for P-Rank iterations, with the aim to find out the number of iterations, k, required to guarantee a desired accuracy. (2) A rigorous bound on the condition number of P-Rank is obtained for stability analysis. Based on this bound, it can be shown that P-Rank is stable and well-conditioned when the damping factors are chosen to be suitably small. (3) Two matrix-based algorithms, applicable to digraphs and undirected graphs, are, respectively, devised for efficient P-Rank computation, which improves the computational time from O(kn3) to O(υn2 + υ6) for digraphs, and to O(υn2) for undirected graphs, where n is the number of vertices in the graph, and υ ( n) is the target rank of the graph. Moreover, our proposed algorithms can significantly reduce the memory space of P-Rank computations from O(n2) to O(υn + υ4) for digraphs, and to O(υn) for undirected graphs, respectively. Finally, extensive experiments on real-world and synthetic datasets demonstrate the usefulness and efficiency of the proposed techniques for P-Rank similarity assessment on various networks. © 2019 Association for Computing Machinery.",Hyperlink analysis; Optimization; Similarity search; Web document ranking,Anomaly detection; Collaborative filtering; Directed graphs; Efficiency; Graphic methods; Hypertext systems; Information retrieval systems; Iterative methods; Number theory; Optimization; Search engines; Accuracy estimation; Hyperlink analysis; Measure of similarities; Number of iterations; Optimization techniques; Similarity measure; Similarity search; Web document; Computational efficiency
Fast and practical snippet generation for RDF datasets,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075629884&doi=10.1145%2f3365575&partnerID=40&md5=1698ecd23a2c445e6cc4eccb5ae65225,"Triple-structured open data creates value in many ways. However, the reuse of datasets is still challenging. Users feel difficult to assess the usefulness of a large dataset containing thousands or millions of triples. To satisfy the needs, existing abstractive methods produce a concise high-level abstraction of data. Complementary to that, we adopt the extractive strategy and aim to select the optimum small subset of data froma dataset as a snippet to compactly illustrate the content of the dataset. This has been formulated as a combinatorial optimization problem in our previous work. In this article, we design a new algorithm for the problem, which is an order of magnitude faster than the previous one but has the same approximation ratio.We also develop an anytime algorithm that can generate empirically better solutions using additional time. To suit datasets that are partially accessible via online query services (e.g., SPARQL endpoints for RDF data), we adapt our algorithms to trade off quality of snippet for feasibility and efficiency in the Web environment. We carry out extensive experiments based on real RDF datasets and SPARQL endpoints for evaluating quality and running time. The results demonstrate the effectiveness and practicality of our proposed algorithms. © 2019 Copyright held by the owner/author(s).",Dataset summarization; Snippet generation; SPARQL endpoint,Approximation algorithms; Combinatorial optimization; Economic and social effects; Open Data; Semantic Web; Any-time algorithms; Combinatorial optimization problems; Dataset summarization; High-level abstraction; Query service; Snippet generation; SPARQL endpoint; Web environment; Large dataset
Detecting cyberbullying and cyberaggression in social media,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075615767&doi=10.1145%2f3343484&partnerID=40&md5=4a6fe6d542d142b06aaf33e90120a164,"Cyberbullying and cyberaggression are increasingly worrisome phenomena affecting people across all demographics. More than half of young social media users worldwide have been exposed to such prolonged and/or coordinated digital harassment. Victims can experience a wide range of emotions, with negative consequences such as embarrassment, depression, isolation from other community members, which embed the risk to lead to even more critical consequences, such as suicide attempts. In this work, we take the first concrete steps to understand the characteristics of abusive behavior in Twitter, one of today's largest social media platforms. We analyze 1.2 million users and 2.1 million tweets, comparing users participating in discussions around seemingly normal topics like the NBA, to those more likely to be hate-related, such as the Gamergate controversy, or the gender pay inequality at the BBC station. We also explore specific manifestations of abusive behavior, i.e., cyberbullying and cyberaggression, in one of the hate-related communities (Gamergate). We present a robust methodology to distinguish bullies and aggressors from normal Twitter users by considering text, user, and network-based attributes. Using various state-of-The-Art machine-learning algorithms, we classify these accounts with over 90% accuracy and AUC. Finally, we discuss the current status of Twitter user accounts marked as abusive by our methodology and study the performance of potential mechanisms that can be used by Twitter to suspend users in the future. © 2019 Association for Computing Machinery. All rights reserved.",aggression; bullying; Online social networks (OSNs); twitter,Computer crime; Learning algorithms; Machine learning; aggression; bullying; Current status; Online social networks (OSNs); Potential mechanism; Social media platforms; State of the art; twitter; Social networking (online)
Learning Linear Influence Models in Social Networks from Transient Opinion Dynamics,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075552455&doi=10.1145%2f3343483&partnerID=40&md5=3f9b5e50e988b3d06f34a871c259bd96,"Social networks, forums, and socialmedia have emerged as global platforms for forming and shaping opinions on a broad spectrum of topics like politics, sports, and entertainment. Users (also called actors) often update their evolving opinions, influenced through discussions with other users. Theoretical models and their analysis on understanding opinion dynamics in social networks abound in the literature. However, these models are often based on concepts from statistical physics. Their goal is to establish specific phenomena like steady state consensus or bifurcation. Analysis of transient effects is largely avoided. Moreover,many of these studies assume that actors' opinions are observed globally and synchronously, which is rarely realistic. In this article, we initiate an investigation into a family of novel data-driven influence models that accurately learn and fit realistic observations.We estimate and do not presume edge strengths from observed opinions at nodes. Our influence models are linear but not necessarily positive or row stochastic in nature. As a consequence, unlike the previous studies, they do not depend on system stability or convergence during the observation period. Furthermore, our models take into account a wide variety of data collection scenarios. In particular, they are robust to missing observations for several timesteps after an actor has changed its opinion. In addition, we consider scenarios where opinion observations may be available only for aggregated clusters of nodes-A practical restriction often imposed to ensure privacy. Finally, to provide a conceptually interpretable design of edge influence, we offer a relatively frugal variant of our influence model, where the strength of influence between two connecting nodes depends on the node attributes (demography, personality, expertise, etc.). Such an approach reduces the number of model parameters, reduces overfitting, and offers a tractable and explicable sketch of edge influences in the context of opinion dynamics. With six real-life datasets crawled from Twitter and Reddit, as well as three more datasets collected from in-house experiments (with 102 volunteers), our proposed system gives a significant accuracy boost over four state-of-The-Art baselines. © 2019 Association for Computing Machinery. All rights reserved.",influence modeling; opinion dynamics; Social networks,Dynamics; Social networking (online); Statistical Physics; Stochastic systems; System stability; Influence model; Missing observations; Model parameters; Observation Period; Opinion dynamics; Real life datasets; State of the art; Transient effect; Stochastic models
User studies on end-user service composition: A literature review and a design framework,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070095196&doi=10.1145%2f3340294&partnerID=40&md5=cdc63ea7eea6da7ecf8ba63c2e876bc9,"Context: End-user service composition (EUSC) is a service-oriented paradigm that aims to empower end users and allow them to compose their own web applications from reusable service components. User studies have been used to evaluate EUSC tools and processes. Such an approach should benefit software development, because incorporating end users' feedback into software development should make software more useful and usable. Problem: There is a gap in our understanding of what constitutes a user study and how a good user study should be designed, conducted, and reported. Goal: This article aims to address this gap. Method: The article presents a systematic review of 47 selected user studies for EUSC. Guided by a review framework, the article systematically and consistently assesses the focus, methodology and cohesion of each of these studies. Results: The article concludes that the focus of these studies is clear, but their methodology is incomplete and inadequate, their overall cohesion is poor. The findings lead to the development of a design framework and a set of questions for the design, reporting, and review of good user studies for EUSC. The detailed analysis and the insights obtained from the analysis should be applicable to the design of user studies for service-oriented systems as well and indeed for any user studies related to software artifacts. © 2019 Association for Computing Machinery.",Design guideline; Empirical studies; End-user service composition; Mapshups; Qualitative studies; Review framework; Service-oriented computing; Systematic review; User studies; Web services,Distributed computer systems; Quality of service; Web services; Empirical studies; End users; Mapshups; Qualitative study; Service oriented computing; Systematic Review; User study; Software design
Cashtag piggybacking: Uncovering spam and bot activity in stock microblogs on twitter,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065784223&doi=10.1145%2f3313184&partnerID=40&md5=dd353023be8a4d80c778292bfd343458,"Microblogs are increasingly exploited for predicting prices and traded volumes of stocks in financial markets. However, it has been demonstrated that much of the content shared in microblogging platforms is created and publicized by bots and spammers. Yet, the presence (or lack thereof) and the impact of fake stock microblogs has never been systematically investigated before. Here, we study 9M tweets related to stocks of the five main financial markets in the US. By comparing tweets with financial data from Google Finance, we highlight important characteristics of Twitter stock microblogs. More importantly, we uncover a malicious practice-referred to as cashtag piggybacking-perpetrated by coordinated groups of bots and likely aimed at promoting low-value stocks by exploiting the popularity of high-value ones. Among the findings of our study is that as much as 71% of the authors of suspicious financial tweets are classified as bots by a state-of-the-art spambot-detection algorithm. Furthermore, 37% of them were suspended by Twitter a few months after our investigation. Our results call for the adoption of spam- and bot-detection techniques in all studies and applications that exploit user-generated content for predicting the stock market. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Social networks security; Social spam; Spam and bot detection; Stock market; Twitter,Botnet; Financial markets; Social networking (online); Bot detections; Detection algorithm; Micro-blogging platforms; Networks security; Social spam; State of the art; Twitter; User-generated content; Commerce
What web template extractor should I use? A benchmarking and comparison for five template extractors,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065737530&doi=10.1145%2f3316810&partnerID=40&md5=250bcbe48794ddc784f674f012cc53f2,"A Web template is a resource that implements the structure and format of a website, making it ready for plugging content into already formatted and prepared pages. For this reason, templates are one of the main development resources for website engineers, because they increase productivity. Templates are also useful for the final user, because they provide uniformity and a common look and feel for all webpages. However, from the point of view of crawlers and indexers, templates are an important problem, because templates usually contain irrelevant information, such as advertisements, menus, and banners. Processing and storing this information leads to a waste of resources (storage space, bandwidth, etc.). It has been measured that templates represent between 40% and 50% of data on the Web. Therefore, identifying templates is essential for indexing tasks. There exist many techniques and tools for template extraction, but, unfortunately, it is not clear at all which template extractor should a user/system use, because they have never been compared, and because they present different (complementary) features such as precision, recall, and efficiency. In this work, we compare the most advanced template extractors. We implemented and evaluated five of the most advanced template extractors in the literature. To compare all of them, we implemented a workbench, where they have been integrated and evaluated. Thanks to this workbench, we can provide a fair empirical comparison of all methods using the same benchmarks, technology, implementation language, and evaluation criteria. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Block detection; Content detection; Information retrieval; Template extraction; Web mining,Digital storage; Extraction; Information retrieval; Websites; Block detection; Content detection; Development resources; Empirical - comparisons; Implementation languages; Techniques and tools; Template extraction; Web Mining; Data mining
Exploiting usage to predict instantaneous app popularity: Trend filters and retention rates,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065784897&doi=10.1145%2f3199677&partnerID=40&md5=5f69c9a4499375ca63b30cca9e3ffa55,"Popularity of mobile apps is traditionally measured by metrics such as the number of downloads, installations, or user ratings. A problem with these measures is that they reflect usage only indirectly. Indeed, retention rates, i.e., the number of days users continue to interact with an installed app, have been suggested to predict successful app lifecycles. We conduct the first independent and large-scale study of retention rates and usage trends on a dataset of app-usage data from a community of 339,842 users and more than 213,667 apps. Our analysis shows that, on average, applications lose 65% of their users in the first week, while very popular applications (top 100) lose only 35%. It also reveals, however, that many applications have more complex usage behaviour patterns due to seasonality, marketing, or other factors. To capture such effects, we develop a novel app-usage trend measure which provides instantaneous information about the popularity of an application. Analysis of our data using this trend filter shows that roughly 40% of all apps never gain more than a handful of users (Marginal apps). Less than 0.1% of the remaining 60% are constantly popular (Dominant apps), 1% have a quick drain of usage after an initial steep rise (Expired apps), and 6% continuously rise in popularity (Hot apps). From these, we can distinguish, for instance, trendsetters from copycat apps. We conclude by demonstrating that usage behaviour trend information can be used to develop better mobile app recommendations. © 2019 Association for Computing Machinery.",Application popularity; Mobile analytics; Trend mining,Large dataset; Life cycle; Behaviour patterns; Large-scale studies; Mobile analytics; Mobile apps; Retention rate; Seasonality; Trend minings; User rating; Filtration
Polarization and fake news: Early warning of potential misinformation targets,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065737907&doi=10.1145%2f3316809&partnerID=40&md5=5287457b6c5a47667df3dfc294346e98,"Users' polarization and confirmation bias play a key role in misinformation spreading on online social media. Our aim is to use this information to determine in advance potential targets for hoaxes and fake news. In this article, we introduce a framework for promptly identifying polarizing content on social media and, thus, “predicting” future fake news topics. We validate the performances of the proposed methodology on a massive Italian Facebook dataset, showing that we are able to identify topics that are susceptible to misinformation with 77% accuracy. Moreover, such information may be embedded as a new feature in an additional classifier able to recognize fake news with 91% accuracy. The novelty of our approach consists in taking into account a series of characteristics related to users' behavior on online social media such as Facebook, making a first, important step towards the mitigation of misinformation phenomena by supporting the identification of potential misinformation targets and thus the design of tailored counter-narratives. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Classification; Fake news; Misinformation; Polarization; Social media,Classification (of information); Polarization; Confirmation bias; Early warning; Fake news; Misinformation; News topics; Online social medias; Potential targets; Social media; Social networking (online)
Layout cross-platform and cross-browser incompatibilities detection using classification of DOM elements,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065769358&doi=10.1145%2f3316808&partnerID=40&md5=10b19f67e9e9b3946f0420ee7bd38d73,"Web applications can be accessed through a variety of user agent configurations, in which the browser, platform, and device capabilities are not under the control of developers. In order to grant the compatibility of a web application in each environment, developers must manually inspect their web application in a wide variety of devices, platforms, and browsers. Web applications can be rendered inconsistently depending on the browser, the platform, and the device capabilities which are used. Furthermore, the devices' different viewport widths impact the way web applications are rendered in them, in which elements can be resized and change their absolute positions in the display. These adaptation strategies must also be considered in automatic incompatibility detection approaches in the state of the art. Hence, we propose a classification approach for detecting Layout Cross-platform and Cross-browser incompatibilities, which considers the adaptation strategies used in responsive web applications. Our approach is an extension of previous Cross-browser incompatibility detection approaches and has the goal of reducing the cost associated with manual inspections in different devices, platforms, and browsers, by automatically detecting Layout incompatibilities in this scenario. The proposed approach classifies each DOM element which composes a web application as an incompatibility or not, based on its attributes, position, alignment, screenshot, and the viewport width of the browser. We report the results of an experiment conducted with 42 Responsive Web Applications, rendered in three devices (Apple iPhone SE, Apple iPhone 8 Plus, and Motorola Moto G4) and browsers (Google Chrome and Apple Safari). The results (with F-measure of 0.70) showed evidence which quantify the effectiveness of our classification approach, and it could be further enhanced for detecting Cross-platform and Cross-browser incompatibilities. Furthermore, in the experiment, our approach also performed better when compared to a former state-of-the-art classification technique for Cross-browser incompatibilities detection. © 2019 Association for Computing Machinery.",Cross-browser incompatibilities; Cross-platform incompatibilities; Incompatibilities automatic detection,Smartphones; Adaptation strategies; Automatic Detection; Classification approach; Classification technique; Cross-browser incompatibilities; Cross-platform; Detection approach; Device capabilities; Display devices
Web portals for high-performance computing: A survey,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062345355&doi=10.1145%2f3197385&partnerID=40&md5=a009365667be73d93b71bf9c5897e7ff,"This article addresses web interfaces for High-performance Computing (HPC) simulation software. First, it presents a brief history, starting in the 1990s with Java applets, of web interfaces used for accessing and making best possible use of remote HPC resources. It introduces HPC web-based portal use cases. Then it identifies and discusses the key features, among functional and non-functional requirements, that characterize such portals. A brief state of the art is then presented. The design and development of Bull extreme factory Computing Studio v3 (XCS3) is chosen as a common thread for showing how the identified key features can all be implemented in one software: multi-tenancy, multi-scheduler compatibility, complete control through an HTTP RESTful API, customizable user interface with Responsive Web Design, HPC application template framework, remote visualization, and access through the Authentication, Authorization, and Accounting security framework with the Role-Based Access Control permission model. Non-functional requirements (security, usability, performance, reliability) are discussed, and the article concludes by giving perspective for future work. © 2019 Copyright held by the owner/author(s).",Application templates; Cloud computing; Customizable GUI; Dashboards; High-performance computing; HPC; HPC-as-a-service; HPCaaS; Job management; RESTful API; SaaS; Science gateway; Service-oriented architectures; Software-as-a-service; User interface; Web portal,Application programming interfaces (API); Application programs; Authentication; Cloud computing; HTTP; Information services; Service oriented architecture (SOA); Software as a service (SaaS); User interfaces; Customizable; Dashboards; High performance computing; Hpc as a services; HPCaaS; Job management; Restful api; SaaS; Science gateway; Portals
A large-scale behavioural analysis of bots and humans on twitter,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062364931&doi=10.1145%2f3298789&partnerID=40&md5=a50fb0f45dfbd8e5561d9f923672ec6a,"Recent research has shown a substantial active presence of bots in online social networks (OSNs). In this article, we perform a comparative analysis of the usage and impact of bots and humans on Twitter-one of the largest OSNs in the world. We collect a large-scale Twitter dataset and define various metrics based on tweet metadata. Using a human annotation task, we assign “bot” and “human” ground-truth labels to the dataset and compare the annotations against an online bot detection tool for evaluation. We then ask a series of questions to discern important behavioural characteristics of bots and humans using metrics within and among four popularity groups. From the comparative analysis, we draw clear differences and interesting similarities between the two entities. © 2019 Association for Computing Machinery.",Behavioural analysis; Bot characterisation; Bot generated content; Bot network traffic,Behavioral research; Large dataset; Social networking (online); Behavioural analysis; Bot characterisation; Bot generated content; Comparative analysis; Human annotations; Network traffic; Online social networks (OSNs); Recent researches; Botnet
Social networks under stress: Specialized team roles and their communication structure,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062327960&doi=10.1145%2f3295460&partnerID=40&md5=3170b33e9838b8830ae15d2343ebe208,"Social network research has begun to take advantage of fine-grained communications regarding coordination, decision-making, and knowledge sharing. These studies, however, have not generally analyzed how external events are associated with a social network's structure and communicative properties. Here, we study how external events are associated with a network's change in structure and communications. Analyzing a complete dataset of millions of instant messages among the decision-makers with different roles in a large hedge fund and their network of outside contacts, we investigate the link between price shocks, network structure, and change in the affect and cognition of decision-makers embedded in the network. We also analyze the communication dynamics among specialized teams in the organization. When price shocks occur the communication network tends not to display structural changes associated with adaptiveness such as the activation of weak ties to obtain novel information. Rather, the network “turtles up.” It displays a propensity for higher clustering, strong tie interaction, and an intensification of insider vs. outsider and within-role vs. between-role communication. Further, we find changes in network structure predict shifts in cognitive and affective processes, execution of new transactions, and local optimality of transactions better than prices, revealing the important predictive relationship between network structure and collective behavior within a social network. © 2019 Association for Computing Machinery.",Collective behavior; Organizations; Social networks; Temporal dynamics,Large dataset; Social networking (online); Societies and institutions; Affective process; Collective behavior; Communication structures; Knowledge-sharing; Network structures; Novel information; Predictive relationships; Temporal dynamics; Decision making
FUSE: Entity-centric data fusion on linked data,2019,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062227912&doi=10.1145%2f3306128&partnerID=40&md5=99816fcff25955399f774c11b0c17e47,"Many current web pages include structured data which can directly be processed and used. Search engines, in particular, gather that structured data and provide question answering capabilities over the integrated data with an entity-centric presentation of the results. Due to the decentralized nature of the web, multiple structured data sources can provide similar information about an entity. But data from different sources may involve different vocabularies and modeling granularities, which makes integration difficult. We present FusE, an approach that identifies similar entity-specific data across sources, independent of the vocabulary and data modeling choices. We apply our method along the scenario of a trustable knowledge panel, conduct experiments in which we identify and process entity data from web sources, and compare the output to a competing system. The results underline the advantages of the presented entity-centric data fusion approach. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Data provenance; Data/knowledge fusion; Entity data fusion; Entity-centric data fusion; Linked data; N-ary relations; Structured data,Data fusion; Data handling; Linked data; Websites; Competing systems; Data provenance; Integrated data; Model choice; Question Answering; Structured data; Web sources; Search engines
Analyzing privacy policies at scale: From crowdsourcing to automated annotations,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058278785&doi=10.1145%2f3230665&partnerID=40&md5=ca7fe57f882e3c3f1f28db5e0618bbe6,"Website privacy policies are often long and difficult to understand. While research shows that Internet users care about their privacy, they do not have the time to understand the policies of every website they visit, and most users hardly ever read privacy policies. Some recent efforts have aimed to use a combination of crowdsourcing, machine learning, and natural language processing to interpret privacy policies at scale, thus producing annotations for use in interfaces that inform Internet users of salient policy details. However, little attention has been devoted to studying the accuracy of crowdsourced privacy policy annotations, how crowdworker productivity can be enhanced for such a task, and the levels of granularity that are feasible for automatic analysis of privacy policies. In this article, we present a trajectory of work addressing each of these topics. We include analyses of crowdworker performance, evaluation of a method to make a privacy-policy oriented task easier for crowdworkers, a coarse-grained approach to labeling segments of policy text with descriptive themes, and a fine-grained approach to identifying user choices described in policy text. Together, the results from these efforts show the effectiveness of using automated and semi-automated methods for extracting from privacy policies the data practice details that are salient to Internet users' interests. 2018 Copyright is held by the owner/author(s). © 2018 Association for Computing Machinery.",Crowdsourcing; Human computer interaction (HCI); Machine learning; Natural language processing; Privacy; Privacy policies,Artificial intelligence; Automation; Crowdsourcing; Data privacy; Human computer interaction; Learning algorithms; Learning systems; Natural language processing systems; Websites; Automatic analysis; Coarse-grained approaches; Data practices; Fine grained; Human Computer Interaction (HCI); Internet users; Privacy policies; Semi-automated; Computer privacy
Mining abstract XML data-types,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058299916&doi=10.1145%2f3267467&partnerID=40&md5=4ecc4929e4bbb14da61c8829ad1c64c2,"Schema integration has been a long-standing challenge for the data-engineering community that has received steady attention over the past three decades. General-purpose integration approaches construct unified schemas that encompass all schema elements. Schema integration has been revisited in the past decade in service-oriented computing since the input/output data-types of service interfaces are heterogeneous XML schemas. However, service integration differs from the traditional integration problem, since it should generalize schemas (mining abstract data-types) instead of unifying all schema elements. To mine well-formed abstract data-types, the fundamental Liskov Substitution Principle (LSP), which generally holds between abstract data-types and their subtypes, should be followed. However, due to the heterogeneity of service datatypes, the strict employment of LSP is not usually feasible. On top of that, XML offers a rich type system, based on which data-types are defined via combining type patterns (e.g., composition, aggregation). The existing integration approaches have not dealt with the challenges of a defining subtyping relation between XML type patterns. To address these challenges, we propose a relaxed version of LSP between XML type patterns and an automated generalization process for mining abstract XML data-types. We evaluate the effectiveness and the efficiency of the process on the schemas of two datasets against two representative state-of-the-art approaches. © 2018 Association for Computing Machinery.",Embedded subtree; Pruning; Subtyping relation; Type pattern,Data integration; Distributed computer systems; Formal languages; XML; Generalization process; Pruning; Service oriented computing; State-of-the-art approach; Sub trees; Substitution principles; Subtyping relation; Type pattern; Abstract data types
"New phone,who dis? modeling millennials' backup behavior",2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058283444&doi=10.1145%2f3208105&partnerID=40&md5=38bf97a30edf97da1b68bab59ffd623a,"Given the ever-rising frequency of malware attacks and other problems leading people to lose their files, backups are an important proactive protective behavior in which users can engage. Backing up files can prevent emotional and financial losses and improve overall user experience. Yet, we find that less than half of young adults perform mobile or computer backups regularly. To understand why, we model the factors that drive mobile and computer backup behavior, and changes in that behavior over time, using data from a panel survey of 384 diverse young adults. We develop a set of models that explain 37% and 38% of the variance in reported mobile and computer backup behaviors, respectively. These models show consistent relationships between Internet skills and backup frequency on both mobile and computer devices. We find that this relationship holds longitudinally: increases in Internet skills lead to increased frequency of computer backups. This article provides a foundation for understanding what drives young adults' backup behavior. It concludes with recommendations for motivating people to back up, and for future work, modeling similar user behaviors. © 2018 Copyright held by the owner/author(s).",Backup behavior; Digital inequality; Internet skill; Survey; Usability; Young adults,Digital storage; Losses; Malware; Surveying; Surveys; Backup behavior; Computer device; Digital inequalities; Financial loss; Malware attacks; Usability; User experience; Young adults; Behavioral research
Test-based security certification of composite services,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058303013&doi=10.1145%2f3267468&partnerID=40&md5=332e6a3558397f8a012054b3e539eb82,"The diffusion of service-based and cloud-based systems has created a scenario where software is often made available as services, offered as commodities over corporate networks or the global net. This scenario supports the definition of business processes as composite services, which are implemented via either static or runtime composition of offerings provided by different suppliers. Fast and accurate evaluation of services' security properties becomes then a fundamental requirement and is nowadays part of the software development process. In this article, we show how the verification of security properties of composite services can be handled by test-based security certification and built to be effective and efficient in dynamic composition scenarios. Our approach builds on existing security certification schemes for monolithic services and extends them towards service compositions. It virtually certifies composite services, starting from certificates awarded to the component services.We describe three heuristic algorithms for generating runtime test-based evidence of the composite service holding the properties. These algorithms are compared with the corresponding exhaustive algorithm to evaluate their quality and performance.We also evaluate the proposed approach in a real-world industrial scenario, which considers ENGpay online payment system of Engineering Ingegneria Informatica S.p.A. The proposed industrial evaluation presents the utility and generality of the proposed approach by showing how certification results can be used as a basis to establish compliance to Payment Card Industry Data Security Standard. © 2018 Association for Computing Machinery.",Cloud; Model-based testing; Security certification; Service composition; Service-oriented architecture; Software-as-a-service; Web services,Clouds; Electronic commerce; Heuristic algorithms; Information services; Model checking; Quality control; Regulatory compliance; Service oriented architecture (SOA); Software as a service (SaaS); Software design; Software testing; Dynamic composition; Industrial evaluations; Industrial scenarios; Model based testing; Payment card industries; Security certification; Service compositions; Software development process; Web services
Imaginary people representing real numbers: Generating personas from online social media data,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060866682&doi=10.1145%2f3265986&partnerID=40&md5=19e8a28f2f2ef0ab809279017d40a87d,"We develop a methodology to automate creating imaginary people, referred to as personas, by processing complex behavioral and demographic data of social media audiences. From a popular social media account containing more than 30 million interactions by viewers from 198 countries engaging with more than 4,200 online videos produced by a global media corporation, we demonstrate that our methodology has several novel accomplishments, including: (a) identifying distinct user behavioral segments based on the user content consumption patterns; (b) identifying impactful demographics groupings; and (c) creating rich persona descriptions by automatically adding pertinent attributes, such as names, photos, and personal characteristics. We validate our approach by implementing the methodology into an actual working system; we then evaluate it via quantitative methods by examining the accuracy of predicting content preference of personas, the stability of the personas over time, and the generalizability of the method via applying to two other datasets. Research findings show the approach can develop rich personas representing the behavior and demographics of real audiences using privacy-preserving aggregated online social media data from major online platforms. Results have implications for media companies and other organizations distributing content via online platforms. © 2018 Association for Computing Machinery.",Persona; User analytics,Behavioral research; Population statistics; Content consumption; Online social medias; Persona; Personal characteristics; Pertinent attributes; Privacy preserving; Quantitative method; User analytics; Social networking (online)
Top-k user-defined vertex scoring queries in edge-labeled graph databases,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054599751&doi=10.1145%2f3213891&partnerID=40&md5=2caab0e57db2379881bc2474f5796294,"We consider identifying highly ranked vertices in large graph databases such as social networks or the Semantic Web where there are edge labels. There are many applications where users express scoring queries against such databases that involve two elements: (i) a set of patterns describing relationships that a vertex of interest to the user must satisfy and (ii) a scoring mechanism in which the user may use properties of the vertex to assign a score to that vertex. We define the concept of a partial pattern map query (partial PM-query), which intuitively allows us to prune partial matchings, and show that finding an optimal partial PM-query is NP-hard. We then propose two algorithms, PScore-LP and PScore-NWST, to find the answer to a scoring (top-k) query. In PScore-LP, the optimal partial PM-query is found using a list-oriented pruning method. PScore-NWST leverages node-weighted Steiner trees to quickly compute slightly sub-optimal solutions. We conduct detailed experiments comparing our algorithms with (i) an algorithm (PScore-Base) that computes all answers to the query, evaluates them according to the scoring method, and chooses the top-k, and (ii) two SemanticWeb query processing systems (Jena and GraphDB). Our algorithms show better performance than PScore-Base and the Semantic Web query processing systems-moreover, PScore-NWST outperforms PScore-LP on large queries and on queries with a tree structure. © 2018 Association for Computing Machinery.",Graph databases; Scoring queries; Top-k querying,Graph Databases; Graph theory; Optimization; Semantic Web; Trees (mathematics); Partial patterns; Pruning methods; Query processing system; Scoring methods; Scoring queries; Suboptimal solution; Top-k querying; Tree structures; Query processing
Understanding cross-site linking in online social networks,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054563702&doi=10.1145%2f3213898&partnerID=40&md5=6b132bae110889be5a2e886abdc03932,"As a result of the blooming of online social networks (OSNs), a user often holds accounts on multiple sites. In this article, we study the emerging ""cross-site linking"" function available on mainstream OSN services including Foursquare, Quora, and Pinterest. We first conduct a data-driven analysis on crawled profiles and social connections of all 61.39 million Foursquare users to obtain a thorough understanding of this function. Our analysis has shown that the cross-site linking function is adopted by 57.10% of all Foursquare users, and the users who have enabled this function are more active than others. We also find that the enablement of cross-site linking might lead to privacy risks. Based on cross-site links between Foursquare and external OSN sites, we formulate cross-site information aggregation as a problem that uses cross-site links to stitch together site-local information fields for OSN users. Using large datasets collected from Foursquare, Facebook, and Twitter, we demonstrate the usefulness and the challenges of cross-site information aggregation. In addition to the measurements, we carry out a survey collecting detailed user feedback on cross-site linking. This survey studies why people choose to or not to enable cross-site linking, as well as the motivation and concerns of enabling this function. © 2018 Association for Computing Machinery.",Cross-site linking; Measurement; Online social networks; Survey,Information management; Measurement; Surveying; Surveys; Cross-site linking; Data-driven analysis; Large datasets; Local information; On-line social networks; Online social networks (OSNs); Site information; Social connection; Social networking (online)
A rule-based transducer forquerying incompletely aligned datasets,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054593608&doi=10.1145%2f3228328&partnerID=40&md5=1cae1645169eac64f2b3109e8a2a9cd5,"A growing number of Linked Open Data sources (from diverse provenances and about different domains) that can be freely browsed and searched to find and extract useful information have been made available. However, access to them is difficult for different reasons. This study addresses access issues concerning heterogeneity. It is common for datasets to describe the same or overlapping domains while using different vocabularies. Our study presents a transducer that transforms a SPARQL query suitably expressed in terms of the vocabularies used in a source dataset into another SPARQL query suitably expressed for a target dataset involving different vocabularies. The transformation is based on existing alignments between terms in different datasets. Whenever the transducer is unable to produce a semantically equivalent query because of the scarcity of term alignments, the transducer produces a semantic approximation of the query to avoid returning the empty answer to the user. Transformation across datasets is achieved through the management of a wide range of transformation rules. The feasibility of our proposal has been validated with a prototype implementation that processes queries that appear in well-known benchmarks and SPARQL endpoint logs. Results of the experiments show that the system is quite effective in achieving adequate transformations. © 2018 Association for Computing Machinery.",Linked open data; Query transformation; RDF; Semantic web; SPARQL,Linked data; Metadata; Semantic Web; Transducers; Different domains; Open data sources; Prototype implementations; Query transformations; Semantic approximation; SPARQL; Sparql queries; Transformation rules; Query processing
Exploring and analysing the African web ecosystem,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054588139&doi=10.1145%2f3213897&partnerID=40&md5=3694731329a596243a21456c38bd2d64,"It is well known that internet infrastructure deployment is progressing at a rapid pace in the African continent. A flurry of recent research has quantified this, highlighting the expansion of its underlying connectivity network. However, improving the infrastructure is not useful without appropriately provisioned services to exploit it. This article measures the availability and utilisation of web infrastructure in Africa.Whereas others have explored web infrastructure in developed regions, we shed light on practices in developing regions. To achieve this, we apply a comprehensive measurement methodology to collect data from a variety of sources. We first focus on Google to reveal that its content infrastructure in Africa is, indeed, expanding. That said, we find that much of its web content is still served from the US and Europe, despite being the most popular website in many African countries. We repeat the same analysis across a number of other regionally popular websites to find that even top African websites prefer to host their content abroad. To explore the reasons for this, we evaluate some of the major bottlenecks facing content delivery networks (CDNs) in Africa. Amongst other factors, we find a lack of peering between the networks hosting our probes, preventing the sharing of CDN servers, as well as poorly configured DNS resolvers. Finally, our mapping of middleboxes in the region reveals that there is a greater presence of transparent proxies in Africa than in Europe or the US. We conclude the work with a number of suggestions for alleviating the issues observed. © 2018 Association for Computing Machinery.",Content infrastructure; DNS; Measurements; Web,Measurement; Websites; Comprehensive measurement; Content delivery network; Content infrastructure; Developed regions; Developing regions; Internet infrastructure; Recent researches; Web infrastructure; Internet protocols
"You, the web, and your device: Longitudinal characterization of browsing habits",2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054576151&doi=10.1145%2f3231466&partnerID=40&md5=56502d8d1f59b143792fdbabdd92a1ca,"Understanding how people interact with the web is key for a variety of applications, e.g., from the design of effective web pages to the definition of successful online marketing campaigns. Browsing behavior has been traditionally represented and studied by means of clickstreams, i.e., graphs whose vertices are web pages, and edges are the paths followed by users. Obtaining large and representative data to extract clickstreams is, however, challenging. The evolution of the web questions whether browsing behavior is changing and, by consequence, whether properties of clickstreams are changing. This article presents a longitudinal study of clickstreams from 2013 to 2016. We evaluate an anonymized dataset of HTTP traces captured in a large ISP, where thousands of households are connected. We first propose a methodology to identify actual URLs requested by users from the massive set of requests automatically fired by browsers when rendering web pages. Then, we characterize web usage patterns and clickstreams, taking into account both the temporal evolution and the impact of the device used to explore the web. Our analyses precisely quantify various aspects of clickstreams and uncover interesting patterns, such as the typical short paths followed by people while navigating the web, the fast increasing trend in browsing from mobile devices, and the different roles of search engines and social networks in promoting content. Finally, we contribute a dataset of anonymized clickstreams to the community to foster new studies. © 2018 Association for Computing Machinery.",Clickstream; Passive measurements; Surfing behavior; Web usage evolution,Marketing; Search engines; Websites; Browsing behavior; Clickstreams; Longitudinal study; Passive measurements; Surfing behavior; Temporal evolution; Web usage; Web usage patterns; HTTP
Unsupervised domain ranking in large-scale web crawls,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054599320&doi=10.1145%2f3182180&partnerID=40&md5=d192b24276f45489fe07c360fa47811a,"With the proliferation of web spam and infinite autogenerated web content, large-scale web crawlers require low-complexity ranking methods to effectively budget their limited resources and allocate bandwidth to reputable sites. In this work, we assume crawls that produce frontiers orders of magnitude larger than RAM, where sorting of pending URLs is infeasible in real time. Under these constraints, the main objective is to quickly compute domain budgets and decide which of them can be massively crawled. Those ranked at the top of the list receive aggressive crawling allowances, while all other domains are visited at some small default rate. To shed light on Internet-wide spam avoidance, we study topology-based ranking algorithms on domain-level graphs from the two largest academic crawls: a 6.3B-page IRLbot dataset and a 1B-page ClueWeb09 exploration. We first propose a new methodology for comparing the various rankings and then show that in-degree BFS-based techniques decisively outperform classic PageRank-style methods, including TrustRank. However, since BFS requires several orders of magnitude higher overhead and is generally infeasible for real-time use, we propose a fast, accurate, and scalable estimation method called TSE that can achieve much better crawl prioritization in practice. It is especially beneficial in applications with limited hardware resources. © 2018 Association for Computing Machinery.",Frontier prioritization; Ranking; Web crawling,Budget control; Information retrieval systems; Topology; Estimation methods; Hardware resources; Orders of magnitude; Prioritization; Ranking; Ranking algorithm; Ranking methods; Web Crawling; Web crawler
BUbiNG: Massive crawling for the masses,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058032773&doi=10.1145%2f3160017&partnerID=40&md5=6f0670c6741d4a751212475b32df26dc,"Although web crawlers have been around for twenty years by now, there is virtually no freely available, open-source crawling software that guarantees high throughput, overcomes the limits of single-machine systems, and, at the same time, scales linearly with the amount of resources available. This article aims at filling this gap, through the description of BUbiNG, our next-generation web crawler built upon the authors’ experience with UbiCrawler [9] and on the last ten years of research on the topic. BUbiNG is an open-source Java fully distributed crawler; a single BUbiNG agent, using sizeable hardware, can crawl several thousand pages per second respecting strict politeness constraints, both host- and IP-based. Unlike existing open-source distributed crawlers that rely on batch techniques (like MapReduce), BUbiNG job distribution is based on modern high-speed protocols to achieve very high throughput. CCS Concepts: • Information systems → Web crawling; Page and site ranking; • Computer systems organization → Peer-to-peer architectures; © 2018 ACM.",Centrality measures; Distributed systems; Web crawling,Distributed computer systems; Network security; Open source software; Open systems; Peer to peer networks; Throughput; Batch techniques; Centrality measures; Computer systems organization; Distributed crawler; Distributed systems; Peer-to-peer architectures; Single- machines; Web Crawling; Web crawler
Characterizing and predicting users⇔ behavior on local search queries,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058052815&doi=10.1145%2f3157059&partnerID=40&md5=2a2d10750dc3df959f5a870e41c43a00,"The use of queries to find products and services that are located nearby is increasing rapidly due mainly to the ubiquity of internet access and location services provided by smartphone devices. Local search engines help users by matching queries with a predefined geographical connotation (“local queries”) against a database of local business listings. Local search differs from traditional Web search because, to correctly capture users’ click behavior, the estimation of relevance between query and candidate results must be integrated with geographical signals, such as distance. The intuition is that users prefer businesses that are physically closer to them or in a convenient area (e.g., close to their home). However, this notion of closeness depends upon other factors, like the business category, the quality of the service provided, the density of businesses in the area of interest, the hour of the day, or even the day of the week. In this work, we perform an extensive analysis of online users’ interactions with a local search engine, investigating their intent, temporal patterns, and highlighting relationships between distance-to-business and other factors, such as business reputation, Furthermore, we investigate the problem of estimating the click-through rate on local search (LCTR) by exploiting the combination of standard retrieval methods with a rich 1 collection of geo-, user-, and business-dependent features. We validate our approach on a large log collected from a real-world local search service. Our evaluation shows that the non-linear combination of business and user information, geo-local and textual relevance features leads to a significant improvements over existing alternative approaches based on a combination of relevance, distance, and business reputation [1]. CCS Concepts: • Information systems → Information retrieval; Environment-specific retrieval; Specialized information retrieval; © 2018 ACM.",Local search; User’s behavior analysis; User’s behavior prediction,Behavioral research; Information retrieval; Query languages; Query processing; Behavior analysis; Behavior prediction; Click-through rate; Local search; Local search engines; Location services; Products and services; Retrieval methods; Search engines
Editorial,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058043092&doi=10.1145%2f3232925&partnerID=40&md5=85d2582af12be85b298d030ede93afcd,[No abstract available],,
Extracting and summarizing situational information from the twitter social media during disasters,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064552004&doi=10.1145%2f3178541&partnerID=40&md5=74de811b66d22ed7443d6e0cceeab69f,"Microblogging sites like Twitter have become important sources of real-time information during disaster events. A large amount of valuable situational information is posted in these sites during disasters; however, the information is dispersed among hundreds of thousands of tweets containing sentiments and opinions of the masses. To effectively utilize microblogging sites during disaster events, it is necessary to not only extract the situational information from the large amounts of sentiments and opinions, but also to summarize the large amounts of situational information posted in real-time. During disasters in countries like India, a sizable number of tweets are posted in local resource-poor languages besides the normal English-language tweets. For instance, in the Indian subcontinent, a large number of tweets are posted in Hindi/Devanagari (the national language of India), and some of the information contained in such non-English tweets is not available (or available at a later point of time) through English tweets. In this work, we develop a novel classification-summarization framework which handles tweets in both English and Hindi—we first extract tweets containing situational information, and then summarize this information. Our proposed methodology is developed based on the understanding of how several concepts evolve in Twitter during disaster. This understanding helps us achieve superior performance compared to the state-of-the-art tweet classifiers and summarization approaches on English tweets. Additionally, to our knowledge, this is the first attempt to extract situational information from non-English tweets. © 2018 ACM.",Classification; Content words; Disasters; Microblogs; Situational tweets; Summarization; Twitter,Developing countries; Disasters; Social networking (online); Content words; Microblogs; Situational tweets; Summarization; Twitter; Classification (of information)
Faster Base64 encoding and decoding using AVX2 instructions,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064548625&doi=10.1145%2f3132709&partnerID=40&md5=bfc924ba6ba0a776d83573830e9c2132,"Web developers use base64 formats to include images, fonts, sounds, and other resources directly inside HTML, JavaScript, JSON, and XML files. We estimate that billions of base64 messages are decoded every day. We are motivated to improve the efficiency of base64 encoding and decoding. Compared to state-of-the-art implementations, we multiply the speeds of both the encoding (≈ 10×) and the decoding (≈ 7×). We achieve these good results by using the single-instruction-multiple-data instructions available on recent Intel processors (AVX2). Our accelerated software abides by the specification and reports errors when encountering characters outside of the base64 set. It is available online as free software under a liberal license. 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Binary-to-text encoding; Data URI; Vectorization; Web performance,Decoding; Encoding (symbols); Data URI; Encoding and decoding; Intel processors; Single instruction multiple data instructions; State of the art; Text encoding; Vectorization; Web performance; Signal encoding
Completeness management for RDF data sources,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064564398&doi=10.1145%2f3196248&partnerID=40&md5=5a0af877d1eb5c9495f106780f591fe1,"The Semantic Web is commonly interpreted under the open-world assumption, meaning that information available (e.g., in a data source) captures only a subset of the reality. Therefore, there is no certainty about whether the available information provides a complete representation of the reality. The broad aim of this article is to contribute a formal study of how to describe the completeness of parts of the Semantic Web stored in RDF data sources. We introduce a theoretical framework allowing augmentation of RDF data sources with statements, also expressed in RDF, about their completeness. One immediate benefit of this framework is that now query answers can be complemented with information about their completeness. We study the impact of completeness statements on the complexity of query answering by considering different fragments of the SPARQL language, including the RDFS entailment regime, and the federated scenario. We implement an efficient method for reasoning about query completeness and provide an experimental evaluation in the presence of large sets of completeness statements. © 2018 ACM.",Data completeness; Metadata; Query answering; RDF; SPARQL,Computer networks; Internet; Metadata; Data completeness; Experimental evaluation; Formal studies; Open world assumption; Query answering; Query completeness; SPARQL; Theoretical framework; Semantic Web
Optimizing whole-page presentation for web search,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061714877&doi=10.1145%2f3204461&partnerID=40&md5=7177bf583da8fca2db4bcdfc3c62391a,"Modern search engines aggregate results from different verticals: webpages, news, images, video, shopping, knowledge cards, local maps, and so on. Unlike “ten blue links,” these search results are heterogeneous in nature and not even arranged in a list on the page. This revolution directly challenges the conventional “ranked list” formulation in ad hoc search. Therefore, finding proper presentation for a gallery of heterogeneous results is critical for modern search engines. We propose a novel framework that learns the optimal page presentation to render heterogeneous results onto search result page (SERP). Page presentation is broadly defined as the strategy to present a set of items on SERP, much more expressive than a ranked list. It can specify item positions, image sizes, text fonts, and any other styles as long as variations are within business and design constraints. The learned presentation is content aware, i.e., tailored to specific queries and returned results. Simulation experiments show that the framework automatically learns eye-catchy presentations for relevant results. Experiments on real data show that simple instantiations of the framework already outperform leading algorithm in federated search result presentation. It means the framework can learn its own result presentation strategy purely from data, without even knowing the “probability ranking principle.” © 2018 ACM.",User satisfaction; Whole-page optimization,Websites; Content-aware; Design constraints; Knowledge card; Local map; Presentation strategies; Probability rankings; User satisfaction; Web searches; Search engines
Localness of location-based knowledge sharing: A study of naver kin “here”,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064539279&doi=10.1145%2f2983645&partnerID=40&md5=719a5aedf02decd03ac783cdf5d32b65,"In location-based social Q&A services, people ask a question with a high expectation that local residents who have local knowledge will answer the question. However, little is known about the locality of user activities in location-based social Q&A services. This study aims to deepen our understanding of location-based knowledge sharing by investigating the following: general behavioral characteristics of users, the topical and typological patterns related to geographic characteristics, geographic locality of user activities, and motivations of local knowledge sharing. To this end, we analyzed a 12-month period Q&A dataset from Naver KiN “Here,” a location-based social Q&A mobile app, in addition to a supplementary survey dataset obtained from 285 mobile users. Our results reveal several unique characteristics of location-based social Q&A. When compared with conventional social Q&A sites, users ask and answer different topical/typological questions. In addition, those who answer have a strong spatial locality wherein they primarily have local knowledge in a few regions, in areas such as their home and work. We also find unique motivators such as ownership of local knowledge and a sense of local community. The findings reported in the article have significant implications for the design of Q&A systems, especially location-based social Q&A systems. © 2018 ACM.",Knowledge sharing; Mobile applications,Knowledge management; Location; Behavioral characteristics; Geographic characteristics; Knowledge-sharing; Local community; Local knowledge; Local residents; Mobile applications; Spatial locality; Location based services
A model of information diffusion in interconnected online social networks,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056780316&doi=10.1145%2f3160000&partnerID=40&md5=76ec664a3bbf01f348897e06157249f2,"Online social networks (OSN) have today reached a remarkable capillary diffusion. There are numerous examples of very large platforms people use to communicate and maintain relationships. People also subscribe to several OSNs, e.g., people create accounts on Facebook, Twitter, and so on. This phenomenon leads to online social internetworking (OSI) scenarios where users who subscribe to multiple OSNs are termed as bridges. Unfortunately, several important features make the study of information propagation in an OSI scenario a difficult task, e.g., correlations in both the structural characteristics of OSNs and the bridge interconnections among them, heterogeneity and size of OSNs, activity factors, cross-posting propensity, and so on. In this article, we propose a directed random graph-based model that is amenable to efficient numerical solution to analyze the phenomenon of information propagation in an OSI scenario; in the model development, we take into account heterogeneity and correlations introduced by both topological (correlations among nodes degrees and among bridge distributions) and user-related factors (activity index, cross-posting propensity). We first validate the model predictions against simulations on snapshots of interconnected OSNs in a reference scenario. Subsequently, we exploit the model to show the impact on the information propagation of several characteristics of the reference scenario, i.e., size and complexity of the OSI scenario, degree distribution and overall number of bridges, growth and decline of OSNs in time, and time-varying cross-posting users propensity. © 2018 ACM",Data diffusion; Data propagation; Data spreading; Generalized random graphs; Information diffusion; Information propagation; Information spreading; Interconnected networks; Mathematical modeling; Multiple online social networks; Online social internetworking; Online social networks,Directed graphs; Graph theory; Graphic methods; Information dissemination; Mathematical models; Data diffusion; Data propagation; Data spreading; Generalized random graphs; Information diffusion; Information propagation; Information spreading; Interconnected network; Internetworking; On-line social networks; Social networking (online)
Evaluating quality in use of corporateweb sites: An empirical investigation,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056794072&doi=10.1145%2f3184646&partnerID=40&md5=337765fb1da25f3e46e5456ca3e6855e,"In our prior work, we presented a novel approach to the evaluation of quality in use of corporate web sites based on an original quality model (QM-U) and a related methodology (EQ-EVAL). This article focuses on two research questions. The first one aims at investigating whether expected quality obtained through the application of EQ-EVAL methodology by employing a small panel of evaluators is a good approximation of actual quality obtained through experimentation with real users. To answer this research question, a comparative study has been carried out involving 5 evaluators and 50 real users. The second research question aims at demonstrating that the adoption of the EQ-EVAL methodology can provide useful information for web site improvement. Three original indicators, namely coherence, coverage and ranking have been defined to answer this question, and an additional study comparing the assessments of two panels of 5 and 10 evaluators, respectively, has been carried out. The results obtained in both studies are largely positive and provide a rational support for the adoption of the EQ-EVAL methodology. © 2018 ACM.",Actual quality; Expected quality; Quality assessment methodology; Quality in use; Quality model; Web site quality,Quality assurance; Websites; Comparative studies; Empirical investigation; Quality assessment; Quality in use; Quality modeling; Research questions; Website quality; Quality control
When simpler data does not imply less information: A study of user profiling scenarios with constrained view of mobile HTTP(S) traffic,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042494208&doi=10.1145%2f3143402&partnerID=40&md5=1e3d1acadcca04798873996a82c22e4a,"The exponential growth in smartphone adoption is contributing to the availability of vast amounts of human behavioral data. This data enables the development of increasingly accurate data-driven user models that facilitate the delivery of personalized services that are often free in exchange for the use of its customers' data. Although such usage conventions have raised many privacy concerns, the increasing value of personal data is motivating diverse entities to aggressively collect and exploit the data. In this article, we unfold profiling scenarios around mobile HTTP(S) traffic, focusing on those that have limited but meaningful segments of the data. The capability of the scenarios to profile personal information is examined with real user data, collected in the wild from 61 mobile phone users for a minimum of 30 days. Our study attempts to model heterogeneous user traits and interests, including personality, boredom proneness, demographics, and shopping interests. Based on our modeling results, we discuss various implications to personalization, privacy, and personal data rights. © 2017 ACM.",Mobile computing; Personalized services; User modeling,Behavioral research; HTTP; Mobile computing; Exponential growth; Heterogeneous users; Mobile-phone users; Personal information; Personalizations; Personalized service; Privacy concerns; User Modeling; Data privacy
Semantics-based analysis of content security policy deployment,2018,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042483756&doi=10.1145%2f3149408&partnerID=40&md5=84af50160abcf65e2bd6ee8973963ada,"Content Security Policy (CSP) is a recentW3C standard introduced to prevent and mitigate the impact of content injection vulnerabilities on websites. In this article, we introduce a formal semantics for the latest stable version of the standard, CSP Level 2. We then perform a systematic, large-scale analysis of the effectiveness of the current CSP deployment, using the formal semantics to substantiate our methodology and to assess the impact of the detected issues. We focus on four key aspects that affect the effectiveness of CSP: browser support,website adoption, correct configuration, and constant maintenance. Our analysis shows that browser support for CSP is largely satisfactory, with the exception of a few notable issues. However, there are several shortcomings relative to the other three aspects. CSP appears to have a rather limited deployment as yet and, more crucially, existing policies exhibit a number of weaknesses and misconfiguration errors. Moreover, content security policies are not regularly updated to ban insecure practices and remove unintended security violations. We argue that many of these problems can be fixed by better exploiting the monitoring facilities of CSP, while other issues deserve additional research, being more rooted into the CSP design. © 2017 ACM.",Content security policy; Formal methods; Web security,Copyrights; Security systems; Semantics; Websites; Constant maintenance; Content security; Formal Semantics; Large-scale analysis; Level 2; Misconfigurations; Security violations; WEB security; Formal methods
Knowledge graph embedding: A locally and temporally adaptive translation-based approach,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040324666&doi=10.1145%2f3132733&partnerID=40&md5=014bc07d9b0a5d74f16c933b6188457d,"A knowledge graph is a graph with entities of different types as nodes and various relations among them as edges. The construction of knowledge graphs in the past decades facilitates many applications, such as link prediction, web search analysis, question answering, and so on. Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, for example, TransE, TransH, and TransR, learn the embedding representation by defining a global margin-based loss function over the data. However, the loss function is determined during experiments whose parameters are examined among a closed set of candidates.Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidates, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this article, a locally adaptive translation method for knowledge graph embedding, called TransA, is proposed to find the loss function by adaptively determining its margin over different knowledge graphs. Then the convergence of TransA is verified from the aspect of its uniform stability. To make the embedding methods up-to-date when new vertices and edges are added into the knowledge graph, the incremental algorithm for TransA, called iTransA, is proposed by adaptively adjusting the optimal margin over time. Experiments on four benchmark data sets demonstrate the superiority of the proposed method, as compared to the state-of-the-art ones. © 2017 ACM.",Convergence; Knowledge graph embedding; Locally and temporally adaptive translation; Optimal margin,Graphic methods; Vector spaces; Convergence; Incremental algorithm; Knowledge graphs; Optimal margin; Question Answering; State of the art; Translation method; Uniform stability; Graph theory
Modeling and simulating the web of things from an information retrieval perspective,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040307488&doi=10.1145%2f3132732&partnerID=40&md5=aa1f4609715a9c5b18f2e6526f9062fd,"Internet and Web technologies have changed our lives in ways we are not yet fully aware of. In the near future, Internet will interconnect more than 50 billion things in the real world, nodes will sense billions of features and properties of interest, and things will be represented by web-based, bi-directional services with highly dynamic content and real-time data. This is the new era of the Internet and theWeb of Things. Since the emergence of such paradigms implies the evolution and integration of the systems with which they interact, it is essential to develop abstract models for representing and simulating theWeb of Things in order to establish new approaches. This article describes a Web of Things model based on a structured XML representation. We also present a simulator whose ultimate goal is to encapsulate the expected dynamics of the Web of Things for the future development of information retrieval (IR) systems. The simulator generates a real-time collection of XML documents containing spatio-temporal contexts and textual and sensed information of highly dynamic dimensions. The simulator is characterized by its flexibility and versatility for representing real-world scenarios and offers a unique perspective for information retrieval. In this article, we evaluate and test the simulator in terms of its performance variables for computing resource consumption and present our experimentation with the simulator on three real scenarios by considering the generation variables for the IR document collection. © 2017 ACM.",Discrete-event systems; Information retrieval; Simulation; Web of things,Abstracting; Discrete event simulation; Internet of things; Search engines; Simulators; XML; Computing resource; Document collection; Modeling and simulating; Performance variables; Real-time collection; Real-world scenario; Simulation; XML representation; Information retrieval
Caching to reduce mobile app energy consumption,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029786687&doi=10.1145%2f3125778&partnerID=40&md5=678678b5e03dda6cf8337a37a881582f,"Mobile applications consume device energy for their operations, and the fast rate of battery depletion on mobile devices poses a major usability hurdle. After the display, data communication is the second-biggest consumer of mobile device energy. At the same time, software applications that run on mobile devices represent a fast-growing product segment. Typically, these applications serve as front-end display mechanisms, which fetch data from remote servers and display the information to the user in an appropriate format-incurring significant data communication overheads in the process. In this work, we propose methods to reduce energy overheads in mobile devices due to data communication by leveraging data caching technology. A review of existing caching mechanisms revealed that they are primarily designed for optimizing response time performance and cannot be easily ported to mobile devices for energy savings. Further, architectural differences between traditional client-server and mobile communications infrastructures make the use of existing caching technologies unsuitable in mobile devices. In this article, we propose a set of two new caching approaches specifically designed with the constraints of mobile devices in mind: (a) a response caching approach and (b) an object caching approach. Our experiments show that, even for a small cache size of 250MB, object caching can reduce energy consumption on average by 45% compared to the no-cache case, and response caching can reduce energy consumption by 20% compared to the no-cache case. The benefits increase with larger cache sizes. These results demonstrate the efficacy of our proposed method and raise the possibility of significantly extending mobile device battery life. © 2017 ACM.",Caching; Energy efficiency; Mobile applications,Application programs; Convolutional codes; Display devices; Electric batteries; Energy conservation; Energy efficiency; Energy utilization; Mobile computing; Mobile devices; Caching; Caching technology; Data-communication; Mobile applications; Mobile communications; Mobile device batteries; Reduce energy consumption; Software applications; Mobile telecommunication systems
Activity recommendation with partners,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029796578&doi=10.1145%2f3121407&partnerID=40&md5=d6a29a8c0c3abf14b73f9788bb294f2d,"Recommending social activities, such as watching movies or having dinner, is a common function found in social networks or e-commerce sites. Besides certain websites which manage activity-related locations (e.g., foursquare.com), many items on product sale platforms (e.g., groupon.com) can naturally be mapped to social activities. For example, movie tickets can be thought of as activity items, which can be mapped as a social activity of ""watch a movie."" Traditional recommender systems estimate the degree of interest for a target user on candidate items (or activities), and accordingly, recommend the top-k activity items to the user. However, these systems ignore an important social characteristic of recommended activities: people usually tend to participate in those activities with friends. This article considers this fact for improving the effectiveness of recommendation in two directions. First, we study the problem of activity-partner recommendation; i.e., for each recommended activity item, find a suitable partner for the user. This (i) saves the user's time for finding activity partners, (ii) increases the likelihood that the activity item will be selected by the user, and (iii) improves the effectiveness of recommender systems to users overall and enkindles their social enthusiasm. Our partner recommender is built upon the users' historical attendance preferences, their social context, and geographic information. Moreover, we explore how to leverage the partner recommendation to help improve the effectiveness of recommending activities to users. Assuming that users tend to select the activities for which they can find suitable partners, we propose a partner-aware activity recommendation model, which integrates this hypothesis into conventional recommendation approaches. Finally, the recommended items not only match users' interests, but also have high chances to be selected by the users, because the users can find suitable partners to attend the corresponding activities together.We conduct experiments on real data to evaluate the effectiveness of activity-partner recommendation and partner-aware activity recommendation. The results verify that (i) suggesting partners greatly improves the likelihood that a recommended activity item is to be selected by the target user and (ii) considering the existence of suitable partners in the ranking of recommended items improves the accuracy of recommendation significantly. © 2017 ACM.",Location-based social network; Recommendation system,Electronic commerce; Motion pictures; Social networking (online); Degree of interests; E-commerce sites; Geographic information; Location-based social networks; Social activities; Social context; Two directions; Users' interests; Recommender systems
A fast and scalable mechanism for web service composition,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029806793&doi=10.1145%2f3098884&partnerID=40&md5=b1bfdb54d0c7f7b242c46ca980ec4e74,"In recent times, automated business processes and web services have become ubiquitous in diverse application spaces. Efficient composition of web services in real time while providing necessary Quality of Service (QoS) guarantees is a computationally complex problem and several heuristic based approaches have been proposed to compose the services optimally. In this article, we present the design of a scalable QoS-aware service composition mechanism that balances the computational complexity of service composition with the QoS guarantees of the composed service and achieves scalability. Our design guarantees a single QoS parameter using an intelligent search and pruning mechanism in the composed service space. We also show that our methodology yields near optimal solutions on real benchmarks. We then enhance our proposed mechanism to guarantee multiple QoS parameters using aggregation techniques. Finally, we explore search time versus solution quality tradeoff using parameterized search algorithms that produce better-quality solutions at the cost of delay. We present experimental results to show the efficiency of our proposed mechanism. © 2017 ACM.",Parameterized search algorithm; Quality of service (QoS); Rank aggregation; Service composition,Learning algorithms; Optimization; Parameter estimation; Telecommunication services; Web services; Websites; Aggregation techniques; Near-optimal solutions; QoS-aware service compositions; Quality of service (QoS) guarantees; Rank aggregation; Search Algorithms; Service compositions; Web service composition; Quality of service
Adaptive knowledge propagation in Web ontologies,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028541171&doi=10.1145%2f3105961&partnerID=40&md5=e0daf81175600486e36a134ad8b05f11,"We focus on the problem of predicting missing assertions in Web ontologies. We start from the assumption that individual resources that are similar in some aspects are more likely to be linked by specific relations: this phenomenon is also referred to as homophily and emerges in a variety of relational domains. In this article, we propose a method for (1) identifying which relations in the ontology are more likely to link similar individuals and (2) efficiently propagating knowledge across chains of similar individuals. By enforcing sparsity in the model parameters, the proposed method is able to select only the most relevant relations for a given prediction task. Our experimental evaluation demonstrates the effectiveness of the proposed method in comparison to state-of-the-art methods from the literature. © 2017 ACM.",Label propagation; Semantic web,Computer networks; Internet; Semantic Web; Experimental evaluation; Knowledge propagation; Label propagation; Model parameters; Prediction tasks; Relevant relations; Similar individuals; State-of-the-art methods; Ontology
Recommendation in a changingworld: Exploiting temporal dynamics in ratings and reviews,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028550636&doi=10.1145%2f3108238&partnerID=40&md5=7cda78d27435d0b777cbca9c0ab7f953,"Users' preferences, and consequently their ratings and reviews to items, change over time. Likewise, characteristics of items are also time-varying. By dividing data into time periods, temporal Recommender Systems (RSs) improve recommendation accuracy by exploring the temporal dynamics in user rating data. However, temporal RSs have to cope with rating sparsity in each time period. Meanwhile, reviews generated by users contain rich information about their preferences, which can be exploited to address rating sparsity and further improve the performance of temporal RSs. In this article, we develop a temporal rating model with topics that jointly mines the temporal dynamics of both user-item ratings and reviews. Studying temporal drifts in reviews helps us understand item rating evolutions and user interest changes over time. Our model also automatically splits the review text in each time period into interim words and intrinsic words. By linking interim words and intrinsic words to short-term and long-term item features, respectively, we jointly mine the temporal changes in user and item latent features together with the associated review text in a single learning stage. Through experiments on 28 real-world datasets collected from Amazon, we show that the rating prediction accuracy of our model significantly outperforms the existing state-of-art RS models. And our model can automatically identify representative interim words in each time period as well as intrinsic words across all time periods. This can be very useful in understanding the time evolution of users' preferences and items' characteristics. © 2017 ACM.",Recommender Systems; Temporal dynamics; Topic models,Dynamics; Recommender systems; Prediction accuracy; Real-world datasets; Recommendation accuracy; Temporal change; Temporal drifts; Temporal dynamics; Time evolutions; Topic model; Rating
Exploring the emerging type of comment for online videos: DanMu,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028540339&doi=10.1145%2f3098885&partnerID=40&md5=1c5ddeb5db90bef34c9761777fcf9a64,"DanMu, an emerging type of user-generated comment, has become increasingly popular in recent years. Many online video platforms such as Tudou.com have provided the DanMu function. Unlike traditional online reviews such as reviews at Youtube.com that are outside the videos, DanMu is a scrolling marquee comment, which is overlaid directly on top of the video and synchronized to a specific playback time. Such comments are displayed as streams of moving subtitles overlaid on the video screen. Viewers could easily write DanMus while watching videos, and the written DanMus will be immediately overlaid onto the video and displayed to writers themselves and other viewers as well. Such DanMu systems have greatly enabled users to communicate with each other in a much more direct way, creating a real-time sharing experience. Although there are several unique features of DanMu and has had a great impact on online video systems, to the best of our knowledge, there is no work that has provided a comprehensive study on DanMu. In this article, as a pilot study, we analyze the unique characteristics of DanMu from various perspectives. Specifically, we first illustrate some unique distributions of DanMus by comparing with traditional reviews (TReviews) that we collected from a real DanMu-enabled online video system. Second, we discover two interesting patterns in DanMu data: a herding effect and multiple-burst phenomena that are significantly different from those in TRviews and reveal important insights about the growth of DanMus on a video. Towards exploring antecedents of both th herding effect and multiple-burst phenomena, we propose to further detect leading DanMus within bursts, because those leading DanMus make the most contribution to both patterns. A framework is proposed to detect leading DanMus that effectively combines multiple factors contributing to leading DanMus. Based on the identified characteristics of DanMu, finally we propose to predict the distribution of future DanMus (i.e., the growth of DanMus), which is important for many DanMu-enabled online video systems, for example, the predicted DanMu distribution could be an indicator of video popularity. This prediction task includes two aspects: One is to predict which videos future DanMus will be posted for, and the other one is to predict which segments of a video future DanMus will be posted on.We develop two sophisticated models to solve both problems. Finally, intensive experiments are conducted with a real-world dataset to validate all methods developed in this article. © 2017 ACM.",Burst detection; DanMu; Herding effect; Leading danmus,Forecasting; Real time systems; Time sharing systems; Burst detection; DanMu; Herding effect; Leading danmus; Multiple factors; Prediction tasks; Scrolling marquees; Unique features; Online systems
Exploring and analyzing the tor hidden services graph,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026636081&doi=10.1145%2f3008662&partnerID=40&md5=22830e1234a679473f6d0a43f6b969c8,"The exploration and analysis of Web graphs has flourished in the recent past, producing a large number of relevant and interesting research results. However, the unique characteristics of the Tor network limit the applicability of standard techniques and demand for specific algorithms to explore and analyze it. The attention of the research community has focused on assessing the security of the Tor infrastructure (i.e., its ability to actually provide the intended level of anonymity) and on discussing what Tor is currently being used for. Since there are no foolproof techniques for automatically discovering Tor hidden services, little or no information is available about the topology of the Tor Web graph. Even less is known on the relationship between content similarity and topological structure. The present article aims at addressing such lack of information. Among its contributions: A study on automatic Tor Web exploration/data collection approaches; the adoption of novel representative metrics for evaluating Tor data; a novel in-depth analysis of the hidden services graph; a rich correlation analysis of hidden services' semantics and topology. Finally, a broad interesting set of novel insights/considerations over the TorWeb organization and content are provided. © 2017 ACM.",Automatic web exploration; Correlation analysis; Network topology; Web graphs,Correlation methods; Network security; Semantics; Content similarity; Correlation analysis; In-depth analysis; Network topology; Research communities; Research results; Topological structure; Web graphs; Topology
Collusive opinion fraud detection in online reviews: A probabilistic modeling approach,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026664845&doi=10.1145%2f3098859&partnerID=40&md5=3bd776a750554e00fe61559082437cce,"We address the collusive opinion fraud problem in online review portals, where groups of people work together to deliver deceptive reviews formanipulating the reputations of targeted items. Such collusive fraud is considered much harder to defend against, since the participants (or colluders) can evade detection by shaping their behaviors collectively so as not to appear suspicious. To alleviate this problem, countermeasures have been proposed that leverage the collective behaviors of colluders. The motivation stems from the observation that colluders typically act in a very synchronized way, as they are instructed by the same campaigns with common items to target and schedules to follow. However, the collective behaviors examined in existing solutions focus mostly on the external appearance of fraud campaigns, such as the campaign size and the size of the targeted item set. These signals may become ineffective once colluders have changed their behaviors collectively. Moreover, the detection algorithms used in existing approaches are designed to only make collusion inference on the input data; predictive models that can be deployed for detecting emerging fraud cannot be learned from the data. In this article, to complement existing studies on collusive opinion fraud characterization and detection, we explore more subtle behavioral trails in collusive fraud practice. In particular, a suite of homogeneity-based measures are proposed to capture the interrelationships among colluders within campaigns.Moreover, a novel statistical model is proposed to further characterize, recognize, and predict collusive fraud in online reviews. The proposed model is fully unsupervised and highly flexible to incorporate effective measures available for better modeling and prediction. Through experiments on two real-world datasets, we show that our method outperforms the state of the art in both characterization and detection abilities. © 2017 ACM.",Collusive fraud; Fraud detection; Opinion manipulation; Prediction and inference; Probabilistic modeling,Forecasting; Inference engines; Collusive frauds; Detection algorithm; Fraud detection; Modeling and predictions; Opinion manipulation; Probabilistic modeling; Real-world datasets; Statistical modeling; Crime
Clickstream user behavior models,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026655170&doi=10.1145%2f3068332&partnerID=40&md5=b484ceab1fbccce07b4a4fe52a94c6e3,"The next generation of Internet services is driven by users and user-generated content. The complex nature of user behavior makes it highly challenging to manage and secure online services. On one hand, service providers cannot effectively prevent attackers from creating large numbers of fake identities to disseminate unwanted content (e.g., spam). On the other hand, abusive behavior from real users also poses significant threats (e.g., cyberbullying). In this article, we propose clickstream models to characterize user behavior in large online services. By analyzing clickstream traces (i.e., sequences of click events from users), we seek to achieve two goals: (1) detection: to capture distinct user groups for the detection ofmalicious accounts, and (2) understanding: to extract semantic information from user groups to understand the captured behavior. To achieve these goals, we build two related systems. The first one is a semisupervised system to detect malicious user accounts (Sybils). The core idea is to build a clickstream similarity graph where each node is a user and an edge captures the similarity of two users' clickstreams. Based on this graph, we propose a coloring scheme to identify groups of malicious accounts without relying on a large labeled dataset.We validate the system using groundtruth clickstream traces of 16,000 real and Sybil users from Renren, a large Chinese social network. The second system is an unsupervised system that aims to capture and understand the fine-grained user behavior. Instead of binary classification (malicious or benign), this model identifies the natural groups of user behavior and automatically extracts features to interpret their semantic meanings. Applying this system to Renren and another online social network,Whisper (100K users), we help service providers identify unexpected user behaviors and even predict users' future actions. Both systems received positive feedback from our industrial collaborators including Renren, LinkedIn, and Whisper after testing on their internal clickstream data. © 2017 ACM.",Behavior model; Clickstream; Online social networks; Spam and abuse,Internet service providers; Online systems; Semantics; Social networking (online); Websites; Behavior model; Binary classification; Clickstreams; Industrial collaborators; On-line social networks; Semantic information; Spam and abuse; User-generated content; Behavioral research
Canonical forms for isomorphic and equivalent RDF graphs: Algorithms for leaning and labelling blank nodes,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026631835&doi=10.1145%2f3068333&partnerID=40&md5=cbb918662539357983a70556ec2986d0,"Existential blank nodes greatly complicate a number of fundamental operations on Resource Description Framework (RDF) graphs. In particular, the problems of determining if two RDF graphs have the same structure modulo blank node labels (i.e., if they are isomorphic), or determining if two RDF graphs have the same meaning under simple semantics (i.e., if they are simple-equivalent), have no known polynomial-time algorithms. In this article, we propose methods that can produce two canonical forms of an RDF graph. The first canonical form preserves isomorphism such that any two isomorphic RDF graphs will produce the same canonical form; this iso-canonical form is produced by modifying the well-known canonical labelling algorithm NAUTY for application to RDF graphs. The second canonical form additionally preserves simpleequivalence such that any two simple-equivalent RDF graphs will produce the same canonical form; this equi-canonical form is produced by, in a preliminary step, leaning the RDF graph, and then computing the iso-canonical form. These algorithms have a number of practical applications, such as for identifying isomorphic or equivalent RDF graphs in a large collection without requiring pairwise comparison, for computing checksums or signing RDF graphs, for applying consistent Skolemisation schemes where blank nodes are mapped in a canonical manner to Internationalised Resource Identifiers (IRIs), and so forth. Likewise a variety of algorithms can be simplified by presupposing RDF graphs in one of these canonical forms. Both algorithms require exponential steps in the worst case; in our evaluation we demonstrate that there indeed exist difficult synthetic cases, but we also provide results over 9.9 million RDF graphs that suggest such cases occur infrequently in the real world, and that both canonical forms can be efficiently computed in all but a handful of such cases. © 2017 ACM.",Isomorphism; Linked data; Semantic web; Signing; Skolemisation,Graphic methods; Polynomial approximation; Semantic Web; Set theory; Fundamental operations; Isomorphism; Linked datum; Pair-wise comparison; Polynomial-time algorithms; Resource description framework; Signing; Skolemisation; Graph theory
A study of web print: What people print in the digital era,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026624838&doi=10.1145%2f3068331&partnerID=40&md5=1e27efffc9464e1757af096e635e2272,"This article analyzes a proprietary log of printed web pages and aims at answering questions regarding the content people print (what), the reasons they print (why), as well as attributes of their print profile (who). We present a classification of pages printed based on their print intent and we describe our methodology for processing the print dataset used in this study. In our analysis, we study the web sites, topics, and print intent of the pages printed along the following aspects: popularity, trends, activity, user diversity, and consistency. We present several findings that reveal interesting insights into printing. We analyze our findings and discuss their impact and directions for future work. © 2017 ACM.",Print intent; User log analysis; Web print study,Classification (of information); Digital era; User log; Web print study; Websites
LDoW-PaN: Linked data on the web-presentation and navigation,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026647988&doi=10.1145%2f2983643&partnerID=40&md5=e4c7ef87a952ed346caf3f86aa197751,"This work aimed to propose LDoW-PaN, a Linked Data presentation and navigation model focused on the average user. The LDoW-PaN model is an extension of the Dexter Hypertext Reference Model. Through the LDoW-PaN model, ordinary people-who have no experience with technologies that involve the Linked Data environment-can interact with the Web of Data (RDF) more closely related to how they interact with the Web of Documents (HTML). To evaluate the proposal, some tools were developed, including the following: (i) a Web Service, which implements the lower-level layers of the LDoW-PaN model; (ii) a client-side script library, which implements the presentation and navigation layer; and (iii) a browser extension, which uses these tools to provide Linked Data presentation and navigation to users browsing the Web. The browser extension was developed using user interface approaches that are well known, well accepted, and evaluated by the Web research community, such as faceted navigation and presentation through tooltips. Therefore, the prototype evaluation included: usability evaluation through two classical techniques; computational complexity measures; and an analysis of the performance of the operations provided by the proposed model. © 2017 ACM.",LDoW-PaN; Linked data; Model; Navigation; Presentation; Web of data,Data handling; Hypertext systems; Models; Navigation; User interfaces; Classical techniques; Computational complexity measure; LDoW-PaN; Linked datum; Presentation; Research communities; Usability evaluation; Web of datum; Web services
Location-based distance measures for geosocial similarity,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024496069&doi=10.1145%2f3054951&partnerID=40&md5=57dceed40024ec21fd2ff2e29a5efb9c,"This article investigates the problem of geosocial similarity among users of online social networks, based on the locations of their activities (e.g., posting messages or photographs). Finding pairs of geosocially similar users or detecting that two sets of locations (of activities) belong to the same user has important applications in privacy protection, recommendation systems, urban planning, and public health, among others. It is explained and shown empirically that common distance measures between sets of locations are inadequate for determining geosocial similarity. Two novel distance measures between sets of locations are introduced. One is the mutually nearest distance that is based on computing a matching between two sets. The second measure uses a quad-tree index. It is highly scalable but incurs the overhead of creating and maintaining the index. Algorithms with optimization techniques are developed for computing the two distance measures and also for finding the k-most-similar users of a given one. Extensive experiments, using geotagged messages from Twitter, show that the new distance measures are both more accurate and more efficient than existing ones. © 2017 ACM.",Earth mover's distance; Geosocial networks; Geosocial similarity; Geospatial similarity; Geotagged posts; Hausdorff distance; Set distance; Social media; Sociospatial analysis,Social networking (online); Earth Mover's distance; Geo-social networks; Geo-spatial; Geosocial similarity; Geotagged posts; Hausdorff distance; Set distances; Social media; Sociospatial analysis; Location
Nucleus decompositions for identifying hierarchy of dense subgraphs,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024486412&doi=10.1145%2f3057742&partnerID=40&md5=52e1bf32cf66f593c0d78f3c83cf0a69,"Finding dense substructures in a graph is a fundamental graph mining operation, with applications in bioinformatics, social networks, and visualization to name a few. Yet most standard formulations of this problem (like clique, quasi-clique, densest at-least-k subgraph) are NP-hard. Furthermore, the goal is rarely to find the ""true optimum"" but to identify many (if not all) dense substructures, understand their distribution in the graph, and ideally determine relationships among them. Current dense subgraph finding algorithms usually optimize some objective and only find a few such subgraphs without providing any structural relations. We define the nucleus decomposition of a graph, which represents the graph as a forest of nuclei. Each nucleus is a subgraph where smaller cliques are present in many larger cliques. The forest of nuclei is a hierarchy by containment, where the edge density increases as we proceed towards leaf nuclei. Sibling nuclei can have limited intersections, which enables discovering overlapping dense subgraphs. With the right parameters, the nucleus decomposition generalizes the classic notions of k-core and k-truss decompositions. We present practical algorithms for nucleus decompositions and empirically evaluate their behavior in a variety of real graphs. The tree of nuclei consistently gives a global, hierarchical snapshot of dense substructures and outputs dense subgraphs of comparable quality with the state-of-the-art solutions that are dense and have non-trivial sizes. Our algorithms can process real-world graphs with tens of millions of edges in less than an hour. We demonstrate how proposed algorithms can be utilized on a citation network. Our analysis showed that dense units identified by our algorithms correspond to coherent articles on a specific area. Our experiments also show that we can identify dense structures that are lost within larger structures by other methods and find further finer grain structure within dense groups. © 2017 ACM.",Dense subgraph discovery; Density hierarchy; Graph decomposition; K-core; K-truss; Overlapping dense subgraphs,Forestry; Trusses; Citation networks; Dense structures; Dense sub-graphs; Dense subgraph; Finding algorithm; Graph decompositions; K-cores; Real-world graphs; Optimization
Modeling and evaluating a robust feedback-based reputation system for e-commerce platforms,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025116832&doi=10.1145%2f3057265&partnerID=40&md5=2d16daf69e2902b46f7cc52ee101e767,"Despite the steady growth of e-commerce communities in the past two decades, little has changed in the way these communities manage reputation for building trust and for protecting their member's financial interests against fraud. As these communities mature and the defects of their reputation systems are revealed, further potential for deception against their members is created, that pushes the need for novel reputation mechanisms. Although a high volume of research works has explored the concepts of reputation and trust in e-communities, most of the proposed reputation systems target decentralized e-communities, focusing on issues related with the decentralized reputationmanagement; they have not thus been integrated in e-commerce platforms. This work's objective is to provide an attackresilient feedback-based reputation system for modern e-commerce platforms, while minimizing the incurred financial burden of potent security schemes. Initially, we discuss a series of attacks and issues in reputation systems and study the different approaches of these problems from related works, while also considering the structural properties, defense mechanisms and policies of existing platforms. Then we present our proposition for a robust reputation system which consists of a novel reputation metric and attack prevention mechanisms. Finally, we describe the simulation framework and tool that we have implemented for thoroughly testing and evaluating the metric's resilience against attacks and present the evaluation experiments and their results. We consider the presented simulation framework as the second contribution of our article, aiming at facilitating the simulation and elaborate evaluation of reputation systems which specifically target e-commerce platforms by thoroughly presenting it, exhibiting its usage and making it available to the research community. © 2017 ACM.",Attacks; Collusion; Evaluation; Incentives; Reputation systems; Unfair ratings,Commerce; Electronic commerce; Attacks; Collusion; Evaluation; Incentives; Reputation systems; Unfair ratings; Distributed computer systems
Wiser: A multi-dimensional framework for searching and ranking Web APIs,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024474997&doi=10.1145%2f3061710&partnerID=40&md5=3e59f2a96c4b65a508b7bbde8c04a374,"Mashups are agile applications that aggregate RESTful services, developed by third parties, whose functions are exposed as Web Application Program Interfaces (APIs) within public repositories. From mashups developers' viewpoint, Web API search may benefit from selection criteria that combine several dimensions used to describe the APIs, such as categories, tags, and technical features (e.g., protocols and data formats). Nevertheless, other dimensions might be fruitfully exploited to support Web API search. Among them, past API usage experiences by other developers may be used to suggest the right APIs for a target application. Past experiences might emerge from the co-occurrence of Web APIs in the same mashups. Ratings assigned by developers after using the Web APIs to create their own mashups or after using mashups developed by others can be considered as well. This article aims to advance the current state of the art for Web API search and ranking from mashups developers' point of view, by addressing two key issues: multi-dimensional modeling and multi-dimensional framework for selection. The model for Web API characterization embraces multiple descriptive dimensions, by considering several public repositories, that focus on different and only partially overlapping dimensions. The proposed Web API selection framework, called WISeR (Web apI Search and Ranking), is based on functions devoted to developers to exploit the multi-dimensional descriptions, in order to enhance the identification of candidate Web APIs to be proposed, according to the given requirements. Furthermore, WISeR adapts to changes that occur during the Web API selection and mashup development, by revising the dimensional attributes in order to conform to developers' preferences and constraints. We also present an experimental evaluation of the framework. © 2017 ACM.",Mashups; Multi-dimensional web API model; RESTful services; Web API search and ranking,Application programs; API modeling; API searches; Experimental evaluation; Mashups; Multi-dimensional model; RESTful Services; Searching and ranking; Selection framework; Application programming interfaces (API)
A Bayesian method for comparing hypotheses about human trails,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021828278&doi=10.1145%2f3054950&partnerID=40&md5=d6bf18dccff909f8d264c79875a63261,"When users interact with the Web today, they leave sequential digital trails on a massive scale. Examples of such human trails includeWeb navigation, sequences of online restaurant reviews, or onlinemusic play lists. Understanding the factors that drive the production of these trails can be useful, for example, for improving underlying network structures, predicting user clicks, or enhancing recommendations. In this work, we present a method called HypTrails for comparing a set of hypotheses about human trails on the Web, where hypotheses represent beliefs about transitions between states. Our method utilizes Markov chain models with Bayesian inference. The main idea is to incorporate hypotheses as informative Dirichlet priors and to calculate the evidence of the data under them. For eliciting Dirichlet priors from hypotheses, we present an adaption of the so-called (trial) roulette method, and to compare the relative plausibility of hypotheses, we employ Bayes factors. We demonstrate the general mechanics and applicability of HypTrails by performing experiments with (i) synthetic trails for which we control the mechanisms that have produced them and (ii) empirical trails stemming from different domains including Web site navigation, business reviews, and online music played. Our work expands the repertoire of methods available for studying human trails. © 2017 ACM.",Bayesian statistics; Human trails; Hypotheses; Markov chain; Paths; Sequences; Sequential human behavior; Web,Bayesian networks; Chains; Digital storage; Inference engines; Markov processes; Bayesian statistics; Human behaviors; Human trails; Hypotheses; Paths; Sequences; Behavioral research
Multirelational recommendation in heterogeneous networks,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021810870&doi=10.1145%2f3054952&partnerID=40&md5=3687ae57b8b818379c19255e8c9cb500,"Recommender systems are key components in information-seeking contexts where personalization is sought. However, the dominant framework for recommendation is essentially two dimensional, with the interaction between users and items characterized by a single relation. In many cases, such as social networks, users and items are joined in a complex web of relations, not readily reduced to a single value. Recent multirelational approaches to recommendation focus on the direct, proximal relations in which users and items may participate. Our approach uses the framework of complex heterogeneous networks to represent such recommendation problems. We propose the weighted hybrid of low-dimensional recommenders (WHyLDR) recommendation model, which uses extended relations, represented as constrained network paths, to effectively augment direct relations. This model incorporates influences from both distant and proximal connections in the network. The WHyLDR approach raises the problem of the unconstrained proliferation of components, built from ever-extended network paths. We show that although component utility is not strictly monotonic with path length, a measure based on information gain can effectively prune and optimize such hybrids. © 2017 ACM.",Heteroegeous information networks Meta-paths; Hybrid recommender systems; Information gain; Multi-relational recommender systems,Complex networks; Heterogeneous networks; Information services; Extended networks; Hybrid recommender systems; Information gain; Information networks; Information seeking; Low dimensional; Network paths; Personalizations; Recommender systems
An empirical investigation of ecommerce-reputation-escalation-as-A-service,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019617751&doi=10.1145%2f2983646&partnerID=40&md5=fdbebd7c0162c3f8add9a10515a4c9c8,"In online markets, a store's reputation is closely tied to its profitability. Sellers' desire to quickly achieve a high reputation has fueled a profitable underground business that operates as a specialized crowdsourcing marketplace and accumulates wealth by allowing online sellers to harness human laborers to conduct fake transactions to improve their stores' reputations. We term such an underground market a seller-reputationescalation (SRE) market. In this article, we investigate the impact of the SRE service on reputation escalation by performing in-depth measurements of the prevalence of the SRE service, the business model and market size of SRE markets, and the characteristics of sellers and offered laborers. To this end, we have infiltrated five SRE markets and studied their operations using daily data collection over a continuous period of 2 months. We identified more than 11,000 online sellers posting at least 219,165 fake-purchase tasks on the five SRE markets. These transactions earned at least $46,438 in revenue for the five SRE markets, and the total value of merchandise involved exceeded $3,452,530. Our study demonstrates that online sellers using the SRE service can increase their stores' reputations at least 10 times faster than legitimate ones while about 25% of them were visibly penalized. Even worse, we found a much stealthier and more hazardous service that can, within a single day, boost a seller's reputation by such a degree that would require a legitimate seller at least a year to accomplish. Armed with our analysis of the operational characteristics of the underground economy, we offer some insights into potential mitigation strategies. Finally, we revisit the SRE ecosystem 1 year later to evaluate the latest dynamism of the SRE markets, especially the statuses of the online stores once identified to launch fake-transaction campaigns on the SRE markets. We observe that the SRE markets are not as active as they were 1 year ago and about 17% of the involved online stores become inaccessible likely because they have been forcibly shut down by the corresponding E-commerce marketplace for conducting fake transactions.",E-commerce; Fake transaction; Reputation manipulation,Commerce; Economics; Profitability; Business modeling; Depth measurements; Empirical investigation; Fake transaction; Mitigation strategy; Online markets; Operational characteristics; Reputation manipulation; Electronic commerce
Value and misinformation in collaborative investing platforms,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019866460&doi=10.1145%2f3027487&partnerID=40&md5=06c0b4eb12948ea3bf4d442f7d783ce6,"It is often difficult to separate the highly capable ""experts"" from the average worker in crowdsourced systems. This is especially true for challenge application domains that require extensive domain knowledge. The problem of stock analysis is one such domain, where even the highly paid, well-educated domain experts are prone to make mistakes. As an extremely challenging problem space, the ""wisdom of the crowds"" property that many crowdsourced applications rely on may not hold. In this article, we study the problem of evaluating and identifying experts in the context of SeekingAlpha and StockTwits, two crowdsourced investment services that have recently begun to encroach on a space dominated for decades by large investment banks. We seek to understand the quality and impact of content on collaborative investment platforms, by empirically analyzing complete datasets of SeekingAlpha articles (9 years) and StockTwits messages (4 years). We develop sentiment analysis tools and correlate contributed content to the historical performance of relevant stocks. While SeekingAlpha articles and StockTwits messages provide minimal correlation to stock performance in aggregate, a subset of experts contribute more valuable (predictive) content. We show that these authors can be easily identified by user interactions, and investments based on their analysis significantly outperform broader markets. This effectively shows that even in challenging application domains, there is a secondary or indirect wisdom of the crowds. Finally, we conduct a user survey that sheds light on users' views of SeekingAlpha content and stock manipulation. We also devote efforts to identify potential manipulation of stocks by detecting authors controlling multiple identities. © 2017 ACM.",Crowdsourcing; sentiment analysis; stock market,Commerce; Crowdsourcing; Data mining; Financial markets; Crowdsourced systems; Domain knowledge; Historical performance; Multiple identities; Sentiment analysis; Stock performance; User interaction; Wisdom of the crowds; Investments
Periodicity in user engagement with a search engine and its application to online controlled experiments,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018623399&doi=10.1145%2f2856822&partnerID=40&md5=c85097d9973f50bb51418f4c344a51cf,"Nowadays, billions of people use the Web in connection with their daily needs. A significant part of these needs are constituted by search tasks that are usually addressed by search engines. Thus, daily search needs result in regular user engagement with a search engine. User engagement with web services was studied in various aspects, but there appears to be little work devoted to its regularity and periodicity. In this article, we study periodicity of user engagement with a popular search engine through applying spectrum analysis to temporal sequences of different engagement metrics. First, we found periodicity patterns of user engagement and revealed classes of users whose periodicity patterns do not change over a long period of time. In addition, we give an exhaustive analysis of the stability and quality of identified clusters. Second, we used the spectrum series as key metrics to evaluate search quality. We found that the novel periodicity metrics outperform the state-of-the-art quality metrics both in terms of significance level ( p-value) and sensitivity to a large set of larges-scale A/B experiments conducted on real search engine users. © 2017 ACM.",A/B test; Amplitude; DFT; Discrete Fourier transform; Frequency domain; Key metric; OAC; OEC; Online controlled experiment; Overall acceptance criterion; Overall evaluation criterion; Periodicity; Quality metrics; Search engine; Spectrum analysis; User engagement,Acceptance tests; Design for testability; Discrete Fourier transforms; Frequency domain analysis; Search engines; Spectrum analysis; Web services; Acceptance criteria; Amplitude; Evaluation criteria; Frequency domains; Key metric; Online controlled experiments; Periodicity; Quality metrics; User engagement; Quality control
Analyzing the adoption and cascading process of OSN-based gifting applications: An empirical study,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018657264&doi=10.1145%2f3023871&partnerID=40&md5=104b4a124b9f823050143847fd757681,"To achieve growth in the user base of online social networks - (OSN) based applications, word-of-mouth diffusion mechanisms, such as user-to-user invitations, are widely used. This article characterizes the adoption and cascading process of OSN-based applications that grow via user invitations. We analyze a detailed large-scale dataset of a popular Facebook gifting application, iHeart, that contains more than 2 billion entries of user activities generated by 190 million users during a span of 64 weeks. We investigate (1) how users invite their friends to an OSN-based application, (2) how application adoption of an individual user can be predicted, (3) what factors drive the cascading process of application adoptions, and (4) what are the good predictors of the ultimate cascade sizes. We find that sending or receiving a large number of invitations does not necessarily help to recruit new users to iHeart. We also find that the average success ratio of inviters is the most important feature in predicting an adoption of an individual user, which indicates that the effectiveness of inviters has strong predictive power with respect to application adoption. Based on the lessons learned from our analyses, we build and evaluate learning-based models to predict whether a user will adopt iHeart. Our proposed model that utilizes additional activity information of individual users from other similar types of gifting applications can achieve high precision (83%) in predicting adoptions in the target application (i.e., iHeart). We next identify a set of distinctive features that are good predictors of the growth of the application adoptions in terms of final population size. We finally propose a prediction model to infer whether a cascade of application adoption will continue to grow in the future based on observing the initial adoption process. Results show that our proposed model can achieve high precision (over 80%) in predicting large cascades of application adoptions. We believe our work can give an important implication in resource allocation of OSN-based product stakeholders, for example, via targeted marketing. © 2017 ACM.",Application adoption; Cascade; Computational social science; Facebook; Human behavior; Information diffusion; Online social networks; Prediction,Cascades (fluid mechanics); Forecasting; Population statistics; Social networking (online); Social sciences; Computational social science; Facebook; Human behaviors; Information diffusion; On-line social networks; Behavioral research
Nonlinear dynamics of information diffusion in social networks,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018839618&doi=10.1145%2f3057741&partnerID=40&md5=a665a3eca6db5745cb4377d5a5c2c84e,"The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated the faster propagation of news and rumors. How quickly does a piece of news spread over these media? How does its popularity diminish over time? Does the rising and falling pattern follow a simple universal law? In this article, we propose SPIKEM, a concise yet flexible analytical model of the rise and fall patterns of information diffusion. Our model has the following advantages. First, unification power: it explains earlier empirical observations and generalizes theoretical models including the SI and SIR models. We provide the threshold of the take-off versus die-out conditions for SPIKEMand discuss the generality of our model by applying it to an arbitrary graph topology. Second, practicality: it matches the observed behavior of diverse sets of real data. Third, parsimony: it requires only a handful of parameters. Fourth, usefulness: it makes it possible to perform analytic tasks such as forecasting, spotting anomalies, and interpretation by reverse engineering the system parameters of interest (quality of news, number of interested bloggers, etc.). We also introduce an efficient and effective algorithm for the real-Time monitoring of information diffusion, namely SPIKESTREAM, which identifies multiple diffusion patterns in a large collection of online event streams. Extensive experiments on real datasets demonstrate that SPIKEM accurately and succinctly describes all patterns of the rise and fall spikes in social networks. © 2017 ACM.",Information diffusion; Nonlinear modeling; Social networks,Reverse engineering; Search engines; Social networking (online); Arbitrary graphs; Diffusion patterns; Effective algorithms; Event streams; Information diffusion; Non-linear model; Real data sets; Real time monitoring; Topology
On obstructing obscenity obfuscation,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018851058&doi=10.1145%2f3032963&partnerID=40&md5=2c3f905428ba1c00e8ba127b0f99852e,"Obscenity (the use of rude words or offensive expressions) has spread from informal verbal conversations to digital media, becoming increasingly common on user-generated comments found inWeb forums, newspaper user boards, social networks, blogs, and media-sharing sites. The basic obscenity-blocking mechanism is based on verbatim comparisons against a blacklist of banned vocabulary; however, creative users circumvent these filters by obfuscating obscenity with symbol substitutions or bogus segmentations that still visually preserve the original semantics, such as writing shit as $ht or s.h.i.t or even worse mixing them as $.h...t. The number of potential obfuscated variants is combinatorial, yielding the verbatim filter impractical. Here we describe a method intended to obstruct this anomaly inspired by sequence alignment algorithms used in genomics, coupled with a tailor-made edit penalty function. The method only requires to set up the vocabulary of plain obscenities; no further training is needed. Its complexity on screening a single obscenity is linear, both in runtime and memory, on the length of the user-generated text. We validated the method on three different experiments. The first one involves a new dataset that is also introduced in this article; it consists of a set of manually annotated real-life comments in Spanish, gathered from the news user boards of an online newspaper, containing this type of obfuscation. The second one is a publicly available dataset of comments in Portuguese from a sports Web site. In these experiments, at the obscenity level, we observed recall rates greater than 90%, whereas precision rates varied between 75% and 95%, depending on their sequence length (shorter lengths yielded a higher number of false alarms). On the other hand, at the comment level, we report recall of 86%, precision of 91%, and specificity of 98%. The last experiment revealed that the method is more effective in matching this type of obfuscation compared to the classical Levenshtein edit distance. We conclude discussing the prospects of the method to help enforcing moderation rules of obscenity expressions or as a preprocessing mechanism for sequence cleaning and/or feature extraction in more sophisticated text categorization techniques. © 2017 ACM.",Obscenity obfuscation dataset; String deobfuscation; User-generated content moderation,Digital storage; Feature extraction; Newsprint; Semantics; Text processing; Blocking mechanisms; Deobfuscation; Levenshtein edit distance; Number of false alarms; Obscenity obfuscation dataset; Sequence alignments; Text categorization; User-generated content; Information dissemination
Toward automated online photo privacy,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017191949&doi=10.1145%2f2983644&partnerID=40&md5=684e361fdb3deb8ae0f4134d75f401e6,"Online photo sharing is an increasingly popular activity for Internet users. More and more users are now constantly sharing their images in various social media, from social networking sites to online communities, blogs, and content sharing sites. In this article, we present an extensive study exploring privacy and sharing needs of users' uploaded images. We develop learning models to estimate adequate privacy settings for newly uploaded images, based on carefully selected image-specific features. Our study investigates both visual and textual features of images for privacy classification. We consider both basic image-specific features, commonly used for image processing, as well as more sophisticated and abstract visual features. Additionally, we include a visual representation of the sentiment evoked by images. To our knowledge, sentiment has never been used in the context of image classification for privacy purposes. We identify the smallest set of features, that by themselves or combined together with others, can perform well in properly predicting the degree of sensitivity of users' images. We consider both the case of binary privacy settings (i.e., public, private), as well as the case of more complex privacy options, characterized by multiple sharing options. Our results show that with few carefully selected features, one may achieve high accuracy, especially when high-quality tags are available. © 2017 ACM.",Image analysis; Machine learning; Privacy; Social networks,Abstracting; Data privacy; Image analysis; Image classification; Image processing; Learning systems; Websites; Degree of sensitivity; On-line communities; Online Photo Sharing; Privacy classifications; Privacy Settings; Social networking sites; Textual features; Visual representations; Social networking (online)
MultiWiki: Interlingual text passage alignment in wikipedia,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017197222&doi=10.1145%2f3004296&partnerID=40&md5=2159e06c416c0b41a392585e97b66418,"In this article, we address the problem of text passage alignment across interlingual article pairs in Wikipedia. We develop methods that enable the identification and interlinking of text passages written in different languages and containing overlapping information. Interlingual text passage alignment can enable Wikipedia editors and readers to better understand language-specific context of entities, provide valuable insights in cultural differences, and build a basis for qualitative analysis of the articles. An important challenge in this context is the tradeoff between the granularity of the extracted text passages and the precision of the alignment. Whereas short text passages can result in more precise alignment, longer text passages can facilitate a better overview of the differences in an article pair. To better understand these aspects from the user perspective, we conduct a user study at the example of the German, Russian, and English Wikipedia and collect a user-Annotated benchmark. Then we propose MultiWiki, a method that adopts an integrated approach to the text passage alignment using semantic similarity measures and greedy algorithms and achieves precise results with respect to the user-defined alignment. The MultiWiki demonstration is publicly available and currently supports four language pairs. © 2017 ACM 1559-1131/2017/04-ART6 $15.00.",Interlingual text alignment; Wikipedia,Semantics; Cultural difference; Integrated approach; Overlapping information; Precise alignments; Qualitative analysis; Semantic similarity measures; Text alignments; Wikipedia; Alignment
"Spam mobile apps: Characteristics, detection, and in the wild analysis",2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017168765&doi=10.1145%2f3007901&partnerID=40&md5=1623c49eb0c2be3ef24bbe51b365a854,"The increased popularity of smartphones has attracted a large number of developers to offer various applications for the different smartphone platforms via the respective app markets. One consequence of this popularity is that the app markets are also becoming populated with spam apps. These spam apps reduce the users' quality of experience and increase the workload of app market operators to identify these apps and remove them. Spam apps can come in many forms such as apps not having a specific functionality, those having unrelated app descriptions or unrelated keywords, or similar apps being made available several times and across diverse categories. Market operators maintain antispam policies and apps are removed through continuous monitoring. Through a systematic crawl of a popular app market and by identifying apps that were removed over a period of time, we propose a method to detect spam apps solely using app metadata available at the time of publication. We first propose a methodology to manually label a sample of removed apps, according to a set of checkpoint heuristics that reveal the reasons behind removal. This analysis suggests that approximately 35% ofthe apps being removed are very likely tobe spam apps. We then map the identified heuristics to several quantifiable features and show how distinguishing these features are for spam apps. We build an Adaptive Boost classifier for early identification of spam apps using only the metadata of the apps. Our classifier achieves an accuracy of over 95% with precision varying between 85% and 95% and recall varying between 38% and 98%. We further show that a limited number of features, in the range of 10-30, generated from app metadata is sufficient to achieve a satisfactory level of performance. On a set of 180,627 apps that were present at the app market during our crawl, our classifier predicts 2.7% of the apps as potential spam. Finally, we perform additional manual verification and show that human reviewers agree with 82% of our classifier predictions. © 2017 ACM.",Android; Mobile apps; Spam; Spam apps,Heuristic methods; Metadata; Quality of service; Smartphones; Android; Anti-spam; Continuous monitoring; Market operators; Mobile apps; Quality of experience (QoE); Spam; Commerce
MyAdChoices: Bringing transparency and control to online advertising,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015616780&doi=10.1145%2f2996466&partnerID=40&md5=a872129b6706af4260e99db7e6c7ecd6,"The intrusiveness and the increasing invasiveness of online advertising have, in the last few years, raised serious concerns regarding user privacy andWeb usability. As a reaction to these concerns, we have witnessed the emergence of a myriad of ad-blocking and antitracking tools, whose aim is to return control to users over advertising. The problem with these technologies, however, is that they are extremely limited and radical in their approach: users can only choose either to block or allow all ads. With around 200 million people regularly using these tools, the economic model of the Web-in which users get content free in return for allowing advertisers to show them ads-is at serious peril. In this article, we propose a smart Web technology that aims at bringing transparency to online advertising, so that users can make an informed and equitable decision regarding ad blocking. The proposed technology is implemented as a Web-browser extension and enables users to exert fine-grained control over advertising, thus providing them with certain guarantees in terms of privacy and browsing experience, while preserving the Internet economic model. Experimental results in a real environment demonstrate the suitability and feasibility of our approach, and provide preliminary findings on behavioral targeting from real user browsing profiles. © 2017 ACM.",Ad-blocking; Behavioral targeting; Online advertising; User profiling; Web tracking; Web transparency,Transparency; Web browsers; Web crawler; Ad-blocking; Behavioral targeting; Economic modeling; Fine-grained control; Internet economics; Online advertising; Real environments; User profiling; Marketing
Effort mediates access to information in online social networks,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015658170&doi=10.1145%2f2990506&partnerID=40&md5=4094ea87612615b43d17c566d11df282,"Individuals' access to information in a social network depends on how it is distributed and where in the network individuals position themselves. In addition, individuals vary in how much effort they invest in managing their social connections. Using data from a social media site, we study how the interplay between effort and network position affects social media users' access to diverse and novel information. Previous studies of the role of networks in information access were limited in their ability to measure the diversity of information. We address this problem by learning the topics of interest to social media users from the messages they share online with followers.We use the learned topics to measure the diversity of information users receive from the people they follow online. We confirm that users in structurally diverse network positions, which bridge otherwise disconnected regions of the follower network, tend to be exposed to more diverse and novel information.We also show that users who investmore effort in their activity on the site are not only located in more structurally diverse positions within the network than the less engaged users but also receive more novel and diverse information when in similar network positions. These findings indicate that the relationship between network structure and access to information in networks is more nuanced than previously thought. © 2017 ACM.",Information in networks; Probabilistic topic models; Social media; Social networks,Statistics; In networks; Information access; Information users; Network structures; On-line social networks; Probabilistic topic models; Social connection; Social media; Social networking (online)
User's web page aesthetics opinion: A matter of low-level image descriptors based on MPEG-7,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015696189&doi=10.1145%2f3019595&partnerID=40&md5=b0d2119cf9a47502617c412b90fb0888,"Analyzing a user's first impression of a Web site is essential for interface designers, as it is tightly related to their overall opinion of a site. In fact, this early evaluation affects user navigation behavior. Perceived usability and user interest (e.g., revisiting and recommending the site) are parameters influenced by first opinions. Thus, predicting the latter when creating a Web site is vital to ensure users' acceptance. In this regard, Web aesthetics is one of the most influential factors in this early perception. We propose the use of low-level image parameters for modeling Web aesthetics in an objective manner, which is an innovative research field. Our model, obtained by applying a stepwise multiple regression algorithm, infers a user's first impression by analyzing three different visual characteristics of Web site screenshots-texture, luminance, and color-which are directly derived from MPEG-7 descriptors. The results obtained over three wide Web site datasets (composed by 415, 42, and 6Web sites, respectively) reveal a high correlation between low-level parameters and the users' evaluation, thus allowing a more precise and objective prediction of users' opinion than previous models that are based on other image characteristics with fewer predictors. Therefore, our model is meant to support a rapid assessment of Web sites in early stages of the design process to maximize the likelihood of the users' final approval. © 2017 ACM.",Aesthetics; First impression; Image descriptors; Regression; Search engine evaluation; Web site visual appeal,Behavioral research; Motion Picture Experts Group standards; Search engines; Web crawler; Web Design; Aesthetics; First impressions; Image descriptors; Regression; Search engine evaluations; Visual appeals; Websites
Information sharing by viewers via second screens for in-real-life events,2017,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015647391&doi=10.1145%2f3009970&partnerID=40&md5=49b1953588b745d694602b84d44c582b,"The use of second screen devices with socialmedia facilitates conversational interaction concerning broadcast media events, creating what we refer to as the social soundtrack. In this research, we evaluate the change of the Super Bowl XLIX social soundtrack across three social media platforms on the topical categories of commercials, music, and game at three game phases (Pre, During, and Post). We perform statistical analysis on more than 3M, 800K, and 50K posts from Twitter, Instagram, and Tumblr, respectively. Findings show that the volume of posts in the During phase is fewer compared to Pre and Post phases; however, the hourly mean in the During phase is considerably higher than it is in the other two phases. We identify the predominant phase and category of interaction across all three social media sites. We also determine the significance of change in absolute scale across the Super Bowl categories (commercials, music, game) and in both absolute and relative scales across Super Bowl phases (Pre, During, Post) for the three social network platforms (Twitter, Tumblr, Instagram). Results show that significant phase-category relationships exist for all three social networks. The results identify the During phase as the predominant one for all three categories on all social media sites with respect to the absolute volume of conversations in a continuous scale. From the relative volume perspective, the During phase is highest for the music category for most social networks. For the commercials and game categories, however, the Post phase is higher than the During phase for Twitter and Instagram, respectively. Regarding category identification, the game category is the highest for Twitter and Instagram but not for Tumblr, which has dominant peaks for music and/or commercials in all three phases. It is apparent that different social media platforms offer various phase and category affordances. These results are important in identifying the influence that second screen technology has on information sharing across different social media platforms and indicates that the viewer role is transitioning from passive to more active. © 2017 ACM.",ANOVA; Cross screens; Dual screens; IRL events; Multiple screens; Second screens; Social media; Social soundtrack; Super Bowl,Analysis of variance (ANOVA); Information analysis; Information dissemination; Sound recording; Dual screen; IRL events; Second screens; Social media; Social soundtrack; Super bowl; Social networking (online)
From footprint to evidence: An exploratory study of mining social data for credit scoring,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008234718&doi=10.1145%2f2996465&partnerID=40&md5=154fe4482c67c46ab5e6b1b4efb8aa7c,"With the booming popularity of online social networks like Twitter and Weibo, online user footprints are accumulating rapidly on the social web. Simultaneously, the question of how to leverage the large-scale user-generated social media data for personal credit scoring comes into the sight of both researchers and practitioners. It has also become a topic of great importance and growing interest in the P2P lending industry. However, compared with traditional financial data, heterogeneous social data presents both opportunities and challenges for personal credit scoring. In this article, we seek a deep understanding of how to learn users' credit labels from social data in a comprehensive and efficient way. Particularly, we explore the social-databased credit scoring problem under the micro-blogging setting for its open, simple, and real-time nature. To identify credit-related evidence hidden in social data, we choose to conduct an analytical and empirical study on a large-scale dataset from Weibo, the largest and most popular tweet-style website in China. Summarizing results from existing credit scoring literature, we first propose three social-data-based credit scoring principles as guidelines for in-depth exploration. In addition, we glean six credit-related insights arising from empirical observations of the testbed dataset. Based on the proposed principles and insights, we extract prediction features mainly from three categories of users' social data, including demographics, tweets, and networks. To harness this broad range of features, we put forward a two-tier stacking and boosting enhanced ensemble learning framework. Quantitative investigation of the extracted features shows that online socialmedia data does have good potential in discriminating good credit users from bad. Furthermore, we perform experiments on the real-world Weibo dataset consisting of more than 7.3 million tweets and 200,000 users whose credit labels are known through our third-party partner. Experimental results show that (i) our approach achieves a roughly 0.625 AUC value with all the proposed social features as input, and (ii) our learning algorithm can outperform traditional credit scoring methods by as much as 17% for social-data-based personal credit scoring. © 2016 ACM.",Consumer finance; Features; P2P lending; Personal credit scoring; Social data; User profiling,Finance; Learning algorithms; Consumer finance; Features; P2p lending; Personal credit scoring; Social datum; User profiling; Social networking (online)
"COIP-Continuous, operable, impartial, and privacy-aware identityvalidity estimation for osn profiles",2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008191271&doi=10.1145%2f3014338&partnerID=40&md5=49fcfba3cc51eaf444a1fbcf2ea0e4fd,"Identity validation of Online Social Networks' (OSNs') peers is a critical concern to the insurance of safe and secure online socializing environments. Starting from the vision of empowering users to determine the validity of OSN identities, we suggest a framework to estimate the trustworthiness of online social profiles based only on the information they contain. Our framework is based on learning identity correlations between profile attributes in an OSN community and on collecting ratings from OSN community members to evaluate the trustworthiness of target profiles. Our system guarantees utility, user anonymity, impartiality in rating, and operability within the dynamics and continuous evolution of OSNs. In this article, we detail the system design, and we prove its correctness against these claimed quality properties. Moreover, we test its effectiveness, feasibility, and efficiency through experimentation on real-world datasets from Facebook and Google+, in addition to using the Adults UCI dataset. © 2016 ACM.",Community-sourcing; Identity validation; Online social networks; Privacy preservation,Statistical tests; Community-sourcing; Identity validation; On-line social networks; Privacy preservation; Quality properties; Real-world datasets; Social profiles; User anonymity; Social networking (online)
Manipulation among the arbiters of collective intelligence: How wikipedia administrators mold public opinion,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008958368&doi=10.1145%2f3001937&partnerID=40&md5=df3fafc0b5efbb338a94e0510b9aac28,"Our reliance on networked, collectively built information is a vulnerability when the quality or reliability of this information is poor. Wikipedia, one such collectively built information source, is often our first stop for information on all kinds of topics; its quality has stood up to many tests, and it prides itself on having a ""neutral point of view."" Enforcement of neutrality is in the hands of comparatively few, powerful administrators. In this article, we document that a surprisingly large number of editors change their behavior and begin focusing more on a particular controversial topic once they are promoted to administrator status. The conscious and unconscious biases of these few, but powerful, administrators may be shaping the information on many of the most sensitive topics on Wikipedia; some may even be explicitly infiltrating the ranks of administrators in order to promote their own points of view. In addition, we ask whether administrators who change their behavior in this suspicious manner can be identified in advance. Neither prior history nor vote counts during an administrator's election are useful in doing so, but we find that an alternative measure, which gives more weight to influential voters, can successfully reject these suspicious candidates. This second result has important implications for how we harness collective intelligence: even if wisdom exists in a collective opinion (like a vote), that signal can be lost unless we carefully distinguish the true expert voter from the noisy or manipulative voter. © 2016 ACM.",Collective intelligence; Governance of crowdsourced information systems,Computer networks; Internet; Collective intelligences; Controversial topics; First-stop; Information sources; Neutral points; nocv1; Public opinions; Wikipedia; Social aspects
Web content classification using distributions of subjective quality evaluations,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996486768&doi=10.1145%2f2994132&partnerID=40&md5=6bba24c7e6af13ab4007160e778ab6d7,"Machine learning algorithms and recommender systems trained on human ratings are widely in use today. However, human ratings may be associated with a high level of uncertainty and are subjective, influenced by demographic or psychological factors. We propose a new approach to the design of object classes from human ratings: the use of entire distributions to construct classes. By avoiding aggregation for class definition, our approach loses no information and can deal with highly volatile or conflicting ratings. The approach is based the concept of the Earth Mover's Distance (EMD), a measure of distance for distributions. We evaluate the proposed approach based on four datasets obtained from diverse Web content or movie quality evaluation services or experiments. We show that clusters discovered in these datasets using the EMD measure are characterized by a consistent and simple interpretation. Quality classes defined using entire rating distributions can be fitted to clusters of distributions in the four datasets using two parameters, resulting in a good overall fit. We also consider the impact of the composition of small samples on the distributions that are the basis of our classification approach. We show that using distributions based on small samples of 10 evaluations is still robust to several demographic and psychological variables. This observation suggests that the proposed approach can be used in practice for quality evaluation, even for highly uncertain and subjective ratings. 2016 Copyright is held by the owner/author(s).",Classification design; Earth mover's distance; Rating distribution; Robustness; Sample composition; Web content quality,Artificial intelligence; Learning algorithms; Learning systems; Population statistics; Robustness (control systems); Websites; Classification approach; Earth Mover's distance; Psychological factors; Psychological variables; Quality evaluation; Sample composition; Subjective quality; Web content; Quality control
Scanpath trend analysis on web pages: Clustering eye tracking scanpaths,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996590491&doi=10.1145%2f2970818&partnerID=40&md5=4625f97359c4ed1d9ed13e79ab71451d,"Eye tracking studies have widely been used in improving the design and usability of web pages and in the research of understanding how users navigate them. However, there is limited research in clustering users' eye movement sequences (i.e., scanpaths) on web pages to identify a general direction they follow. Existing research tends to be reductionist, which means that the resulting path is so short that it is not useful. Moreover, there is little work on correlating users' scanpaths with visual elements of web pages and the underlying source code, which means the result cannot be used for further processing. In order to address these limitations, we introduce a new concept in clustering scanpaths called Scanpath Trend Analysis (STA) that not only considers the visual elements visited by all users, but also considers the visual elements visited by the majority in any order. We present an algorithm which automatically does this trend analysis to identify a trending scanpath for multiple web users in terms of visual elements of a web page. In contrast to existing research, the STA algorithm first analyzes the most visited visual elements in given scanpaths, clusters the scanpaths by arranging these visual elements based on their overall positions in the individual scanpaths, and then constructs a trending scanpath in terms of these visual elements. This algorithm was experimentally evaluated by an eye tracking study on six web pages for two different kinds of tasks (12 cases in total). Our experimental results show that the STA algorithm generates a trending scanpath that addresses the reductionist problem of existing work by preventing the loss of commonly visited visual elements for all cases. Based on the statistical tests, the STA algorithm also generates a trending scanpath that is significantly more similar to the inputted scanpaths compared to other existing work in 10 out of 12 cases. In the remaining cases, the STA algorithm still performs significantly better than some other existing work. This algorithm contributes to behavior analysis research on the web that can be used for different purposes: for example, re-engineering web pages guided by the trending scanpath to improve users' experience or guiding designers to improve their design. © 2016 ACM.",Algorithm; Clustering; Eye tracking; Scanpath; Trend analysis,Algorithms; Behavioral research; Data mining; Eye movements; Web Design; Websites; Behavior analysis; Clustering; Clustering users; Eye-tracking; Eye-tracking studies; Scan path; Trend analysis; Users' experiences; Clustering algorithms
Scalable and efficient web search result diversification,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983621552&doi=10.1145%2f2907948&partnerID=40&md5=bcde4764fc3002813343fce9e24840c1,"It has been shown that top-k retrieval quality can be considerably improved by taking not only relevance but also diversity into account. However, currently proposed diversification approaches have not put much attention on practical usability in large-scale settings, such as modern web search systems. In this work, we make two contributions toward this goal. First, we propose a combination of optimizations and heuristics for an implicit diversification algorithm based on the desirable facility placement principle, and present two algorithms that achieve linear complexity without compromising the retrieval effectiveness. Instead of an exhaustive comparison of documents, these algorithms first perform a clustering phase and then exploit its outcome to compose the diverse result set. Second, we describe and analyze two variants for distributed diversification in a computing cluster, for large-scale IR where the document collection is too large to keep in one node. Our contribution in this direction is pioneering, as there exists no earlier work in the literature that investigates the effectiveness and efficiency of diversification on a distributed setup. Extensive evaluations on a standard TREC framework demonstrate a competitive retrieval quality of the proposed optimizations to the baseline algorithm while reducing the processing time by more than 80% and up to 97%, and shed light on the efficiency and effectiveness tradeoffs of diversification when applied on top of a distributed architecture. © 2016 ACM.",Distributed result diversification; Web search engines,Algorithms; Cluster computing; Clustering algorithms; Distributed computer systems; Efficiency; Information retrieval; Optimization; Search engines; Websites; Combination of optimizations; Computing clusters; Distributed architecture; Distributed result diversification; Distributed setups; Document collection; Effectiveness and efficiencies; Retrieval effectiveness; World Wide Web
A large-scale evaluation of U.S. financial institutions' standardized privacy notices,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984806924&doi=10.1145%2f2911988&partnerID=40&md5=f850c588ed040dda7cc7d52aaf9ee1fc,"Financial institutions in the United States are required by the Gramm-Leach-Bliley Act to provide annual privacy notices. In 2009, eight federal agencies jointly released a model privacy form for these disclosures. While the use of this model privacy form is not required, it has been widely adopted. We automatically evaluated 6,191 U.S. financial institutions' privacy notices posted on the World Wide Web. We found large variance in stated practices, even among institutions of the same type. While thousands of financial institutions share personal information without providing the opportunity for consumers to opt out, some institutions' practices are more privacy protective. Regression analyses show that large institutions and those headquartered in the northeastern region share consumers' personal information at higher rates than all other institutions. Furthermore, our analysis helped us uncover institutions that do not let consumers limit data sharing when legally required to do so, as well as institutions making self-contradictory statements. We discuss implications for privacy in the financial industry, issues with the design and use of the model privacy form on the World Wide Web, and future directions for standardized privacy notice. © 2016 ACM.",Bank; Data sharing; Disclosure; Financial industry; Large-scale comparison; Opt-out; Privacy; Standard format; Web; Www,Finance; Regression analysis; Societies and institutions; World Wide Web; Bank; Data Sharing; Disclosure; Financial industry; Large-scale comparison; Opt-out; Standard format; Data privacy
Peace-ful web event extraction and processing as bitemporal mutable events,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983683607&doi=10.1145%2f2911989&partnerID=40&md5=d4211c1000d8bb84cf4d9596f167af54,"The web is the largest bulletin board of the world. Events of all types, from flight arrivals to business meetings, are announced on this board. Tracking and reacting to such event announcements, however, is a tedious manual task, only slightly alleviated by email or similar notifications. Announcements are published with human readers in mind, and updates or delayed announcements are frequent. These characteristics have hampered attempts at automatic tracking. PeaCE provides the first integrated framework for event processing on top of web event ads, consisting of event extraction, complex event processing, and action execution in response to these events. Given a schema of the events to be tracked, the framework populates this schema by extracting events from announcement sources. This extraction is performed by little programs called wrappers that produce the events including updates and retractions. PEACE then queries these events to detect complex events, often combining announcements from multiple sources. To deal with updates and delayed announcements, PEACE's schemas are bitemporal, to distinguish between occurrence and detection time. This allows complex event specifications to track updates and to react upon differences in occurrence and detection time. In case of new, changing, or deleted events, PEACE allows one to execute actions, such as tweeting or sending out email notifications. Actions are typically specified as web interactions, for example, to fill and submit a form with attributes of the triggering event. Our evaluation shows that PEACE's processing is dominated by the time needed for accessing the web to extract events and perform actions, allotting to 97.4%. Thus, PEACE requires only 2.6% overhead, and therefore, the complex event processor scales well even with moderate resources. We further show that simple and reasonable restrictions on complex event specifications and the timing of constituent events suffice to guarantee that PEACE only requires a constant buffer to process arbitrarily many event announcements. © 2016 ACM.",Active database systems; Complex event processing; Temporal databases; Web engineering; Web events,Data mining; Electronic mail; Information analysis; Specifications; Active database system; Complex event processing; Temporal Database; Web engineering; Web events; Extraction
Prediction and predictability for search query acceleration,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983605217&doi=10.1145%2f2943784&partnerID=40&md5=eef28abe02d58063d6a7188e64e30cf0,"A commercial web search engine shards its index among many servers, and therefore the response time of a search query is dominated by the slowest server that processes the query. Prior approaches target improving responsiveness by reducing the tail latency, or high-percentile response time, of an individual search server. They predict query execution time, and if a query is predicted to be long-running, it runs in parallel; otherwise, it runs sequentially. These approaches are, however, not accurate enough for reducing a high tail latency when responses are aggregated from many servers because this requires each server to reduce a substantially higher tail latency (e.g., the 99.99th percentile), which we call extreme tail latency. To address tighter requirements of extreme tail latency, we propose a new design space for the problem, subsuming existing work and also proposing a new solution space. Existing work makes a prediction using features available at indexing time and focuses on optimizing prediction features for accelerating tail queries. In contrast, we identify ""when to predict?"" as another key optimization question. This opens up a new solution of delaying a prediction by a short duration to allow many short-running queries to complete without parallelization and, at the same time, to allow the predictor to collect a set of dynamic features using runtime information. This new question expands a solution space in two meaningful ways. First, we see a significant reduction of tail latency by leveraging ""dynamic"" features collected at runtime that estimate query execution time with higher accuracy. Second, we can ask whether to override prediction when the ""predictability"" is low. We show that considering predictability accelerates the query by achieving a higher recall. With this prediction, we propose to accelerate the queries that are predicted to be long-running. In our preliminary work, we focused on parallelization as an acceleration scenario. We extend to consider heterogeneous multicore hardware for acceleration. This hardware combines processor cores with different microarchitectures such as energy-efficient little cores and high-performance big cores, and accelerating web search using this hardware has remained an open problem. We evaluate the proposed prediction framework in two scenarios: (1) query parallelization on a multicore processor and (2) query scheduling on a heterogeneous processor. Our extensive evaluation results show that, for both scenarios of query acceleration using parallelization and heterogeneous cores, the proposed framework is effective in reducing the extreme tail latency compared to a start-of-the-art predictor because of its higher recall, and it improves server throughput by more than 70% because of its improved precision. © 2016 ACM.",Parallelization; Prediction; Web search,Computer hardware; Energy efficiency; Forecasting; Hardware; Information retrieval; Query languages; Reconfigurable hardware; Search engines; Websites; Heterogeneous multicore; Heterogeneous processors; Micro architectures; Multi-core processor; Parallelizations; Query execution time; Run-time information; Web searches; World Wide Web
A comprehensive survey and classification of approaches for community question answering,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977527429&doi=10.1145%2f2934687&partnerID=40&md5=c408f49933434efc2114f89e8b4505d2,"Community question-answering (CQA) systems, such as Yahoo! Answers or Stack Overflow, belong to a prominent group of successful and popular Web 2.0 applications, which are used every day by millions of users to find an answer on complex, subjective, or context-dependent questions. In order to obtain answers effectively, CQA systems should optimally harness collective intelligence of the whole online community, which will be impossible without appropriate collaboration support provided by information technologies. Therefore, CQA became an interesting and promising subject of research in computer science and now we can gather the results of 10 years of research. Nevertheless, in spite of the increasing number of publications emerging each year, so far the research on CQA systems has missed a comprehensive state-of-the-art survey. We attempt to fill this gap by a review of 265 articles published between 2005 and 2014, which were selected from major conferences and journals. According to this evaluation, at first we propose a framework that defines descriptive attributes of CQA approaches. Second, we introduce a classification of all approaches with respect to problems they are aimed to solve. The classification is consequently employed in a review of a significant number of representative approaches, which are described by means of attributes from the descriptive framework. As a part of the survey, we also depict the current trends as well as highlight the areas that require further attention from the research community. © 2016 ACM.",Adaptive collaboration support; Community question answering; Content modeling; Exploratory studies; Knowledge sharing; Online communities; User modeling,Social networking (online); Surveys; Websites; Adaptive collaboration; Community question answering; Content model; Exploratory studies; Knowledge-sharing; On-line communities; User Modeling; Online systems
A novel evidence-based Bayesian similarity measure for recommender systems,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974560573&doi=10.1145%2f2856037&partnerID=40&md5=9bfaa17ff46a237c131ef21ba2f97754,"User-based collaborative filtering, a widely used nearest neighbour-based recommendation technique, predicts an item's rating by aggregating its ratings from similar users. User similarity is traditionally calculated by cosine similarity or the Pearson correlation coefficient. However, both of these measures consider only the direction of rating vectors, and suffer from a range of drawbacks. To overcome these issues, we propose a novel Bayesian similarity measure based on the Dirichlet distribution, taking into consideration both the direction and length of rating vectors. We posit that not all the rating pairs should be equally counted in order to accurately model user correlation. Three different evidence factors are designed to compute the weights of rating pairs. Further, our principled method reduces correlation due to chance and potential system bias. Experimental results on six real-world datasets show that our method achieves superior accuracy in comparison with counterparts. © 2016 ACM.",Bayesian similarity; Dirichlet distribution; Recommender systems; Similarity measure,Collaborative filtering; Correlation methods; Recommender systems; Bayesian; Cosine similarity; Dirichlet distributions; Pearson correlation coefficients; Potential systems; Real-world datasets; Recommendation techniques; Similarity measure; Rating
Probabilistic QoS aggregations for service composition,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973352734&doi=10.1145%2f2876513&partnerID=40&md5=e56c7959d0b191f3f971024f9d9cf48f,"In this article, we propose a comprehensive approach for Quality of Service (QoS) calculation in service composition. Differing from the existing work on QoS aggregations that represent QoS as single values, discrete values with frequencies, or standard statistical distributions, the proposed approach has the capability to handle any type of QoS probability distribution. A set of formulae and algorithms are developed to calculate the QoS of a composite service according to four identified basic patterns as sequential, parallel, conditional, and loop. We demonstrate that the proposed QoS calculation method is much more efficient than existing simulation methods. It has a high scalability and builds a solid foundation for real-time QoS analysis and prediction in service composition. Experiment results are provided to show the effectiveness and efficiency of the proposed method. © 2016 ACM.",Probabilistic QoS; QoS aggregation; Service composition,Probability distributions; Composite services; Discrete values; Effectiveness and efficiencies; High scalabilities; QoS aggregation; Service compositions; Single-value; Statistical distribution; Quality of service
Q2P: Discovering query templates via autocompletion,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973375271&doi=10.1145%2f2873061&partnerID=40&md5=cbdb3583e6385d59e41c9c6c1b9e2e5a,"We present Q2P, a system that discovers query templates from search engines via their query autocompletion services. Q2P is distinct from the existing works in that it does not rely on query logs of search engines that are typically not readily available. Q2P is also unique in that it uses a trie to economically store queries sampled from a search engine and employs a beam-search strategy that focuses the expansion of the trie on its most promising nodes. Furthermore, Q2P leverages the trie-based storage of query sample to discover query templates using only two passes over the trie. Q2P is a key part of our ongoing project Deep2Q on a template-driven data integration on the Deep Web, where the templates learned by Q2P are used to guide the integration process in Deep2Q. Experimental results on four major search engines indicate that (1) Q2P sends only a moderate number of queries (ranging from 597 to 1,135) to the engines, while obtaining a significant number of completions per query (ranging from 4.2 to 8.5 on the average); (2) a significant number of templates (ranging from 8 to 32 when the minimum support for frequent templates is set to 1%) may be discovered from the samples. © 2016 ACM.",Autocompletion; Pattern discovery; Query templates; Search engines; Trie,Data integration; Digital storage; Query processing; Search engines; Autocompletion; Beam search; Integration process; Minimum support; Pattern discovery; Query templates; Template-driven; Trie; Information retrieval
Activity dynamics in collaboration networks,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973369389&doi=10.1145%2f2873060&partnerID=40&md5=427c30228bf364186aed98d9e926dc0d,"Many online collaboration networks struggle to gain user activity and become self-sustaining due to the ramp-up problem or dwindling activity within the system. Prominent examples include online encyclopedias such as (Semantic) MediaWikis, Question and Answering portals such as StackOverflow, and many others. Only a small fraction of these systems manage to reach self-sustaining activity, a level of activity that prevents the system from reverting to a nonactive state. In this article, we model and analyze activity dynamics in synthetic and empirical collaboration networks. Our approach is based on two opposing and well-studied principles: (i) without incentives, users tend to lose interest to contribute and thus, systems become inactive, and (ii) people are susceptible to actions taken by their peers (social or peer influence). With the activity dynamics model that we introduce in this article we can represent typical situations of such collaboration networks. For example, activity in a collaborative network, without external impulses or investments, will vanish over time, eventually rendering the system inactive. However, by appropriately manipulating the activity dynamics and/or the underlying collaboration networks, we can jump-start a previously inactive system and advance it toward an active state. To be able to do so, we first describe our model and its underlying mechanisms. We then provide illustrative examples of empirical datasets and characterize the barrier that has to be breached by a system before it can become self-sustaining in terms of critical mass and activity dynamics. Additionally, we expand on this empirical illustration and introduce a new metric p - the Activity Momentum - to assess the activity robustness of collaboration networks. © 2016 ACM.",Activity dynamics; Activity momentum; Collaboration networks; Critical mass; Dynamical systems; Network science,Dynamical systems; Semantics; Activity dynamics; Collaboration network; Collaborative network; Critical mass; Network science; On-line collaborations; Online encyclopedia; User activity; Dynamics
What users actually do in a social tagging system: A study of user behavior in BibSonomy,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973397943&doi=10.1145%2f2896821&partnerID=40&md5=f9794e40bf41dc7d8a321dfbf32b16dc,"Social tagging systems have established themselves as an important part in today's Web and have attracted the interest of our research community in a variety of investigations. Henceforth, several aspects of social tagging systems have been discussed and assumptions have emerged on which our community builds their work. Yet, testing such assumptions has been difficult due to the absence of suitable usage data in the past. In this work, we thoroughly investigate and evaluate four aspects about tagging systems, covering social interaction, retrieval of posted resources, the importance of the three different types of entities, users, resources, and tags, as well as connections between these entities' popularity in posted and in requested content. For that purpose, we examine live server log data gathered from the real-world, public social tagging system BibSonomy. Our empirical results paint a mixed picture about the four aspects. Although typical assumptions hold to a certain extent for some, other aspects need to be reflected in a very critical light. Our observations have implications for the understanding of social tagging systems and the way they are used on the Web. We make the dataset used in this work available to other researchers. © 2016 ACM.",Assumptions testing; Behavior; Book-marking; Folksonomy; Social sharing; Social tagging,Behavioral research; Social networking (online); Thesauri; Behavior; Book-marking; Folksonomies; Social sharing; Social tagging; User interfaces
A buyer-friendly and mediated watermarking protocol for Web context,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973358581&doi=10.1145%2f2856036&partnerID=40&md5=5bf689d6973c60ed7b05018203abc6df,"Watermarking protocols are used in conjunction with digital watermarking techniques to protect digital copyright on the Internet. They define the schemes of the web transactions by which buyers can purchase protected digital content distributed by content providers in a secure manner. Over the last few years, significant examples of watermarking protocols have been proposed in literature. However, a detailed examination of such protocols has revealed a number of problems that have to be addressed in order to make them suited for current web context. Therefore, based on the most relevant problems derived from literature, this article identifies the main challenges posed by the development of watermarking protocols for web context and presents a watermarking protocol that follows a new secure, buyer-centric and mediated design approach able to meet such challenges. © 2016 ACM.",Digital copyright protection; Watermarking protocols,Copyrights; Digital watermarking; Sales; Content providers; Design approaches; Digital contents; Digital copyright protection; Digital copyrights; Digital watermarking technique; Watermarking protocol; Web transactions; Internet protocols
Search and breast cancer: On episodic shifts of attention over life histories of an Illness,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973352410&doi=10.1145%2f2893481&partnerID=40&md5=2ec18c9fbd7038a73593520bda591833,"We seek to understand the evolving needs of people who are faced with a life-changing medical diagnosis based on analyses of queries extracted from an anonymized search query log. Focusing on breast cancer, we manually tag a set of Web searchers as showing patterns of search behavior consistent with someone grappling with the screening, diagnosis, and treatment of breast cancer. We build and apply probabilistic classifiers to detect these searchers from multiple sessions and to identify the timing of diagnosis using temporal and statistical features. We explore the changes in information seeking over time before and after an inferred diagnosis of breast cancer by aligning multiple searchers by the estimated time of diagnosis. We employ the classifier to automatically identify 1,700 candidate searchers with an estimated 90% precision, and we predict the day of diagnosis within 15 days with an 88% accuracy. We show that the geographic and demographic attributes of searchers identified with high probability are strongly correlated with ground truth of reported incidence rates. We then analyze the content of queries over time for inferred cancer patients, using a detailed ontology of cancer-related search terms. The analysis reveals the rich temporal structure of the evolving queries of people likely diagnosed with breast cancer. Finally, we focus on subtypes of illness based on inferred stages of cancer and show clinically relevant dynamics of information seeking based on the dominant stage expressed by searchers. © 2016 ACM.",Behavior analysis; Cancer; Medical search,Diagnosis; Information retrieval; Information use; Behavior analysis; Cancer; Information seeking; Medical search; Probabilistic classifiers; Related search terms; Statistical features; Temporal structures; Diseases
Discovering best teams for data leak-aware crowdsourcing in social networks,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968834788&doi=10.1145%2f2814573&partnerID=40&md5=39b18d660edbabe840860f595fdbe6fb,"Crowdsourcing is emerging as a powerful paradigm to help perform a wide range of tedious tasks in various enterprise applications. As such applications become more complex, crowdsourcing systems often require the collaboration of several experts connected through professional/social networks and organized in various teams. For instance, a well-known car manufacturer asked fans to contribute ideas for the kinds of technologies that should be incorporated into one of its cars. For that purpose, fans needed to collaborate and form teams competing with each others to come up with the best ideas. However, once teams are formed, each one would like to provide the best solution and treat that solution as a ""trade secret,"" hence preventing any data leak to its competitors (i.e., the other teams). In this article, we propose a data leak-aware crowdsourcing system called SocialCrowd. We introduce a clustering algorithm that uses social relationships between crowd workers to discover all possible teams while avoiding interteam data leakage. We also define a ranking mechanism to select the ""best"" team configurations. Our mechanism is based on the semiring approach defined in the area of soft constraints programming. Finally, we present experiments to assess the efficiency of the proposed approach. © 2016 ACM.",Clustering; Crowdsourcing; Data leakage; Social networks,Automobile manufacture; Clustering algorithms; Complex networks; Crowdsourcing; Car manufacturers; Clustering; Data leakage; Enterprise applications; Ranking mechanisms; Social relationships; Soft constraint; Team configuration; Social networking (online)
Flexible construction of executable service compositions from reusable semantic knowledge,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964466884&doi=10.1145%2f2842628&partnerID=40&md5=30cea5d4bd6197a0768eec2bdd66f2b2,"Most service composition approaches rely on top-down decomposition of a problem and AI-style planning to assemble service components into a meaningful whole, impeding reuse and flexibility. In this article, we propose an approach that starts from declarative knowledge about the semantics of individual service components and algorithmically constructs a full-blown service orchestration process that supports sequence, choice, and parallelism. The output of our algorithm can be mapped directly into a number of service orchestration languages such as OWL-S and BPEL. The approach consists of two steps. First, semantic links specifying data dependencies among the services are derived and organized in a flexible network. Second, based on a user request indicating the desired outcomes from the composition, an executable composition is constructed from the network that satisfies the dependencies. The approach is unique in producing complex compositions out of semantic links between services in a flexible way. It also allows reusing knowledge about semantic dependencies in the network to generate new compositions through new requests and modification of services at runtime. The approach has been implemented in a prototype that outperforms related composition prototypes in experiments. © 2016 ACM.",Semantic web services; Service composition,Complex networks; Quality of service; Semantic Web; Complex compositions; Declarative knowledge; Flexible construction; Individual service; Semantic dependency; Semantic knowledge; Service compositions; Service orchestration; Web services
Individual judgments versus consensus: Estimating query-URL relevance,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968867233&doi=10.1145%2f2834122&partnerID=40&md5=dd7477dd1cbf48aeccfb6ee19bae761c,"Query-URL relevance, measuring the relevance of each retrieved URL with respect to a given query, is one of the fundamental criteria to evaluate the performance of commercial search engines. The traditional way to collect reliable and accurate query-URL relevance requires multiple annotators to provide their individual judgments based on their subjective expertise (e.g., understanding of user intents). In this case, the annotators' subjectivity reflected in each annotator individual judgment (AIJ) inevitably affects the quality of the ground truth relevance (GTR). But to the best of our knowledge, the potential impact of AIJs on estimating GTRs has not been studied and exploited quantitatively by existing work. This article first studies how multiple AIJs and GTRs are correlated. Our empirical studies find that the multiple AIJs possibly provide more cues to improve the accuracy of estimating GTRs. Inspired by this finding, we then propose a novel approach to integrating the multiple AIJs with the features characterizing query-URL pairs for estimating GTRs more accurately. Furthermore, we conduct experiments in a commercial search engine-Baidu.com- and report significant gains in terms of the normalized discounted cumulative gains. © 2016 ACM.",Performance evaluation; Relevance feedback; Web search,Feedback; Search engines; Empirical studies; Ground truth; Performance evaluation; Potential impacts; Relevance feedback; Web searches; World Wide Web
A spatial- Temporal qoS prediction approach for time- Aware web service recommendation,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964702324&doi=10.1145%2f2801164&partnerID=40&md5=5f4c9e247a35714fabf05f6926dc4ef0,"Due to the popularity of service-oriented architectures for various distributed systems, an increasing number of Web services have been deployed all over the world. Recently, Web service recommendation became a hot research topic, one that aims to accurately predict the quality of functional satisfactory services for each end user. Generally, the performance of Web service changes over time due to variations of service status and network conditions. Instead of employing the conventional temporal models, we propose a novel spatial- Temporal QoS prediction approach for time- Aware Web service recommendation, where a sparse representation is employed to model QoS variations. Specifically, we make a zero-mean Laplace prior distribution assumption on the residuals of the QoS prediction, which corresponds to a Lasso regression problem. To effectively select the nearest neighbor for the sparse representation of temporal QoS values, the geolocation of web service is employed to reduce searching range while improving prediction accuracy. The extensive experimental results demonstrate that the proposed approach outperforms state-of- Art methods with more than 10% improvement on the accuracy of temporal QoS prediction for time- Aware Web service recommendation. ©2016 ACM 1559-1131/2016/01-ART7 $15.00.",Qos prediction; Service recommendation; Spatial- Temporal qos prediction; Web service,Forecasting; Information services; Quality of service; Service oriented architecture (SOA); Websites; Distributed systems; Hot research topics; Prediction accuracy; Qos predictions; Service recommendations; Sparse representation; State-of-art methods; Web service recommendations; Web services
Detecting spam and promoting campaigns in twitter,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964612254&doi=10.1145%2f2846102&partnerID=40&md5=055361b14b34c132f461457ff067f2e8,"Twitter has become a target platform for both promoters and spammers to disseminate their messages, which are more harmful than traditional spamming methods, such as email spamming. Recently, large amounts of campaigns that contain lots of spam or promotion accounts have emerged in Twitter. The campaigns cooperatively post unwanted information, and thus they can infect more normal users than individual spam or promotion accounts. Organizing or participating in campaigns has become the main technique to spread spam or promotion information in Twitter. Since traditional solutions focus on checking individual accounts or messages, efficient techniques for detecting spam and promotion campaigns in Twitter are urgently needed. In this article, we propose a framework to detect both spam and promotion campaigns. Our framework consists of three steps: the first step links accounts who post URLs for similar purposes; the second step extracts candidate campaigns that may be for spam or promotion purposes; and the third step classifies the candidate campaigns into normal, spam, and promotion groups. The key point of the framework is how to measure the similarity between accounts' purposes of posting URLs. We present two measure methods based on Shannon information theory: the first one uses the URLs posted by the users, and the second one considers both URLs and timestamps. Experimental results demonstrate that the proposed methods can extract the majority of the candidate campaigns correctly, and detect promotion and spam campaigns with high precision and recall. © 2016 ACM.",Campaign detection; Social network; Social spam,Information theory; Social networking (online); Spamming; High-precision; Keypoints; Large amounts; Shannon information theory; Social spam; Spammers; Time stamps; Internet
W-tree: A compact external memory representation for webgraphs,2016,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964669819&doi=10.1145%2f2835181&partnerID=40&md5=d5a4bc545be01e7f36b17d58744bdab7,"World Wide Web applications need to use, constantly update, and maintain large webgraphs for executing several tasks, such as calculating the web impact factor, finding hubs and authorities, performing link analysis by webometrics tools, and ranking webpages by web search engines. Such webgraphs need to use a large amount of main memory, and, frequently, they do not completely fit in, even if compressed. Therefore, applications require the use of external memory. This article presents a new compact representation for webgraphs, called w-tree,which is designed specifically for externalmemory. It supports the execution of basic queries (e.g., full read, random read, and batch random read), set-oriented queries (e.g., superset, subset, equality, overlap, range, inlink, and co-inlink), and some advanced queries, such as edge reciprocal and hub and authority. Furthermore, a new layout tree designed specifically for webgraphs is also proposed, reducing the overall storage cost and allowing the random read query to be performed with an asymptotically faster runtime in the worst case. To validate the advantages of the w-tree, a series of experiments are performed to assess an implementation of the w-tree comparing it to a compact main memory representation. The results obtained show that w-tree is competitive in compression time and rate and in query time, which may execute several orders of magnitude faster for set-oriented queries than its competitors. The results provide empirical evidence that it is feasible to use a compact external memory representation for webgraphs in real applications, contradicting the previous assumptions made by several researchers.",Compression; Data structure; External memory; Representation; Webgraph,Compaction; Data compression; Data structures; Digital storage; Forestry; Search engines; Websites; Compact representation; External memory; Orders of magnitude; Real applications; Representation; Set-oriented queries; Web graphs; Web impact factors; World Wide Web
BNoteHelper: A Note-based Outline Generation Tool for Structured Learning on Video-sharing Platforms,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190588419&doi=10.1145%2f3638775&partnerID=40&md5=7b68a848884bd466e5d1cc6f2569b05c,"Usually generated by ordinary users and often not particularly designed for learning, the videos on video-sharing platforms are mostly not structured enough to support learning purposes, although they are increasingly leveraged for that. Most existing studies attempt to structure the video using video summarization techniques. However, these methods focus on extracting information from within the video and aiming to consume the video itself. In this article, we design and implement BNoteHelper, a note-based video outline prototype that generates outline titles by extracting user-generated notes on Bilibili, using the BART model fine-tuned on a built dataset. As a browser plugin, BNoteHelper provides users with video overview and navigation as well as note-taking template, via two main features: outline table and navigation marker. The model and prototype are evaluated through automatic and human evaluations. The automatic evaluation reveals that, both before and after fine-tuning, the BART model outperforms T5-Pegasus in BLEU and Perplexity metrics. Also, the results from user feedback reveal that the generation outline sourced from notes is preferred by users over that sourced from video captions due to its more concise, clear, and accurate characteristics but also too general with less details and diversities sometimes. Two features of the video outline are also found to have respective advantages, especially in holistic and fine-grained aspects. Based on these results, we propose insights into designing a video summary from the user-generated creation perspective, customizing it based on video types, and strengthening the advantages of its different visual styles on video-sharing platforms. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bilibili; note-based video outline; user-generated notes; Video summarization; video-sharing platform,Automatic evaluation; Bilibili; Generation tools; Note-based video outline; Sharing platforms; User-generated; User-generated note; Video sharing; Video summarization; Video-sharing platform; Video recording
"“HOT” ChatGPT: The Promise of ChatGPT in Detecting and Discriminating Hateful, Offensive, and Toxic Comments on Social Media",2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190603730&doi=10.1145%2f3643829&partnerID=40&md5=b2fc7882223ebb94055743339d0e9f79,"Harmful textual content is pervasive on social media, poisoning online communities and negatively impacting participation. A common approach to this issue is developing detection models that rely on human annotations. However, the tasks required to build such models expose annotators to harmful and offensive content and may require significant time and cost to complete. Generative AI models have the potential to understand and detect harmful textual content. We used ChatGPT to investigate this potential and compared its performance with MTurker annotations for three frequently discussed concepts related to harmful textual content on social media: Hateful, Offensive, and Toxic (HOT). We designed five prompts to interact with ChatGPT and conducted four experiments eliciting HOT classifications. Our results show that ChatGPT can achieve an accuracy of approximately 80% when compared to MTurker annotations. Specifically, the model displays a more consistent classification for non-HOT comments than HOT comments compared to human annotations. Our findings also suggest that ChatGPT classifications align with the provided HOT definitions. However, ChatGPT classifies “hateful” and “offensive” as subsets of “toxic.” Moreover, the choice of prompts used to interact with ChatGPT impacts its performance. Based on these insights, our study provides several meaningful implications for employing ChatGPT to detect HOT content, particularly regarding the reliability and consistency of its performance, its understanding and reasoning of the HOT concept, and the impact of prompts on its performance. Overall, our study provides guidance on the potential of using generative AI models for moderating large volumes of user-generated textual content on social media. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ChatGPT; Generative AI; hate speech; MTurker annotation; offensive language; online toxicity; prompt engineering,ChatGPT; Generative AI; Hate speech; Mturke annotation; Offensive languages; Online toxicity; Performance; Prompt engineering; Social media; Textual content; Social networking (online)
Nudges to Mitigate Confirmation Bias during Web Search on Debated Topics: Support vs. Manipulation,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190282978&doi=10.1145%2f3635034&partnerID=40&md5=edb60bdb55c85e30ebaf6afdb48747cd,"When people use web search engines to find information on debated topics, the search results they encounter can influence opinion formation and practical decision-making with potentially far-reaching consequences for the individual and society. However, current web search engines lack support for information-seeking strategies that enable responsible opinion formation, e.g., by mitigating confirmation bias and motivating engagement with diverse viewpoints. We conducted two preregistered user studies to test the benefits and risks of an intervention aimed at confirmation bias mitigation. In the first study, we tested the effect of warning labels, warning of the risk of confirmation bias, combined with obfuscations, hiding selected search results per default. We observed that obfuscations with warning labels effectively reduce engagement with search results. These initial findings did not allow conclusions about the extent to which the reduced engagement was caused by the warning label (reflective nudging element) versus the obfuscation (automatic nudging element). If obfuscation was the primary cause, this would raise concerns about harming user autonomy. We thus conducted a follow-up study to test the effect of warning labels and obfuscations separately. According to our findings, obfuscations run the risk of manipulating behavior instead of guiding it, while warning labels without obfuscations (purely reflective) do not exhaust processing capacities but encourage users to actively choose to decrease engagement with attitude-confirming search results. Therefore, given the risks and unclear benefits of obfuscations and potentially other automatic nudging elements to guide engagement with information, we call for prioritizing interventions that aim to enhance human cognitive skills and agency instead. © 2024 Copyright held by the owner/author(s).",cognitive bias mitigation; cognitive reflection; debated topics; nudging; Web search,Behavioral research; Decision making; Information retrieval; Websites; Cognitive bias; Cognitive bias mitigation; Cognitive reflection; Confirmation bias; Debated topic; Decisions makings; Nudging; Opinion formation; Warning labels; Web searches; Search engines
DeLink: An Adversarial Framework for Defending against Cross-site User Identity Linkage,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190590653&doi=10.1145%2f3643828&partnerID=40&md5=a523d00cb3f06110b7a52d6444456f19,"Cross-site user identity linkage (UIL) aims to link the identities of the same person across different social media platforms. Social media practitioners and service providers can construct composite user portraits based on cross-site UIL, which helps understand user behavior holistically and conduct accurate recommendations and personalization. However, many social media users expect each profile to stay within the platform where it was created and thus do not want the identities of different platforms to be linked. For this problem, we first investigate the approaches people would like to use to defend against cross-site UIL and the corresponding challenges. Based on the findings, we build an adversarial framework, DeLink, based on the thoughts of adversarial text generation to help people improve their social media screen names to defend against cross-site UIL. DeLink can support both Chinese and English languages and has good generalizability to the varying numbers of social media accounts and different cross-site user identity linkage models. Extensive evaluations validate DeLink’s better performance, including a higher success rate, higher efficiency, less impact on human perception, and capability to defend against different cross-site UIL models. © 2024 Copyright held by the owner/author(s).",adversarial text generation; cross-site; Social media; user identity linkage,Social networking (online); User profile; Adversarial text generation; Cross-site; Linkage models; Service provider; Social media; Social media platforms; Text generations; User behaviors; User identity; User identity linkage; Behavioral research
Random Testing and Evolutionary Testing for Fuzzing GraphQL APIs,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183330314&doi=10.1145%2f3609427&partnerID=40&md5=b73a688f1c9c86ccd23f3c3a0d42ea64,"The Graph Query Language (GraphQL) is a powerful language for application programming interface (API) manipulation in web services. It has been recently introduced as an alternative solution for addressing the limitations of RESTful APIs. This article introduces an automated solution for GraphQL API testing. We present a full framework for automated API testing, from the schema extraction to test case generation. In addition, we consider two kinds of testing: white-box and black-box testing. The white-box testing is performed when the source code of the GraphQL API is available. Our approach is based on evolutionary search. Test cases are evolved to intelligently explore the solution space while maximizing code coverage and fault-finding criteria. The black-box testing does not require access to the source code of the GraphQL API. It is therefore of more general applicability, albeit it has worse performance. In this context, we use a random search to generate GraphQL data. The proposed framework is implemented and integrated into the open source EvoMaster tool. With enabled white-box heuristics (i.e., white-box mode), experiments on 7 open source GraphQL APIs and three search algorithms show statistically significant improvement of the evolutionary approach compared to the baseline random search. In addition, experiments on 31 online GraphQL APIs reveal the ability of the black-box mode to detect real faults. © 2024 Copyright held by the owner/author(s).",dom; fuzzing; SBSE; SBST,Application programming interfaces (API); Evolutionary algorithms; Open source software; Query languages; Web services; Applications programming interfaces; Dom; Fuzzing; Graph query language; Interface testings; Random searches; SBSE; SBST; Source codes; White box; Black-box testing
Bridging Performance of X (formerly known as Twitter) Users: A Predictor of Subjective Well-Being During the Pandemic,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183452077&doi=10.1145%2f3635033&partnerID=40&md5=dc55c24e3c625975f97d872e6b3a60ef,"The outbreak of the COVID-19 pandemic triggered the perils of misinformation over social media. By amplifying the spreading speed and popularity of trustworthy information, influential social media users have been helping overcome the negative impacts of such flooding misinformation. In this article, we use the COVID-19 pandemic as a representative global health crisisand examine the impact of the COVID-19 pandemic on these influential users’ subjective well-being (SWB), one of the most important indicators of mental health. We leverage X (formerly known as Twitter) as a representative social media platform and conduct the analysis with our collection of 37,281,824 tweets spanning almost two years. To identify influential X users, we propose a new measurement called user bridging performance (UBM) to evaluate the speed and wideness gain of information transmission due to their sharing. With our tweet collection, we manage to reveal the more significant mental sufferings of influential users during the COVID-19 pandemic. According to this observation, through comprehensive hierarchical multiple regression analysis, we are the first to discover the strong relationship between individual social users’ subjective well-being and their bridging performance. We proceed to extend bridging performance from individuals to user subgroups. The new measurement allows us to conduct a subgroup analysis according to users’ multilingualism and confirm the bridging role of multilingual users in the COVID-19 information propagation. We also find that multilingual users not only suffer from a much lower SWB in the pandemic, but also experienced a more significant SWB drop. © 2024 Copyright held by the owner/author(s).",bridging performance; COVID-19; datasets; Information diffusion; subjective well-being; Twitter,Information dissemination; Regression analysis; Social networking (online); Bridging performance; Dataset; Influential users; Information diffusion; Performance; Social media; Spreading speed; Subjective well-being; Twitter; Well being; COVID-19
Learning Neighbor User Intention on User–Item Interaction Graphs for Better Sequential Recommendation,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190233946&doi=10.1145%2f3580520&partnerID=40&md5=ded9d021a865951b1c37dd82c70117e2,"The task of sequential recommendation aims to predict a user’s preference by analyzing the user’s historical behaviours. Existing methods model item transitions through leveraging sequential patterns. However, they mainly consider the target user’s behaviours and dynamic characteristics, while often ignoring high-order collaborative connections when modelling user preferences. Some recent works try to use graph-based methods to introduce high-order collaborative signals for sequential recommendation. However, these methods are flawed by two problems: the sequential patterns cannot be effectively mined and their way of introducing high-order collaborative signals is not suitable for sequential recommendation. To address these problems, we propose to fully exploit sequence features and model high-order collaborative signals for sequential recommendation. We propose a Neighbor user Intention-based Sequential Recommender (NISRec), which utilizes the intentions of high-order connected neighbor users as high-order collaborative signals in order to improve recommendation performance for the target user. The NISRec contains two main modules: the neighbor user intention embedding module (NIE) and the fusion module. The NIE module describes both the long-term and short-term intentions of neighbor users and aggregates them separately. The fusion module uses these two types of aggregated intentions to model high-order collaborative signals in both the embedding process and user preference modelling phase for recommendations of the target user. Experimental results show that our new approach outperforms the state-of-the-art methods on both sparse and dense datasets. Extensive studies further show the effectiveness of the diverse neighbor intentions introduced by the NISRec. © 2024 Copyright held by the owner/author(s).",,Graphic methods; User profile; Behavior characteristic; Collaborative signal; Dynamics characteristic; Fusion modules; High-order; Higher-order; Interaction graphs; Method model; Sequential patterns; User's intentions; Embeddings
A Dual-channel Semi-supervised Learning Framework on Graphs via Knowledge Transfer and Meta-learning,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190277287&doi=10.1145%2f3577033&partnerID=40&md5=874ebce767a65faca3c7cd7e6ff5f804,"This article studies the problem of semi-supervised learning on graphs, which aims to incorporate ubiquitous unlabeled knowledge (e.g., graph topology, node attributes) with few-available labeled knowledge (e.g., node class) to alleviate the scarcity issue of supervised information on node classification. While promising results are achieved, existing works for this problem usually suffer from the poor balance of generalization and fitting ability due to the heavy reliance on labels or task-agnostic unsupervised information. To address the challenge, we propose a dual-channel framework for semi-supervised learning on Graphs via Knowledge Transfer between independent supervised and unsupervised embedding spaces, namely, GKT. Specifically, we devise a dual-channel framework including a supervised model for learning the label probability of nodes and an unsupervised model for extracting information from massive unlabeled graph data. A knowledge transfer head is proposed to bridge the gap between the generalization and fitting capability of the two models. We use the unsupervised information to reconstruct batch-graphs to smooth the label probability distribution on the graphs to improve the generalization of prediction. We also adaptively adjust the reconstructed graphs by encouraging the label-related connections to solidify the fitting ability. Since the optimization of the supervised channel with knowledge transfer contains that of the unsupervised channel as a constraint and vice versa, we then propose a meta-learning-based method to solve the bi-level optimization problem, which avoids the negative transfer and further improves the model’s performance. Finally, extensive experiments validate the effectiveness of our proposed framework by comparing state-of-the-art algorithms. CCS Concepts: • Theory of computation → Semi-supervised learning; • Computing methodologies → Unsupervised learning; Transfer learning © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",meta learning; Social network,Classification (of information); Computation theory; Graph theory; Knowledge management; Learning systems; Probability distributions; Social networking (online); Supervised learning; Dual channel; Generalisation; Graph topology; Knowledge transfer; Learning frameworks; Metalearning; Node attribute; Semi-supervised learning; Social network; Transfer learning; Graphic methods
Incorporating a Triple Graph Neural Network with Multiple Implicit Feedback for Social Recommendation,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190264493&doi=10.1145%2f3580517&partnerID=40&md5=99a14b3810440521ec22f82b2c518631,"Graph neural networks have been clearly proven to be powerful in recommendation tasks since they can capture high-order user-item interactions and integrate them with rich attributes. However, they are still limited by the cold-start problem and data sparsity. Using social relationships to assist recommendation is an effective practice, but it can only moderately alleviate these problems. In addition, rich attributes are often unavailable, which prevents graph neural networks from being fully effective. Hence, we propose to enrich the model by mining multiple implicit feedback and constructing a triple GCN component. We have noticed that users may be influenced not only by their trusted friends but also by the ratings that already exist. The implicit influence spreads among the item’s previous and potential raters, and makes a difference on future ratings. The implicit influence is analyzed on the mechanism of information propagation, and fused with the user’s binary implicit attitude, since negative influence propagates as well as the positive one. Furthermore, we leverage explicit feedback, social relationships, and multiple implicit feedback in the triple GCN component. Abundant experiments on real-world datasets reveal that our model has improved significantly in the rating prediction task compared with other state-of-the-art methods. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph neural network; rating prediction; social recommendation,Backpropagation; Information dissemination; Social aspects; Cold start problems; Data sparsity; Effective practices; Graph neural networks; High-order; Higher-order; Implicit feedback; Rating prediction; Social recommendation; Social relationships; Graph neural networks
Deep Adaptive Graph Clustering via von Mises-Fisher Distributions,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190700217&doi=10.1145%2f3580521&partnerID=40&md5=87b181a2095252ff3ca18cc93dc80d98,"Graph clustering has been a hot research topic and is widely used in many fields, such as community detection in social networks. Lots of works combining auto-encoder and graph neural networks have been applied to clustering tasks by utilizing node attributes and graph structure. These works usually assumed the inherent parameters (i.e., size and variance) of different clusters in the latent embedding space are homogeneous, and hence the assigned probability is monotonous over the Euclidean distance between node embeddings and centroids. Unfortunately, this assumption usually does not hold since the size and concentration of different clusters can be quite different, which limits the clustering accuracy. In addition, the node embeddings in deep graph clustering methods are usually L2 normalized so that it lies on the surface of a unit hyper-sphere. To solve this problem, we proposed Deep Adaptive Graph Clustering via von Mises-Fisher distributions, namely DAGC. DAGC assumes the node embeddings H can be drawn from a von Mises-Fisher distribution and each cluster k is associated with cluster inherent parameters ρk which includes cluster center μ and cluster cohesion degree κ. Then we adopt an EM-like approach (i.e., P(H|ρ) and P(ρ|H), respectively) to learn the embedding and cluster inherent parameters alternately. Specifically, with the node embeddings, we proposed to update the cluster centers in an attraction-repulsion manner to make the cluster centers more separable. And given the cluster inherent parameters, a likelihood-based loss is proposed to make node embeddings more concentrated around cluster centers. Thus, DAGC can simultaneously improve the intra-cluster compactness and inter-cluster heterogeneity. Finally, extensive experiments conducted on four benchmark datasets have demonstrated that the proposed DAGC consistently outperforms the state-of-the-art methods, especially on imbalanced datasets. © 2024 Copyright held by the owner/author(s).",graph clustering; Graph embedding; vMF,Graph embeddings; Graph structures; Graph theory; Structure (composition); Auto encoders; Cluster centers; Community detection; Graph clustering; Graph embeddings; Graph neural networks; Hot research topics; VMF; Von Mises-Fisher distribution; Graph neural networks
PIDKG: Propagating Interaction Influence on the Dynamic Knowledge Graph for Recommendation,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190282239&doi=10.1145%2f3593314&partnerID=40&md5=b81105296f4e12e68326468fe8b767a4,"Modeling the dynamic interactions between users and items on knowledge graphs is crucial for improving the accuracy of recommendation. Although existing methods have made great progress in modeling the dynamic knowledge graphs for recommendation, they usually only consider the mutual influence between users and items involved in the interactions, and ignore the influence propagation from the interacting nodes (i.e., users and items) on dynamic knowledge graphs. In this article, we propose an influence propagation-enhanced deep co-evolutionary method for recommendation, which can capture not only the direct mutual influence between interacting users and items but also influence propagation from multiple interacting nodes to their high-order neighbors at the same time on the dynamic knowledge graph. Specifically, the proposed model consists of two main components: the direct mutual influence component and the influence propagation component. The former captures direct interaction influence between the interacting users and items to generate the effective representations for them. The latter refines their representations via aggregating the interaction influence propagated from multiple interacting nodes. In this process, a neighbor selection mechanism is designed for selecting more effective propagation influence, which can significantly reduce the computational cost and accelerate the training. Finally, the refined representations of users and items are used to predict which item the user is most likely to interact with. The experimental results on three real-world datasets illustrate that the effectiveness and robustness of PIDKG outperform all state-of-the-art baselines and the efficiency of it is faster than most comparative baselines. © 2024 Copyright held by the owner/author(s).",dynamic knowledge graph embedding; graph neural network; influence propagation; neighbor selection mechanism; Recommendation system,Backpropagation; Data mining; Dynamics; Graph embeddings; Graph neural networks; Graph theory; User profile; Dynamic interaction; Dynamic knowledge graph embedding; Graph embeddings; Graph neural networks; Influence propagation; Interacting nodes; Knowledge graphs; Neighbor selection mechanism; Neighbour selections; Selection mechanism; Knowledge graph
Community-enhanced Link Prediction in Dynamic Networks,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190611561&doi=10.1145%2f3580513&partnerID=40&md5=27716861ac6cf2b364fc38f67ab03e84,"The growing popularity of online social networks is quite evident nowadays and provides an opportunity to allow researchers in finding solutions for various practical applications. Link prediction is the technique of understanding network structure and identifying missing and future links in social networks. One of the well-known classes of methods in link prediction is a similarity-based method, which uses local and global topological information of the network to predict missing links. Some methods also exist based on quasi-local features to achieve a trade-off between local and global information on static networks. These quasi-local similarity-based methods are not best suited for considering community information in dynamic networks, failing to balance accuracy and efficiency. Therefore, a community-enhanced framework is presented in this article to predict missing links on dynamic social networks. First, a link prediction framework is presented to predict missing links using parameterized influence regions of nodes and their contribution in community partitions. Then, a unique feature set is generated using local, global, and quasi-local similarity-based as well as community information-based features. This feature set is further optimized using scoring-based feature selection methods to select only the most relevant features. Finally, four machine learning-based classification models are used for link prediction. The experiments are performed on six well-known dynamic networks and three performance metrics, and the results demonstrate that the proposed method outperforms the state-of-the-art methods. © 2024 Copyright held by the owner/author(s).",community detection; dynamic networks; link prediction; Social network analysis,Economic and social effects; Feature Selection; Forecasting; Class of methods; Community detection; Dynamic network; Features sets; Finding solutions; Link prediction; Local similarity; Network structures; Similarity-Based Methods; Social Network Analysis; Social networking (online)
Contrastive Graph Similarity Networks,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181891512&doi=10.1145%2f3580511&partnerID=40&md5=fe3d564b7964ad67afd5ff77ee2954ac,"Graph similarity learning is a significant and fundamental issue in the theory and analysis of graphs, which has been applied in a variety of fields, including object tracking, recommender systems, similarity search, and so on. Recent methods for graph similarity learning that utilize deep learning typically share two deficiencies: (1) they leverage graph neural networks as backbones for learning graph representations but have not well captured the complex information inside data, and (2) they employ a cross-graph attention mechanism for graph similarity learning, which is computationally expensive. Taking these limitations into consideration, a method for graph similarity learning is devised in this study, namely, Contrastive Graph Similarity Network (CGSim). To enhance graph similarity learning, CGSim makes use of the complementary information of two input graphs and captures pairwise relations in a contrastive learning framework. By developing a dual contrastive learning module with a node-graph matching and a graph-graph matching mechanism, our method significantly reduces the quadratic time complexity for cross-graph interaction modeling to linear time complexity. Jointly learning in an end-to-end framework, the graph representation embedding module and the well-designed contrastive learning module can be beneficial to one another. A comprehensive series of experiments indicate that CGSim outperforms state-of-the-art baselines on six datasets and significantly reduces the computational cost, which demonstrates our CGSim model’s superiority over other baselines. © 2024 Copyright held by the owner/author(s).",contrastive learning; graph neural networks; Graph similarity learning,Complex networks; Deep learning; Learning systems; Pattern matching; Contrastive learning; Graph matchings; Graph neural networks; Graph representation; Graph similarity; Graph similarity learning; Learning modules; Object Tracking; Similarity learning; Similarity network; Graph neural networks
Semantic Interaction Matching Network for Few-Shot Knowledge Graph Completion,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180247294&doi=10.1145%2f3589557&partnerID=40&md5=8ca6107ad438cc63d4184ebf24aad9aa,"The prosperity of knowledge graphs, as well as related downstream applications, has raised the urgent need for knowledge graph completion techniques that fully support knowledge graph reasoning tasks, especially under the circumstance of training data scarcity. Although large efforts have been made on solving this challenge via few-shot learning tools, they mainly focus on simply aggregating entity neighbors to represent few-shot references, whereas the enhancement from latent semantic correlation within neighbors has been largely ignored. To that end, in this article, we propose a novel few-shot learning solution named SIM, a Semantic Interaction Matching network that applies a Transformer framework to enhance the entity representation with capturing semantic interaction between entity neighbors. Specifically, we first design an entity-relation fusion module to adaptively encode neighbors with incorporating relation representation. Along this line, Transformer layers are integrated to capture latent correlation within neighbors, as well as the semantic diversification of the support set. Finally, a similarity score is attentively estimated with the attention mechanism. Extensive experiments on two public benchmark datasets demonstrate that our model outperforms a variety of state-of-the-art methods by a significant margin. © 2024 Copyright held by the owner/author(s).",Few-shot learning; knowledge graph completion; knowledge representation,Semantic Web; Semantics; Downstream applications; Few-shot learning; Knowledge graph completion; Knowledge graphs; Knowledge-representation; Matching networks; Reasoning tasks; Semantic interactions; Support knowledge; Training data; Knowledge graph
BehaviorNet: A Fine-grained Behavior-aware Network for Dynamic Link Prediction,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190585545&doi=10.1145%2f3580514&partnerID=40&md5=2d1b4a4c9221dcf1934b1a0b7aa2aea6,"Dynamic link prediction has become a trending research subject because of its wide applications in the web, sociology, transportation, and bioinformatics. Currently, the prevailing approach for dynamic link prediction is based on graph neural networks, in which graph representation learning is the key to perform dynamic link prediction tasks. However, there are still great challenges because the structure of graphs evolves over time. A common approach is to represent a dynamic graph as a collection of discrete snapshots, in which information over a period is aggregated through summation or averaging. This way results in some fine-grained time-related information loss, which further leads to a certain degree of performance degradation. We conjecture that such fine-grained information is vital because it implies specific behavior patterns of nodes and edges in a snapshot. To verify this conjecture, we propose a novel fine-grained behavior-aware network (BehaviorNet) for dynamic network link prediction. Specifically, BehaviorNet adapts a transformer-based graph convolution network to capture the latent structural representations of nodes by adding edge behaviors as an additional attribute of edges. GRU is applied to learn the temporal features of given snapshots of a dynamic network by utilizing node behaviors as auxiliary information. Extensive experiments are conducted on several real-world dynamic graph datasets, and the results show significant performance gains for BehaviorNet over several state-of-the-art (SOTA) discrete dynamic link prediction baselines. Ablation study validates the effectiveness of modeling fine-grained edge and node behaviors. © 2024 Copyright held by the owner/author(s).",dynamic networks; dynamic representation learning; graph neural networks; Link prediction,Data mining; Forecasting; Dynamic graph; Dynamic links; Dynamic network; Dynamic representation; Dynamic representation learning; Fine grained; Graph neural networks; Graph representation; Link prediction; Research subjects; Graph neural networks
"Introduction to the Special Issue on Advanced Graph Mining on the Web: Theory, Algorithms, and Applications: Part 2",2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190601118&doi=10.1145%2f3631941&partnerID=40&md5=6a4a5d66351fede5f744ac8556f3678d,[No abstract available],,
Heterogeneous Information Crossing on Graphs for Session-Based Recommender Systems,2024,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190604171&doi=10.1145%2f3572407&partnerID=40&md5=9fa5b0c8e654c762d69ba693212a97b7,"Recommender systems are fundamental information filtering techniques to recommend content or items that meet users’ personalities and potential needs. As a crucial solution to address the difficulty of user identification and unavailability of historical information, session-based recommender systems provide recommendation services that only rely on users’ behaviors in the current session. However, most existing studies are not well-designed for modeling heterogeneous user behaviors and capturing the relationships between them in practical scenarios. To fill this gap, in this article, we propose a novel graph-based method, namely Heterogeneous Information Crossing on Graphs (HICG). HICG utilizes multiple types of user behaviors in the sessions to construct heterogeneous graphs, and captures users’ current interests with their long-term preferences by effectively crossing the heterogeneous information on the graphs. In addition, we also propose an enhanced version, named HICG-CL, which incorporates the contrastive learning (CL) technique to enhance item representation ability. By utilizing the item co-occurrence relationships across different sessions, HICG-CL improves the recommendation performance of HICG. We conduct extensive experiments on three real-world recommendation datasets, and the results verify that (i) HICG achieves state-of-the-art performance by utilizing multiple types of behaviors on the heterogeneous graph. (ii) HICG-CL further significantly improves the recommendation performance of HICG by the proposed contrastive learning module. © 2024 Association for Computing Machinery.",graph neural network; heterogeneous information; Session-based recommendation,Behavioral research; Graph neural networks; Graphic methods; Information filtering; User profile; 'current; Filtering technique; Graph neural networks; Heterogeneous graph; Heterogeneous information; Potential needs; Recommendation performance; Session-based recommendation; User behaviors; User personalities; Recommender systems
Adoption of Recurrent Innovations: A Large-Scale Case Study on Mobile App Updates,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183320676&doi=10.1145%2f3626189&partnerID=40&md5=ddf6954117bed0a7050a863d29fa3a8e,"Modern technology innovations feature a successive and even recurrent procedure. Intervals between old and new generations of technology are shrinking, and the Internet and Web services have facilitated the fast adoption of an innovation even before the convergence of its predecessor. While the adoption and diffusion of innovations have been studied for decades, most theories and analyses focus on single and one-time innovations. Meanwhile, limited work has investigated successive innovations while lacking user-level analysis, possibly due to the unavailability of fine-grained adoption behavior data. In this study, we present the first large-scale analysis of the adoption of recurrent innovations in the context of mobile app updates, investigating how millions of users consume various versions of thousands of apps on their mobile devices. Our analysis reveals novel patterns of crowd and individual adoption behaviors, which suggest the need for new categories of adopters to be added on top of the Rogers model of innovation diffusion. We show that standard machine learning models are able to pick up various sources of signals to predict whether users in these different categories will adopt a new version of an app and how soon they will adopt it. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",diffusion of innovations; mobile app; recurrent innovation,Diffusion; Adoption behavior; Case-studies; Diffusion of innovations; Generations of technology; Internet-services; Large-scales; Mobile app; Modern technologies; Recurrent innovation; Technology innovation; Web services
CLHHN: Category-aware Lossless Heterogeneous Hypergraph Neural Network for Session-based Recommendation,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183318884&doi=10.1145%2f3626569&partnerID=40&md5=818a92d3749a96775ac8bff970e6db5a,"In recent years, session-based recommendation (SBR), which seeks to predict the target user’s next click based on anonymous interaction sequences, has drawn increasing interest for its practicality. The key to completing the SBR task is modeling user intent accurately. Due to the popularity of graph neural networks (GNNs), most state-of-the-art (SOTA) SBR approaches attempt to model user intent from the transitions among items in a session with GNNs. Despite their accomplishments, there are still two limitations. First, most existing SBR approaches utilize limited information from short user–item interaction sequences and suffer from the data sparsity problem of session data. Second, most GNN-based SBR approaches describe pairwise relations between items while neglecting complex and high-order data relations. Although some recent studies based on hypergraph neural networks have been proposed to model complex and high-order relations, they usually output unsatisfactory results due to insufficient relation modeling and information loss. To this end, we propose a category-aware lossless heterogeneous hypergraph neural network (CLHHN) in this article to recommend possible items to the target users by leveraging the category of items. More specifically, we convert each category-aware session sequence with repeated user clicks into a lossless heterogeneous hypergraph consisting of item and category nodes as well as three types of hyperedges, each of which can capture specific relations to reflect various user intents. Then, we design an attention-based lossless hypergraph convolutional network to generate sessionwise and multi-granularity intent-aware item representations. Experiments on three real-world datasets indicate that CLHHN can outperform the SOTA models in making a better tradeoff between prediction performance and training efficiency. An ablation study also demonstrates the necessity of CLHHN’s key components. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Complex networks; Data mining; User profile; Data sparsity problems; Graph neural networks; High-order; Higher-order; Hyper graph; Limited information; Lossless; Network-based; Neural-networks; State of the art; Graph neural networks
A Graph-Based Context-Aware Model to Understand Online Conversations,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183452774&doi=10.1145%2f3624579&partnerID=40&md5=ae738ca48ccb32ae5d1a8c301d91cbe2,"Online forums that allow for participatory engagement between users have been transformative for the public discussion of many important issues. However, such conversations can sometimes escalate into full-blown exchanges of hate and misinformation. Existing approaches in natural language processing (NLP), such as deep learning models for classification tasks, use as inputs only a single comment or a pair of comments depending upon whether the task concerns the inference of properties of the individual comments or the replies between pairs of comments, respectively. However, in online conversations, comments and replies may be based on external context beyond the immediately relevant information that is input to the model. Therefore, being aware of the conversations’ surrounding contexts should improve the model’s performance for the inference task at hand. We propose GraphNLI,1 a novel graph-based deep learning architecture that uses graph walks to incorporate the wider context of a conversation in a principled manner. Specifically, a graph walk starts from a given comment and samples “nearby” comments in the same or parallel conversation threads, which results in additional embeddings that are aggregated together with the initial comment’s embedding. We then use these enriched embeddings for downstream NLP prediction tasks that are important for online conversations. We evaluate GraphNLI on two such tasks - polarity prediction and misogynistic hate speech detection - and find that our model consistently outperforms all relevant baselines for both tasks. Specifically, GraphNLI with a biased root-seeking random walk performs with a macro-F1 score of 3 and 6 percentage points better than the best-performing BERT-based baselines for the polarity prediction and hate speech detection tasks, respectively. We also perform extensive ablative experiments and hyperparameter searches to understand the efficacy of GraphNLI. This demonstrates the potential of context-aware models to capture the global context along with the local context of online conversations for these two tasks. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph walks; hate speech detection; Kialo; Online conversations; polarity prediction; Reddit,Deep learning; Embeddings; Graphic methods; Natural language processing systems; Social networking (online); Speech recognition; Context-aware models; Embeddings; Graph walk; Graph-based; Hate speech detection; Kialo; Online conversation; Polarity prediction; Reddit; Speech detection; Forecasting
Multiresolution Local Spectral Attributed Community Search,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183324726&doi=10.1145%2f3624580&partnerID=40&md5=e04b1b78fe069cdea21578298e345d45,"Community search has become especially important in graph analysis task, which aims to identify latent members of a particular community from a few given nodes. Most of the existing efforts in community search focus on exploring the community structure with a single scale in which the given nodes are located. Despite promising results, the following two insights are often neglected. First, node attributes provide rich and highly related auxiliary information apart from network interactions for characterizing the node properties. Attributes may indicate the community assignment of a node with very few links, which would be difficult to determine from the network structure alone. Second, the multiresolution community affords latent information to depict the hierarchical relation of the network and ensure that one of them is closest to the real one. It is essential for users to understand the underlying structure of the network and explore the community with strong structure and attribute cohesiveness at disparate scales. These aspects motivate us to develop a new community search framework called Multiresolution Local Spectral Attributed Community Search (MLSACS). Specifically, inspired by the local modularity, graph wavelets, and scaling functions, we propose a new Multiresolution Local modularity (MLQ) based on a reconstructed node attribute graph. Furthermore, to detect local communities with cohesive structures and attributes at different scales, a sparse indicator vector is developed based on MLQ by solving a linear programming problem. Extensive experimental results on both synthetic and real-world attributed graphs have demonstrated the detected communities are meaningful and the scale can be changed reasonably. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attributed graph; graph wavelets; local spectral; Multiresolution community search; scaling functions,Graph theory; Attributed graphs; Community structures; Graph analysis; Graph wavelets; Local modularity; Local spectral; Multiresolution; Multiresolution community search; Node attribute; Scaling functions; Linear programming
Triangle-oriented Community Detection Considering Node Features and Network Topology,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183316790&doi=10.1145%2f3626190&partnerID=40&md5=040400587c41ebad5db41f9b3b8ea3c7,"The joint use of node features and network topology to detect communities is called community detection in attributed networks. Most of the existing work along this line has been carried out through objective function optimization and has proposed numerous approaches. However, they tend to focus only on lower-order details, i.e., capture node features and network topology from node and edge views, and purely seek a higher degree of optimization to guarantee the quality of the found communities, which exacerbates unbalanced communities and free-rider effect. To further clarify and reveal the intrinsic nature of networks, we conduct triangle-oriented community detection considering node features and network topology. Specifically, we first introduce a triangle-based quality metric to preserve higher-order details of node features and network topology, and then formulate so-called two-level constraints to encode lower-order details of node features and network topology. Finally, we develop a local search framework based on optimizing our objective function consisting of the proposed quality metric and two-level constraints to achieve both non-overlapping and overlapping community detection in attributed networks. Extensive experiments demonstrate the effectiveness and efficiency of our framework and its potential in alleviating unbalanced communities and free-rider effect. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attributed network; Community detection; community structure; graph clustering; optimization; quality metric,Data mining; Feature extraction; Optimization; Population dynamics; Attributed network; Community detection; Community structures; Free-riders; Graph clustering; Low order; Network topology; Objective functions; Optimisations; Quality metrices; Network topology
An Empirical Analysis of Web Storage and Its Applications to Web Tracking,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183450711&doi=10.1145%2f3623382&partnerID=40&md5=6b2347ba84dbdec7b86dde614b57ea97,"In this article, we present a large-scale empirical analysis of the use of web storage in the wild.By using dynamic taint tracking at the level of JavaScript and by performing an automated classification of the detected information flows, we shed light on the key characteristics of web storage uses in the Tranco Top 10k. Our analysis shows that web storage is routinely accessed by third parties, including known web trackers, who are particularly eager to have both read and write access to persistent web storage information. We then deep dive in web tracking as a prominent case study: our analysis shows that web storage is not yet as popular as cookies for tracking purposes; however, taint tracking is useful to detect potential new trackers not included in standard filter lists. Moreover, we observe that many websites do not comply with the General Data Protection Regulation directives when it comes to their use of web storage. © 2023 Copyright held by the owner/author(s).",JavaScript; taint analysis; web storage; web tracking,Classification (of information); High level languages; Automated classification; Dynamic taint tracking; Empirical analysis; Information flows; ITS applications; Javascript; Large-scales; Taint analyse; Web storage; Web tracking; Digital storage
Understanding Rug Pulls: An In-depth Behavioral Analysis of Fraudulent NFT Creators,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183327003&doi=10.1145%2f3623376&partnerID=40&md5=7e61d25507ffb24f6aa70035c31e051b,"The explosive growth of non-fungible tokens (NFTs) on Web3 has created a new frontier for digital art and collectibles and an emerging space for fraudulent activities. This study provides an in-depth analysis of NFT rug pulls, the fraudulent schemes that steal investors’ funds. From a curated dataset of 760 rug pulls across 10 NFT marketplaces, we examine these schemes’ structural and behavioral properties, identify the characteristics and motivations of rug pullers, and classify NFT projects into 20 groups based on creators’ association with their accounts. Our findings reveal that repeated rug pulls account for a significant proportion of the rise in NFT-related cryptocurrency crimes, with one NFT creator attempting 37 rug pulls within 3 months. Additionally, we identify the largest group of creators influencing the majority of rug pulls and demonstrate the connection between rug pullers of different NFT projects using the same wallets to store and move money. Our study contributes to understanding NFT market risks and provides insights for designing preventative strategies to mitigate future losses. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Blockchain; cyber frauds; illicit activities; non-fungible tokens,Arts computing; Classification (of information); Commerce; Investments; Behavioral analysis; Behavioral properties; Block-chain; Cybe fraud; Digital art; Explosive growth; Group-based; Illicit activity; In-depth analysis; Non-fungible token; Blockchain
Human Team Behavior and Predictability in the Massively Multiplayer Online Game WOT Blitz,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183330519&doi=10.1145%2f3617509&partnerID=40&md5=a76188625ecae9e037f376441d7be4bd,"Massively multiplayer online games (MMOGs) played on the Web provide a new form of social, computer-mediated interactions that allow the connection of millions of players worldwide. The rules governing team-based MMOGs are typically complex and nondeterministic giving rise to an intricate dynamical behavior. However, due to the novelty and complexity of MMOGs, their behavior is understudied. In this article, we investigate the MMOG World of Tanks Blitz by using a combined approach based on data science and complex adaptive systems. We analyze data on the population level to get insights into organizational principles of the game and its game mechanics. For this reason, we study the scaling behavior and the predictability of system variables. As a result, we find a power-law behavior on the population level revealing long-range interactions between system variables. Furthermore, we identify and quantify the predictability of summary statistics of the game and its decomposition into explanatory variables. This reveals a heterogeneous progression through the tiers and identifies only a single system variable as key driver for the win rate. © 2023 Copyright held by the owner/author(s).",complex system; computational social science; human behavior; Massively multiplayer online games; prediction; statistical model,Computer games; Distributed computer systems; Human computer interaction; Interactive computer graphics; Population statistics; Social networking (online); Blitz++; Computational social science; Computer-mediated interactions; Human behaviors; Massively multiplayer online games; New forms; Population levels; Statistic modeling; System variables; Team behaviour; Behavioral research
Causality and Correlation Graph Modeling for Effective and Explainable Session-Based Recommendation,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183326322&doi=10.1145%2f3593313&partnerID=40&md5=e3b02357f6b582228d8da130894f83a5,"Session-based recommendation, which has witnessed a booming interest recently, focuses on predicting a user’s next interested item(s) based on an anonymous session. Most existing studies adopt complex deep learning techniques (e.g., graph neural networks) for effective session-based recommendation. However, they merely address co-occurrence between items, but fail to distinguish a causality and correlation relationship. Considering the varied interpretations and characteristics of causality and correlation relationships between items, in this study, we propose a novel method denoted as CGSR by jointly modeling causality and correlation relationships between items. In particular, we construct cause, effect, and correlation graphs from sessions by simultaneously considering the false causality problem. We further design a graph neural network–based method for session-based recommendation. To conclude, we strive to explore the relationship between items from specific “causality” (directed) and “correlation” (undirected) perspectives. Extensive experiments on three datasets show that our model outperforms other state-of-the-art methods in terms of recommendation accuracy. Moreover, we further propose an explainable framework on CGSR and demonstrate the explainability of our model via case studies on an Amazon dataset. © 2023 Association for Computing Machinery. All rights reserved.",graph neural network; product relationship; Session-based recommendation,Data mining; Deep learning; User profile; Cause-effect; Co-occurrence; Correlation graphs; Graph model; Graph neural networks; Learning techniques; Network-based; Novel methods; Product relationship; Session-based recommendation; Graph neural networks
Layout Cross-Browser Failure Classification for Mobile Responsive Design Web Applications: Combining Classification Models Using Feature Selection,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176382784&doi=10.1145%2f3580518&partnerID=40&md5=7512cbcd0c0e5620ff2c54471f699a6b,"Cross-browser incompatibilities (XBIs) are defined as inconsistencies that can be observed in Web applications when they are rendered in a specific browser compared to others. These inconsistencies are associated with differences in the way each browser implements its capabilities and renders Web applications. The inconsistencies range from minor layout differences to lack of core functionalities of Web applications when rendered in specific browsers. The state of the art proposes different approaches for detecting XBIs and many of them are based on classification models, using features extracted from the document object model (DOM) structure (DOM-based approaches) and screenshots (computer vision approaches) of Web applications. To the best of our knowledge, a comparison between DOM-based and computer vision classification models has not yet been reported in the literature, and a combination between both approaches could possibly lead to increased accuracy of classification models. In this article, we extend the use of these classification models for detecting layout XBIs in responsive design Web applications, rendered on different browser viewport widths and devices (iPhone 12 mini, iPhone 12, iPhone 12 Pro Max, and Pixel XL). We investigate the use of state-of-the-art classification models (Browserbite, CrossCheck, and our previous work) for detecting layout cross-browser failures, which consist of layout XBIs that negatively affect the layout of responsive design Web applications. Furthermore, we propose an enhanced classification model that combines features from different state-of-the-art classification models (DOM based and computer vision) using feature selection. We built two datasets for evaluating the efficacy of classification models in separately detecting external and internal layout failures using data from 72 responsive design Web applications. The proposed classification model reported the highest F1-score for detecting external layout failures (0.65) and internal layout failures (0.35), and these results reported significant differences compared to Browserbite and CrossCheck classification models. Nevertheless, the experiment showed a lower accuracy in the classification of internal layout failures and suggests the use of other image similarity metrics or deep learning models for increasing the efficacy of classification models. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cross-browser incompatibilities; incompatibilities automatic detection; responsive Web applications; XBI; XBI classification,Computer vision; Deep learning; Feature extraction; Image enhancement; Information retrieval systems; Rendering (computer graphics); Smartphones; Automatic Detection; Classification models; Cross-browser incompatibility; Incompatibility automatic detection; Responsive designs; Responsive web application; WEB application; Web applications; XBI; XBI classification; Classification (of information)
Summarizing Web Archive Corpora via Social Media Storytelling by Automatically Selecting and Visualizing Exemplars,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174605905&doi=10.1145%2f3606030&partnerID=40&md5=dba2fad7e209f763bdb8ed3f53599a7f,"People often create themed collections to make sense of an ever-increasing number of archived web pages. Some of these collections contain hundreds of thousands of documents. Thousands of collections exist, many covering the same topic. Few collections include standardized metadata. This scale makes understanding a collection an expensive proposition. Our Dark and Stormy Archives (DSA) five-process model implements a novel summarization method to help users understand a collection by combining web archives and social media storytelling. The five processes of the DSA model are: select exemplars, generate story metadata, generate document metadata, visualize the story, and distribute the story. Selecting exemplars produces a set of k documents from the N documents in the collection, where k << N, thus reducing the number of documents visitors need to review to understand a collection. Generating story and document metadata selects images, titles, descriptions, and other content from these exemplars. Visualizing the story ties this metadata together in a format the visitor can consume. Without distributing the story, it is not shared for others to consume. We present a research study demonstrating that our algorithmic primitives can be combined to select relevant exemplars that are otherwise undiscoverable using a conventional search engine and query generation methods. Having demonstrated improved methods for selecting exemplars, we visualize the story. Previous work established that the social card is the best format for visitors to consume surrogates. The social card combines metadata fields, including the document’s title, a brief description, and a striking image. Social cards are commonly found on social media platforms. We discovered that these platforms perform poorly for mementos and rely on web page authors to supply the necessary values for these metadata fields. With web archives, we often encounter archived web pages that predate the existence of this metadata. To generate this missing metadata and ensure that storytelling is available for these documents, we apply machine learning to generate the images needed for social cards with a Precision@1 of 0.8314. We also provide the length values needed for executing automatic summarization algorithms to generate document descriptions. Applying these concepts helps us create the visualizations needed to fulfill the final processes of story generation. We close this work with examples and applications of this technology. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Search engines; Social networking (online); Algorithmics; Document metadatas; Engine generation; Generation method; Process-models; Query generation; Research studies; Social media; Web archives; Web-page; Metadata
SHGCN: Socially Enhanced Heterogeneous Graph Convolutional Network for Multi-behavior Prediction,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183321564&doi=10.1145%2f3617510&partnerID=40&md5=23ccdc3aed24d6338c31a4ef6e0ec364,"In recent years, multi-behavior information has been utilized to address data sparsity and cold-start issues. The general multi-behavior models capture multiple behaviors of users to make the representation of relevant features more fine-grained and informative. However, most current multi-behavior recommendation methods neglect the exploration of social relations between users. Actually, users’ potential social connections are critical to assist them in filtering multifarious messages, which may be one key for models to tap deeper into users’ interests. Additionally, existing models usually focus on the positive behaviors (e.g., click, follow, and purchase) of users and tend to ignore the value of negative behaviors (e.g., unfollow and badpost). In this work, we present a Multi-Behavior Graph (MBG) construction method based on user behaviors and social relationships and then introduce a novel socially enhanced and behavior-aware graph neural network for behavior prediction. Specifically, we propose a Socially Enhanced Heterogeneous Graph Convolutional Network (SHGCN) model, which utilizes behavior heterogeneous graph convolution module and social graph convolution module to effectively incorporate behavior features and social information to achieve precise multi-behavior prediction. In addition, the aggregation pooling mechanism is suggested to integrate the outputs of different graph convolution layers, and a dynamic adaptive loss (DAL) method is presented to explore the weight of each behavior. The experimental results on the datasets of the e-commerce platforms (i.e., Epinions and Ciao) indicate the promising performance of SHGCN. Compared with the most powerful baseline, SHGCN achieves 3.3% and 1.4% uplift in terms of AUC on the Epinions and Ciao datasets. Further experiments, including model efficiency analysis, DAL mechanism, and ablation experiments, confirm the validity of the multi-behavior information and social enhancement. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Behavioral research; Data mining; Forecasting; Graph neural networks; Social aspects; Taps; Address datum; Behavior prediction; Behaviour models; Cold-start; Convolutional networks; Data sparsity; Dynamic-adaptive; Fine grained; Heterogeneous graph; Relevant features; Convolution
Understanding the Contribution of Recommendation Algorithms on Misinformation Recommendation and Misinformation Dissemination on Social Networks,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176764013&doi=10.1145%2f3616088&partnerID=40&md5=81d824dc429cd86b8d4f54c46406852b,"Social networks are a platform for individuals and organizations to connect with each other and inform, advertise, spread ideas, and ultimately influence opinions. These platforms have been known to propel misinformation. We argue that this could be compounded by the recommender algorithms that these platforms use to suggest items potentially of interest to their users, given the known biases and filter bubbles issues affecting recommender systems. While much has been studied about misinformation on social networks, the potential exacerbation that could result from recommender algorithms in this environment is in its infancy. In this manuscript, we present the result of an in-depth analysis conducted on two datasets (Politifact FakeNewsNet dataset and HealthStory FakeHealth dataset) in order to deepen our understanding of the interconnection between recommender algorithms and misinformation spread on Twitter. In particular, we explore the degree to which well-known recommendation algorithms are prone to be impacted by misinformation. Via simulation, we also study misinformation diffusion on social networks, as triggered by suggestions produced by these recommendation algorithms. Outcomes from this work evidence that misinformation does not equally affect all recommendation algorithms. Popularity-based and network-based recommender algorithms contribute the most to misinformation diffusion. Users who are known to be superspreaders are known to directly impact algorithmic performance and misinformation spread in specific scenarios. Findings emerging from our exploration result in a number of implications for researchers and practitioners to consider when designing and deploying recommender algorithms in social networks.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesMisinformation; diffusion; news; recommendation algorithms; social networks; Twitter,Diffusion; Additional key word and phrasesmisinformation; And filters; In-depth analysis; Key words; Network-based; News; Recommendation algorithms; Recommender algorithms; Social network; Twitter; Social networking (online)
"Joint Credibility Estimation of News, User, and Publisher via Role-relational Graph Convolutional Networks",2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173550813&doi=10.1145%2f3617418&partnerID=40&md5=328584d47c86d84f9d36c86017c5a286,"The presence of fake news on online social media is overwhelming and is responsible for having impacted several aspects of people’s lives, from health to politics, the economy, and response to natural disasters. Although significant effort has been made to mitigate fake news spread, current research focuses on single aspects of the problem, such as detecting fake news spreaders and classifying stories as either factual or fake. In this article, we propose a new method to exploit inter-relationships between stories, sources, and final users and integrate prior knowledge of these three entities to jointly estimate the credibility degree of each entity involved in the news ecosystem. Specifically, we develop a new graph convolutional network, namely, Role-Relational Graph Convolutional Networks (Role-RGCN), to learn, for each node type (or role), a unique node representation space and jointly connect the different representation spaces with edge relations. To test our proposed approach, we conducted an experimental evaluation on the state-of-the-art FakeNewsNet-Politifact dataset and a new dataset with ground truth on news credibility degrees we collected. Experimental results show a superior performance of our Role-RGCN proposed method at predicting the credibility degree of stories, sources, and users compared to state-of-the-art approaches and other baselines. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",credibility degrees; graph convolutional networks; Misinformation detection; representation learning,Convolution; Disasters; Fake detection; Graph theory; Social networking (online); 'current; Convolutional networks; Credibility degree; Graph convolutional network; Misinformation detection; Natural disasters; Online social medias; Relational graph; Representation learning; Representation space; Statistical tests
Pre-Training Across Different Cities for Next POI Recommendation,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176758533&doi=10.1145%2f3605554&partnerID=40&md5=23a00c37b841ad1f29bc45106b30867f,"The Point-of-Interest (POI) transition behaviors could hold absolute sparsity and relative sparsity very differently for different cities. Hence, it is intuitive to transfer knowledge across cities to alleviate those data sparsity and imbalance problems for next POI recommendation. Recently, pre-training over a large-scale dataset has achieved great success in many relevant fields, like computer vision and natural language processing. By devising various self-supervised objectives, pre-training models can produce more robust representations for downstream tasks. However, it is not trivial to directly adopt such existing pre-training techniques for next POI recommendation, due to the lacking of common semantic objects (users or items) across different cities. Thus in this paper, we tackle such a new research problem of pre-training across different cities for next POI recommendation. Specifically, to overcome the key challenge that different cities do not share any common object, we propose a novel pre-training model named CATUS, by transferring the category-level universal transition knowledge over different cities. Firstly, we build two self-supervised objectives in CATUS: next category prediction and next POI prediction, to obtain the universal transition-knowledge across different cities and POIs. Then, we design a category-transition oriented sampler on the data level and an implicit and explicit transfer strategy on the encoder level to enhance this transfer process. At the fine-tuning stage, we propose a distance oriented sampler to better align the POI representations into the local context of each city. Extensive experiments on two large datasets consisting of four cities demonstrate the superiority of our proposed CATUS over the state-of-the-art alternatives. The code and datasets are available at https://github.com/NLPWM-WHU/CATUS. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSequential POI recommendation; pre-training; sparsity,Natural language processing systems; Semantics; Additional key word and phrasessequential point-of-interest recommendation; Data imbalance; Data sparsity problems; Imbalance problem; Key words; Pre-training; Relative sparsities; Sparsity; Training model; Transition behavior; Large dataset
Privacy Scoring over OSNs: Shared Data Granularity as a Latent Dimension,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178875742&doi=10.1145%2f3604909&partnerID=40&md5=053f0d079a236eac3aa691ac5ce69022,"Privacy scoring aims at measuring the privacy violation risk of a user over an online social network (OSN) based on attribute values shared in the user's OSN profile page and the user's position in the network. Existing studies on privacy scoring rely on possibly biased or emotional survey data. In this study, we work with real-world data collected from the professional LinkedIn OSN and show that probabilistic scoring models derived from the item response theory fit real-world data better than naive approaches. We also introduce the granularity of the data an OSN user shares on her profile as a latent dimension of the OSN privacy scoring problem. Incorporating data granularity into our model, we build the most comprehensive solution to the OSN privacy scoring problem. Extensive experimental evaluation of various scoring models indicates the effectiveness of the proposed solution.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPrivacy scoring; data granularity; item response theory (IRT); LinkedIn; online social network (OSN),Data privacy; User profile; Additional key word and phrasesprivacy scoring; Data granularity; Item response theory; Key words; LinkedIn; Network privacy; Online social network; Real-world; Scoring models; Social networking (online)
Deep Gated Multi-modal Fusion for Image Privacy Prediction,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176782086&doi=10.1145%2f3608446&partnerID=40&md5=aa626b9cd7a07a40b9e418d9fe2be59d,"With the rapid development of technologies in mobile devices, people can post their daily lives on social networking sites such as Facebook, Flickr, and Instagram. This leads to new privacy concerns due to people's lack of understanding that private information can be leaked and used to their detriment. Image privacy prediction models are developed to predict whether images contain sensitive information (private images) or are safe to be shared online (public images). Despite significant progress on this task, there are still some crucial problems that remain to be solved. Firstly, images' content and tags are found to be useful modalities to automatically predict images' privacy. To date, most image privacy prediction models use single modalities (image-only or tag-only), which limits their performance. Secondly, we observe that current image privacy prediction models are surprisingly vulnerable to even small perturbations in the input data. Attackers can add small perturbations to input data and easily damage a well-trained image privacy prediction model. To address these challenges, in this article, we propose a new decision-level Gated multi-modal fusion (GMMF) approach that fuses object, scene, and image tags modalities to predict privacy for online images. In particular, the proposed approach identifies fusion weights of class probability distributions generated by single-modal classifiers according to their reliability of the privacy prediction for each target image in a sample-by-sample manner and performs a weighted decision-level fusion, so that modalities with high reliability are assigned with higher fusion weights while ones with low reliability are restrained with lower fusion weights. The results of our experiments show that the gated multi-modal fusion network effectively fuses single modalities and outperforms state-of-the-art models for image privacy prediction. Moreover, we perform adversarial training on our proposed GMMF model using multiple types of noise on input data (i.e., images and/or tags). When some modalities are failed by input data with noise attacks, our approach effectively utilizes clean modalities and minimizes negative influences brought by degraded ones using fusion weights, achieving significantly stronger robustness over traditional fusion methods for image privacy prediction. The robustness of our GMMF model against data noise can even be generalized to more severe noise levels. To the best of our knowledge, we are the first to investigate the robustness of image privacy prediction models against noise attacks. Moreover, as the performance of decision-level multi-modal fusion depends highly on the quality of single-modal networks, we investigate self-distillation on single-modal privacy classifiers and observe that transferring knowledge from a trained teacher model to a student model is beneficial in our proposed approach.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep neural network; Image privacy prediction; multi-modal fusion,Deep neural networks; Image fusion; Input output programs; Probability distributions; Reliability; Social networking (online); Social sciences computing; Daily lives; Decision levels; Fusion model; Image privacy prediction; Input datas; Multi-modal fusion; Performance; Prediction modelling; Single-modal; Small perturbations; Forecasting
Scraping Relevant Images from Web Pages without Download,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174856054&doi=10.1145%2f3616849&partnerID=40&md5=4345ad78fea462987895ca0d32c12190,"Automatically scraping relevant images from web pages is an error-prone and time-consuming task, leading experts to prefer manually preparing extraction patterns for a website. Existing web scraping tools are built on these patterns. However, this manual approach is laborious and requires specialized knowledge. Automatic extraction approaches, while a potential solution, require large training datasets and numerous features, including width, height, pixels, and file size, that can be difficult and time-consuming to obtain. To address these challenges, we propose a semi-automatic approach that does not require an expert, utilizes small training datasets, and has a low error rate while saving time and storage. Our approach involves clustering web pages from a website and suggesting several pages for a non-expert to annotate relevant images. The approach then uses these annotations to construct a learning model based on textual data from the HTML elements. In the experiments, we used a dataset of 635,015 images from 200 news websites, each containing 100 pages, with 22,632 relevant images. When comparing several machine learning methods for both automatic approaches and our proposed approach, the AdaBoost method yields the best performance results. When using automatic extraction approaches, the best f-Measure that can be achieved is 0.805 with a learning model constructed from a large training dataset consisting of 120 websites (12,000 web pages). In contrast, our approach achieved an average f-Measure of 0.958 for 200 websites with only six web pages annotated per website. This means that a non-expert only needs to examine 1,200 web pages to determine the relevant images for 200 websites. Our approach also saves time and storage space by not requiring the download of images and can be easily integrated into currently available web scraping tools, because it is based on textual data. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",crawler design; evaluation; relevant images; web data extraction; Web mining,Adaptive boosting; Data mining; Digital storage; Extraction; Large datasets; Learning systems; Web crawler; Automatic approaches; Automatic extraction; Crawler design; Evaluation; Relevant image; Training dataset; Web data extraction; Web Mining; Web scrapings; Web-page; Websites
Dynamic Bayesian Contrastive Predictive Coding Model for Personalized Product Search,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176729877&doi=10.1145%2f3609225&partnerID=40&md5=05abf006190ba02167c6027dbb5fb1ae,"In this article, we study the problem of dynamic personalized product search. Due to the data-sparsity problem in the real world, existing methods suffer from the challenge of data inefficiency. We address the challenge by proposing a Dynamic Bayesian Contrastive Predictive Coding model (DBCPC), which aims to capture the rich structured information behind search records to improve data efficiency. Our proposed DBCPC utilizes contrastive predictive learning to jointly learn dynamic embeddings with structure information of entities (i.e., users, products, and words). Specifically, our DBCPC employs structured prediction to tackle the intractability caused by non-linear output space and utilizes the time embedding technique to avoid designing different encoders each time in the Dynamic Bayesian models. In this way, our model jointly learns the underlying embeddings of entities (i.e., users, products, and words) via prediction tasks, which enables the embeddings to focus more on their general attributes and capture the general information during the preference evolution with time. For inferring the dynamic embeddings, we propose an inference algorithm combining the variational objective and the contrastive objectives. Experiments were conducted on an Amazon dataset and the experimental results show that our proposed DBCPC can learn the higher-quality embeddings and outperforms the state-of-the-art non-dynamic and dynamic models for product search.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesContrastive learning; contrastive predictive coding; dynamic model; product search,Bayesian networks; Data mining; Embeddings; Inference engines; Information retrieval; Additional key word and phrasescontrastive learning; Bayesian; Coding models; Contrastive predictive coding; Dynamics models; Embeddings; Key words; Learn+; Predictive coding; Product search; Dynamic models
A Novel Review Helpfulness Measure Based on the User-Review-Item Paradigm,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170643494&doi=10.1145%2f3585280&partnerID=40&md5=188990cf4806f1736d40e10921e7630f,"Review platforms are viral online services where users share and read opinions about products (e.g., a smartphone) or experiences (e.g., a meal at a restaurant). Other users may be influenced by such opinions when deciding what to buy. The usability of review platforms is currently limited by the massive number of opinions on many products. Therefore, showing only the most helpful reviews for each product is in the best interest of both users and the platform (e.g., Amazon). The current state of the art is far from accurate in predicting how helpful a review is. First, most existing works lack compelling comparisons as many studies are conducted on datasets that are not publicly available. As a consequence, new studies are not always built on top of prior baselines. Second, most existing research focuses only on features derived from the review text, ignoring other fundamental aspects of the review platforms (e.g., the other reviews of a product, the order in which they were submitted).In this article, we first carefully review the most relevant works in the area published during the last 20 years. We then propose the User-Review-Item (URI) paradigm, a novel abstraction for modeling the problem that moves the focus of the feature engineering from the review to the platform level. We empirically validate the URI paradigm on a dataset of products from six Amazon categories with 270 trained models: on average, classifiers gain +4% in F1-score when considering the whole review platform context. In our experiments, we further emphasize some problems with the helpfulness prediction task: (1) the users' writing style changes over time (i.e., concept drift), (2) past models do not generalize well across different review categories, and (3) past methods to generate the ground truth produced unreliable helpfulness scores, affecting the model evaluation phase.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",e-commerce; NLP; online platforms; Online reviews,Classification (of information); User profile; 'current; E- commerces; New study; On-line service; Online platforms; Online reviews; Research focus; Smart phones; State of the art; User reviews; Petroleum reservoir evaluation
"Reverse Maximum Inner Product Search: Formulation, Algorithms, and Analysis",2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168653881&doi=10.1145%2f3587215&partnerID=40&md5=164f651af60a99022f0bd2479534eb1c,"The maximum inner product search (MIPS), which finds the item with the highest inner product with a given query user, is an essential problem in the recommendation field. Usually e-commerce companies face situations where they want to promote and sell new or discounted items. In these situations, we have to consider the following questions: Who is interested in the items, and how do we find them? This article answers this question by addressing a new problem called reverse maximum inner product search (reverse MIPS). Given a query vector and two sets of vectors (user vectors and item vectors), the problem of reverse MIPS finds a set of user vectors whose inner product with the query vector is the maximum among the query and item vectors. Although the importance of this problem is clear, its straightforward implementation incurs a computationally expensive cost.We therefore propose Simpfer, a simple, fast, and exact algorithm for reverse MIPS. In an offline phase, Simpfer builds a simple index that maintains a lower bound of the maximum inner product. By exploiting this index, Simpfer judges whether the query vector can have the maximum inner product or not, for a given user vector, in a constant time. Our index enables filtering user vectors, which cannot have the maximum inner product with the query vector, in a batch. We theoretically demonstrate that Simpfer outperforms baselines employing state-of-the-art MIPS techniques. In addition, we answer two new research questions. Can approximation algorithms further improve reverse MIPS processing? Is there an exact algorithm that is faster than Simpfer? For the former, we show that approximation with quality guarantee provides a little speed-up. For the latter, we propose Simpfer++, a theoretically and practically faster algorithm than Simpfer. Our extensive experiments on real datasets show that Simpfer is at least two orders of magnitude faster than the baselines, and Simpfer++ further improves the online processing time.  © 2023 Copyright held by the owner/author(s).",algorithm; high dimensional data; Reverse maximum inner product search,Approximation algorithms; Clustering algorithms; E- commerces; Essential problems; Exact algorithms; Fast algorithms; High dimensional data; Inner product; Query vectors; Reverse maximum inner product search; SIMPLE algorithm; SIMPLER algorithms; Vectors
To Re-experience the Web: A Framework for the Transformation and Replay of Archived Web Pages,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161797146&doi=10.1145%2f3589206&partnerID=40&md5=faa3e40102d60173c5edb67a529b06cc,"When replaying an archived web page, or memento, the fundamental expectation is that the page should be viewable and function exactly as it did at the archival time. However, this expectation requires web archives upon replay to modify the page and its embedded resources so that all resources and links reference the archive rather than the original server. Although these modifications necessarily change the state of the representation, it is understood that without them the replay of mementos from the archive would not be possible. The process of replaying mementos and the modifications made to the representations by web archives varies between archives. Because of this, there is no standard terminology for describing the replay and needed modifications. In this article, we propose terminology for describing the existing styles of replay and the modifications made on the part of web archives to mementos to facilitate replay. Because of issues discovered with server-side only modifications, we propose a general framework for the auto-generation of client-side rewriting libraries. Finally, we evaluate the effectiveness of using a generated client-side rewriting library to augment the existing replay systems of web archives by crawling mementos replayed from the Internet Archive's Wayback Machine with and without the generated client-side rewriter. By using the generated client-side rewriter, we were able to decrease the cumulative number of requests blocked by the content security policy of the Wayback Machine for 577 mementos by 87.5% and increased the cumulative number of requests made by 32.8%. We were also able to replay mementos that were previously not replayable from the Internet Archive. Many of the client-side rewriting ideas described in this work have been implemented into Wombat, a client-side URL rewriting system that is used by the Webrecorder, Pywb, and Wayback Machine playback systems. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",client-side; Internet Archive; JavaScript; replay; Web archiving,Websites; Auto generations; Client sides; Cumulative number; Internet archive; Javascript; Replay; Server sides; Web archives; Web Archiving; Web-page; Terminology
Into the Unknown: Exploration of Search Engines' Responses to Users with Depression and Anxiety,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176777362&doi=10.1145%2f3580283&partnerID=40&md5=4655041c0467c34a311f5bf2e4b83eea,"Researchers worldwide have explored the behavioral nuances that emerge from interactions of individuals afflicted by mental health disorders (MHD) with persuasive technologies, mainly social media. Yet, there is a gap in the analysis pertaining to a persuasive technology that is part of their everyday lives: web search engines (SE). Each day, users with MHD embark on information seeking journeys using popular SE, like Google or Bing. Every step of the search process for better or worse has the potential to influence a searcher's mindset. In this work, we empirically investigate what subliminal stimulus SE present to these vulnerable individuals during their searches. For this, we use synthetic queries to produce associated query suggestions and search engine results pages. Then we infer the subliminal stimulus present in text from SE, i.e., query suggestions, snippets, and web resources. Findings from our empirical analysis reveal that the subliminal stimulus displayed by SE at different stages of the information seeking process differ between MHD searchers and our control group composed of ""average""SE users. Outcomes from this work showcase open problems related to query suggestions, search engine result pages, and ranking that the information retrieval community needs to address so that SE can better support individuals with MHD.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",emotion; Mental health; sentiment; web search engines,Information retrieval; Information use; Websites; Emotion; Google+; Health disorders; Information seeking; Mental health; Persuasive technology; Query suggestion; Search engine results pages; Sentiment; Social media; Search engines
Closeness Centrality on Uncertain Graphs,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176731165&doi=10.1145%2f3604912&partnerID=40&md5=82bdb08538b6d88a719a9ea82f9c4496,"Centrality is a family of metrics for characterizing the importance of a vertex in a graph. Although a large number of centrality metrics have been proposed, a majority of them ignores uncertainty in graph data. In this article, we formulate closeness centrality on uncertain graphs and define the batch closeness centrality evaluation problem that computes the closeness centrality of a subset of vertices in an uncertain graph. We develop three algorithms, MS-BCC, MG-BCC, and MGMS-BCC, based on sampling to approximate the closeness centrality of the specified vertices. All these algorithms require to perform breadth-first searches (BFS) starting from the specified vertices on a large number of sampled possible worlds of the uncertain graph. To improve the efficiency of the algorithms, we exploit operation-level parallelism of the BFS traversals and simultaneously execute the shared sequences of operations in the breadth-first searches. Parallelization is realized at different levels in these algorithms. The experimental results show that the proposed algorithms can efficiently and accurately approximate the closeness centrality of the given vertices. MGMS-BCC is faster than both MS-BCC and MG-BCC because it avoids more repeated executions of the shared operation sequences in the BFS traversals. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",breadth-first search; parallelism; PhrasesCloseness centrality; uncertain graph,Breadth-first-search; Closeness centralities; Evaluation problems; Graph data; Operation-level parallelism; Parallelism; Phrasescloseness centrality; Possible worlds; Uncertain graphs; Uncertainty
"Introduction to the Special Issue on Advanced Graph Mining on the Web: Theory, Algorithms, and Applications: Part 1",2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168841882&doi=10.1145%2f3579360&partnerID=40&md5=74f451f5e177d5e64c4a0b798e0e3f88,[No abstract available],,
Type Information Utilized Event Detection via Multi-Channel GNNs in Electrical Power Systems,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168803143&doi=10.1145%2f3577031&partnerID=40&md5=910789ea69c5cac326e9ba12a561abc8,"Event detection in power systems aims to identify triggers and event types, which helps relevant personnel respond to emergencies promptly and facilitates the optimization of power supply strategies. However, the limited length of short electrical record texts causes severe information sparsity, and numerous domain-specific terminologies of power systems makes it difficult to transfer knowledge from language models pre-trained on general-domain texts. Traditional event detection approaches primarily focus on the general domain and ignore these two problems in the power system domain. To address the above issues, we propose a Multi-Channel graph neural network utilizing Type information for Event Detection in power systems, named MC-TED, leveraging a semantic channel and a topological channel to enrich information interaction from short texts. Concretely, the semantic channel refines textual representations with semantic similarity, building the semantic information interaction among potential event-related words. The topological channel generates a relation-type-aware graph modeling word dependencies, and a word-type-aware graph integrating part-of-speech tags. To further reduce errors worsened by professional terminologies in type analysis, a type learning mechanism is designed for updating the representations of both the word type and relation type in the topological channel. In this way, the information sparsity and professional term occurrence problems can be alleviated by enabling interaction between topological and semantic information. Furthermore, to address the lack of labeled data in power systems, we built a Chinese event detection dataset based on electrical Power Event texts, named PoE. In experiments, our model achieves compelling results not only on the PoE dataset, but on general-domain event detection datasets including ACE 2005 and MAVEN.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEvent detection; multi-channel; power systems; semantic channel; topological channel,Graph neural networks; Terminology; Topology; Additional key word and phrasesevent detection; Events detection; Information interaction; Key words; Multi channel; Power; Power system; Semantic channel; Topological channel; Type information; Semantics
SGrow: Explaining the Scale-Invariant Strength Assortativity of Streaming Butterflies,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168796586&doi=10.1145%2f3572408&partnerID=40&md5=fb85ae09609e0cd2d639272e0c380148,"Bipartite graphs are rich data structures with prevalent applications and characteristic structural features. However, less is known about their growth patterns, particularly in streaming settings. Current works study the patterns of static or aggregated temporal graphs optimized for certain downstream analytics or ignoring multipartite/non-stationary data distributions, emergence patterns of subgraphs, and streaming paradigms. To address these, we perform statistical network analysis over web log streams and identify the governing patterns underlying the bursty emergence of mesoscopic building blocks, 2, 2-bicliques, leading to a phenomenon that we call scale-invariant strength assortativity of streaming butterflies. We provide the graph-theoretic explanation of this phenomenon. We further introduce a set of micro-mechanics in the body of a streaming growth algorithm, sGrow, to pinpoint the generative origins. sGrow supports streaming paradigms, emergence of four-vertex graphlets, and provides user-specified configurations for the scale, burstiness, level of strength assortativity, probability of out-of-order records, generation time, and time-sensitive connections. Comprehensive evaluations on pattern reproducing and stress testing validate the effectiveness, efficiency, and robustness of sGrow in realization of the observed patterns independent of initial conditions, scale, temporal characteristics, and model configurations. Theoretical and experimental analysis verify sGrow's robustness in generating streaming graphs based on user-specified configurations that affect the scale and burstiness of the stream, level of strength assortativity, probability of out-of-order streaming records, generation time, and time-sensitive connections.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesStrength assortativity; butterflies; streaming growth algorithm,Data mining; Additional key word and phrasesstrength assortativity; Bipartite graphs; Burstiness; Butterfly; Generation time; Growth algorithms; Key words; Out of order; Scale-invariant; Streaming growth algorithm; Graph theory
Niffler: Real-time Device-level Anomalies Detection in Smart Home,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168810061&doi=10.1145%2f3586073&partnerID=40&md5=e480b4fcd3377a9a369702ad2f496694,"Device-level security has become a major concern in smart home systems. Detecting problems in smart home sytems strives to increase accuracy in near real time without hampering the regular tasks of the smart home. The current state of the art in detecting anomalies in smart home devices is mainly focused on the app level, which provides a basic level of security by assuming that the devices are functioning correctly. However, this approach is insufficient for ensuring the overall security of the system, as it overlooks the possibility of anomalies occurring at the lower layers such as the devices. In this article, we propose a novel notion, correlated graph, and with the aid of that, we develop our system to detect misbehaving devices without modifying the existing system. Our correlated graphs explicitly represent the contextual correlations among smart devices with little knowledge about the system. We further propose a linkage path model and a sensitivity ranking method to assist in detecting the abnormalities. We implement a semi-automatic prototype of our approach, evaluate it in real-world settings, and demonstrate its efficiency, which achieves an accuracy of around 90% in near real time.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMisbehaving device detection; smart homes; streaming graphs,Automation; Intelligent buildings; 'current; Additional key word and phrasesmisbehaving device detection; Anomaly detection; Key words; Near-real time; Real- time; Smart homes; Smart-home system; State of the art; Streaming graph; Anomaly detection
RoSGAS: Adaptive Social Bot Detection with Reinforced Self-supervised GNN Architecture Search,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168801516&doi=10.1145%2f3572403&partnerID=40&md5=5d765fc09b4470acd1af0befda180bc0,"Social bots are referred to as the automated accounts on social networks that make attempts to behave like humans. While Graph Neural Networks (GNNs) have been massively applied to the field of social bot detection, a huge amount of domain expertise and prior knowledge is heavily engaged in the state-of-the-art approaches to design a dedicated neural network architecture for a specific classification task. Involving oversized nodes and network layers in the model design, however, usually causes the over-smoothing problem and the lack of embedding discrimination. In this article, we propose RoSGAS, a novel Reinforced and Self-supervised GNN Architecture Search framework to adaptively pinpoint the most suitable multi-hop neighborhood and the number of layers in the GNN architecture. More specifically, we consider the social bot detection problem as a user-centric subgraph embedding and classification task. We exploit the heterogeneous information network to present the user connectivity by leveraging account metadata, relationships, behavioral features, and content features. RoSGAS uses a multi-agent deep reinforcement learning (RL), 31 pages. mechanism for navigating the search of optimal neighborhood and network layers to learn individually the subgraph embedding for each target user. A nearest neighbor mechanism is developed for accelerating the RL training process, and RoSGAS can learn more discriminative subgraph embedding with the aid of self-supervised learning. Experiments on five Twitter datasets show that RoSGAS outperforms the state-of-the-art approaches in terms of accuracy, training efficiency, and stability and has better generalization when handling unseen samples.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGraph neural network; architecture search; reinforcement learning,Deep learning; Graph neural networks; Information services; Multi agent systems; Multilayer neural networks; Network architecture; Network embeddings; Network layers; Additional key word and phrasesgraph neural network; Architecture search; Bot detections; Embeddings; Graph neural networks; Key words; Neural network architecture; Neural-networks; Reinforcement learnings; Social bots; Reinforcement learning
Heterogeneous Graph Transformer for Meta-structure Learning with Application in Text Classification,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168805421&doi=10.1145%2f3580508&partnerID=40&md5=754f3ee1a93c064e160327ae0cfac62d,"The prevalent heterogeneous Graph Neural Network (GNN) models learn node and graph representations using pre-defined meta-paths or only automatically discovering meta-paths. However, the existing methods suffer from information loss due to neglecting undiscovered meta-structures with richer semantics than meta-paths in heterogeneous graphs. To take advantage of the current rich meta-structures in heterogeneous graphs, we propose a novel approach called HeGTM to automatically extract essential meta-structures (i.e., meta-paths and meta-graphs) from heterogeneous graphs. The discovered meta-structures can capture more prosperous relations between different types of nodes that can help the model to learn representations. Furthermore, we apply the proposed approach for text classification. Specifically, we first design a heterogeneous graph for the text corpus, and then apply HeGTM on the constructed text graph to learn better text representations that contain various semantic relations. In addition, our approach can also be used as a strong meta-structure extractor for other GNN models. In other words, the auto-discovered meta-structures can replace the pre-defined meta-paths. The experimental results on text classification demonstrate the effectiveness of our approach to automatically extracting informative meta-structures from heterogeneous graphs and its usefulness in acting as a meta-structure extractor for boosting other GNN models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesHeterogeneous graph; graph neural network; meta-structure; text classification,Classification (of information); Graph structures; Graph theory; Graphic methods; Knowledge graph; Neural network models; Semantics; Text processing; Additional key word and phrasesheterogeneous graph; Graph neural networks; Graph representation; Heterogeneous graph; Key words; Learn+; Metastructures; Neural network model; Structure-learning; Text classification; Graph neural networks
Reinforced MOOCs Concept Recommendation in Heterogeneous Information Networks,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168807201&doi=10.1145%2f3580510&partnerID=40&md5=d07095eeb3edb7b6c21c0b6aa0f26911,"Massive open online courses (MOOCs), which offer open access and widespread interactive participation through the internet, are quickly becoming the preferred method for online and remote learning. Several MOOC platforms offer the service of course recommendation to users, to improve the learning experience of users. Despite the usefulness of this service, we consider that recommending courses to users directly may neglect their varying degrees of expertise. To mitigate this gap, we examine an interesting problem of concept recommendation in this paper, which can be viewed as recommending knowledge to users in a fine-grained way. We put forward a novel approach, termed HinCRec-RL, for Concept Recommendation in MOOCs, which is based on Heterogeneous Information Networks and Reinforcement Learning. In particular, we propose to shape the problem of concept recommendation within a reinforcement learning framework to characterize the dynamic interaction between users and knowledge concepts in MOOCs. Furthermore, we propose to form the interactions among users, courses, videos, and concepts into a heterogeneous information network (HIN) to learn the semantic user representations better. We then employ an attentional graph neural network to represent the users in the HIN, based on meta-paths. Extensive experiments are conducted on a real-world dataset collected from a Chinese MOOC platform, XuetangX, to validate the efficacy of our proposed HinCRec-RL. Experimental results and analysis demonstrate that our proposed HinCRec-RL performs well when compared with several state-of-the-art models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesHIN; concept recommendation; MOOCs; reinforcement learning,E-learning; Graph neural networks; Information services; Learning systems; Semantic Web; Semantics; Additional key word and phraseshin; Concept recommendation; Heterogeneous information; Information networks; Key words; Massive open online course; Online learning; OpenAccess; Reinforcement learnings; Remote learning; Reinforcement learning
Graph Attention Network for Text Classification and Detection of Mental Disorder,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168795582&doi=10.1145%2f3572406&partnerID=40&md5=05c497464333de0ecaacd4bbc8309b78,"A serious issue in today's society is Depression, which can have a devastating impact on a person's ability to cope in daily life. Numerous studies have examined the use of data generated directly from users using social media to diagnose and detect Depression as a mental illness. Therefore, this paper investigates the language used in individuals' personal expressions to identify depressive symptoms via social media. Graph Attention Networks (GATs) are used in this study as a solution to the problems associated with text classification of depression. These GATs can be constructed using masked self-attention layers. Rather than requiring expensive matrix operations such as similarity or knowledge of network architecture, this study implicitly assigns weights to each node in a neighbourhood. This is possible because nodes and words can carry properties and sentiments of their neighbours. Another aspect of the study that contributed to the expansion of the emotion lexicon was the use of hypernyms. As a result, our method performs better when applied to data from the Reddit subreddit Depression. Our experiments show that the emotion lexicon constructed by using the Graph Attention Network ROC achieves 0.91 while remaining simple and interpretable.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesGraph attention networks; Deep Learning (DL); mental healthy; Twitter data,Classification (of information); Deep learning; Diseases; Social networking (online); Text processing; Additional key word and phrasesgraph attention network; Daily lives; Deep learning; Key words; Mental disorders; Mental healthy; Social media; Text classification; Text detection; Twitter data; Network architecture
Constructing Spatio-Temporal Graphs for Face Forgery Detection,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168802496&doi=10.1145%2f3580512&partnerID=40&md5=ad94193ec392c0def2dfdad74cbbc815,"Recently, advanced development of facial manipulation techniques threatens web information security, thus, face forgery detection attracts a lot of attention. It is clear that both spatial and temporal information of facial videos contains the crucial manipulation traces, which are inevitably created during the generation process. However, most existing face forgery detectors only focus on the spatial artifacts or the temporal incoherence, and they are struggling to learn a significant and general kind of representations for manipulated facial videos. In this work, we propose to construct spatial-temporal graphs for fake videos to capture the spatial inconsistency and the temporal incoherence at the same time. To model the spatial-temporal relationship among the graph nodes, a novel forgery detector named Spatio-Temporal Graph Network (STGN) is proposed, which contains two kinds of graph-convolution-based units, the Spatial Relation Graph Unit (SRGU) and the Temporal Attention Graph Unit (TAGU). To exploit spatial information, the SRGU models the inconsistency between each pair of patches in the same frame, instead of focusing on the low-level local spatial artifacts which are vulnerable to samples created by unseen manipulation methods. And, the TAGU is proposed to model the long-distance temporal relation among the patches at the same spatial position in different frames with a graph attention mechanism based on the inter-node similarity. With the SRGU and the TAGU, our STGN can combine the discriminative power of spatial inconsistency and the generalization capacity of temporal incoherence for face forgery detection. Our STGN achieves state-of-the-art performances on several popular forgery detection datasets. Extensive experiments demonstrate both the superiority of our STGN on intra manipulation evaluation and the effectiveness for new sorts of face forgery videos on cross manipulation evaluation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSpatio-temporal graph; forgery detection; spatial inconsistency; temporal incoherence,Face recognition; Fake detection; Security of data; Additional key word and phrasesspatio-temporal graph; Forgery detections; Graph networks; Key words; Spatial inconsistency; Spatial informations; Spatial relation graphs; Spatio-temporal graphs; Temporal graphs; Temporal incoherence; Graph theory
GroupAligner: A Deep Reinforcement Learning with Domain Adaptation for Social Group Alignment,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168808066&doi=10.1145%2f3580509&partnerID=40&md5=8d9e5ad721501896d5c2739ee0d541e5,"Social network alignment, which aims to uncover the correspondence across different social networks, shows fundamental importance in a wide spectrum of applications such as cross-domain recommendation and information propagation. In the literature, the vast majority of the existing studies focus on the social network alignment at user level. In practice, the user-level alignment usually relies on abundant personal information and high-quality supervision, which is expensive and even impossible in the real-world scenario. Alternatively, we propose to study the problem of social group alignment across different social networks, focusing on the interests of social groups rather than personal information. However, social group alignment is non-trivial and faces significant challenges in both (i) feature inconsistency across different social networks and (ii) group discovery within a social network. To bridge this gap, we present a novel GroupAligner, a deep reinforcement learning with domain adaptation for social group alignment. In GroupAligner, to address the first issue, we propose the cycle domain adaptation approach with the Wasserstein distance to transfer the knowledge from the source social network, aligning the feature space of social networks in the distribution level. To address the second issue, we model the group discovery as a sequential decision process with reinforcement learning in which the policy is parameterized by a proposed proximity-enhanced Graph Neural Network (pGNN) and a GNN-based discriminator to score the reward. Finally, we utilize pre-training and teacher forcing to stabilize the learning process of GroupAligner. Extensive experiments on several real-world datasets are conducted to evaluate GroupAligner, and experimental results show that GroupAligner outperforms the alternative methods for social group alignment. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGraph neural network; network alignment; reinforcement learning; social network,Alignment; Deep learning; Graph neural networks; Information dissemination; Learning systems; Personnel training; Additional key word and phrasesgraph neural network; Domain adaptation; Key words; Network alignments; Neural-networks; Personal information; Reinforcement learnings; Social groups; Social network; User levels; Reinforcement learning
A Multi-Task Graph Neural Network with Variational Graph Auto-Encoders for Session-Based Travel Packages Recommendation,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167429187&doi=10.1145%2f3577032&partnerID=40&md5=9ea8ec61857eaa3a2226ce5ade4c9481,"Session-based travel packages recommendation aims to predict users' next click based on their current and historical sessions recorded by Online Travel Agencies (OTAs). Recently, an increasing number of studies attempted to apply Graph Neural Networks (GNNs) to the session-based recommendation and obtained promising results. However, most of them do not take full advantage of the explicit latent structure from attributes of items, making learned representations of items less effective and difficult to interpret. Moreover, they only combine historical sessions (long-term preferences) with a current session (short-term preference) to learn a unified representation of users, ignoring the effects of historical sessions for the current session. To this end, this article proposes a novel session-based model named STR-VGAE, which fills subtasks of the travel packages recommendation and variational graph auto-encoders simultaneously. STR-VGAE mainly consists of three components: travel packages encoder, users behaviors encoder, and interaction modeling. Specifically, the travel packages encoder module is used to learn a unified travel package representation from co-occurrence attribute graphs by using multi-view variational graph auto-encoders and a multi-view attention network. The users behaviors encoder module is used to encode user' historical and current sessions with a personalized GNN, which considers the effects of historical sessions on the current session, and coalesce these two kinds of session representations to learn the high-quality users' representations by exploiting a gated fusion approach. The interaction modeling module is used to calculate recommendation scores over all candidate travel packages. Extensive experiments on a real-life tourism e-commerce dataset from China show that STR-VGAE yields significant performance advantages over several competitive methods, meanwhile provides an interpretation for the generated recommendation list.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTravel recommendation; auto-encoder; graph mining; multi-task learning; session-based recommendation,Data mining; Learning systems; Signal encoding; User profile; 'current; Additional key word and phrasestravel recommendation; Auto encoders; Graph mining; Graph neural networks; Key words; Multitask learning; Package recommendations; Session-based recommendation; Travel packages; Graph neural networks
A Large-Scale Characterization of How Readers Browse Wikipedia,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154548707&doi=10.1145%2f3580318&partnerID=40&md5=e82ee03fe03b8b55c0c6ad4317b98c26,"Despite the importance and pervasiveness of Wikipedia as one of the largest platforms for open knowledge, surprisingly little is known about how people navigate its content when seeking information. To bridge this gap, we present the first systematic large-scale analysis of how readers browse Wikipedia. Using billions of page requests from Wikipedia's server logs, we measure how readers reach articles, how they transition between articles, and how these patterns combine into more complex navigation paths. We find that navigation behavior is characterized by highly diverse structures. Although most navigation paths are shallow, comprising a single pageload, there is much variety, and the depth and shape of paths vary systematically with topic, device type, and time of day. We show that Wikipedia navigation paths commonly mesh with external pages as part of a larger online ecosystem, and we describe how naturally occurring navigation paths are distinct from targeted navigation in lab-based settings. Our results further suggest that navigation is abandoned when readers reach low-quality pages. Taken together, these insights contribute to a more systematic understanding of readers' information needs and allow for improving their experience on Wikipedia and the Web in general. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",information needs; server logs; web navigation; Wikipedia,Complex navigation; Information need; Large-scale analysis; Large-scales; Navigation behavior; Navigation paths; Scale characterization; Server logs; Web navigation; Wikipedia; Navigation
Opinion Leaders for Information Diffusion Using Graph Neural Network in Online Social Networks,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153937343&doi=10.1145%2f3580516&partnerID=40&md5=7807c5ffbf227c704f62571c882d81b0,"Various opportunities are available to depict different domains due to the diverse nature of social networks and researchers' insatiable. An opinion leader is a human entity or cluster of people who can redirect human assessment strategy by intellectual skills in a social network. A more comprehensive range of approaches is developed to detect opinion leaders based on network-specific and heuristic parameters. For many years, deep learning-based models have solved various real-world multifaceted, graph-based problems with high accuracy and efficiency. The Graph Neural Network (GNN) is a deep learning-based model that modernized neural networks' efficiency by analyzing and extracting latent dependencies and confined embedding via messaging and neighborhood aggregation of data in the network. In this article, we have proposed an exclusive GNN for Opinion Leader Identification (GOLI) model utilizing the power of GNNs to categorize the opinion leaders and their impact on online social networks. In this model, we first measure the n-node neighbor's reputation of the node based on materialized trust. Next, we perform centrality conciliation instead of the input data's conventional node-embedding mechanism. We experiment with the proposed model on six different online social networks consisting of billions of users' data to validate the model's authenticity. Finally, after training, we found the top-N opinion leaders for each dataset and analyzed how the opinion leaders are influential in information diffusion. The training-testing accuracy and error rate are also measured and compared with the other state-of-art standard Social Network Analysis (SNA) measures. We determined that the GNN-based model produced high performance concerning accuracy and precision. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph Neural Network; information diffusion; Opinion Leader; reputation,Deep learning; Efficiency; Embeddings; Graph neural networks; Graphic methods; Assessment strategies; Different domains; Embeddings; Graph neural networks; Human assessment; Information diffusion; Intellectual skills; Learning Based Models; Opinion leaders; Reputation; Social networking (online)
A User-Centric Analysis of Social Media for Stock Market Prediction,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154575716&doi=10.1145%2f3532856&partnerID=40&md5=3c6f9705a10c3c6fffd5bfd5681bb8f6,"Social media platforms such as Twitter or StockTwits are widely used for sharing stock market opinions between investors, traders, and entrepreneurs. Empirically, previous work has shown that the content posted on these social media platforms can be leveraged to predict various aspects of stock market performance. Nonetheless, actors on these social media platforms may not always have altruistic motivations and may instead seek to influence stock trading behavior through the (potentially misleading) information they post. While a lot of previous work has sought to analyze how social media can be used to predict the stock market, there remain many questions regarding the quality of the predictions and the behavior of active users on these platforms. To this end, this article seeks to address a number of open research questions: Which social media platform is more predictive of stock performance? What posted content is actually predictive, and over what time horizon? How does stock market posting behavior vary among different users? Are all users trustworthy or do some user's predictions consistently mislead about the true stock movement? To answer these questions, we analyzed data from Twitter and StockTwits covering almost 5 years of posted messages spanning 2015 to 2019. The results of this large-scale study provide a number of important insights among which we present the following: (i) StockTwits is a more predictive source of information than Twitter, leading us to focus our analysis on StockTwits; (ii) on StockTwits, users' self-labeled sentiments are correlated with the stock market but are only slightly predictive in aggregate over the short-term; (iii) there are at least three clear types of temporal predictive behavior for users over a 144 days horizon: short, medium, and long term; and (iv) consistently incorrect users who are reliably wrong tend to exhibit what we conjecture to be ""botlike""post content and their removal from the data tends to improve stock market predictions from self-labeled content.  © 2023 Association for Computing Machinery.",classification; Social network analysis; stock market prediction,Commerce; Forecasting; Investments; Social networking (online); Misleading informations; Research questions; Social media; Social media platforms; Social Network Analysis; Stock market performance; Stock market prediction; Stock performance; Stock trading; User-centric; Financial markets
Improving Conformance of Web Services: A Constraint-based Model-driven Approach,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154597721&doi=10.1145%2f3580515&partnerID=40&md5=09c702cbd50884185a7df2ffbeb10edb,"Web services have been widely used to develop complex distributed software systems in the context of Service Oriented Architecture (SOA). As a standard for describing Web services, the Web Service Description Language (WSDL) provides a universal mechanism to describe the service's functionalities for the service consumers. However, the current WSDL only provides the description of the interfaces to a Web Service without any restrictions or assumptions on how to properly invoke the service, resulting in divergent understanding of the Web service's behavior between the service developer and service consumer. A particular challenge is how to make explicit the various behavior assumptions and restrictions of a service (for the user), and make sure that the service implementation conforms to them (for the developer). In this article, we propose a constraint-based model-driven approach to improving the behavior conformance of Web services. In our approach, constraints are introduced in an extended WSDL, called CxWSDL, to formally and explicitly express the implicit restrictions and assumptions on the behavior of a Web service, and then the predefined constraints are used to derive test cases in a model-driven manner to test the service implementation's conformance to its behavior constraints from the user's perspective. An empirical study involving four real-life Web services was conducted to evaluate the effectiveness of our approach, and four actual inconsistencies were discovered. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",conformance testing; model-driven testing; test case generation; Web services,Consumer behavior; Information services; Model checking; Service oriented architecture (SOA); Software testing; Websites; WSDL; Complex distributed software systems; Conformance testing; Constraint-based modeling; Constraints-based modeling; Model driven approach; Model-driven testing; Service consumers; Test case generation; Web service description language; Webs services; Web services
Disentangling Decentralized Finance (DeFi) Compositions,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144478293&doi=10.1145%2f3532857&partnerID=40&md5=21652f64c44a06a14488bf2b27459b6c,"We present a measurement study on compositions of Decentralized Finance (DeFi) protocols, which aim to disrupt traditional finance and offer services on top of distributed ledgers, such as Ethereum. Understanding DeFi compositions is of great importance, as they may impact the development of ecosystem interoperability, are increasingly integrated with web technologies, and may introduce risks through complexity. Starting from a dataset of 23 labeled DeFi protocols and 10,663,881 associated Ethereum accounts, we study the interactions of protocols and associated smart contracts. From a network perspective, we find that decentralized exchange (DEX) and lending protocol account nodes have high degree and centrality values, that interactions among protocol nodes primarily occur in a strongly connected component, and that known community detection methods cannot disentangle DeFi protocols. Therefore, we propose an algorithm to decompose a protocol call into a nested set of building blocks that may be part of other DeFi protocols. This allows us to untangle and study protocol compositions. With a ground truth dataset that we have collected, we can demonstrate the algorithm's capability by finding that swaps are the most frequently used building blocks. As building blocks can be nested, that is, contained in each other, we provide visualizations of composition trees for deeper inspections. We also present a broad picture of DeFi compositions by extracting and flattening the entire nested building block structure across multiple DeFi protocols. Finally, to demonstrate the practicality of our approach, we present a case study that is inspired by the recent collapse of the UST stablecoin in the Terra ecosystem. Under the hypothetical assumption that the stablecoin USD Tether would experience a similar fate, we study which building blocks-and, thereby, DeFi protocols-would be affected. Overall, our results and methods contribute to a better understanding of a new family of financial products. © 2023 Association for Computing Machinery.",blockchain; Decentralized Finance; DeFi; Ethereum; networks,Buildings; Ecosystems; Ethereum; Finance; Block-chain; Building blockes; Decentralised; Decentralized exchange; Decentralized finance; Measurement study; Network; Web technologies; Blockchain
Investment and Risk Management with Online News and Heterogeneous Networks,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154609289&doi=10.1145%2f3532858&partnerID=40&md5=bb3e7f441ffeff826e0e044f8fe368df,"Stock price movements in financial markets are influenced by large volumes of news from diverse sources on the web, e.g., online news outlets, blogs, social media. Extracting useful information from online news for financial tasks, e.g., forecasting stock returns or risks, is, however, challenging due to the low signal-to-noise ratios of such online information. Assessing the relevance of each news article to the price movements of individual stocks is also difficult, even for human experts. In this article, we propose the Guided Global-Local Attention-based Multimodal Heterogeneous Network (GLAM) model, which comprises novel attention-based mechanisms for multimodal sequential and graph encoding, a guided learning strategy, and a multitask training objective. GLAM uses multimodal information, heterogeneous relationships between companies and leverages significant local responses of individual stock prices to online news to extract useful information from diverse global online news relevant to individual stocks for multiple forecasting tasks. Our extensive experiments with multiple datasets show that GLAM outperforms other state-of-the-art models on multiple forecasting tasks and investment and risk management application case-studies.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention mechanisms; embeddings; finance; Graph neural networks; multimodality; natural language processing; networks; time-series forecasting; transformers,Deep learning; Embeddings; Financial markets; Forecasting; Graph neural networks; Information use; Investments; Natural language processing systems; Risk management; Signal to noise ratio; Attention mechanisms; Embeddings; Graph neural networks; Language processing; Multi-modality; Natural language processing; Natural languages; Network; Time series forecasting; Transformer; Heterogeneous networks
FinTech on the Web: An Overview,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154588015&doi=10.1145%2f3572404&partnerID=40&md5=0b8d1e168aeac162f9765e63bfb45a71,"In this article, we provide an overview of ACM TWEB's special issue, Financial Technology on the Web. This special issue covers diverse topics: (1) a new architecture for leveraging online news to investment and risk management, (2) a cross-platform analysis of the post quality and users' behaviors, and (3) an empirical study on disentangling decentralized finance compositions. In addition to a guide for the special issue, we also share a brief opinion on the future of financial technology on the Web.  © 2023 Copyright held by the owner/author(s).",Financial technology; FinTech; World Wide Web,Finance; Risk assessment; Risk management; Cross-platform; Decentralised; Empirical studies; Financial technology; Investment management; Online news; Risks management; User behaviors; World Wide Web
Enhancing Conversational Recommendation Systems with Representation Fusion,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149442295&doi=10.1145%2f3577034&partnerID=40&md5=24be2805ce7c7f486aefad3008c16c2c,"Conversational Recommendation Systems (CRSs) aim to improve recommendation performance by utilizing information from a conversation session. A CRS first constructs questions and then asks users for their feedback in each conversation session to refine better recommendation lists to users. The key design of CRS is to construct proper questions and obtain users' feedback in response to these questions so as to effectively capture user preferences. Many CRS works have been proposed; however, they suffer from defects when constructing questions for users to answer: (1) employing a dialogue policy agent for constructing questions is one of the most common choices in CRS, but it needs to be trained with a huge corpus, and (2) it is not appropriate that constructing questions from a single policy (e.g., a CRS only selects attributes that the user has interacted with) for all users with different preferences. To address these defects, we propose a novel CRS model, namely a Representation Fusion-based Conversational Recommendation model, where the whole conversation session is divided into two subsessions (i.e., Local Question Search subsession and Global Question Search subsession) and two different question search methods are proposed to construct questions in the corresponding subsessions without employing policy agents. In particular, in the Local Question Search subsession we adopt a novel graph mining method to find questions, where the paths in the graph between users and attributes can eliminate irrelevant attributes; in the Global Question Search subsession we propose to initialize user preference on items with the user and all item historical rating records and construct questions based on user's preference. Then, we update the embeddings independently over the two subsessions according to user's feedback and fuse the final embeddings from the two subsessions for the recommendation. Experiments on three real-world recommendation datasets demonstrate that our proposed method outperforms five state-of-the-art baselines.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Conversational Recommendation Systems; interactive recommendation; representation learning,Data mining; Defects; Embeddings; Conversational recommendation system; Conversational recommendations; Embeddings; Interactive recommendation; Policy agents; Recommendation performance; Representation learning; System models; User feedback; User's preferences; Recommender systems
Decoding the Kodi Ecosystem,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149434826&doi=10.1145%2f3563700&partnerID=40&md5=bb9d742f1be6136f5170d20f7991635a,"Free and open-source media centers are experiencing a boom in popularity for the convenience they offer users seeking to remotely consume digital content. Kodi is today's most popular home media center, with millions of users worldwide. Kodi's popularity derives from its ability to centralize the sheer amount of media content available on the Web, both free and copyrighted. Researchers have been hinting at potential security concerns around Kodi, due to add-ons injecting unwanted content as well as user settings linked with security holes. Motivated by these observations, this article conducts the first comprehensive analysis of the Kodi ecosystem: 15,000 Kodi users from 104 countries, 11,000 unique add-ons, and data collected over 9 months.Our work makes three important contributions. Our first contribution is that we build ""crawling""software (de-Kodi) which can automatically install a Kodi add-on, explore its menu, and locate (video) content. This is challenging for two main reasons. First, Kodi largely relies on visual information and user input which intrinsically complicates automation. Second, the potential sheer size of this ecosystem (i.e., the number of available add-ons) requires a highly scalable crawling solution. Our second contribution is that we develop a solution to discover Kodi add-ons. Our solution combines Web crawling of popular websites where Kodi add-ons are published (LazyKodi and GitHub) and SafeKodi, a Kodi add-on we have developed which leverages the help of Kodi users to learn which add-ons are used in the wild and, in return, offers information about how safe these add-ons are, e.g., do they track user activity or contact sketchy URLs/IP addresses. Our third contribution is a classifier to passively detect Kodi traffic and add-on usage in the wild.Our analysis of the Kodi ecosystem reveals the following findings. We find that most installed add-ons are unofficial but safe to use. Still, 78% of the users have installed at least one unsafe add-on, and even worse, such add-ons are among the most popular. In response to the information offered by SafeKodi, one-third of the users reacted by disabling some of their add-ons. However, the majority of users ignored our warnings for several months attracted by the content such unsafe add-ons have to offer. Last but not least, we show that Kodi's auto-update, a feature active for 97.6% of SafeKodi users, makes Kodi users easily identifiable by their ISPs. While passively identifying which Kodi add-on is in use is, as expected, much harder, we also find that many unofficial add-ons do not use HTTPS yet, making their passive detection straightforward. © 2023 Association for Computing Machinery.",crawling; crowdsourcing; Kodi; measurement,Decoding; Ecosystems; Open source software; Web crawler; Websites; Comprehensive analysis; Crawling; Digital contents; Kodi; Media center; Media content; Open-source; Security holes; User setting; Video contents; Crowdsourcing
ColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval,2023,ACM Transactions on the Web,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149439731&doi=10.1145%2f3572405&partnerID=40&md5=604be15eedde02002a4b3a0c7ff8020a,"Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models, have shown the usefulness of expanding and reweighting the users' initial queries using information occurring in an initial set of retrieved documents, known as the pseudo-relevant set. Recently, dense retrieval - through the use of neural contextual language models such as BERT for analysing the documents' and queries' contents and computing their relevance scores - has shown a promising performance on several information retrieval tasks still relying on the traditional inverted index for identifying documents relevant to a query. Two different dense retrieval families have emerged: the use of single embedded representations for each passage and query, e.g., using BERT's [CLS] token, or via multiple representations, e.g., using an embedding for each token of the query and document (exemplified by ColBERT). In this work, we conduct the first study into the potential for multiple representation dense retrieval to be enhanced using pseudo-relevance feedback and present our proposed approach ColBERT-PRF. In particular, based on the pseudo-relevant set of documents identified using a first-pass dense retrieval, ColBERT-PRF extracts the representative feedback embeddings from the document embeddings of the pseudo-relevant set. Among the representative feedback embeddings, the embeddings that most highly discriminate among documents are employed as the expansion embeddings, which are then added to the original query representation. We show that these additional expansion embeddings both enhance the effectiveness of a reranking of the initial query results as well as an additional dense retrieval operation. Indeed, experiments on the MSMARCO passage ranking dataset show that MAP can be improved by up to 26% on the TREC 2019 query set and 10% on the TREC 2020 query set by the application of our proposed ColBERT-PRF method on a ColBERT dense retrieval approach.We further validate the effectiveness of our proposed pseudo-relevance feedback technique for a dense retrieval model on MSMARCO document ranking and TREC Robust04 document ranking tasks. For instance, ColBERT-PRF exhibits up to 21% and 14% improvement in MAP over the ColBERT E2E model on the MSMARCO document ranking TREC 2019 and TREC 2020 query sets, respectively. Additionally, we study the effectiveness of variants of the ColBERT-PRF model with different weighting methods. Finally, we show that ColBERT-PRF can be made more efficient, attaining up to 4.54× speedup over the default ColBERT-PRF model, and with little impact on effectiveness, through the application of approximate scoring and different clustering methods. © 2023 Association for Computing Machinery.",BERT; dense retrieval; pseudo-relevance feedback; Query expansion,Information retrieval; Search engines; Semantics; BERT; Dense retrieval; Document ranking; Document Retrieval; Embeddings; Expansion embedding; Multiple representation; Passage retrieval; Pseudo-relevance feedbacks; Query expansion; Embeddings
