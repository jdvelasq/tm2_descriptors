Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Learning join queries from user examples,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954235801&doi=10.1145%2f2818637&partnerID=40&md5=a0d6fde14ca8b6f8d5cc455804c8bf18,"We investigate the problem of learning join queries from user examples. The user is presented with a set of candidate tuples and is asked to label them as positive or negative examples, depending on whether or not she would like the tuples as part of the join result. The goal is to quickly infer an arbitrary n-ary join predicate across an arbitrary number mof relations while keeping the number of user interactions as minimal as possible. We assume no prior knowledge of the integrity constraints across the involved relations. Inferring the join predicate across multiple relations when the referential constraints are unknown may occur in several applications, such as data integration, reverse engineering of database queries, and schema inference. In such scenarios, the number of tuples involved in the join is typically large. We introduce a set of strategies that let us inspect the search space and aggressively prune what we call uninformative tuples, and we directly present to the user the informative ones-that is, those that allow the user to quickly find the goal query she has inmind. In this article, we focus on the inference of joins with equality predicates and also allow disjunctive join predicates and projection in the queries. We precisely characterize the frontier between tractability and intractability for the following problems of interest in these settings: consistency checking, learnability, and deciding the informativeness of a tuple. Next, we propose several strategies for presenting tuples to the user in a given order that allows minimization of the number of interactions. We show the efficiency of our approach through an experimental study on both benchmark and synthetic datasets. © 2015 ACM.",Incomplete schema; Reverse engineering; SQL query discovery,Data integration; Database systems; Query processing; Reverse engineering; Arbitrary number; Consistency checking; Following problem; Incomplete schema; Integrity constraints; Negative examples; SQL query; Synthetic datasets; Query languages
Weaker forms of monotonicity for declarative networking: A more fine-grained answer to the CALM-conjecture,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953295646&doi=10.1145%2f2809784&partnerID=40&md5=cb7aee3f7f93830dbc510efac3f7e8f0,"The CALM-conjecture, first stated by Hellerstein [2010] and proved in its revised form by Ameloot et al. [2013] within the framework of relational transducer networks, asserts that a query has a coordination-free execution strategy if and only if the query is monotone. Zinn et al. [2012] extended the framework of relational transducer networks to allow for specific data distribution strategies and showed that the nonmonotone win-move query is coordination-free for domain-guided data distributions. In this article, we extend the story by equating increasingly larger classes of coordination-free computations with increasingly weaker forms of monotonicity and present explicit Datalog variants that capture each of these classes. One such fragment is based on stratified Datalog where rules are required to be connected with the exception of the last stratum. In addition, we characterize coordination-freeness as those computations that do not require knowledge about all other nodes in the network, and therefore, can not globally coordinate. The results in this article can be interpreted as a more fine-grained answer to the CALM-conjecture.",,Database systems; Information systems; Data distribution; Datalog; Declarative networkings; Execution strategies; Fine grained; Monotonicity; Nonmonotone; Transducers
Reducing layered database applications to their essence through vertical integration,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946908647&doi=10.1145%2f2818180&partnerID=40&md5=6b4261864fd4257089fc0fbf54c7c818,"In the last decade, improvements on single-core performance of CPUs has stagnated. Consequently, methods for the development and optimization of software for these platforms have to be reconsidered. Software must be optimized such that the available single-core performance is exploited more effectively. This can be achieved by reducing the number of instructions that need to be executed. In this article, we show that layered database applications execute many redundant, nonessential, instructions that can be eliminated without affecting the course of execution and the output of the application. This elimination is performed using a vertical integration process which breaks down the different layers of layered database applications. By doing so, applications are being reduced to their essence, and as a consequence, transformations can be carried out that affect both the application code and the data access code which were not possible before. We show that this vertical integration process can be fully automated and, as such, be integrated in an operational workflow. Experimental evaluation of this approach shows that up to 95% of the instructions can be eliminated. The reduction of instructions leads to a more efficient use of the available hardware resources. This results in greatly improved performance of the application and a significant reduction in energy consumption. © 2015 ACM.",Database applications; instruction reduction; intermediate representation; optimization; program transformation; software overhead; vertical integration,Codes (symbols); Database systems; Energy utilization; Integration; Optimization; Program processors; Application codes; Database applications; Experimental evaluation; Hardware resources; Intermediate representations; Program transformations; Reduction in energy consumption; Vertical integration; Application programs
SCANRAW: A database meta-operator for parallel in-situ processing and loading,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946897948&doi=10.1145%2f2818181&partnerID=40&md5=11c9e468607be46c13e49696ac3f70f2,"Traditional databases incur a significant data-to-query delay due to the requirement to load data inside the system before querying. Since this is not acceptable in many domains generating massive amounts of raw data (e.g., genomics), databases are entirely discarded. External tables, on the other hand, provide instant SQL querying over raw files. Their performance across a query workload is limited though by the speed of repeated full scans, tokenizing, and parsing of the entire file. In this article, we propose SCANRAW, a novel database meta-operator for in-situ processing over raw files that integrates data loading and external tables seamlessly, while preserving their advantages: optimal performance across a query workload and zero time-to-query. We decompose loading and external table processing into atomic stages in order to identify common functionality.We analyze alternative implementations and discuss possible optimizations for each stage. Our major contribution is a parallel superscalar pipeline implementation that allows SCANRAW to take advantage of the current many- and multicore processors by overlapping the execution of independent stages. Moreover, SCANRAW overlaps query processing with loading by speculatively using the additional I/O bandwidth arising during the conversion process for storing data into the database, such that subsequent queries execute faster. As a result, SCANRAW makes intelligent use of the available system resources-CPU cycles and I/O bandwidth-by switching dynamically between tasks to ensure that optimal performance is achieved. We implement SCANRAW in a state-of-the-art database system and evaluate its performance across a variety of synthetic and real-world datasets. Our results show that SCANRAW with speculative loading achieves the best-possible performance for a query sequence at any point in the processing. Moreover, SCANRAW maximizes resource utilization for the entire workload execution while speculatively loading data and without interfering with normal query processing. © 2015 ACM.",access path; Data access operator; data loading; database operator; external table; system,Bandwidth; In situ processing; Pipeline processing systems; Query languages; Query processing; Search engines; access path; Data access; Database operators; external table; system; Loading
Optimal location queries in road networks,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946905140&doi=10.1145%2f2818179&partnerID=40&md5=c7a637a7fe003c8ff86c0f55ed25e461,"In this article, we study an optimal location query based on a road network. Specifically, given a road network containing clients and servers, an optimal location query finds a location on the road network such that when a new server is set up at this location, a certain cost function computed based on the clients and servers (including the new server) is optimized. Two types of cost functions, namely, MinMax and MaxSum, have been used for this query. The optimal location query problem with MinMax as the cost function is called the MinMax query, which finds a location for setting up a new server such that themaximum cost of a client being served by his/her closest server is minimized. The optimal location query problem with MaxSum as the cost function is called the MaxSum query, which finds a location for setting up a new server such that the sum of the weights of clients attracted by the new server is maximized. The MinMax query and the MaxSum query correspond to two types of optimal location query with the objectives defined from the clients' perspective and from the new server's perspective, respectively. Unfortunately, the existing solutions for the optimal query problem are not efficient. In this article, we propose an efficient algorithm, namely, MinMax-Alg (MaxSum- Alg), for the MinMax (MaxSum) query, which is based on a novel idea of nearest location component. We also discuss two extensions of the optimal location query, namely, the optimal multiple-location query and the optimal location query on a 3D road network. Extensive experiments were conducted, showing that our algorithms are faster than the state of the art by at least an order of magnitude on large real benchmark datasets. For example, in our largest real datasets, the state of the art ran for more than 10 (12) hours while our algorithm ran within 3 (2) minutes only for the MinMax (MaxSum) query, that is, our algorithm ran at least 200 (600) times faster than the state of the art. © 2015 ACM.",nearest location component; Optimal location query; road network,Cost functions; Large dataset; Motor transportation; Roads and streets; Benchmark datasets; Location queries; nearest location component; Optimal query; Optimal-location query; Real data sets; Road network; State of the art; Location
Uncertain graph processing through representative instances,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946908911&doi=10.1145%2f2818182&partnerID=40&md5=0927057d1764ac2481cfbd0e37260ef5,"Data in several applications can be represented as an uncertain graph whose edges are labeled with a probability of existence. Exact query processing on uncertain graphs is prohibitive for most applications, as it involves evaluation over an exponential number of instantiations. Thus, typical approaches employ Monte-Carlo sampling, which (i) draws a number of possible graphs (samples), (ii) evaluates the query on each of them, and (iii) aggregates the individual answers to generate the final result. However, this approach can also be extremely time consuming for large uncertain graphs commonly found in practice. To facilitate efficiency, we study the problem of extracting a single representative instance from an uncertain graph. Conventional processing techniques can then be applied on this representative to closely approximate the result on the original graph. In order to maintain data utility, the representative instance should preserve structural characteristics of the uncertain graph. We start with representatives that capture the expected vertex degrees, as this is a fundamental property of the graph topology. We then generalize the notion of vertex degree to the concept of n-clique cardinality, that is, the number of cliques of size n that contain a vertex. For the first problem, we propose two methods: Average Degree Rewiring (ADR), which is based on random edge rewiring, and Approximate B-Matching (ABM), which applies graph matching techniques. For the second problem, we develop a greedy approach and a game-theoretic framework. We experimentally demonstrate, with real uncertain graphs, that indeed the representative instances can be used to answer, efficiently and accurately, queries based on several metrics such as shortest path distance, clustering coefficient, and betweenness centrality. © 2015 ACM.",n-clique cardinality; possible world; representative; Uncertain graph,Game theory; Graph algorithms; Graph theory; Graphic methods; Monte Carlo methods; Query processing; Betweenness centrality; Cardinalities; Conventional processing; Fundamental properties; Possible worlds; representative; Structural characteristics; Uncertain graphs; Graph structures
Boosting the quality of approximate string matching by synonyms,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946921713&doi=10.1145%2f2818177&partnerID=40&md5=91952ed59402985e7d5fa3a614bddbf7,"A string-similarity measure quantifies the similarity between two text strings for approximate string matching or comparison. For example, the strings ""Sam"" and ""Samuel"" can be considered to be similar. Most existing work that computes the similarity of two strings only considers syntactic similarities, for example, number of common words or q-grams. While this is indeed an indicator of similarity, there are many important cases where syntactically-different strings can represent the same real-world object. For example, ""Bill"" is a short form of ""William,"" and ""Database Management Systems"" can be abbreviated as ""DBMS."" Given a collection of predefined synonyms, the purpose of this article is to explore such existing knowledge to effectively evaluate the similarity between two strings and efficiently perform similarity searches and joins, thereby boosting the quality of approximate string matching. In particular, we first present an expansion-based framework to measure string similarities efficiently while considering synonyms. We then study efficient algorithms for similarity searches and joins by proposing two novel indexes, called SI-trees and QP-trees, which combine signature-filtering and length-filtering strategies. In order to improve the efficiency of our algorithms, we develop an estimator to estimate the size of candidates to enable an online selection of signature filters. This estimator provides strong low-error, high-confidence guarantees while requiring only logarithmic space and time costs, thus making our method attractive both in theory and in practice. Finally, the experimental results from a comprehensive study of the algorithms with three real datasets verify the effectiveness and efficiency of our approaches. © 2015 ACM.",semantic search; similarity join; String similarity search,Database systems; Efficiency; Forestry; Semantics; Approximate string matching; Effectiveness and efficiencies; Filtering strategies; Semantic search; Signature filtering; Similarity join; String similarity; Syntactic similarities; Cryptography
Workload-driven antijoin cardinality estimation,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946902152&doi=10.1145%2f2818178&partnerID=40&md5=dbf35cd26a881f1f6758ae12e5403ab2,"Antijoin cardinality estimation is among a handful of problems that has eluded accurate efficient solutions amenable to implementation in relational query optimizers. Given the widespread use of antijoin and subsetbased queries in analytical workloads and the extensive research targeted at join cardinality estimation-a seemingly related problem-the lack of adequate solutions for antijoin cardinality estimation is intriguing. In this article, we introduce a novel sampling-based estimator for antijoin cardinality that (unlike existent estimators) provides sufficient accuracy and efficiency to be implemented in a query optimizer. The proposed estimator incorporates three novel ideas. First, we use prior workload information when learning a mixture superpopulationmodel of the data offline. Second, we design a Bayesian statistics framework that updates the superpopulation model according to the live queries, thus allowing the estimator to adapt dynamically to the online workload. Third, we develop an efficient algorithm for sampling from a hypergeometric distribution in order to generate Monte Carlo trials, without explicitly instantiating either the population or the sample. When put together, these ideas form the basis of an efficient antijoin cardinality estimator satisfying the strict requirements of a query optimizer, as shown by the extensive experimental results over syntheticallygenerated as well as massive TPC-H data. © 2015 ACM.",Antijoin operator; Bayesian statistics; Monte Carlo; query optimization; sampling,Monte Carlo methods; Antijoin operator; Bayesian statistics; Cardinality estimations; Monte Carlo trials; Query optimization; Query optimizer; Relational queries; Super-population models; Sampling
Closing the gap: Sequence mining at scale,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934765088&doi=10.1145%2f2757217&partnerID=40&md5=1eca825244ca22d2366a8710d81aa4bd,"Frequent sequence mining is one of the fundamental building blocks in data mining. While the problem has been extensively studied, few of the available techniques are sufficiently scalable to handle datasets with billions of sequences; such large-scale datasets arise, for instance, in text mining and session analysis. In this article, we propose MG-FSM, a scalable algorithm for frequent sequence mining on MapReduce. MG-FSM can handle so-called ""gap constraints"", which can be used to limit the output to a controlled set of frequent sequences. Both positional and temporal gap constraints, as well as appropriate maximality and closedness constraints, are supported. At its heart, MG-FSM partitions the input database in a way that allows us to mine each partition independently using any existing frequent sequence mining algorithm. We introduce the notion of w-equivalency, which is a generalization of the notion of a ""projected database"" used by many frequent pattern mining algorithms. We also present a number of optimization techniques that minimize partition size, and therefore computational and communication costs, while still maintaining correctness. Our experimental study in the contexts of text mining and session analysis suggests that MG-FSM is significantly more efficient and scalable than alternative approaches.",Algorithms; Experimentation; Performance; Theory,Algorithms; Computation theory; Large dataset; Experimentation; Frequent pattern mining; Fundamental building blocks; Large-scale datasets; Optimization techniques; Performance; Scalable algorithms; Theory; Text mining
"""Differential dependencies: Reasoning and discovery"" Revisited",2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934760646&doi=10.1145%2f2757214&partnerID=40&md5=74c219c8a6fa3fd9f325331092cad9a5,"To address the frequently occurring situation where data is inexact or imprecise, a number of extensions to the classical notion of a functional dependency (FD) integrity constraint have been proposed in recent years. One of these extensions is the notion of a differential dependency (DD), introduced in the recent article ""Differential Dependencies: Reasoning and Discovery"" by Song and Chen in the March 2011 edition of this journal. A DD generalises the notion of an FD by requiring only that the values of the attribute from the RHS of the DD satisfy a distance constraint whenever the values of attributes from the LHS of the DD satisfy a distance constraint. In contrast, an FD requires that the values from the attributes in the RHS of an FD be equal whenever the values of the attributes from the LHS of the FD are equal. The article ""Differential Dependencies: Reasoning and Discovery"" investigated a number of aspects of DDs, the most important of which, since they form the basis for the other topics investigated, were the consistency problem (determining whether there exists a relation instance that satisfies a set of DDs) and the implication problem (determining whether a set of DDs logically implies another DD). Concerning these problems, a number of results were claimed in ""Differential Dependencies: Reasoning and Discovery"". In this article we conduct a detailed analysis of the correctness of these results. The outcomes of our analysis are that, for almost every claimed result, we show there are either fundamental errors in the proof or the result is false. For some of the claimed results we are able to provide corrected proofs, but for other results their correctness remains open. © 2015 ACM.",Design; Management; Theory,Database systems; Design; Information systems; Management; Consistency problems; Distance constraints; Functional dependency; Implication problem; Integrity constraints; Theory; Finite difference method
Deciding determinism with fairness for simple transducer networks,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934753202&doi=10.1145%2f2757215&partnerID=40&md5=a7f716702efd7f1e1701b33cb27fce98,"A distributed database system often operates in an asynchronous communication model where messages can be arbitrarily delayed. This communication model causes nondeterministic effects like unpredictable arrival orders of messages. Nonetheless, in general we want the distributed system to be deterministic; the system should produce the same output despite the nondeterministic effects on messages. Previously, two interpretations of determinism have been proposed. The first says that all infinite fair computation traces produce the same output. The second interpretation is a confluence notion, saying that all finite computation traces can still be extended to produce the same output. A decidability result for the confluence notion was previously obtained for so-called simple transducer networks, a model from the field of declarative networking. In the current article, we also present a decidability result for simple transducer networks, but this time for the first interpretation of determinism, with infinite fair computation traces. We also compare the expressivity of simple transducer networks under both interpretations. © 2015 ACM.",Languages; Theory,Computability and decidability; Information theory; Query languages; Transducers; Asynchronous communication model; Communication modeling; Declarative networkings; Distributed systems; Finite computations; Theory; Distributed database systems
Editorial: The Best of Two Worlds - Present Your TODS Paper at SIGMOD,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934754254&doi=10.1145%2f2770931&partnerID=40&md5=5204831005e75e3ed6ab5b47fe22e0df,[No abstract available],,
Efficient processing of skyline-join queries over multiple data sources,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934768251&doi=10.1145%2f2699483&partnerID=40&md5=748001c01fe40a00b69776f9489b50d0,"Efficient processing of skyline queries has been an area of growing interest. Many of the earlier skyline techniques assumed that the skyline query is applied to a single data table. Naturally, these algorithms were not suitable for many applications in which the skyline query may involve attributes belonging to multiple data sources. In other words, if the data used in the skyline query are stored in multiple tables, then join operations would be required before the skyline can be searched. The task of computing skylines on multiple data sources has been coined as the skyline-join problem and various skyline-join algorithms have been proposed. However, the current proposals suffer several drawbacks: they often need to scan the input tables exhaustively in order to obtain the set of skyline-join results; moreover, the pruning techniques employed to eliminate the tuples are largely based on expensive pairwise tuple-to-tuple comparisons. In this article, we aim to address these shortcomings by proposing two novel skyline-join algorithms, namely skyline-sensitive join (S2J) and symmetric skyline-sensitive join (S3J), to process skyline queries over two data sources. Our approaches compute the results using a novel layer/region pruning technique (LR-pruning) that prunes the join space in blocks as opposed to individual data points, thereby avoiding excessive pairwise point-to-point dominance checks. Furthermore, the S3J algorithm utilizes an early stopping condition in order to successfully compute the skyline results by accessing only a subset of the input tables. In addition to S2J and S3J, we also propose the S2J-M and S3J-M algorithms. These algorithms extend S2J's and S3J's two-way skyline-join ability to efficiently process skyline-join queries over more than two data sources. S2J-M and S3J-M leverage the extended concept of LR-pruning, called M-way LR-pruning, to compute multi-way skyline-joins in which more than two data sources are integrated during skyline processing. We report extensive experimental results that confirm the advantages of the proposed algorithms over state-of-the-art skyline-join techniques.",Algorithms; Design; Performance,Algorithms; Database systems; Design; Indexing (of information); Early stopping; Join algorithm; Join operation; Multiple data sources; Performance; Pruning techniques; Skyline Processing; State of the art; Query processing
Optimizing batch linear queries under exact and approximate differential privacy,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934766644&doi=10.1145%2f2699501&partnerID=40&md5=70d03ab3b7dd4556f4bf4abbe274db42,"Differential privacy is a promising privacy-preserving paradigm for statistical query processing over sensitive data. It works by injecting random noise into each query result such that it is provably hard for the adversary to infer the presence or absence of any individual record from the published noisy results. The main objective in differentially private query processing is to maximize the accuracy of the query results while satisfying the privacy guarantees. Previous work, notably Li et al. [2010], has suggested that, with an appropriate strategy, processing a batch of correlated queries as a whole achieves considerably higher accuracy than answering them individually. However, to our knowledge there is currently no practical solution to find such a strategy for an arbitrary query batch; existing methods either return strategies of poor quality (often worse than naive methods) or require prohibitively expensive computations for even moderately large domains. Motivated by this, we propose a low-rank mechanism (LRM), the first practical differentially private technique for answering batch linear queries with high accuracy. LRM works for both exact (i.e., ε-) and approximate (i.e., (ε, δ)-) differential privacy definitions. We derive the utility guarantees of LRM and provide guidance on how to set the privacy parameters, given the user's utility expectation. Extensive experiments using real data demonstrate that our proposed method consistently outperforms state-of-the-art query processing solutions under differential privacy, by large margins. © 2015 ACM.",Algorithms; Experimentation; Theory,Algorithms; Query processing; Differential privacies; Experimentation; Practical solutions; Privacy preserving; Processing solutions; Provide guidances; Statistical queries; Theory; Data privacy
Efficient processing of spatial group keyword queries,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934765568&doi=10.1145%2f2772600&partnerID=40&md5=1ab47737c76800caf9a262ba3bb3ab38,"With the proliferation of geo-positioning and geo-tagging techniques, spatio-textual objects that possess both a geographical location and a textual description are gaining in prevalence, and spatial keyword queries that exploit both location and textual description are gaining in prominence. However, the queries studied so far generally focus on finding individual objects that each satisfy a query rather than finding groups of objects where the objects in a group together satisfy a query. We define the problem of retrieving a group of spatio-textual objects such that the group's keywords cover the query's keywords and such that the objects are nearest to the query location and have the smallest inter-object distances. Specifically, we study three instantiations of this problem, all of which are NP-hard. We devise exact solutions as well as approximate solutions with provable approximation bounds to the problems. In addition, we solve the problems of retrieving top-k groups of three instantiations, and study a weighted version of the problem that incorporates object weights. We present empirical studies that offer insight into the efficiency of the solutions, as well as the accuracy of the approximate solutions. © 2015 ACM.",Algorithms; Experimentation; Performance,Algorithms; Location; Approximate solution; Approximation bounds; Empirical studies; Experimentation; Geographical locations; Individual objects; Performance; Textual description; Query processing
Cost-effective conceptual design for information extraction,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934783256&doi=10.1145%2f2716321&partnerID=40&md5=eb9758f79856e9e132b8d843110dc724,"It is well established that extracting and annotating occurrences of entities in a collection of unstructured text documents with their concepts improves the effectiveness of answering queries over the collection. However, it is very resource intensive to create and maintain large annotated collections. Since the available resources of an enterprise are limited and/or its users may have urgent information needs, it may have to select only a subset of relevant concepts for extraction and annotation. We call this subset a conceptual design for the annotated collection. In this article, we introduce and formally define the problem of cost-effective conceptual design where, given a collection, a set of relevant concepts, and a fixed budget, one likes to find a conceptual design that most improves the effectiveness of answering queries over the collection. We provide efficient algorithms for special cases of the problem and prove it is generally NP-hard in the number of relevant concepts. We propose three efficient approximations to solve the problem: a greedy algorithm, an approximate popularity maximization (APM for short), and approximate annotation-benefit maximization (AAM for short). We show that, if there are no constraints regrading the overlap of concepts, APM is a fully polynomial time approximation scheme. We also prove that if the relevant concepts are mutually exclusive, the greedy algorithm delivers a constant approximation ratio if the concepts are equally costly, APM has a constant approximation ratio, and AAM is a fully polynomial-time approximation scheme. Our empirical results using a Wikipedia collection and a search engine query log validate the proposed formalization of the problem and show that APM and AAM efficiently compute conceptual designs. They also indicate that, in general, APM delivers the optimal conceptual designs if the relevant concepts are not mutually exclusive. Also, if the relevant concepts are mutually exclusive, the conceptual designs delivered by AAM improve the effectiveness of answering queries over the collection more than the solutions provided by APM. © 2015 ACM.",Algorithms; Design; Performance,Algorithms; Budget control; Conceptual design; Cost effectiveness; Design; Information retrieval; Polynomial approximation; Search engines; Answering queries; Approximation ratios; Benefit maximization; Cost effective; Fully polynomial time approximation schemes; Greedy algorithms; Performance; Unstructured texts; Approximation algorithms
Editorial: Updates to the editorial board,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926653961&doi=10.1145%2f2747020&partnerID=40&md5=d3c3efe275c8bb18bf223f5ca6b37bf4,[No abstract available],,
Size bounds for factorised representations of query results,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926683999&doi=10.1145%2f2656335&partnerID=40&md5=4576a309bffccdcbe2e2e6d78bd865f7,"We study two succinct representation systems for relational data based on relational algebra expressionswith unions, Cartesian products, and singleton relations: f-representations, which employ algebraic factorisation using distributivity of product over union, and d-representations, which are f-representations where further succinctness is brought by explicit sharing of repeated subexpressions. In particular we study such representations for results of conjunctive queries. We derive tight asymptotic bounds for representation sizes and present algorithms to compute representations within these bounds. We compare the succinctness of f-representations and d-representations for results of equi-join queries, and relate them to fractional edge covers and fractional hypertree decompositions of the query hypergraph. Recent work showed that f-representations can significantly boost the performance of query evaluation in centralised and distributed settings and of machine learning tasks. © 2015 ACM.",Conjunctive queries; Data factorisation; Hypertree decompositions; Query evaluation; Size bounds; Succinct representation,Factorization; Conjunctive queries; Hypertree decomposition; Query evaluation; Size bounds; Succinct representation; Algebra
Online updates on data warehouses via judicious use of solid-state storage,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926685215&doi=10.1145%2f2699484&partnerID=40&md5=1f36768e46181fcd6b1f6f9dd1265b53,"Data warehouses have been traditionally optimized for read-only query performance, allowing only offline updates at night, essentially trading off data freshness for performance. The need for 24x7 operations in global markets and the rise of online and other quickly reacting businesses make concurrent online updates increasingly desirable. Unfortunately, state-of-the-art approaches fall short of supporting fast analysis queries over fresh data. The conventional approach of performing updates in place can dramatically slow down query performance, while prior proposals using differential updates either require large in-memory buffers or may incur significant update migration cost. This article presents a novel approach for supporting online updates in data warehouses that overcomes the limitations of prior approaches by making judicious use of available SSDs to cache incoming updates. We model the problem of query processing with differential updates as a type of outer join between the data residing on disks and the updates residing on SSDs.We present MaSM algorithms for performing such joins and periodicmigrations, with smallmemory footprints, low query overhead, low SSD writes, efficient in-place migration of updates, and correct ACID support. We present detailed modeling of the proposed approach, and provide proofs regarding the fundamental properties of the MaSM algorithms. Our experimentation shows that MaSM incurs only up to 7% overhead both on synthetic range scans (varying range size from 4KB to 100GB) and in a TPC-H query replay study, while also increasing the update throughput by orders of magnitude. © 2015 ACM.",Data warehouses; Materialized sort merge; Online updates; SSD,Data warehouses; International trade; Conventional approach; Detailed modeling; Fundamental properties; On-line updates; Orders of magnitude; Query performance; Solid-state storage; State-of-the-art approach; Digital storage
Time- and space-efficient sliding window top-k query processing,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926618762&doi=10.1145%2f2736701&partnerID=40&md5=2d4251328f00632a1a149ac19ce3fc40,"A sliding window top-k (top-k/w) query monitors incoming data stream objects within a sliding window of size w to identify the k highest-ranked objects with respect to a given scoring function over time. Processing of such queries is challenging because, even when an object is not a top-k/w object at the time when it enters the processing system, it might become one in the future. Thus a set of potential top-k/w objects has to be stored in memory while its size should be minimized to efficiently cope with high data streaming rates. Existing approaches typically store top-k/w and candidate sliding window objects in a k-skyband over a two-dimensional score-time space. However, due to continuous changes of the k-skyband, its maintenance is quite costly. Probabilistic k-skyband is a novel data structure storing data stream objects from a sliding window with significant probability to become top-k/w objects in future. Continuous probabilistic k-skyband maintenance offers considerably improved runtime performance compared to k-skyband maintenance, especially for large values of k, at the expense of a small and controllable error rate. We propose two possible probabilistic k-skyband usages: (i)When it is used to process all sliding window objects, the resulting top-k/w algorithm is approximate and adequate for processing random-order data streams. (ii) When probabilistic k-skyband is used to process only a subset of most recent sliding window objects, it can improve the runtime performance of continuous k-skyband maintenance, resulting in a novel exact top-k/w algorithm. Our experimental evaluation systematically compares different top-k/w processing algorithms and shows that while competing algorithms offer either time efficiency at the expanse of space efficiency or vice-versa, our algorithms based on the probabilistic k-skyband are both time and space efficient. © 2015 ACM.",Continuous top-k queries; Data stream processing; Sliding windows,Decoding; Efficiency; Information retrieval; Maintenance; Competing algorithms; Continuous top-k queries; Data stream processing; Experimental evaluation; Processing algorithms; Run-time performance; Sliding Window; Top-k query processing; Data streams
Efficient computation of the tree edit distance,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926685209&doi=10.1145%2f2699485&partnerID=40&md5=0ef85f9f686d4ca53b99a9faa7d538b6,"We consider the classical tree edit distance between ordered labelled trees, which is defined as the minimumcost sequence of node edit operations that transform one tree into another. The state-of-the-art solutions for the tree edit distance are not satisfactory. The main competitors in the field either have optimal worstcase complexity but the worst case happens frequently, or they are very efficient for some tree shapes but degenerate for others. This leads to unpredictable and often infeasible runtimes. There is no obvious way to choose between the algorithms. In this article we present RTED, a robust tree edit distance algorithm. The asymptotic complexity of our algorithm is smaller than or equal to the complexity of the best competitors for any input instance, that is, our algorithm is both efficient and worst-case optimal. This is achieved by computing a dynamic decomposition strategy that depends on the input trees. RTED is shown optimal among all algorithms that use LRH (left-right-heavy) strategies, which include RTED and the fastest tree edit distance algorithms presented in literature. In our experiments on synthetic and real-world data we empirically evaluate our solution and compare it to the state-of-the-art. © 2015 ACM.",Approximate matching; Similarity search; Tree edit distance,Algorithms; Computation; Computational complexity; Optimization; Approximate matching; Asymptotic complexity; Dynamic decomposition; Efficient computation; Similarity search; State of the art; Tree edit distance; Worst-case complexity; Trees (mathematics)
Multiple radii disc diversity: Result diversification based on dissimilarity and coverage,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926646675&doi=10.1145%2f2699499&partnerID=40&md5=7e61525373e0e8aec81d59c7a9b8159e,"Recently, result diversification has attracted a lot of attention as a means to improve the quality of results retrieved by user queries. In this article, we introduce a novel definition of diversity called DisC diversity. Given a tuning parameter r, which we call radius, we consider two items to be similar if their distance is smaller than or equal to r. A DisC diverse subset of a result contains items such that each item in the result is represented by a similar item in the diverse subset and the items in the diverse subset are dissimilar to each other.We show that locating a minimum DisC diverse subset is an NP-hard problem and provide algorithms for its approximation.We extend our definition to the multiple radii case, where each item is associated with a different radius based on its importance, relevance, or other factors.We also propose adapting DisC diverse subsets to a different degree of diversification by adjusting r, that is, increasing the radius (or zooming-out) and decreasing the radius (or zooming-in). We present efficient implementations of our algorithms based on the M-tree, a spatial index structure, and experimentally evaluate their performance. © 2015 ACM.",,NP-hard; Efficient implementation; Quality of results; Spatial index structure; Tuning parameter; User query; Zooming-in; Zooming-out; Set theory
Analysis of schemas with access restrictions,2015,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926637834&doi=10.1145%2f2699500&partnerID=40&md5=6bb6ecfb9cbc0bdab9cb7885c9a910f6,"We study verification of systems whose transitions consist of accesses to a Web-based data source. An access is a lookup on a relation within a relational database, fixing values for a set of positions in the relation. For example, a transition can represent access to a Web form, where the user is restricted to filling in values for a particular set of fields. We look at verifying properties of a schema describing the possible accesses of such a system. We present a language where one can describe the properties of an access path and also specify additional restrictions on accesses that are enforced by the schema. Our main property language, AccLTL, is based on a first-order extension of linear-time temporal logic, interpreting access paths as sequences of relational structures. We also present a lower-level automaton model, A-automata, into which AccLTL specifications can compile. We show that AccLTL and A-automata can express static analysis problems related to ""querying with limited access patterns"" that have been studied in the database literature in the past, such as whether an access is relevant to answering a query and whether two queries are equivalent in the accessible data they can return. We prove decidability and complexity results for several restrictions and variants of AccLTL and explain which properties of paths can be expressed in each restriction. © 2015 ACM.",Access methods; Hidden weba; Optimization,Automata theory; Optimization; Query languages; Query processing; Robots; Access methods; Access restriction; Analysis problems; Complexity results; Hidden weba; Linear time temporal logic; Relational Database; Relational structures; Temporal logic
I/O-efficient algorithms on triangle listing and counting,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920932368&doi=10.1145%2f2691190.2691193&partnerID=40&md5=ed7e7f04b9e69a5a89beda9e6fe0a3ec,"This article studies I/O-efficient algorithms for the triangle listing problem and the triangle counting problem, whose solutions are basic operators in dealing withmany other graph problems. In the former problem, given an undirected graph G, the objective is to find all the cliques involving 3 vertices in G. In the latter problem, the objective is to report just the number of such cliques without having to enumerate them. Both problems have been well studied in internal memory, but still remain as difficult challenges when G does not fit in memory, thus making it crucial to minimize the number of disk I/Os performed. Although previous research has attempted to tackle these challenges, the state-of-the-art solutions rely on a set of crippling assumptions to guarantee good performance. Motivated by this, we develop a new algorithm that is provably I/O and CPU efficient at the same time, without making any assumption on the input G at all. The algorithm uses ideas drastically different from all the previous approaches, and outperforms the existing competitors by a factor of over an order of magnitude in our extensive experimentation.",,Graph theory; Counting problems; Graph problems; Input-g; Internal memory; State of the art; Undirected graph; Graph algorithms
Naïve evaluation of queries over incomplete databases,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995595381&doi=10.1145%2f2691190.2691194&partnerID=40&md5=7b23e851723ed22d7d8d4729216b1178,"The term naïve evaluation refers to evaluating queries over incomplete databases as if nulls were usual data values, that is, to using the standard database query evaluation engine. Since the semantics of query answering over incomplete databases is that of certain answers,we would like to knowwhen naïve evaluation computes them, that is, when certain answers can be found without inventing new specialized algorithms. For relational databases it is well known that unions of conjunctive queries possess this desirable property, and results on preservation of formulae under homomorphisms tell us that, within relational calculus, this class cannot be extended under the open-world assumption. Our goal here is twofold. First, we develop a general framework that allows us to determine, for a given semantics of incompleteness, classes of queries for which naïve evaluation computes certain answers. Second, we apply this approach to a variety of semantics, showing that for many classes of queries beyond unions of conjunctive queries, naïve evaluationmakes perfect sense under assumptions different from open world. Our key observations are: (1) naïve evaluation is equivalent tomonotonicity of queries with respect to a semantics-induced ordering, and (2) for most reasonable semantics of incompleteness, such monotonicity is captured by preservation under various types of homomorphisms. Using these results we find classes of queries for which naïve evaluation works, for example, positive first-order formulae for the closed-world semantics. Even more, we introduce a general relation-based framework for defining semantics of incompleteness, show how it can be used to capture many known semantics and to introduce new ones, and describe classes of first-order queries for which naïve evaluation works under such semantics. 2014 Copyright held by the owner/author(s).",Algorithms; Certain answers; H.2.1 [database management]: languages - query languages;; H.2.1 [database management]: logical design - data models;; H.2.4 [database management]: systems - query processing; Homomorphisms; Incompleteness; Languages; Naive tables/evaluation; Orderings; Theory,Algebra; Algorithms; Calculations; Formal logic; Information management; Management information systems; Query processing; Search engines; Semantics; Certain answers; Database management; Homomorphisms; Incompleteness; Logical design data models; Naive tables/evaluation; Orderings; Systems-query processing; Theory; Query languages
"Ontology-based data access: A study through disjunctive datalog, CSP, and MMSNP",2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920917118&doi=10.1145%2f2661643&partnerID=40&md5=7edfb7093720af83a15d572def479027,"Ontology-based data access is concerned with querying incomplete data sources in the presence of domainspecific knowledge provided by an ontology. A central notion in this setting is that of an ontology-mediated query, which is a database query coupled with an ontology. In this article, we study several classes of ontology-mediated queries, where the database queries are given as some form of conjunctive query and the ontologies are formulated in description logics or other relevant fragments of first-order logic, such as the guarded fragment and the unary negation fragment. The contributions of the article are threefold. First, we show that popular ontology-mediated query languages have the same expressive power as natural fragments of disjunctive datalog, and we study the relative succinctness of ontology-mediated queries and disjunctive datalog queries. Second, we establish intimate connections between ontology-mediated queries and constraint satisfaction problems (CSPs) and their logical generalization, MMSNP formulas. Third, we exploit these connections to obtain new results regarding: (i) first-order rewritability and datalog rewritability of ontology-mediated queries; (ii) P/NP dichotomies for ontology-mediated queries; and (iii) the query containment problem for ontology-mediated queries.",,Constraint satisfaction problems; Data description; Formal logic; Intelligent databases; Query languages; Query processing; Conjunctive queries; Description logic; Disjunctive datalog; Domain-specific knowledge; First order logic; Guarded fragment; Ontology-based data access; Query containment; Ontology
A scalable lock manager for multicores,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920932455&doi=10.1145%2f2691190.2691192&partnerID=40&md5=abf58234845f2e12059036e47c3eaa4b,"Modern implementations of DBMS software are intended to take advantage of high core counts that are becoming common in high-end servers. However, we have observed that several database platforms, including MySQL, Shore-MT, and a commercial system, exhibit throughput collapse as load increases into oversaturation (where there are more request threads than cores), even for a workload with little or no logical contention for locks, such as a read-only workload. Our analysis of MySQL identifies latch contention within the lock manager as the bottleneck responsible for this collapse. We design a lock manager with reduced latching, implement it in MySQL, and show that it avoids the collapse and generally improves performance. Our efficient implementation of a lockmanager is enabled by a staged allocation and deallocation of locks. Locks are preallocated in bulk, so that the lock manager only has to perform simple list manipulation operations during the acquire and release phases of a transaction. Deallocation of the lock data structures is also performed in bulk, which enables the use of fast implementations of lock acquisition and release as well as concurrent deadlock checking.",,Concurrency control; Managers; Commercial systems; Database platforms; Efficient implementation; Fast implementation; High-end servers; Lock acquisition; Oversaturation; Throughput collapse; Locks (fasteners)
Top-k and clustering with noisy comparisons,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920879928&doi=10.1145%2f2684066&partnerID=40&md5=88cd5ccaa4ece0b1aedb24c217f89342,"We study the problems of max/top-k and clustering when the comparison operations may be performed by oracles whose answer may be erroneous. Comparisons may either be of type or of value: given two data elements, the answer to a type comparison is ""yes"" if the elements have the same type and therefore belong to the same group (cluster); the answer to a value comparison orders the two data elements. We give efficient algorithms that are guaranteed to achieve correct results with high probability, analyze the cost of these algorithms in terms of the total number of comparisons (i.e., using a fixed-cost model), and show that they are essentially the best possible. We also show that fewer comparisons are needed when values and types are correlated, or when the error model is one in which the error decreases as the distance between the two elements in the sorted order increases. Finally, we examine another important class of cost functions, concave functions, which balances the number of rounds of interaction with the oracle with the number of questions asked of the oracle. Results of this article form an important first step in providing a formal basis for max/top-k and clustering queries in crowdsourcing applications, that is, when the oracle is implemented using the crowd. We explain what simplifying assumptions are made in the analysis, what results carry to a generalized crowdsourcing setting, and what extensions are required to support a full-fledged model.",,Crowdsourcing; Clustering queries; Concave function; Data elements; Error model; Fixed cost; High probability; Simplifying assumptions; Cost functions
Foreword to invited articles issue,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920881713&doi=10.1145%2f2697050&partnerID=40&md5=3f45358b107c4526e5007cbef6ed548d,[No abstract available],,
A theory of pricing private data,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995577334&doi=10.1145%2f2691190.2691191&partnerID=40&md5=4cfa489d8af24ab97ce054a7ae235588,"Personal data has value to both its owner and to institutions who would like to analyze it. Privacy mechanisms protect the owner's data while releasing to analysts noisy versions of aggregate query results. But such strict protections of the individual's data have not yet found wide use in practice. Instead, Internet companies, for example, commonly provide free services in return for valuable sensitive information from users, which they exploit and sometimes sell to third parties. As awareness of the value of personal data increases, so has the drive to compensate the end-user for her private information. The idea of monetizing private data can improve over the narrower view of hiding private data, since it empowers individuals to control their data through financial means. In this article we propose a theoretical framework for assigning prices to noisy query answers as a function of their accuracy, and for dividing the price amongst data owners who deserve compensation for their loss of privacy. Our framework adopts and extends key principles from both differential privacy and query pricing in data markets. We identify essential properties of the pricing function and micropayments, and characterize valid solutions. © 2014 ACM.",Arbitrage; Data pricing; Differential privacy; Economics; H.2.8 [database applications]: statistical databases; Theory,Costs; Digital storage; Economics; Query processing; Arbitrage; Data pricing; Differential privacies; Statistical database; Theory; Data privacy
The complexity of mining maximal frequent subgraphs,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995549297&doi=10.1145%2f2629550&partnerID=40&md5=cf1129df483dbac5d0d304f2bf2bf3c0,"A frequent subgraph of a given collection of graphs is a graph that is isomorphic to a subgraph of at least as many graphs in the collection as a given threshold. Frequent subgraphs generalize frequent itemsets and arise in various contexts, from bioinformatics to the Web. Since the space of frequent subgraphs is typically extremely large, research in graph mining has focused on special types of frequent subgraphs that can be orders of magnitude smaller in number, yet encapsulate the space of all frequent subgraphs. Maximal frequent subgraphs (i.e., the ones not properly contained in any frequent subgraph) constitute the most useful such type. In this article, we embark on a comprehensive investigation of the computational complexity of mining maximal frequent subgraphs. Our study is carried out by considering the effect of three different parameters: possible restrictions on the class of graphs; a fixed bound on the threshold; and a fixed bound on the number of desired answers.We focus on specific classes of connected graphs: general graphs, planar graphs, graphs of bounded degree, and graphs of bounded treewidth (trees being a special case). Moreover, each class has two variants: that in which the nodes are unlabeled, and that in which they are uniquely labeled. We delineate the complexity of the enumeration problem for each of these variants by determining when it is solvable in (total or incremental) polynomial time and when it is NP-hard. Specifically, for the labeled classes, we show that bounding the threshold yields tractability but, in most cases, bounding the number of answers does not, unless P=NP; an exception is the case of labeled trees, where bounding either of these two parameters yields tractability. The state of affairs turns out to be quite different for the unlabeled classes. The main (and most challenging to prove) result concerns unlabeled trees: we show NP-hardness, even if the input consists of two trees and both the threshold and the number of desired answers are equal to just two. In other words, we establish that the following problem is NP-complete: given two unlabeled trees, do they have more than one maximal subtree in common? © 2014 ACM.",Algorithms; Enumeration complexity; G.2.2 [discrete mathematics]: graph theory - graph algorithms; Graph mining; H.2.8 [database management]: database applications-data mining; Maximal frequent subgraphs; Theory,Algorithms; Bioinformatics; Computational complexity; Data mining; Graph theory; Graphic methods; Polynomial approximation; Enumeration complexity; Frequent subgraphs; Graph algorithms; Graph mining; H.2.8 [database management]: database applications - data minings; Theory; Trees (mathematics)
Discovering XSD keys from XML data,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920870134&doi=10.1145%2f2638547&partnerID=40&md5=9531399682f51244d4bd0cbcaa8426c4,"A great deal of research into the learning of schemas from XML data has been conducted in recent years to enable the automatic discovery of XML schemas from XML documents when no schema or only a low-quality one is available. Unfortunately, and in strong contrast to, for instance, the relational model, the automatic discovery of even the simplest of XML constraints, namely XML keys, has been left largely unexplored in this context. A major obstacle here is the unavailability of a theory on reasoning about XML keys in the presence of XML schemas, which is needed to validate the quality of candidate keys. The present article embarks on a fundamental study of such a theory and classifies the complexity of several crucial properties concerning XML keys in the presence of an XSD, like, for instance, testing for consistency, boundedness, satisfiability, universality, and equivalence. Of independent interest, novel results are obtained related to cardinality estimation of XPath result sets. A mining algorithm is then developed within the framework of levelwise search. The algorithm leverages known discovery algorithms for functional dependencies in the relational model, but incorporates the properties mentioned before to assess and refine the quality of derived keys. An experimental study on an extensive body of real-world XML data evaluating the effectiveness of the proposed algorithm is provided.",,Relational database systems; Automatic discovery; Cardinality estimations; Discovery algorithm; Functional dependency; Fundamental studies; Mining algorithms; Relational Model; Strong contrast; XML
Lightweight query authentication on streams,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920926778&doi=10.1145%2f2656336&partnerID=40&md5=f25011c5498a4ded6f8895b48d0a38db,"We consider a stream outsourcing setting, where a data owner delegates the management of a set of disjoint data streams to an untrusted server. The owner authenticates his streams via signatures. The server processes continuous queries on the union of the streams for clients trusted by the owner. Along with the results, the server sends proofs of result correctness derived from the owner's signatures, which are veri-fiable by the clients. We design novel constructions for a collection of fundamental problems over streams represented as linear algebraic queries. In particular, our basic schemes authenticate dynamic vector sums, matrix products, and dot products. These techniques can be adapted for authenticating a wide range of important operations in streaming environments, including group-by queries, joins, in-network aggregation, similarity matching, and event processing. We also present extensions to address the case of sliding window queries, and when multiple clients are interested in different subsets of the data. These methods take advantage of a novel nonce chaining technique that we introduce, which is used to reduce the verification cost without affecting any other costs. All our schemes are lightweight and offer strong cryptographic guarantees derived from formal definitions and proofs. We experimentally confirm the practicality of our schemes in the performance-sensitive streaming setting.",,Authentication; Outsourcing; Query processing; Continuous queries; Event Processing; Formal definition; In-network aggregation; Multiple clients; Novel construction; Query authentications; Similarity-matching; Data streams
Private analysis of graph structure,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995700579&doi=10.1145%2f2611523&partnerID=40&md5=f5e68a012a0c792d8fde5ac5288587f2,"We present efficient algorithms for releasing useful statistics about graph data while providing rigorous privacy guarantees. Our algorithms work on datasets that consist of relationships between individuals, such as social ties or email communication. The algorithms satisfy edge differential privacy, which essentially requires that the presence or absence of any particular relationship be hidden. Our algorithms output approximate answers to subgraph counting queries. Given a query graph H, for example, a triangle, k-star, or k-triangle, the goal is to return the number of edge-induced isomorphic copies of H in the input graph. The special case of triangles was considered by Nissim et al. [2007] and a more general investigation of arbitrary query graphs was initiated by Rastogi et al. [2009]. We extend the approach of Nissim et al. to a new class of statistics, namely k-star queries.We also give algorithms for k-triangle queries using a different approach based on the higher-order local sensitivity. For the specific graph statistics we consider (i.e., k-stars and k-triangles), we significantly improve on the work of Rastogi et al.: our algorithms satisfy a stronger notion of privacy and add less noise to the answers before releasing them. We evaluate the accuracy of our algorithms both theoretically and empirically, using a variety of real and synthetic datasets. We give explicit, simple conditions under which these algorithms add a small amount of noise. We also provide the average-case analysis in the Erdo{combining double acute accent}s-Rényi-Gilbert G(n, p) random graph model. Finally, we give hardness results indicating that the approach Nissim et al. used for triangles cannot easily be extended to k-triangles (hence justifying our development of a new algorithmic approach). © 2014 ACM.",Algorithms; Differential privacy; Graph algorithms; Security; Social network analysis; Statistical data privacy; Theory; [Security and privacy]: data anonymization and Sanitization; [Theory of computation]: graph algorithms analysis,Algorithms; Aluminum; Computation theory; Data privacy; Higher order statistics; Social networking (online); Stars; Data anonymization; Differential privacies; Graph algorithms; Security; Statistical datas; Theory; Graph theory
Editorial,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907927344&doi=10.1145%2f2662448&partnerID=40&md5=f2b05d6edc84fb7bd31dd3e966eedaa0,[No abstract available],,
A join-like operator to combine data cubes and answer queries from multiple data cubes,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907932549&doi=10.1145%2f2638545&partnerID=40&md5=c7b6de92a1133dde981892b0549889af,"In order to answer a ""joint"" query from multiple data cubes, Pourabass and Shoshani [2007] distinguish the data cube on the measure of interest (called the ""primary"" data cube) from the other data cubes (called ""proxy"" data cubes) that are used to involve the dimensions (in the query) not in the primary data cube. They demonstrate in study cases that, if the measures of the primary and proxy data cubes are correlated, then the answer to a joint query is an accurate estimate of its true value. Needless to say, for two or more proxy data cubes, the result depends upon the way the primary and proxy data cubes are combined together; however, for certain combination schemes Pourabass and Shoshani provide a sufficient condition, that they call proxy noncornrnonality, for the invariance of the result. In this article, we introduce: (1) a merge operator combining the contents of a primary data cube with the contents of a proxy data cube, (2) merge expressions for general combination schemes, and (3) an equivalence relation between merge expressions having the same pattern. Then, we prove that proxy noncornrnonality characterizes patterns for which every two merge expressions are equivalent. Moreover, we provide an efficient procedure for answering joint queries in the special case of perfect merge expressions. Finally, we show that our results apply to data cubes in which measures are obtained from unaggregated data using the aggregate functions SUM, COUNT, MAX, and MIN, and a lot more. © 2014 ACM.",Data cubes; Data integration; OLAP,Data cube; Multiple data; OLAP; Data integration
Finding robust itemsets under subsampling,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907931600&doi=10.1145%2f2656261&partnerID=40&md5=dcd443ed36a2aaad3235fdc779097edb,"Mining frequent patterns is plagued by the problem of pattern explosion, making pattern reduction techniques a key challenge in pattern mining. In this article we propose a novel theoretical framework for pattern reduction by measuring the robustness of a property of an itemset such as closedness or nonderivability. The robustness of a property is the probability that this property holds on random subsets of the original data. We study four properties, namely an itemset being closed, free, non-derivable, or totally shattered, and demonstrate how to compute the robustness analytically without actually sampling the data. Our concept of robustness has many advantages: Unlike statistical approaches for reducing patterns, we do not assume a null hypothesis or any noise model and, in contrast to noise-tolerant or approximate patterns, the robust patterns for a given property are always a subset of the patterns with this property. If the underlying property is monotonie then the measure is also monotonie, allowing us to efficiently mine robust itemsets. We further derive a parameter-free technique for ranking itemsets that can be used for top-k approaches. Our experiments demonstrate that we can successfully use the robustness measure to reduce the number of patterns and that ranking yields interesting itemsets. © 2014 ACM.",Closed itemsets; Free itemsets; Non-derivable itemsets; Pattern reduction; Robust itemsets; Totally shattered itemsets,Closed itemsets; Item sets; Pattern reductions
Query rewriting and optimization for ontological databases,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907937930&doi=10.1145%2f2638546&partnerID=40&md5=123c68043085923885c0fdb2017583d3,"Ontological queries are evaluated against a knowledge base consisting of an extensional database and an ontology (i.e., a set of logical assertions and constraints that derive new intensional knowledge from the extensional database), rather than directly on the extensional database. The evaluation and optimization of such queries is an intriguing new problem for database research. In this article, we discuss two important aspects of this problem: query rewriting and query optimization. Query rewriting consists of the compilation of an ontological query into an equivalent first-order query against the underlying extensional database. We present a novel query rewriting algorithm for rather general types of ontological constraints that is well suited for practical implementations. In particular, we show how a conjunctive query against a knowledge base, expressed using linear and sticky existential rules, that is, members of the recently introduced Datalog family of ontology languages, can be compiled into a union of conjunctive queries (UCQ) against the underlying database. Ontological query optimization, in this context, attempts to improve this rewriting process soas to produce possibly small and cost-effective UCQ rewritings for an input query. © 2014 ACM.",Ontological query answering; Query optimization; Query rewriting; Tuple-generating dependencies,Ontological database; Ontological query; Query optimization; Query rewritings; Tuple-generating dependencies
Towards a painless index for spatial objects,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907818806&doi=10.1145%2f2629333&partnerID=40&md5=14d80d84127f3251d6315092a4ba4103,"Conventional spatial indexes, represented by the R-tree, employ multidimensional tree structures that are complicated and require enormous efforts to implement in a full-fledged database management system (DBMS). An alternative approach for supporting spatial queries is mapping-based indexing, which maps both data and queries into a one-dimensional space such that data can be indexed and queries can be processed through a one-dimensional indexing structure such as the B+-tree. Mapping-based indexing requires implementing only a few mapping functions, incurring much less effort in implementation compared to conventional spatial index structures. Yet, a major concern about using mapping-based indexes is their lower efficiency than conventional tree structures. In this article, we propose a mapping-based spatial indexing scheme called Size Separation Indexing (SSI). SSI is equipped with a suite of techniques including size separation, data distribution transformation, and more efficient mapping algorithms. These techniques overcome the drawbacks of existing mappingbased indexes and significantly improve the efficiency of query processing. We show through extensive experiments that, for window queries on spatial objects with nonzero extents, SSI has two orders of magnitude better performance than existing mapping-based indexes and competitive performance to the R-tree as a standalone implementation. We have also implemented SSI on top of two off-the-shelf DBMSs, PostgreSQL and a commercial platform, both having R-tree implementation. In this case, SSI is up to two orders of magnitude faster than their provided spatial indexes. Therefore, we achieve a spatial index more efficient than the R-tree in a DBMS implementation that is at the same time easy to implement. This result may upset a common perception that has existed for a long time in this area that the R-tree is the best choice for indexing spatial objects. © 2014 ACM.",Mapping-based indexing; Spacefilling Curves; Spatial databases; Window queries,Space-filling curve; Spatial database; Spatial objects
Privacy-preserving ad-hoc Equi-join on outsourced data,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907811575&doi=10.1145%2f2629501&partnerID=40&md5=51af95854abb8971f499eed44d0747a6,"In IT outsourcing, a user may delegate the data storage and query processing functions to a third-party server that is not completely trusted. This gives rise to the need to safeguard the privacy of the database as well as the user queries over it. In this article, we address the problem of running ad hoc equi-join queries directly on encrypted data in such a setting. Our contribution is the first solution that achieves constant complexity per pair of records that are evaluated for the join. After formalizing the privacy requirements pertaining to the database and user queries, we introduce a cryptographic construct for securely joining records across relations. The construct protects the database with a strong encryption scheme. Moreover, information disclosure after executing an equi-join is kept to the minimum-that two input records combine to form an output record if and only if they share common join attribute values. There is no disclosure on records that are not part of the join result. Building on this construct, we then present join algorithms that optimize the join execution by eliminating the need to match every record pair from the input relations. We provide a detailed analysis of the cost of the algorithms and confirm the analysis through extensive experiments with both synthetic and benchmark workloads. Through this evaluation, we tease out useful insights on how to configure the join algorithms to deliver acceptable execution time in practice. © 2014 ACM.",Data and query privacy; Equi-join; Query over encrypted data,Encrypted data; Equi-join; Outsourced datum; Privacy preserving; Query privacy
Maximizing range sum in external memory,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907814922&doi=10.1145%2f2629477&partnerID=40&md5=9a2f6b86c2f2f23b478bce2a7ba237cf,"This article studies the MaxRS problem in spatial databases. Given a set O of weighted points and a rectangle r of a given size, the goal of the MaxRS problem is to find a location of r such that the sum of the weights of all the points covered by r is maximized. This problem is useful in many location-based services such as finding the best place for a new franchise store with a limited delivery range and finding the hotspot with the largest number of nearby attractions for a tourist with a limited reachable range. However, the problem has been studied mainly in the theoretical perspective, particularly in computational geometry. The existing algorithms from the computational geometry community are in-memory algorithms that do not guarantee the scalability. In this article, we propose a scalable external-memory algorithm (ExactMaxRS) for the MaxRS problem that is optimal in terms of the I/O complexity. In addition, we propose an approximation algorithm (ApproxMaxCRS) for the MaxCRS problem that is a circle version of the MaxRS problem. We prove the correctness and optimality of the ExactMaxRS algorithm along with the approximation bound of the ApproxMaxCRS algorithm. Furthermore, motivated by the fact that all the existing solutions simply assume that there is no tied area for the best location, we extend the MaxRS problem to a more fundamental problem, namely AllMaxRS, so that all the locations with the same best score can be retrieved. We first prove that the AllMaxRS problem cannot be trivially solved by applying the techniques for the MaxRS problem. Then we propose an outputsensitive external-memory algorithm (TwoPhaseMaxRS) that gives the exact solution for the AllMaxRS problem through two phases. Also, we prove both the soundness and completeness of the result returned from TwoPhaseMaxRS. From extensive experimental results, we show that ExactMaxRS and ApproxMaxCRS are several orders of magnitude faster than methods adapted from existing algorithms, the approximation bound in practice is much better than the theoretical bound of ApproxMaxCRS, and TwoPhaseMaxRS is not only much faster but also more robust than the straightforward extension of ExactMaxRS. © 2014 ACM.",External memory; Optimal-location query; Range sum; Spatial databases,External memory; Optimal-location query; Range sum; Spatial database
Efficient algorithms and cost models for reverse spatial-keyword k-nearest neighbor search,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901491070&doi=10.1145%2f2576232&partnerID=40&md5=5d420b0195f7df46e5a51bae465311aa,"Geographic objects associated with descriptive texts are becoming prevalent, justifying the need for spatialkey word queries that consider both locations and textual descriptions of the objects. Specifically, the relevance of an object to a query is measured by spatial-textual similarity that is based on both spatial proximity and textual similarity. In this article, we introduce the Reverse Spatial-Keyword k-Nearest Neighbor (RSKkNN) query, which finds those objects that have the query as one of their k-nearest spatial-textual objects. The RSKkNN queries have numerous applications in online maps and GIS decision support systems. To answer RSKkNN queries efficiently, we propose a hybrid index tree, called IUR-tree (Intersection-Union R-tree) that effectively combines location proximity with textual similarity. Subsequently, we design a branch-and-bound search algorithm based on the IUR-tree. To accelerate the query processing, we improve IUR-tree by leveraging the distribution of textual description, leading to some variants of the IUR-tree called Clustered IUR-tree (CIUR-tree) and combined clustered IUR-tree (C2IUR-tree), for each of which we develop optimized algorithms. We also provide a theoretical cost model to analyze the efficiency of our algorithms. Our empirical studies show that the proposed algorithms are efficient and scalable. © 2014 ACM.",Performance analysis; Reverse k-nearest neighbor queries; Spatial-keyword query,Algorithms; Forestry; GIS; Mathematics; Trees; Algorithms; Artificial intelligence; Decision support systems; Decision trees; Forestry; Geographic information systems; Graphical user interfaces; Trees (mathematics); K nearest neighbor search; K-nearest neighbors; Optimized algorithms; Performance analysis; Reverse k-nearest neighbors; Spatial-keyword query; Textual description; Textual similarities; Query processing
Domination in the probabilistic world: Computing skylines for arbitrary correlations and ranking semantics,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901489038&doi=10.1145%2f2602135&partnerID=40&md5=b1225592d256529ce1e88d5fa4f5b8d5,"In a probabilistic database, deciding if a tuple u is better than another tuple v has not a univocal solution, rather it depends on the specific Probabilistic Ranking Semantics (PRS) one wants to adopt so as to combine together tuples' scores and probabilities. In deterministic databases it is known that skyline queries are a remarkable alternative to (top-k) ranking queries, because they remove from the user the burden of specifying a scoring function that combines values of different attributes into a single score. The skyline of a deterministic relation R is the set of undominated tuples in R - tuple u dominates tuple v iff on all the attributes of interest u is better than or equal to v and strictly better on at least one attribute. Domination is equivalent to having s(u) ≥ s(v) for all monotone scoring functions s(). The skyline of a probabilistic relation Rp can be similarly defined as the set of P-undominated tuples in Rp, where now u P-dominates v iff, whatever monotone scoring function one would use to combine the skyline attributes, u is reputed better than v by the PRS at hand. This definition, which is applicable to arbitrary ranking semantics and probabilistic correlation models, is parametric in the adopted PRS, thus it ensures that ranking and skyline queries will always return consistent results. In this article we provide an overall view of the problem of computing the skyline of a probabilistic relation. We show how, under mild conditions that indeed hold for all known PRSs, checking P-domination can be cast into an optimization problem, whose complexity we characterize for a variety of combinations of ranking semantics and correlation models. For each analyzed case we also provide specific P-domination rules, which are exploited by the algorithm we detail for the case where the probabilistic model is known to the query processor. We also consider the case in which the probability of tuple events can only be obtained through an oracle, and describe another skyline algorithm for this loosely integrated scenario. Our experimental evaluation of P-domination rules and skyline algorithms confirms the theoretical analysis. © 2014 ACM.",Probabilistic database; Ranking semantics; Skyline queries,Database systems; Model checking; Optimization; Query processing; Arbitrary correlation; Experimental evaluation; Optimization problems; Probabilistic database; Probabilistic modeling; Probabilistic ranking; Probabilistic relations; Skyline query; Semantics
Robust distributed query processing for streaming data,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901501772&doi=10.1145%2f2602138&partnerID=40&md5=a6f52e86c5f5d350193b57c08b3a607d,"Distributed stream processing systems must function efficiently for data streams that fluctuate in their arrival rates and data distributions. Yet repeated and prohibitively expensive load reallocation across machines may make these systems ineffective, potentially resulting in data loss or even system failure. To overcome this problem, we propose a comprehensive solution, called the Robust Load Distribution (RLD) strategy, that is resilient under data fluctuations. RLD provides ∈-optimal query performance under an expected range of load fluctuations without suffering from the performance penalty caused by load migration. RLD is based on three key strategies. First, we model robust distributed stream processing as a parametric query optimization problem in a parameter space that captures the stream fluctuations. The notions of both robust logical and robust physical plans that work together to proactively handle all ranges of expected fluctuations in parameters are abstracted as overlays of this parameter space. Second, our Early-terminated Robust Partitioning (ERP) finds a combination of robust logical plans that together cover the parameter space, while minimizing the number of prohibitively expensive optimizer calls with a probabilistic bound on the space coverage. Third, we design a family of algorithms for physical plan generation. Our GreedyPhy exploits a probabilistic model to efficiently find a robust physical plan that sustains most frequently used robust logical plans at runtime. Our CorPhy algorithm exploits operator correlations for the robust physical plan optimization. The resulting physical plan smooths the workload on each node under all expected fluctuations. Our OptPrune algorithm, using CorPhy as baseline, is guaranteed to find the optimal physical plan that maximizes the parameter space coverage with a practical increase in optimization time. Lastly, we further expand the capabilities of our proposed RLD framework to also appropriately react under so-called ""space drifts"", that is, a space drift is a change of the parameter space where the observed runtime statistics deviate from the expected optimization-time statistics. Our RLD solution is capable of adjusting itself to the unexpected yet significant data fluctuations beyond those planned for via covering the parameter space. Our experimental study using stock market and sensor network streams demonstrates that our RLD methodology consistently outperforms state-of-the-art solutions in terms of efficiency and effectiveness in highly fluctuating data stream environments. © 2014 ACM.",Distributed system; Query optimization; Stream processing,Data communication systems; Optimization; Sensor networks; Distributed query processing; Distributed stream processing; Distributed systems; Minimizing the number of; Performance penalties; Probabilistic modeling; Query optimization; Stream processing; Distributed parameter control systems
Distributed geometric query monitoring using prediction models,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901496328&doi=10.1145%2f2602137&partnerID=40&md5=4e21c8d50e8af359381152f27acc951f,"Many modern streaming applications, such as online analysis of financial, network, sensor, and other forms of data, are inherently distributed in nature. An important query type that is the focal point in such application scenarios regards actuation queries, where proper action is dictated based on a trigger condition placed upon the current value that a monitored function receives. Recent work [Sharfman et al. 2006, 2007b, 2008] studies the problem of (nonlinear) sophisticated function tracking in a distributive manner. The main concept behind the geometric monitoring approach proposed there is for each distributed site to perform the function monitoring over an appropriate subset of the input domain. In the current work, we examine whether the distributed monitoring mechanism can become more efficient, in terms of the number of communicated messages, by extending the geometric monitoring framework to utilize prediction models. We initially describe a number of local estimators (predictors) that are useful for the applications that we consider and which have already been shown particularly useful in past work. We then demonstrate the feasibility of incorporating predictors in the geometric monitoring framework and show that predictionbased geometric monitoring in fact generalizes the original geometric monitoring framework. We propose a large variety of different prediction-based monitoring models for the distributed threshold monitoring of complex functions. Our extensive experimentation with a variety of real datasets, functions, and parameter settings indicates that our approaches can provide significant communication savings ranging between two times and up to three orders of magnitude, compared to the transmission cost of the original monitoring framework. © 2014 ACM.",Continuous distributed monitoring; Data streams; Prediction models,Complex networks; Computer aided network analysis; Geometry; Mathematical models; Application scenario; Continuous distributed monitoring; Data stream; Distributed monitoring; Monitoring frameworks; Prediction model; Streaming applications; Three orders of magnitude; Monitoring
On the complexity of query result diversification,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901488039&doi=10.1145%2f2602136&partnerID=40&md5=3418e109d3eb23a1355192e0b6e959dd,"Query result diversification is a bi-criteria optimization problem for ranking query results. Given a database D, a query Q, and a positive integer k, it is to find a set of k tuples from Q(D) such that the tuples are as relevant as possible to the query, and at the same time, as diverse as possible to each other. Subsets of Q(D) are ranked by an objective function defined in terms of relevance and diversity. Query result diversification has found a variety of applications in databases, information retrieval, and operations research. This article investigates the complexity of result diversification for relational queries. (1) We identify three problems in connection with query result diversification, to determine whether there exists a set of k tuples that is ranked above a bound with respect to relevance and diversity, to assess the rank of a given k-element set, and to count how many k-element sets are ranked above a given bound based on an objective function. (2) We study these problems for a variety of query languages and for the three objective functions proposed in Gollapudi and Sharma [2009]. We establish the upper and lower bounds of these problems, all matching, for both combined complexity and data complexity. (3) We also investigate several special settings of these problems, identifying tractable cases. Moreover, (4) we reinvestigate these problems in the presence of compatibility constraints commonly found in practice, and provide their complexity in all these settings. © 2014 ACM.",Combined complexity; Counting problems; Data complexity; Database queries; Diversity; Recommender systems; Relevance; Result diversification,Operations research; Optimization; Query languages; Recommender systems; Combined complexity; Counting problems; Data complexity; Database queries; Diversity; Relevance; Result diversification; Query processing
Sharing across multiple MapReduce Jobs,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901479130&doi=10.1145%2f2560796&partnerID=40&md5=4f7182879757bf71e48e743b74decdb0,"Large-scale data analysis lies in the core of modern enterprises and scientific research. With the emergence of cloud computing, the use of an analytical query processing infrastructure can be directly associated with monetary cost. MapReduce has been a popular framework in the context of cloud computing, designed to serve long-running queries (jobs) which can be processed in batch mode. Taking into account that different jobs often perform similar work, there are many opportunities for sharing. In principle, sharing similar work reduces the overall amount of work, which can lead to reducing monetary charges for utilizing the processing infrastructure. In this article we present a sharing framework tailored to MapReduce, namely, MRShare. Our framework, MRShare, transforms a batch of queries into a new batch that will be executed more efficiently, by merging jobs into groups and evaluating each group as a single query. Based on our cost model for MapReduce, we define an optimization problem and we provide a solution that derives the optimal grouping of queries. Given the query grouping, we merge jobs appropriately and submit them to MapReduce for processing. A key property of MRShare is that it is independent of the MapReduce implementation. Experiments with our prototype, built on top of Hadoop, demonstrate the overall effectiveness of our approach. MRShare is primarily designed for handling I/O-intensive queries. However, with the development of highlevel languages operating on top of MapReduce, user queries executed in this model become more complex and CPU intensive. Commonly, executed queries can be modeled as evaluating pipelines of CPU-expensive filters over the input stream. Examples of such filters include, but are not limited to, index probes, or certain types of joins. In this article we adapt some of the standard techniques for filter ordering used in relational and stream databases, propose their extensions, and implement them through MRAdaptive Filter, an extension of MRShare for expensive filter ordering tailored to MapReduce, which allows one to handle both single- and batch-query execution modes. We present an experimental evaluation that demonstrates additional benefits of MRAdaptive Filter, when executing CPU-intensive queries in MRShare. © 2014 ACM.",MapReduce; Query processing; Sharing MapReduce jobs; Systems,Cloud computing; Computer systems; Optimization; Query languages; Analytical queries; Experimental evaluation; Large-scale data analysis; Map-reduce; Optimization problems; Overall effectiveness; Processing infrastructures; Scientific researches; Query processing
Fast distributed transactions and strongly consistent replication for OLTP database systems,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901479771&doi=10.1145%2f2556685&partnerID=40&md5=c9162b4ee358868b617d3789d46a2ff9,"As more data management software is designed for deployment in public and private clouds, or on a cluster of commodity servers, new distributed storage systems increasingly achieve high data access throughput via partitioning and replication. In order to achieve high scalability, however, today's systems generally reduce transactional support, disallowing single transactions from spanning multiple partitions. This article describes Calvin, a practical transaction scheduling and data replication layer that uses a deterministic ordering guarantee to significantly reduce the normally prohibitive contention costs associated with distributed transactions. This allows near-linear scalability on a cluster of commodity machines, without eliminating traditional transactional guarantees, introducing a single point of failure, or requiring application developers to reason about data partitioning. By replicating transaction inputs instead of transactional actions, Calvin is able to support multiple consistency levels - including Paxos-based strong consistency across geographically distant replicas - at no cost to transactional throughput. Furthermore, Calvin introduces a set of tools that will allow application developers to gain the full performance benefit of Calvin's server-side transaction scheduling mechanisms without introducing the additional code complexity and inconvenience normally associated with using DBMS stored procedures in place of ad hoc client-side transactions. © 2014 ACM.",Determinism; Distributed database systems; Replication; Transaction processing,Database systems; Digital storage; Distributed database systems; Information management; Multiprocessing systems; Application developers; Data management software; Determinism; Distributed storage system; Distributed transaction; Replication; Transaction processing; Transaction scheduling; Scheduling
Optimality of clustering properties of space-filling curves,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901482384&doi=10.1145%2f2556686&partnerID=40&md5=976af370641eb96f3be015167483c533,"Space-filling curves have been used in the design of data structures for multidimensional data for many decades. A fundamental quality metric of a space-filling curve is its ""clustering number"" with respect to a class of queries, which is the average number of contiguous segments on the space-filling curve that a query region can be partitioned into. We present a characterization of the clustering number of a general class of space-filling curves, as well as the first nontrivial lower bounds on the clustering number for any space-filling curve. Our results answer questions that have been open for more than 15 years. © 2014 ACM.",Clustering; Hilbert curve; Lower bound; Space-filling curves; Z-curve,Information systems; Clustering; Hilbert curve; Lower bounds; Space-filling curve; Z-curves; Database systems
Indexing for summary queries: Theory and practice,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893211040&doi=10.1145%2f2508702&partnerID=40&md5=ed2b34c80a7e904e2cf3c15569eca162,"Database queries can be broadly classified into two categories: reporting queries and aggregation queries. The former retrieves a collection of records from the database that match the query's conditions, while the latter returns an aggregate, such as count, sum, average, or max (min), of a particular attribute of these records. Aggregation queries are especially useful in business intelligence and data analysis applications where users are interested not in the actual records, but some statistics of them. They can also be executed much more efficiently than reporting queries, by embedding properly precomputed aggregates into an index. However, reporting and aggregation queries provide only two extremes for exploring the data. Data analysts often need more insight into the data distribution than what those simple aggregates provide, and yet certainly do not want the sheer volume of data returned by reporting queries. In this article, we design indexing techniques that allow for extracting a statistical summary of all the records in the query. The summaries we support include frequent items, quantiles, and various sketches, all of which are of central importance in massive data analysis. Our indexes require linear space and extract a summary with the optimal or near-optimal query cost. We illustrate the efficiency and usefulness of our designs through extensive experiments and a system demonstration. © 2014 ACM.",Indexing; Summary queries,Aggregates; Optimization; Query processing; Aggregation queries; Data analysts; Data distribution; Database queries; Indexing techniques; Statistical summary; Summary queries; Theory and practice; Indexing (of information)
Mining order-preserving submatrices from probabilistic matrices,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893237032&doi=10.1145%2f2533712&partnerID=40&md5=c33cdeecb04517fc627a63a714afbb2d,"Order-preserving submatrices (OPSMs) capture consensus trends over columns shared by rows in a data matrix. Mining OPSM patterns discovers important and interesting local correlations in many real applications, such as those involving biological data or sensor data. The prevalence of uncertain data in various applications, however, poses new challenges for OPSM mining, since data uncertainty must be incorporated into OPSM modeling and the algorithmic aspects. In this article, we define new probabilisticmatrix representations to model uncertain data with continuous distributions. A novel probabilistic order-preserving submatrix (POPSM) model is formalized in order to capture similar local correlations in probabilistic matrices. The POPSM model adopts a new probabilistic support measure that evaluates the extent to which a row belongs to a POPSM pattern. Due to the intrinsic high computational complexity of the POPSM mining problem, we utilize the anti-monotonic property of the probabilistic support measure and propose an efficient Apriori-based mining framework called PROBAPRI to mine POPSM patterns. The framework consists of two mining methods, UNIAPRI and NORMAPRI, which are developed for mining POPSM patterns, respectively, from two representative types of probabilistic matrices, the UniDist matrix (assuming uniform data distributions) and the NormDist matrix (assuming normal data distributions). We show that the NORMAPRI method is practical enough for mining POPSM patterns from probabilistic matrices that model more general data distributions. We demonstrate the superiority of our approach by two applications. First, we use two biological datasets to illustrate that the POPSM model better captures the characteristics of the expression levels of biologically correlated genes and greatly promotes the discovery of patterns with high biological significance. Our result is significantly better than the counterpart OPSMRM (OPSM with repeated measurement) model which adopts a set-valued matrix representation to capture data uncertainty. Second, we run the experiments on an RFID trace dataset and show that our POPSM model is effective and efficient in capturing the common visiting subroutes among users. © 2014 ACM.",OPSM mining; Order-preserving submatrices; Probabilistic matrices; Probabilistic support,Gene expression; Mining; Probability distributions; Uncertainty analysis; Algorithmic aspects; Biological significance; Continuous distribution; Local correlations; Matrix representation; Order-preserving submatrix; Repeated measurements; Sub-matrices; Matrix algebra
Efficient range searching for categorical and plain data,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893265734&doi=10.1145%2f2543924&partnerID=40&md5=682d4d25cc837e8dd0b261240562b481,"In the orthogonal range-searching problem, we store a set of input points S in a data structure; the answer to a query Q is a piece of information about points in Q ∩ S, for example, the list of all points in Q ∩ S or the number of points in Q. In the colored (or categorical) range-searching problem, the set of input points is partitioned into categories; the answer to a query is a piece of information about categories of points in a query range. In this article, we describe several new results for one- and two-dimensional range-searching problems. We obtain an optimal adaptive data structure for counting the number of objects in a three-sided range and for counting categories of objects in a one-dimensional range. We also obtain new results on color range reporting in two dimensions, approximate color counting in one dimension, and some other related problems. © 2014 ACM.",Colored range reporting; Data structures; Orthogonal range reporting,Data structures; Color ranges; Colored range reporting; New results; One dimension; Orthogonal range reporting; Range searching; Two-dimension; Query processing
Strong simulation: Capturing topology in graph pattern matching,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893318501&doi=10.1145%2f2528937&partnerID=40&md5=1bb7398d3e94562f267c01518047d44c,"Graph pattern matching is finding all matches in a data graph for a given pattern graph and is often defined in terms of subgraph isomorphism, an NP-complete problem. To lower its complexity, various extensions of graph simulation have been considered instead. These extensions allow graph pattern matching to be conducted in cubic time. However, they fall short of capturing the topology of data graphs, that is, graphs may have a structure drastically different from pattern graphs they match, and the matches found are often too large to understand and analyze. To rectify these problems, this article proposes a notion of strong simulation, a revision of graph simulation for graph pattern matching. (1) We identify a set of criteria for preserving the topology of graphs matched. We show that strong simulation preserves the topology of data graphs and finds a bounded number of matches. (2) We show that strong simulation retains the same complexity as earlier extensions of graph simulation by providing a cubic-time algorithm for computing strong simulation. (3) We present the locality property of strong simulation which allows us to develop an effective distributed algorithm to conduct graph pattern matching on distributed graphs. (4)We experimentally verify the effectiveness and efficiency of these algorithms using both real-life and synthetic data. © 2014 ACM.",Data locality; Dual simulation; Graph simulation; Strong simulation; Subgraph isomorphism,Graphic methods; Pattern matching; Set theory; Topology; Data graph; Data locality; Dual simulation; Effectiveness and efficiencies; Graph pattern matching; Graph simulation; Property; Strong simulation; Subgraph isomorphism; Time algorithms; Computational complexity
Classification of annotation semirings over containment of conjunctive queries,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893218337&doi=10.1145%2f2556524&partnerID=40&md5=4c0bb31cf714458ba4ea6829006edb9f,"We study the problem of query containment of conjunctive queries over annotated databases. Annotations are typically attached to tuples and represent metadata, such as probability, multiplicity, comments, or provenance. It is usually assumed that annotations are drawn from a commutative semiring. Such databases pose new challenges in query optimization, since many related fundamental tasks, such as query containment, have to be reconsidered in the presence of propagation of annotations. We axiomatize several classes of semirings for each of which containment of conjunctive queries is equivalent to existence of a particular type of homomorphism. For each of these types, we also specify all semirings for which existence of a corresponding homomorphism is a sufficient (or necessary) condition for the containment. We develop new decision procedures for containment for some semirings which are not in any of these classes. This generalizes and systematizes previous approaches. © 2014 ACM.",Annotation; Provenance; Query optimization,Optimization; Annotated database; Annotation; Conjunctive queries; Decision procedure; Provenance; Query containment; Query optimization; Semi-ring; Equivalence classes
Oblivious bounds on the probability of boolean functions,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893209619&doi=10.1145%2f2532641&partnerID=40&md5=55e96e3eb868d43431da908b7d1b120f,"This article develops upper and lower bounds for the probability of Boolean functions by treating multiple occurrences of variables as independent and assigning them new individual probabilities. We call this approach dissociation and give an exact characterization of optimal oblivious bounds, that is, when the new probabilities are chosen independently of the probabilities of all other variables. Our motivation comes from the weighted model counting problem (or, equivalently, the problem of computing the probability of a Boolean function), which is #P-hard in general. By performing several dissociations, one can transform a Boolean formula whose probability is difficult to compute into one whose probability is easy to compute, and which is guaranteed to provide an upper or lower bound on the probability of the original formula by choosing appropriate probabilities for the dissociated variables. Our new bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space.We also show how our theory allows a standard relational databasemanagement system (DBMS) to both upper and lower bound hard probabilistic queries in guaranteed polynomial time. © 2014 ACM.",Boolean expressions; Oblivious approximations; Probabilistic databases; Relaxation; Weighted model counting,Boolean functions; Dissociation; Polynomial approximation; Boolean expressions; Oblivious approximations; Probabilistic database; Relaxation; Weighted models; Probability
Deletion without rebalancing in multiway search trees,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893210549&doi=10.1145%2f2540068&partnerID=40&md5=3ca51f901ebb32f352b9fbcfe569273e,"Some database systems that use a form of B-tree for the underlying data structure do not do rebalancing on deletion. This means that a bad sequence of deletions can create a very unbalanced tree. Yet such databases perform well in practice. Avoidance of rebalancing on deletion has been justified empirically and by averagecase analysis, but to our knowledge, no worst-case analysis has been done. We do such an analysis. We show that the tree height remains logarithmic in the number of insertions, independent of the number of deletions. Furthermore, the amortized time for an insertion or deletion, excluding the search time, is O(1), and nodes are modified by insertions and deletions with a frequency that is exponentially small in their height. The latter results do not hold for standard B-trees. By adding periodic rebuilding of the tree, we obtain a data structure that is theoretically superior to standard B-trees in many ways. Our results suggest that rebalancing on deletion not only is unnecessary but may be harmful. © 2014 ACM.",Amortized complexity; B-trees; Database access methods; Exponential potential function; I/O model,Data structures; Database systems; Amortized complexity; Average-case analysis; B trees; Database access; Insertions and deletions; Multi-way searches; Potential function; Worst-case analysis; Trees (mathematics)
Pufferfish: A framework for mathematical privacy definitions,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893284736&doi=10.1145%2f2514689&partnerID=40&md5=d82a8569314a00bffb1d3b2a73942c39,"In this article, we introduce a new and general privacy framework called Pufferfish. The Pufferfish framework can be used to create new privacy definitions that are customized to the needs of a given application. The goal of Pufferfish is to allow experts in an application domain, who frequently do not have expertise in privacy, to develop rigorous privacy definitions for their data sharing needs. In addition to this, the Pufferfish framework can also be used to study existing privacy definitions. We illustrate the benefits with several applications of this privacy framework: we use it to analyze differential privacy and formalize a connection to attackers who believe that the data records are independent; we use it to create a privacy definition called hedging privacy, which can be used to rule out attackers whose prior beliefs are inconsistent with the data; we use the framework to define and study the notion of composition in a broader context than before; we show how to apply the framework to protect unbounded continuous attributes and aggregate information; and we show how to use the framework to rigorously account for prior data releases. © 2014 ACM.",Differential privacy; Privacy,Data privacy; Information systems; Continuous attribute; Data release; Data Sharing; Differential privacies; Privacy frameworks; Pufferfish; Database systems
Extending string similarity join to tolerant fuzzy token matching,2014,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893327937&doi=10.1145%2f2535628&partnerID=40&md5=8b38d894d23ae0b77a2bf5837b1170a6,"String similarity join that finds similar string pairs between two string sets is an essential operation in many applications and has attracted significant attention recently in the database community. A significant challenge in similarity join is to implement an effective fuzzy match operation to find all similar string pairs which may not match exactly. In this article, we propose a new similarity function, called fuzzy-tokenmatching- based similarity which extends token-based similarity functions (e.g., jaccard similarity and cosine similarity) by allowing fuzzy match between two tokens. We study the problem of similarity join using this new similarity function and present a signature-based method to address this problem. We propose new signature schemes and develop effective pruning techniques to improve the performance.We also extend our techniques to support weighted tokens. Experimental results show that our method achieves high efficiency and result quality and significantly outperforms state-of-the-art approaches. © 2014 ACM.",Fuzzy token matching-based similarity; Signature scheme; Similarity function; String similarity join; Weighted tokens,Database systems; Information systems; Fuzzy token matching-based similarity; Signature Scheme; Similarity functions; String similarity; Weighted tokens; Authentication
Mergeable summaries,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890342633&doi=10.1145%2f2500128&partnerID=40&md5=e3869a9c40ad4067fd0b6785b0c4ead5,"We study the mergeability of data summaries. Informally speaking, mergeability requires that, given two summaries on two datasets, there is a way to merge the two summaries into a single summary on the two datasets combined together, while preserving the error and size guarantees. This property means that the summaries can be merged in a way akin to other algebraic operators such as sum and max, which is especially useful for computing summaries on massive distributed data. Several data summaries are trivially mergeable by construction, most notably all the sketches that are linear functions of the datasets. But some other fundamental ones, like those for heavy hitters and quantiles, are not (known to be) mergeable. In this article, we demonstrate that these summaries are indeed mergeable or can be made mergeable after appropriatemodifications. Specifically, we show that for e-approximate heavy hitters, there is a deterministic mergeable summary of size O(1/e); for e-approximate quantiles, there is a deterministic summary of size O((1/e) log(en)) that has a restricted form of mergeability, and a randomized one of size O((1/e) log3/2(1/e)) with full mergeability. We also extend our results to geometric summaries such as e-approximations which permit approximate multidimensional range counting queries. While most of the results in this article are theoretical in nature, some of the algorithms are actually very simple and even perform better than the previously best known algorithms, which we demonstrate through experiments in a simulated sensor network. We also achieve two results of independent interest: (1) we provide the best known randomized streaming bound for ε-approximate quantiles that depends only on e, of size O((1/e) log3/2(1/e)), and (2) we demonstrate that the MG and the SpaceSaving summaries for heavy hitters are isomorphic. © 2013.",Data summarization; Heavy hitters; Quantiles,Sensor networks; Best-known algorithms; Data summaries; Data summarizations; Distributed data; Heavy-hitter; Linear functions; Multidimensional range; Quantiles; Algorithms
Consistent thinning of large geographical data for map visualization,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890419730&doi=10.1145%2f2539032.2539034&partnerID=40&md5=7879b50e2d2eb62faf68748cbb3c2fcd,"Large-scalemap visualization systems play an increasingly important role in presenting geographic datasets to end-users. Since these datasets can be extremely large, a map rendering system often needs to select a small fraction of the data to visualize them in a limited space. This article addresses the fundamental challenge of thinning: determining appropriate samples of data to be shown on specific geographical regions and zoom levels. Other than the sheer scale of the data, the thinning problem is challenging because of a number of other reasons: (1) data can consist of complex geographical shapes, (2) rendering of data needs to satisfy certain constraints, such as data being preserved across zoom levels and adjacent regions, and (3) after satisfying the constraints, an optimal solution needs to be chosen based on objectives such as maximality, fairness, and importance of data. This article formally defines and presents a complete solution to the thinning problem. First, we express the problem as an integer programming formulation that efficiently solves thinning for desired objectives. Second, we present more efficient solutions for maximality, based on DFS traversal of a spatial tree. Third, we consider the common special case of point datasets, and present an even more efficient randomized algorithm. Fourth, we show that contiguous regions are tractable for a general version ofmaximality for which arbitrary regions are intractable. Fifth, we examine the structure of our integer programming formulation and show that for point datasets, our program is integral. Finally, we have implemented all techniques from this article in Google Maps [Google 2005] visualizations of fusion tables [Gonzalez et al. 2010], and we describe a set of experiments that demonstrate the trade-offs among the algorithms. © 2013.",Data visualization; Geographical databases; Indexing; Maps; Query processing; Spatial sampling,Algorithms; Constraint satisfaction problems; Geographical regions; Indexing (of information); Integer programming; Maps; Query processing; Visualization; Complete solutions; Geographic datasets; Geographical database; Integer programming formulations; Map visualizations; Randomized Algorithms; Spatial sampling; Visualization system; Data visualization
Static analysis and optimization of semantic web queries,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890759173&doi=10.1145%2f2500130&partnerID=40&md5=1931ba5e1e4ffe17ceecf9dc779addd9,"Static analysis is a fundamental task in query optimization. In this article we study static analysis and optimization techniques for SPARQL, which is the standard language for querying Semantic Web data. Of particular interest for us is the optionality feature in SPARQL. It is crucial in Semantic Web data management, where data sources are inherently incomplete and the user is usually interested in partial answers to queries. This feature is one of themost complicated constructors in SPARQL and also the one that makes this language depart from classical query languages such as relational conjunctive queries. We focus on the class of well-designed SPARQL queries, which has been proposed in the literature as a fragment of the language with good properties regarding query evaluation.We first propose a tree representation for SPARQL queries, called pattern trees, which captures the class of well-designed SPARQL graph patterns. Among other results, we propose several rules that can be used to transform pattern trees into a simple normal form, and study equivalence and containment. We also study the evaluation and enumeration problems for this class of queries. © 2013.",,Information management; Optimization; Query languages; Semantic Web; Trees (mathematics); XML; Conjunctive queries; Enumeration problems; Graph patterns; Optimization techniques; Query optimization; Querying semantics; Tree representation; Web data management; Static analysis
Foreword to invited papers issue,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890321284&doi=10.1145%2f2539032.2539033&partnerID=40&md5=bd2379ac0ae11cfb390483af560a75a1,[No abstract available],,
Learning schema mappings,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890413976&doi=10.1145%2f2539032.2539035&partnerID=40&md5=94a61cf06b288c70f88621f2e5168b3f,"A schema mapping is a high-level specification of the relationship between a source schema and a target schema. Recently, a line of research has emerged that aims at deriving schema mappings automatically or semi-automatically with the help of data examples, that is, pairs consisting of a source instance and a target instance that depict, in some precise sense, the intended behavior of the schema mapping. Several different uses of data examples for deriving, refining, or illustrating a schema mapping have already been proposed and studied. In this article, we use the lens of computational learning theory to systematically investigate the problem of obtaining algorithmically a schema mapping from data examples. Our aim is to leverage the rich body of work on learning theory in order to develop a framework for exploring the power and the limitations of the various algorithmic methods for obtaining schema mappings from data examples. We focus on GAV schema mappings, that is, schema mappings specified by GAV (Global-As-View) constraints. GAV constraints are the most basic and the most widely supported language for specifying schema mappings.We present an efficient algorithm for learning GAV schema mappings using Angluin's model of exact learning with membership and equivalence queries. This is optimal, since we show that neither membership queries nor equivalence queries suffice, unless the source schema consists of unary relations only. We also obtain results concerning the learnability of schema mappings in the context of Valiant's well-known PAC (Probably-Approximately-Correct) learning model, and concerning the learnability of restricted classes of GAV schema mappings. Finally, as a byproduct of our work, we show that there is no efficient algorithm for approximating the shortest GAV schema mapping fitting a given set of examples, unless the source schema consists of unary relations only. © 2013.",,Database systems; Information systems; Algorithmic methods; Computational learning theory; Equivalence queries; High level specification; Learning models; Learning Theory; Membership query; Schema mappings; Algorithms
Validating xml documents in the streaming model with external memory,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890376033&doi=10.1145%2f2504590&partnerID=40&md5=e2b69955efdd8c710c33477061fa527e,"We study the problem of validating XML documents of size N against general DTDs in the context of streaming algorithms. The starting point of this work is a well-known space lower bound. There are XML documents and DTDs for which p-pass streaming algorithms require Ω(N/p) space. We show that when allowing access to external memory, there is a deterministic streaming algorithm that solves this problem with memory space O(log2N), a constant number of auxiliary read/write streams, and O(log N) total number of passes on the XML document and auxiliary streams. An important intermediate step of this algorithm is the computation of the First-Child-Next-Sibling (FCNS) encoding of the initial XML document in a streaming fashion.We study this problem independently, andwe also provide memory-efficient streaming algorithms for decoding an XML document given in its FCNS encoding. Furthermore, validating XML documents encoding binary trees against any DTD in the usual streaming model without external memory can be done with sublinear memory. There is a one-pass algorithm using O(ΩNlog N) space, and a bidirectional two-pass algorithm using O(log2 N) space which perform this task. © 2013.",,Encoding (symbols); XML; External memory; Lower bounds; Memory space; One-pass; Streaming algorithm; Streaming model; Sublinear; Algorithms
The Complexity of regular expressions and property paths in sparql,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890733454&doi=10.1145%2f2494529&partnerID=40&md5=505ae8a1f18cf2df5098209609ba86e6,"TheWorldWideWeb Consortium (W3C) recently introduced property paths in SPARQL 1.1, a query language for RDF data. Property paths allow SPARQL queries to evaluate regular expressions over graph-structured data. However, they differ from standard regular expressions in several notable aspects. For example, they have a limited form of negation, they have numerical occurrence indicators as syntactic sugar, and their semantics on graphs is defined in a nonstandard manner. We formalize the W3C semantics of property paths and investigate various query evaluation problems on graphs. More specifically, let x and y be two nodes in an edge-labeled graph and r be an expression.We study the complexities of: (1) deciding whether there exists a path from x to y that matches r and (2) counting how many paths from x to y match r. Our main results show that, compared to an alternative semantics of regular expressions on graphs, the complexity of (1) and (2) under W3C semantics is significantly higher. Whereas the alternative semantics remains in polynomial time for large fragments of expressions, the W3C semantics makes problems (1) and (2) intractable almost immediately. As a side-result, we prove that themembership problem for regular expressions with numerical occurrence indicators and negation is in polynomial time. © 2013.",Graph databases; Query evaluation; Regular expressions,Pattern matching; Polynomial approximation; Query languages; Semantics; Alternative Semantics; Graph database; Graph-structured datum; Numerical occurrence indicators; Polynomial-time; Query evaluation; Regular expressions; Syntactic sugars; Graph theory
Xlynx-An Fpga-Based xml filter for hybrid xquery processing,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890388378&doi=10.1145%2f2536800&partnerID=40&md5=a37bc73339fb687b142c549e900f37cd,"While offering unique performance and energy-saving advantages, the use of Field-Programmable Gate Arrays (FPGAs) for database acceleration has demanded major concessions from system designers. Either the programmable chips have been used for very basic application tasks (such as implementing a rigid class of selection predicates) or their circuit definition had to be completely recompiled at runtime-a very CPU-intensive and time-consuming effort. This work eliminates the need for such concessions. As part of our XLynx implementation-an FPGAbased XML filter-we present skeleton automata, which is a design principle for data-intensive hardware circuits that offers high expressiveness and quick reconfiguration at the same time. Skeleton automata provide a generic implementation for a class of finite-state automata. They can be parameterized to any particular automaton instance in a matter of microseconds or less (as opposed to minutes or hours for complete recompilation). We showcase skeleton automata based on XML projection [Marian and Siméon 2003], a filtering technique that illustrates the feasibility of our strategy for a real-world and challenging task. By performing XML projection in hardware and filtering data in the network, we report on performance improvements of several factors while remaining nonintrusive to the back-end XML processor (we evaluate XLynx using the Saxon engine).",Fpga; Projection; Skeleton automaton; Xml; Xquery,Field programmable gate arrays (FPGA); Hardware; Musculoskeletal system; XML; Filtering technique; Finite-state automata; Generic implementation; Programmable chips; Projection; Quick reconfiguration; Skeleton automaton; Xquery; Automata theory
Flexible and extensible preference evaluation in database systems,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883569126&doi=10.1145%2f2493268&partnerID=40&md5=aad932328e631a527976bf581e1cac3d,"Personalized database systems give users answers tailored to their personal preferences. While numerous preference evaluation methods for databases have been proposed (e.g., skyline, top-k, k-dominance, k-frequency), the implementation of these methods at the core of a database system is a double-edged sword. Core implementation provides efficient query processing for arbitrary database queries, however, this approach is not practical since each existing (and future) preference method requires implementation within the database engine. To solve this problem, this article introduces FlexPref, a framework for extensible preference evaluation in database systems. FlexPref, implemented in the query processor, aims to support a wide array of preference evaluation methods in a single extensible code base. Integration with FlexPref is simple, involving the registration of only three functions that capture the essence of the preference method. Once integrated, the preference method ""lives"" at the core of the database, enabling the efficient execution of preference queries involving common database operations. This article also provides a query optimization framework for FlexPref, as well as a theoretical framework that defines the properties a preference method must exhibit to be implemented in FlexPref. To demonstrate the extensibility of FlexPref, this article also provides case studies detailing the implementation of seven state-of-the-art preference evaluation methods within FlexPref. We also experimentally study the strengths and weaknesses of an implementation of FlexPref in PostgreSQL over a range of single-table and multitable preference queries. © 2013 ACM.",Extensibility; Preference query processing,Database systems; Database engine; Database queries; Evaluation methods; Extensibility; Preference queries; Preference query processing; Query optimization; Theoretical framework; Query processing
Algebraic incremental maintenance of XML views,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883581350&doi=10.1145%2f2508020.2508021&partnerID=40&md5=3fd45a651c3f089b5df2864214f5594b,"Materialized views can bring important performance benefits when querying XML documents. In the presence of XML document changes, materialized views need to be updated to faithfully reflect the changed document. In this work, we present an algebraic approach for propagating source updates to XML materialized views expressed in a powerful XML tree pattern formalism. Our approach differs from the state-of-the-art in the area in two important ways. First, it relies on set-oriented, algebraic operations, to be contrasted with node-based previous approaches. Second, it exploits state-of-the-art features of XML stores and XML query evaluation engines, notably XML structural identifiers and associated structural join algorithms. We present algorithms for determining how updates should be propagated to views, and highlight the benefits of our approach over existing algorithms through a series of experiments. © 2013 ACM.",XML query processing; XML updates; XML view maintenance,Algebra; Algorithms; Algebraic approaches; Algebraic operations; Incremental maintenance; Materialized view; Performance benefits; XML query processing; XML update; XML views; XML
Almost-linear inclusion for XML regular expression types,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883586493&doi=10.1145%2f2508020.2508022&partnerID=40&md5=01dea01d6e307b3eaaa8968be42afbe1,"Type inclusion is a fundamental operation in every type-checking compiler, but it is quite expensive for XML manipulation languages. A polynomial inclusion checking algorithm for an expressive family of XML type languages is known, but it runs in quadratic time both in the best and in the worst cases. We present here an algorithm that has a linear-time backbone, and resorts to the quadratic approach for some specific parts of the compared types. Our experiments show that the new algorithm is much faster than the quadratic one, and that it typically runs in linear time, hence it can be used as a building block for a practical type-checking compiler. © 2013 ACM.",Language inclusion; Regular expressions; XML,Algorithms; Computer programming languages; Program compilers; XML; Building blockes; Fundamental operations; Linear time; Quadratic time; Regular expressions; Typechecking; XML types; Pattern matching
Asymmetric signature schemes for efficient exact edit similarity query processing,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883578904&doi=10.1145%2f2508020.2508023&partnerID=40&md5=6ab52cd97937347cb9845566d2de24d5,"Given a query string Q, an edit similarity search finds all strings in a database whose edit distance with Qis no more than a given threshold. Most existing methods answering edit similarity queries employ schemes to generate string subsequences as signatures and generate candidates by set overlap queries on query and data signatures. In this article, we show that for any such signature scheme, the lower bound of the minimum number of signatures is + 1, which is lower than what is achieved by existing methods. We then propose several asymmetric signature schemes, that is, extracting different numbers of signatures for the data and query strings, which achieve this lower bound. A basic asymmetric scheme is first established on the basis of matching q-chunks and q-grams between two strings. Two efficient query processing algorithms (IndexGram and IndexChunk) are developed on top of this scheme. We also propose novel candidate pruning methods to further improve the efficiency. We then generalize the basic scheme by incorporating novel ideas of floating q-chunks, optimal selection of q-chunks, and reducing the number of signatures using global ordering. As a result, the Super and Turbo families of schemes are developed together with their corresponding query processing algorithms. We have conducted a comprehensive experimental study using the six asymmetric algorithms and nine previous state-of-the-art algorithms. The experiment results clearly showcase the efficiency of our methods and demonstrate space and time characteristics of our proposed algorithms. © 2013 ACM.",Asymmetric scheme; Edit distance; Lower bound; Near-duplicate detection; Q-chunk; Q-gram; Query optimization; Query processing; Signature,Algorithms; Authentication; Optimization; Query languages; Asymmetric scheme; Edit distance; Lower bounds; Near-duplicate detection; Q-chunk; Q-gram; Query optimization; Signature; Query processing
Collaborative data sharing via update exchange and provenance,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883596616&doi=10.1145%2f2500127&partnerID=40&md5=29f9b35906286c37e1ee95d3523a1e0a,"Recent work [Ives et al. 2005] proposed a new class of systems for supporting data sharing among scientific and other collaborations: this new collaborative data sharing system connects heterogeneous logical peers using a network of schema mappings. Each peer has a locally controlled and edited database instance, but wants to incorporate related data from other peers as well. To achieve this, every peer's data and updates propagate along themappings to the other peers. However, this operation, termed update exchange, is filtered by trust conditions expressing what data and sources a peer judges to be authoritative which may cause a peer to reject another's updates. In order to support such filtering, updates carry provenance information. This article develops methods for realizing such systems: we build upon techniques from data integration, data exchange, incremental view maintenance, and view update to propagate updates along mappings, both to derived and optionally to source instances. We incorporate a novel model for tracking data provenance, such that curators may filter updates based on trust conditions over this provenance. We implement our techniques in a layer above an off-the-shelf RDBMS, and we experimentally demonstrate the viability of these techniques in the ORCHESTRA prototype system. © 2013 ACM.",Data exchange; Data provenance; Schema mappings; Systems; Update exchange,Computer systems; Electronic data interchange; Data provenance; Data Sharing; Data sharing systems; Incremental view maintenance; Prototype system; Schema mappings; Tracking data; View update; Information filtering
Incremental graph pattern matching,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883579815&doi=10.1145%2f2489791&partnerID=40&md5=288bb91c4d5b7b571b7c3207e34a9728,"Graph pattern matching is commonly used in a variety of emerging applications such as social network analysis. These applications highlight the need for studying the following two issues. First, graph pattern matching is traditionally defined in terms of subgraph isomorphism or graph simulation. These notions, however, often impose too strong a topological constraint on graphs to identify meaningful matches. Second, in practice a graph is typically large, and is frequently updated with small changes. It is often prohibitively expensive to recompute matches starting from scratch via batch algorithms when the graph is updated. This article studies these two issues. (1) We propose to define graph pattern matching based on a notion of bounded simulation, which extends graph simulation by specifying the connectivity of nodes in a graph within a predefined number of hops. We show that bounded simulation is able to find sensible matches that the traditional matching notions fail to catch. We also show that matching via bounded simulation is in cubic time, by giving such an algorithm. (2) We provide an account of results on incremental graph pattern matching, for matching defined with graph simulation, bounded simulation, and subgraph isomorphism.We show that the incremental matching problem is unbounded, that is, its cost is not determined alone by the size of the changes in the input and output, for all these matching notions. Nonetheless, when matching is defined in terms of simulation or bounded simulation, incremental matching is semibounded, that is, its worst-time complexity is bounded by a polynomial in the size of the changes in the input, output, and auxiliary information that is necessarily maintained to reuse previous computation, and the size of graph patterns.We also develop incremental matching algorithms for graph simulation and bounded simulation, by minimizing unnecessary recomputation. In contrast, matching based on subgraph isomorphism is neither bounded nor semibounded. (3) We experimentally verify the effectiveness and efficiency of these algorithms, and show that: (a) the revised notion of graph pattern matching allows us to identify communities commonly found in real-life networks, and (b) the incremental algorithms substantially outperform their batch counterparts in response to small changes. These suggest a promising framework for real-life graph pattern matching. © 2013 ACM.",Graph pattern matching; Graph simulation; Incremental pattern matching; Subgraph isomorphism,Algorithms; Pattern matching; Set theory; Social networking (online); Auxiliary information; Effectiveness and efficiencies; Emerging applications; Graph pattern matching; Graph simulation; Incremental graph pattern matching; Subgraph isomorphism; Topological constraints; Topology
"Revisiting ""forward Node-Selecting Queries over Trees""",2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880357568&doi=10.1145%2f2487259.2487265&partnerID=40&md5=84cc8cc436bf987d873ecf67a9fcd82c,"In ""Forward Node-Selecting Queries over Trees,"" Olteanu [2007] gives three rewriting systems for eliminating reverse XPath axis steps from node-selecting queries over trees, together with arguments for their correctness and termination for a large class of input graphs, including cyclic ones. These proofs are valid for tree or acyclic formulas, but two of the rewrite systems (TRS2 and TRS3) do not terminate on cyclic graphs; that is, there are infinite rewrite sequences that never yield a normal form. We investigate the reasons why the termination arguments do not work for general cyclic formulas, and develop alternative algorithms that can be used instead. We prove that TRS2 is weakly normalizing, while TRS3 is not weakly normalizing, but it can be extended to a weakly normalizing system TRS 3. The algorithms and proof techniques illustrate unforeseen subtleties in the handling of cyclic queries. © ACM 2013.",Rewriting; XML; XPath,Forestry; Mathematics; Trees; Forestry; XML; Alternative algorithms; Cyclic graph; Input graphs; Rewrite systems; Rewriting; Rewriting systems; Termination arguments; XPath; Trees (mathematics)
Top-k diversity queries over bounded regions,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880367920&doi=10.1145%2f2487259.2487262&partnerID=40&md5=535dfa0c0b996c56bfb270d24658f9ab,"Top-k diversity queries over objects embedded in a low-dimensional vector space aim to retrieve the best k objects that are both relevant to given user's criteria and well distributed over a designated region. An interesting case is provided by spatial Web objects, which are produced in great quantity by location-based services that let users attach content to places and are found also in domains like trip planning, news analysis, and real estate. In this article we present a technique for addressing such queries that, unlike existing methods for diversified top-k queries, does not require accessing and scanning all relevant objects in order to find the best k results. Our Space Partitioning and Probing (SPP) algorithm works by progressively exploring the vector space, while keeping track of the already seen objects and of their relevance and position. The goal is to provide a good quality result set in terms of both relevance and diversity. We assess quality by using as a baseline the result set computed by MMR, one of the most popular diversification algorithms, while minimizing the number of accessed objects. In order to do so, SPP exploits score-based and distance-based access methods, which are available, for instance, in most geo-referencedWeb data sources. Experiments with both synthetic and real data show that SPP produces results that are relevant and spatially well distributed, while significantly reducing the number of accessed objects and incurring a very low computational overhead. © ACM 2013.",Aggregation; Diversification; Ranking; Scoring; Top-k,Agglomeration; Location based services; Vector spaces; Computational overheads; Diversification; Minimizing the number of; Ranking; Scoring; Space partitioning; Synthetic and real data; Top-k; Algorithms
Generalizing database forensics,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880389355&doi=10.1145%2f2487259.2487264&partnerID=40&md5=9c2e01b5d3102b298b2547384a90c7f9,"In this article we present refinements on previously proposed approaches to forensic analysis of database tampering.We significantly generalize the basic structure of these algorithms to admit new characterizations of the ""where"" axis of the corruption diagram. Specifically, we introduce page-based partitioning as well as attribute-based partitioning along with their associated corruption diagrams. We compare the structure of all the forensic analysis algorithms and discuss the various design choices available with respect to forensic analysis. We characterize the forensic cost of the newly introduced algorithms, compare their forensic cost, and give our recommendations. We then introduce a comprehensive taxonomy of the types of possible corruption events, along with an associated forensic analysis protocol that consolidates all extant forensic algorithms and the corresponding type(s) of corruption events they detect. The result is a generalization of these algorithms and an overarching characterization of the process of database forensic analysis, thus providing a context within the overall operation of a DBMS for all existing forensic analysis algorithms. © ACM 2013.",A3D Algorithm; Attribute-based partitioning; Compliant records; Corruption event taxonomy; Forensic analysis algorithm; Forensic analysis protocol; Forensic cost; Monochromatic Algorithm; Page-based partitioning,Costs; Crime; Database systems; Taxonomies; Attribute-based; Basic structure; Compliant records; Forensic analysis; Page-based partitioning; Algorithms
A partition-based method for string similarity joins with edit-distance constraints,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880356257&doi=10.1145%2f2487259.2487261&partnerID=40&md5=1345fde79541de3283c6c56788fd1330,"As an essential operation in data cleaning, the similarity join has attracted considerable attention from the database community. In this article, we study string similarity joins with edit-distance constraints, which find similar string pairs from two large sets of strings whose edit distance is within a given threshold. Existing algorithms are efficient either for short strings or for long strings, and there is no algorithm that can efficiently and adaptively support both short strings and long strings. To address this problem, we propose a new filter, called the segment filter. We partition a string into a set of segments and use the segments as a filter to find similar string pairs. We first create inverted indices for the segments. Then for each string, we select some of its substrings, identify the selected substrings from the inverted indices, and take strings on the inverted lists of the found substrings as candidates of this string. Finally, we verify the candidates to generate the final answer. We devise efficient techniques to select substrings and prove that our method can minimize the number of selected substrings. We develop novel pruning techniques to efficiently verify the candidates. We also extend our techniques to support normalized edit distance. Experimental results show that our algorithms are efficient for both short strings and long strings, and outperform state-of-the-art methods on real-world datasets. © ACM 2013.",Edit distance; Segment filter; String similarity join,Database systems; Information systems; Database community; Edit distance; Normalized edit distance; Pruning techniques; Real-world datasets; Segment filter; State-of-the-art methods; String similarity; Algorithms
Analysis and optimization for boolean expression indexing,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880385098&doi=10.1145%2f2487259.2487260&partnerID=40&md5=f6e29169d815d58e3fae9f935e3c7947,"BE-Tree is a novel dynamic data structure designed to efficiently index Boolean expressions over a highdimensional discrete space. BE-Tree copes with both high-dimensionality and expressiveness of Boolean expressions by introducing an effective two-phase space-cutting technique that specifically utilizes the discrete and finite domain properties of the space. Furthermore, BE-Tree employs self-adjustment policies to dynamically adapt the tree as the workload changes. Moreover, in BE-Tree, we develop two novel cache-conscious predicate evaluation techniques, namely, lazy and bitmap evaluations, that also exploit the underlying discrete and finite space to substantially reduce BE-Tree's matching time by up to 75%. BE-Tree is a general index structure for matching Boolean expression which has a wide range of applications including (complex) event processing, publish/subscribe matching, emerging applications in cospaces, profile matching for targeted web advertising, and approximate string matching. Finally, the superiority of BE-Tree is proven through a comprehensive evaluation with state-of-the-art index structures designed for matching Boolean expressions. © ACM 2013.",Boolean expressions; Complex event processing; Data structure; Publish/subscribe,Data; Forestry; Publishing; Structures; Data structures; Pattern matching; Phase space methods; Approximate string matching; Boolean expression indexing; Boolean expressions; Complex event processing; Comprehensive evaluation; Dynamic data structure; Emerging applications; Publish/subscribe; Forestry
Schema matching and embedded value mapping for databases with opaque column names and mixed continuous and discrete-valued data fields,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878330002&doi=10.1145%2f2445583.2445585&partnerID=40&md5=363a62ebd65e05dbe7aea33429489c0b,"Schema matching and value mapping across two information sources, such as databases, are critical information aggregation tasks. Before data can be integrated from multiple tables, the columns and values within the tables must be matched. The complexities of both these problems grow quickly with the number of attributes to be matched and due to multiple semantics of data values. Traditional research has mostly tackled schema matching and value mapping independently, and for categorical (discrete-valued) attributes. We propose novel methods that leverage value mappings to enhance schema matching in the presence of opaque column names for schemas consisting of both continuous and discrete-valued attributes. An additional source of complexity is that a discrete-valued attribute in one schema could in fact be a quantized, encoded version of a continuous-valued attribute in the other schema. In our approach, which can tackle both ""onto"" and bijective schema matching, the fitness objective for matching a pair of attributes from two schemas exploits the statistical distribution over values within the two attributes. Suitable fitness objectives are based on Euclidean-distance and the data log-likelihood, both of which are applied in our experimental study. A heuristic local descent optimization strategy that uses two-opt switching to optimize attribute matches, while simultaneously embedding value mappings, is applied for our matching methods. Our experiments show that the proposed techniques matched mixed continuous and discrete-valued attribute schemas with high accuracy and, thus, should be a useful addition to a framework of (semi) automated tools for data alignment. © 2013 ACM.",Database alignment; Embedded value mapping; Schema matching,Alignment; Database systems; Heuristic methods; Optimization; Semantics; Continuous-valued attribute; Critical information; Experimental studies; Information sources; Matching methods; Optimization strategy; Schema matching; Statistical distribution; Mapping
High-level change detection in RDF(S) KBs,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878319863&doi=10.1145%2f2445583.2445584&partnerID=40&md5=1d6f78109d273b8d6081b46646aa9750,"With the increasing use of Web 2.0 to create, disseminate, and consume large volumes of data, more and more information is published and becomes available for potential data consumers, that is, applications/services, individual users and communities, outside their production site. The most representative example of this trend is Linked Open Data (LOD), a set of interlinked data and knowledge bases. The main challenge in this context is data governance within loosely coordinated organizations that are publishing added-value interlinked data on the Web, bringing together issues related to data management and data quality, in order to support the full lifecycle of data production, consumption, and management. In this article, we are interested in curation issues for RDF(S) data, which is the default data model for LOD. In particular, we are addressing change management for RDF(S) data maintained by large communities (scientists, librarians, etc.) which act as curators to ensure high quality of data. Such curated Knowledge Bases (KBs) are constantly evolving for various reasons, such as the inclusion of new experimental evidence or observations, or the correction of erroneous conceptualizations. Managing such changes poses several research problems, including the problem of detecting the changes (delta) between versions of the same KB developed and maintained by different groups of curators, a crucial task for assisting them in understanding the involved changes. This becomes all the more important as curated KBs are interconnected (through copying or referencing) and thus changes need to be propagated from one KB to another either within or across communities. This article addresses this problem by proposing a change language which allows the formulation of concise and intuitive deltas. The language is expressive enough to describe unambiguously any possible change encountered in curated KBs expressed in RDF(S), and can be efficiently and deterministically detected in an automated way. Moreover, we devise a change detection algorithm which is sound and complete with respect to the aforementioned language, and study appropriate semantics for executing the deltas expressed in our language in order to move backwards and forwards in a multiversion repository, using only the corresponding deltas. Finally, we evaluate through experiments the effectiveness and efficiency of our algorithms using real ontologies from the cultural, bioinformatics, and entertainment domains. © 2013 ACM.",Change detection; Change management; Delta; Deterministic change detection; Diff; High-level changes; RDF(S),Bioinformatics; Information management; Semantics; Change detection; Change management; Delta; Diff; High-level changes; RDF(S); Signal detection
"Moving spatial keyword queries: Formulation, methods, and analysis",2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878292633&doi=10.1145%2f2445583.2445590&partnerID=40&md5=de013323311e7884c85d8da044964de4,"Web users and content are increasingly being geo-positioned. This development gives prominence to spatial keyword queries, which involve both the locations and textual descriptions of content. We study the efficient processing of continuously moving top-k spatial keyword (MkSK) queries over spatial text data. State-of-the-art solutions for moving queries employ safe zones that guarantee the validity of reported results as long as the user remains within the safe zone associated with a result. However, existing safe-zone methods focus solely on spatial locations and ignore text relevancy. We propose two algorithms for computing safe zones that guarantee correct results at any time and that aim to optimize the server-side computation as well as the communication between the server and the client. We exploit tight and conservative approximations of safe zones and aggressive computational space pruning. We present techniques that aim to compute the next safe zone efficiently, and we present two types of conservative safe zones that aim to reduce the communication cost. Empirical studies with real data suggest that the proposals are efficient. To understand the effectiveness of the proposed safe zones, we study analytically the expected area of a safe zone, which indicates on average for how long a safe zone remains valid, and we study the expected number of influence objects needed to define a safe zone, which gives an estimate of the average communication cost. The analytical modeling is validated through empirical studies. © 2013 ACM.",Moving query; Multiplicatively weighted Voronoi diagram; Safe zone; Spatial keyword query; Voronoi diagram,Communication; Computational geometry; Graphic methods; Query processing; Keyword queries; Moving query; Safe zones; Voronoi diagrams; Weighted voronoi diagram; Cost benefit analysis
A graph-theoretic approach to map conceptual designs to XML schemas,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878290078&doi=10.1145%2f2445583.2445589&partnerID=40&md5=3af7ffa6fc426fe4ca4417d05d9824f4,"We propose a mapping from a database conceptual design to a schema for XML that produces highly connected and nested XML structures. We first introduce two alternative definitions of the mapping, one modeling entities as global XML elements and expressing relationships among them in terms of keys and key references (flat design), the other one encoding relationships by properly including the elements for some entities into the elements for other entities (nest design). Then we provide a benchmark evaluation of the two solutions showing that the nest approach, compared to the flat one, leads to improvements in both query and validation performances. This motivates us to systematically investigate the best way to nest XML structures. We identify two different nesting solutions: a maximum depth nesting, that keeps low the number of costly join operations that are necessary to reconstruct information at query time using the mapped schema, and a maximum density nesting, that minimizes the number of schema constraints used in the mapping of the conceptual schema, thus reducing the validation overhead. On the one hand, the problem of finding a maximum depth nesting turns out to be NP-complete and, moreover, it admits no constant ratio approximation algorithm. On the other hand, we devise a graph-theoretic algorithm, NiduX, that solves the maximum density problem in linear time. Interestingly, NiduX finds the optimal solution for the harder maximum depth problem whenever the conceptual design graph is either acyclic or complete. In randomly generated intermediate cases of the graph topology, we experimentally show that NiduX finds a good approximation of the optimal solution. © 2013 ACM.",Computational complexity; Conceptual modeling; Entity-relationship model; Graph theory; Logical design; XML schema,Approximation algorithms; Computational complexity; Conceptual design; Data mining; Graph theory; Logic design; Mapping; Optimal systems; Query processing; XML; Benchmark evaluation; Conceptual modeling; Conceptual schemas; Database conceptual design; Entity-relationship model; Graph theoretic approach; Logical design; XML schemas; Structural design
Optimizing XML querying using type-based document projection,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878308648&doi=10.1145%2f2445583.2445587&partnerID=40&md5=af2f2b11bd470830f7b6ff3cfa44c7e6,"XML data projection (or pruning) is a natural optimization for main memory query engines: given a query Q over a document D, the subtrees of D that are not necessary to evaluate Q are pruned, thus producing a smaller document D′, the query Q is then executed on D′, hence avoiding to allocate and process nodes that will never be reached by Q. In this article, we propose a new approach, based on types, that greatly improves current solutions. Besides providing comparable or greater precision and far lesser pruning overhead, our solution - unlike current approaches - takes into account backward axes, predicates, and can be applied to multiple queries rather than just to single ones. A side contribution is a new type system for XPath able to handle backward axes. The soundness of our approach is formally proved. Furthermore, we prove that the approach is also complete (i.e., yields the best possible type-driven pruning) for a relevant class of queries and Schemas. We further validate our approach using the XMark and XPathMark benchmarks and show that pruning not only improves the main memory query engine's performances (as expected) but also those of state of the art native XML databases. © 2013 ACM.",Document pruning; Query optimization; Type Projection; Type systems; XML,Digital storage; Engines; Optimization; XML; Document pruning; Multiple queries; Native xml database; New approaches; Query optimization; State of the art; Type Projection; Type systems; Search engines
"ElasTraS: An elastic, scalable, and self-managing transactional database for the cloud",2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878271760&doi=10.1145%2f2445583.2445588&partnerID=40&md5=6fbed8a768e41bb9e2fcaa36cd38d220,"A database management system (DBMS) serving a cloud platform must handle large numbers of application databases (or tenants) that are characterized by diverse schemas, varying footprints, and unpredictable load patterns. Scaling out using clusters of commodity servers and sharing resources among tenants (i.e., multi-tenancy) are important features of such systems. Moreover, when deployed on a pay-per-use infrastructure, minimizing the system's operating cost while ensuring good performance is also an important goal. Traditional DBMSs were not designed for such scenarios and hence do not possess the mentioned features critical for DBMSs in the cloud. We present ElasTraS, which combines three design principles to build an elastically-scalable multitenant DBMS for transaction processing workloads. These design principles are gleaned from a careful analysis of the years of research in building scalable key-value stores and decades of research in high performance transaction processing systems. ElasTraS scales to thousands of tenants, effectively consolidates tenants with small footprints while scaling-out large tenants across multiple servers in a cluster. ElasTraS also supports low-latency multistep ACID transactions, is fault-tolerant, self-managing, and highly available to support mission critical applications. ElasTraS leverages Albatross, a low overhead on-demand live database migration technique, for elastic load balancing by adding more servers during high load and consolidating to fewer servers during usage troughs. This elastic scaling minimizes the operating cost and ensures good performance even in the presence of unpredictable changes to the workload. We elucidate the design principles, explain the architecture, describe a prototype implementation, present the detailed design and implementation of Albatross, and experimentally evaluate the implementation using a variety of transaction processing workloads. On a cluster of 20 commodity servers, our prototype serves thousands of tenants and serves more than 1 billion transactions per day while migrating tenant databases with minimal overhead to allow lightweight elastic scaling. Using a cluster of 30 commodity servers, ElasTraS can scale-out a terabyte TPC-C database serving an aggregate throughput of approximately one quarter of a million TPC-C transactions per minute. © 2013 ACM.",ACID; Cloud computing; Elastic data management; Fault-tolerance; Scalability; Transactions,Acids; Cloud computing; Fault tolerance; Information management; Operating costs; Scalability; Aggregate throughput; Database migrations; Prototype implementations; Transaction processing; Transaction processing systems; Transactional database; Transactions; Unpredictable changes; Database systems
Observing SQL queries in their natural habitat,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878327497&doi=10.1145%2f2445583.2445586&partnerID=40&md5=b3a0dde653f09f2875c54926d0cd5943,"We describe HABITAT, a declarative observational debugger for SQL. HABITAT facilitates true language-level (not: plan-level) debugging of, probably flawed, SQL queries that yield unexpected results. Users mark SQL subexpressions of arbitrary size and then observe whether these evaluate as expected. HABITAT understands query nesting and free row variables in correlated subqueries, and generally aims to not constrain users while suspect subexpressions are marked for observation. From the marked SQL text, HABITAT's algebraic compiler derives a new query whose result represents the values of the desired observations. These observations are generated by the target SQL database host itself and are derived from the original data: HABITAT does not require prior data extraction or extra debugging middleware. Experiments with TPC-H database instances indicate that observations impose a runtime overhead sufficiently low to allow for interactive debugging sessions. © 2013 ACM.",Declarative debugging; Query languages; Relational databases; SQL,Middleware; Query languages; Query processing; Data extraction; Declarative debugging; Interactive debugging; Natural habitat; Relational Database; Runtime overheads; SQL; Sub-expressions; Ecosystems
High-Performance Complex Event Processing Over Hierarchical Data,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890901482&doi=10.1145%2f2536779&partnerID=40&md5=6c5713c706e489eaaa69cbb9a26051ef,"While Complex Event Processing (CEP) constitutes a considerable portion of the so-called Big Data analytics, current CEP systems can only process data having a simple structure, and are otherwise limited in their ability to efficiently support complex continuous queries on structured or semistructured information. However, XML-like streams represent a very popular form of data exchange, comprising large portions of social network and RSS feeds, financial feeds, configuration files, and similar applications requiring advanced CEP queries. In this article, we present the XSeq language and system that support CEP on XML streams, via an extension of XPath that is both powerful and amenable to an efficient implementation. Specifically, the XSeq language extends XPath with natural operators to express sequential and Kleene-* patterns over XML streams, while remaining highly amenable to efficient execution. In fact, XSeq is designed to take full advantage of the recently proposed Visibly Pushdown Automata (VPA), where higher expressive power can be achieved without compromising the computationally attractive properties of finite state automata. Besides the efficiency and expressivity benefits, the choice of VPA as the underlying model also enables XSeq to go beyond XML streams and be easily applicable to any data with both sequential and hierarchical structures, including JSON messages, RNA sequences, and software traces. Therefore, we illustrate the XSeq's power for CEP applications through examples from different domains and provide formal results on its expressiveness and complexity. Finally, we present several optimization techniques for XSeq queries. Our extensive experiments indicate that XSeq brings outstanding performance to CEP applications: two orders of magnitude improvement is obtained over the same queries executed in general-purpose XML engines. © 2013, ACM. All rights reserved.",Algorithms; big data analytics; Complex event processing; Design; JSON; Performance; pushdown automata; visibly; XML,Automata theory; Electronic data interchange; Complex event processing; Complex event processing (CEP); Configuration files; Efficient implementation; Hierarchical structures; Optimization techniques; Semi-structured information; Visibly pushdown automaton; XML
RFID-data compression for supporting aggregate queries,2013,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884633504&doi=10.1145%2f2487259.2487263&partnerID=40&md5=64b38eda6ab00f096b395e01ee59b128,"RFID-based systems for object tracking and supply chain management have been emerging since the RFID technology proved effective in monitoring movements of objects. The monitoring activity typically results in huge numbers of readings, thus making the problem of efficiently retrieving aggregate information from the collected data a challenging issue. In fact, tackling this problem is of crucial importance, as fast answers to aggregate queries are often mandatory to support the decision making process. In this regard, a compression technique for RFID data is proposed, and used as the core of a system supporting the efficient estimation of aggregate queries. Specifically, this technique aims at constructing a lossy synopsis of the data over which aggregate queries can be estimated, without accessing the original data. Owing to the lossy nature of the compression, query estimates are approximate, and are returned along with intervals that are guaranteed to contain the exact query answers. The effectiveness of the proposed approach has been experimentally validated, showing a remarkable trade-off between the efficiency and the accuracy of the query estimation. © 2013.",Data compression; RFID data,Decision making; Economic and social effects; Object tracking; Query processing; Radio frequency identification (RFID); Supply chain management; Aggregate queries; Compression techniques; Decision making process; Efficient estimation; Lossy nature; Monitoring activities; Query estimates; RFID Technology; Data compression
Foreword,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878514324&doi=10.1145%2f2389241.2389242&partnerID=40&md5=b14dfba89bf41ea4c42421c0ae9c0ae9,[No abstract available],,
Determining the currency of data,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878497828&doi=10.1145%2f2389241.2389244&partnerID=40&md5=9bd5459923e99db95846659d10043a99,"Data in real-life databases become obsolete rapidly. One often finds that multiple values of the same entity reside in a database. While all of these values were once correct, most of them may have become stale and inaccurate. Worse still, the values often do not carry reliable timestamps. With this comes the need for studying data currency, to identify the current value of an entity in a database and to answer queries with the current values, in the absence of reliable timestamps. This article investigates the currency of data. (1) We propose a model that specifies partial currency orders in terms of simple constraints. The model also allows us to express what values are copied from other data sources, bearing currency orders in those sources, in terms of copy functions defined on correlated attributes. (2) We study fundamental problems for data currency, to determine whether a specification is consistent, whether a value is more current than another, and whether a query answer is certain no matter how partial currency orders are completed. (3) Moreover, we identify several problems associated with copy functions, to decide whether a copy function imports sufficient current data to answer a query, whether a copy function can be extended to import necessary current data for a query while respecting the constraints, and whether it suffices to copy data of a bounded size. (4) We establish upper and lower bounds of these problems, all matching, for combined complexity and data complexity, and for a variety of query languages. We also identify special cases that warrant lower complexity. © 2012 ACM.",Currency; Data quality,Pattern matching; Query languages; Combined complexity; Currency; Currency orders; Data complexity; Data quality; Lower complexity; Real-life database; Upper and lower bounds; Query processing
SCALLA: A platform for scalable one-pass analytics using MapReduce,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878511551&doi=10.1145%2f2389241.2389246&partnerID=40&md5=6c02f5db82520f618af655ef5f913832,"Today's one-pass analytics applications tend to be data-intensive in nature and require the ability to process high volumes of data efficiently. MapReduce is a popular programming model for processing large datasets using a cluster of machines. However, the traditional MapReduce model is not well-suited for one-pass analytics, since it is geared towards batch processing and requires the dataset to be fully loaded into the cluster before running analytical queries. This article examines, from a systems standpoint, what architectural design changes are necessary to bring the benefits of the MapReduce model to incremental one-pass analytics. Our empirical and theoretical analyses of Hadoop-based MapReduce systems show that the widely used sort-merge implementation for partitioning and parallel processing poses a fundamental barrier to incremental one-pass analytics, despite various optimizations. To address these limitations, we propose a new data analysis platform that employs hash techniques to enable fast in-memory processing, and a new frequent key based technique to extend such processing to workloads that require a large key-state space. Evaluation of our Hadoop-based prototype using real-world workloads shows that our new platform significantly improves the progress of map tasks, allows the reduce progress to keep up with the map progress, with up to 3 orders of magnitude reduction of internal data spills, and enables results to be returned continuously during the job. © 2012 ACM.",Incremental computation; One-pass analytics; Parallel processing,Architectural design; Analytical queries; Fundamental barriers; Incremental computation; MapReduce models; One-pass; Orders of magnitude; Parallel processing; Programming models; Batch data processing
Expressive languages for path queries over graph-structured data,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878479219&doi=10.1145%2f2389241.2389250&partnerID=40&md5=1dd65d62f1062c01f7dd575018d0ba49,"For many problems arising in the setting of graph querying (such as finding semantic associations in RDF graphs, exact and approximate pattern matching, sequence alignment, etc.), the power of standard languages such as the widely studied conjunctive regular path queries (CRPQs) is insufficient in at least two ways. First, they cannot output paths and second, more crucially, they cannot express relationships among paths. We thus propose a class of extended CRPQs, called ECRPQs, which add regular relations on tuples of paths, and allow path variables in the heads of queries. We provide several examples of their usefulness in querying graph structured data, and study their properties. We analyze query evaluation and representation of tuples of paths in the output by means of automata. We present a detailed analysis of data and combined complexity of queries, and consider restrictions that lower the complexity of ECRPQs to that of relational conjunctive queries. We study the containment problem, and look at further extensions with firstorder features, and with nonregular relations that add arithmetic constraints on the lengths of paths and numbers of occurrences of labels. © 2012 ACM.",Conjunctive queries; Graph databases; Regular path Queries; Regular relations,Automata theory; Pattern matching; Semantic Web; Semantics; Approximate pattern matching; Arithmetic constraints; Conjunctive queries; Graph database; Graph structured data; Graph-structured datum; Regular path queries; Regular relations; Query languages
Partial evaluation for distributed xpath query processing and beyond,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876802200&doi=10.1145%2f2389241.2389251&partnerID=40&md5=6a536ae3a7e7ac779be1c3a57cf5d6d5,"This article proposes algorithms for evaluating XPath queries over an XML tree that is partitioned horizontally and vertically, and is distributed across a number of sites. The key idea is based on partial evaluation: it is to send the whole query to each site that partially evaluates the query, in parallel, and sends the results as compact (Boolean) functions to a coordinator that combines these to obtain the result. This approach possesses the following performance guarantees. First, each site is visited at most twice for data-selecting XPath queries, and only once for Boolean XPath queries. Second, the network traffic is determined by the answer to the query, rather than the size of the tree. Third, the total computation is comparable to that of centralized algorithms on the tree stored in a single site, regardless of how the tree is fragmented and distributed. We also present a MapReduce algorithm for evaluating Boolean XPath queries, based on partial evaluation. In addition, we provide algorithms to evaluate XPath queries on very large XML trees, in a centralized setting. We show both analytically and empirically that our techniques are scalable with large trees and complex XPath queries. These results, we believe, illustrate the usefulness and potential of partial evaluation in distributed systems as well as centralized XML stores for evaluating XPath queries and beyond. © 2012 ACM.",Distributed XML documents; Parallel query processing; XPath queries,Algorithms; Forestry; Languages; Algorithms; Boolean functions; Forestry; Query processing; XML; Centralized algorithms; Distributed systems; Distributed XML documents; Network traffic; Parallel query processing; Partial evaluation; Performance guarantees; Xpath queries; Query languages
WHAM: A high-throughput sequence alignment method,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878503914&doi=10.1145%2f2389241.2389247&partnerID=40&md5=f12478925c66e12492c35b6c937bfcef,"Over the last decade, the cost of producing genomic sequences has dropped dramatically due to the current so-called next-generation sequencing methods. However, these next-generation sequencing methods are critically dependent on fast and sophisticated data processing methods for aligning a set of query sequences to a reference genome using rich string matching models. The focus of this work is on the design, development and evaluation of a data processing system for this crucial ""short read alignment"" problem. Our system, called WHAM, employs hash-based indexing methods and bitwise operations for sequence alignments. It allows rich match models and it is significantly faster than the existing state-of-the-art methods. In addition, its relative speedup over the existing method is poised to increase in the future in which read sequence lengths will increase. © 2012 ACM.",Approximate string matching; Bit-parrallism; Sequence alignment,Data processing; Genes; Approximate string matching; Bit-parrallism; Data processing methods; Data processing systems; Next-generation sequencing; Sequence Alignment Methods; Sequence alignments; State-of-the-art methods; Alignment
Worst-case I/O-efficient skyline algorithms,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878479445&doi=10.1145%2f2389241.2389245&partnerID=40&md5=84576f3f84de284e13721d0cbac3f09c,"We consider the skyline problem (aka the maxima problem), which has been extensively studied in the database community. The input is a set P of d-dimensional points. A point dominates another if the coordinate of the former is at most that of the latter on every dimension. The goal is to find the skyline, which is the set of points p ∈ P such that p is not dominated by any other point in P. The main result of this article is that, for any fixed dimensionality d ≥ 3, in external memory the skyline problem can be settled by performing O((N/B) logd-2M/B(N/B)) I/Os in the worst case, where N is the cardinality of P, B the size of a disk block, and M the capacity of main memory. Similar bounds can also be achieved for computing several skyline variants, including the k-dominant skyline, k-skyband, and α-skyline. Furthermore, the performance can be improved if some dimensions of the data space have small domains. When the dimensionality d is not fixed, the challenge is to outperform the naive algorithm that simply checks all pairs of points in P × P. We give an algorithm that terminates in O((N/B) logd-2 N) I/Os, thus beating the naive solution for any d = O(log N/log log N). © 2012 ACM.",Admission point; Maxima set; Pareto set; Skyline,Database systems; Information systems; Admission point; Cardinalities; Database community; External memory; Main memory; Maxima set; Pareto set; Skyline; Algorithms
Finding alternative shortest paths in spatial networks,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878502787&doi=10.1145%2f2389241.2389248&partnerID=40&md5=960d70e8437729138bd83c4fab592d5e,"Shortest path query is one of the most fundamental queries in spatial network databases. There exist algorithms that can process shortest path queries in real time. However, many complex applications require more than just the calculation of a single shortest path. For example, one of the common ways to determine the importance (or price) of a vertex or an edge in spatial network is to use Vickrey pricing, which intuitively values the vertex v (or edge e) based on how much harder for travelling from the sources to the destinations without using v (or e). In such cases, the alternative shortest paths without using v (or e) are required. In this article, we propose using a precomputation based approach for both single pair alternative shortest path and all pairs shortest paths processing. To compute the alternative shortest path between a source and a destination efficiently, a naïve way is to precompute and store all alternative shortest paths between every pair of vertices avoiding every possible vertex (or edge), which requires O(n4) space. Currently, the state of the art approach for reducing the storage cost is to choose a subset of the vertices as center points, and only store the single-source alternative shortest paths from those center points. Such approach has the space complexity of O(n2 log n). We propose a storage scheme termed iSPQF, which utilizes shortest path quadtrees by observing the relationships between each avoiding vertex and its corresponding alternative shortest paths. We have reduced the space complexity from the naïve O(n4) (or the state of the art O(n2 log n)) to O(min(γ, L)n1.5) with comparable query performance of O(K), where K is the number of vertices in the returned paths, L is the diameter of the spatial network, and γ is a value that depends on the structure of the spatial network, which is empirically estimated to be 40 for real road networks. Experiments on real road networks have shown that the space cost of the proposed iSPQF is scalable, and both the algorithms based on iSPQF are efficient. © 2012 ACM.",Real-time query processing; Shortest paths; Spatial networks,Algorithms; Costs; All pairs shortest paths; Complex applications; Real road networks; Shortest path; Shortest path queries; Spatial network; Spatial network database; State-of-the-art approach; Graph theory
Maximizing conjunctive views in deletion propagation,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878475349&doi=10.1145%2f2389241.2389243&partnerID=40&md5=48b53cd56b788e972989c8cea3659b2f,"In deletion propagation, tuples from the database are deleted in order to reflect the deletion of a tuple from the view. Such an operation may result in the (often necessary) deletion of additional tuples from the view, besides the intentionally deleted one. The article studies the complexity of deletion propagation, where the view is defined by a conjunctive query (CQ), and the goal is to maximize the number of tuples that remain in the view. Buneman et al. showed that for some simple CQs, this problem can be solved by a straightforward algorithm, which is called here the unidimensional algorithm. The article identifies additional cases of CQs where the unidimensional algorithm succeeds, and in contrast, shows that for some other CQs the problem is NP-hard to approximate better than some constant ratio. In fact, it is shown here that among the CQs without self joins, the hard CQs are exactly the ones that the unidimensional algorithm fails on. In other words, the following dichotomy result is proved: for every CQ without self joins, deletion propagation is either APX-hard or solvable (in polynomial time) by the unidimensional algorithm. The article then presents approximation algorithms for certain CQs where deletion propagation is APX-hard. Specifically, two constant-ratio (and polynomial-time) approximation algorithms are given for the class of sunflower CQs (i.e., CQs having a sunflower hypergraph) without self joins. The first algorithm, providing the approximation ratio 1 - 1/e, is obtained by formulating the problem at hand as that of maximizing a monotone submodular function subject to a matroid constraint, and then using a known algorithm for such maximization. The second algorithm gives a smaller approximation ratio, 1/2, yet in polynomial time even under combined complexity. Finally, it is shown that self joins can significantly harden approximation in deletion propagation. © 2012 ACM.",Approximation; Deletion propagation; Dichotomy,Polynomial approximation; Approximation; Approximation ratios; Combined complexity; Conjunctive queries; Deletion propagation; Dichotomy; Dichotomy results; Submodular functions; Approximation algorithms
Revisiting answering tree pattern queries using views,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866521417&doi=10.1145%2f2338626.2338631&partnerID=40&md5=2cd0e714dafbadbb87ee09e2b07eede7,"We revisit the problem of answering tree pattern queries using views.We first show that, for queries and views that do not have nodes labeled with the wildcard *, there is an approach which does not require us to find any rewritings explicitly, yet which produces the same answers as the maximal contained rewriting. Then, using the new approach, we give simple conditions and a corresponding algorithm for identifying redundant view answers, which are view answers that can be ignored when evaluating the maximal contained rewriting.We also consider redundant view answers in the case where there are multiple views, the relationship between redundant views and redundant view answers, and discuss how to combine the removal of redundant view answers and redundant rewritings. We show that the aforesaid results can be extended to a number of other special cases. Finally, for arbitrary queries and views in P{/,//,*,[]}, we provide a method to find the maximal contained rewriting and show how to answer the query using views without explicitly finding the rewritings. © 2012 ACM.",Query processing; Rewriting; Tree pattern; View; XML database; XPath,Forestry; Mathematics; Trees; Forestry; Trees (mathematics); Rewriting; Tree pattern; View; XML database; XPath; Query processing
Efficient reasoning about data trees via integer linear programming,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866523351&doi=10.1145%2f2338626.2338632&partnerID=40&md5=2492a0a0321072f402d436c0e5b1c48f,"Data trees provide a standard abstraction of XML documents with data values: they are trees whose nodes, in addition to the usual labels, can carry labels from an infinite alphabet (data). Therefore, one is interested in decidable formalisms for reasoning about data trees. While some are known-such as the two-variable logic-they tend to be of very high complexity, and most decidability proofs are highly nontrivial. We are therefore interested in reasonable complexity formalisms as well as better techniques for proving decidability. Here we show that many decidable formalisms for data trees are subsumed-fully or partially-by the power of tree automata together with set constraints and linear constraints on cardinalities of various sets of data values. All these constraints can be translated into instances of integer linear programming, giving us an NP upper bound on the complexity of the reasoning tasks. We prove that this bound, as well as the key encoding technique, remain very robust, and allow the addition of features such as counting of paths and patterns, and even a concise encoding of constraints, without increasing the complexity. The NP bound is tight, as we also show that the satisfiability of a single set constraint is already NP-hard. We then relate our results to several reasoning tasks over XML documents, such as satisfiability of schemas and data dependencies and satisfiability of the two-variable logic. As a final contribution, we describe experimental results based on the implementation of some reasoning tasks using the SMT solver Z3. © 2012 ACM.",Data values; Integer linear programming; Presburger arithmetic; Reasoning; Tree languages; XML,Data; Forestry; Languages; Mathematics; Symbols; Trees; Computability and decidability; Encoding (symbols); Forestry; XML; Data values; Integer Linear Programming; Presburger arithmetic; Reasoning; Tree languages; Trees (mathematics)
Foster b-trees,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866511729&doi=10.1145%2f2338626.2338630&partnerID=40&md5=e7f38d0653a76d16be9621b4b549b2bc,"Foster B-trees are a new variant of B-trees that combines advantages of prior B-tree variants optimized for many-core processors and modern memory hierarchies with flash storage and nonvolatile memory. Specific goals include: (i) minimal concurrency control requirements for the data structure, (ii) efficient migration of nodes to new storage locations, and (iii) support for continuous and comprehensive self-testing. Like Blinktrees, Foster B-trees optimize latching without imposing restrictions or specific designs on transactional locking, for example, key range locking. Like write-optimized B-trees, and unlike Blink-trees, Foster B-trees enable large writes on RAID and flash devices as well as wear leveling and efficient defragmentation. Finally, they support continuous and inexpensive yet comprehensive verification of all invariants, including all cross-node invariants of the B-tree structure. An implementation and a performance evaluation show that the Foster B-tree supports high concurrency and high update rates without compromising consistency, correctness, or read performance. © 2012 ACM.",Databases; Foster B-trees; Indexes; Latching; Locking,Coefficients; Data; Forestry; Locking; Mathematics; Optimization; Structures; Trees; Concurrency control; Data structures; Database systems; Flash memory; Nonvolatile storage; Optimization; Trees (mathematics); B trees; Control requirements; De-fragmentation; Flash devices; Flash storage; Indexes; Latching; Locking; Many-core; Memory hierarchy; Non-volatile memories; Performance evaluation; Read performance; Self-testing; Specific design; Storage location; Wear leveling; Forestry
Entangled queries: Enabling declarative data-driven coordination,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866483436&doi=10.1145%2f2338626.2338629&partnerID=40&md5=93c49f724f8957b3bad92ab9a4cd3ce0,"Many data-driven social andWeb applications involve collaboration and coordination. The vision of Declarative Data-Driven Coordination (D3C), proposed in Kot et al. [2010], is to support coordination in the spirit of data management: to make it data-centric and to specify it using convenient declarative languages. This article introduces entangled queries, a language that extends SQL by constraints that allow for the coordinated choice of result tuples across queries originating from different users or applications. It is nontrivial to define a declarative coordination formalism without arriving at the general (NPcomplete) Constraint Satisfaction Problem from AI. In this article, we propose an efficiently enforceable syntactic safety condition that we argue is at the sweet spot where interesting declarative power meets applicability in large-scale data management systems and applications. The key computational problem of D3C is to match entangled queries to achieve coordination. We present an efficient matching algorithm which statically analyzes query workloads and merges coordinating entangled queries into compound SQL queries. These can be sent to a standard database system and return only coordinated results. We present the overall architecture of an implemented system that contains our evaluation algorithm. We also describe a proof-of-concept Facebook application we have built on top of this system to allow friends to coordinate flight plans. Finally, we evaluate the performance of the matching algorithm experimentally on realistic coordination workloads. © 2012 ACM.",,Database systems; Information management; Computational problem; Constraint satisfaction problems; Data centric; Declarative Languages; Declarative power; Evaluation algorithm; Facebook applications; Flight plans; Large-scale data management; Matching algorithm; NP Complete; Proof of concept; Safety condition; SQL query; Sweet spot; Algorithms
Artifact systems with data dependencies and arithmetic,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866501849&doi=10.1145%2f2338626.2338628&partnerID=40&md5=f76a207241d1909595aec6e2d5863252,"We study the static verification problem for data-centric business processes, specified in a variant of IBM's ""business artifact"" model. Artifacts are records of variables that correspond to business-relevant objects and are updated by a set of services equippedwith pre- and postconditions, that implement business process tasks. The verification problem consists in statically checking whether all runs of an artifact system satisfy desirable properties expressed in a first-order extension of linear-time temporal logic. Previous work identified the class of guarded artifact systems and properties, for which verification is decidable. However, the results suffer an important limitation: they fail in the presence of even very simple data dependencies or arithmetic, both crucial to real-life business processes. In this article, we extend the artifact model and verification results to alleviate this limitation. We identify a practically significant class of business artifacts with data dependencies and arithmetic, for which verification is decidable. The technicalmachinery needed to establish the results is fundamentally different from previous work. While the worst-case complexity of verification is nonelementary, we identify various realistic restrictions yielding more palatable upper bounds. © 2012 ACM.",,Information systems; Business Process; Data centric; Data dependencies; First-order; Linear time temporal logic; Static verification; Upper Bound; Verification problems; Verification results; Worst-case complexity; Database systems
Reordering rows for better compression: Beyond the lexicographic order,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866496977&doi=10.1145%2f2338626.2338633&partnerID=40&md5=8a5c11b03d87ecab1a2b88c7281215e0,"Sorting database tables before compressing them improves the compression rate. Can we do better than the lexicographical order? For minimizing the number of runs in a run-length encoding compression scheme, the best approaches to row-ordering are derived from traveling salesman heuristics, although there is a significant trade-off between running time and compression. A new heuristic, MULTIPLE LISTS, which is a variant on NEAREST NEIGHBOR that trades off compression for a major running-time speedup, is a good option for very large tables. However, for some compression schemes, it is more important to generate long runs rather than few runs. For this case, another novel heuristic, VORTEX, is promising. We find that we can improve run-length encoding up to a factor of 3 whereas we can improve prefix coding by up to 80%: these gains are on top of the gains due to lexicographically sorting the table.We prove that the new row reordering is optimal (within 10%) at minimizing the runs of identical values within columns, in a few cases. © 2012 ACM.",Compression; Datawarehousing; Gray codes; Hamming distance; Traveling salesman problem,Compaction; Hamming distance; Traveling salesman problem; Compression rates; Compression scheme; Database tables; Datawarehousing; Gray codes; Lexicographic order; Lexicographically sorting; Nearest neighbors; Run-length encoding; Running time; Traveling salesman; Encoding (symbols)
Foreword,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866512242&doi=10.1145%2f2338626.2338627&partnerID=40&md5=b9e446e2e444215d17ef7b13beb7f0e3,[No abstract available],,
Certain conjunctive query answering in first-order logic,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863624432&doi=10.1145%2f2188349.2188351&partnerID=40&md5=5da01a62cfdfe510d2c1610c2bd0ad0d,"Primary key violations provide a natural means for modeling uncertainty in the relational data model. A repair (or possible world) of a database is then obtained by selecting a maximal number of tuples without ever selecting two distinct tuples that have the same primary key value. For a Boolean query q, the problem CERTAINTY(q) takes as input a database db and asks whether q evaluates to true on every repair of db. We are interested in determining queries q for which CERTAINTY(q) is first-order expressible (and hence in the low complexity class AC0). For queries q in the class of conjunctive queries without self-join, we provide a necessary syntactic condition for first-order expressibility of CERTAINTY(q). For acyclic queries (in the sense of Beeri et al. [1983]), this necessary condition is also a sufficient condition. So we obtain a decision procedure for first-order expressibility of CERTAINTY(q) when q is acyclic and without self-join. We also show that if CERTAINTY(q) is first-order expressible, its first-order definition, commonly called certain first-order rewriting, can be constructed in a rather straightforward way. © 2012 ACM.",Conjunctive queries; Consistent query answering; First-order expressibility; Primary keys,Uncertainty analysis; Boolean queries; Complexity class; Conjunctive queries; Consistent query answering; Decision procedure; Expressibility; First order logic; First-order; First-order rewriting; Modeling uncertainties; Possible worlds; Primary keys; Relational data models; Self-join; Sufficient conditions; Query languages
Cardinal directions between complex regions,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863607820&doi=10.1145%2f2188349.2188350&partnerID=40&md5=e19bdc038e6327028f74495ae8fef2ec,"Besides topological relationships and approximate relationships, cardinal directions like north and southwest have turned out to be an important class of qualitative spatial relationships. They are of interdisciplinary interest in fields like cognitive science, robotics, artificial intelligence, and qualitative spatial reasoning. In spatial databases and Geographic Information Systems (GIS) they are frequently used as join and selection criteria in spatial queries. However, the available computational models of cardinal directions suffer a number of problems like the use of too coarse approximations of the two spatial operand objects in terms of single representative points or minimum bounding rectangles, the lacking property of converseness of the cardinal directions computed, and the limited applicability to simple instead of complex regions only. This article proposes and formally defines a novel two-phase model, called the Objects Interaction Matrix (OIM) model, that solves these problems, and determines cardinal directions for even complex regions. The model consists of a tiling phase and an interpretation phase. In the tiling phase, a tiling strategy first determines the zones belonging to the nine cardinal directions of each individual region object and then intersects them. The result leads to a bounded grid called objects interaction grid. For each grid cell the information about the region objects that intersect it is stored in an objects interaction matrix. In the subsequent interpretation phase, a well-defined interpretation method is applied to such a matrix and determines the cardinal direction. Spatial example queries illustrate our new cardinal direction concept that is embedded in a spatial extension of SQL and provides user-defined cardinal direction predicates. © 2012 ACM.",Cardinal direction; Directional relationship; Spatial database,Artificial intelligence; Query languages; Cardinal direction; Cognitive science; Computational model; Directional relationship; Grid cells; In-field; Interaction matrices; Interpretation methods; Minimum bounding rectangle; Qualitative spatial reasoning; Selection criteria; Spatial database; Spatial extension; Spatial queries; Spatial relationships; Topological relationships; Two-phase model; Geographic information systems
Online subspace skyline query processing using the Compressed SkyCube,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863620812&doi=10.1145%2f2188349.2188357&partnerID=40&md5=3fd32b67300888bb5572603afe024b60,"The skyline query can help identify the ""best"" objects in a multi-attribute dataset. During the past decade, this query has received considerable attention in the database research community. Most research focused on computing the ""skyline"" of a dataset, or the set of ""skyline objects"" that are not dominated by any other object. Such algorithms are not appropriate in an online system, which should respond in real time to skyline query requests with arbitrary subsets of the attributes (also called subspaces). To guarantee realtime response, an online system should precompute the skylines for all subspaces, and look up a skyline upon query. Unfortunately, because the number of subspaces is exponential to the number of attributes, such pre computation has very expensive storage cost and update cost. We propose the Compressed SkyCube (CSC) that is much more compact, yet can still return the skyline of any subspace without consulting the base table. The CSC therefore combines the advantage of precomputation in that it can respond to queries in real time, and the advantage of no-precomputation in that it has efficient space cost and update cost. This article presents the CSC data structures, the CSC query algorithm, the CSC update algorithm, and the CSC initial computation scheme. A solution to extend to high-dimensional data is also proposed. © 2012 ACM.",Compressed; Skyline; Subspace; Update support; Workload,Algorithms; Costs; Data structures; Arbitrary subsets; Base table; Compressed; Data sets; Database research; High dimensional data; Multi-attributes; Pre-computation; Precompute; Query algorithms; Real time; Real-time response; SKYCUBE; Skyline; Skyline query; Storage costs; Subspace; Workload; Data processing
"The implication problem of data dependencies over SQL table definitions: Axiomatic, algorithmic and logical characterizations",2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863632110&doi=10.1145%2f2188349.2188355&partnerID=40&md5=a7b7e09fab09a253d47b49af53729109,"We investigate the implication problem for classes of data dependencies over SQL table definitions. Under Zaniolo's ""no information"" interpretation of null markers we establish an axiomatization and algorithms to decide the implication problem for the combined class of functional and multivalued dependencies in the presence of NOT NULL constraints. The resulting theory subsumes three previously orthogonal frameworks. We further show that the implication problem of this class is equivalent to that in a propositional fragment of Schaerf and Cadoli's [1995] family of para-consistent S-3 logics. In particular, S is the set of variables that correspond to attributes declared NOT NULL. We also show how our equivalences for multivalued dependencies can be extended to Delobel's class of full first-order hierarchical decompositions, and the equivalences for functional dependencies can be extended to arbitrary Boolean dependencies. These dualities allow us to transfer several findings from the propositional fragments to the corresponding classes of data dependencies, and vice versa. We show that our results also apply to Codd's null interpretation ""value unknown at present"", but not to Imielinski's [1989] or-relations utilizing Levene and Loizou's weak possible world semantics [Levene and Loizou 1998]. Our findings establish NOT NULL constraints as an effective mechanism to balance not only the certainty in database relations but also the expressiveness with the efficiency of entailment relations. They also control the degree by which the implication of data dependencies over total relations is soundly approximated in SQL table definitions. © 2012 ACM.",Axiomatization; Boolean dependency; Boolean logic; Functional dependency; Implication; Incomplete information; Logic of paradox; Multivalued dependency; S-3 logic; SQL,Algorithms; Semantics; Axiomatization; Boolean dependency; Boolean logic; Functional dependency; Implication; Incomplete information; Logic of paradox; Multivalued dependency; S-3 logic; SQL; Equivalence classes
Stochastic skylines,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863615587&doi=10.1145%2f2188349.2188356&partnerID=40&md5=91c1b408b3c38a51584fa222377901a7,"In many applications involving multiple criteria optimal decision making, users may often want to make a personal trade-off among all optimal solutions for selecting one object that fits best their personal needs. As a key feature, the skyline in a multidimensional space provides the minimum set of candidates for such purposes by removing all points not preferred by any (monotonic) utility/scoring functions; that is, the skyline removes all objects not preferred by any user no matter how their preferences vary. Driven by many recent applications with uncertain data, the probabilistic skyline model is proposed to retrieve uncertain objects based on skyline probabilities. Nevertheless, skyline probabilities cannot capture the preferences of monotonic utility functions. Motivated by this, in this article we propose a novel skyline operator, namely stochastic skylines. In the light of the expected utility principle, stochastic skylines guarantee to provide the minimum set of candidates to optimal solutions over a family of utility functions. We first propose the lskyline operator based on the lower orthant orders. lskyline guarantees to provide the minimum set of candidates to the optimal solutions for the family of monotonic multiplicative utility functions. While lskyline works very effectively for the family of multiplicative functions, it may miss optimal solutions for other utility/scoring functions (e.g., linear functions). To resolve this, we also propose a general stochastic skyline operator, gskyline, based on the usual orders. gskyline provides the minimum candidate set to the optimal solutions for all monotonic functions. For the first time regarding the existing literature, we investigate the complexities of determining a stochastic order between two uncertain objects whose probability distributions are described discretely. We firstly show that determining the lower orthant order is NP-complete with respect to the dimensionality; consequently the problem of computing lskyline is NP-complete. We also show an interesting result as follows. While the usual order involves more complicated geometric forms than the lower orthant order, the usual order may be determined in polynomial time regarding all the inputs, including the dimensionality; this implies that gskyline can be computed in polynomial time. A general framework is developed for efficiently and effectively retrieving lskyline and gskyline from a set of uncertain objects, respectively, together with efficient and effective filtering techniques. Novel and efficient verification algorithms are developed to efficiently compute lskyline over multidimensional uncertain data, which run in polynomial time if the dimensionality is fixed, and to efficiently compute gskyline in polynomial time regarding all inputs. We also show, by theoretical analysis and experiments, that the sizes of lskyline and gskyline are both quite similar to that of conventional skyline over certain data. Comprehensive experiments demonstrate that our techniques are efficient and scalable regarding both CPU and IO costs. © 2012 ACM.",Skyline; Stochastic order; Uncertain data,Computational complexity; Experiments; Optimal systems; Polynomial approximation; Probability distributions; Expected utility; Filtering technique; Geometric form; Key feature; Linear functions; Monotonic functions; Multi-dimensional space; Multiple criteria; Multiplicative functions; NP Complete; Objects-based; Optimal decision making; Optimal solutions; Orthant; Polynomial-time; Skyline; Skyline operator; Stochastic order; Uncertain datas; Utility functions; Verification algorithms; Stochastic systems
Exact and approximate algorithms for the Most Connected Vertex problem,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863631671&doi=10.1145%2f2188349.2188354&partnerID=40&md5=81bf7d2da1898df14a94d41351d14fee,"An (edge) hidden graph is a graph whose edges are not explicitly given. Detecting the presence of an edge requires an expensive edge probing query. We consider the k Most Connected Vertex (k-MCV) problem on hidden bipartite graphs. Given a bipartite graph G with independent vertex sets B and W, the goal is to find the k vertices in B with the largest degrees using the minimum number of queries. This problem can be regarded as a top-k extension of semi-join, and is encountered in several applications in practice. If B and W have n and m vertices, respectively, the number of queries needed to solve the problem is nm in the worst case. This, however, is a pessimistic estimate on how many queries are necessary on practical data. In fact, on some inputs, the problem may be settled with only km + n queries, which is significantly lower than nm for k ≤ n. The huge difference between km + n and nm makes it interesting to design an adaptive algorithm that is guaranteed to achieve the best possible performance on every input G. For k ≤ n/2, we give an algorithm that is instance optimal among a broad class of solutions. This means that, for any G, our algorithm can perform more queries than the optimal solution (which is unknown) by only a constant factor, which can be shown at most 2. As a second step, we study an ∈-approximate version of the k-MCV problem, where ∈ is a parameter satisfying 0 < ∈ < 1. The goal is to return k black vertices b1v,...,bk such that the degree of bi (i = k) can be smaller than ti by a factor of at most ∈, where t1,..., tk (in nonascending order) are the degrees of the k most connected black vertices. We give an efficient randomized algorithm that successfully finds the correct answer with high probability. In particular, for a fixed ∈ and a fixed success probability, our algorithm performs o(nm) queries in expectation for tk = ω(logn). In other words, whenever t k is greater than log n by more than a constant, our algorithm beats the Ω(nm) lower bound for solving the k-MCV problem exactly. All the proposed algorithms, despite the complication of their underlying theory, are simple enough for easy implementation in practice. Extensive experiments have confirmed that their performance in reality agrees with our theoretical findings very well. © 2012 ACM.",Bipartite graph; Competitive analysis; Maximum degree,Adaptive algorithms; Problem solving; Approximate algorithms; Bipartite graphs; Competitive analysis; Constant factors; High probability; Independent vertex sets; Lower bounds; Maximum degree; Optimal solutions; Randomized Algorithms; Graph theory
Secure distributed computation of anonymized views of shared databases,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863607195&doi=10.1145%2f2188349.2188353&partnerID=40&md5=2d7aa87faa541d147b9e375a34445170,"We consider the problem of computing efficient anonymizations of partitioned databases. Given a database that is partitioned between several sites, either horizontally or vertically, we devise secure distributed algorithms that allow the different sites to obtain a k-anonymized and £-diverse view of the union of their databases, without disclosing sensitive information. Our algorithms are based on the sequential algorithm [Goldberger and Tassa 2010] that offers anonymizations with utility that is significantly better than other anonymization algorithms, and in particular those that were implemented so far in the distributed setting. Our algorithms can apply to different generalization techniques and utility measures and to any number of sites. While previous distributed algorithms depend on costly cryptographic primitives, the cryptographic assumptions of our solution are surprisingly minimal. © 2012 ACM.",Distributed data mining; K-anonymization; Privacy-preserving data mining; Secure multiparty computation,Algorithms; Cryptography; Data mining; Data privacy; Anonymization; Cryptographic assumptions; Cryptographic primitives; Distributed computations; Distributed data mining; K-anonymization; Privacy preserving data mining; Secure multi-party computation; Sensitive informations; Sequential algorithm; Utility measure; Database systems
Shared execution strategy for neighbor-based pattern mining requests over streaming windows,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859093114&doi=10.1145%2f2109196.2109201&partnerID=40&md5=1fca3b206bf46348069127375df569a1,"In diverse applications ranging from stock trading to traffic monitoring, data streams are continuously monitored by multiple analysts for extracting patterns of interest in real time. These analysts often submit similar pattern mining requests yet customized with different parameter settings. In this work, we present shared execution strategies for processing a large number of neighbor-based pattern mining requests of the same type yet with arbitrary parameter settings. Such neighbor-based pattern mining requests cover a broad range of popular mining query types, including detection of clusters, outliers, and nearest neighbors. Given the high algorithmic complexity of the mining process, serving multiple such queries in a single system is extremely resource intensive. The naive method of detecting and maintaining patterns for different queries independently is often infeasible in practice, as its demands on system resources increase dramatically with the cardinality of the query workload. In order to maximize the efficiency of the system resource utilization for executing multiple queries simultaneously, we analyze the commonalities of the neighborbased pattern mining queries, and identify several general optimization principles which lead to significant system resource sharing among multiple queries. In particular, as a preliminary sharing effort, we observe that the computation needed for the range query searches (the process of searching the neighbors for each object) can be shared among multiple queries and thus saves the CPU consumption. Then we analyze the interrelations between the patterns identified by queries with different parameters settings, including both pattern-specific and window-specific parameters. For that, we first introduce an incremental pattern representation, which represents the patterns identified by queries with different pattern-specific parameters within a single compact structure. This enables integrated pattern maintenance for multiple queries. Second, by leveraging the potential overlaps among sliding windows, we propose a metaquery strategy which utilizes a single query to answer multiple queries with different window-specific parameters. By combining these three techniques, namely the range query search sharing, integrated pattern maintenance, and metaquery strategy, our framework realizes fully shared execution of multiple queries with arbitrary parameter settings. It achieves significant savings of computational and memory resources due to shared execution. Our comprehensive experimental study, using real data streams from domains of stock trades and moving object monitoring, demonstrates that our solution is significantly faster than the independent execution strategy, while using only a small portion of memory space compared to the independent execution.We also show that our solution scales in handling large numbers of queries in the order of hundreds or even thousands under high input data rates. © 2012 ACM.",Multiple query optimization; Pattern mining; Stream processing,Commerce; Data communication systems; Data mining; Distributed computer systems; Maintainability; Algorithmic complexity; Cardinalities; Compact structures; Data stream; Diverse applications; Execution strategies; Experimental studies; Input datas; Integrated pattern; Memory resources; Memory space; Metaquery; Mining process; Moving objects; Multiple queries; Multiple query optimizations; Nearest neighbors; Optimization principle; Parameter setting; Parameters setting; Pattern mining; Pattern representation; Query types; Range query; Real time; Similar pattern mining; Sliding Window; Stock trading; Stream processing; System resource utilization; System resources; Traffic monitoring; Query processing
Understanding cardinality estimation using entropy maximization,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859056328&doi=10.1145%2f2109196.2109202&partnerID=40&md5=0c8f17d5cb5f2b7587cd4adb41c335a4,"Cardinality estimation is the problem of estimating the number of tuples returned by a query; it is a fundamentally important task in data management, used in query optimization, progress estimation, and resource provisioning. We study cardinality estimation in a principled framework: given a set of statistical assertions about the number of tuples returned by a fixed set of queries, predict the number of tuples returned by a new query. We model this problem using the probability space, over possible worlds, that satisfies all provided statistical assertions and maximizes entropy. We call this the Entropy Maximization model for statistics (MaxEnt). In this article we develop the mathematical techniques needed to use the MaxEnt model for predicting the cardinality of conjunctive queries. © 2012 ACM.",Cardinality estimation; Entropy maximization; Entropy models; Query processing,Entropy; Information management; Query processing; Cardinalities; Cardinality estimations; Conjunctive queries; Entropy maximization; Entropy models; MaxEnt models; Possible worlds; Probability spaces; Progress estimations; Query optimization; Resource provisioning; Estimation
Exploiting web querying for web people search,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859061919&doi=10.1145%2f2109196.2109203&partnerID=40&md5=f083ea10be1c4970254701b2427edcc7,"Searching for people on the Web is one of the most common query types submitted to Web search engines today. However, when a person name is queried, the returned Webpages often contain documents related to several distinct namesakeswho have the queried name. The task of disambiguating and finding theWebpages related to the specific person of interest is left to the user. Many Web People Search (WePS) approaches have been developed recently that attempt to automate this disambiguation process. Nevertheless, the disambiguation quality of these techniques leaves major room for improvement. In this article, we present a new WePS approach. It is based on issuing additional auxiliary queries to the Web to gain additional knowledge about the Webpages that need to be disambiguated. Thus, the approach uses the Web as an external data source by issuing queries to collect co-occurrence statistics. These statistics are used to assess the overlap of the contextual entities extracted from the Webpages. The article also proposes a methodology to make this Web querying technique efficient. Further, the article proposes an approach that is capable of combining various types of disambiguating information, including other common types of similarities, by applying a correlation clustering approach with after-clustering of singleton clusters. These properties allow the framework to get an advantage in terms of result quality over other state-of-The-art WePS techniques. © 2012 ACM.",Automated Web querying; Entity resolution; Skyline-based classifier; Web people search; WePS,Search engines; Additional knowledge; Co-occurrence statistics; Correlation clustering; External data sources; Query types; Web people search; Web querying; WePS; Websites
Attribute and object selection queries on objects with probabilistic attributes,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859012047&doi=10.1145%2f2109196.2109199&partnerID=40&md5=8c24967b0c81975a2ad00c026afe8b12,"Modern data processing techniques such as entity resolution, data cleaning, information extraction, and automated tagging often produce results consisting of objects whose attributes may contain uncertainty. This uncertainty is frequently captured in the form of a set of multiple mutually exclusive value choices for each uncertain attribute along with a measure of probability for alternative values. However, the lay end-user, as well as some end-applications, might not be able to interpret the results if outputted in such a form. Thus, the question is how to present such results to the user in practice, for example, to support attribute-value selection and object selection queries the user might be interested in. Specifically, in this article we study the problem of maximizing the quality of these selection queries on top of such a probabilistic representation. The quality is measured using the standard and commonly used set-based quality metrics. We formalize the problem and then develop efficient approaches that provide high-quality answers for these queries. The comprehensive empirical evaluation over three different domains demonstrates the advantage of our approach over existing techniques. © 2012 ACM.",Attribute value selection; F-measure; Object selection query; Probabilistic data; Result quality,Database systems; Information systems; Attribute-value selection; Data cleaning; Data processing techniques; Different domains; Empirical evaluations; End users; F-measure; High quality; Information Extraction; Object selection; Probabilistic data; Probabilistic representation; Quality metrics; Data processing
Proximity measures for rank join,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859033240&doi=10.1145%2f2109196.2109198&partnerID=40&md5=6935f7d82e26113f5fce78cc200f482a,"We introduce the proximity rank join problem, where we are given a set of relations whose tuples are equipped with a score and a real-valued feature vector. Given a target feature vector, the goal is to return the K combinations of tuples with high scores that are as close as possible to the target and to each other, according to some notion of distance or dissimilarity. The setting closely resembles that of traditional rank join, but the geometry of the vector space plays a distinctive role in the computation of the overall score of a combination. Also, the input relations typically return their results either by distance from the target or by score. Because of these aspects, it turns out that traditional rank join algorithms, such as the well-known HRJN, have shortcomings in solving the proximity rank join problem, as they may read more input than needed. To overcome this weakness, we define a tight bound (used as a stopping criterion) that guarantees instance optimality, that is, an I/O cost is achieved that is always within a constant factor of optimal. The tight bound can also be used to drive an adaptive pulling strategy, deciding at each step which relation to access next. For practically relevant classes of problems, we show how to compute the tight bound efficiently. An extensive experimental study validates our results and demonstrates significant gains over existing solutions. © 2012 ACM.",Proximity; Rank-aware processing; Top-k,Database systems; Information systems; Constant factors; Experimental studies; Feature vectors; Instance optimality; Notion of distance; Proximity; Proximity measure; Rank-join algorithms; Stopping criteria; Target feature; Tight bound; Top-k; Optimization
A survey of B-Tree logging and recovery techniques,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859050284&doi=10.1145%2f2109196.2109197&partnerID=40&md5=0214fe537c2267cbff67f750214c212b,"B-Trees have been ubiquitous in database management systems for several decades, and they serve in many other storage systems as well. Their basic structure and their basic operations are well understood including search, insertion, and deletion. However, implementation of transactional guarantees such as all-or-nothing failure atomicity and durability in spite of media and system failures seems to be difficult. High-performance techniques such as pseudo-deleted records, allocation-only logging, and transaction processing during crash recovery are widely used in commercial b-Tree implementations but not widely understood. This survey collects many of these techniques as a reference for students, researchers, system architects, and software developers. Central in this discussion are physical data independence, separation of logical database contents and physical representation, and the concepts of user transactions and system transactions. Many of the techniques discussed are applicable beyond b-Trees. © 2012 ACM.",,Forestry; Surveys; Systems Engineering; Database systems; Surveys; Systems engineering; B trees; Basic operation; Basic structure; High-performance techniques; Logical database; Physical data; Software developer; Storage systems; System architects; System failures; Transaction processing; User transaction; Forestry
Differentiating search results on structured data,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859054640&doi=10.1145%2f2109196.2109200&partnerID=40&md5=7d622f86f78c0dc3a40563f2d3a24837,"Studies show that about 50% of Web search is for information exploration purposes, where a user would like to investigate, compare, evaluate, and synthesize multiple relevant results. Due to the absence of general tools that can effectively analyze and differentiate multiple results, a user has to manually read and comprehend potential large results in an exploratory search. Such a process is time consuming, labor intensive and error prone. Interestingly, we find that the metadata information embedded in structured data provides a potential for automating or semi-automating the comparison of multiple results. In this article we present an approach for structured data search result differentiation. We define the differentiability of query results and quantify the degree of difference. Then we define the problem of identifying a limited number of valid features in a result that can maximally differentiate this result from the others, which is proved NP-hard. We propose two local optimality conditions, namely single-swap and multi-swap, and design efficient algorithms to achieve local optimality.We then present a feature type-based approach, which further improves the quality of the features identified for result differentiation. To show the usefulness of our approach, we implemented a system CompareIt, which can be used to compare structured search results as well as any objects. Our empirical evaluation verifies the effectiveness and efficiency of the proposed approach. © 2012 ACM.",Comparison; Databases; Differentiation; Keyword search; Result analysis; Structured data; XML data,Database systems; Differentiation (calculus); Metadata; Search engines; Comparison; Keyword search; Result analysis; Structured data; XML data; Algorithms
Comparing workflow specification languages: A matter of Views,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863618419&doi=10.1145%2f2188349.2188352&partnerID=40&md5=a90382a7334dfd39c7c0db99f6a94cab,"We address the problem of comparing the expressiveness of workflow specification formalisms using a notion of view of a workflow. Views allow to compare widely different workflow systems by mapping them to a common representation capturing the observables relevant to the comparison. Using this framework, we compare the expressiveness of several workflow specification mechanisms, including automata, temporal constraints, and pre-and postconditions, with XML and relational databases as underlying data models. One surprising result shows the considerable power of static constraints to simulate apparently much richer workflow control mechanisms. © 2012 ACM.",Design; Languages; Theory,Design; Query languages; Specification languages; Relational Database; Static constraints; Temporal constraints; Theory; Work-flow systems; Workflow control; Workflow specification; Specifications
On provenance minimization,2012,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878489895&doi=10.1145%2f2389241.2389249&partnerID=40&md5=0acca485c052284342fd7e9606d1040a,"Provenance information has been proved to be very effective in capturing the computational process performed by queries, and has been used extensively as the input to many advanced data management tools (e.g., view maintenance, trust assessment, or query answering in probabilistic databases). We observe here that while different (set-)equivalent queries may admit different provenance expressions when evaluated on the same database, there is always some part of these expressions that is common to all. We refer to this part as the core provenance. In addition to being informative, the core provenance is also useful as a compact input to the aforementioned data management tools. We formally define the notion of core provenance. We study algorithms that, given a query, compute an equivalent (called p-minimal) query that for every input database, the provenance of every result tuple is the core provenance. We study such algorithms for queries of varying expressive power (namely conjunctive queries with disequalities and unions thereof). Finally, we observe that, in general, one would not want to require database systems to execute a specific p-minimal query, but instead to be able to find, possibly off-line, the core provenance of a given tuple in the output (computed by an arbitrary equivalent query), without reevaluating the query. We provide algorithms for such direct computation of the core provenance. © 2012 ACM.",Provenance; Query minimization,Algorithms; Information management; Query languages; Computational process; Conjunctive queries; Data management tools; Direct computations; Probabilistic database; Provenance; Query minimization; Trust assessments; Query processing
The data cyclotron query processing scheme,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855213912&doi=10.1145%2f2043652.2043660&partnerID=40&md5=482aea486b0d23f6e17b22de2c82c87d,"A grand challenge of distributed query processing is to devise a self-organizing architecture which exploits all hardware resources optimally to manage the database hot set, minimize query response time, and maximize throughput without single point global coordination. The Data Cyclotron architecture [Goncalves and Kersten 2010] addresses this challenge using turbulent data movement through a storage ring built from distributedmainmemory and capitalizing on the functionality offered bymodern remote-DMA network facilities. Queries assigned to individual nodes interact with the storage ring by picking up data fragments, which are continuously flowing around, that is, the hot set. The storage ring is steered by the Level Of Interest (LOI) attached to each data fragment, which represents the cumulative query interest as it passes around the ring multiple times. A fragment with LOI below a given threshold, inversely proportional to the ring load, is pulled out to free up resources. This threshold is dynamically adjusted in a fully distributed manner based on ring characteristics and locally observed query behavior. It optimizes resource utilization by keeping the average data access latency low. The approach is illustrated using an extensive and validated simulation study. The results underpin the fragment hot set management robustness in turbulent workload scenarios. A fully functional prototype of the proposed architecture has been implemented using modest extensions to MonetDB and runs within amultirack cluster equipped with Infiniband. Extensive experimentation using both microbenchmarks and high-volume workloads based on TPC-H demonstrates its feasibility. The Data Cyclotron architecture and experiments open a new vista for modern distributed database architectures with a plethora of new research challenges. © 2011 ACM.",Column-stores; Infiniband; Throughput,Cluster computing; Cyclotrons; Experiments; Image recording; Network architecture; Query processing; Queueing networks; Storage rings; Throughput; Average data; Column-stores; Data fragments; Data movements; Distributed database; Distributed query processing; Functional Prototypes; Global coordination; Grand Challenge; Hardware resources; Infiniband; Level Of Interest; Maximize throughput; Microbenchmarks; Picking up; Proposed architectures; Query behavior; Query response time; Research challenges; Resource utilizations; Ring characteristics; Ring loads; Self organizing; Simulation studies; Single point; Data handling
Collaborative personalized top-k processing,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855208717&doi=10.1145%2f2043652.2043659&partnerID=40&md5=93a2bbe02d937b9bef90bf7da38ae968,"This article presents P4Q, a fully decentralized gossip-based protocol to personalize query processing in social tagging systems. P4Q dynamically associates each user with social acquaintances sharing similar tagging behaviors. Queries are gossiped among such acquaintances, computed on-the-fly in a collaborative, yet partitioned manner, and results are iteratively refined and returned to the querier. Analytical and experimental evaluations convey the scalability of P4Q for top-k query processing, as well its inherent ability to cope with users updating profiles and departing. © 2011 ACM.",Gossip; Peer-to-peer networks; Personalization; Top-k processing,Peer to peer networks; Query processing; Social networking (online); User interfaces; Experimental evaluation; Gossip; Gossip-based protocol; On-the-fly; Peer to peer; Personalization; Social tagging; Top-k query processing; Distributed computer systems
Designing fast architecture-sensitive tree search on modern multicore/many-core processors,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855233939&doi=10.1145%2f2043652.2043655&partnerID=40&md5=5c1426cf526d2aead39ee1476a4821f3,"In-memory tree structured index search is a fundamental database operation. Modern processors provide tremendous computing power by integrating multiple cores, each with wide vector units. There has been much work to exploit modern processor architectures for database primitives like scan, sort, join, and aggregation. However, unlike other primitives, tree search presents significant challenges due to irregular and unpredictable data accesses in tree traversal. In this article, we present FAST, an extremely fast architecturesensitive layout of the index tree. FAST is a binary tree logically organized to optimize for architecture features like page size, cache line size, and Single Instruction Multiple Data (SIMD) width of the underlying hardware. FAST eliminates the impact of memory latency, and exploits thread-level and data-level parallelism on both CPUs and GPUs to achieve 50 million (CPU) and 85 million (GPU) queries per second forlarge trees of 64M elements, with even better results on smaller trees. These are 5X (CPU) and 1.7X (GPU) faster than the best previously reported performance on the same architectures. We also evaluated FAST on the Intel® Many Integrated Core architecture (IntelR MIC), showing a speedup of 2.4X-3X over CPU and 1.8X-4.4X over GPU. FAST supports efficient bulk updates by rebuilding index trees in less than 0.1 seconds for datasets as large as 64M keys and naturally integrates compression techniques, overcoming the memory bandwidth bottleneck and achieving a 6X performance improvement over uncompressed index search for large keys on CPUs. © 2011 ACM.",Compression; CPU; GPU; Many-core; Multicore; Single instruction multiple data (SIMD); Tree search,Compaction; Compression; Data; Equipment; Fire Fighting; Forestry; Mathematics; Trees; Bandwidth compression; Binary trees; Cache memory; Compaction; Fire fighting equipment; Forestry; Memory architecture; Program processors; CPU; GPU; Many-core; Multi core; Single instruction multiple data; Tree search; Trees (mathematics)
Capturing continuous data and answering aggregate queries in probabilistic XML,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855216736&doi=10.1145%2f2043652.2043658&partnerID=40&md5=be4bcc52d50b1f530343440cd737fb61,"Sources of data uncertainty and imprecision are numerous. A way to handle this uncertainty is to associate probabilistic annotations to data. Many such probabilistic database models have been proposed, both in the relational and in the semi-structured setting. The latter is particularly well adapted to the management of uncertain data coming from a variety of automatic processes. An important problem, in the context of probabilistic XML databases, is that of answering aggregate queries (count, sum, avg, etc.), which has received limited attention so far. In a model unifying the various (discrete) semi-structured probabilistic models studied up to now, we present algorithms to compute the distribution of the aggregation values (exploiting some regularity properties of the aggregate functions) and probabilistic moments (especially expectation and variance) of this distribution.We also prove the intractability of some of these problems and investigate approximation techniques. We finally extend the discrete model to a continuous one, in order to take into account continuous data values, such as measurements from sensor networks, and extend our algorithms and complexity results to the continuous case. © 2011 ACM.",Aggregate queries; Aggregation; Continuous distributions; Probabilistic databases; Probabilistic XML; XML,Agglomeration; Approximation algorithms; Database systems; Sensor networks; XML; Aggregate function; Aggregate queries; Algorithms and complexity; Approximation techniques; Continuous data; Continuous distributions; Data uncertainty; Discrete models; Expectation and variance; Probabilistic database; Probabilistic models; Probabilistic XML; Regularity properties; Semi-structured; Uncertain datas; Probability distributions
Finding maximal cliques in massive networks,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855223733&doi=10.1145%2f2043652.2043654&partnerID=40&md5=e6189471412d80c2108717014c61e8ce,"Maximal clique enumeration is a fundamental problem in graph theory and has important applications in many areas such as social network analysis and bioinformatics. The problem is extensively studied; however, the best existing algorithms require memory space linear in the size of the input graph. This has become a serious concern in view of the massive volume of today's fast-growing networks. We propose a general framework for designing external-memory algorithms for maximal clique enumeration in large graphs. The general framework enables maximal clique enumeration to be processed recursively in small subgraphs of the input graph, thus allowing in-memory computation of maximal cliques without the costly random disk access. We prove that the set of cliques obtained by the recursive local computation is both correct (i.e., globally maximal) and complete. The subgraph to be processed each time is defined based on a set of base vertices that can be flexibly chosen to achieve different purposes. We discuss the selection of the base vertices to fully utilize the available memory in order to minimize I/O cost in static graphs, and for update maintenance in dynamic graphs. We also apply our framework to design an external-memory algorithm for maximum clique computation in a large graph. © 2011 ACM.",Dynamic graphs; H*-graph; H-index; Massive networks; Maximal clique enumeration; Scale-free networks,Algorithms; Bioinformatics; Graphic methods; Random access storage; Social networking (online); Dynamic graph; H-index; Massive networks; Maximal clique enumerations; Scale free networks; Graph theory
Foreword to TODS invited papers issue 2011,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855226759&doi=10.1145%2f2043652.2043653&partnerID=40&md5=27bc018d59833de05a58f56ee6ffc10b,[No abstract available],,
Bag equivalence of tree patterns,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855253268&doi=10.1145%2f2043652.2043657&partnerID=40&md5=a964f61e5cb5a16790c1c27558c798ab,"When a query is evaluated under bag semantics, each answer is returned asmany times as it has derivations. Bag semantics has long been recognized as important, especially when aggregation functions will be applied to query results. This article is the first to focus on bag semantics for tree pattern queries. In particular, the problem of bag equivalence of a large class of tree pattern queries (which can be used to model XPath) is explored. The queries can contain unions, branching, label wildcards, the vertical child and descendant axes, the horizontal following and following-sibling axes, as well as positional (i.e., first and last) axes. Equivalence characterizations are provided, and their complexity is analyzed. As the descendant axis involves a recursive relationship, this article is also the first to address bag equivalence over recursive queries, in any setting. © 2011 ACM.",Bag semantics; Query equivalence; Tree patterns; XPath,Forestry; Models; Trees; Equivalence classes; Forestry; Semantics; Aggregation functions; Large class; Query equivalence; Query results; Tree pattern; Tree pattern queries; Tree patterns; XPath; Query processing
Characterizing schema mappings via data examples,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855238164&doi=10.1145%2f2043652.2043656&partnerID=40&md5=ca19ccd96da6fc710734d6a4b31e22a8,"Schema mappings are high-level specifications that describe the relationship between two database schemas; they are considered to be the essential building blocks in data exchange and data integration, and have been the object of extensive research investigations. Since in real-life applications schema mappings can be quite complex, it is important to develop methods and tools for understanding, explaining, and refining schema mappings. A promising approach to this effect is to use ""good data examples that illustrate the schema mapping at hand. We develop a foundation for the systematic investigation of data examples and obtain a number of results on both the capabilities and the limitations of data examples in explaining and understanding schema mappings. We focus on schema mappings specified by source-to-target tuple generating dependencies (s-t tgds) and investigate the following problem: which classes of s-t tgds can be ""uniquely characterized by a finite set of data examples? Our investigation begins by considering finite sets of positive and negative examples, which are arguably the most natural choice of data examples. However, we show that they are not powerful enough to yield interesting unique characterizations. We then consider finite sets of universal examples, where a universal example is a pair consisting of a source instance and a universal solution for that source instance. We first show that unique characterizations via universal examples is, in a precise sense, equivalent to the existence of Armstrong bases (a relaxation of the classical notion of Armstrong databases). After this, we show that every schema mapping specified by LAV s-t tgds is uniquely characterized by a finite set of universal examples with respect to the class of LAV s-t tgds. Moreover, this positive result extends to the much broader classes of n-modular schema mappings, n a positive integer. Finally, we study the unique characterizability of GAV schema mappings. It turns out that some GAV schema mappings are uniquely characterizable by a finite set of universal examples with respect to the class of GAV s-t tgds, while others are not. By unveiling a tight connection with homomorphism dualities, we establish an effective, sound, and complete criterion for determining whether or not a GAV schema mapping is uniquely characterizable by a finite set of universal examples with respect to the class of GAV s-t tgds. © 2011 ACM.",Data examples; Data exchange; Data integration; Schema mappings,Electronic data interchange; Building blockes; Data examples; Data integration; Database schemas; Finite set; Following problem; Good data; High level specification; Negative examples; Positive integers; Real-life applications; Schema mappings; Systematic investigations; Universal solutions; Set theory
Embedding-based subsequence matching in time-series databases,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052349421&doi=10.1145%2f2000824.2000827&partnerID=40&md5=b9b2d5fa0c1121eae0e74bf735c2bc4d,"We propose an embedding-based framework for subsequence matching in time-series databases that improves the efficiency of processing subsequence matching queries under the Dynamic Time Warping (DTW) distance measure. This framework partially reduces subsequence matching to vector matching, using an embedding that maps each query sequence to a vector and each database time series into a sequence of vectors. The database embedding is computed offline, as a preprocessing step. At runtime, given a query object, an embedding of that object is computed online. Relatively few areas of interest are efficiently identified in the database sequences by comparing the embedding of the query with the database vectors. Those areas of interest are then fully explored using the exact DTW-based subsequence matching algorithm. We apply the proposed framework to define two specific methods. The first method focuses on time-series subsequence matching under unconstrained Dynamic Time Warping. The second method targets subsequence matching under constrained Dynamic Time Warping (cDTW), where warping paths are not allowed to stray too much off the diagonal. In our experiments, good trade-offs between retrieval accuracy and retrieval efficiency are obtained for both methods, and the results are competitive with respect to current state-of-the-art methods. © 2011 ACM.",Embedding methods; Nearest neighbor retrieval; Non-Euclidean spaces; Nonmetric spaces; Similarity matching,Time series; Vectors; Embedding method; Nearest neighbors; Non-Euclidean spaces; Non-metric spaces; Similarity-matching; Database systems
"A Survey on representation, composition and application of preferences in database systems",2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052370899&doi=10.1145%2f2000824.2000829&partnerID=40&md5=5721a47e1df7fd8506d759b4147479e7,"Preferences have been traditionally studied in philosophy, psychology, and economics and applied to decision making problems. Recently, they have attracted the attention of researchers in other fields, such as databases where they capture soft criteria for queries. Databases bring a whole fresh perspective to the study of preferences, both computational and representational. From a representational perspective, the central question is how we can effectively represent preferences and incorporate them in database querying. From a computational perspective, we can look at how we can efficiently process preferences in the context of database queries. Several approaches have been proposed but a systematic study of these works is missing. The purpose of this survey is to provide a framework for placing existing works in perspective and highlight critical open challenges to serve as a springboard for researchers in database systems.We organize our study around three axes: preference representation, preference composition, and preference query processing. © 2011 ACM.",Preference modeling; Preference queries,Decision making; Query processing; Surveys; Database queries; Database querying; Decision-making problem; Preference modeling; Preference queries; Preference query processing; Preference representation; Systematic study; Three axes; Query languages
Differential dependencies: Reasoning and discovery,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052365341&doi=10.1145%2f2000824.2000826&partnerID=40&md5=dbf355fb980b40962ed6880b3164c8fc,"The importance of difference semantics (e.g., ""similar"" or ""dissimilar"") has been recently recognized for declaring dependencies among various types of data, such as numerical values or text values. We propose a novel form of Differential Dependencies (DDs), which specifies constraints on difference, called differential functions, instead of identification functions in traditional dependency notations like functional dependencies. Informally, a differential dependency states that if two tuples have distances on attributes X agreeing with a certain differential function, then their distances on attributes Y should also agree with the corresponding differential function on Y. For example, [date(≤ 7)] → [price(< 100)] states that the price difference of any two days within a week length should be no greater than 100 dollars. Such differential dependencies are useful in various applications, for example, violation detection, data partition, query optimization, record linkage, etc. In this article, we first address several theoretical issues of differential dependencies, including formal definitions of DDs and differential keys, subsumption order relation of differential functions, implication of DDs, closure of a differential function, a sound and complete inference system, and minimal cover for DDs. Then, we investigate a practical problem, that is, how to discover DDs and differential keys from a given dataset. Due to the intrinsic hardness, we develop several pruning methods to improve the discovery efficiency in practice. Finally, through an extensive experimental evaluation on real datasets, we demonstrate the discovery performance and the effectiveness of DDs in several real applications. © 2011 ACM.",Data dependencies; Differential dependencies,Character recognition; Data handling; Search engines; Semantics; Data dependencies; Data partition; Data sets; Differential dependencies; Differential functions; Experimental evaluation; Formal definition; Functional dependency; Inference systems; Intrinsic hardness; Numerical values; Order relation; Practical problems; Price difference; Pruning methods; Query optimization; Real applications; Real data sets; Record linkage; Differentiation (calculus)
The Monte Carlo database system: Stochastic analysis close to the data,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052344771&doi=10.1145%2f2000824.2000828&partnerID=40&md5=4bc8501688c87235be845e89789342e0,"The application of stochastic models and analysis techniques to large datasets is now commonplace. Unfortunately, in practice this usually means extracting data from a database system into an external tool (such as SAS, R, Arena, or Matlab), and then running the analysis there. This extract-and-model paradigm is typically error-prone, slow, does not support fine-grained modeling, and discourages what-if and sensitivity analyses. In this article we describe MCDB, a database system that permits a wide spectrum of stochastic models to be used in conjunction with the data stored in a large database, without ever extracting the data. MCDB facilitates in-database execution of tasks such as risk assessment, prediction, and imputation of missing data, as well as management of errors due to data integration, information extraction, and privacy-preserving data anonymization. MCDB allows a user to define ""random"" relations whose contents are determined by stochastic models. The models can then be queried using standard SQL. Monte Carlo techniques are used to analyze the probability distribution of the result of an SQL query over random relations. Novel ""tuple-bundle"" processing techniques can effectively control the Monte Carlo overhead, as shown in our experiments. © 2011 ACM.",MCDB; Relational database systems; Uncertainty,Data mining; Monte Carlo methods; Probability distributions; Relational database systems; Risk assessment; Sensitivity analysis; Stochastic systems; Uncertainty analysis; Analysis techniques; Data anonymization; Data integration; Error prones; External tools; Information Extraction; Large database; Large datasets; MCDB; Missing data; MONTE CARLO; Monte Carlo techniques; Privacy preserving; Processing technique; Random relation; Relational Database; SQL query; Stochastic analysis; Uncertainty; Wide spectrum; Stochastic models
Efficient similarity joins for near-duplicate detection,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052344031&doi=10.1145%2f2000824.2000825&partnerID=40&md5=e6a0662aebf121344ff8590de5ee4769,"With the increasing amount of data and the need to integrate data from multiple data sources, one of the challenging issues is to identify near-duplicate records efficiently. In this article, we focus on efficient algorithms to find a pair of records such that their similarities are no less than a given threshold. Several existing algorithms rely on the prefix filtering principle to avoid computing similarity values for all possible pairs of records. We propose new filtering techniques by exploiting the token ordering information; they are integrated into the existing methods and drastically reduce the candidate sizes and hence improve the efficiency. We have also studied the implementation of our proposed algorithm in stand-alone and RDBMSbased settings. Experimental results show our proposed algorithms can outperform previous algorithms on several real datasets. © 2011 ACM.",Near duplicate detection; Similarity join,Efficient algorithm; Filtering technique; Multiple data sources; Near-duplicate detection; Real data sets; Similarity join; Algorithms
Continuous nearest-neighbor search in the presence of obstacles,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960780753&doi=10.1145%2f1966385.1966387&partnerID=40&md5=08076db1466eaa5b9480ec8f20651b69,"Despite the ubiquity of physical obstacles (e.g., buildings, hills, and blindages, etc.) in the real world, most of spatial queries ignore the obstacles. In this article, we study a novel form of continuous nearest-neighbor queries in the presence of obstacles, namely continuous obstructed nearest-neighbor (CONN) search, which considers the impact of obstacles on the distance between objects. Given a data set P, an obstacle set O, and a query line segment q, in a two-dimensional space, a CONN query retrieves the nearest neighbor p ε P of each point p' on q according to the obstructed distance, the shortest path between p and p' without crossing any obstacle in O. We formalize CONN search, analyze its unique properties, and develop algorithms for exact CONN query-processing assuming that both P and O are indexed by conventional data-partitioning indices (e.g., R-trees). Our methods tackle CONN retrieval by performing a single query for the entire query line segment, and only process the data points and obstacles relevant to the final query result via a novel concept of control points and an efficient quadratic-based split point computation approach. Then, we extend our techniques to handle variations of CONN queries, including (1) continuous obstructed k nearest neighbor (COkNN) search which, based on obstructed distances, finds the k (≥1) nearest neighbors (NNs) to every point along q; and (2) trajectory obstructed k nearest-neighbor (TOkNN) search, which, according to obstructed distances, returns the k NNs for each point on an arbitrary trajectory (consisting of several consecutive line segments). Finally, we explore approximate COkNN (ACOkNN) retrieval. Extensive experiments with both real and synthetic datasets demonstrate the efficiency and effectiveness of our proposed algorithms under various experimental settings. © 2011 ACM.",Continuous nearest neighbor; Nearest neighbor; Obstacle; Spatial databases,Algorithms; Decision trees; Continuous nearest neighbors; Control point; Data partitioning; Data points; Data sets; K-nearest neighbors; Line segment; Nearest Neighbor search; Nearest neighbors; Nearest-neighbor query; Novel concept; Obstacle; Obstructed distance; Query results; R-trees; Shortest path; Spatial database; Spatial queries; Synthetic datasets; Two dimensional spaces; Trees (mathematics)
Closed world data exchange,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960768768&doi=10.1145%2f1966385.1966392&partnerID=40&md5=d498fcba1eaa39d59a93994d45a95a34,"Data exchange deals with translating data structured in some source format into data structured in some target format, given a specification of the relationship between the source and the target and possibly constraints on the target; and answering queries over the target in a way that is semantically consistent with the information in the source. Theoretical foundations of data exchange have been actively explored recently. It was also noticed that the standard semantics for query answering in data exchange may lead to counterintuitive or anomalous answers. In the present article, we explain that this behavior is due to the fact that solutions can contain invented information (information that is not related to the source instance), and that the presence of incomplete information in target instances has been ignored. In particular, proper query evaluation techniques for databases with nulls have not been used, and the distinction between closed and open world semantics has not been made. We present a concept of solutions, called CWA-solutions, that is based on the closed world assumption. For data exchange settings without constraints on the target, the space of CWA-solutions has two extreme points: the canonical universal solution (the maximal CWA-solution) and the core of the universal solutions (the minimal CWA-solution), both of them well studied in data exchange. In the presence of constraints on the target, the core of the universal solutions is still the minimal CWA-solution, but there may be no uniquemaximal CWA-solution.We show how to define the semantics of query-answering taking into account incomplete information, and show that some of the well-known anomalies go away with the new semantics. The article also contains results on the complexity of query-answering, upper approximations to queries (maybe-answers), and various extensions. © 2011 ACM.",Canonical universal solution; Certain answers; Closed world assumption; Core; Databases; Maybe answers; Null values; The chase,Certain answers; Closed world assumption; Core; Maybe answers; Null value; The chase; Universal solutions; Semantics
Design and analysis of a ranking approach to private location-based services,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960787246&doi=10.1145%2f1966385.1966388&partnerID=40&md5=456d04df970b874c3d50b6f5646752fb,"Users of mobile services wish to retrieve nearby points of interest without disclosing their locations to the services. This article addresses the challenge of optimizing the query performance while satisfying given location privacy and query accuracy requirements. The article's proposal, SpaceTwist, aims to offer location privacy for k nearest neighbor (kNN) queries at low communication cost without requiring a trusted anonymizer. The solution can be used with a conventional DBMS as well as with a server optimized for location-based services. In particular, we believe that this is the first solution that expresses the server-side functionality in a single SQL statement. In its basic form, SpaceTwist utilizes well-known incremental NN query processing on the server. When augmented with a server-side granular search technique, SpaceTwist is capable of exploiting relaxed query accuracy guarantees for obtaining better performance. We extend SpaceTwist with so-called ring ranking, which improves the communication cost, delayed termination, which improves the privacy afforded the user, and the ability to function in spatial networks in addition to Euclidean space. We report on analytical and empirical studies that offer insight into the properties of SpaceTwist and suggest that our proposal is indeed capable of offering privacy with very good performance in realistic settings. © 2011 ACM.",Location privacy; Mobile service,Anonymizer; Communication cost; Design and analysis; Empirical studies; Euclidean spaces; K-nearest neighbors; Location privacy; Location-Based Services; Mobile service; Points of interest; Query performance; Ranking approach; Search technique; Spatial network; Optimization
A relational approach to functional decomposition of logic circuits,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960787881&doi=10.1145%2f1966385.1966391&partnerID=40&md5=f50df3f4492be4b5e3d71316af7f8865,"Functional decomposition of Boolean functions has a profound influence on all quality aspects of costeffectively bimplementing modern digital systems and data-mining. The relational databases are multivalued tables,which include any truth tables of logic functions as special cases. In this article, we propose a relational database approach to the decomposition of logic circuits. The relational algebra consists of a set ofwell-defined algebraic operations that can be performed on multivalued tables. Our approach shows that the functional decomposition of logic circuits is similar to the normalization of relational databases; they are governed by the same concepts of functional dependency (FD) and multivalued dependency (MVD). The completeness of relational algebra demonstrated by our approach to functional decomposition reveals that the relational database is a fundamental computation model, the same as the Boolean logic circuit. © 2011 ACM.",Functional decomposition; Functional dependency (FD); Multivalued dependency (MVD); Partition; Relational data model,Algebra; Boolean functions; Database systems; Digital circuits; Finite difference method; Algebraic operations; Boolean logic circuits; Computation model; Digital system; Functional decomposition; Functional dependency; Logic functions; Multivalued dependency (MVD); Quality aspects; Relational algebra; Relational data model; Relational Database; Truth tables; Logic circuits
Asymptotically efficient algorithms for skyline probabilities of uncertain data,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960814826&doi=10.1145%2f1966385.1966390&partnerID=40&md5=74d7c940dfb1a78fbbf02c662a951038,"Skyline computation is widely used in multicriteria decision making. As research in uncertain databases draws increasing attention, skyline queries with uncertain data have also been studied. Some earlier work focused on probabilistic skylines with a given threshold; Atallah and Qi [2009] studied the problem to compute skyline probabilities for all instances of uncertain objects without the use of thresholds, and proposed an algorithm with subquadratic time complexity. In this work, we propose a new algorithm for computing all skyline probabilities that is asymptotically faster: worst-case O(n√nlog n) time and O(n) space for 2D data; O(n2-1/dlogd-1n) time and O(nlogd-2n) space for d-dimensional data. Furthermore, we study the online version of the problem: Given any query point p (unknown until the query time), return the probability that no instance in the given data set dominates p. We propose an algorithm for answering such an online query for d-dimensional data in O(n1-1/dlogd-1n) time after preprocessing the data in O(n2-1/dlogd-1) time and space. © 2011 ACM.",Probabilistic skyline; Uncertain data,Algorithms; Decision making; Probability; Asymptotically efficient; Data sets; Multi criteria decision making; Online versions; Probabilistic skyline; Query points; Query time; Skyline query; Time complexity; Uncertain database; Uncertain datas; Computational efficiency
Reverse data exchange: Coping with nulls,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960831063&doi=10.1145%2f1966385.1966389&partnerID=40&md5=6e13fea32a957dd1d4ff6c51b30fbf09,"An inverse of a schema mapping M is intended to undo what M does, thus providing a way to perform reverse data exchange. In recent years, three different formalizations of this concept have been introduced and studied, namely the notions of an inverse of a schema mapping, a quasi-inverse of a schema mapping, and a maximum recovery of a schema mapping. The study of these notions has been carried out in the context in which source instances are restricted to consist entirely of constants, while target instances may contain both constants and labeled nulls. This restriction on source instances is crucial for obtaining some of the main technical results about these three notions, but, at the same time, limits their usefulness, since reverse data exchange naturally leads to source instances that may contain both constants and labeled nulls. We develop a new framework for reverse data exchange that supports source instances that may contain nulls, and we thereby overcome the semantic mismatch between source and target instances of the previous formalizations. The development of this new framework requires a careful reformulation of all the important notions, including the notions of the identity schema mapping, inverse, and maximum recovery. To this effect, we introduce the notions of extended identity schema mapping, extended inverse, and maximum extended recovery, by making systematic use of the homomorphism relation on instances. We give results concerning the existence of extended inverses and of maximum extended recoveries, and results concerning their applications to reverse data exchange and query answering.Moreover,we show thatmaximum extended recoveries can be used to capture in a quantitative way, the amount of information loss embodied in a schema mapping specified by source-to-target tuple-generating dependencies. © 2011 ACM.",Chase; Data exchange; Data integration; Inverse; Maximum recovery; Model management; Quasi-inverse; Schema mapping,Semantics; Chase; Data integration; Inverse; Maximum recovery; Model management; Quasi-inverse; Schema mappings; Recovery
Relational languages and data models for continuous queries on sequences and data streams,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960822347&doi=10.1145%2f1966385.1966386&partnerID=40&md5=ea40ea0d0bb6441444aca3c4c8d4b1e0,"Most data stream management systems are based on extensions of the relational data model and query languages, but rigorous analyses of the problems and limitations of this approach, and how to overcome them, are still wanting. In this article, we elucidate the interaction between stream-oriented extensions of the relational model and continuous query language constructs, and show that the resulting expressive power problems are even more serious for data streams than for databases. In particular, we study the loss of expressive power caused by the loss of blocking query operators, and characterize nonblocking queries as monotonic functions on the database. Thus we introduce the notion of N B-completeness to assure that a query language is as suitable for continuous queries as it is for traditional database queries. We show that neither RA nor SQL are N B-complete on unordered sets of tuples, and the problem is even more serious when the datamodel is extended to support order-a sine-qua-non in data stream applications. The new limitations of SQL, compounded with well-known problems in applications such as sequence queries and data mining, motivate our proposal of extending the language with user-defined aggregates (UDAs). These can be natively coded in SQL, according to simple syntactic rules that set nonblocking aggregates apart from blocking ones. We first prove that SQL with UDAs is Turing complete. We then prove that SQL with monotonic UDAs and union operators can express all monotonic set functions computable by a Turing machine (N B-completeness) and finally extend this result to queries on sequences ordered by their timestamps. The proposed approach supports data streammodels that are more sophisticated than append-only relations, along with datamining queries, and other complex applications. © 2011 ACM.",Data streams; Expressivity; Queries,Computer hardware description languages; Data communication systems; Data mining; Information management; Models; Query processing; Search engines; Turing machines; Complex applications; Continuous queries; Continuous query languages; Data stream; Data stream management systems; Database queries; Expressive power; Expressivity; Monotonic functions; Non-blocking; Queries; Query operators; Relational data models; Relational languages; Relational Model; Rigorous analysis; Sequence queries; Set function; Syntactic rules; Time stamps; User-defined aggregates; Query languages
Generating sound workflow views for correct provenance analysis,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953235515&doi=10.1145%2f1929934.1929940&partnerID=40&md5=22d74410ca3b9cf58bd66fedd24fbf09,"Workflow views abstract groups of tasks in a workflow into high level composite tasks, in order to reuse subworkflows and facilitate provenance analysis. However, unless a view is carefully designed, it may not preserve the dataflow between tasks in the workflow, that is, it may not be sound. Unsound views can be misleading and cause incorrect provenance analysis. This article studies the problem of efficiently identifying and correcting unsound workflow views with minimal changes, and constructing minimal sound and elucidative workflow views with a set of user-specified relevant tasks. In particular, two related problems are investigated. First, given a workflow view, we wish to split each unsound composite task into the minimal number of tasks, such that the resulting view is sound. Second, given a workflow and a set of user specified relevant tasks, we generate a sound view, such that each composite task contains at most one relevant task, and the total number of tasks is minimized. We prove that both problems are NP-hard by reduction from independent set. We then propose two local optimality conditions (weak and strong) for each problem, and design polynomial time algorithms for both problems to meet these conditions. Experiments show that our proposed algorithms are reasonably effective and efficient. The proposed techniques are useful for view analysis/construction for not only workflows, but general networks as well. ©2011.",Network; Provenance; Soundness; View; Workflow,Polynomial approximation; Network; Provenance; Soundness; View; Workflow; Algorithms
Output privacy in data mining,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953253848&doi=10.1145%2f1929934.1929935&partnerID=40&md5=d8443d3c730e43cb896169de4694de7b,"Privacy has been identified as a vital requirement in designing and implementing data mining systems. In general, privacy preservation demands protecting both input and output privacy: the former refers to sanitizing the raw data itself before performing mining; while the latter refers to preventing the mining output (models or patterns) from malicious inference attacks. This article presents a systematic study on the problem of protecting output privacy in data mining, and particularly, stream mining: (i) we highlight the importance of this problem by showing that even sufficient protection of input privacy does not guarantee that of output privacy; (ii) we present a general inferencing and disclosure model that exploits the intrawindow and interwindow privacy breaches in stream mining output; (iii) we propose a light-weighted countermeasure that effectively eliminates these breaches without explicitly detecting them, while minimizing the loss of output accuracy; (iv) we further optimize the basic scheme by taking account of two types of semantic constraints, aiming at maximally preserving utility-related semantics while maintaining hard privacy guarantee; (v) finally, we conduct extensive experimental evaluation over both synthetic and real data to validate the efficacy of our approach. ©2011.",Data perturbation; Output privacy; Stream mining,Data mining; Hydraulics; Optimization; Semantics; Data mining system; Data perturbation; Experimental evaluation; Input and outputs; Output accuracy; Output privacy; Privacy breaches; Privacy preservation; Semantic constraints; Stream mining; Synthetic and real data; Systematic study; Data privacy
Instant anonymization,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953253398&doi=10.1145%2f1929934.1929936&partnerID=40&md5=148c8b3261af88d7e892a508e6ef5ce3,"Anonymization-based privacy protection ensures that data cannot be traced back to individuals. Researchers working in this area have proposed a wide variety of anonymization algorithms, many of which require a considerable number of database accesses. This is a problem of efficiency, especially when the released data is subject to visualization or when the algorithm needs to be run many times to get an acceptable ratio of privacy/utility. In this paper, we present two instant anonymization algorithms for the privacy metrics κ-anonymity and l-diversity. Proposed algorithms minimize the number of data accesses by utilizing the summary structure already maintained by the database management system for query selectivity. Experiments on real data sets show that in most cases our algorithm produces an optimal anonymization, and it requires a single scan of data as opposed to hundreds of scans required by the state-of-the-art algorithms. ©2011.",κ-anonymity; Algorithms; Ell-diversity; Privacy,Data visualization; Database systems; Information management; Search engines; Visualization; Anonymization; Data-base management systems; Database access; Ell-diversity; Number of datum; Privacy; Privacy metrics; Privacy protection; Query selectivity; Real data sets; Single scan; State-of-the-art algorithms; Summary structures; Algorithms
Querying XML data sources that export very large sets of views,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953246606&doi=10.1145%2f1929934.1929939&partnerID=40&md5=7f71788cb6271afb045ec4b52356000f,"We study the problem of querying XML data sources that accept only a limited set of queries, such as sources accessible by Web services which can implement very large (potentially infinite) families of XPath queries. To compactly specify such families of queries we adopt the Query Set Specifications, a formalism close to context-free grammars. We say that query Q is expressible by the specification P if it is equivalent to some expansion of P. Q is supported by P if it has an equivalent rewriting using some finite set of P's expansions. We study the complexity of expressibility and support and identify large classes of XPath queries for which there are efficient (PTIME) algorithms. Our study considers both the case in which the XML nodes in the results of the queries lose their original identity and the one in which the source exposes persistent node ids. ©2011.",Limited capabilities; Query rewriting; Semi-structured data; Views; XML,Specifications; Web services; Expressibility; Finite set; Large class; Limited capabilities; Query rewritings; Semi-structured data; Views; XML data sources; XPath queries; XML
Path-tree: An efficient reachability indexing scheme for large directed graphs,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953234479&doi=10.1145%2f1929934.1929941&partnerID=40&md5=575fc3f55fe9aeb790cabf5ae57872f5,"Reachability query is one of the fundamental queries in graph database. The main idea behind answering reachability queries is to assign vertices with certain labels such that the reachability between any two vertices can be determined by the labeling information. Though several approaches have been proposed for building these reachability labels, it remains open issues on how to handle increasingly large number of vertices in real-world graphs, and how to find the best tradeoff among the labeling size, the query answering time, and the construction time. In this article, we introduce a novel graph structure, referred to as pathtree, to help labeling very large graphs. The path-tree cover is a spanning subgraph of G in a tree shape. We show path-tree can be generalized to chain-tree which theoretically can has smaller labeling cost. On top of path-tree and chain-tree index, we also introduce a new compression scheme which groups vertices with similar labels together to further reduce the labeling size. In addition, we also propose an efficient incremental update algorithm for dynamic index maintenance. Finally, we demonstrate both analytically and empirically the effectiveness and efficiency of our new approaches. ©2011.",Graph indexing; Maximal directed spanning tree; Path-tree cover; Reachability queries; Transitive closure,Graph theory; Indexing (of information); Parallel architectures; Query languages; Query processing; Graph indexing; Maximal directed spanning tree; Path-tree cover; Reachability queries; Transitive closure; Trees (mathematics)
Query-preserving watermarking of relational databases and XML documents,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953235167&doi=10.1145%2f1929934.1929937&partnerID=40&md5=9217ebcae5a8b3587052816c29edd2b5,"Watermarking allows robust and unobtrusive insertion of information in a digital document. During the last few years, techniques have been proposed for watermarking relational databases or XML documents, where information insertion must preserve a specific measure on data (for example the mean and variance of numerical attributes). In this article we investigate the problem of watermarking databases or XML while preserving a set of parametric queries in a specified language, up to an acceptable distortion. We first show that unrestricted databases can not be watermarked while preserving trivial parametric queries. We then exhibit query languages and classes of structures that allow guaranteed watermarking capacity, namely 1) local query languages on structures with bounded degree Gaifman graph, and 2) monadic second-order queries on trees or treelike structures. We relate these results to an important topic in computational learning theory, the VC-dimension. We finally consider incremental aspects of query-preserving watermarking. ©2011.",VC-dimension; Watermarking,Computation; Mathematics; Trees; Computation theory; Query languages; Trees (mathematics); XML; Bounded degree; Computational learning theory; Digital Documents; Numerical attributes; Relational Database; Second orders; Tree-like structures; VC-dimension; Watermarking capacity; Watermarking relational database; Watermarking
Using structural information in XML keyword search effectively,2011,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953242508&doi=10.1145%2f1929934.1929938&partnerID=40&md5=dea079084cfe4fcb0bb1e98b97266d99,"The popularity of XML has exacerbated the need for an easy-to-use, high precision query interface for XML data. When traditional document-oriented keyword search techniques do not suffice, natural language interfaces and keyword search techniques that take advantage of XML structure make it very easy for ordinary users to query XML databases. Unfortunately, current approaches to processing these queries rely heavily on heuristics that are intuitively appealing but ultimately ad hoc. These approaches often retrieve false positive answers, overlook correct answers, and cannot rank answers appropriately. To address these problems for data-centric XML, we propose coherency ranking (CR), a domain- and database designindependent ranking method for XML keyword queries that is based on an extension of the concepts of data dependencies and mutual information.With coherency ranking, the results of a keyword query are invariant under a class of equivalency-preserving schema reorganizations. We analyze the way in which previous approaches to XML keyword search approximate coherency ranking, and present efficient algorithms to process queries and rank their answers using coherency ranking. Our empirical evaluation with two realworld XML data sets shows that coherency ranking has better precision and recall and provides better ranking than all previous approaches. ©2011.",Correlation Mining; Keyword Queries; XML,Algorithms; Query languages; Query processing; Search engines; Correlation mining; Data centric; Data dependencies; Efficient algorithm; Empirical evaluations; False positive; High precision; Keyword queries; Keyword search; Natural language interfaces; Precision and recall; Query interfaces; Ranking methods; Real-world; Structural information; XML data; XML database; XML
Foreword to TODS invited papers issue,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650651235&doi=10.1145%2f1862919.1862920&partnerID=40&md5=1d4de562f31220af5ff636247a4963bc,[No abstract available],,
Space-optimal heavy hitters with strong error bounds,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650668413&doi=10.1145%2f1862919.1862923&partnerID=40&md5=37b31bc9853899682fb4af5548d48647,"The problem of finding heavy hitters and approximating the frequencies of items is at the heart of many problems in data stream analysis. It has been observed that several proposed solutions to this problem can outperform their worst-case guarantees on real data. This leads to the question of whether some stronger bounds can be guaranteed. We answer this in the positive by showing that a class of counter-based algorithms (including the popular and very space-efficient FREQUENT and SPACESAVING algorithms) provides much stronger approximation guarantees than previously known. Specifically, we show that errors in the approximation of individual elements do not depend on the frequencies of the most frequent elements, but only on the frequency of the remaining tail. This shows that counter-based methods are the most space-efficient (in fact, space-optimal) algorithms having this strong error bound. This tail guarantee allows these algorithms to solve the sparse recovery problem. Here, the goal is to recover a faithful representation of the vector of frequencies, f .We prove that using space O(k), the algorithms construct an approximation f * to the frequency vector f so that the L1 error ∥ f - f * ∥1 is close to the best possible error min f′ ∥ f′ - f ∥1, where f′ ranges over all vectors with at most k non-zero entries. This improves the previously best known space bound of about O(klog n) for streams without element deletions (where n is the size of the domain from which stream elements are drawn). Other consequences of the tail guarantees are results for skewed (Zipfian) data, and guarantees for accuracy of merging multiple summarized streams. © 2010 ACM.",Frequency estimation; Heavy hitters; Streaming algorithms,Error analysis; Frequency estimation; Optimization; Vector spaces; Data stream; Error bound; Frequency vector; Heavy-hitter; Space bounds; Sparse recovery; Streaming algorithm; Approximation algorithms
Incremental XPath evaluation,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650663403&doi=10.1145%2f1862919.1862926&partnerID=40&md5=f92864334605d6296e3189328d212953,"Incremental view maintenance for XPath queries asks to maintain a materialized XPath view over an XML database. It assumes an underlying XML database D and a query Q. One is given a sequence of updates U to D, and the problem is to compute the result of Q(U(D)): the result of evaluating query Q on database D after having applied updates U. This article initiates a systematic study of the Boolean version of this problem. In the Boolean version, one only wants to know whether Q(U(D)) is empty or not. In order to quickly answer this question, we are allowed to maintain an auxiliary data structure. The complexity of the maintenance algorithms is measured in, (1) the size of the auxiliary data structure, (2) the worst-case time per update needed to compute Q(U(D)), and (3) the worst-case time per update needed to bring the auxiliary data structure up to date. We allow three kinds of updates: node insertion, node deletion, and node relabeling. Our main results are that downward XPath queries can be incrementally maintained in time O(depth(D) · poly(|Q|)) per update and conjunctive forward XPath queries in time O(depth(D) · log(width(D)) · poly(|Q|)) per update, where |Q| is the size of the query, and depth(D) and width(D) are the nesting depth and maximum number of siblings in database D, respectively. The auxiliary data structures for maintenance are linear in |D| and polynomial in |Q| in all these cases. © 2010 ACM.",View maintenance; XML; XPath,Data structures; Maintenance; Salinity measurement; XML; Auxiliary data structures; Incremental view maintenance; Maintenance algorithms; Materialized xpath views; Node deletion; Relabeling; Systematic study; View maintenance; XML database; XPath; XPath queries; Query languages
Relative information completeness,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650642617&doi=10.1145%2f1862919.1862924&partnerID=40&md5=5724a2fa85c83deab981dec79e993da9,"This article investigates the question of whether a partially closed database has complete information to answer a query. In practice an enterprise often maintains master data Dm, a closed-world database. We say that a database D is partially closed if it satisfies a set V of containment constraints of the form q(D) ⊆ p(Dm), where q is a query in a language LC and p is a projection query. The part of D not constrained by (Dm, V) is open, from which some tuples may be missing. The database D is said to be complete for a query Q relative to (D m, V) if for all partially closed extensions D′ of D, Q(D′) = Q(D), i.e., adding tuples to D either violates some constraints in V or does not change the answer to Q. We first show that the proposedmodel can also capture the consistency of data, in addition to its relative completeness. Indeed, integrity constraints studied for data consistency can be expressed as containment constraints. We then study two problems. One is to decide, given Dm, V, a query Q in a language LQ, and a partially closed database D, whether D is complete for Q relative to (Dm, V). The other is to determine, given Dm, V and Q, whether there exists a partially closed database that is complete for Q relative to (Dm, V).We establish matching lower and upper bounds on these problems for a variety of languages LQ and LC. We also provide characterizations for a database to be relatively complete, and for a query to allow a relatively complete database, when LQ and LC are conjunctive queries. © 2010 ACM.",Complexity; Incomplete information; Master data management; Partially closed databases; Relative completeness,Information management; Query languages; Complexity; Incomplete information; Master data management; Partially closed databases; Relative completeness; Query processing
I/O efficient algorithms for serial and parallel suffix tree construction,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650636112&doi=10.1145%2f1862919.1862922&partnerID=40&md5=fd92c099bbd527891812bb14bbdabe7d,"Over the past three decades, the suffix tree has served as a fundamental data structure in string processing. However, its widespread applicability has been hindered due to the fact that suffix tree construction does not scale well with the size of the input string. With advances in data collection and storage technologies, large strings have become ubiquitous, especially across emerging applications involving text, time series, and biological sequence data. To benefit from these advances, it is imperative that we have a scalable suffix tree construction algorithm. The past few years have seen the emergence of several disk-based suffix tree construction algorithms. However, construction times continue to be daunting - for example, indexing the entire human genome still takes over 30 hours on a system with 2 gigabytes of physical memory. In this article, we will empirically demonstrate and argue that all existing suffix tree construction algorithms have a severe limitation - to glean reasonable disk I/O efficiency, the input string being indexed must fit in main memory. This limitation is attributed to the poor locality exhibited by existing suffix tree construction algorithms and inhibits both sequential and parallel scalability. To deal with this limitation, we will show that through careful algorithm design, one of the simplest suffix tree construction algorithms can be rearchitected to build a suffix tree in a tiled manner, allowing the execution to operate within a fixed main memory budget when indexing strings of any size. We will also present a parallel extension of our algorithm that is designed for massively parallel systems like the IBM Blue Gene. An experimental evaluation will show that the proposed approach affords an improvement of several orders of magnitude in serial performance when indexing large strings. Furthermore, the proposed parallel extension is shown to be scalable - it is now possible to index the entire human genome on a 1024 processor IBM Blue Gene system in under 15 minutes. © 2010 ACM.",Disk-based; External memory; Genome indexing; Parallel; Sequence indexing; Suffix tree,Algorithms; Data structures; Disks (structural components); Genes; Indexing (of information); Supercomputers; Time series; Disk-based; External memory; Genome indexing; Parallel; Sequence indexing; Suffix-trees; Trees (mathematics)
Towards a theory of search queries,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650656200&doi=10.1145%2f1862919.1862925&partnerID=40&md5=2b3dce7e25d3c62812e09fbda0a7cab6,"The need to manage diverse information sources has triggered the rise of very loosely structured data models, known as dataspace models. Such information management systems must allow querying in simple ways, mostly by a form of searching. Motivated by these developments, we propose a theory of search queries in a general model of dataspaces. In this model, a dataspace is a collection of data objects, where each data object is a collection of data items. Basic search queries are expressed using filters on data items, following the basic model of Boolean search in information retrieval. We characterize semantically the class of queries that can be expressed by searching. We apply our theory to classical relational databases, where we connect search queries to the known class of fully generic queries, and to dataspaces where data items are formed by attribute-value pairs. We also extend our theory to a more powerful, associative form of searching, where one can ask for objects that are similar to objects satisfying given search conditions. Such associative search queries are shown to correspond to a very limited kind of joins.We show that the basic search language extended with associative search can exactly define the queries definable in a restricted fragment of the semijoin algebra working on an explicit relational representation of the dataspace. © 2010 ACM.",Dataspaces; Genericity; Search,Information management; Information retrieval; Associative Search; Attribute-value pairs; Basic models; Data items; Data objects; Data space; General model; Genericity; Information management systems; Information sources; Relational Database; Relational representations; Search; Search queries; Structured data; Data acquisition
An architecture for recycling intermediates in a column-store,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650660986&doi=10.1145%2f1862919.1862921&partnerID=40&md5=115beb45a07ccd9ef5d837a338899f96,"Automatic recycling of intermediate results to improve both query response time and throughput is a grand challenge for state-of-the-art databases. Tuples are loaded and streamed through a tuple-at-a-time processing pipeline, avoiding materialization of intermediates as much as possible. This limits the opportunities for reuse of overlapping computations to DBA-defined materialized views and function/result cache tuning. In contrast, the operator-at-a-time execution paradigm produces fully materialized results in each step of the query plan. To avoid resource contention, these intermediates are evicted as soon as possible. In this article we study an architecture that harvests the byproducts of the operator-at-a-time paradigm in a column-store system using a lightweight mechanism, the recycler. The key challenge then becomes the selection of the policies to admit intermediates to the resource pool, to determine their retention period, and devise the eviction strategy when facing resource limitations. The proposed recycling architecture has been implemented in an open-source system. An experimental analysis against the TPC-H ad-hoc decision support benchmark and a complex, real-world application (SkyServer) demonstrates its effectiveness in terms of self-organizing behavior and its significant performance gains. The results indicate the potentials of recycling intermediates and charts a route for further development of database kernels. © 2010 ACM.",Caching; Column-stores; Database kernels,Architecture; Benchmarking; Byproducts; Database systems; Decision support systems; Lakes; Cache tuning; Caching; Column-store; Column-stores; Database kernels; Decision supports; Execution paradigm; Experimental analysis; Further development; Grand Challenge; Intermediate results; Materialized view; Open source system; Performance Gain; Query response time; Real-world application; Resource contention; Resource limitations; Self-organizing behavior; Time processing; Recycling
Views and queries: Determinacy and rewriting,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955211754&doi=10.1145%2f1806907.1806913&partnerID=40&md5=df20f6560e89d6c1fe94e0764c125fd5,"We investigate the question of whether a query Q can be answered using a set V of views. We first define the problem in information-theoretic terms: we say that V determines Qif V provides enough information to uniquely determine the answer to Q. Next, we look at the problem of rewriting Q in terms of V using a specific language. Given a view language V and query language Q, we say that a rewriting language R is complete for V-to-Q rewritings if every Q ε Q can be rewritten in terms of Vε V using a query in R, whenever V determines Q. While query rewriting using views has been extensively investigated for some specific languages, the connection to the informationtheoretic notion of determinacy, and the question of completeness of a rewriting language have received little attention. In this article we investigate systematically the notion of determinacy and its connection to rewriting. The results concern decidability of determinacy for various view and query languages, as well as the power required of complete rewriting languages. We consider languages ranging from first-order to conjunctive queries. © 2010 ACM.",Algorithms; Design; Security; Theory; Verification,Computability and decidability; Information theory; Linguistics; Conjunctive queries; Design security; First-order; Query rewriting using views; Security; Specific languages; Query languages
Efficient and accurate nearest neighbor and closest pair search in high-dimensional space,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955220172&doi=10.1145%2f1806907.1806912&partnerID=40&md5=e11f1da3e80f776e37f0356c8b89226a,"Nearest Neighbor (NN) search in high-dimensional space is an important problem in many applications. From the database perspective, a good solution needs to have two properties: (i) it can be easily incorporated in a relational database, and (ii) its query cost should increase sublinearly with the dataset size, regardless of the data and query distributions. Locality-Sensitive Hashing (LSH) is a well-known methodology fulfilling both requirements, but its current implementations either incur expensive space and query cost, or abandon its theoretical guarantee on the quality of query results. Motivated by this, we improve LSH by proposing an access method called the Locality-Sensitive B-tree (LSB-tree) to enable fast, accurate, high-dimensional NN search in relational databases. The combination of several LSB-trees forms a LSB-forest that has strong quality guarantees, but improves dramatically the efficiency of the previous LSH implementation having the same guarantees. In practice, the LSB-tree itself is also an effective index which consumes linear space, supports efficient updates, and provides accurate query results. In our experiments, the LSB-tree was faster than: (i) iDistance (a famous technique for exact NN search) by two orders ofmagnitude, and (ii) MedRank (a recent approximate method with nontrivial quality guarantees) by one order of magnitude, and meanwhile returned much better results. As a second step, we extend our LSB technique to solve another classic problem, called Closest Pair (CP) search, in high-dimensional space. The long-term challenge for this problem has been to achieve subquadratic running time at very high dimensionalities, which fails most of the existing solutions. We show that, using a LSB-forest, CP search can be accomplished in (worst-case) time significantly lower than the quadratic complexity, yet still ensuring very good quality. In practice, accurate answers can be found using just two LSB-trees, thus giving a substantial reduction in the space and running time. In our experiments, our technique was faster: (i) than distance browsing (a well-known method for solving the problem exactly) by several orders of magnitude, and (ii) than D-shift (an approximate approach with theoretical guarantees in low-dimensional space) by one order of magnitude, and at the same time, outputs better results. © 2010 ACM.",Algorithms; Experimentation; Theory,Approximation theory; Database systems; Problem solving; Access methods; Approximate methods; Data set size; Effective index; High dimensional spaces; High dimensionality; High-dimensional; Linear spaces; Locality sensitive hashing; Low-dimensional spaces; Nearest neighbor searches; Nearest neighbors; Order of magnitude; Orders of magnitude; Quadratic complexity; Query costs; Query distributions; Query results; Relational Database; Running time; Substantial reduction; Experiments
A survey of B-tree locking techniques,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955207233&doi=10.1145%2f1806907.1806908&partnerID=40&md5=e5a838d6b0e689deb711679e491e41e2,"B-trees have been ubiquitous in database management systems for several decades, and they are used in other storage systems as well. Their basic structure and basic operations are well and widely understood including search, insertion, and deletion. Concurrency control of operations in B-trees, however, is perceived as a difficult subject with many subtleties and special cases. The purpose of this survey is to clarify, simplify, and structure the topic of concurrency control in B-trees by dividing it into two subtopics and exploring each of them in depth. © 2010 ACM.",Algorithms,Database systems; Management information systems; Surveys; Trees (mathematics); B trees; Basic operation; Basic structure; Data-base management systems; Locking technique; Storage systems; Concurrency control
Improving XML search by generating and utilizing informative result snippets,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955207478&doi=10.1145%2f1806907.1806911&partnerID=40&md5=e776b2291afe36c734758ef934d43ec5,"Snippets are used by almost every text search engine to complement the ranking scheme in order to effectively handle user searches, which are inherently ambiguous and whose relevance semantics are difficult to assess. Despite the fact that XML is a standard representation format of Web data, research on generating result snippets for XML search remains limited. To tackle this important yet open problem, in this article, we present a system eXtract which generates snippets for XML search results. We identify that a good XML result snippet should be a meaningful information unit of a small size that effectively summarizes this query result and differentiates it from others, according to which users can quickly assess the relevance of the query result. We have designed and implemented a novel algorithm to satisfy these requirements. Furthermore, we propose to cluster the query results based on their snippets. Since XML result clustering can only be done at query time, snippet-based clustering significantly improves the efficiency while compromising little clustering accuracy.We verified the efficiency and effectiveness of our approach through experiments. © 2010 ACM.",Algorithms; Design,Algorithms; Markup languages; Search engines; World Wide Web; Novel algorithm; Open problems; Query results; Query time; Search results; Small size; Text search; Web data; XML
Privacy-aware location data publishing,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955179024&doi=10.1145%2f1806907.1806910&partnerID=40&md5=ec37f52e3d46abf0b8b2e93ccd3e748c,"This article examines a new problem of k-anonymity with respect to a reference dataset in privacyaware location data publishing: given a user dataset and a sensitive event dataset, we want to generalize the user dataset such that by joining it with the event dataset through location, each event is covered by at least k users. Existing k-anonymity algorithms generalize every k user locations to the same vague value, regardless of the events. Therefore, they tend to overprotect against the privacy compromise and make the published data less useful. In this article, we propose a new generalization paradigm called local enlargement, as opposed to conventional hierarchy- or partition-based generalization. Local enlargement guarantees that user locations are enlarged just enough to cover all events k times, and thus maximize the usefulness of the published data. We develop an O(Hn)-approximate algorithm under the local enlargement paradigm, where n is the maximum number of events a user could possibly cover and Hn is the Harmonic number of n. With strong pruning techniques and mathematical analysis, we show that it runs efficiently and that the generalized user locations are up to several orders of magnitude smaller than those by the existing algorithms. In addition, it is robust enough to protect against various privacy attacks. © 2010 ACM.",Algorithms; Experimentation; Theory,Algorithms; Location; Approximate algorithms; Data sets; Harmonic number; K-Anonymity; Location data; Mathematical analysis; Orders of magnitude; Pruning techniques; User location; Data privacy
Towards a logical reconstruction of a theory for locally closed databases,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955177496&doi=10.1145%2f1806907.1806914&partnerID=40&md5=727101cf3d754c6c8f0b4280472f1ffa,"The Closed World Assumption (CWA) on databases expresses the assumption that an atom not in the database is false. This assumption is applicable only in cases where the database has complete knowledge about the domain of discourse. In this article, we investigate locally closed databases, that is: databases that are sound but partially incomplete about their domain. Such databases consist of a standard database instance, augmented with a collection of Local Closed World Assumptions (LCWAs). A LCWA is a ""local"" form of the CWA, expressing that a database relation is complete in a certain area, called a window of expertise. In this work, we study locally closed databases both from a knowledge representation and from a computational perspective. At the representation level, the approach taken in this article distinguishes between the data that is conveyed by a database and the metaknowledge about the area in which the data is complete. We study the semantics of the LCWA's and relate it to several knowledge representation formalisms. At the reasoning level, we study the complexity of, and algorithms for two basic reasoning tasks: computing certain and possible answers to queries and determining whether a database has complete knowledge on a query. As the complexity of these tasks is unacceptably high, we develop efficient approximate methods for query answering. We also prove that for useful classes of queries and locally closed databases, these methods are optimal, and thus they solve the original query in a tractable way. As a result, we obtain classes of queries and locally closed databases for which query answering is tractable. © 2010 ACM.",Theory,Knowledge representation; Query languages; Semantics; Approximate methods; Closed world assumption; Database relations; Knowledge representation formalism; Meta-knowledge; Query answering; Reasoning tasks; Theory; Query processing
Continuous online index tuning in moving object databases,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955208153&doi=10.1145%2f1806907.1806909&partnerID=40&md5=f870dfbb0a95f80d40b74660c64a617d,"In a Moving Object Database (MOD), the dataset, for example, the location of objects and their distribution, and the workload change frequently. Traditional static indexes are not able to cope well with such changes, that is, their effectiveness and efficiency are seriously affected. This calls for the development of novel indexes that can be reconfigured automatically based on the state of the system. In this article, we design and present the ST 2B-tree, a Self-Tunable Spatio-Temporal B+-tree index for MODs. In ST2B-tree, the data space is partitioned into regions of different density with respect to a set of reference points. Based on the density, objects in a region are managed using a grid of appropriate granularity; intuitively, a dense region employs a grid with fine granularity, while a sparse region uses a grid with coarse granularity. In this way, the ST2B-tree adapts itself to workload diversity in space. To enable online tuning, the ST2B-tree employs a ""multitree"" indexing technique. The underlying B+-tree is logically divided into two subtrees. Objects are dispatched to either subtree depending on their last update time. The two subtrees are rebuilt periodically and alternately. Whenever a subtree is rebuilt, it is tuned to optimize performance by picking an appropriate setting (e.g., the set of reference points and grid granularity) based on the most recent data and workload. To cut down the overhead of rebuilding, we propose an eager update technique to construct the subtree. Finally, we present a tuning framework for the ST2B-tree, where the tuning is conducted online and automatically without human intervention, and without interfering with the regular functions of the MOD. We have implemented the tuning framework and the ST2B-tree, and conducted extensive performance evaluations. The results show that the self-tuning mechanism minimizes the degradation of performance caused by workload changes without any noticeable overhead. © 2010 ACM.",Algorithms; Design; Experimentation; Performance,Algorithms; Design; Object-oriented databases; Different densities; Effectiveness and efficiencies; Experimentation; Moving object database; Performance; Performance evaluations; Self-tuning mechanisms; Workload diversities; Trees (mathematics)
Transparent anonymization: Thwarting adversaries who know the algorithm,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952081925&doi=10.1145%2f1735886.1735887&partnerID=40&md5=9a73eedf4d4379e6ad0f612c117e94c8,"Numerous generalization techniques have been proposed for privacy-preserving data publishing. Most existing techniques, however, implicitly assume that the adversary knows little about the anonymization algorithm adopted by the data publisher. Consequently, they cannot guard against privacy attacks that exploit various characteristics of the anonymization mechanism. This article provides a practical solution tothis problem. First, we propose an analytical model for evaluating disclosure risks, when an adversary knows everything in the anonymization process, except the sensitive values. Based on this model, we develop a privacy principle, transparent l-diversity, which ensures privacy protection against such powerful adversaries. We identify three algorithms that achieve transparent l-diversity, and verify their effectiveness and efficiency through extensive experiments with real data. © 2010 ACM.",Generalization; L-diversity; Privacy-preserving data publishing,Algorithms; Mathematical models; Publishing; Analytical model; Anonymization; Data publishing; Disclosure risk; Practical solutions; Privacy preserving; Privacy principle; Privacy protection; Data privacy
Inference of concise regular expressions and DTDs,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952077835&doi=10.1145%2f1735886.1735890&partnerID=40&md5=a5d846103b71870e9444b2a6ae36a688,"We consider the problem of inferring a concise Document Type Definition (DTD) for a given set of XML-documents, a problem that basically reduces to learning concise regular expressions from positive examples strings. We identify two classes of concise regular expressions-the single occurrence regular expressions (SOREs) and the chain regular expressions (CHAREs)-that capture the far majority of expressions used in practical DTDs. For the inference of SOREs we present several algorithms that first infer an automaton for a given set of example strings and then translate that automaton to a corresponding SORE, possibly repairing the automaton when no equivalent SORE can be found. In the process, we introduce a novel automaton to regular expression rewrite technique which is of independent interest. When only a very small amount of XML data is available, however (for instance when the data is generated by Web service requests or by answers to queries), these algorithms produce regular expressions that are too specific. Therefore, we introduce a novel learning algorithm crx that directly infers CHAREs (which form a subclass of SOREs) without going through an automaton representation. We show that crx performs very well within its target class on very small datasets. © 2010 ACM.",Regular expressions; Schema inference; XML,Learning algorithms; Markup languages; Robots; Translation (languages); XML; Document type definition; Positive examples; Regular expressions; Schema inference; Small data set; Target class; XML data; Inference engines
Querying and repairing inconsistent numerical databases,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952042489&doi=10.1145%2f1735886.1735893&partnerID=40&md5=be63eafd3609f7c4be19a655e0eef4f2,"The problem of extracting consistent information from relational databases violating integrity constraints on numerical data is addressed. In particular, aggregate constraints defined as linear inequalities on aggregate-sum queries on input data are considered. The notion of repair as consistent set of updates at attribute-value level is exploited, and the characterization of several data-complexity issues related to repairing data and computing consistent query answers is provided. Moreover, a method for computing ""reasonable"" repairs of inconsistent numerical databases is provided, for a restricted but expressive class of aggregate constraints. Several experiments are presented which assess the effectiveness of the proposed approach in real-life application scenarios. © 2010 ACM.",Aggregate constraints; Consistent query answer; Inconsistent databases; Repairs,Aggregates; Database systems; Aggregate constraint; Complexity issues; Consistent query answer; Inconsistent database; Input datas; Integrity constraints; Linear inequalities; Numerical data; Real-life applications; Relational Database; Sum-query; Repair
Optimal matching between spatial datasets under capacity constraints,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952028212&doi=10.1145%2f1735886.1735888&partnerID=40&md5=42ab0583bb2f743a0aedd28c98b35c2a,"Consider a set of customers (e.g., WiFi receivers) and a set of service providers (e.g., wireless access points), where each provider has a capacity and the quality of service offered to its customers is anti-proportional to their distance. The Capacity Constrained Assignment (CCA) is a matching between the two sets such that (i) each customer is assigned to at most one provider, (ii) every provider serves no more customers than its capacity, (iii) the maximum possible number of customers are served, and (iv) the sum of Euclidean distances within the assigned provider-customer pairs is minimized. Although max-flow algorithms are applicable to this problem, they require the complete distance-based bipartite graph between the customer and provider sets. For large spatial datasets, this graph is expensive to compute and it may be too large to fit in main memory. Motivated by this fact, we propose efficient algorithms for optimal assignment that employ novel edge-pruning strategies, based on the spatial properties of the problem. Additionally, we develop incremental techniques that maintain an optimal assignment (in the presence of updates) with a processing cost several times lower than CCA recomputation from scratch. Finally, we present approximate (i.e., suboptimal) CCA solutions that provide a tunable trade-off between result accuracy and computation cost, abiding by theoretical quality guarantees. A thorough experimental evaluation demonstrates the efficiency and practicality of the proposed techniques. © 2010 ACM.",Optimal assignment; Spatial databases,Algorithms; Customer satisfaction; Optimization; Quality of service; Bipartite graphs; Capacity constraints; Computation costs; Distance-based; Efficient algorithm; Euclidean distance; Experimental evaluation; Incremental techniques; Main memory; Max flow algorithm; Optimal assignment; Optimal matching; Processing costs; Pruning strategy; Recomputation; Service provider; Set of customers; Spatial database; Spatial datasets; Spatial properties; Wi-Fi receivers; Wireless access points; Sales
Personalizing queries based on networks of composite preferences,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952029785&doi=10.1145%2f1735886.1735892&partnerID=40&md5=47052235526d2331e1dd95badcfbde82,"People's preferences are expressed at varying levels of granularity and detail as a result of partial or imperfect knowledge. One may have some preference for a general class of entities, for example, liking comedies, and another one for a fine-grained, specific class, such as disliking recent thrillers with Al Pacino. In this article, we are interested in capturing such complex, multi-granular preferences for personalizing database queries and in studying their impact on query results. We organize the collection of one's preferences in a preference network (a directed acyclic graph), where each node refers to a subclass of the entities that its parent refers to, and whenever they both apply, more specific preferences override more generic ones. We study query personalization based on networks of preferences and provide efficient algorithms for identifying relevant preferences, modifying queries accordingly, and processing personalized queries. Finally, we present results of both synthetic and real-user experiments, which: (a) demonstrate the efficiency of our algorithms, (b) provide insight as to the appropriateness of the proposed preference model, and (c) show the benefits of query personalization based on composite preferences compared to simpler preference representations. © 2010 ACM.",Personalization; Preference modeling; Preference networks,Database queries; Directed acyclic graphs; Efficient algorithm; General class; Personalizations; Preference modeling; Preference models; Preference representation; Query personalization; Query results; Algorithms
Encryption policies for regulating access to outsourced data,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952046953&doi=10.1145%2f1735886.1735891&partnerID=40&md5=df19ffa46903aa2d7b013d83cd294280,"Current access control models typically assume that resources are under the strict custody of a trusted party which monitors each access request to verify if it is compliant with the specified access control policy. There are many scenarios where this approach is becoming no longer adequate. Many clear trends in Web technology are creating a need for owners of sensitive information to manage access to it by legitimate users using the services of honest but curious third parties, that is, parties trusted with providing the required service but not authorized to read the actual data content. In this scenario, the data owner encrypts the data before outsourcing and stores them at the server. Only the data owner and users with knowledge of the key will be able to decrypt the data. Possible access authorizations are to be enforced by the owner. In this article, we address the problem of enforcing selective access on outsourced data without need of involving the owner in the access control process. The solution puts forward a novel approach that combines cryptography with authorizations, thus enforcing access control via selective encryption. The article presents a formal model for access control management and illustrates how an authorization policy can be translated into an equivalent encryption policy while minimizing the amount of keys and cryptographic tokens to be managed. The article also introduces a two-layer encryption approach that allows the data owner to outsource, besides the data, the complete management of the authorization policy itself, thus providing efficiency and scalability in dealing with policy updates. We also discuss experimental results showing that our approach is able to efficiently manage complex scenarios. © 2010 ACM.",Data outsourcing; Encryption policy; Privacy,Access control; Data privacy; Outsourcing; Security systems; Access control models; Access control policies; Authorization policy; Data contents; Data outsourcing; Formal model; Legitimate users; Outsource; Selective encryption; Sensitive informations; Third parties; Trusted party; Two layers; Web technologies; Cryptography
Comments on an integrated efficient solution for computing frequent and top-k elements in data streams,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952019982&doi=10.1145%2f1735886.1735894&partnerID=40&md5=77e829acd93a658e711fd5b6e471e8a3,"We investigate a well-known algorithm, Space-Saving [Metwally et al. 2006], which has been proven efficient and effective at mining frequent elements in data streams. We discovered an error in one of the theorems in Metwally et al. [2006]. Experiments are conducted to illustrate the error. © 2010 ACM.",Approximate queries; Data streams; Frequent elements; Top-k elements; Zipfian distributions,Data communication systems; Approximate query; Data stream; Data streams; K elements; Data mining
Return specification inference and result clustering for keyword search on XML,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952066916&doi=10.1145%2f1735886.1735889&partnerID=40&md5=afa15c5389fb59a48d2486b59b1ed7cd,"Keyword search enables Web users to easily access XML data without the need to learn a structured query language and to study possibly complex data schemas. Existing work has addressed the problem of selecting qualified data nodes that match keywords and connecting them in a meaningful way, in the spirit of inferring the where clause in XQuery. However, how to infer the return clause for keyword searches is an open problem. To address this challenge, we present a keyword search engine for data-centric XML, XSeek, to infer the semantics of the search and identify return nodes effectively. XSeek recognizes possible entities and attributes inherently represented in the data. It also distinguishes between predicates and return specifications in query keywords. Then based on the analysis of both XML data structures and keyword patterns, XSeek generates return nodes. Furthermore, when the query is ambiguous and it is hard or impossible to determine the desirable return information, XSeek clusters the query results according to their semantics based on the user-specified granularity, and enables the user to easily browse and select the desired ones. Extensive experimental studies show the effectiveness and efficiency of XSeek. © 2010 ACM.",Keyword search; Result clustering; XML,Cluster analysis; Content based retrieval; Data structures; Markup languages; Query languages; Search engines; Semantics; Specifications; XML; Complex data; Data centric; Experimental studies; Keyword search; Open problems; Query results; Schemas; Structured Query Language; Web users; XML data; World Wide Web
An information-theoretic analysis of worst-case redundancy in database design,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249147548&doi=10.1145%2f1670243.1670248&partnerID=40&md5=3dcd587d4c88eae815a714d67b1d8545,"Normal forms that guide the process of database schema design have several key goals such as elimination of redundancies and preservation of integrity constraints, such as functional dependencies. It has long been known that complete elimination of redundancies and complete preservation of constraints cannot be achieved simultaneously. In this article, we use a recently introduced information-theoretic framework, and provide a quantitative analysis of the redundancy/integrity preservation trade-off, and give techniques for comparing different schema designs in terms of the amount of redundancy they carry. The main notion of the information-theoretic framework is that of an information content of each datum in an instance (which is a number in [0, 1]): the closer to 1, the less redundancy it carries. We start by providing a combinatorial criterion that lets us calculate, for a relational schema with functional dependencies, the lowest information content in its instances. This indicates how good the schema design is in terms of allowing redundant information. We then study the normal form 3NF, which tolerates some redundancy to guarantee preservation of functional dependencies. The main result provides a formal justification for normal form 3NF by showing that this normal form pays the smallest possible price, in terms of redundancy, for achieving dependency preservation. We also give techniques for quantitative comparison of different normal forms based on the redundancy they tolerate. © 2010 ACM.",Database design; Functional dependency; Redundancy; Third normal form (3NF),Database systems; Design; Information theory; Quality assurance; Case redundancy; Database design; Database schemas; Functional dependency; Information contents; Information-theoretic analysis; Integrity constraints; Normal form; Quantitative analysis; Quantitative comparison; Redundant informations; Relational schemas; Schema design; Third normal form (3NF); Redundancy
Automatic virtual machine configuration for database workloads,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249140703&doi=10.1145%2f1670243.1670250&partnerID=40&md5=b6e4afece37fd34af7be2b7db7139e1d,"Virtual machine monitors are becoming popular tools for the deployment of database management systems and other enterprise software. In this article, we consider a common resource consolidation scenario in which several database management system instances, each running in a separate virtual machine, are sharing a common pool of physical computing resources. We address the problem of optimizing the performance of these database management systems by controlling the configurations of the virtual machines in which they run. These virtual machine configurations determine how the shared physical resources will be allocated to the different database system instances. We introduce a virtualization design advisor that uses information about the anticipated workloads of each of the database systems to recommend workload-specific configurations offline. Furthermore, runtime information collected after the deployment of the recommended configurations can be used to refine the recommendation and to handle changes in the workload. To estimate the effect of a particular resource allocation on workload performance, we use the query optimizer in a new what-if mode. We have implemented our approach using both PostgreSQL and DB2, and we have experimentally evaluated its effectiveness using DSS and OLTP workloads. © 2010 ACM.",Virtual machine configuration; Virtualization,Management; Management information systems; Common resources; Data-base management systems; Database workload; Enterprise software; Offline; Optimizers; Physical computing; Physical resources; PostgreSQL; Run-time information; Running-in; Virtual machine configuration; Virtual machine monitors; Virtual machines; Virtualizations; Database systems
Analyses of multi-level and multi-component compressed bitmap indexes,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249148544&doi=10.1145%2f1670243.1670245&partnerID=40&md5=426da7d52578c12afe086083228fa69f,"Bitmap indexes are known as the most effective indexing methods for range queries on append-only data, and many different bitmap indexes have been proposed in the research literature. However, only two of the simplest ones are used in commercial products. To better understand the benefits offered by the more sophisticated variations, we conduct an analytical comparison of well-known bitmap indexes, most of which are in the class of multi-component bitmap indexes. Our analysis is the first to fully incorporate the effects of compression on their performance. We produce closed-form formulas for both the index sizes and the query processing costs for the worst cases. One surprising finding is that the two simple indexes are in fact the best among multi-component indexes. Additionally, we investigate a number of novel variations in a class of multi-level indexes, and find that they answer queries faster than the best of multi-component indexes. More specifically, some two-level indexes are predicted by analyses and verified with experiments to be 5 to 10 times faster than well-known indexes. Furthermore, these two-level indexes have the optimal computational complexity for answering queries. © 2010 ACM.",Compression; Multi-component bitmap index; Multi-level bitmap index; Performance analysis,Computational complexity; Answering queries; Bitmap indexes; Closed form; Commercial products; Indexing methods; Multi-level; Multicomponents; Optimal computational complexity; Performance analysis; Range query; Worst case; Indexing (of information)
Reverse skyline search in uncertain databases,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249161360&doi=10.1145%2f1670243.1670246&partnerID=40&md5=d9e6379472e617d2bf14c75ae924dc66,"Reverse skyline queries over uncertain databases have many important applications such as sensor data monitoring and business planning. Due to the wide existence of uncertainty in many real-world data, answering reverse skyline queries accurately and efficiently over uncertain data has become increasingly important. In this article, we formalize the probabilistic reverse skyline query over uncertain data, in both monochromatic and bichromatic cases, and propose effective pruning methods, namely spatial pruning and probabilistic pruning, to reduce the search space of the reverse skyline query processing. Moreover, efficient query procedures have been presented seamlessly integrating the proposed pruning methods. Furthermore, a novel query type, namely Probabilistic Reverse Furthest Skyline (PRFS) query, is proposed and tackled under the larger, the better dominance semantics of skyline. Variants of probabilistic reverse skyline have been proposed and tackled, including those that return objects with top-k highest probabilities and that retrieve top-k reverse skylines. Extensive experiments demonstrated the efficiency and effectiveness of our approaches with various experimental settings. © 2010 ACM.",Bichromatic reverse skyline; Monochromatic reverse skyline; Uncertain database,Indexing (of information); Monochromators; Strategic planning; Bichromatic reverse skyline; Business planning; Monochromatic reverse skyline; Pruning methods; Query types; Real world data; Search spaces; Sensor data; Skyline query; Uncertain database; Uncertain datas; Database systems
The pq-gram distance between ordered labeled trees,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249122728&doi=10.1145%2f1670243.1670247&partnerID=40&md5=f2029bade98111da3e64290d97fd0fdc,"When integrating data from autonomous sources, exact matches of data items that represent the same real-world object often fail due to a lack of common keys. Yet in many cases structural information is available and can be used to match such data. Typically the matching must be approximate since the representations in the sources differ. We propose pq-grams to approximately match hierarchical data from autonomous sources and define the pq-gram distance between ordered labeled trees as an effective and efficient approximation of the fanout weighted tree edit distance. We prove that the pq-gram distance is a lower bound of the fanout weighted tree edit distance and give a normalization of the pq-gram distance for which the triangle inequality holds. Experiments on synthetic and real-world data (residential addresses and XML) confirm the scalability of our approach and show the effectiveness of pq-grams. © 2010 ACM.",Approximate matching; Database algorithms; Distance metric; Hierarchical data; Similarity search; Tree edit distance; XML,Markup languages; Approximate matching; Data items; Database algorithm; Distance metrics; Exact match; Fan-out; Hierarchical data; Lower bounds; Ordered labeled trees; Real world data; Real-world objects; Structural information; Triangle inequality; Weighted tree; XML
Optimal algorithms for evaluating rank joins in database systems,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249131966&doi=10.1145%2f1670243.1670249&partnerID=40&md5=9efaff7351fdebf54d117bf16f796834,"In the rank join problem, we are given a set of relations and a scoring function, and the goal is to return the join results with the top k scores. It is often the case in practice that the inputs may be accessed in ranked order and the scoring function is monotonic. These conditions allow for efficient algorithms that solve the rank join problem without reading all of the input. In this article, we present a thorough analysis of such rank join algorithms. A strong point of our analysis is that it is based on a more general problem statement than previous work, making it more relevant to the execution model that is employed by database systems. One of our results indicates that the well-known HRJN algorithm has shortcomings, because it does not stop reading its input as soon as possible. We find that it is NP-hard to overcome this weakness in the general case, but cases of limited query complexity are tractable. We prove the latter with an algorithm that infers provably tight bounds on the potential benefit of reading more input in order to stop as soon as possible. As a result, the algorithm achieves a cost that is within a constant factor of optimal. © 2010 ACM.",,Computational complexity; Database systems; Constant factors; Efficient algorithm; Execution model; NP-hard; Optimal algorithm; Potential benefits; Problem statement; Query complexity; Rank-join algorithms; Scoring functions; Tight bound; Algorithms
Supporting views in data stream management systems,2010,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249116911&doi=10.1145%2f1670243.1670244&partnerID=40&md5=7864e07c14b0e04d9558cfbe1a93eb90,"In relational database management systems, views supplement basic query constructs to cope with the demand for higher-level views of data. Moreover, in traditional query optimization, answering a query using a set of existing materialized views can yield a more efficient query execution plan. Due to their effectiveness, views are attractive to data stream management systems. In order to support views over streams, a data stream management system should employ a closed (or composable) continuous query language. A closed query language is a language in which query inputs and outputs are interpreted in the same way, hence allowing query composition. This article introduces the Synchronized SQL (or SyncSQL) query language that defines a data stream as a sequence of modify operations against a relation. SyncSQL enables query composition through the unified interpretation of query inputs and outputs. An important issue in continuous queries over data streams is the frequency by which the answer gets refreshed and the conditions that trigger the refresh. Coarser periodic refresh requirements are typically expressed as sliding windows. In this article, the sliding window approach is generalized by introducing the synchronization principle that empowers SyncSQL with a formal mechanism to express queries with arbitrary refresh conditions. After introducing the semantics and syntax, we lay the algebraic foundation for SyncSQL and propose a query-matching algorithm for deciding containment of SyncSQL expressions. Then, the article introduces the Nile-SyncSQL prototype to support SyncSQL queries. Nile-SyncSQL employs a pipelined incremental evaluation paradigm in which the query pipeline consists of a set of differential operators. A cost model is developed to estimate the cost of SyncSQL query execution pipelines and to choose the best execution plan from a set of different plans for the same query. An experimental study is conducted to evaluate the performance of Nile-SyncSQL. The experimental results illustrate the effectiveness of Nile-SyncSQL and the significant performance gains when views are enabled in data stream management systems. © 2010 ACM.",Data streams; Expression matching; Incremental evaluation; Query language; Views,Computer hardware description languages; Cost benefit analysis; Data communication systems; Decoding; Differential equations; Gene expression; Hydraulics; Linguistics; Management information systems; Mathematical operators; Pipelines; Query processing; Relational database systems; Windows; Best executions; Continuous queries; Continuous query languages; Cost models; Data stream; Data stream management systems; Differential operators; Experimental studies; Expression matching; Incremental evaluation; Matching algorithm; Materialized view; Performance Gain; Query composition; Query execution; Query execution plan; Query optimization; Query pipeline; Relational database management systems; Sliding Window; Query languages
Serializable isolation for snapshot databases,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149142210&doi=10.1145%2f1620585.1620587&partnerID=40&md5=876e10290506096d606de8d52e9e0a46,"Many popular database management systems implement a multiversion concurrency control algorithm called snapshot isolation rather than providing full serializability based on locking. There are well-known anomalies permitted by snapshot isolation that can lead to violations of data consistency by interleaving transactions that would maintain consistency if run serially. Until now, the only way to prevent these anomalies was to modify the applications by introducing explicit locking or artificial update conflicts, following careful analysis of conflicts between all pairs of transactions. This article describes a modification to the concurrency control algorithm of a database management system that automatically detects and prevents snapshot isolation anomalies at runtime for arbitrary applications, thus providing serializable isolation. The new algorithm preserves the properties that make snapshot isolation attractive, including that readers do not block writers and vice versa. An implementation of the algorithm in a relational DBMS is described, along with a benchmark and performance study, showing that the throughput approaches that of snapshot isolation in most cases. © 2009 ACM.",Multiversion concurrency control; Serializability; Snapshot isolation,Algorithms; Database systems; Management; Management information systems; Concurrency control algorithms; Data consistency; Data-base management systems; Multiversion concurrency control; Performance study; Relational DBMS; Runtimes; Serializability; Snapshot isolation; Concurrency control
Static analysis of active XML systems,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149136072&doi=10.1145%2f1620585.1620590&partnerID=40&md5=ca170b336a6d2fad7e6ae59ee0c5a720,"Active XML is a high-level specification language tailored to data-intensive, distributed, dynamic Web services. Active XML is based on XML documents with embedded function calls. The state of a document evolves depending on the result of internal function calls (local computations) or external ones (interactions with users or other services). Function calls return documents that may be active, and so may activate new subtasks. The focus of this article is on the verification of temporal properties of runs of Active XML systems, specified in a tree-pattern-based temporal logic, Tree-LTL, which allows expressing a rich class of semantic properties of the application. The main results establish the boundary of decidability and the complexity of automatic verification of Tree-LTL properties. © 2009 ACM.",Automatic verification; Web services; Workflows; XML,Computability and decidability; Gene expression; High level languages; Management; Markup languages; Specification languages; Temporal logic; Web services; Automatic verification; Based on XML documents; Embedded function; Function calls; High level specification; Internal function; Local computation; LTL property; Semantic properties; Subtasks; Temporal property; Work-flows; XML system; XML
Foreword to TODS SIGMOD/PODS 2008 special issue,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149131053&doi=10.1145%2f1620585.1620586&partnerID=40&md5=5c4ebc9e8b99a5df2e3c76b4cadf96f4,[No abstract available],,
The recovery of a schema mapping: Bringing exchanged data back,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149121534&doi=10.1145%2f1620585.1620589&partnerID=40&md5=1fbc76dcbc4c5683f24b2a87af17ea9e,"A schema mapping is a specification that describes how data from a source schema is to be mapped to a target schema. Once the data has been transferred from the source to the target, a natural question is whether one can undo the process and recover the initial data, or at least part of it. In fact, it would be desirable to find a reverse schema mapping from target to source that specifies how to bring the exchanged data back. In this article, we introduce the notion of a recovery of a schema mapping: it is a reverse mapping, M′ for a mapping M, that recovers sound data with respect to M. We further introduce an order relation on recoveries. This allows us to choose mappings that recover the maximum amount of sound information. We call such mappings maximum recoveries. We study maximum recoveries in detail, providing a necessary and sufficient condition for their existence. In particular, we prove that maximum recoveries exist for the class of mappings specified by FO-to-CQ source-to-target dependencies. This class subsumes the class of source-to-target tuple-generating dependencies used in previous work on data exchange. For the class of mappings specified by FO-to-CQ dependencies, we provide an exponential-time algorithm for computing maximum recoveries, and a simplified version for full dependencies that works in quadratic time. We also characterize the language needed to express maximum recoveries, and we include a detailed comparison with the notion of inverse (and quasi-inverse) mapping previously proposed in the data exchange literature. In particular, we show that maximum recoveries strictly generalize inverses. We finally study the complexity of some decision problems related to the notions of recovery and maximum recovery. © 2009 ACM.",Data exchange; Data integration; Inverse; Maximum recovery; Metadata management; Recovery; Schema mapping,Computation theory; Data handling; Mapping; Metadata; Targets; Data exchange; Data integration; Decision problems; Exponential-time algorithms; Metadata management; Order relation; Quadratic time; Quasi-inverse; Reverse mapping; Schema mappings; Sound data; Sufficient conditions; Recovery
From XQuery to relational logics,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149137272&doi=10.1145%2f1620585.1620592&partnerID=40&md5=a412d78f1c9762b863eeee4c33b60d77,"Predicate logic has long been seen as a good foundation for querying relational data. This is embodied in the correspondence between relational calculus and first-order logic, and can also be seen in mappings from fragments of the standard relational query language SQL to extensions of first-order logic (e.g. with counting). A key question is what is the analog to this correspondence for querying tree-structured data, as seen, for example, in XML documents. We formalize this as the question of the appropriate logical query language for defining transformations on tree-structured data. The predominant practitioner paradigm for defining such transformations is top-down tree building. This is embodied by the XQuery query language, which builds the output tree in parallel starting at the root, based on variable bindings and nodeset queries in the XPath language. The goal of this article is to compare the expressiveness of top-down tree-building languages based on a benchmark of predicate logic. We start by giving a formalized XQuery XQ that can serve as a representative of the top-down approach. We show that all queries in XQ with only atomic equality are equivalent to first-order interpretations, an analog to first-order logic (FO) in the setting of transformations of tree-structured data. We then consider fragments of atomic XQ. We identify a fragment that maps efficiently into first-order, a fragment that maps into existential first-order logic, and a fragment that maps into the navigationally two-variable fragment of first-order logican analog of two-variable logic in the setting where data values are unbounded. When XQ is considered with deep equality, we find that queries can be translated into FO with counting (FO(Cnt)). Translations from XQ to logical languages on relations have a number of consequences. We use them to derive complexity bounds for XQ fragments, and to bound the Boolean expressiveness of XQ fragments. © 2009 ACM.",,Formal logic; Linguistics; Translation (languages); Complexity bounds; Data values; Extensions of first-order logic; First order logic; First-order; Logical language; Predicate logic; Relational calculus; Relational data; Relational logic; Relational query languages; Top-down approach; Top-down trees; Tree-structured data; Variable binding; Variable fragment; Query languages
Casper*: Query processing for location services without compromising privacy,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149103618&doi=10.1145%2f1620585.1620591&partnerID=40&md5=82f6d177512eb4022b5c206c1dd4efe9,"In this article, we present a new privacy-aware query processing framework, Capser*, in which mobile and stationary users can obtain snapshot and/or continuous location-based services without revealing their private location information. In particular, we propose a privacy-aware query processor embedded inside a location-based database server to deal with snapshot and continuous queries based on the knowledge of the user's cloaked location rather than the exact location. Our proposed privacy-aware query processor is completely independent of how we compute the user's cloaked location. In other words, any existing location anonymization algorithms that blur the user's private location into cloaked rectilinear areas can be employed to protect the user's location privacy. We first propose a privacy-aware query processor that not only supports three new privacy-aware query types, but also achieves a trade-off between query processing cost and answer optimality. Then, to improve system scalability of processing continuous privacy-aware queries, we propose a shared execution paradigm that shares query processing among a large number of continuous queries. The proposed scalable paradigm can be tuned through two parameters to trade off between system scalability and answer optimality. Experimental results show that our query processor achieves high quality snapshot and continuous location-based services while supporting queries and/or data with cloaked locations. © 2009 ACM.",Continuous queries; Location privacy; Location-based services; Privacy-aware query processing,Ad hoc networks; Query processing; Scalability; Transfer cases (vehicles); Continuous locations; Continuous queries; Data-base servers; Execution paradigm; High quality; Location anonymization; Location based; Location information; Location privacy; Location services; Location-Based Services; Optimality; Query processor; Query types; Rectilinear area; System scalability; Trade off; Two parameter; Location
Relational query coprocessing on graphics processors,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149104641&doi=10.1145%2f1620585.1620588&partnerID=40&md5=4cd5fec3d0c34980e2000a9df5dc75c8,"Graphics processors (GPU s) have recently emerged as powerful coprocessors for general purpose computation. Compared with commodity CPU s, GPU s have an order of magnitude higher computation power as well as memory bandwidth. Moreover, new-generation GPUs allow writes to random memory locations, provide efficient interprocessor communication through on-chip local memory, and support a general purpose parallel programming model. Nevertheless, many of the GPU features are specialized for graphics processing, including the massively multithreaded architecture, the Single-Instruction- Multiple-Data processing style, and the execution model of a single application at a time. Additionally, GPU s rely on a bus of limited bandwidth to transfer data to and from the CPU, do not allow dynamic memory allocation from GPU kernels, and have little hardware support for write conflicts. Therefore, a careful design and implementation is required to utilize the GPU for coprocessing database queries. In this article, we present our design, implementation, and evaluation of an in-memory relational query coprocessing system, GDB, on the GPU. Taking advantage of the GPU hardware features, we design a set of highly optimized data-parallel primitives such as split and sort, and use these primitives to implement common relational query processing algorithms. Our algorithms utilize the high parallelism as well as the high memory bandwidth of the GPU, and use parallel computation and memory optimizations to effectively reduce memory stalls. Furthermore, we propose coprocessing techniques that take into account both the computation resources and the GPU-CPU data transfer cost so that each operator in a query can utilize suitable processorsthe CPU, the GPU, or bothfor an optimized overall performance. We have evaluated our GDB system on a machine with an Intel quad-core CPU and an NVIDIA GeForce 8800 GTX GPU. Our workloads include microbenchmark queries on memory-resident data as well as TPC-H queries that involve complex data types and multiple query operators on data sets larger than the GPU memory. Our results show that our GPU-based algorithms are 2 - 27x faster than their optimized CPU-based counterparts on in-memory data. Moreover, the performance of our coprocessing scheme is similar to, or better than, both the GPU-only and the CPU-only schemes. © 2009 ACM.",Graphics processors; Join; Parallel processing; Primitive; Relational database; Sort,Bandwidth; Computational efficiency; Data transfer; Database systems; Mathematical operators; Optimization; Parallel programming; Program processors; Co-processors; Complex data; Computation power; Computation resources; Coprocessing; Data parallel; Data sets; Database queries; Dynamic memory allocation; Execution model; General purpose; GPU-based algorithms; Graphics processing; Graphics processor; Hardware features; Hardware supports; High memory bandwidth; Inter processor communication; Limited bandwidth; Local memories; Memory bandwidths; Memory locations; Memory optimization; Memory-resident; Micro-benchmark; Multiple queries; Multithreaded architecture; On chips; Order of magnitude; Parallel Computation; Parallel processing; Parallel programming model; Relational Database; Relational queries; Computer graphics equipment
Incorporating constraints in probabilistic XML,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349104151&doi=10.1145%2f1567274.1567280&partnerID=40&md5=60a7ffa6c799c6b2cd44ffa1c1e8b01a,"Constraints are important, not only for maintaining data integrity, but also because they capture natural probabilistic dependencies among data items. A probabilistic XML database (PXDB) is the probability subspace comprising the instances of a p-document that satisfy a set of constraints. In contrast to existing models that can express probabilistic dependencies, it is shown that query evaluation is tractable in PXDBs. The problems of sampling and determining well-definedness (i.e., whether the aforesaid subspace is nonempty) are also tractable. Furthermore, queries and constraints can include the aggregate functions count, max, min, and ratio. Finally, this approach can be easily extended to allow a probabilistic interpretation of constraints. © 2009 ACM.",Constraints; Probabilistic databases; Probabilistic XML; Sampling probabilistic data,Database systems; XML; Aggregate function; Constraints; Data integrity; Data items; Probabilistic databases; Probabilistic interpretation; Probabilistic XML; Query evaluation; Markup languages
Keyword search over relational tables and streams,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349144021&doi=10.1145%2f1567274.1567279&partnerID=40&md5=ae0cb66df1655bd41a7fa3e8e90b233c,"Relational Keyword Search (R-KWS) provides an intuitive way to query relational data without requiring SQL, or knowledge of the underlying schema. In this article we describe a comprehensive framework for R-KWS covering snapshot queries on conventional tables and continuous queries on relational streams. Our contributions are summarized as follows: (i) We provide formal semantics, addressing the temporal validity and order of results, spanning uniformly over tables and streams; (ii) we investigate two general methodologies for query processing, graph based and operator based, that resolve several problems of previous approaches; and (iii) we develop a range of algorithms and optimizations covering both methodologies. We demonstrate the effectiveness of R-KWS, as well as the significant performance benefits of the proposed techniques, through extensive experiments with static and streaming datasets. © 2009 ACM.",Data graph; Data streams; Query processing; Relational databases; Search,Data communication systems; Formal methods; Mathematical operators; Online searching; Query processing; Continuous queries; Data graph; Data sets; Data streams; Formal Semantics; Graph-based; Keyword search; Performance benefits; Relational data; Relational databases; Relational tables; Search; Snapshot queries; Semantics
Small synopses for group-by query verification on outsourced data streams,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349124128&doi=10.1145%2f1567274.1567277&partnerID=40&md5=59a6fcea7f9421b2df602ea3e5539f40,"Due to the overwhelming flow of information in many data stream applications, data outsourcing is a natural and effective paradigm for individual businesses to address the issue of scale. In the standard data outsourcing model, the data owner outsources streaming data to one or more third-party servers, which answer queries posed by a potentially large number of clients on the data owner's behalf. Data outsourcing intrinsically raises issues of trust, making outsourced query assurance on data streams a problem with important practical implications. Existing solutions proposed in this model all build upon cryptographic primitives such as signatures and collision-resistant hash functions, which only work for certain types of queries, for example, simple selection/aggregation queries. In this article, we consider another common type of queries, namely, GROUP BY, SUM queries, which previous techniques fail to support. Our new solutions are not based on cryptographic primitives, but instead use algebraic and probabilistic techniques to compute a small synopsis on the true query result, which is then communicated to the client so as to verify the correctness of the query result returned by the server. The synopsis uses a constant amount of space irrespective of the result size, has an extremely small probability of failure, and can be maintained using no extra space when the query result changes as elements stream by. We then generalize our synopsis to allow some tolerance on the number of erroneous groups, in order to support semantic load shedding on the server. When the number of erroneous groups is indeed tolerable, the synopsis can be strengthened so that we can locate and even correct these errors. Finally, we implement our techniques and perform an empirical evaluation using live network traffic. © 2009 ACM.",Data streams; Outsourcing; Synopses,Hash functions; Internet; Outsourcing; Servers; Collision-resistant hash functions; Cryptographic primitives; Data outsourcing; Data stream; Data streams; Empirical evaluations; Live network traffic; New solutions; Probabilistic technique; Probability of failure; Query results; Semantic load shedding; Streaming data; Sum-query; Synopses; Data communication systems
Processing spatial skyline queries in both vector spaces and spatial network databases,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349157204&doi=10.1145%2f1567274.1567276&partnerID=40&md5=68f0961b7053ef87b6c3275902e3dd19,"In this article, we first introduce the concept of Spatial Skyline Queries (SSQ). Given a set of data points P and a set of query points Q, each data point has a number of derived spatial attributes each of which is the point's distance to a query point. An SSQ retrieves those points of P which are not dominated by any other point in P considering their derived spatial attributes. The main difference with the regular skyline query is that this spatial domination depends on the location of the query points Q. SSQ has application in several domains such as emergency response and online maps. The main intuition and novelty behind our approaches is that we exploit the geometric properties of the SSQ problem space to avoid the exhaustive examination of all the point pairs in P and Q. Consequently, we reduce the complexity of SSQ search from O(P2Q) to O(S2C + P), where S and C are the solution size and the number of vertices of the convex hull of Q, respectively. Considering Euclidean distance, we propose two algorithms, B2S2 and VS2, for static query points and one algorithm, VCS2, for streaming Q whose points change location over time (e.g., are mobile). VCS 2 exploits the pattern of change in Q to avoid unnecessary recomputation of the skyline and hence efficiently perform updates. We also propose two algorithms, SNS2 and VSNS2, that compute the spatial skyline with respect to the network distance in a spatial network database. Our extensive experiments using real-world datasets verify that both R-tree-based B2S2 and Voronoi-based VS2 outperform the best competitor approach in terms of both processing time and I/O cost. Furthermore, their output computed based on Euclidean distance is a good approximation of the spatial skyline in network space. For accurate computation of spatial skylines in network space, our experiments showed the superiority of VSNS2 over SNS2. © 2009 ACM.",Spatial databases; Spatial skyline; Voronoi diagrams,Competition; Computational efficiency; Database systems; Decision trees; Indexing (of information); Wireless networks; Convex hull; Data points; Emergency response; Euclidean distance; Geometric properties; In-network; Network distance; Online maps; Problem space; Processing Time; Query points; Real-world datasets; Recomputation; Skyline query; Spatial attribute; Spatial databases; Spatial network database; Spatial skyline; Tree-based; Vector spaces; Voronoi; Voronoi diagrams; Set theory
Input-sensitive scalable continuous join query processing,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349133459&doi=10.1145%2f1567274.1567275&partnerID=40&md5=b9c5ab208d7982b3da9ec45b9e883f8c,"This article considers the problem of scalably processing a large number of continuous queries. Our approach, consisting of novel data structures and algorithms and a flexible processing framework, advances the state-of-the-art in several ways. First, our approach is query sensitive in the sense that it exploits potential overlaps in query predicates for efficient group processing. We partition the collection of continuous queries into groups based on the clustering patterns of the query predicates, and apply specialized processing strategies to heavily clustered groups (or hotspots). We show how to maintain the hotspots efficiently, and use them to scalably process continuous select-join, band-join, and window-join queries. Second, our approach is also data sensitive, in the sense that it makes cost-based decisions on how to process each incoming tuple based on its characteristics. Experiments demonstrate that our approach can improve the processing throughput by orders of magnitude. © 2009 ACM.",Continuous queries; Data streams; Event matching; Publish/subscribe,Data structures; Continuous queries; Data streams; Event matching; Flexible processing; Group processing; Hotspots; Orders of magnitude; Publish/subscribe; Query predicates; Data communication systems
Semantics and complexity of SPARQL,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349088138&doi=10.1145%2f1567274.1567278&partnerID=40&md5=1ca39094b7dfb610ebd3098cd278c87d,"SPARQL is the standard language for querying RDF data. In this article, we address systematically the formal study of the database aspects of SPARQL, concentrating in its graph pattern matching facility. We provide a compositional semantics for the core part of SPARQL, and study the complexity of the evaluation of several fragments of the language. Among other complexity results, we show that the evaluation of general SPARQL patterns is PSPACE-complete. We identify a large class of SPARQL patterns, defined by imposing a simple and natural syntactic restriction, where the query evaluation problem can be solved more efficiently. This restriction gives rise to the class of well-designed patterns. We show that the evaluation problem is coNP-complete for well-designed patterns. Moreover, we provide several rewriting rules for well-designed patterns whose application may have a considerable impact in the cost of evaluating SPARQL queries. © 2009 ACM.",Complexity; Query language; RDF; Semantic Web; SPARQL,Linguistics; Pattern matching; Semantic Web; Semantics; Complexity; Complexity results; Compositional semantics; Core part; Evaluation problems; Graph pattern matching; Large class; Query evaluation; RDF; RDF data; Rewriting rules; SPARQL; Query languages
A framework for efficient data anonymization under privacy and accuracy constraints,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68649129111&doi=10.1145%2f1538909.1538911&partnerID=40&md5=29dba727d6cad2cc3b6cad1852bdd3dc,"Recent research studied the problem of publishing microdata without revealing sensitive information, leading to the privacy-preserving paradigms of k-anonymity and l-diversity. k-anonymity protects against the identification of an individual's record. l-diversity, in addition, safeguards against the association of an individual with specific sensitive information. However, existing approaches suffer from at least one of the following drawbacks: (i) l-diversification is solved by techniques developed for the simpler k-anonymization problem, causing unnecessary information loss. (ii) The anonymization process is inefficient in terms of computational and I/O cost. (iii) Previous research focused exclusively on the privacy-constrained problem and ignored the equally important accuracy-constrained (or dual) anonymization problem. In this article, we propose a framework for efficient anonymization of microdata that addresses these deficiencies. First, we focus on one-dimensional (i.e., single-attribute) quasi-identifiers, and study the properties of optimal solutions under the k-anonymity and l-diversity models for the privacy-constrained (i.e., direct) and the accuracy-constrained (i.e., dual) anonymization problems. Guided by these properties, we develop efficient heuristics to solve the one-dimensional problems in linear time. Finally, we generalize our solutions to multidimensional quasi-identifiers using space-mapping techniques. Extensive experimental evaluation shows that our techniques clearly outperform the existing approaches in terms of execution time and information loss. © 2009 ACM.",Anonymity; Privacy,One dimensional; Anonymity; Anonymization; Constrained problem; Data anonymization; Execution time; Experimental evaluation; Information loss; K-Anonymity; K-anonymization; Linear time; Mapping techniques; Microdata; One-dimensional problem; Optimal solutions; Privacy; Privacy and accuracy; Privacy preserving; Sensitive informations; Data privacy
ODE: Ontology-assisted data extraction,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549083531&doi=10.1145%2f1538909.1538914&partnerID=40&md5=f1efa04f231c14606d0b57b38fe00f65,"Online databases respond to a user query with result records encoded in HTML files. Data extraction, which is important for many applications, extracts the records from the HTML files automatically. We present a novel data extraction method, ODE (Ontology-assisted Data Extraction), which automatically extracts the query result records from the HTML pages. ODE first constructs an ontology for a domain according to information matching between the query interfaces and query result pages from different Web sites within the same domain. Then, the constructed domain ontology is used during data extraction to identify the query result section in a query result page and to align and label the data values in the extracted records. The ontology-assisted data extraction method is fully automatic and overcomes many of the deficiencies of current automatic data extraction methods. Experimental results show that ODE is extremely accurate for identifying the query result section in an HTML page, segmenting the query result section into query result records, and aligning and labeling the data values in the query result records. © 2009 ACM.",Data value alignment; Domain ontology; Label assignment,Data mining; Extraction; HTML; Ontology; Ordinary differential equations; Data extraction; Data values; Domain ontologies; HTML files; Online database; Query interfaces; Query results; User query; Query processing
Snapshot isolation and integrity constraints in replicated databases,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549115322&doi=10.1145%2f1538909.1538913&partnerID=40&md5=18abf8463d27198ac863b6ee44e73365,"Database replication is widely used for fault tolerance and performance. However, it requires replica control to keep data copies consistent despite updates. The traditional correctness criterion for the concurrent execution of transactions in a replicated database is 1-copy-serializability. It is based on serializability, the strongest isolation level in a nonreplicated system. In recent years, however, Snapshot Isolation (SI), a slightly weaker isolation level, has become popular in commercial database systems. There exist already several replica control protocols that provide SI in a replicated system. However, most of the correctness reasoning for these protocols has been rather informal. Additionally, most of the work so far ignores the issue of integrity constraints. In this article, we provide a formal definition of 1-copy-SI using and extending a well-established definition of SI in a nonreplicated system. Our definition considers integrity constraints in a way that conforms to the way integrity constraints are handled in commercial systems. We discuss a set of necessary and sufficient conditions for a replicated history to be producible under 1-copy-SI. This makes our formalism a convenient tool to prove the correctness of replica control algorithms. © 2009 ACM.",Integrity constraints; Replication; Snapshot isolation,Algorithms; Fault tolerance; Fault tolerant computer systems; Quality assurance; Commercial database system; Commercial systems; Concurrent execution; Correctness criterion; Database replication; Formal definition; Integrity constraints; Isolation level; Replica control; Replicated database; Replicated systems; Replication; Serializability; Snapshot isolation; Sufficient conditions; Database systems
Efficient reasoning about a robust XML key fragment,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67651242794&doi=10.1145%2f1538909.1538912&partnerID=40&md5=62965e364e6d3a25f5127689bcc305d2,"We review key constraints in the context of XML as introduced by Buneman et al. We demonstrate that: (1) one of the proposed inference rules is not sound in general, and (2) the inference rules are incomplete for XML key implication, even for nonempty sets of simple key paths. This shows, in contrast to earlier statements, that the axiomatizability of XML keys is still open, and efficient algorithms for deciding their implication still need to be developed. Solutions to these problems have a wide range of applications including consistency validation, XML schema design, data exchange and integration, consistent query answering, XML query optimization and rewriting, and indexing. In this article, we investigate the axiomatizability and implication problem for XML keys with nonempty sets of simple key paths. In particular, we propose a set of inference rules that is indeed sound and complete for the implication of such XML keys. We demonstrate that this fragment is robust by showing the duality of XML key implication to the reachability problem of fixed nodes in a suitable digraph. This enables us to develop a quadratic-time algorithm for deciding implication, and shows that reasoning about this XML key fragment is practically efficient. Therefore, XML applications can be unlocked effectively since they benefit not only from those XML keys specified explicitly by the data designer but also from those that are specified implicitly. © 2009 ACM.",Axiomatization; Implication; Reachability; XML data; XML key,Electronic data interchange; Axiomatization; Implication; Reachability; XML data; XML key; XML
Anonymization-based attacks in privacy-preserving data publishing,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549087035&doi=10.1145%2f1538909.1538910&partnerID=40&md5=25b3f8242cbf51586ce36c9e4d3b4ce6,"Data publishing generates much concern over the protection of individual privacy. Recent studies consider cases where the adversary may possess different kinds of knowledge about the data. In this article, we show that knowledge of the mechanism or algorithm of anonymization for data publication can also lead to extra information that assists the adversary and jeopardizes individual privacy. In particular, all known mechanisms try to minimize information loss and such an attempt provides a loophole for attacks. We call such an attack a minimality attack. In this article, we introduce a model called m-confidentiality which deals with minimality attacks, and propose a feasible solution. Our experiments show that minimality attacks are practical concerns on real datasets and that our algorithm can prevent such attacks with very little overhead and information loss. © 2009 ACM.",Data publishing; K-anonymity; L-diversity; Minimality attack; Privacy preservation,Publishing; Data publishing; K-anonymity; L-diversity; Minimality attack; Privacy preservation; Data privacy
Robust approximate aggregation in sensor data management systems,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649233397&doi=10.1145%2f1508857.1508863&partnerID=40&md5=c46d3ca7750f6bc63f2a3b25a2cd7de8,"In the emerging area of sensor-based systems, a significant challenge is to develop scalable, fault-tolerant methods to extract useful information from the data the sensors collect. An approach to this data management problem is the use of sensor database systems, which allow users to perform aggregation queries such as MIN, COUNT, and AVG on the readings of a sensor network. In addition, more advanced queries such as frequency counting and quantile estimation can be supported. Due to energy limitations in sensor-based networks, centralized data collection is generally impractical, so most systems use in-network aggregation to reduce network traffic. However, even these aggregation strategies remain bandwidth-intensive when combined with the fault-tolerant, multipath routing methods often used in these environments. To avoid this expense, we investigate the use of approximate in-network aggregation using small sketches. We present duplicate-insensitive sketching techniques that can be implemented efficiently on small sensor devices with limited hardware support and we analyze both their performance and accuracy. Finally, we present an experimental evaluation that validates the effectiveness of our methods. © 2009 ACM.",Aggregation; Approximation algorithms; Sensor databases; Sketches; Synopses,Data acquisition; Database systems; Frequency estimation; Internet; Maximum likelihood estimation; Sensor networks; Sensors; Aggregation; Aggregation queries; Aggregation strategy; Data collection; Data management problems; Energy limitations; Experimental evaluation; Fault-tolerant; Fault-tolerant method; Frequency counting; Hardware supports; In-network aggregation; Multi path routing; Network traffic; Quantile estimation; Sensor based systems; Sensor data management; Sensor databases; Sensor device; Sketches; Synopses; Approximation algorithms
The design of a query monitoring system,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649227399&doi=10.1145%2f1508857.1508858&partnerID=40&md5=e163c9eb57a7b0906a656687413d1c67,"Query monitoring refers to the problem of observing and predicting various parameters related to the execution of a query in a database system. In addition to being a useful tool for database users and administrators, it can also serve as an information collection service for resource allocation and adaptive query processing techniques. In this article, we present a query monitoring system from the ground up, describing various new techniques for query monitoring, their implementation inside a real database system, and a novel interface that presents the observed and predicted information in an accessible manner. To enable this system, we introduce several lightweight online techniques for progressively estimating and refining the cardinality of different relational operators using information collected at query execution time. These include binary and multiway joins as well as typical grouping operations and combinations thereof. We describe the various algorithms used to efficiently implement estimators and present the results of an evaluation of a prototype implementation of our framework in an open-source data management system. Our results demonstrate the feasibility and practical utility of the approach presented herein. © 2009 ACM.",Progress estimation; Query monitoring,Database systems; Distributed computer systems; Estimation; Monitoring; Planning; Resource allocation; Software prototyping; Adaptive query processing; Cardinality; Information collections; Multi-way join; Online technique; Open-source; Progress estimation; Prototype implementations; Query execution time; Query monitoring; Relational operator; Indexing (of information)
TuG synopses for approximate query answering,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649214261&doi=10.1145%2f1508857.1508860&partnerID=40&md5=1a9c40443a3f6622a430fc0596fad927,"This article introduces the Tuple Graph (TuG) synopses, a new class of data summaries that enable accurate approximate answers for complex relational queries. The proposed summarization framework adopts a semi-structured view of the relational database, modeling a relational data set as a graph of tuples and join queries as graph traversals, respectively. The key idea is to approximate the structure of the induced data graph in a concise synopsis, and to approximate the answer to a query by performing the corresponding traversal over the summarized graph. We detail the (TuG) synopsis model that is based on this novel approach, and we describe an efficient and scalable construction algorithm for building accurate (TuG) within a specific storage budget. We validate the performance of (TuG) with an extensive experimental study on real-life and synthetic datasets. Our results verify the effectiveness of (TuG) in generating accurate approximate answers for complex join queries, and demonstrate their benefits over existing summarization techniques. © 2009 ACM.",Approximate query processing; Data synopses; Selectivity estimation,Query processing; Approximate query answering; Approximate query processing; AS graph; Construction algorithms; Data graph; Data summaries; Data synopses; Experimental studies; New class; Relational data; Relational Database; Relational queries; Selectivity estimation; Semi-structured; Synthetic datasets; Data processing
Semantics and implementation of continuous sliding window queries over data streams,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649197680&doi=10.1145%2f1508857.1508861&partnerID=40&md5=83203ec161cd5e3cdd46d8d83d7371dd,"In recent years the processing of continuous queries over potentially infinite data streams has attracted a lot of research attention. We observed that the majority of work addresses individual stream operations and system-related issues rather than the development of a general-purpose basis for stream processing systems. Furthermore, example continuous queries are often formulated in some declarative query language without specifying the underlying semantics precisely enough. To overcome these deficiencies, this article presents a consistent and powerful operator algebra for data streams which ensures that continuous queries have well-defined, deterministic results. In analogy to traditional database systems, we distinguish between a logical and a physical operator algebra. While the logical algebra specifies the semantics of the individual operators in a descriptive but concrete way over temporal multisets, the physical algebra provides efficient implementations in the form of stream-to-stream operators. By adapting and enhancing research from temporal databases to meet the challenging requirements in streaming applications, we are able to carry over the conventional transformation rules from relational databases to stream processing. For this reason, our approach not only makes it possible to express continuous queries with a sound semantics, but also provides a solid foundation for query optimization, one of the major research topics in the stream community. Since this article seamlessly explains the steps from query formulation to query execution, it outlines the innovative features and operational functionality implemented in our state-of-the-art stream processing infrastructure. © 2009 ACM.",Continuous queries; Data streams; Query optimization; Semantics,Algebra; Data communication systems; Optimization; Query languages; Relational database systems; Semantics; Continuous queries; Data stream; Data streams; Declarative query languages; Efficient implementation; Multi-sets; Operator algebras; Physical algebra; Query execution; Query formulation; Query optimization; Relational Database; Research topics; Sliding Window; Stream communities; Stream operations; Stream processing; Stream processing systems; Streaming applications; Temporal Database; Transformation rules; Industrial research
Detecting outlying properties of exceptional objects,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649205496&doi=10.1145%2f1508857.1508864&partnerID=40&md5=4654d1607dc2245c9327180fdfe66bf4,"Assume you are given a data population characterized by a certain number of attributes. Assume, moreover, you are provided with the information that one of the individuals in this data population is abnormal, but no reason whatsoever is given to you as to why this particular individual is to be considered abnormal. In several cases, you will be indeed interested in discovering such reasons. This article is precisely concerned with this problem of discovering sets of attributes that account for the (a priori stated) abnormality of an individual within a given dataset. A criterion is presented to measure the abnormality of combinations of attribute values featured by the given abnormal individual with respect to the reference population. In this respect, each subset of attributes is intended to somehow represent a property of individuals. We distinguish between global and local properties. Global properties are subsets of attributes explaining the given abnormality with respect to the entire data population. With local ones, instead, two subsets of attributes are singled out, where the former one justifies the abnormality within the data subpopulation selected using the values taken by the exceptional individual on those attributes included in the latter one. The problem of individuating abnormal properties with associated explanations is formally stated and analyzed. Such a formal characterization is then exploited in order to devise efficient algorithms for detecting both global and local forms of most abnormal properties. The experimental evidence, which is accounted for in the article, shows that the algorithms are both able to mine meaningful information and to accomplish the computational task by examining a negligible fraction of the search space. © 2009 ACM.",Data mining; Knowledge discovery; Outlier characterization,Information management; Mining; Apriori; Attribute values; Computational task; Data population; Data sets; Efficient algorithm; Experimental evidence; Global properties; Knowledge discovery; Local property; Outlier characterization; Search spaces; Population statistics
Efficient query processing on graph databases,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649202530&doi=10.1145%2f1508857.1508859&partnerID=40&md5=3534350dbd29356bd707f4ad63d78371,"We study the problem of processing subgraph queries on a database that consists of a set of graphs. The answer to a subgraph query is the set of graphs in the database that are supergraphs of the query. In this article, we propose an efficient index, FG*-index, to solve this problem. The cost of processing a subgraph query using most existing indexes mainly consists of two parts: the index probing cost and the candidate verification cost. Index probing is to find the query in the index, or to find the graphs from which we can generate a candidate answer set for the query. Candidate verification is to test whether each graph in the candidate set is indeed a supergraph of the query. We design FG*-index to minimize these two costs as follows. FG*-index consists of three components: the FG-index, the feature-index, and the FAQ-index. First, the FG-index employs the concept of Frequent subGraph (FG) to allow the set of queries that are FGs to be answered without candidate verification. We call this set of queries FG-queries. We can enlarge the set of FG-queries so that more queries can be answered without candidate verification; however, a larger set of FG-queries implies a larger FG-index and hence the index probing cost also increases. We propose the feature-index to reduce the index probing cost. The feature-index uses features to filter false results that are matched in the FG-index, so that we can quickly find the truly matching graphs for a query. For processing non-FG-queries, we propose the FAQ-index, which is dynamically constructed from the set of Frequently Asked non-FG-Queries (FAQs). Using the FAQ-index, verification is not required for processing FAQs and only a small number of candidates need to be verified for processing non-FG-queries that are not frequently asked. Finally, a comprehensive set of experiments verifies that query processing using FG*-index is up to orders of magnitude more efficient than state-of-the-art indexes and it is also more scalable. © 2009 ACM.",Frequent subgraphs; Graph databases; Graph indexing; Graph query processing,Costs; Indexing (of information); Query processing; Answer set; Frequent subgraph; Frequent subgraphs; Graph database; Graph databases; Graph indexing; Graph query processing; Matching graph; Orders of magnitude; Subgraph; Supergraph; Three component; Indexing (materials working)
A quality-aware optimizer for information extraction,2009,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63749118928&doi=10.1145%2f1508857.1508862&partnerID=40&md5=765a52de6ff013968c36765f93e13a45,"A large amount of structured information is buried in unstructured text. Information extraction systems can extract structured relations from the documents and enable sophisticated, SQL-like queries over unstructured text. Information extraction systems are not perfect and their output has imperfect precision and recall (i.e., contains spurious tuples and misses good tuples). Typically, an extraction system has a set of parameters that can be used as knobs to tune the system to be either precision- or recall-oriented. Furthermore, the choice of documents processed by the extraction system also affects the quality of the extracted relation. So far, estimating the output quality of an information extraction task has been an ad hoc procedure, based mainly on heuristics. In this article, we show how to use Receiver Operating Characteristic (ROC) curves to estimate the extraction quality in a statistically robust way and show how to use ROC analysis to select the extraction parameters in a principled manner. Furthermore, we present analytic models that reveal how different document retrieval strategies affect the quality of the extracted relation. Finally, we present our maximum likelihood approach for estimating, on the fly, the parameters required by our analytic models to predict the runtime and the output quality of each execution plan. Our experimental evaluation demonstrates that our optimization approach predicts accurately the output quality and selects the fastest execution plan that satisfies the output quality restrictions. © 2009 ACM.",Information Extraction; ROC curves,Maximum likelihood estimation; Security of data; Analytic models; Document Retrieval; Experimental evaluation; Extraction systems; Information Extraction; Information extraction systems; Like queries; Maximum likelihood approaches; On the flies; Optimization approach; Optimizer; Output quality; Precision and recall; Receiver operating characteristic curves; ROC analysis; ROC curves; Runtime; Spurious tuples; Structured information; Information analysis
Estimating statistical aggregates on probabilistic data streams,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849134258&doi=10.1145%2f1412331.1412338&partnerID=40&md5=fd7abfb99339e745c534dc501325c69f,"The probabilistic stream model was introduced by Jayram et al. [2007]. It is a generalization of the data stream model that is suited to handling probabilistic data, where each item of the stream represents a probability distribution over a set of possible events. Therefore, a probabilistic stream determines a distribution over a potentially exponential number of classical deterministic streams, where each item is deterministically one of the domain values. We present algorithms for computing commonly used aggregates on a probabilistic stream. We present the first one pass streaming algorithms for estimating the expected mean of a probabilistic stream. Next, we consider the problem of estimating frequency moments for probabilistic data. We propose a general approach to obtain unbiased estimators working over probabilistic data by utilizing unbiased estimators designed for standard streams. Applying this approach, we extend a classical data stream algorithm to obtain a one-pass algorithm for estimating F2, the second frequency moment.We present the first known streaming algorithms for estimating F0, the number of distinct items on probabilistic streams. Our work also gives an efficient one-pass algorithm for estimating the median, and a two-pass algorithm for estimating the range. © 2008 ACM.",Frequency moments; Mean; Median; OLAP; Probabilistic streams,Aggregates; Data handling; Data mining; Image coding; Mathematical models; Phase matching; Probability; Frequency moments; Mean; Median; OLAP; Probabilistic streams; Probability distributions
Compiling mappings to bridge applications and databases,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57949112536&doi=10.1145%2f1412331.1412334&partnerID=40&md5=42f55bf483ad379297a3f7fdcb9bc777,"Translating data and data access operations between applications and databases is a longstanding data management problem. We present a novel approach to this problem, in which the relationship between the application data and the persistent storage is specified using a declarative mapping, which is compiled into bidirectional views that drive the data transformation engine. Expressing the application model as a view on the database is used to answer queries, while expressing the database schema as a view on the application model allows us to leverage view maintenance algorithms for update translation. This approach has been implemented in a commercial product. It enables developers to interact with a relational database via a conceptual schema and an object-oriented programming surface. We outline the implemented system and focus on the challenges of mapping compilation, which include rewriting queries under constraints and supporting nonrelational constructs. © 2008 ACM.",Mapping; Query rewriting; Updateable views,Applications; Conformal mapping; Management information systems; Object oriented programming; Application datums; Application models; Bridge applications; Commercial products; Conceptual schemata; Data access operations; Data management problems; Data transformations; Database schemata; Object-oriented programmings; Query rewriting; Relational databases; Updateable views; View maintenances; Database systems
Commutativity analysis for XML updates,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849105131&doi=10.1145%2f1412331.1412341&partnerID=40&md5=ee59a5e4661c326526d57961f8b404f3,"An effective approach to support XML updates is to use XQuery extended with update operations. This approach results in very expressive languages which are convenient for users but are difficult to optimize or reason about. A crucial question underlying many static analysis problems for such languages, from optimization to view maintenance, is whether two expressions commute. Unfortunately, commutativity is undecidable for most existing XML update languages. In this article, we propose a conservative analysis for an expressive XML update language that can be used to determine commutativity. The approach relies on a form of path analysis that computes upper bounds for the nodes that are accessed or modified in a given expression. Our main result is a theorem that can be used to identify commuting expressions. We illustrate how the technique applies to concrete examples of query optimization in the presence of updates. © 2008 ACM.",Commutativity; Optimization; Updates; XML; XQuery,Computer programming languages; Linguistics; Markup languages; Optimization; Query languages; Regression analysis; Analysis problems; Commutativity; Path analyses; Query optimizations; Updates; Upper bounds; View maintenances; Xml updates; XQuery; XML
Expressiveness and complexity of XML publishing transducers,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849084002&doi=10.1145%2f1412331.1412337&partnerID=40&md5=de6d792a7a1b2a39b974c1b4c535e722,"A number of languages have been developed for specifying XML publishing, that is, transformations of relational data into XML trees. These languages generally describe the behaviors of a middleware controller that builds an output tree iteratively, issuing queries to a relational source and expanding the tree with the query results at each step. To study the complexity and expressive power of XML publishing languages, this article proposes a notion of publishing transducers, which generate XML trees from relational data. We study a variety of publishing transducers based on what relational queries a transducer can issue, what temporary stores a transducer can use during tree generation, and whether or not some tree nodes are allowed to be virtual, that is, excluded from the output tree. We first show how existing XML publishing languages can be characterized by such transducers, and thus provide a synergy between theory and practice. We then study the membership, emptiness, and equivalence problems for various classes of transducers. We establish lower and upper bounds, all matching, ranging from PTIME to undecidable. Finally, we investigate the expressive power of these transducers and existing languages. We show that when treated as relational query languages, different classes of transducers capture either complexity classes (e.g., PSPACE) or fragments of datalog (e.g., linear datalog). For tree generation, we establish connections between publishing transducers and logical transductions, among other things. © 2008 ACM.",Complexity; Data exchange; Expressiveness; Transducer; XML publishing,Data communication systems; Equivalence classes; Linguistics; Markup languages; Middleware; Probabilistic logics; Transducers; Trees (mathematics); XML; Complexity; Complexity classes; Data exchange; Equivalence problems; Expressive powers; Expressiveness; Lower and upper bounds; Query results; Relational datums; Relational queries; Relational query languages; Tree generations; Tree nodes; XML publishing; XML trees; Query languages
Foreword to TODS SIGMOD/PODS/ICDT 2007 special issue,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849152551&doi=10.1145%2f1412331.1412332&partnerID=40&md5=d1262e65dad1765620b2542d7fedd6b1,[No abstract available],,
Scalable approximate query processing with the DBO engine,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849140231&doi=10.1145%2f1412331.1412335&partnerID=40&md5=d57794ac85f1f498cc5f162508a55b0c,"This article describes query processing in the DBO database system. Like other database systems designed for ad hoc analytic processing, DBO is able to compute the exact answers to queries over a large relational database in a scalable fashion. Unlike any other system designed for analytic processing, DBO can constantly maintain a guess as to the final answer to an aggregate query throughout execution, along with statistically meaningful bounds for the guess's accuracy. As DBO gathers more and more information, the guess gets more and more accurate, until it is 100% accurate as the query is completed. This allows users to stop the execution as soon as they are happy with the query accuracy, and thus encourages exploratory data analysis. © 2008 ACM.",Online aggregation; Randomized algorithms; Sampling,Query processing; Relational database systems; Aggregate queries; Approximate query processing; Exploratory data analyses; Online aggregation; Randomized algorithms; Relational databases; Database systems
Efficient sort-based skyline evaluation,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849112824&doi=10.1145%2f1412331.1412343&partnerID=40&md5=5e9782d0f124839734cc1e69692442f7,"Skyline queries compute the set of Pareto-optimal tuples in a relation, that is, those tuples that are not dominated by any other tuple in the same relation. Although several algorithms have been proposed for efficiently evaluating skyline queries, they either necessitate the relation to have been indexed or have to perform the dominance tests on all the tuples in order to determine the result. In this article we introduce salsa, a novel skyline algorithm that exploits the idea of presorting the input data so as to effectively limit the number of tuples to be read and compared. This makes salsa also attractive when skyline queries are executed on top of systems that do not understand skyline semantics, or when the skyline logic runs on clients with limited power and/or bandwidth. We prove that, if one considers symmetric sorting functions, the number of tuples to be read is minimized by sorting data according to a minimum coordinate, minC, criterion, and that performance can be further improved if data distribution is known and an asymmetric sorting function is used. Experimental results obtained on synthetic and real datasets show that salsa consistently outperforms state-of-the-art sequential skyline algorithms and that its performance can be accurately predicted. © 2008 ACM.",Monotone functions; Skyline query,Distribution functions; Function evaluation; Functions; Indexing (of information); Information theory; Probability density function; If datums; Input datums; Monotone functions; Real data sets; Skyline query; Sorting
On the expressiveness of implicit provenance in query and update languages,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849161060&doi=10.1145%2f1412331.1412340&partnerID=40&md5=9bc38ee1c6987ac0cab048edbb7fe78c,"Information describing the origin of data, generally referred to as provenance, is important in scientific and curated databases where it is the basis for the trust one puts in their contents. Since such databases are constructed using operations of both query and update languages, it is of paramount importance to describe the effect of these languages on provenance. In this article we study provenance for query and update languages that are closely related to SQL, and compare two ways in which they can manipulate provenance so that elements of the input are rearranged to elements of the output: implicit provenance, where a query or update only provides the rearranged output, and provenance is provided implicitly by a default provenance semantics; and explicit provenance, where a query or update provides both the output and the description of the provenance of each component of the output. Although explicit provenance is in general more expressive, we show that the classes of implicit provenance operations expressible by query and update languages correspond to natural semantic subclasses of the explicit provenance queries. One of the consequences of this study is that provenance separates the expressive power of query and update languages. The model is also relevant to annotation propagation schemes in which annotations on the input to a query or update have to be transferred to the output or vice versa. © 2008 ACM.",Conservativity; Nested relational calculus; Nested update language; Provenance,Database systems; Functions; Information theory; Linguistics; Semantics; Annotation propagations; Conservativity; Expressive powers; Nested relational calculus; Nested update language; Provenance; Two ways; Query languages
Forensic analysis of database tampering,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849102169&doi=10.1145%2f1412331.1412342&partnerID=40&md5=37f58b40c8a9ed7fd33c755c78385945,"Regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. One approach is tamper detection via cryptographic hashing. This article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. We present four successively more sophisticated forensic analysis algorithms: the Monochromatic, RGBY, Tiled Bitmap, and a3D algorithms, and characterize their forensic cost under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. A lower bound on forensic cost is derived, with RGBY and a3D being shown optimal for a large number of corruptions. We also provide validated cost formulæ for these algorithms and recommendations for the circumstances in which each algorithm is indicated. © 2008 ACM.",A3D algorithm; Compliant records; Forensic analysis algorithm; Forensic cost; Monochromatic algorithm; Polychromatic algorithm; RGBY algorithm; Tiled Bitmap algorithm,Control theory; Costs; Database systems; Monochromators; A3D algorithm; Compliant records; Forensic analysis algorithm; Forensic cost; Monochromatic algorithm; Polychromatic algorithm; RGBY algorithm; Tiled Bitmap algorithm; Cost benefit analysis
Introduction to ICDT 2007 special section,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849156550&doi=10.1145%2f1412331.1412339&partnerID=40&md5=8d64c686d454d179021e8ff11f473ebf,[No abstract available],,
Introduction to the PODS 2007 special section,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849109520&doi=10.1145%2f1412331.1412336&partnerID=40&md5=b0db83cd5bb67ab83f5d9bee8fb15b10,[No abstract available],,
Introduction to ACM SIGMOD 2007 special section,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849139390&doi=10.1145%2f1412331.1412333&partnerID=40&md5=20c2e13d670a3bea99183b079a0f4cb4,[No abstract available],,
Correlated pattern mining in quantitative databases,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51149111835&doi=10.1145%2f1386118.1386120&partnerID=40&md5=bf98333fb15e806805f91e41e806779d,"We study mining correlations from quantitative databases and show that this is a more effective approach than mining associations to discover useful patterns. We propose the novel notion of quantitative correlated pattern (QCP), which is founded on two formal concepts, mutual information and all-confidence. We first devise a normalization on mutual information and apply it to the problem of QCP mining to capture the dependency between the attributes. We further adopt all-confidence as a quality measure to ensure, at a finer granularity, the dependency between the attributes with specific quantitative intervals. We also propose an effective supervised method that combines the consecutive intervals of the quantitative attributes based on mutual information, such that the interval-combining is guided by the dependency between the attributes. We develop an algorithm, QCoMine, to mine QCPs efficiently by utilizing normalized mutual information and all-confidence to perform bilevel pruning. We also identify the redundancy existing in the set of QCPs and propose effective techniques to eliminate the redundancy. Our extensive experiments on both real and synthetic datasets verify the efficiency of QCoMine and the quality of the QCPs. The experimental results also justify the effectiveness of our proposed techniques for redundancy elimination. To further demonstrate the usefulness and the quality of QCPs, we study an application of QCPs to classification. We demonstrate that the classifier built on the QCPs achieves higher classification accuracy than the state-of-the-art classifiers built on association rules. © 2008 ACM.",Correlated patterns; Information-theoretic approach; Mutual information; Quantitative databases,Associative processing; Classification (of information); Classifiers; Database systems; Learning systems; Quality assurance; Redundancy; Reliability; Correlated patterns; Information-theoretic approach; Mutual information; Quantitative databases; Mining
Hierarchical synopses with optimal error guarantees,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51149084988&doi=10.1145%2f1386118.1386124&partnerID=40&md5=479bd9459aeb57eebf1b23907265afb5,"Hierarchical synopsis structures offer a viable alternative in terms of efficiency and flexibility in relation to traditional summarization techniques such as histograms. Previous research on such structures has mostly focused on a single model, based on the Haar wavelet decomposition. In previous work, we have introduced a more refined, wavelet-inspired hierarchical index structure for synopsis construction: the Haar+ tree. The chief advantages of this structure are twofold. First, it achieves higher synopsis quality at the task of summarizing data sets with sharp discontinuities than state-of-the-art histogram and Haar wavelet techniques. Second, thanks to its search space delimitation capacity, Haar+ synopsis construction operates in time linear in the size of the data set for any monotonic distributive error metric. Contemporaneous research has introduced another hierarchical synopsis structure, the compact hierarchical histogram (CHH). In this article, we elaborate on both these structures. First, we formally prove that the CHH, in its default binary-hierarchy form, is a simplified variant of a Haar+ tree. We then focus on the summarization problem, with both these hierarchical synopsis structures, in which an error guarantee expressed by a maximum-error metric is required. We show that this problem is most efficiently solved through its dual, space-minimization counterpart, which can also achieve optimal quality. In this case, there is a benefit to be gained by specializing the algorithm for each structure; hence, our algorithm for optimal-quality maximum-error CHH requires low polynomial time; on the other hand, optimal-quality Haar+ synopses for maximum-error metrics are constructed in exponential time; hence, we also develop a low-polynomial-time approximation scheme for the maximum-error Haar+ case. Furthermore, we extend our approach for both general-error and maximum-error Haar+ synopses to arbitrary dimensionality. In our experimental study, (i) we confirm the theoretically expected superiority of Haar+ synopses over Haar wavelet methods in both construction time and achieved quality for representative error metrics; (ii) we demonstrate that Haar+ synopses are also constructed faster than optimal plain histograms, and, moreover, achieve higher synopsis quality with highly discontinuous data sets; such an advantage of a hierarchical synopsis structure over a histogram had been intuitively expressed, but never experimentally verified; and (iii) we show that Haar+ synopsis quality supersedes that of a CHH. © 2008 ACM.",Approximate query processing; Data synopses; Summarization,Model structures; Wavelet transforms; Approximate query processing; Data synopses; Haar wavelet decomposition; Index structures; Summarization; Wavelet decomposition
Confidence bounds for sampling-based group by estimates,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51149118028&doi=10.1145%2f1386118.1386122&partnerID=40&md5=47a12b397180ed173b4508d8d38a4121,"Sampling is now a very important data management tool, to such an extent that an interface for database sampling is included in the latest SQL standard. In this article we reconsider in depth what at first may seem like a very simple problemcomputing the error of a sampling-based guess for the answer to a GROUP BY query over a multitable join. The difficulty when sampling for the answer to such a query is that the same sample will be used to guess the result of the query for each group, which induces correlations among the estimates. Thus, from a statistical point-of-view it is very problematic and even dangerous to use traditional methods such as confidence intervals for communicating estimate accuracy to the user. We explore ways to address this problem, and pay particular attention to the computational aspects of computing safe confidence intervals. © 2008 ACM.",Approximate query processing; Multiple hypothesis testing; Sampling,Financial data processing; Management information systems; Statistical methods; Approximate query processing; Confidence intervals; Database sampling; Multiple hypothesis testing; Sampling
Probabilistic top-k and ranking-aggregate queries,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51149112283&doi=10.1145%2f1386118.1386119&partnerID=40&md5=5dbdbf3a8277822d7fbee0fde6beaf64,"Ranking and aggregation queries are widely used in data exploration, data analysis, and decision-making scenarios. While most of the currently proposed ranking and aggregation techniques focus on deterministic data, several emerging applications involve data that is unclean or uncertain. Ranking and aggregating uncertain (probabilistic) data raises new challenges in query semantics and processing, making conventional methods inapplicable. Furthermore, uncertainty imposes probability as a new ranking dimension that does not exist in the traditional settings. In this article we introduce new probabilistic formulations for top-k and ranking-aggregate queries in probabilistic databases. Our formulations are based on marriage of traditional top-k semantics with possible worlds semantics. In the light of these formulations, we construct a generic processing framework supporting both query types, and leveraging existing query processing and indexing capabilities in current RDBMSs. The framework encapsulates a state space model and efficient search algorithms to compute query answers. Our proposed techniques minimize the number of accessed tuples and the size of materialized search space to compute query answers. Our experimental study shows the efficiency of our techniques under different data distributions with orders of magnitude improvement over nave methods. © 2008 ACM.",Aggregation; Probabilistic data; Query processing; Ranking; Top-k,Agglomeration; Aggregates; Database systems; Decision making; Indexing (materials working); Information theory; Learning algorithms; Probability; Probability distributions; Problem solving; Query processing; Risk assessment; Semantics; State space methods; Statistical methods; Aggregate queries; Aggregation; Probabilistic data; Ranking; Top-k; Data processing
Workload-aware anonymization techniques for large-scale datasets,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51149098991&doi=10.1145%2f1386118.1386123&partnerID=40&md5=012a15eb0b20e082683da08ebc06b364,"Protecting individual privacy is an important problem in microdata distribution and publishing. Anonymization algorithms typically aim to satisfy certain privacy definitions with minimal impact on the quality of the resulting data. While much of the previous literature has measured quality through simple one-size-fits-all measures, we argue that quality is best judged with respect to the workload for which the data will ultimately be used. This article provides a suite of anonymization algorithms that incorporate a target class of workloads, consisting of one or more data mining tasks as well as selection predicates. An extensive empirical evaluation indicates that this approach is often more effective than previous techniques. In addition, we consider the problem of scalability. The article describes two extensions that allow us to scale the anonymization algorithms to datasets much larger than main memory. The first extension is based on ideas from scalable decision trees, and the second is based on sampling. A thorough performance evaluation indicates that these techniques are viable in practice. © 2008 ACM.",Anonymity; Data mining; Databases; Performance; Privacy; Scalability,Decision support systems; Decision theory; Decision trees; Information management; Mathematical models; Anonymity; Data mining; Databases; Performance; Privacy; Scalability; Decision making
Sketches for size of join estimation,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51149086160&doi=10.1145%2f1386118.1386121&partnerID=40&md5=7f831669ce8483e73f2d303458c64e1d,"Sketching techniques provide approximate answers to aggregate queries both for data-streaming and distributed computation. Small space summaries that have linearity properties are required for both types of applications. The prevalent method for analyzing sketches uses moment analysis and distribution-independent bounds based on moments. This method produces clean, easy to interpret, theoretical bounds that are especially useful for deriving asymptotic results. However, the theoretical bounds obscure fine details of the behavior of various sketches and they are mostly not indicative of which type of sketches should be used in practice. Moreover, no significant empirical comparison between various sketching techniques has been published, which makes the choice even harder. In this article we take a close look at the sketching techniques proposed in the literature from a statistical point of view with the goal of determining properties that indicate the actual behavior and producing tighter confidence bounds. Interestingly, the statistical analysis reveals that two of the techniques, Fast-AGMS and Count-Min, provide results that are in some cases orders of magnitude better than the corresponding theoretical predictions. We conduct an extensive empirical study that compares the different sketching techniques in order to corroborate the statistical analysis with the conclusions we draw from it. The study indicates the expected performance of various sketches, which is crucial if the techniques are to be used by practitioners. The overall conclusion of the study is that Fast-AGMS sketches are, for the full spectrum of problems, either the best, or close to the best, sketching technique. We apply the insights obtained from the statistical study and the experimental results to design effective algorithms for sketching interval data. We show how the two basic methods for sketching interval data, DMAP and fast range-summation, can be improved significantly with respect to the update time without a significant loss in accuracy. The gain in update time can be as large as two orders of magnitude, thus making the improved methods practical. The empirical study suggests that DMAP is preferable when update time is the critical requirement and fast range-summation is desirable for better accuracy. © 2008 ACM.",AGMS sketches; Count-Min sketches; DMAP; Fast range-summation; Fast-AGMS sketches; Fast-Count sketches; Size of join estimation,Statistics; AGMS sketches; Count-Min sketches; DMAP; Fast range-summation; Fast-AGMS sketches; Fast-Count sketches; Size of join estimation; Method of moments
Efficient online index construction for text databases,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51149086157&doi=10.1145%2f1386118.1386125&partnerID=40&md5=2dd902aea7b7b0287d593931ed9bcc21,"Inverted index structures are a core element of current text retrieval systems. They can be constructed quickly using offline approaches, in which one or more passes are made over a static set of input data, and, at the completion of the process, an index is available for querying. However, there are search environments in which even a small delay in timeliness cannot be tolerated, and the index must always be queryable and up to date. Here we describe and analyze a geometric partitioning mechanism for online index construction that provides a range of tradeoffs between costs, and can be adapted to different balances of insertion and querying operations. Detailed experimental results are provided that show the extent of these tradeoffs, and that these new methods can yield substantial savings in online indexing costs. © 2008 ACM.",Index construction; Index update; Search engines; Text indexing,Index construction; Index update; Search engines; Text indexing
Metric space similarity joins,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46649104057&doi=10.1145%2f1366102.1366104&partnerID=40&md5=e3452c009430cf6c162e718a76858db0,"Similarity join algorithms find pairs of objects that lie within a certain distance ε of each other. Algorithms that are adapted from spatial join techniques are designed primarily for data in a vector space and often employ some form of a multidimensional index. For these algorithms, when the data lies in a metric space, the usual solution is to embed the data in vector space and then make use of a multidimensional index. Such an approach has a number of drawbacks when the data is high dimensional as we must eventually find the most discriminating dimensions, which is not trivial. In addition, although the maximum distance between objects increases with dimension, the ability to discriminate between objects in each dimension does not. These drawbacks are overcome via the introduction of a new method called Quickjoin that does not require a multidimensional index and instead adapts techniques used in distance-based indexing for use in a method that is conceptually similar to the Quicksort algorithm. A formal analysis is provided of the Quickjoin method. Experiments show that the Quickjoin method significantly outperforms two existing techniques. © 2008 ACM.",Distance-based indexing; External memory algorithms; Nearest neighbor queries; Range queries; Ranking; Similarity join,Diffractive optical elements; Set theory; Topology; Vectors; (2+1) dimensions; Distance-based; Formal analysis; High-dimensional; Join algorithms; maximum distance; Metric spaces; Multidimensional indexing; New methods; Quick sort; Spatial joins; Techniques used; Vector spaces; Indexing (of information)
Cache-oblivious databases: Limitations and opportunities,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46649110107&doi=10.1145%2f1366102.1366105&partnerID=40&md5=6c7474559d293e50d3fdf789f8762a0f,"Cache-oblivious techniques, proposed in the theory community, have optimal asymptotic bounds on the amount of data transferred between any two adjacent levels of an arbitrary memory hierarchy. Moreover, this optimal performance is achieved without any hardware platform specific tuning. These properties are highly attractive to autonomous databases, especially because the hardware architectures are becoming increasingly complex and diverse. In this article, we present our design, implementation, and evaluation of the first cache-oblivious in-memory query processor, EaseDB. Moreover, we discuss the inherent limitations of the cache-oblivious approach as well as the opportunities given by the upcoming hardware architectures. Specifically, a cache-oblivious technique usually requires sophisticated algorithm design to achieve a comparable performance to its cache-conscious counterpart. Nevertheless, this development-time effort is compensated by the automaticity of performance achievement and the reduced ownership cost. Furthermore, this automaticity enables cache-oblivious techniques to outperform their cache-conscious counterparts in multi-threading processors. © 2008 ACM.",Cache-conscious; Cache-oblivious; Chip multiprocessors; Data caches; Simultaneous multithreading,"Database systems; (e ,2e) theory; (PL) properties; algorithm designs; Asymptotic bounds; Cache-oblivious; Hardware architecture (graphics processors); Hardware platforms; Inherent limitations; memory hierarchies; Multithreading (MT); Optimal performances; query processors; Cache memory"
Approximate continuous querying over distributed streams,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46649103439&doi=10.1145%2f1366102.1366106&partnerID=40&md5=5786c2162fc355976ea691ec051def67,"While traditional database systems optimize for performance on one-shot query processing, emerging large-scale monitoring applications require continuous tracking of complex data-analysis queries over collections of physically distributed streams. Thus, effective solutions have to be simultaneously space/time efficient (at each remote monitor site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality approximate query answers. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking a broad class of complex aggregate queries in such a distributed-streams setting. Our tracking schemes maintain approximate query answers with provable error guarantees, while simultaneously optimizing the storage space and processing time at each remote site, and the communication cost across the network. In a nutshell, our algorithms rely on tracking general-purpose randomized sketch summaries of local streams at remote sites along with concise prediction models of local site behavior in order to produce highly communication- and space/time-efficient solutions. The end result is a powerful approximate query tracking framework that readily incorporates several complex analysis queries (including distributed join and multi-join aggregates, and approximate wavelet representations), thus giving the first known low-overhead tracking solution for such queries in the distributed-streams model. Experiments with real data validate our approach, revealing significant savings over naive solutions as well as our analytical worst-case guarantees. © 2008 ACM.",Approximate query processing; Continuous distributed monitoring; Data stream algorithms; Data synopses,Aggregates; Communication; Data processing; Data storage equipment; Database systems; Mathematical models; Modal analysis; Monitoring; Optimization; Query processing; Rivers; Solutions; Aggregate queries; Algorithmic solutions; Approximate query; communication costs; Communication networks; Complex analysis; Complex data; Continuous tracking; Efficient solutions; General (CO); In order; Large-scale monitoring; Multi join; prediction modeling; Real data; Remote monitoring; Remote sites; Storage spaces; Distributed database systems
Quasi-inverses of schema mappings,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46649086093&doi=10.1145%2f1366102.1366108&partnerID=40&md5=18a407685d64ecb493ceda751a3ffe4a,"Schema mappings are high-level specifications that describe the relationship between two database schemas. Two operators on schema mappings, namely the composition operator and the inverse operator, are regarded as especially important. Progress on the study of the inverse operator was not made until very recently, as even finding the exact semantics of this operator turned out to be a fairly delicate task. Furthermore, this notion is rather restrictive, since it is rare that a schema mapping possesses an inverse. In this article, we introduce and study the notion of a quasi-inverse of a schema mapping. This notion is a principled relaxation of the notion of an inverse of a schema mapping; intuitively, it is obtained from the notion of an inverse by not differentiating between instances that are equivalent for data-exchange purposes. For schema mappings specified by source-to-target tuple-generating dependencies (s-t tgds), we give a necessary and sufficient combinatorial condition for the existence of a quasi-inverse, and then use this condition to obtain both positive and negative results about the existence of quasi-inverses. In particular, we show that every LAV (local-as-view) schema mapping has a quasi-inverse, but that there are schema mappings specified by full s-t tgds that have no quasi-inverse. After this, we study the language needed to express quasi-inverses of schema mappings specified by s-t tgds, and we obtain a complete characterization. We also characterize the language needed to express inverses of schema mappings, and thereby solve a problem left open in the earlier study of the inverse operator. Finally, we show that quasi-inverses can be used in many cases to recover the data that was exported by the original schema mapping when performing data exchange. © 2008 ACM.",Chase; Data exchange; Data integration; Dependencies; Inverse; Metadata model management; Quasi-inverse; Schema mapping,"Conformal mapping; Information theory; Linguistics; Specifications; (I ,J) conditions; (U ,V) operator; Composition operators; Data exchange (DX); Database schemas; High level specifications; Local-as-view (LAV); Schema mappings; Inverse problems"
Conditional functional dependencies for capturing data inconsistencies,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46649106686&doi=10.1145%2f1366102.1366103&partnerID=40&md5=7cbe36ce70fe195198dacb6841320ab1,"We propose a class of integrity constraints for relational databases, referred to as conditional functional dependencies (CFDs), and study their applications in data cleaning. In contrast to traditional functional dependencies (FDs) that were developed mainly for schema design, CFDs aim at capturing the consistency of data by enforcing bindings of semantically related values. For static analysis of CFDs we investigate the consistency problem, which is to determine whether or not there exists a nonempty database satisfying a given set of CFDs, and the implication problem, which is to decide whether or not a set of CFDs entails another CFD. We show that while any set of transitional FDs is trivially consistent, the consistency problem is NP-complete for CFDs, but it is in PTIME when either the database schema is predefined or no attributes involved in the CFDs have a finite domain. For the implication analysis of CFDs, we provide an inference system analogous to Armstrong's axioms for FDs, and show that the implication problem is coNP-complete for CFDs in contrast to the linear-time complexity for their traditional counterpart. We also present an algorithm for computing a minimal cover of a set of CFDs. Since CFDs allow data bindings, in some cases CFDs may be physically large, complicating the detection of constraint violations. We develop techniques for detecting CFD violations in SQL as well as novel techniques for checking multiple constraints by a single query. We also provide incremental methods for checking CFDs in response to changes to the database. We experimentally verify the effectiveness of our CFD-based methods for inconsistency detection. This work not only yields a constraint theory for CFDs but is also a step toward a practical constraint-based method for improving data quality. © 2008 ACM.",Data cleaning; Functional dependency; SQL,"Constraint theory; Nuclear propulsion; Quality assurance; Reliability; Static analysis; (e ,2e) theory; Capturing data; Consistency problems; constraint violations; Constraint-based; Data cleaning; Data quality (DQ); database schema; Finite domains; Functional dependencies (FD); Implication problem; Inconsistency detection; Inference systems; Integrity constraints (IC); multiple constraints; Novel techniques; NP Complete; Relational databases; schema design; Single query; time complexities; Database systems"
On computing temporal aggregates with range predicates,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46649115298&doi=10.1145%2f1366102.1366109&partnerID=40&md5=58a0b38ed3bac168ca06c5b8aee99bfd,"Computing temporal aggregates is an important but costly operation for applications that maintain time-evolving data (data warehouses, temporal databases, etc.) Due to the large volume of such data, performance improvements for temporal aggregate queries are critical. Previous approaches have aggregate predicates that involve only the time dimension. In this article we examine techniques to compute temporal aggregates that include key-range predicates as well (range-temporal aggregates). In particular we concentrate on the SUM aggregate, while COUNT is a special case. To handle arbitrary key ranges, previous methods would need to keep a separate index for every possible key range. We propose an approach based on a new index structure called the Multiversion SB-Tree, which incorporates features from both the SB-Tree and the Multiversion B+ - tree, to handle arbitrary key-range temporal aggregate queries. We analyze the performance of our approach and present experimental results that show its efficiency. Furthermore, we address a novel and practical variation called functional range-temporal aggregates. Here, the value of any record is a function over time. The meaning of aggregates is altered such that the contribution of a record to the aggregate result is proportional to the size of the intersection between the record's time interval and the query time interval. Both analytical and experimental results show the efficiency of our result. © 2008 ACM.",Functional aggregates; Indexing; Range predicates; Temporal aggregates,Administrative data processing; Data warehouses; Database systems; (+ mod 2N) operation; Aggregate queries; B+ trees; Experimental results; index structures; performance improvements; query time; temporal databases; time dimensions; Time intervals; Aggregates
Repair localization for query answering from inconsistent databases,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46649108915&doi=10.1145%2f1366102.1366107&partnerID=40&md5=00016d50aaa9d615841762a984f66734,"Query answering from inconsistent databases amounts to finding ""meaningful"" answers to queries posed over database instances that do not satisfy integrity constraints specified over their schema. A declarative approach to this problem relies on the notion of repair, that is, a database that satisfies integrity constraints and is obtained from the original inconsistent database by ""minimally"" adding and/or deleting tuples. Consistent answers to a user query are those answers that are in the evaluation of the query over each repair. Motivated by the fact that computing consistent answers from inconsistent databases is in general intractable, the present paper investigates techniques that allow to localize the difficult part of the computation on a small fragment of the database at hand, called ""affected"" part. Based on a number of localization results, an approach to query answering from inconsistent data is presented, in which the query is evaluated over each of the repairs of the affected part only, augmented with the part that is not affected. Single query results are then suitably recombined. For some relevant settings, techniques are also discussed to factorize repairs into components that can be processed independently of one another, thereby guaranteeing exponential gain w.r.t. the basic approach, which is not based on localization. The effectiveness of the results is demonstrated for consistent query answering over expressive schemas, based on logic programming specifications as proposed in the literature. © 2008 ACM.",Consistent query answering; Data integration; Database repairs; Inconsistency management in databases; Logic programming; Stable models,BASIC (programming language); Computer programming; Logic programming; Maintenance; Quality assurance; Reliability; Repair; Specifications; Consistent query answering; Exponential gain; General (CO); Inconsistent data; Inconsistent databases; Integrity constraints (IC); query answering; Schemas; Single query; user queries; Database systems
On static and dynamic methods for condensation-based privacy-preserving data mining,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549109791&doi=10.1145%2f1331904.1331906&partnerID=40&md5=39e6dedf6bf976d359d86adf69fcdf5f,"In recent years, privacy-preserving data mining has become an important problem because of the large amount of personal data which is tracked by many business applications. In many cases, users are unwilling to provide personal information unless the privacy of sensitive information is guaranteed. In this paper, we propose a new framework for privacy-preserving data mining of multidimensional data. Previous work for privacy-preserving data mining uses a perturbation approach which reconstructs data distributions in order to perform the mining. Such an approach treats each dimension independently and therefore ignores the correlations between the different dimensions. In addition, it requires the development of a new distribution-based algorithm for each data mining problem, since it does not use the multidimensional records, but uses aggregate distributions of the data as input. This leads to a fundamental re-design of data mining algorithms. In this paper, we will develop a new and flexible approach for privacy-preserving data mining that does not require new problem-specific algorithms, since it maps the original data set into a new anonymized data set. These anonymized data closely match the characteristics of the original data including the correlations among the different dimensions. We will show how to extend the method to the case of data streams. We present empirical results illustrating the effectiveness of the method. We also show the efficiency of the method for data streams. © 2008 ACM.",Databases data mining; K-anonymity; Privacy,Algorithms; Data privacy; Perturbation techniques; Problem solving; Data streams; Databases data mining; K-anonymity; Data mining
Fault-tolerance in the borealis distributed stream processing system,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549146660&doi=10.1145%2f1331904.1331907&partnerID=40&md5=e9a5394e80d393bfdc311abc4f870043,"Over the past few years, Stream Processing Engines (SPEs) have emerged as a new class of software systems, enabling low latency processing of streams of data arriving at high rates. As SPEs mature and get used in monitoring applications that must continuously run (e.g., in network security monitoring), a significant challenge arises: SPEs must be able to handle various software and hardware faults that occur, masking them to provide high availability (HA). In this article, we develop, implement, and evaluate DPC (Delay, Process, and Correct), a protocol to handle crash failures of processing nodes and network failures in a distributed SPE. Like previous approaches to HA, DPC uses replication and masks many types of node and network failures. In the presence of network partitions, the designer of any replication system faces a choice between providing availability or data consistency across the replicas. In DPC, this choice is made explicit: the user specifies an availability bound (no result should be delayed by more than a specified delay threshold even under failure if the corresponding input is available), and DPC attempts to minimize the resulting inconsistency between replicas (not all of which might have seen the input data) while meeting the given delay threshold. Although conceptually simple, the DPC protocol tolerates the occurrence of multiple simultaneous failures as well as any further failures that occur during recovery. This article describes DPC and its implementation in the Borealis SPE. We show that DPC enables a distributed SPE to maintain low-latency processing at all times, while also achieving eventual consistency, where applications eventually receive the complete and correct output streams. Furthermore, we show that, independent of system size and failure location, it is possible to handle failures almost up-to the user-specified bound in a manner that meets the required availability without introducing any inconsistency. © 2008 ACM.",Availability; Consistency; Distributed stream processing; Fault-tolerance,Computer software; Computer system recovery; Fault tolerance; Network protocols; Security of data; Distributed stream processing; User-specified bound; Distributed computer systems
Information preserving XML schema embedding,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549103739&doi=10.1145%2f1331904.1331908&partnerID=40&md5=405ca04721b01b307d40411fdbd6ede7,"A fundamental concern of data integration in an XML context is the ability to embed one or more source documents in a target document so that (a) the target document conforms to a target schema and (b) the information in the source documents is preserved. In this paper, information preservation for XML is formally studied, and the results of this study guide the definition of a novel notion of schema embedding between two XML DTD schemas represented as graphs. Schema embedding generalizes the conventional notion of graph similarity by allowing an edge in a source DTD schema to be mapped to a path in the target DTD. Instance-level embeddings can be derived from the schema embedding in a straightforward manner, such that conformance to a target schema and information preservation are guaranteed. We show that it is NP-complete to find an embedding between two DTD schemas. We also outline efficient heuristic algorithms to find candidate embeddings, which have proved effective by our experimental study. These yield the first systematic and effective approach to finding information preserving XML mappings. © 2008 ACM.",Data transformation; Information integration; Information preservation; Schema embedding; Schema mapping; XML; XSLT,Computational complexity; Embedded systems; Graph theory; Information analysis; Semantics; Data transformation; Information integration; Information preservation; Schema embedding; Schema mapping; XML
Algorithms and metrics for processing multiple heterogeneous continuous queries,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549138276&doi=10.1145%2f1331904.1331909&partnerID=40&md5=360f9907377b67e6b2dd819db5b97fc1,"The emergence of monitoring applications has precipitated the need for Data Stream Management Systems (DSMSs), which constantly monitor incoming data feeds (through registered continuous queries), in order to detect events of interest. In this article, we examine the problem of how to schedule multiple Continuous Queries (CQs) in a DSMS to optimize different Quality of Service (QoS) metrics. We show that, unlike traditional online systems, scheduling policies in DSMSs that optimize for average response time will be different from policies that optimize for average slowdown, which is a more appropriate metric to use in the presence of a heterogeneous workload. Towards this, we propose policies to optimize for the average-case performance for both metrics. Additionally, we propose a hybrid scheduling policy that strikes a fine balance between performance and fairness, by looking at both the average- and worst-case performance, for both metrics. We also show how our policies can be adaptive enough to handle the inherent dynamic nature of monitoring applications. Furthermore, we discuss how our policies can be efficiently implemented and extended to exploit sharing in optimized multi-query plans and multi-stream CQs. Finally, we experimentally show using real data that our policies consistently outperform currently used ones. © 2008 ACM.",Continuous queries; Data stream management system; Operator scheduling,Algorithms; Database systems; Online systems; Quality of service; Scheduling; Continuous queries; Data stream management system; Operator scheduling; Query processing
Authority-based keyword search in databases,2008,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549166108&doi=10.1145%2f1331904.1331905&partnerID=40&md5=90e2c1edc2ae1ff5b57970d546a3eafb,"Our system applies authority-based ranking to keyword search in databases modeled as labeled graphs. Three ranking factors are used: the relevance to the query, the specificity and the importance of the result. All factors are handled using authority-flow techniques that exploit the link-structure of the data graph, in contrast to traditional Information Retrieval. We address the performance challenges in computing the authority flows in databases by using precomputation and exploiting the database schema if present. We conducted user surveys and performance experiments on multiple real and synthetic datasets, to assess the semantic meaningfulness and performance of our system. © 2008 ACM.",Authority flow; PageRank; Quality experiments; Ranking; Specificity,Computation theory; Database systems; Graph theory; Information retrieval; Semantics; Authority flow; PageRank; Quality experiments; Search engines
Physical design refinement: The merge-reduce approach,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949041072&doi=10.1145%2f1292609.1292618&partnerID=40&md5=752536522de509f63b797b11656a62d3,"Physical database design tools rely on a DBA-provided workload to pick an optimal set of indexes and materialized views. Such tools allow either creating a new such configuration or adding new structures to existing ones. However, these tools do not provide adequate support for the incremental and flexible refinement of existing physical structures. Although such refinements are often very valuable for DBAs, a completely manual approach to refinement can lead to infeasible solutions (e.g., excessive use of space). In this article, we focus on the important problem of physical design refinement and propose a transformational architecture that is based upon two novel primitive operations, called merging and reduction. These operators help refine a configuration, treating indexes and materialized views in a unified way, as well as succinctly explain the refinement process to DBAs. © 2007 ACM.",Physical database design; Physical design refinment; View merging and reduction,Computer aided software engineering; Data structures; Merging; Network architecture; Problem solving; Software design; Physical database designs; Physical design refinment; Transformational architecture; View merging and reduction; Database systems
Towards a query optimizer for text-centric tasks,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949038529&doi=10.1145%2f1292609.1292611&partnerID=40&md5=b2fa82f69c6b367e38ead155e3638263,"Text is ubiquitous and, not surprisingly, many important applications rely on textual data for a variety of tasks. As a notable example, information extraction applications derive structured relations from unstructured text; as another example, focused crawlers explore the Web to locate pages about specific topics. Execution plans for text-centric tasks follow two general paradigms for processing a text database: either we can scan, or crawl, the text database or, alternatively, we can exploit search engine indexes and retrieve the documents of interest via carefully crafted queries constructed in task-specific ways. The choice between crawl- and query-based execution plans can have a substantial impact on both execution time and output completeness (e.g., in terms of recall). Nevertheless, this choice is typically ad hoc and based on heuristics or plain intuition. In this article, we present fundamental building blocks to make the choice of execution plans for text-centric tasks in an informed, cost-based way. Towards this goal, we show how to analyze query- and crawl-based plans in terms of both execution time and output completeness. We adapt results from random-graph theory and statistics to develop a rigorous cost model for the execution plans. Our cost model reflects the fact that the performance of the plans depends on fundamental task-specific properties of the underlying text databases. We identify these properties and present efficient techniques for estimating the associated parameters of the cost model. We also present two optimization approaches for text-centric tasks that rely on the cost-model parameters and select efficient execution plans. Overall, our optimization approaches help build efficient execution plans for a task, resulting in significant efficiency and output completeness benefits. We complement our results with a large-scale experimental evaluation for three important text-centric tasks and over multiple real-life data sets. © 2007 ACM.",Distributed information retrieval; Focused crawling; Information extraction; Metasearching; Text database selection,Database systems; Information retrieval; Optimization; World Wide Web; Data sets; Focused crawling; Information extraction; Metasearching; Text database selection; Text processing
NaLIX: A generic natural language search environment for XML data,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949038999&doi=10.1145%2f1292609.1292620&partnerID=40&md5=74ac0550c89b2aced127b0cee446c629,"We describe the construction of a generic natural language query interface to an XML database. Our interface can accept a large class of English sentences as a query, which can be quite complex and include aggregation, nesting, and value joins, among other things. This query is translated, potentially after reformulation, into an XQuery expression. The translation is based on mapping grammatical proximity of natural language parsed tokens in the parse tree of the query sentence to proximity of corresponding elements in the XML data to be retrieved. Iterative search in the form of followup queries is also supported. Our experimental assessment, through a user study, demonstrates that this type of natural language interface is good enough to be usable now, with no restrictions on the application domain. © 2007 ACM.",Dialog system; Iterative search; Natural language interface; XML; XQuery,Data reduction; Interfaces (computer); Iterative methods; Natural language processing systems; Query languages; XML; Dialog systems; Iterative search; Natural language interfaces; XQuery expression; Tabu search
Exporting and interactively querying web service-accessed sources: The CLIDE system,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949024728&doi=10.1145%2f1292609.1292612&partnerID=40&md5=5f0c56f17152cd528ac79fb0de5f9b66,"The CLIDE System assists the owners of sources that participate in Web service-based data publishing systems to publish a restricted set of parameterized queries over the schema of their sources and package them as WSDL services. The sources may be relational databases, which naturally have a schema, or ad hoc information/application systems whereas the owner publishes a virtual schema. CLIDE allows information clients to pose queries over the published schema and utilizes prior work on answering queries using views to answer queries that can be processed by combining and processing the results of one or more Web service calls. These queries are called feasible. Contrary to prior work, where infeasible queries are rejected without an explanatory feedback, leading the user into a frustrating trial-and-error cycle, CLIDE features a query formulation interface, which extends the QBE-like query builder of Microsoft's SQL Server with a color scheme that guides the user toward formulating feasible queries. CLIDE guarantees that the suggested query edit actions are complete (i.e., each feasible query can be built by following only suggestions), rapidly convergent (the suggestions are tuned to lead to the closest feasible completions of the query), and suitably summarized (at each interaction step, only a minimal number of actions needed to preserve completeness are suggested). We present the algorithms, implementation, and performance evaluation showing that CLIDE is a viable on-line tool. © 2007 ACM.",Limited access patterns; Middleware; Query rewriting; Web services,Algorithms; Middleware; Query processing; Relational database systems; Servers; Data publishing systems; Limited access patterns; Query rewriting; Web services
Inverting schema mappings,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949003730&doi=10.1145%2f1292609.1292615&partnerID=40&md5=da8a4eb43148dc0b2b78c9838a4f7da9,"A schema mapping is a specification that describes how data structured under one schema (the source schema) is to be transformed into data structured under a different schema (the target schema). Although the notion of an inverse of a schema mapping is important, the exact definition of an inverse mapping is somewhat elusive. This is because a schema mapping may associate many target instances with each source instance, and many source instances with each target instance. Based on the notion that the composition of a mapping and its inverse is the identity, we give a formal definition for what it means for a schema mapping M′ to be an inverse of a schema mapping M for a class S of source instances. We call such an inverse an S-inverse. A particular case of interest arises when S is the class of all source instances, in which case an S-inverse is a global inverse. We focus on the important and practical case of schema mappings specified by source-to-target tuple-generating dependencies, and uncover a rich theory. When S is specified by a set of dependencies with a finite chase, we show how to construct an S-inverse when one exists. In particular, we show how to construct a global inverse when one exists. Given M and M′, we show how to define the largest class S such that M′ is an S-inverse of M. © 2007 ACM.",Chase; Computational complexity; Data exchange; Data integration; Dependencies; Inverse; Metadata model management; Schema mapping; Second-order logic,Computational complexity; Data structures; Electronic data interchange; Inverse kinematics; Metadata; Model checking; Data integration; Metadata model management; Schema mapping; Second-order logic; Conformal mapping
Introduction to the EDBT 2006 special section,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949015606&doi=10.1145%2f1292609.1292617&partnerID=40&md5=a08f49145ab53381c9a90bb18ac08f0b,[No abstract available],,
Unified framework for fast exact and approximate search in dissimilarity spaces,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949005004&doi=10.1145%2f1292609.1292619&partnerID=40&md5=cf07f76fba0f04a39f940b0e75baca93,"In multimedia systems we usually need to retrieve database (DB) objects based on their similarity to a query object, while the similarity assessment is provided by a measure which defines a (dis)similarity score for every pair of DB objects. In most existing applications, the similarity measure is required to be a metric, where the triangle inequality is utilized to speed up the search for relevant objects by use of metric access methods (MAMs), for example, the M-tree. A recent research has shown, however, that nonmetric measures are more appropriate for similarity modeling due to their robustness and ease to model a made-to-measure similarity. Unfortunately, due to the lack of triangle inequality, the nonmetric measures cannot be directly utilized by MAMs. From another point of view, some sophisticated similarity measures could be available in a black-box nonanalytic form (e.g., as an algorithm or even a hardware device), where no information about their topological properties is provided, so we have to consider them as nonmetric measures as well. From yet another point of view, the concept of similarity measuring itself is inherently imprecise and we often prefer fast but approximate retrieval over an exact but slower one. To date, the mentioned aspects of similarity retrieval have been solved separately, that is, exact versus approximate search or metric versus nonmetric search. In this article we introduce a similarity retrieval framework which incorporates both of the aspects into a single unified model. Based on the framework, we show that for any dissimilarity measure (either a metric or nonmetric) we are able to change the amount of triangle inequality, and so obtain an approximate or full metric which can be used for MAM-based retrieval. Due to the varying amount of triangle inequality, the measure is modified in a way suitable for either an exact but slower or an approximate but faster retrieval. Additionally, we introduce the TriGen algorithm aimed at constructing the desired modification of any black-box distance automatically, using just a small fraction of the database. © 2007 ACM.",Approximate and exact search; Similarity retrieval,Approximation theory; Database systems; Information retrieval; Multimedia systems; Tabu search; Approximate and exact search; Black-box nonanalytic forms; Metric access methods (MAM); Similarity retrieval; Search engines
An adaptive packed-memory array,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949001372&doi=10.1145%2f1292609.1292616&partnerID=40&md5=cea382275093451a3ee2d9aad6723cec,"The packed-memory array (PMA) is a data structure that maintains a dynamic set of N elements in sorted order in a Θ(N)-sized array. The idea is to intersperse Θ(N) empty spaces or gaps among the elements so that only a small number of elements need to be shifted around on an insert or delete. Because the elements are stored physically in sorted order in memory or on disk, the PMA can be used to support extremely efficient range queries. Specifically, the cost to scan L consecutive elements is O(1 + L/B) memory transfers. This article gives the first adaptive packed-memory array (APMA), which automatically adjusts to the input pattern. Like the traditional PMA, any pattern of updates costs only O(log2 N) amortized element moves and O(1 + (log2 N)/B) amortized memory transfers per update. However, the APMA performs even better on many common input distributions achieving only O(log N) amortized element moves and O(1+(log N)/B) amortized memory transfers. The article analyzes sequential inserts, where the insertions are to the front of the APMA, hammer inserts, where the insertions ""hammer"" on one part of the APMA, random inserts, where the insertions are after random elements in the APMA, and bulk inserts, where for constant α ∈ [0, 1], Nα elements are inserted after random elements in the APMA. The article then gives simulation results that are consistent with the asymptotic bounds. For sequential insertions of roughly 1.4 million elements, the APMA has four times fewer element moves per insertion than the traditional PMA and running times that are more than seven times faster. © 2007 ACM.",Adaptive packed-memory array; Cache oblivious; Locality preserving; Packed-memory array; Range query; Rebalance; Sequential file maintenance; Sequential scan; Sparse array,Cache memory; Data transfer; Dynamic programming; Query languages; Random processes; Adaptive packed-memory array (APMA); Adaptive packed-memory arrays; Cache oblivious; Locality preserving; Range query; Rebalance; Sequential file maintenance; Sequential scans; Sparse arrays; Adaptive systems
A geometric approach to monitoring threshold functions over distributed data streams,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949013219&doi=10.1145%2f1292609.1292613&partnerID=40&md5=25954638d67c85aeed5f759909c15cf1,"Monitoring data streams in a distributed system is the focus of much research in recent years. Most of the proposed schemes, however, deal with monitoring simple aggregated values, such as the frequency of appearance of items in the streams. More involved challenges, such as the important task of feature selection (e.g., by monitoring the information gain of various features), still require very high communication overhead using naive, centralized algorithms. We present a novel geometric approach which reduces monitoring the value of a function (vis - vis a threshold) to a set of constraints applied locally on each of the streams. The constraints are used to locally filter out data increments that do not affect the monitoring outcome, thus avoiding unnecessary communication. As a result, our approach enables monitoring of arbitrary threshold functions over distributed data streams in an efficient manner. We present experimental results on real-world data which demonstrate that our algorithms are highly scalable, and considerably reduce communication load in comparison to centralized algorithms. © 2007 ACM.",Distributed monitoring,Algorithms; Constraint theory; Data reduction; Feature extraction; Functions; Information analysis; Centralized algorithms; Data increments; Distributed data streams; Distributed monitoring; Information gain; Distributed computer systems
Introduction to ACM SIGMOD 2006 conference papers,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949014559&doi=10.1145%2f1292609.1292610&partnerID=40&md5=e79c7081db3e52e6b7d90bc9604ec2d8,[No abstract available],,
Introduction to the PODS 2006 special section,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949006519&doi=10.1145%2f1292609.1292614&partnerID=40&md5=9b4f6af08b44701054c662b915aeeade,[No abstract available],,
Efficient top-k aggregation of ranked inputs,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548407283&doi=10.1145%2f1272743.1272749&partnerID=40&md5=23a21b83bd90de43d922f160eaf9a5a8,"A top-k query combines different rankings of the same set of objects and returns the k objects with the highest combined score according to an aggregate function. We bring to light some key observations, which impose two phases that any top-k algorithm, based on sorted accesses, should go through. Based on them, we propose a new algorithm, which is designed to minimize the number of object accesses, the computational cost, and the memory requirements of top-k search with monotone aggregate functions. We provide an analysis for its cost and show that it is always no worse than the baseline no random accesses algorithm in terms of computations, accesses, and memory required. As a side contribution, we perform a space analysis, which indicates the memory requirements of top-k algorithms that only perform sorted accesses. For the case, where the required space exceeds the available memory, we propose disk-based variants of our algorithm. We propose and optimize a multiway top-k join operator, with certain advantages over evaluation trees of binary top-k join operators. Finally, we define and study the computation of top-k cubes and the implementation of roll-up and drill-down operations in such cubes. Extensive experiments with synthetic and real data show that, compared to previous techniques, our method accesses fewer objects, while being orders of magnitude faster. © 2007 ACM.",Rank aggregation; Top-k queries,Algorithms; Computational efficiency; Cost effectiveness; Storage allocation (computer); Random accesses algorithm; Rank aggregation; Top-k queries; Top-k search; Query languages
Improving hash join performance through prefetching,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548432523&doi=10.1145%2f1272743.1272747&partnerID=40&md5=e86650f4cc9d0a9da6b010dfe07a015a,"Hash join algorithms suffer from extensive CPU cache stalls. This article shows that the standard hash join algorithm for disk-oriented databases (i.e. GRACE) spends over 80% of its user time stalled on CPU cache misses, and explores the use of CPU cache prefetching to improve its cache performance. Applying prefetching to hash joins is complicated by the data dependencies, multiple code paths, and inherent randomness of hashing. We present two techniques, group prefetching and software-pipelined prefetching, that overcome these complications. These schemes achieve 1.29 - 4.04X speedups for the join phase and 1.37 - 3.49X speedups for the partition phase over GRACE and simple prefetching approaches. Moreover, compared with previous cache-aware approaches (i.e. cache partitioning), the schemes are at least 36% faster on large relations and do not require exclusive use of the CPU cache to be effective. Finally, comparing the elapsed real times when disk I/Os are in the picture, our cache prefetching schemes achieve 1.12 - 1.84X speedups for the join phase and 1.06 - 1.60X speedups for the partition phase over the GRACE hash join algorithm. © 2007 ACM.",CPU cache performance; CPU cache prefetching; Group prefetching; Hash join; Software-pipelined prefetching,Cache memory; Data storage equipment; Distributed database systems; Program processors; Random processes; Real time control; Disk-oriented databases; Group prefetching; Hash join algorithm; Hashing; Algorithms
Multi-resolution bitmap indexes for scientific data,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548455871&doi=10.1145%2f1272743.1272746&partnerID=40&md5=388d9948ec727ea9230908882d7d8626,"The unique characteristics of scientific data and queries cause traditional indexing techniques to perform poorly on scientific workloads, occupy excessive space, or both. Refinements of bitmap indexes have been proposed previously as a solution to this problem. In this article, we describe the difficulties we encountered in deploying bitmap indexes with scientific data and queries from two real-world domains. In particular, previously proposed methods of binning, encoding, and compressing bitmap vectors either were quite slow for processing the large-range query conditions our scientists used, or required excessive storage space. Nor could the indexes easily be built or used on parallel platforms. In this article, we show how to solve these problems through the use of multi-resolution, parallelizable bitmap indexes, which support a fine-grained trade-off between storage requirements and query performance. Our experiments with large data sets from two scientific domains show that multi-resolution, parallelizable bitmap indexes occupy an acceptable amount of storage while improving range query performance by roughly a factor of 10, compared to a single-resolution bitmap index of reasonable size. © 2007 ACM.",Bitmap index; Parallel index; Query processing; Scientific data management,Encoding (symbols); Natural sciences computing; Problem solving; Query processing; Storage allocation (computer); Virtual reality; Bitmap index; Parallel index; Scientific data management; Scientific workloads; Indexing (of information)
Modeling and managing changes in text databases,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548396669&doi=10.1145%2f1272743.1272744&partnerID=40&md5=229302b275fc16dccb5e40dd390005a2,"Large amounts of (often valuable) information are stored in web-accessible text databases. Metasearchers provide unified interfaces to query multiple such databases at once. For efficiency, metasearchers rely on succinct statistical summaries of the database contents to select the best databases for each query. So far, database selection research has largely assumed that databases are static, so the associated statistical summaries do not evolve over time. However, databases are rarely static and the statistical summaries that describe their contents need to be updated periodically to reflect content changes. In this article, we first report the results of a study showing how the content summaries of 152 real web databases evolved over a period of 52 weeks. Then, we show how to use survival analysis techniques in general, and Cox's proportional hazards regression in particular, to model database changes over time and predict when we should update each content summary. Finally, we exploit our change model to devise update schedules that keep the summaries up to date by contacting databases only when needed, and then we evaluate the quality of our schedules experimentally over real web databases. © 2007 ACM.",Distributed information retrieval; Metasearching; Text database selection,Data mining; Information retrieval; Query processing; Scheduling algorithms; Statistical methods; Text processing; Distributed information retrieval; Metasearching; Text database selection; Database systems
Range search on multidimensional uncertain data,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548430892&doi=10.1145%2f1272743.1272745&partnerID=40&md5=ce59e04a66e74901bda6270f6219780a,"In an uncertain database, every object o is associated with a probability density function, which describes the likelihood that o appears at each position in a multidimensional workspace. This article studies two types of range retrieval fundamental to many analytical tasks. Specifically, a nonfuzzy query returns all the objects that appear in a search region rq with at least a certain probability tq. On the other hand, given an uncertain object q, fuzzy search retrieves the set of objects that are within distance εq from q with no less than probability tq. The core of our methodology is a novel concept of probabilistically constrained rectangle, which permits effective pruning/validation of nonqualifying/ qualifying data. We develop a new index structure called the U-tree for minimizing the query overhead. Our algorithmic findings are accompanied with a thorough theoretical analysis, which reveals valuable insight into the problem characteristics, and mathematically confirms the efficiency of our solutions. We verify the effectiveness of the proposed techniques with extensive experiments. © 2007 ACM.",Range search; Uncertain databases,Algorithms; Fuzzy logic; Probabilistic logics; Probability density function; Query processing; Uncertain systems; Index structure; Probabilistically constrained rectangle; Range search; Database systems
SQL query optimization through nested relational algebra,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548432956&doi=10.1145%2f1272743.1272748&partnerID=40&md5=9b7dd0b6d5150dcc16ecd63e560b5e0d,"Most research work on optimization of nested queries focuses on aggregate subqueries. In this article, we show that existing approaches are not adequate for nonaggregate subqueries, especially for those having multiple subqueries and certain comparison operators. We then propose a new efficient approach, the nested relational approach, based on the nested relational algebra. The nested relational approach treats all subqueries in a uniform manner, being able to deal with nested queries of any type and any level. We report on experimental work that confirms that existing approaches have difficulties dealing with nonaggregate subqueries, and that the nested relational approach offers better performance. We also discuss algebraic optimization rules for further optimizing the nested relational approach and the issue of integrating it into relational database systems. © 2007 ACM.",Nested queries; Nested relational algebra; Nonrelational query processing,Algebra; Data structures; Mathematical operators; Optimization; Query processing; Relational database systems; Algebraic optimization rules; Nested queries; Nested relational algebra; Query languages
Query-sensitive embeddings,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547492888&doi=10.1145%2f1242524.1242525&partnerID=40&md5=0dfdf6c521c0fb0bad08c3b4e7a4f5b6,"A common problem in many types of databases is retrieving the most similar matches to a query object. Finding these matches in a large database can be too slow to be practical, especially in domains where objects are compared using computationally expensive similarity (or distance) measures. Embedding methods can significantly speed-up retrieval by mapping objects into a vector space, where distances can be measured rapidly using a Minkowski metric. In this article we present a novel way to improve embedding quality. In particular, we propose to construct embeddings that use a query-sensitive distance measure for the target space of the embedding. This distance measure is used to compare those vectors that the query and database objects are mapped to. The term query-sensitive means that the distance measure changes, depending on the current query object. We demonstrate theoretically that using a query-sensitive distance measure increases the modeling power of embeddings and allows them to capture more of the structure of the original space. We also demonstrate experimentally that query-sensitive embeddings can significantly improve retrieval performance. In experiments with an image database of handwritten digits and a time-series database, the proposed method outperforms existing state-of-the-art non-Euclidean indexing methods, meaning that it provides significantly better tradeoffs between efficiency and retrieval accuracy. © 2007 ACM.",Embedding methods; Nearest-neighbor retrieval; Non-Euclidean spaces; Nonmetric spaces; Similarity matching,Conformal mapping; Database systems; Indexing (of information); Information retrieval; Time series analysis; Embedding methods; Nearest-neighbor retrieval; Non-Euclidean spaces; Nonmetric spaces; Similarity matching; Query processing
Extended wavelets for multiple measures,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547480462&doi=10.1145%2f1242524.1242527&partnerID=40&md5=ed0a0a830d123759f56e80f8259ffd3a,"Several studies have demonstrated the effectiveness of the Haar wavelet decomposition as a tool for reducing large amounts of data down to compact wavelet synopses that can be used to obtain fast, accurate approximate answers to user queries. Although originally designed for minimizing the overall mean-squared (i.e., L 2-norm) error in the data approximation, recently proposed methods also enable the use of Haar wavelets in minimizing other error metrics, such as the relative error in data value reconstruction, which is arguably the most important for approximate query answers. Relatively little attention, however, has been paid to the problem of using wavelet synopses as an approximate query answering tool over complex tabular datasets containing multiple measures, such as those typically found in real-life OLAP applications. Existing decomposition approaches will either operate on each measure individually, or treat all measures as a vector of values and process them simultaneously. As we demonstrate in this article, these existing individual or combined storage approaches for the wavelet coefficients of different measures can easily lead to suboptimal storage utilization, resulting in drastically reduced accuracy for approximate query answers. To address this problem, in this work, we introduce the notion of an extended wavelet coefficient as a flexible, efficient storage method for wavelet coefficients over multimeasure data. We also propose novel algorithms for constructing effective (optimal or near-optimal) extended wavelet-coefficient synopses under a given storage constraint, for both sum-squared error and relative-error norms. Experimental results with both real-life and synthetic datasets validate our approach, demonstrating that our techniques consistently obtain significant gains in approximation accuracy compared to existing solutions. © 2007 ACM.",Approximate query processing; Data synopses; Wavelets,Algorithms; Approximation theory; Constraint theory; Error analysis; Problem solving; Query processing; Approximate query processing; Data synopses; Data value reconstruction; Wavelet transforms
Estimating the selectivity of approximate string queries,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547421874&doi=10.1145%2f1242524.1242529&partnerID=40&md5=bed0743d63072abdfc1aa4a413181647,"Approximate queries on string data are important due to the prevalence of such data in databases and various conventions and errors in string data. We present the VSol estimator, a novel technique for estimating the selectivity of approximate string queries. The VSol estimator is based on inverse strings and makes the performance of the selectivity estimator independent of the number of strings. To get inverse strings we decompose all database strings into overlapping substrings of length q (q-grams) and then associate each q-gram with its inverse string: the IDs of all strings that contain the q-gram. We use signatures to compress inverse strings, and clustering to group similar signatures. We study our technique analytically and experimentally. The space complexity of our estimator only depends on the number of neighborhoods in the database and the desired estimation error. The time to estimate the selectivity is independent of the number of database strings and linear with respect to the length of query string. We give a detailed empirical performance evaluation of our solution for synthetic and real-world datasets. We show that VSol is effective for large skewed databases of short strings. © 2007 ACM.",Inverse strings; Min-wise hash signatures; Q-grams,Approximation algorithms; Database systems; Electronic document identification systems; Error analysis; Linear programming; Virtual reality; Database strings; Inverse strings; Min-wise hash signatures; Q-grams; Query processing
Out-of-core coherent closed quasi-clique mining from large dense graph databases,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547455408&doi=10.1145%2f1242524.1242530&partnerID=40&md5=b7fbcab89ea4de69a5179e1c154ca321,"Due to the ability of graphs to represent more generic and more complicated relationships among different objects, graph mining has played a significant role in data mining, attracting increasing attention in the data mining community. In addition, frequent coherent subgraphs can provide valuable knowledge about the underlying internal structure of a graph database, and mining frequently occurring coherent subgraphs from large dense graph databases has witnessed several applications and received considerable attention in the graph mining community recently. In this article, we study how to efficiently mine the complete set of coherent closed quasi-cliques from large dense graph databases, which is an especially challenging task due to the fact that the downward-closure property no longer holds. By fully exploring some properties of quasi-cliques, we propose several novel optimization techniques which can prune the unpromising and redundant subsearch spaces effectively. Meanwhile, we devise an efficient closure checking scheme to facilitate the discovery of closed quasi-cliques only. Since large databases cannot be held in main memory, we also design an out-of-core solution with efficient index structures for mining coherent closed quasi-cliques from large dense graph databases. We call this Cocain*. Thorough performance study shows that Cocain* is very efficient and scalable for large dense graph databases. © 2007 ACM.",Coherent subgraph; Frequent closed subgraph; Graph mining; Out-of-core algorithm; Quasi-clique,Data structures; Database systems; Graph theory; Indexing (of information); Redundancy; Coherent subgraphs; Frequent closed subgraphs; Graph mining; Out-of-core algorithms; Quasi-clique; Data mining
Pseudo-random number generation for sketch-based estimations,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547438873&doi=10.1145%2f1242524.1242528&partnerID=40&md5=2245380d94b296b0109dfbb672c12fc6,"The exact computation of aggregate queries, like the size of join of two relations, usually requires large amounts of memory (constrained in data-streaming) or communication (constrained in distributed computation) and large processing times. In this situation, approximation techniques with provable guarantees, like sketches, are one possible solution. The performance of sketches depends crucially on the ability to generate particular pseudo-random numbers. In this article we investigate both theoretically and empirically the problem of generating k-wise independent pseudo-random numbers and, in particular, that of generating 3- and 4-wise independent pseudo-random numbers that are fast range-summable (i.e., they can be summed in sublinear time). Our specific contributions are: (a) we provide a thorough comparison of the various pseudo-random number generating schemes; (b) we study both theoretically and empirically the fast range-summation property of 3- and 4-wise independent generating schemes; (c) we provide algorithms for the fast range-summation of two 3-wise independent schemes, BCH and extended Hamming; and (d) we show convincing theoretical and empirical evidence that the extended Hamming scheme performs as well as any 4-wise independent scheme for estimating the size of join of two relations using AMS sketches, even though it is only 3-wise independent. We use this scheme to generate estimators that significantly outperform state-of-the-art solutions for two problems, namely, size of spatial joins and selectivity estimation. © 2007 ACM.",Approximate query processing; Data synopses; Fast range-summation; Sketches,Algorithms; Computation theory; Constrained optimization; Number theory; Problem solving; Query processing; Approximate query processing; Data synopses; Fast range-summation; Pseudo-random numbers; Sketches; Random processes
Optimized stratified sampling for approximate query processing,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547483963&doi=10.1145%2f1242524.1242526&partnerID=40&md5=d83bf529ce84fcbe61c399fbe6aba731,"The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem where, given a workload of queries, we select a stratified random sample of the original data such that the error in answering the workload queries using the sample is minimized. A key novelty of our approach is that we can tailor the choice of samples to be robust, even for workloads that are similar but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server that demonstrate the superior quality of our method compared to previous work. © 2007 ACM.",Approximation; Query processing; Random sampling,Approximation algorithms; Data mining; Error analysis; Optimization; Query processing; Sampling; Data distribution; Data mining tools; Random sampling; Random processes
Efficient estimation of joint queries from multiple OLAP databases,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947678840&doi=10.1145%2f1206049.1206051&partnerID=40&md5=1dec8e0e0812706156c58988b6d20a2b,"Given an OLAP query expressed over multiple source OLAP databases, we study the problem of estimating the resulting OLAP target database. The problem arises when it is not possible to derive the result from a single database. The method we use is linear indirect estimation, commonly used for statistical estimation. We examine two obvious computational methods for computing such a target database, called the full cross-product (F) and preaggregation (P) methods. We study the accuracy and computational cost of these methods. While the F method provides a more accurate estimate, it is more expensive computationally than P. Our contribution is in proposing a third, new method, called the partial preaggregation method (PP), which is significantly less expensive than F, but just as accurate. We prove formally that the PP method yields the same results as the F method, and provide analytical and experimental results on the accuracy and computational benefits of the PP method. © 2007 ACM.",Multiple summary databases; OLAP; Query estimation,Computational complexity; Computational methods; Costs; Problem solving; Relational database systems; Joint queries; Multiple summary databases; OLAP; Preaggregation method (PP); Query estimation; Query languages
Spatial join techniques,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947670542&doi=10.1145%2f1206049.1206056&partnerID=40&md5=0df989c5b877a2f2468f0264df44b53f,"A variety of techniques for performing a spatial join are reviewed. Instead of just summarizing the literature and presenting each technique in its entirety, distinct components of the different techniques are described and each is decomposed into an overall framework for performing a spatial join. A typical spatial join technique consists of the following components: partitioning the data, performing internal-memory spatial joins on subsets of the data, and checking if the full polygons intersect. Each technique is decomposed into these components and each component addressed in a separate section so as to compare and contrast similar aspects of each technique. The goal of this survey is to describe the algorithms within each component in detail, comparing and contrasting competing methods, thereby enabling further analysis and experimentation with each component and allowing the best algorithms for a particular situation to be built piecemeal, or, even better, enabling an optimizer to choose which algorithms to use. © 2007 ACM.",External memory algorithms; Plane-sweep; Spatial join,Algorithms; Computational complexity; Data reduction; Optimization; Relational database systems; External memory algorithms; Plane-sweep; Spatial join; Query languages
Optimizing top-k queries for middleware access: A unified cost-based approach,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947629301&doi=10.1145%2f1206049.1206054&partnerID=40&md5=f387d42b029e4b0b399122e87e4c2f10,"This article studies optimizing top-k queries in middlewares. While many assorted algorithms have been proposed, none is generally applicable to a wide range of possible scenarios. Existing algorithms lack both the generality to support a wide range of access scenarios and the systematic adaptivity to account for runtime specifics. To fulfill this critical lacking, we aim at taking a cost-based optimization approach: By runtime search over a space of algorithms, cost-based optimization is general across a wide range of access scenarios, yet adaptive to the specific access costs at runtime. While such optimization has been taken for granted for relational queries from early on, it has been clearly lacking for ranked queries. In this article, we thus identify and address the barriers of realizing such a unified framework. As the first barrier, we need to define a comprehensive space encompassing all possibly optimal algorithms to search over. As the second barrier and a conflicting goal, such a space should also be focused enough to enable efficient search. For SQL queries that are explicitly composed of relational operators, such a space, by definition, consists of schedules of relational operators (or query plans). In contrast, top-k queries do not have logical tasks, such as relational operators. We thus define the logical tasks of top-k queries as building blocks to identify a comprehensive and focused space for top-k queries. We then develop efficient search schemes over such space for identifying the optimal algorithm. Our study indicates that our framework not only unifies, but also outperforms existing algorithms specifically designed for their scenarios. © 2007 ACM.",Middlewares; Top-k query processing,Algorithms; Computational complexity; Costs; Middleware; Optimization; Relational operators; Systematic adaptivity; Top-k query processing; Query processing
Editorial: Single- versus double-blind reviewing,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947666180&doi=10.1145%2f1206049.1206050&partnerID=40&md5=de6510ef8968953bb0ca01b694c61a13,"This editorial analyzes from a variety of perspectives the controversial issue of single-blind versus double-blind reviewing. In single-blind reviewing, the reviewer is unknown to the author, but the identity of the author is known to the reviewer. Double-blind reviewing is more symmetric: The identity of the author and the reviewer are not revealed to each other. We first examine the significant scholarly literature regarding blind reviewing. We then list six benefits claimed for double-blind reviewing and 21 possible costs. To compare these benefits and costs, we propose a double-blind policy for TODS that attempts to minimize the costs while retaining the core benefit of fairness that double-blind reviewing provides, and evaluate that policy against each of the listed benefits and costs. Following that is a general discussion considering several questions: What does this have to do with TODS, does bias exist in computer science, and what is the appropriate decision procedure We explore the knobs a policy design can manipulate to fine-tune a double-blind review policy. This editorial ends with a specific decision. © 2007 ACM.",Anonymous citation; Blinding efficacy; Double-blind review; Gender bias; Single-blind review; Status bias,Computer science; Costs; Database systems; Decision making; Optimization; Anonymous citation; Blinding efficacy; Double-blind reviewing; Gender bias; Single-blind review; Status bias; Information management
Mining constraint violations,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947632005&doi=10.1145%2f1206049.1206055&partnerID=40&md5=2186eabf8e71eaddd52938f22713fb15,"In this article, we introduce pesudoconstraints, a novel data mining pattern aimed at identifying rare events in databases. At first, we formally define pesudoconstraints using a probabilistic model and provide a statistical test to identify pesudoconstraints in a database. Then, we focus on a specific class of pesudoconstraints, named cycle pesudoconstraints, which often occur in databases. We define cycle pesudoconstraints in the context of the ER model and present an automatic method for detecting cycle pesudoconstraints from a relational database. Finally, we present an experiment to show cycle pesudoconstraints at work on real data. © 2007 ACM.",Deviation detection; Probabilistic models; Relational data mining,Constraint theory; Data structures; Mathematical models; Probabilistic logics; Relational database systems; Deviation detection; Probabilistic models; Relational data mining; Data mining
Composition of mappings given by embedded dependencies,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947667913&doi=10.1145%2f1206049.1206053&partnerID=40&md5=3fd0a12547ba5aab2204dcd5c3441348,"Composition of mappings between schemas is essential to support schema evolution, data exchange data integration, and other data management tasks. In many applications, mappings are given by embedded dependencies. In this article, we study the issues nvolved in composing such mappings. Our algorithms and results extend those of Fagin et al. [2004], who studied the composition of mappings given by several kinds of constraints. In particular, they proved that full source-to-target tuple-generating dependencies(tgds) are closed under composition, but embedded source-to-target tgds are not. They introduced a class of second-order constraints, SO tgds, that is closed under composition and has desirable ro perties for data exchange. We study constraints that need not be source-to-target and we concentrate on obtaining (firstorder) embedded dependencies. As part of this study, we also consider full dependencies and secondorder constraints that arise from kolemizing embedded dependencies. For each of the three classes of mappings that we study, we provide: (a) an algorithm that attempts to compute the composition; and (b) sufficient conditions on the input mappings which guarantee that the algorithm will succeed. In addition, we give several negative results. In particular, we show that full and second-order dependencies that are not limited to be source-to-target are not closed under composition (for the latter, under the additional restriction that no new function symbols are introduced). Furthermore, we show that determining whether the composition can be given by these kinds of dependencies is undecidable. © 2007 ACM.",Database theory; Metadata management,Algorithms; Computational complexity; Electronic data interchange; Information dissemination; Information management; Dependencies; Metadata management; Second-order dependencies; Relational database systems
Forward node-selecting queries over trees,2007,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947651738&doi=10.1145%2f1206049.1206052&partnerID=40&md5=fda5cd68981d5704faef8c326242d3c5,"Node-selecting queries over trees lie at the core of several important XML languages for the web, such as the node-selection language XPath, the query language XQuery, and the transformation language XSLT. The main syntactic constructs of such queries are the backward predicates, for example, ancestor and preceding, and the forward predicates, for example, descendant and following. Forward predicates are included in the depth-first, left-to-right preorder relation associated with the input tree, whereas backward predicates are included in the inverse of this preorder relation. This work is devoted to an expressiveness study of node-selecting queries with proven theoretical and practical applicability, especially in the field of query evaluation against XML streams. The main question it answers positively is whether, for each input query with forward and backward predicates, there exists an equivalent forward-only output query. This question is then positively answered for input and output queries of varying structural complexity, using LOGLIN and PSPACE reductions. Various existing applications based on the results of this work are reported, including query optimization and streamed evaluation. © 2007 ACM.",Expressiveness; Rewriting; Streams; XML; XPath,Computational complexity; Syntactics; Trees (mathematics); User interfaces; World Wide Web; XML; Expressiveness; Rewriting; Streams; Syntactic constructs; XPath; Query languages
Expressive power of an algebra for data mining,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846219163&doi=10.1145%2f1189769.1189770&partnerID=40&md5=8f9518d4d9edd1b130de47d2a5e1893e,"The relational data model has simple and clear foundations on which significant theoretical and systems research has flourished. By contrast, most research on data mining has focused on algorithmic issues. A major open question is: what's an appropriate foundation for data mining, which can accommodate disparate mining tasks? We address this problem by presenting a database model and an algebra for data mining. The database model is based on the 3W-model introduced by Johnson et al. [2000]. This model relied on black box mining operators. A main contribution of this article is to open up these black boxes, by using generic operators in a data mining algebra. Two key operators in this algebra are regionize, which creates regions (or models) from data tuples, and a restricted form of looping called mining loop. Then the resulting data mining algebra MA is studied and properties concerning expressive power and complexity are established. We present results in three directions: (1) expressiveness of the mining algebra; (2) relations with alternative frameworks, and (3) interactions between regionize and mining loop. © 2006 ACM.",Algebra; Data mining; Expressive power,Algebra; Algorithms; Data mining; Information technology; Mathematical models; Relational database systems; Data mining algebra; Generic operators; Mining loop; Database systems
The sort-Merge-Shrink join,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846208392&doi=10.1145%2f1189769.1189775&partnerID=40&md5=2a1629c5a77c5d90ed04d186f3c5f3b6,"One of the most common operations in analytic query processing is the application of an aggregate function to the result of a relational join. We describe an algorithm called the Sort-Merge-Shrink (SMS) Join for computing the answer to such a query over large, disk-based input tables. The key innovation of the SMS join is that if the input data are clustered in a statistically random fashion on disk, then at all times, the join provides an online, statistical estimator for the eventual answer to the query as well as probabilistic confidence bounds. Thus, a user can monitor the progress of the join throughout its execution and stop the join when satisfied with the estimate's accuracy or run the algorithm to completion with a total time requirement that is not much longer than that of other common join algorithms. This contrasts with other online join algorithms, which either do not offer such statistical guarantees or can only offer guarantees so long as the input data can fit into main memory. © 2006 ACM.",Nonparametric statistics; OLAP; Online algorithms,Algorithms; Data processing; Query languages; Statistical methods; Online algorithms; Sort Merge Shrink (SMS) Join; Statistical estimator; Database systems
Foreword to special section on SIGMOD/PODS 2005,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846263708&doi=10.1145%2f1189769.1189776&partnerID=40&md5=d74a37f1f4cf1539ea03a20747518f54,[No abstract available],,
On the complexity of nonrecursive XQuery and functional query languages on complex values,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846217476&doi=10.1145%2f1189769.1189771&partnerID=40&md5=c0a40cb2bca5f60c16251f114c7394e9,"This article studies the complexity of evaluating functional query languages for complex values such as monad algebra and the recursion-free fragment of XQuery. We show that monad algebra, with equality restricted to atomic values, is complete for the class TA[2O(n), O(n)] of problems solvable in linear exponential time with a linear number of alternations if the query is assumed to be part of the input. The monotone fragment of monad algebra with atomic value equality but without negation is NEXPTIME-complete. For monad algebra with deep value equality, that is, equality of complex values, we establish TA[2O(n), O(n)] lower and exponential-space upper bounds. We also study a fragment of XQuery, Core XQuery, that seems to incorporate all the features of a query language on complex values that are traditionally deemed essential. A close connection between monad algebra on lists and Core XQuery (with ""child"" as the only axis) is exhibited. The two languages are shown expressively equivalent up to representation issues. We show that Core XQuery is just as hard as monad algebra with respect to query and combined complexity. As Core XQuery is NEXPTIME-hard, the best-known techniques for processing such problems require exponential amounts of working memory and doubly exponential time in the worst case.We present a property of queries-the lack of a certain form of composition-that virtually all real-world XQueries have and that allows for query evaluation in PSPACE and thus singly exponential time. Still, we are able to show for an important special case-Core XQuery with equality testing restricted to atomic values-that the composition-free language is just as expressive as the language with composition. Thus, under widely-held complexity-theoretic assumptions, the language with composition is an exponentially more succinct version of the composition-free language. © 2006 ACM.",Complex values; Complexity; Conservativity; Expressiveness; Monad algebra; Nested-relational algebra; XML; XQuery,Algebra; Information technology; Query languages; Recursive functions; Core XQuery; Monad algebra; Recursion free fragment; Database systems
A divide-and-merge methodology for clustering,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846213661&doi=10.1145%2f1189769.1189779&partnerID=40&md5=7d80d4ff2e63931088959a9f14a72889,"We present a divide-and-merge methodology for clustering a set of objects that combines a top-down divide phase with a bottom-up merge phase. In contrast, previous algorithms use either top-down or bottom-up methods to construct a hierarchical clustering or produce a flat clustering using local search (e.g., k-means). For the divide phase, which produces a tree whose leaves are the elements of the set, we suggest an efficient spectral algorithm. When the data is in the form of a sparse document-term matrix, we show how to modify the algorithm so that it maintains sparsity and runs in linear space. The merge phase quickly finds the optimal partition that respects the tree for many natural objective functions, for example, k-means, min-diameter, min-sum, correlation clustering, etc. We present a thorough experimental evaluation of the methodology. We describe the implementation of a meta-search engine that uses this methodology to cluster results from web searches. We also give comparative empirical results on several real datasets. © 2006 ACM.",Clustering; Data mining; Information retrieval,Algorithms; Data structures; Functions; Information dissemination; Information retrieval; Metadata; Online searching; Search engines; Clustering; Hierarchical clustering; Web searches; Data mining
Peer data exchange,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846210676&doi=10.1145%2f1189769.1189778&partnerID=40&md5=12e5179332d56f192d492aa1af3f64cb,"In this article, we introduce and study a framework, called peer data exchange, for sharing and exchanging data between peers. This framework is a special case of a full-fledged peer data management system and a generalization of data exchange between a source schema and a target schema. The motivation behind peer data exchange is to model authority relationships between peers, where a source peer may contribute data to a target peer, specified using source-to-target constraints, and a target peer may use target-to-source constraints to restrict the data it is willing to receive, but cannot modify the data of the source peer.A fundamental algorithmic problem in this framework is that of deciding the existence of a solution: given a source instance and a target instance for a fixed peer data exchange setting, can the target instance be augmented in such a way that the source instance and the augmented target instance satisfy all constraints of the setting We investigate the computational complexity of the problem for peer data exchange settings in which the constraints are given by tuple generating dependencies. We show that this problem is always in NP, and that it can be NP-complete even for acyclic peer data exchange settings. We also show that the data complexity of the certain answers of target conjunctive queries is in coNP, and that it can be coNP-complete even for acyclic peer data exchange settings.After this, we explore the boundary between tractability and intractability for deciding the existence of a solution and for computing the certain answers of target conjunctive queries. To this effect, we identify broad syntactic conditions on the constraints between the peers under which the existence-of-solutions problem is solvable in polynomial time. We also identify syntactic conditions between peer data exchange settings and target conjunctive queries that yield polynomial-time algorithms for computing the certain answers. For both problems, these syntactic conditions turn out to be tight, in the sense that minimal relaxations of them lead to intractability. Finally, we introduce the concept of a universal basis of solutions in peer data exchange and explore its properties. © 2006 ACM.",Certain answers; Conjunctive queries; Data exchange; Data integration; Metadata model management; Schema mapping,Computational complexity; Constraint theory; Data reduction; Information management; Metadata; Problem solving; Conjunctive queries; Data exchange; Data integration; Metadata model management; Schema mapping; Electronic data interchange
Adaptive rank-aware query optimization in relational databases,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846192864&doi=10.1145%2f1189769.1189772&partnerID=40&md5=dcf2cffa6b36797ea4df82f4044140d3,"Rank-aware query processing has emerged as a key requirement in modern applications. In these applications, efficient and adaptive evaluation of top-k queries is an integral part of the application semantics. In this article, we introduce a rank-aware query optimization framework that fully integrates rank-join operators into relational query engines. The framework is based on extending the System R dynamic programming algorithm in both enumeration and pruning. We define ranking as an interesting physical property that triggers the generation of rank-aware query plans. Unlike traditional join operators, optimizing for rank-join operators depends on estimating the input cardinality of these operators. We introduce a probabilistic model for estimating the input cardinality, and hence the cost of a rank-join operator. To our knowledge, this is the first effort in estimating the needed input size for optimal rank aggregation algorithms. Costing ranking plans is key to the full integration of rank-join operators in real-world query processing engines.Since optimal execution strategies picked by static query optimizers lose their optimality due to estimation errors and unexpected changes in the computing environment, we introduce several adaptive execution strategies for top-k queries that respond to these unexpected changes and costing errors. Our reactive reoptimization techniques change the execution plan at runtime to significantly enhance the performance of running queries. Since top-k query plans are usually pipelined and maintain a complex ranking state, altering the execution strategy of a running ranking query is an important and challenging task.We conduct an extensive experimental study to evaluate the performance of the proposed framework. The experimental results are twofold: (1) we show the effectiveness of our cost-based approach of integrating ranking plans in dynamic programming cost-based optimizers; and (2) we show a significant speedup (up to 300%) when using our adaptive execution of ranking plans over the state-of-the-art mid-query reoptimization strategies. © 2006 ACM.",Adaptive processing; Advanced query processing; Rank-aware optimization; Ranking; Top-k,Algorithms; Cost effectiveness; Information technology; Mathematical models; Optimization; Relational database systems; Semantics; Optimal rank aggregation algorithms; Probabilistic model; Query optimization; Query processing engines; Rank join operator; Reactive reoptimization techniques; Query languages
Maintaining stream statistics over multiscale sliding windows,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846229332&doi=10.1145%2f1189769.1189773&partnerID=40&md5=76b40f521316dc9c5060154c00c6118c,"In this article, we propose a new multiscale sliding window model which differentiates data items in different time periods of the data stream, based on a reasonable monotonicity of resolution assumption. Our model, as a well-motivated extension of the sliding window model, stands halfway between the traditional all-history and time-decaying models. We also present algorithms for estimating two significant data stream statistics - -F0 and Jacard's similarity coefficient - -with reasonable accuracies under the new model. Our algorithms use space logarithmic in the data stream size and linear in the number of windows; they support update time logarithmic in the number of windows and independent of the accuracy required. Our algorithms are easy to implement. Experimental results demonstrate the efficiencies of our algorithms. Our techniques apply to scenarios in which universe sampling is used. © 2006 ACM.",Data stream; F<sub>0</sub>; Jacard's similarity coefficient; Multiscale sliding window model,Data processing; Information technology; Mathematical models; Statistical methods; Data stream size; Multiscale sliding windows; Sliding window model; Space logarithmic; Database systems
Towards multidimensional subspace skyline analysis,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846200457&doi=10.1145%2f1189769.1189774&partnerID=40&md5=ed22652b70afeabb9996bdcb074fcf6e,"The skyline operator is important for multicriteria decision-making applications. Although many recent studies developed efficient methods to compute skyline objects in a given space, none of them considers skylines in multiple subspaces simultaneously. More importantly, the fundamental problem on the semantics of skylines remains open: Why and in which subspaces is (or is not) an object in the skyline Practically, users may also be interested in the skylines in any subspaces. Then, what is the relationship between the skylines in the subspaces and those in the super-spaces How can we effectively analyze the subspace skylines Can we efficiently compute skylines in various subspaces and answer various analytical queriesIn this article, we tackle the problem of multidimensional subspace skyline computation and analysis. We explore skylines in subspaces. First, we propose the concept of Skycube, which consists of skylines of all possible nonempty subspaces of a given full space. Once a Skycube is materialized, any subspace skyline queries can be answered online. However, Skycube cannot fully address the semantic concerns and may contain redundant information. To tackle the problem, we introduce a novel notion of skyline group which essentially is a group of objects that coincide in the skylines of some subspaces. We identify the decisive subspaces that qualify skyline groups in the subspace skylines. The new notions concisely capture the semantics and the structures of skylines in various subspaces. Multidimensional roll-up and drill-down analysis is introduced. We also develop efficient algorithms to compute Skycube, skyline groups and their decisive subspaces. A systematic performance study using both real data sets and synthetic data sets is reported to evaluate our approach. © 2006 ACM.",Data cubing; Multidimensional data analysis; Skyline query,Algorithms; Data processing; Decision making; Mathematical techniques; Semantics; Skyline operator; Subspace skyline analysis; Subspace skyline queries; Database systems
Feature-based similarity search in graph structures,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846200039&doi=10.1145%2f1189769.1189777&partnerID=40&md5=5a5242d28e8685f5e21afad39364d529,"Similarity search of complex structures is an important operation in graph-related applications since exact matching is often too restrictive. In this article, we investigate the issues of substructure similarity search using indexed features in graph databases. By transforming the edge relaxation ratio of a query graph into the maximum allowed feature misses, our structural filtering algorithm can filter graphs without performing pairwise similarity computation. It is further shown that using either too few or too many features can result in poor filtering performance. Thus the challenge is to design an effective feature set selection strategy that could maximize the filtering capability. We prove that the complexity of optimal feature set selection is (2m) in the worst case, where m is the number of features for selection. In practice, we identify several criteria to build effective feature sets for filtering, and demonstrate that combining features with similar size and selectivity can improve the filtering and search performance significantly within a multifilter composition framework. The proposed feature-based filtering concept can be generalized and applied to searching approximate nonconsecutive sequences, trees, and other structured data as well. © 2006 ACM.",Complexity; Graph database; Index; Similarity search,Algorithms; Computation theory; Computational complexity; Database systems; Graph theory; Indexing (of information); Feature-based filtering; Graph databases; Graph structures; Similarity search; Online searching
Expressiveness and complexity of XML schema,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750148943&doi=10.1145%2f1166074.1166076&partnerID=40&md5=3b2d7fe3fc6f18e7fcb8bc2c7f7811a7,"The common abstraction of XML Schema by unranked regular tree languages is not entirely accurate. To shed some light on the actual expressive power of XML Schema, intuitive semantical characterizations of the Element Declarations Consistent (EDC) rule are provided. In particular, it is obtained that schemas satisfying EDC can only reason about regular properties of ancestors of nodes. Hence, with respect to expressive power, XML Schema is closer to DTDs than to tree automata. These theoretical results are complemented with an investigation of the XML Schema Definitions (XSDs) occurring in practice, revealing that the extra expressiveness of XSDs over DTDs is only used to a very limited extent. As this might be due to the complexity of the XML Schema specification and the difficulty of understanding the effect of constraints on typing and validation of schemas, a simpler formalism equivalent to XSDs is proposed. It is based on contextual patterns rather than on recursive types and it might serve as a light-weight front end for XML Schema. Next, the effect of EDC on the way XML documents can be typed is discussed. It is argued that a cleaner, more robust, larger but equally feasible class is obtained by replacing EDC with the notion of 1-pass preorder typing (1PPT): schemas that allow one to determine the type of an element of a streaming document when its opening tag is met. This notion can be defined in terms of grammars with restrained competition regular expressions and there is again an equivalent syntactical formalism based on contextual patterns. Finally, algorithms for recognition, simplification, and inclusion of schemas for the various classes are given. © 2006 ACM.",Validation; XML; XML Schema,Abstracting; Algorithms; Constraint theory; Pattern recognition; Robustness (control systems); Semantics; XML; Contextual patterns; Element Declarations Consistent (EDC); Validation; XML Schema; Database systems
XSKETCH synopses for XML data graphs,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750183317&doi=10.1145%2f1166074.1166082&partnerID=40&md5=a60d66d0ba47bfbd065f6086191f1316,"Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows (1) path navigation and branching through the label structure of the XML data graph, and (2) predicates on the values of specific path/branch nodes, in order to reach the desired data elements. Clearly, optimizing such queries requires approximating the result cardinality of the referenced paths and hence hinges on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over the base XML data. In this article, we introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model (termed XSKETCH) exploits localized graph stability and value-distribution summaries (e.g., histograms) to accurately approximate (in limited space) the path and branching distribution, as well as the complex correlation patterns that can exist between and across path structure and element values in the data graph. To the best of our knowledge, ours is the first work to address this timely problem in the most general setting of graph-structured XML data with values, and complex (branching) path expressions. © 2006 ACM.",Approximate query processing; Data synopses; Path expressions; XML,Approximation theory; Correlation theory; Data structures; Optimization; Pattern recognition; Query languages; XML; Approximate query processing; Data synopses; Path expressions; Graph theory
Improving instruction cache performance in OLTP,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750190611&doi=10.1145%2f1166074.1166079&partnerID=40&md5=54093b432da811adf2950f95614dc1dd,"Instruction-cache misses account for up to 40% of execution time in online transaction processing (OLTP) database workloads. In contrast to data cache misses, instruction misses cannot be over-lapped with out-of-order execution. Chip design limitations do not allow increases in the size or associativity of instruction caches that would help reduce misses. On the contrary, the effective instruction cache size is expected to further decrease with the adoption of multicore and multithreading chip designs (multiple on-chip processor cores and multiple simultaneous threads per core). Different concurrent database threads, however, execute similar instruction sequences over their lifetime, too long to be captured and exploited in hardware. The challenge, from a software designer's point of view, is to identify and exploit common code paths across threads executing arbitrary operations, thereby eliminating extraneous instruction misses. In this article, we describe Synchronized Threads through Explicit Processor Scheduling (STEPS), a methodology and tool to increase instruction locality in database servers executing transaction processing workloads. STEPS works at two levels to increase reusability of instructions brought in the cache. At a higher level, synchronization barriers form teams of threads that execute the same system component. Within a team, STEPS schedules special fast context-switches at very fine granularity to reuse sets of instructions across team members. To find points in the code where context-switches should occur, we develop autoSTEPS, a code profiling tool that runs directly on the DBMS binary. STEPS can minimize both capacity and conflict instruction cache misses for arbitrarily long code paths. We demonstrate the effectiveness of our approach on Shore, a research prototype database system shown to be governed by similar bottlenecks as commercial systems. Using microbenchmarks on real and simulated processors, we observe that STEPS eliminates up to 96% of instructioncache misses for each additional team thread and at the same time eliminates up to 64% of mispredicted branches by providing a repetitive execution pattern to the processor. When performing a full-system evaluation on real hardware using TPC-C, the industry-standard transactional benchmark, STEPS eliminates two-thirds of instruction-cache misses and provides up to 1.4 overall speedup. © 2006 ACM.",Cache misses; Instruction cache,Computer software; Database systems; Microprocessor chips; Product design; Program processors; Rapid prototyping; Cache misses; Instruction cache; Research prototype database systems; Synchronization barriers; Cache memory
Theory of nearest neighbors indexability,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750197371&doi=10.1145%2f1166074.1166077&partnerID=40&md5=2da9c50435f21298e239ade9d4685de8,"In this article, we consider whether traditional index structures are effective in processing unstable nearest neighbors workloads. It is known that under broad conditions, nearest neighbors workloads become unstable - distances between data points become indistinguishable from each other. We complement this earlier result by showing that if the workload for an application is unstable, you are not likely to be able to index it efficiently using (almost all known) multidimensional index structures. For a broad class of data distributions, we prove that these index structures will do no better than a linear scan of the data as dimensionality increases. Our result has implications for how experiments should be designed on index structures such as R-Trees, X-Trees, and SR-Trees: simply put, experiments trying to establish that these index structures scale with dimensionality should be designed to establish crossover points, rather than to show that the methods scale to an arbitrary number of dimensions. In other words, experiments should seek to establish the dimensionality of the dataset at which the proposed index structure deteriorates to linear scan, for each data distribution of interest; that linear scan will eventually dominate is a given. An important problem is to analytically characterize the rate at which index structures degrade with increasing dimensionality, because the dimensionality of a real data set may well be in the range that a particular method can handle. The results in this article can be regarded as a step toward solving this problem. Although we do not characterize the rate at which a structure degrades, our techniques allow us to reason directly about a broad class of index structures rather than the geometry of the nearest neighbors problem, in contrast to earlier work. © 2006 ACM.",Multidimensional indexing; Nearest neighbors,Data transfer; Database systems; Indexing (of information); Problem solving; Trees (mathematics); Multidimensional indexing; Nearest neighbors; Data structures
The role of domain ontologies in database design: An ontology management and conceptual modeling environment,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750180632&doi=10.1145%2f1166074.1166083&partnerID=40&md5=a75a3501717fb1a27c23751e5695dbe9,"Database design is difficult because it involves a database designer understanding an application and translating the design requirements into a conceptual model. However, the designer may have little or no knowledge about the application or task for which the database is being designed. This research presents a methodology for supporting database design creation and evaluation that makes use of domain-specific knowledge about an application stored in the form of domain ontologies. The methodology is implemented in a prototype system, the Ontology Management and Database Design Environment. Initial testing of the prototype illustrates that the incorporation and use of ontologies is effective in creating entity-relationship models. © 2006 ACM.",Conceptual modeling; Database design; Entity-relationship modeling; Integrity constraints; Ontology; Ontology management and database design environment,Data structures; Knowledge representation; Mathematical models; Product design; Rapid prototyping; Research and development management; Conceptual modeling; Database design; Entity-relationship modeling; Integrity constraints; Ontology; Ontology management and database design environment; Database systems
Introduction to special ICDT section,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750177599&doi=10.1145%2f1166074.1166075&partnerID=40&md5=e8f9f0dc3adee61980fc30d26afe4785,[No abstract available],,
An integrated efficient solution for computing frequent and top-k elements in data streams,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750185779&doi=10.1145%2f1166074.1166084&partnerID=40&md5=ab41280ff663f536e1f90055ef075aff,"We propose an approximate integrated approach for solving both problems of finding the most popular k elements, and finding frequent elements in a data stream coming from a large domain. Our solution is space efficient and reports both frequent and top-k elements with tight guarantees on errors. For general data distributions, our top-k algorithm returns k elements that have roughly the highest frequencies; and it uses limited space for calculating frequent elements. For realistic Zipfian data, the space requirement of the proposed algorithm for solving the exact frequent elements problem decreases dramatically with the parameter of the distribution; and for top-k queries, the analysis ensures that only the top-k elements, in the correct order, are reported. The experiments, using real and synthetic data sets, show space reductions with hardly any loss in accuracy. Having proved the effectiveness of the proposed approach through both analysis and experiments, we extend it to be able to answer continuous queries about frequent and top-k elements. Although the problems of incremental reporting of frequent and top-k elements are useful in many applications, to the best of our knowledge, no solution has been proposed. © 2006 ACM.",Advertising networks; Approximate queries; Continuous queries; Data streams; Exact queries; Frequent elements; Top-k elements; Zipfian data,Algorithms; Approximation theory; Error correction; Problem solving; Query languages; Approximate queries; Continuous queries; Data streams; Exact queries; Frequent elements; Top-k elements; Zipfian data; Data structures
PATAXÓ: A framework to allow updates through XML views,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750196409&doi=10.1145%2f1166074.1166078&partnerID=40&md5=dbdc0d15fdd9656f287bc7da65b76fdc,"XML has become an important medium for data exchange, and is frequently used as an interface to (i.e., a view of) a relational database. Although a lot of work has been done on querying relational databases through XML views, the problem of updating relational databases through XML views has not received much attention. In this work, we map XML views expressed using a subset of XQuery to a corresponding set of relational views. Thus, we transform the problem of updating relational databases through XML views into a classical problem of updating relational databases through relational views. We then show how updates on the XML view are mapped to updates on the corresponding relational views. Existing work on updating relational views can then be leveraged to determine whether or not the relational views are updatable with respect to the relational updates, and if so, to translate the updates to the underlying relational database. © 2006 ACM.",Relational databases; Updates; XML views,Data transfer; Database systems; Problem solving; Query languages; Set theory; Relational databases; XML views; XML
Strategies for query unnesting in XML databases,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750200740&doi=10.1145%2f1166074.1166081&partnerID=40&md5=f88a6004d6a704e6ede6700b2b1c6b1c,"Queries formulated in a nested way are very common in XQuery. Unfortunately, their evaluation is usually very inefficient when done in a straightforward fashion. We present a framework for handling nested queries that is based on unnesting the queries after having translated them into an algebra. We not only present a collection of algebraic equivalences, but also supply a strategy on how to use them effectively. The full potential of the approach is demonstrated by applying our rewrites to actual queries and showing that performance gains of several orders of magnitude are possible. © 2006 ACM.",Nested queries; Query decorrelation; Query optimization; XML; XQuery,Algebra; Optimization; Program translators; Query languages; Strategic planning; XML; Nested queries; Query decorrelation; Query optimization; XQuery; Database systems
Triggers over nested views of relational data,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750173774&doi=10.1145%2f1166074.1166080&partnerID=40&md5=33281c2ea91ed854e8f4660d0bf5eb8d,"Current systems that publish relational data as nested (XML) views are passive in the sense that they can only respond to user-initiated queries over the nested views. In this article, we propose an active system whereby users can place triggers on (unmaterialized) nested views of relational data. In this architecture, we present scalable and efficient techniques for processing triggers over nested views by leveraging existing support for SQL triggers over flat relations in commercial relational databases. We have implemented our proposed techniques in the context of the Quark XML middleware system. Our performance results indicate that our proposed techniques are a feasible approach to supporting triggers over nested views of relational data. © 2006 ACM.",Nested views; Relational databases; Triggers; XML,Computer architecture; Database systems; Information retrieval systems; Middleware; Query languages; Nested views; Quark XML middleware system; SQL triggers; XML
Probabilistic information retrieval approach for ranking of database query results,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750181508&doi=10.1145%2f1166074.1166085&partnerID=40&md5=6a6e74a620330aae175052cedec66e09,"We investigate the problem of ranking the answers to a database query when many tuples are returned. In particular, we present methodologies to tackle the problem for conjunctive and range queries, by adapting and applying principles of probabilistic models from information retrieval for structured data. Our solution is domain independent and leverages data and workload statistics and correlations. We evaluate the quality of our approach with a user survey on a real database. Furthermore, we present and experimentally evaluate algorithms to efficiently retrieve the top ranked results, which demonstrate the feasibility of our ranking system. © 2006 ACM.",Automatic ranking; Experimentation; Indexing; Probabilistic information retrieval; Relational queries; User survey; Workload,Correlation theory; Data structures; Database systems; Indexing (of information); Probabilistic logics; Problem solving; Query languages; Statistical methods; Automatic ranking; Probabilistic information retrieval; Relational queries; User survey; Information retrieval
Query optimization in distributed networks of autonomous database systems,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745230763&doi=10.1145%2f1138394.1138397&partnerID=40&md5=4e45af79ad40011722c745698b32f781,"Large-scale distributed environments, where each node is completely autonomous and offers services to its peers through external communication, pose significant challenges to query processing and optimization. Autonomy is the main source of the problem, as it results in lack of knowledge about any particular node with respect to the information it can produce and its characteristics, for example, cost of production or quality of produced results. In this article, inspired by e-commerce technology, we recognize queries as commodities and model query optimization as a trading negotiation process. Subquery answers and subquery operator execution jobs are traded between nodes until deals are struck with some nodes for all of them. Such trading may also occur recursively, in the sense that some nodes may play the role of intermediaries between other nodes (subcontracting). We identify the key parameters of the overall framework and suggest several potential alternatives for each one. In comparison to trading negotiations for e-commerce, query optimization faces unique new challenges that stem primarily from the fact that queries have a complex structure and can be broken into smaller parts. We address these challenges through a particular instantiation of our framework focusing primarily on the optimization algorithms run on ""buying"" and ""selling"" nodes, the evaluation metrics of the queries, and the negotiation strategy. Finally, we present the results of several experiments that demonstrate the performance characteristics of our approach compared to those of traditional query optimization. © 2006 ACM.",Query optimization,Algorithms; Database systems; Distributed computer systems; Query languages; Technology transfer; Nodes; Optimization algorithms; Query optimization; Optimization
Domain-independent data cleaning via analysis of entity-relationship graph,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745266392&doi=10.1145%2f1138394.1138401&partnerID=40&md5=df9ff2fa2c307bea5aa9a95ce47b9601,"In this article, we address the problem of reference disambiguation. Specifically, we consider a situation where entities in the database are referred to using descriptions (e.g., a set of instantiated attributes). The objective of reference disambiguation is to identify the unique entity to which each description corresponds. The key difference between the approach we propose (called RELDC) and the traditional techniques is that RELDC analyzes not only object features but also inter-object relationships to improve the disambiguation quality. Our extensive experiments over two real data sets and over synthetic datasets show that analysis of relationships significantly improves quality of the result. © 2006 ACM.",Connection strength; Data cleaning; Entity resolution; Graph analysis; Reference disambiguation; Relationship analysis; RELDC,Database systems; Graph theory; Object recognition; ]; Connection strength; Data cleaning; Entity resolution; Graph analysis; Reference disambiguation; Relationship analysis; RELDC; Data acquisition
Adaptive pull-based policies for wide area data delivery,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745284304&doi=10.1145%2f1138394.1138399&partnerID=40&md5=751979ea0884ba62cf8f7860b2efef03,"Wide area data delivery requires timely propagation of up-to-date information to thousands of clients over a wide area network. Applications include web caching, RSS source monitoring, and email access via a mobile network. Data sources vary widely in their update patterns and may experience different update rates at different times or unexpected changes to update patterns. Traditional data delivery solutions are either push-based, which requires servers to push updates to clients, or pull-based, which require clients to check for updates at servers. While push-based solutions ensure timely data delivery, they are not always feasible to implement and may not scale to a large number of clients. In this article, we present adaptive pull-based policies that explicitly aim to reduce the overhead of contacting remote servers, compared to existing pull-based policies, while meeting freshness requirements. We model updates to data sources using update histories, and present two novel history-based policies to estimate when updates occur; they are based on individual history and aggregate history. These policies are presented within an architectural framework that supports their deployment either client-side or server-side. We further develop two adaptive policies to handle objects that initially may have insufficient history or objects that experience changes in update patterns. Extensive experimental evaluation using three data traces from diverse applications shows that history-based policies can reduce contact between clients and servers by up to 60% compared to existing pull-based policies while providing a comparable level of data freshness. Our experiments further demonstrate that our adaptive policies can select the best policy to match the behavior of an object and perform better than any individual policy, thus they dominate standalone policies. © 2006 ACM.",Caching; Data delivery; Pull-based; Update models,Client server computer systems; Information retrieval; Mobile telecommunication systems; Servers; Caching; Data delivery; Pull-based; Update models; Database systems
Integrating XML data sources using approximate joins,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745218927&doi=10.1145%2f1132863.1132868&partnerID=40&md5=763876500f1cd8d3b853a9c96470bb03,"XML is widely recognized as the data interchange standard of tomorrow because of its ability to represent data from a variety of sources. Hence, XML is likely to be the format through which data from multiple sources is integrated. In this article, we study the problem of integrating XML data sources through correlations realized as join operations. A challenging aspect of this operation is the XML document structure. Two documents might convey approximately or exactly the same information but may be quite different in structure. Consequently, an approximate match in structure, in addition to content, has to be folded into the join operation. We quantify an approximate match in structure and content for pairs of XML documents using well defined notions of distance. We show how notions of distance that have metric properties can be incorporated in a framework for joins between XML data sources and introduce the idea of reference sets to facilitate this operation. Intuitively, a reference set consists of data elements used to project the data space. We characterize what constitutes a good choice of a reference set, and we propose sampling-based algorithms to identify them. We then instantiate our join framework using the tree edit distance between a pair of trees. We next turn our attention to utilizing well known index structures to improve the performance of approximate XML join operations. We present a methodology enabling adaptation of index structures for this problem, and we instantiate it in terms of the R-tree. We demonstrate the practical utility of our solutions using large collections of real and synthetic XML data sets, varying parameters of interest, and highlighting the performance benefits of our approach. © 2006 ACM.",Approximate joins; Data integration; Joins; Tree edit distance; XML,Algorithms; Correlation methods; Data structures; Database systems; Information dissemination; Trees (mathematics); Approximate joins; Data integration; Joins; Tree edit distance; XML
Automatic complex schema matching across web query interfaces: A correlation mining approach,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745213976&doi=10.1145%2f1132863.1132872&partnerID=40&md5=1275ba033dc9be3712ffd739ec7954b9,"To enable information integration, schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sources. While complex matchings are common, because of their far more complex search space, most existing techniques focus on simple 1:1 matchings. To tackle this challenge, this article takes a conceptually novel approach by viewing schema matching as correlation mining, for our task of matching Web query interfaces to integrate the myriad databases on the Internet. On this ""deep Web,"" query interfaces generally form complex matchings between attribute groups (e.g., {author} corresponds to {first name, last name} in the Books domain). We observe that the co-occurrences patterns across query interfaces often reveal such complex semantic relationships: grouping attributes (e.g., (first name, last name}) tend to be co-present in query interfaces and thus positively correlated. In contrast, synonym attributes are negatively correlated because they rarely co-occur. This insight enables us to discover complex matchings by a correlation mining approach. In particular, we develop the DCM framework, which consists of data preprocessing, dual mining of positive and negative correlations, and finally matching construction. We evaluate the DCM framework on manually extracted interfaces and the results show good accuracy for discovering complex matchings. Further, to automate the entire matching process, we incorporate automatic techniques for interface extraction. Executing the DCM framework on automatically extracted interfaces, we find that the inevitable errors in automatic interface extraction may significantly affect the matching result. To make the DCM framework robust against such ""noisy"" schemas, we integrate it with a novel ""ensemble"" approach, which creates an ensemble of DCM matchers, by randomizing the schema data into many trials and aggregating their ranked results by taking majority voting. As a principled basis, we provide analytic justification of the robustness of the ensemble approach. Empirically, our experiments show that the ""ensemblization"" indeed significantly boosts the matching accuracy, over automatically extracted and thus noisy schema data. By employing the DCM framework with the ensemble approach, we thus complete an automatic process of matchings Web query interfaces. © 2006 ACM.",Bagging predictors; Correlation mining; Data integration; Deep Web; Ensemble; Schema matching,Correlation methods; Data mining; Interfaces (computer); Internet; Query languages; World Wide Web; Bagging predictors; Correlation mining; Data integration; Deep Web; Ensemble; Schema matching; Pattern matching
Optimizing bitmap indices with efficient compression,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745215599&doi=10.1145%2f1132863.1132864&partnerID=40&md5=b37e7c6f9e63fdc718be6b9ea44c1bc2,"Bitmap indices are efficient for answering queries on low-cardinality attributes. In this article, we present a new compression scheme called Word-Aligned Hybrid (WAH) code that makes compressed bitmap indices efficient even for high-cardinality attributes. We further prove that the new compressed bitmap index, like the best variants of the B-tree index, is optimal for one-dimensional range queries. More specifically, the time required to answer a one-dimensional range query is a linear function of the number of hits. This strongly supports the well-known observation that compressed bitmap indices are efficient for multidimensional range queries because results of one-dimensional range queries computed with bitmap indices can be easily combined to answer multidimensional range queries. Our timing measurements on range queries not only confirm the linear relationship between the query response time and the number of hits, but also demonstrate that WAH compressed indices answer queries faster than the commonly used indices including projection indices, B-tree indices, and other compressed bitmap indices. © 2006 ACM.",Bitmap index; Compression; Query processing,Optimization; Query languages; Response time (computer systems); Trees (mathematics); Bitmap index; Compression; Projection indices; Query processing; Image compression
Summarizing level-two topological relations in large spatial datasets,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745245022&doi=10.1145%2f1138394.1138398&partnerID=40&md5=fc55bb78918f172ad1e4944dff87e3cc,"Summarizing topological relations is fundamental to many spatial applications including spatial query optimization. In this article, we present several novel techniques to effectively construct cell density based spatial histograms for range (window) summarizations restricted to the four most important level-two topological relations: contains, contained, overlap, and disjoint. We first present a novel framework to construct a multiscale Euler histogram in 2D space with the guarantee of the exact summarization results for aligned windows in constant time. To minimize the storage space in such a multiscale Euler histogram, an approximate algorithm with the approximate ratio 19/12 is presented, while the problem is shown NP-hard generally. To conform to a limited storage space where a multiscale histogram may be allowed to have only k Euler histograms, an effective algorithm is presented to construct multiscale histograms to achieve high accuracy in approximately summarizing aligned windows. Then, we present a new approximate algorithm to query an Euler histogram that cannot guarantee the exact answers; it runs in constant time. We also investigate the problem of nonaligned windows and the problem of effectively partitioning the data space to support nonaligned window queries. Finally, we extend our techniques to 3D space. Our extensive experiments against both synthetic and real world datasets demonstrate that the approximate multiscale histogram techniques may improve the accuracy of the existing techniques by several orders of magnitude while retaining the cost efficiency, and the exact multiscale histogram technique requires only a storage space linearly proportional to the number of cells for many popular real datasets. © 2006 ACM.",Histograms; Spatial query processing and optimization,Algorithms; Approximation theory; Optimization; Problem solving; Query languages; Topology; Cost efficiency; Histograms; Spatial query processing and optimization; Database systems
Maintenance of k-nn and spatial join queries on continuously moving points,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745275587&doi=10.1145%2f1138394.1138396&partnerID=40&md5=4f584a7d4938db445971ff179a11c9e4,"Cars, aircraft, mobile cell phones, ships, tanks, and mobile robots all have the common property that they are moving objects. A kinematic representation can be used to describe the location of these objects as a function of time. For example, a moving point can be represented by the function p(t) = x→ 0 + (t - t 0)v→ where x→ 0 is the start location, t 0 is the start time, and v→ is its velocity vector. Instead of storing the location of the object at a given time in a database, the coefficients of the function are stored. When an object's behavior changes enough so that the function describing its location is no longer accurate, the function coefficients for the object are updated. Because the location of each object is represented as a function of time, spatial query results can change even when no transactions update the database. We present efficient algorithms to maintain k -nearest neighbor, and spatial join queries in this domain as time advances and updates occur. We assume no previous knowledge of what the updates will be before they occur. We experimentally compare these new algorithms with more straight forward adaptations of previous work to support updates. Experiments are conducted using synthetic uniformly distributed data, and real aircraft flight data. The primary metric of comparison is the number of I/O disk accesses needed to maintain the query results and the supporting data structures. © 2006 ACM.",Continuously moving objects; K-nearest neighbor; Materialized view maintenance; Moving object databases; Spatial join; Temporal databases,Data acquisition; Functions; Inverse kinematics; Knowledge acquisition; Mobile robots; Vectors; Continuously moving objects; K-nearest neighbor; Materialized view maintenance; Moving object databases; Spatial join; Temporal databases; Query languages
Rewriting queries with arbitrary aggregation functions using views,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745247381&doi=10.1145%2f1138394.1138400&partnerID=40&md5=ffa3aaf8421d0a7e0be46f3f692a4ec2,"The problem of rewriting aggregate queries using views is studied for conjunctive queries with arbitrary aggregation functions and built-in predicates. Two types of queries over views are introduced for rewriting aggregate queries: pure candidates and aggregate candidates. Pure candidates can be used to rewrite arbitrary aggregate queries. Aggregate candidates can be used to rewrite queries containing aggregate functions definable in terms of a commutative-semigroup operation. For both types of candidates (as well as for several relaxations of these candidates), the unfolding property holds. This allows characterizations for query equivalence to be used to determine whether a candidate is a rewriting of a query. The complexity of the rewriting-existence problem is also studied and upper and lower complexity bounds are given. © 2006 ACM.",Query equivalence; Query rewriting; View usability,Computational complexity; Function evaluation; Problem solving; Query equivalence; Query rewriting; View usability; Query languages
Topological relationships between complex spatial objects,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745215387&doi=10.1145%2f1132863.1132865&partnerID=40&md5=ebec9eb77c7397a5d7b6935ae13d2738,"For a longtime topological relationships between spatial objects have been a focus of research in a number of disciplines like artificial intelligence, cognitive science, linguistics, robotics, and spatial reasoning. Especially as predicates they support the design of suitable query languages for spatial data retrieval and analysis in spatial databases and geographical information systems (GIS). Unfortunately, they have so far only been defined for and applicable to simplified abstractions of spatial objects like single points, continuous lines, and simple regions. With the introduction of complex spatial data types an issue arises regarding the design, definition, and number of topological relationships operating on these complex types. This article closes this gap and first introduces definitions of general and versatile spatial data types for complex points, complex lines, and complex regions. Based on the well known 9-intersection model, it then determines the complete sets of mutually exclusive topological relationships for all type combinations. Completeness and mutual exclusion are shown by a proof technique called proof-by-constraint-and-drawing. Due to the resulting large numbers of predicates and the difficulty of handling them, the user is provided with the concepts of topological cluster predicates and topological predicate groups, which permit one to reduce the number of predicates to be dealt with in a user-defined and/or application-specific manner. © 2006 ACM.",9-intersection model; Complex spatial data type; Proof-by-constraint-and-drawing; Topological cluster predicate; Topological constraint rule; Topological predicate; Topological predicate group,Artificial intelligence; Cognitive systems; Formal logic; Geographic information systems; Information retrieval; Object recognition; Query languages; Robotics; 9-intersection model; Complex spatial data type; Proof-by-constraint-and-drawing; Topological cluster predicate; Topological constraint rule; Topological predicate; Topological predicate group; Topology
B-Tree concurrency control and recovery in page-server database systems,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745221394&doi=10.1145%2f1132863.1132866&partnerID=40&md5=c40badf59a05191dde5c7d55094ca825,"We develop new algorithms for the management of transactions in a page-shipping client-server database system in which the physical database is organized as a sparse B-tree index. Our starvation-free fine-grained locking protocol combines adaptive callbacks with key-range locking and guarantees repeatable-read-level isolation (i.e., serializability) for transactions containing any number of record insertions, record deletions, and key-range scans. Partial and total roll-backs of client transactions are performed by the client. Each structure modification such as a page split or merge is defined as an atomic action that affects only two levels of the B-tree and is logged using a single redo-only log record, so that the modification never needs to be undone during transaction rollback or restart recovery. The steal-and-no-force buffering policy is applied by the server when flushing updated pages onto disk and by the clients when shipping updated data pages to the server, while pages involved in a structure modification are forced to the server when the modification is finished. The server performs the restart recovery from client and system failures using an ARIES/CSA-based recovery protocol. Our algorithms avoid accessing stale data but allow a data page to be updated by one client transaction and read by many other client transactions simultaneously, and updates may migrate from a data page to another in structure modifications caused by other transactions while the updating transaction is still active. © 2006 ACM.",Design; General Terms: Algorithms,Algorithms; Client server computer systems; Concurrency control; Data structures; Information management; Trees (mathematics); General Terms: Algorithms; Page-shipping; Physical database; Repeatable-read-level isolation; Database systems
Dynamic indexing for multidimensional non-ordered discrete data spaces using a data-partitioning approach,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745213260&doi=10.1145%2f1138394.1138395&partnerID=40&md5=55120439b6ffeb78c753ecfc934892e2,"Similarity searches in multidimensional Non-ordered Discrete Data Spaces (NDDS) are becoming increasingly important for application areas such as bioinformatics, biometrics, data mining and E-commerce. Efficient similarity searches require robust indexing techniques. Unfortunately, existing indexing methods developed for multidimensional (ordered) Continuous Data Spaces (CDS) such as the R-tree cannot be directly applied to an NDDS. This is because some essential geometric concepts/properties such as the minimum bounding region and the area of a region in a CDS are no longer valid in an NDDS. Other indexing methods based on metric spaces such as the M-tree and the Slim-trees are too general to effectively utilize the special characteristics of NDDSs, resulting in nonoptimized performance. In this article, we propose a new dynamic data-partitioning-based indexing technique, called the ND-tree, to support efficient similarity searches in an NDDS. The key idea is to extend the relevant geometric concepts as well as some indexing strategies used in CDSs to NDDSs. Efficient algorithms for ND-tree construction and techniques to solve relevant issues such as handling dimensions with different alphabets in an NDDS are presented. Our experimental results on synthetic data and real genome sequence data demonstrate that the ND-tree outperforms the linear scan, the M-tree and the Slim-trees for similarity searches in multidimensional NDDSs. A theoretical model is also developed to predict the performance of the ND-tree for random data. © 2006 ACM.",Hamming distance; Multidimensional index tree; Non-ordered discrete data spaces; Similarity search,Computational geometry; Data structures; Database systems; Indexing (of information); Mathematical models; Trees (mathematics); Hamming distance; Multidimensional index tree; Non-ordered discrete data spaces; Similarity search; Dynamic programming
Sequencing XML data and query twigs for fast pattern matching,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745196648&doi=10.1145%2f1132863.1132871&partnerID=40&md5=af976cfb976563c31163a5a27feaacaa,"We propose a new way of indexing XML documents and processing twig patterns in an XML database. Every XML document in the database can be transformed into a sequence of labels by Prüfer's method that constructs a one-to-one correspondence between trees and sequences. During query processing, a twig pattern is also transformed into its Prüfer sequence. By performing subsequence matching on the set of sequences in the database and performing a series of refinement phases that we have developed, we can find all the occurrences of a twig pattern in the database. Our approach allows holistic processing of a twig pattern without breaking the twig into root-to-leaf paths and processing these paths individually. Furthermore, we show in the article that all correct answers are found without any false dismissals or false alarms. Experimental results demonstrate the performance benefits of our proposed techniques. © 2006 ACM.",Prüfer sequences; Twig query processing; XML indexing,Database systems; Mathematical transformations; Pattern matching; Query languages; Trees (mathematics); False dismissals; Prüfer sequences; Twig query processing; XML indexing; XML
"Indexing the past, present, and anticipated future positions of moving objects",2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745208652&doi=10.1145%2f1132863.1132870&partnerID=40&md5=24bea58232ce1d4842449e937e40860d,"With the proliferation of wireless communications and geo-positioning, e-services are envisioned that exploit the positions of a set of continuously moving users to provide context-aware functionality to each individual user. Because advances in disk capacities continue to outperform Moore's Law, it becomes increasingly feasible to store online all the position information obtained from the moving e-service users. With the much slower advances in I/O speeds and many concurrent users, indexing techniques are of the essence in this scenario. Existing indexing techniques come in two forms. Some techniques capture the position of an object up until the time of the most recent position sample, while other techniques represent an object's position as a constant or linear function of time and capture the position from the current time and into the (near) future. This article offers an indexing technique capable of capturing the positions of moving objects at all points in time. The index substantially modifies partial persistence techniques, which support transaction time, to support valid time for monitoring applications. The performance of a timeslice query is independent of the number of past position samples stored for an object. No existing indices exist with these characteristics. © 2006 ACM.",Continuous variable; Indexing; Moving object; Polyline; Querying; Trajectory; Update,Concurrency control; Information analysis; Object recognition; Query languages; Wireless telecommunication systems; Continuous variable; Moving object; Polyline; Querying; Update; Indexing (of information)
Consensus on transaction commit,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745187320&doi=10.1145%2f1132863.1132867&partnerID=40&md5=c66183775f40f2113061b58185d66815,"The distributed transaction commit problem requires reaching agreement on whether a transaction is committed or aborted. The classic Two-Phase Commit protocol blocks if the coordinator fails. Fault-tolerant consensus algorithms also reach agreement, but do not block whenever any majority of the processes are working. The Paxos Commit algorithm runs a Paxos consensus algorithm on the commit/abort decision of each participant to obtain a transaction commit protocol that uses 2F + 1 coordinators and makes progress if at least F + 1 of them are working properly. Paxos Commit has the same stable-storage write delay, and can be implemented to have the same message delay in the fault-free case as Two-Phase Commit, but it uses more messages. The classic Two-Phase Commit algorithm is obtained as the special F = 0 case of the Paxos Commit algorithm. © 2006 ACM.",Consensus; Paxos; Two-phase commit,Algorithms; Decision making; Network protocols; Problem solving; Consensus; Paxos; Stable-storage write delay; Two-phase commit; Distributed computer systems
Approximation and streaming algorithms for histogram construction problems,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745187544&doi=10.1145%2f1132863.1132873&partnerID=40&md5=f1fe352270500de47adfef5a86ec5738,"Histograms and related synopsis structures are popular techniques for approximating data distributions. These have been successful in query optimization and a variety of applications, including approximate querying, similarity searching, and data mining, to name a few. Histograms were a few of the earliest synopsis structures proposed and continue to be used widely. The histogram construction problem is to construct the best histogram restricted to a space bound that reflects the data distribution most accurately under a given error measure. The histograms are used as quick and easy estimates. Thus, a slight loss of accuracy, compared to the optimal histogram under the given error measure, can be offset by fast histogram construction algorithms. A natural question arises in this context: Can we find a fast near optimal approximation algorithm for the histogram construction problem? In this article, we give the first linear time (1+ε)-factor approximation algorithms (for any ε ≥ 0) for a large number of histogram construction problems including the use of piece wise small degree polynomials to approximate data, workloads, etc. Several of our algorithms extend to data streams. Using synthetic and real-life data sets, we demonstrate that in many scenarios the approximate histograms are almost identical to optimal histograms in quality and are significantly faster to construct. © 2006 ACM.",Approximation algorithm; Data Streams; Histograms,Approximation theory; Data mining; Error analysis; Online searching; Optimization; Problem solving; Query languages; Approximation algorithm; Data Streams; Histograms; Algorithms
Representing and querying XML with incomplete information,2006,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745203463&doi=10.1145%2f1132863.1132869&partnerID=40&md5=05ddcd1ef0d5787eacc7c82ea52dd2dd,"We study the representation and querying of XML with incomplete information. We consider a simple model for XML data and their DTDs, a very simple query language, and a representation system for incomplete information in the spirit of the representations systems developed by Imielinski and Lipski [1984] for relational databases. In the scenario we consider, the incomplete information about an XML document is continuously enriched by successive queries to the document. We show that our representation system can represent partial information about the source document acquired by successive queries, and that it can be used to intelligently answer new queries. We also consider the impact on complexity of enriching our representation system or query language with additional features. The results suggest that our approach achieves a practically appealing balance between expressiveness and tractability. © 2006 ACM.",Incomplete information; XML,Computational complexity; Information analysis; Knowledge representation; Query languages; Relational database systems; Incomplete information; Partial information; Representation system; Successive queries; XML
Generalized multidimensional data mapping and query processing,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745214431&doi=10.1145%2f1093382.1093383&partnerID=40&md5=f7d5315e5903a28fc73b8bf8c565d125,"Multidimensional data points can be mapped to one-dimensional space to exploit single dimensional indexing structures such as the B +-tree. In this article we present a Generalized structure for data Mapping and query Processing (GiMP), which supports extensible mapping methods and query processing. GiMP can be easily customized to behave like many competent indexing mechanisms for multi-dimensional indexing, such as the UB-Tree, the Pyramid technique, the iMinMax, and the iDistance. Besides being an extendible indexing structure, GiMP also serves as a framework to study the characteristics of the mapping and hence the efficiency of the indexing scheme. Specifically, we introduce a metric called mapping redundancy to characterize the efficiency of a mapping method in terms of disk page accesses and analyze its behavior for point, range and kNN queries. We also address the fundamental problem of whether an efficient mapping exists and how to define such a mapping for a given data set. © 2005 ACM.",Data mapping; Efficiency; Indexing,Data structures; Efficiency; Indexing (of information); Information technology; Mapping; Problem solving; Query languages; Data mapping; Indexing; Indexing structures; Mapping redundancy; Data processing
Self-tuning cost modeling of user-defined functions in an object-relational DBMS,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745200962&doi=10.1145%2f1093382.1093387&partnerID=40&md5=ccfd0ad66ac5ce9b0063ad636da90baa,"Query optimizers in object-relational database management systems typically require users to provide the execution cost models of user-defined functions (UDFs). Despite this need, however, there has been little work done to provide such a model. The existing approaches are static in that they require users to train the model a priori with pregenerated UDF execution cost data. Static approaches can not adapt to changing UDF execution patterns and thus degrade in accuracy when the UDF executions used for generating training data do not reflect the patterns of those performed during operation. This article proposes a new approach based on the recent trend of self-tuning DBMS by which the cost model is maintained dynamically and incrementally as UDFs are being executed online. In the context of UDF cost modeling, our approach faces a number of challenges, that is, it should work with limited memory, work with limited computation time, and adjust to the fluctuations in the execution costs (e.g., caching effect). In this article, we first provide a set of guidelines for developing techniques that meet these challenges, while achieving accurate and fast cost prediction with small overheads. Then, we present two concrete techniques developed under the guidelines. One is an instance-based technique based on the conventional k-nearest neighbor (KNN) technique which uses a multidimensional index like the R*-tree. The other is a summary-based technique which uses the quadtree to store summary values at multiple resolutions. We have performed extensive performance evaluations comparing these two techniques against existing histogram-based techniques and the KNN technique, using both real and synthetic UDFs/data sets. The results show our techniques provide better performance in most situations considered. © 2005 ACM.",Cost modeling; K -nearest neighbors; Object relational DBMS; Quadtree; Query optimization; Self-tuning,Computer simulation; Costs; Data structures; Mathematical models; Query languages; Storage allocation (computer); Cost modeling; K -nearest neighbors; Object relational Data Based Management Sytems (DBMS); Quadtree; Query optimization; Self tuning; Database systems
Incremental maintenance of shortest distance and transitive closure in first-order logic and SQL,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745187346&doi=10.1145%2f1093382.1093384&partnerID=40&md5=bcc1559b1c8cc68b18c4a99c67425027,"Given a database, the view maintenance problem is concerned with the efficient computation of the new contents of a given view when updates to the database happen. We consider the view maintenance problem for the situation when the database contains a weighted graph and the view is either the transitive closure or the answer to the all-pairs shortest-distance problem (APSD). We give incremental algorithms for APSD, which support both edge insertions and deletions. For transitive closure, the algorithm is applicable to a more general class of graphs than those previously explored. Our algorithms use first-order queries, along with addition (+) and less-than (<) operations (FO(+, <)); they store O(n 2) number of tuples, where n is the number of vertices, and have AC 0 data complexity for integer weights. Since FO(+, <) is a sublanguage of SQL and is supported by almost all current database systems, our maintenance algorithms are more appropriate for database applications than nondatabase query types of maintenance algorithms. © 2005 ACM.",Database view; First-order logic; Incremental algorithm; Maintenance; SQL,Algorithms; Computational complexity; Computational methods; Formal logic; Maintenance; Query languages; Servers; Database view; First-order logic; Incremental algorithms; Structured query language (SQL); Database systems
Database repairing using updates,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745206041&doi=10.1145%2f1093382.1093385&partnerID=40&md5=2e1fe0550b296632647401ef7ceea4dc,"Repairing a database means bringing the database in accordance with a given set of integrity constraints by applying some minimal change. If a database can be repaired in more than one way, then the consistent answer to a query is defined as the intersection of the query answers on all repaired versions of the database. Earlier approaches have confined the repair work to deletions and insertions of entire tuples. We propose a theoretical framework that also covers updates as a repair primitive. Update-based repairing is interesting in that it allows rectifying an error within a tuple without deleting the tuple, thereby preserving consistent values in the tuple. Another novel idea is the construct of nucleus: a single database that yields consistent answers to a class of queries, without the need for query rewriting. We show the construction of nuclei for full dependencies and conjunctive queries. Consistent query answering and constructing nuclei is generally intractable under update-based repairing. Nevertheless, we also show some tractable cases of practical interest. © 2005 ACM.",Consistent query answering; Database repairing,Data structures; Error analysis; Error correction; Query languages; Consistent query answering; Database; Database repairing; Update-based repairing; Database systems
Capturing summarizability with integrity constraints in OLAP,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745214210&doi=10.1145%2f1093382.1093388&partnerID=40&md5=6f0a0649ac5ff36e96254a74c4bbbc47,"In multidimensional data models intended for online analytic processing (OLAP), data are viewed as points in a multidimensional space. Each dimension has structure, described by a directed graph of categories, a set of members for each category, and a child/parent relation between members. An important application of this structure is to use it to infer summarizability, that is, whether an aggregate view defined for some category can be correctly derived from a set of precomputed views defined for other categories. A dimension is called structurally heterogeneous if two members in a given category are allowed to have ancestors in different categories. In this article, we propose a class of integrity constraints, dimension constraints, that allow us to reason about summarizability in heterogeneous dimensions. We introduce the notion of frozen dimensions which are minimal homogeneous dimension instances representing the different structures that are implicitly combined in a heterogeneous dimension. Frozen dimensions provide the basis for efficiently testing the implication of dimension constraints and are a useful aid to understanding heterogeneous dimensions. We give a sound and complete algorithm for solving the implication of dimension constraints that uses heuristics based on the structure of the dimension and the constraints to speed up its execution. We study the intrinsic complexity of the implication problem and the running time of our algorithm. © 2005 ACM.",Data warehousing; Integrity constraints; OLAP; Query-optimization; Summarizability,Algorithms; Data processing; Data warehouses; Graph theory; Mathematical models; Online systems; Set theory; Integrity constraints; Online analytic processing (OLAP); Query optimization; Summarizability; Data structures
LH* RS-A highly-available scalable distributed data structure,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745183330&doi=10.1145%2f1093382.1093386&partnerID=40&md5=290e0e46e015a1dde1efe7c5054a66c7,"LH* RS is a high-availability scalable distributed data structure (SDDS). An LH* RS file is hash partitioned over the distributed RAM of a multicomputer, for example, a network of PCs, and supports the unavailability of any k ≥ 1 of its server nodes. The value of k transparently grows with the file to offset the reliability decline. Only the number of the storage nodes potentially limits the file growth. The high-availability management uses a novel parity calculus that we have developed, based on Reed-Salomon erasure correcting coding. The resulting parity storage overhead is about the lowest possible. The parity encoding and decoding are faster than for any other candidate coding we are aware of. We present our scheme and its performance analysis, including experiments with a prototype implementation on Wintel PCs. The capabilities of LH* RS offer new perspectives to data intensive applications, including the emerging ones of grids and of P2P computing. © 2005 ACM.",Grid computing; High-availability; Linear hashing; P2P; Physical database design; Scalable distributed data structure,Binary codes; Computer networks; Data structures; Personal computers; Servers; Software prototyping; Storage allocation (computer); Grid computing; High availability; Linear hashing; P2P; Physical database design; Scalable distributed data structure; Distributed database systems
Aggregate nearest neighbor queries in spatial databases,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944510654&doi=10.1145%2f1071610.1071616&partnerID=40&md5=a746d6271e87bcd1282d4d2b9de963ec,"Given two spatial datasets P (e.g., facilities) and Q (queries), an aggregate nearest neighbor (ANN) query retrieves the point(s) of P with the smallest aggregate distance(s) to points in Q. Assuming, for example, n users at locations q 1, . . . q n, an ANN query outputs the facility p ε P that minimizes the sum of distances |pq i| for 1 ≤ i ≤ n that the users have to travel in order to meet there. Similarly, another ANN query may report the point p ε P that minimizes the maximum distance that any user has to travel, or the minimum distance from some user to his/her closest facility. If Q fits in memory and P is indexed by an R-tree, we develop algorithms for aggregate nearest neighbors that capture several versions of the problem, including weighted queries and incremental reporting of results. Then, we analyze their performance and propose cost models for query optimization. Finally, we extend our techniques for disk-resident queries and approximate ANN retrieval. The efficiency of the algorithms and the accuracy of the cost models are evaluated through extensive experiments with real and synthetic datasets. © 2005 ACM.",Aggregation; Nearest neighbor queries; Spatial database,Algorithms; Costs; Data storage equipment; Optimization; Query languages; Aggregate nearest neighbor (ANN); Disk-resident queries; Spatial datasets; Weighted queries; Database systems
Efficient algorithms for processing Xpath queries,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944498592&doi=10.1145%2f1071610.1071614&partnerID=40&md5=39b6d547504e98e5e8b408f80f511770,"Our experimental analysis of several popular XPath processors reveals a striking fact: Query evaluation in each of the systems requires time exponential in the size of queries in the worst case. We show that XPath can be processed much more efficiently, and propose main-memory algorithms for this problem with polynomial-time combined query evaluation complexity. Moreover, we show how the main ideas of our algorithm can be profitably integrated into existing XPath processors. Finally, we present two fragments of XPath for which linear-time query processing algorithms exist and another fragment with linear-space/quadratic-time query processing. © 2005 ACM.",Additional Key Words and Phrases: XML; Efficient Algorithms; XPath,Computational complexity; Data storage equipment; Linearization; Polynomials; Program processors; Quadratic programming; Query languages; Linear-space query processing; Linear-time query processing algorithms; Query evaluation; Xpath queries; Algorithms
XQBE (Xquery by example): A visual interface to the standard XML query language,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944509036&doi=10.1145%2f1071610.1071613&partnerID=40&md5=e74bf1adb72c96b4dfe56851d14a0289,"The spreading of XML data in many contexts of modern computing infrastructures and systems causes a pressing need for adequate XML querying capabilities; to address this need, the W3C is proposing XQuery as the standard query language for XML, with a language paradigm and a syntactic flavor comparable to the SQL relational language. XQuery is designed for meeting the requirements of skilled database programmers; its inherent complexity makes the new language unsuited to unskilled users. In this article we present XQBE (XQuery By Example), a visual query language for expressing a large subset of XQuery in a visual form. In designing XQBE, we targeted both unskilled users and expert users wishing to speed up the construction of their queries; we have been inspired by QBE, a relational language initially proposed as an alternative to SQL, which is supported by Microsoft Access. QBE is extremely successful among users who are not computer professionals and do not understand the subtleties of query languages, as well as among professionals who can draft their queries very quickly. According to the hierarchical nature of XML, XQBE's main graphical elements are trees. One or more trees denote the documents assumed as query input, and one tree denotes the document produced by the query. Similar to QBE, trees are annotated so as to express selection predicates, joins, and the passing of information from the input trees to the output tree. This article formally defines the syntax and semantics of XQBE, provides a large set of examples, and presents a prototype implementation. © 2005 ACM.",Additional Key Words and Phrases: Human interfaces; Semi-structured data; Visual query languages; XML; XQuery,Computer programming; Graphic methods; Interfaces (computer); Personnel; Query languages; Semantics; Software engineering; Trees (mathematics); Database programmers; Inherent complexity; Microsoft Access; Relational languages; XML
Tight upper bounds on the number of candidate patterns,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944472645&doi=10.1145%2f1071610.1071611&partnerID=40&md5=606d7ceccde0b07b32daca8dd8aa0b27,"In the context of mining for frequent patterns using the standard levelwise algorithm, the following question arises: given the current level and the current set of frequent patterns, what is the maximal number of candidate patterns that can be generated on the next level? We answer this question by providing tight upper bounds, derived from a combinatorial result from the sixties by Kruskal and Katona. Our result is useful to secure existing algorithms from a combinatorial explosion of the number of candidate patterns. © 2005 ACM.",Additional Key Words and Phrases: Data mining; Frequent patterns; Upper bounds,Algorithms; Combinatorial mathematics; Data mining; Candidate patterns; Combinatorial explosion; Standard levelwise algorithm; Pattern recognition
XSQ: A streaming XPath engine,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944524334&doi=10.1145%2f1071610.1071617&partnerID=40&md5=29551dbdec7d22e393687ca8ad4b320e,"We have implemented and released the XSQ system for evaluating XPath queries on streaming XML data. XSQ supports XPath features such as multiple predicates, closures, and aggregation, which pose interesting challenges for streaming evaluation. Our implementation is based on using a hierarchical arrangement of augmented finite state automata. A design goal of XSQ is buffering data for the least amount of time possible. We present a detailed experimental study that characterizes the performance of XSQ and related systems, and that illustrates the performance implications of XPath features such as closures. © 2005 ACM.",Streaming processing; Xpath,Computer science; Data reduction; Finite automata; XML; Multiple predicates; Related systems; XPath engines; XPath queries; Database systems
Making snapshot isolation serializable,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944513290&doi=10.1145%2f1071610.1071615&partnerID=40&md5=604132a76110e6f8c6a33e244bb93bce,"Snapshot Isolation (SI) is a multiversion concurrency control algorithm, first described in Berenson et al. [1995]. SI is attractive because it provides an isolation level that avoids many of the common concurrency anomalies, and has been implemented by Oracle and Microsoft SQL Server (with certain minor variations). SI does not guarantee serializability in all cases, but the TPC-C benchmark application [TPC-C], for example, executes under SI without serialization anomalies. All major database system products are delivered with default nonserializable isolation levels, often ones that encounter serialization anomalies more commonly than SI, and we suspect that numerous isolation errors occur each day at many large sites because of this, leading to corrupt data sometimes noted in data warehouse applications. The classical justification for lower isolation levels is that applications can be run under such levels to improve efficiency when they can be shown not to result in serious errors, but little or no guidance has been offered to application programmers and DBAs by vendors as to how to avoid such errors. This article develops a theory that characterizes when nonserializable executions of applications can occur under SI. Near the end of the article, we apply this theory to demonstrate that the TPC-C benchmark application has no serialization anomalies under SI, and then discuss how this demonstration can be generalized to other applications. We also present a discussion on how to modify the program logic of applications that are nonserializable under SI so that serializability will be guaranteed. © 2005 ACM.",Anomaly; Concurrency control; Consistency; Multiversion concurrency; Serializability; Snapshot isolation; Weak isolation,Benchmarking; Computer applications; Computer programming languages; Concurrency control; Data reduction; Data warehouses; Error analysis; Relational database systems; Concurrency control algorithms; Program logic; Serialization anomalies; Snapshot isolation (SI); Algorithms
Relational languages for metadata integration,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944487719&doi=10.1145%2f1071610.1071618&partnerID=40&md5=93c15b4726afe02ee787fc6d94301ac9,"In this article, we develop a relational algebra for metadata integration, Federated Interoperable Relational Algebra (FIRA). FIRA has many desirable properties such as compositionality, closure, a deterministic semantics, a modest complexity, support for nested queries, a subalgebra equivalent to canonical Relational Algebra (RA), and robustness under certain classes of schema evolution. Beyond this, FIRA queries are capable of producing fully dynamic output schemas, where the number of relations and/or the number of columns in relations of the output varies dynamically with the input instance. Among existing query languages for relational metadata integration, only FIRA provides generalized dynamic output schemas, where the values in any (fixed) number of input columns can determine output schemas. Further contributions of this article include development of an extended relational model for metadata integration, the Federated Relational Data Model, which is strictly downward compatible with the relational model. Additionally, we define the notion of Transformational Completeness for relational query languages and postulate FIRA as a canonical transformationally complete language. We also give a declarative, SQL-like query language that is equivalent to FIRA, called Federated Interoperable Structured Query Language (FISQL). While our main contributions are conceptual, the federated model, FISQL/FIRA, and the notion of transformational completeness nevertheless have important applications to data integration and OLAP. In addition to summarizing these applications, we illustrate the use of FIRA to optimize FISQL queries using rule-based transformations that directly parallel their canonical relational counterparts. We conclude the article with an extended discussion of related work as well as an indication of current and future work on FISQL/FIRA. © 2005 ACM.",Data integration; Federated data model; Federated databases; Interoperability; Metadata integration; Metadata querying; Multidatabases; Relational query algebra; Schema integration; Transformational completeness,Algebra; Integration; Interoperability; Metadata; Query languages; Semantics; Data integration; Federated interoperable relational algebra (FIRA); Metadata integration; Relational query languages; Computer programming languages
IDistance: An adaptive B +-tree based indexing method for nearest neighbor search,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944466912&doi=10.1145%2f1071610.1071612&partnerID=40&md5=1a4024673fdca52b9748bb24b34f9853,"In this article, we present an efficient B +-tree based indexing method, called iDistance, for K-nearest neighbor (KNN) search in a high-dimensional metric space. iDistance partitions the data based on a space- or data-partitioning strategy, and selects a reference point for each partition. The data points in each partition are transformed into a single dimensional value based on their similarity with respect to the reference point. This allows the points to be indexed using a B +-tree structure and KNN search to be performed using one-dimensional range search. The choice of partition and reference points adapts the index structure to the data distribution. We conducted extensive experiments to evaluate the iDistance technique, and report results demonstrating its effectiveness. We also present a cost model for iDistance KNN search, which can be exploited in query optimization. © 2005 ACM.",Additional Key Words and Phrases: Indexing; KNN; Nearest neighbor queries,Data processing; Data transfer; Optimization; Trees (mathematics); Data-partitioning strategy; High-dimensional metric space; K-nearest neighbor (KNN); Space-partitioning strategy; Indexing (of information)
Advanced SQL modeling in RDBMS,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944519449&doi=10.1145%2f1061318.1061321&partnerID=40&md5=a2eacbf39e8d79902bf960d8a134251a,"Commercial relational database systems lack support for complex business modeling. ANSI SQL cannot treat relations as multidimensional arrays and define multiple, interrelated formulas over them, operations which are needed for business modeling. Relational OLAP (ROLAP) applications have to perform such tasks using joins, SQL Window Functions, complex CASE expressions, and the GROUP BY operator simulating the pivot operation. The designated place in SQL for calculations is the SELECT clause, which is extremely limiting and forces the user to generate queries with nested views, subqueries and complex joins. Furthermore, SQL query optimizers are preoccupied with determining efficient join orders and choosing optimal access methods and largely disregard optimization of multiple, interrelated formulas. Research into execution methods has thus far concentrated on efficient computation of data cubes and cube compression rather than on access structures for random, interrow calculations. This has created a gap that has been filled by spreadsheets and specialized MOLAP engines, which are good at specification of formulas for modeling but lack the formalism of the relational model, are difficult to coordinate across large user groups, exhibit scalability problems, and require replication of data between the tool and RDBMS. This article presents an SQL extension called SQL Spreadsheet, to provide array calculations over relations for complex modeling. We present optimizations, access structures, and execution models for processing them efficiently. Special attention is paid to compile time optimization for expensive operations like aggregation. Furthermore, ANSI SQL does not provide a good separation between data and computation and hence cannot support parameterization for SQL Spreadsheets models. We propose two parameterization methods for SQL. One parameterizes ANSI SQL view using subqueries and scalars, which allows passing data to SQL Spreadsheet. Another method presents parameterization of the SQL Spreadsheet formulas. This supports building stand-alone SQL Spreadsheet libraries. These models are then subject to the SQL Spreadsheet optimizations during model invocation time. © 2005 ACM.",Analytic computations; Excel; OLAP; Spreadsheet,Calculations; Computer operating systems; Optimization; Parameter estimation; Problem solving; Query languages; Spreadsheets; Analytic computations; Excel; OLAP; Relational database systems
XML stream processing using tree-edit distance embeddings,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944432428&doi=10.1145%2f1061318.1061326&partnerID=40&md5=d031bf8d3151008a56923708531f4613,"We propose the first known solution to the problem of correlating, in small space, continuous streams of XML data through approximate (structure and content) matching, as defined by a general tree-edit distance metric. The key element of our solution is a novel algorithm for obliviously embedding tree-edit distance metrics into an L 1 vector space while guaranteeing a (worst-case) upper bound of O (log 2 n log* n) on the distance distortion between any data trees with at most n nodes. We demonstrate how our embedding algorithm can be applied in conjunction with known random sketching techniques to (1) build a compact synopsis of a massive, streaming XML data tree that can be used as a concise surrogate for the full tree in approximate tree-edit distance computations; and (2) approximate the result of tree-edit-distance similarity joins over continuous XML document streams. Experimental results from an empirical study with both synthetic and real-life XML data trees validate our approach, demonstrating that the average-case behavior of our embedding techniques is much better than what would be predicted from our theoretical worst-case distortion bounds. To the best of our knowledge, these are the first algorithmic results on low-distortion embeddings for tree-edit distance metrics, and on correlating (e.g., through similarity joins) XML data in the streaming model. © 2005 ACM.",Approximate query processing; Data streams; Data synopses; Metric-space embeddings; Tree-edit distance; XML,Algorithms; Correlation theory; Data acquisition; Query languages; Data streams; Data synopses; Tree edit distance; XML
Data exchange: Getting to the core,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944491687&doi=10.1145%2f1061318.1061323&partnerID=40&md5=c99cb1144c47f123e3e7b691fad56fe0,"Data exchange is the problem of taking data structured under a source schema and creating an instance of a target schema that reflects the source data as accurately as possible. Given a source instance, there may be many solutions to the data exchange problem, that is, many target instances that satisfy the constraints of the data exchange problem. In an earlier article, we identified a special class of solutions that we call universal. A universal solution has homomorphisms into every possible solution, and hence is a ""most general possible"" solution. Nonetheless, given a source instance, there may be many universal solutions. This naturally raises the question of whether there is a ""best"" universal solution, and hence a best solution for data exchange. We answer this question by considering the well-known notion of the core of a structure, a notion that was first studied in graph theory, and has also played a role in conjunctive-query processing. The core of a structure is the smallest substructure that is also a homomorphic image of the structure. All universal solutions have the same core (up to isomorphism); we show that this core is also a universal solution, and hence the smallest universal solution. The uniqueness of the core of a universal solution together with its minimality make the core an ideal solution for data exchange. We investigate the computational complexity of producing the core. Well-known results by Chandra and Merlin imply that, unless P = NP, there is no polynomial-time algorithm that, given a structure as input, returns the core of that structure as output. In contrast, in the context of data exchange, we identify natural and fairly broad conditions under which there are polynomial-time algorithms for computing the core of a universal solution. We also analyze the computational complexity of the following decision problem that underlies the computation of cores: given two graphs G and H, is H the core of G? Earlier results imply that this problem is both NP-hard and coNP-hard. Here, we pinpoint its exact complexity by establishing that it is a DP-complete problem. Finally, we show that the core is the best among all universal solutions for answering existential queries, and we propose an alternative semantics for answering queries in data exchange settings. © 2005 ACM.",Certain answers; Chase; Computational complexity; Conjunctive queries; Core; Data exchange; Data integration; Dependencies; Query answering; Universal solutions,Computational complexity; Constraint theory; Problem solving; Query languages; Semantics; Chase; Core; Dependencies; Universal solutions; Data acquisition
Progressive skyline computation in database systems,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944477932&doi=10.1145%2f1061318.1061320&partnerID=40&md5=8daec6d5921b96f6faab806a1ebe1029,"The skyline of a d -dimensional dataset contains the points that are not dominated by any other point on all dimensions. Skyline computation has recently received considerable attention in the database community, especially for progressive methods that can quickly return the initial results without reading the entire database. All the existing algorithms, however, have some serious shortcomings which limit their applicability in practice. In this article we develop branch-and-bound skyline (BBS), an algorithm based on nearest-neighbor search, which is I/O optimal, that is, it performs a single access only to those nodes that may contain skyline points. BBS is simple to implement and supports all types of progressive processing (e.g., user preferences, arbitrary dimensionality, etc). Furthermore, we propose several interesting variations of skyline computation, and show how BBS can be applied for their efficient processing. © 2005 ACM.",Branch-and-bound algorithms; Multidimensional access methods; Skyline query,Algorithms; Calculations; Datasets; Multidimensional access methods; Skyline query; Database systems
Exchanging intensional XML data,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944503352&doi=10.1145%2f1061318.1061319&partnerID=40&md5=eece073ed348928cab0ebdb8e8e2e380,"XML is becoming the universal format for data exchange between applications. Recently, the emergence of Web services as standard means of publishing and accessing data on the Web introduced a new class of XML documents, which we call intensional documents. These are XML documents where some of the data is given explicitly while other parts are denned only intensionally by means of embedded calls to Web services. When such documents are exchanged between applications, one has the choice of whether or not to materialize the intensional data (i.e., to invoke the embedded calls) before the document is sent. This choice may be influenced by various parameters, such as performance and security considerations. This article addresses the problem of guiding this materialization process. We argue that - like for regular XML data - schemas (à la DTD and XML Schema) can be used to control the exchange of intensional data and, in particular, to determine which data should be materialized before sending a document, and which should not. We formalize the problem and provide algorithms to solve it. We also present an implementation that complies with real-life standards for XML data, schemas, and Web services, and is used in the Active XML system. We illustrate the usefulness of this approach through a real-life application for peer-to-peer news exchange. © 2005 ACM.",Data exchange; Intensional information; Typing; Web services; XML,Control theory; Data acquisition; Parameter estimation; Problem solving; World Wide Web; Data exchange; Intensional information; Typing; Web services; XML
What's hot and what's not: Tracking most frequent items dynamically,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944436942&doi=10.1145%2f1061318.1061325&partnerID=40&md5=f801ad91126c1eb7339c52d08e20c649,"Most database management systems maintain statistics on the underlying relation. One of the important statistics is that of the ""hot items"" in the relation: those that appear many times (most frequently, or more than some threshold). For example, end-biased histograms keep the hot items as part of the histogram and are used in selectivity estimation. Hot items are used as simple outliers in data mining, and in anomaly detection in many applications. We present new methods for dynamically determining the hot items at any time in a relation which is undergoing deletion operations as well as inserts. Our methods maintain small space data structures that monitor the transactions on the relation, and, when required, quickly output all hot items without rescanning the relation in the database. With user-specified probability, all hot items are correctly reported. Our methods rely on ideas from ""group testing."" They are simple to implement, and have provable quality, space, and time guarantees. Previously known algorithms for this problem that make similar quality and performance guarantees cannot handle deletions, and those that handle deletions cannot make similar guarantees without rescanning the database. Our experiments with real and synthetic data show that our algorithms are accurate in dynamically tracking the hot items independent of the rate of insertions and deletions. © 2005 ACM.",Approximate query answering; Data stream processing,Algorithms; Data structures; Probability; Problem solving; Statistical methods; Approximate query answering; Data stream processing; Transactions; Database systems
TinyDB: An acquisitional query processing system for sensor networks,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944487783&doi=10.1145%2f1061318.1061322&partnerID=40&md5=cc415f37de4014a8738b0cce0c3ea440,"We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. © 2005 ACM.",Data acquisition; Query processing; Sensor networks,Data acquisition; Electric power utilization; Passive systems; Query processing; Query processor; Sensors
Concise descriptions of subsets of structured sets,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23944444342&doi=10.1145%2f1061318.1061324&partnerID=40&md5=da1d3c8f2cba4ff40242f34b04229ac6,"We study the problem of economical representation of subsets of structured sets, which are sets equipped with a set cover or a family of preorders. Given a structured set U, and a language ℒ whose expressions define subsets of U, the problem of minimum description length in ℒ(ℒ-MDL) is: ""given a subset V of U, find a shortest string in ℒ that defines V."" Depending on the structure and the language, the MDL-problem is in general intractable. We study the complexity of the MDL-problem for various structures and show that certain specializations are tractable. The families of focus are hierarchy, linear order, and their multidimensional extensions; these are found in the context of statistical and OLAP databases. In the case of general OLAP databases, data organization is a mixture of multidimensionality, hierarchy, and ordering, which can also be viewed naturally as a cover-structured ordered set. Efficient algorithms are provided for the MDL-problem for hierarchical and linearly ordered structures, and we prove that the multidimensional extensions are NP-complete. Finally, we illustrate the application of the theory to summarization of large result sets and (multi) query optimization for ROLAP queries © 2005 ACM.",Minimal description length; OLAP; Query optimization; Summarization,Algorithms; Database systems; Formal languages; Hierarchical systems; Optimization; Problem solving; OLAP; Query optimization; Summarization; Set theory
Composing schema mappings: Second-order dependencies to the rescue,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745201812&doi=10.1145%2f1114244.1114249&partnerID=40&md5=f483a0e70c60096a7228a8365af17a71,"A schema mapping is a specification that describes how data structured under one schema (the source schema) is to be transformed into data structured under a different schema (the target schema). A fundamental problem is composing schema mappings: given two successive schema mappings, derive a schema mapping between the source schema of the first and the target schema of the second that has the same effect as applying successively the two schema mappings. In this article, we give a rigorous semantics to the composition of schema mappings and investigate the definability and computational complexity of the composition of two schema mappings. We first study the important case of schema mappings in which the specification is given by a finite set of source-to-target tuple-generating dependencies (source-to-target tgds). We show that the composition of a finite set of full source-to-target tgds with a finite set of tgds is always definable by a finite set of source-to-target tgds, but the composition of a finite set of source-to-target tgds with a finite set of full source-to-target tgds may not be definable by any set (finite or infinite) of source-to-target tgds; furthermore, it may not be definable by any formula of least fixed-point logic, and the associated composition query may be NP-complete. After this, we introduce a class of existential second-order formulas with function symbols and equalities, which we call second-order tgds, and make a case that they are the ""right"" language for composing schema mappings. Specifically, we show that second-order tgds form the smallest class (up to logical equivalence) that contains every source-to-target tgd and is closed under conjunction and composition. Allowing equalities in second-order tgds turns out to be of the essence, even though the ""obvious"" way to define second-order tgds does not require equalities. We show that second-order tgds without equalities are not sufficiently expressive to define the composition of finite sets of source-to-target tgds. Finally, we show that second-order tgds possess good properties for data exchange and query answering: the chase procedure can be extended to second-order tgds so that it produces polynomial-time computable universal solutions in data exchange settings specified by second-order tgds. © 2005 ACM.",Certain answers; Chase; Composition; Computational complexity; Conjunctive queries; Data exchange; Data integration; Dependencies; Metadata model management; Query answering; Schema mapping; Second-order logic; Universal solution,Composition; Computational complexity; Data structures; Metadata; Semantics; Set theory; Certain answers; Chase; Data exchange; Data integration; Dependencies; Metadata model management; Query answering; Schema mapping; Second-order logic; Universal solution; Conformal mapping
Optimization of query streams using semantic prefetching,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745189362&doi=10.1145%2f1114244.1114250&partnerID=40&md5=45d4285957ed777dabd1492abe4123f6,"Streams of relational queries submitted by client applications to database servers contain patterns that can be used to predict future requests. We present the Scalpel system, which detects these patterns and optimizes request streams using context-based predictions of future requests. Scalpel uses its predictions to provide a form of semantic prefetching, which involves combining a predicted series of requests into a single request that can be issued immediately. Scalpel's semantic prefetching reduces not only the latency experienced by the application but also the total cost of query evaluation. We describe how Scalpel learns to predict optimizable request patterns by observing the application's request stream during a training phase. We also describe the types of query pattern rewrites that Scalpel's cost-based optimizer considers. Finally, we present empirical results that show the costs and benefits of Scalpel's optimizations. © 2005 ACM.",Prefetching; Query streams,Context sensitive languages; Database systems; Optimization; Semantics; Servers; Cost-based optimizers; Prefetching; Query evaluation; Query streams; Query languages
ACM Transactions on Database Systems: Foreword,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745191835&doi=10.1145%2f1114244.1114245&partnerID=40&md5=d2f2dcf15557a92feaf8fdd890ddf1ee,[No abstract available],,
Wavelet synopses for general error metrics,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745197947&doi=10.1145%2f1114244.1114246&partnerID=40&md5=527c69dd03b525a2a66e1fb4acc3e91d,"Several studies have demonstrated the effectiveness of the wavelet decomposition as a tool for reducing large amounts of data down to compact wavelet synopses that can be used to obtain fast, accurate approximate query answers. Conventional wavelet synopses that greedily minimize the overall root-mean-squared (i.e., L2-norm) error in the data approximation can suffer from important problems, including severe bias and wide variance in the quality of the data reconstruction, and lack of nontrivial guarantees for individual approximate answers. Thus, probabilistic thresholding schemes have been recently proposed as a means of building wavelet synopses that try to probabilistically control maximum approximation-error metrics (e.g., maximum relative error). A key open problem is whether it is possible to design efficient deterministic wavelet-thresholding algorithms for minimizing general, non-L2 error metrics that are relevant to approximate query processing systems, such as maximum relative or maximum absolute error. Obviously, such algorithms can guarantee better maximum-error wavelet synopses and avoid the pitfalls of probabilistic techniques (e.g., ""bad"" coin-flip sequences) leading to poor solutions; in addition, they can be used to directly optimize the synopsis construction process for other useful error metrics, such as the mean relative error in data-value reconstruction. In this article, we propose novel, computationally efficient schemes for deterministic wavelet thresholding with the objective of optimizing general approximation-error metrics. We first consider the problem of constructing wavelet synopses optimized for maximum error, and introduce an optimal low polynomial-time algorithm for one-dimensional wavelet thresholding - our algorithm is based on a new Dynamic-Programming (DP) formulation, and can be employed to minimize the maximum relative or absolute error in the data reconstruction. Unfortunately, directly extending our one-dimensional DP algorithm to multidimensional wavelets results in a super-exponential increase in time complexity with the data dimensionality. Thus, we also introduce novel, polynomial-time approximation schemes (with tunable approximation guarantees) for deterministic wavelet thresholding in multiple dimensions. We then demonstrate how our optimal and approximate thresholding algorithms for maximum error can be extended to handle a broad, natural class of distributive error metrics, which includes several important error measures, such as mean weighted relative error and weighted Lp-norm error. Experimental results on real-world and synthetic data sets evaluate our novel optimization algorithms, and demonstrate their effectiveness against earlier wavelet-thresholding schemes. © 2005 ACM.",Approximate query processing; Data synopses; Haar wavelets,Approximation theory; Computation theory; Design; Dynamic programming; Error analysis; Problem solving; Query languages; Approximate query processing; Data synopses; Haar wavelets; Wavelet transforms
Graph indexing based on discriminative frequent structure analysis,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745185086&doi=10.1145%2f1114244.1114248&partnerID=40&md5=d65c6296fe9e7b187a2aca4dbc7fe3c3,"Graphs have become increasingly important in modelling complicated structures and schemaless data such as chemical compounds, proteins, and XML documents. Given a graph query, it is desirable to retrieve graphs quickly from a large database via indices. In this article, we investigate the issues of indexing graphs and propose a novel indexing model based on discriminative frequent structures that are identified through a graph mining process. We show that the compact index built under this model can achieve better performance in processing graph queries. Since discriminative frequent structures capture the intrinsic characteristics of the data, they are relatively stable to database updates, thus facilitating sampling-based feature extraction and incremental index maintenance. Our approach not only provides an elegant solution to the graph indexing problem, but also demonstrates how database indexing and query processing can benefit from data mining, especially frequent pattern mining. Furthermore, the concepts developed here can be generalized and applied to indexing sequences, trees, and other complicated structures as well. © 2005 ACM.",Frequent pattern; Graph database; Index,Data mining; Data structures; Graph theory; Mathematical models; Proteins; XML; Database indexing; Frequent pattern; Graph database; Index; Indexing (of information)
Conditional XPath,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745196199&doi=10.1145%2f1114244.1114247&partnerID=40&md5=2ef758a1bb8ca869932ad91cd884a2e1,"XPath 1.0 is a variable free language designed to specify paths between nodes in XML documents. Such paths can alternatively be specified in first-order logic. The logical abstraction of XPath 1.0, usually called Navigational or Core XPath, is not powerful enough to express every first-order definable path. In this article, we show that there exists a natural expansion of Core XPath in which every first-order definable path in XML document trees is expressible. This expansion is called Conditional XPath. It contains additional axis relations of the form (child: :n[F])+, denoting the transitive closure of the path expressed by child: :n[F]. The difference with XPath's descendant : :n[F] is that the path (child: :n[F] )+ is conditional on the fact that all nodes in between the start and end node of the path should also be labeled by n and should make the predicate F true. This result can be viewed as the XPath analogue of the expressive completeness of the relational algebra with respect to first-order logic. © 2005 ACM.",Semistructured data; XML; XPath,Logic design; Matrix algebra; Trees (mathematics); XML; Conditional XPath; First-order definable path; Semistructured data; XPath; Computer programming languages
Synopses for query optimization: A space-complexity perspective,2005,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745216885&doi=10.1145%2f1114244.1114251&partnerID=40&md5=0519c4bbbdaac4319168899f1a4e5a85,"Database systems use precomputed synopses of data to estimate the cost of alternative plans during query optimization. A number of alternative synopsis structures have been proposed, but histograms are by far the most commonly used. While histograms have proved to be very effective in (cost estimation for) single-table selections, queries with joins have long been seen as a challenge; under a model where histograms are maintained for individual tables, a celebrated result of loannidis and Christodoulakis [1991] observes that errors propagate exponentially with the number of joins in a query. In this article, we make two main contributions. First, we study the space complexity of using synopses for query optimization from a novel information-theoretic perspective. In particular, we offer evidence in support of histograms for single-table selections, including an analysis over data distributions known to be common in practice, and illustrate their limitations for join queries. Second, for a broad class of common queries involving joins (specifically, all queries involving only key-foreign key joins) we show that the strategy of storing a small precomputed sample of the database yields probabilistic guarantees that are almost space-optimal, which is an important property if these samples are to be used as database statistics. This is the first such optimality result, to our knowledge, and suggests that precomputed samples might be an effective way to circumvent the error propagation problem for queries with key-foreign key joins. We support this result empirically through an experimental study that demonstrates the effectiveness of precomputed samples, and also shows the increasing difference in the effectiveness of samples versus multidimensional histograms as the number of joins in the query grows. © 2005 ACM.",Cardinality estimation; Histograms; Sampling,Computational complexity; Database systems; Estimation; Optimization; Probabilistic logics; Sampling; Cardinality estimation; Error propagation; Histograms; Key-foreign key joins; Query languages
Processing XML streams with deterministic automata and stream indexes,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11344273306&doi=10.1145%2f1042046.1042051&partnerID=40&md5=8dea759f5545ca2730f385e1b8e85bdc,"We consider the problem of evaluating a large number of XPath expressions on a stream of XML packets. We contribute two novel techniques. The first is to use a single Deterministic Finite Automaton (DFA). The contribution here is to show that the DFA can be used effectively for this problem: in our experiments we achieve a constant throughput, independently of the number of XPath expressions. The major issue is the size of the DFA, which, in theory, can be exponential in the number of XPath expressions. We provide a series of theoretical results and experimental evaluations that show that the lazy DFA has a small number of states, for all practical purposes. These results are of general interest in XPath processing, beyond stream processing. The second technique is the Streaming IndeX (SLK), which consists of adding a small amount of binary data to each XML packet that allows the query processor to achieve significant speedups. As an application of these techniques we describe the XML Toolkit (XMLTK), a collection of command-line tools providing highly scalable XML data processing.",Stream processing; XML processing,Algorithms; Automata theory; Data processing; Finite automata; Formal languages; Problem solving; Free languages; Stream index; Stream processing; XML processing; XML
Multiversion-based view maintenance over distributed data sources,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11344267374&doi=10.1145%2f1042046.1042049&partnerID=40&md5=9dec4dbcfb98ba2b71820df863781fa5,"Materialized views can be maintained by submitting maintenance queries to the data sources. However, the query results may be erroneous due to concurrent source updates. State-of-the-art maintenance strategies typically apply compensations to resolve such conflicts and assume all source schemata remain stable over time. In a loosely coupled dynamic environment, the sources may autonomously change not only their data but also their schema or semantics. Consequently, either the maintenance or the compensation queries may be broken. Unlike compensation-based approaches found in the literature, we instead model the complete materialized view maintenance process as a view maintenance transaction (VM_Transaction). This way, the anomaly problem can be rephrased as the serializability of VM_Transactions. To achieve VM_Transaction serializability, we propose a multiversion concurrency control algorithm, called TxnWrap, which is shown to be the appropriate design for loosely coupled environments with autonomous data sources. TxnWrap is complementary to the maintenance algorithms proposed in the literature, since it removes con-currency issues from consideration allowing the designer to focus on the maintenance logic. We show several optimizations of TxnWrap, in particular, (1) space optimizations on versioned data materialization and (2) parallel maintenance scheduling. With these optimizations, TxnWrap even outperforms state-of-the-art view maintenance solutions in terms of refresh time. Further, several design choices of TxnWrap are studied each having its respective advantages for certain environmental settings. A correctness proof based on transaction theory for TxnWrap is also provided. Last, we have implemented TxnWrap. The experimental results confirm that TxnWrap achieves predictable performance under a varying rate of concurrency.",Transaction processing; View maintenance,Algorithms; Electronic commerce; Information analysis; Maintenance; Optimization; Scheduling; World Wide Web; Data sources; Experimentation; Transaction processing; View maintenance; Distributed database systems
Essential classification rule sets,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11344271623&doi=10.1145%2f1042046.1042048&partnerID=40&md5=19ccb61839bdcdfa05038d72c705b530,"Given a class model built from a dataset including labeled data, classification assigns a new data object to the appropriate class. In associative classification the class model (i.e., the classifier) is a set of association rules. Associative classification is a promising technique for the generation of highly accurate classifiers. In this article, we present a compact form which encodes without information loss the classification knowledge available in a classification rule set. This form includes the rules that are essential for classification purposes, and thus it can replace the complete rule set. The proposed form is particularly effective in dense datasets, where traditional extraction techniques may generate huge rule sets. The reduction in size of the rule set allows decreasing the complexity of both the rule generation step and the rule pruning step. Hence, classification rule extraction can be performed also with low support, in order to extract more, possibly useful, rules.",Association rules; Associative classification; Concise representations,Algorithms; Computational complexity; Data mining; Data structures; Information analysis; Knowledge acquisition; Mathematical models; Association rules; Associative classification; Concise representations; Datasets; Database systems
Incremental validation of XML documents,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11344272275&doi=10.1145%2f1042046.1042050&partnerID=40&md5=e38b2d1c116685bd46a5ef65ca5ba9f7,"We investigate the incremental validation of XML documents with respect to DTDs, specialized DTDs, and XML Schemas, under updates consisting of element tag renamings, insertions, and deletions. DTDs are modeled as extended context-free grammars. ""Specialized DTDs"" allow the decoupling of element types from element tags. XML Schemas are abstracted as specialized DTDs with limitations on the type assignment. For DTDs and XML Schemas, we exhibit an O(m log n) incremental validation algorithm using an auxiliary structure of size O(n), where n is the size of the document and m the number of updates. The algorithm does not handle the incremental validation of XML Schema wrt renaming of internal nodes, which is handled by the specialized DTDs incremental validation algorithm. For specialized DTDs, we provide an O(m log 2n) incremental algorithm, again using an auxiliary structure of size O(n). This is & significant improvement over brute-force re-validation from scratch. We exhibit a restricted class of DTDs called local that arise commonly in practice and for which incremental validation can be done in practically constant time by maintaining only a list of counters. We present implementations of both general incremental validation and local validation on an XML database built on top of a relational database. Our experimentation includes a study of the applicability of local validation in practice, results on the calibration of parameters of the auxiliary data structure, and results on the performance comparison between the general incremental validation technique, the local validation technique, and brute-force validation from scratch.",Update; Validation; XML,Algorithms; Computer programming languages; Context free grammars; Data reduction; Optimization; Problem solving; Relational database systems; Auxilary structures; Experimentation; Update; Validation; XML
Decoupling partitioning and grouping: Overcoming shortcomings of spatial indexing with bucketing,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11344253943&doi=10.1145%2f1042046.1042052&partnerID=40&md5=e799bc96e90e1379137888fccddfa99f,"The principle of decoupling the partitioning and grouping processes that form the basis of most spatial indexing methods that use tree directories of buckets is explored. The decoupling is designed to overcome the following drawbacks of traditional solutions: (1) multiple postings in disjoint space decomposition methods that lead to balanced trees such as the hB-tree where a node split in the event of node overflow may be such that one of the children of the node that was split becomes a child of both of the nodes resulting from the split; (2) multiple coverage and nondisjointness of methods based on object hierarchies such as the R-tree which lead to nonunique search paths; (3) directory nodes with similarly-shaped hyper-rectangle bounding boxes with minimum occupancy in disjoint space decomposition methods such as those based on quadtrees and k-d trees that make use of regular decomposition. The first two drawbacks are shown to be overcome by the BV-tree where as a result of decoupling the partitioning and grouping processes, the union of the regions associated with the nodes at a given level of the directory does not necessarily contain all of the data points although all searches take the same amount of time. The BV-tree is not plagued by the third drawback. The third drawback is shown to be overcome by the PK-tree where the grouping process is based on ensuring that every node has at least k objects or blocks. The PK-tree is not plagued by the first two drawbacks as they are inapplicable to it. In both cases, the downside of decoupling the partitioning and grouping processes is that the resulting structure is not necessarily balanced, although, since the nodes have a relatively large fanout, the deviation from a balanced structure is relatively small.",BV-trees; Decoupling; Object hierarchies; PK-trees; R-trees; Space decomposition; Spatial indexing,Algorithms; Computer science; Data reduction; Data structures; Database systems; Geographic information systems; BV-trees; Decoupling; Object hierarchies; PK-trees; R-trees; Space decomposition; Spatial indexing; Indexing (of information)
Querying web metadata: Native score management and text support in databases,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11344289038&doi=10.1145%2f1042046.1042047&partnerID=40&md5=54898b473c8ff12d368ec8d024445d9d,"In this article, we discuss the issues involved in adding a native score management system to object-relational databases, to be used in querying Web metadata (that describes the semantic content of Web resources). The Web metadata model is based on topics (representing entities), relationships among topics (called metalinks), and importance scores (sideway values) of topics and metalinks. We extend database relations with scoring functions and importance scores. We add to SQL score-management clauses with well-defined semantics, and propose the sidewayvalue algebra (SVA), to evaluate the extended SQL queries. SQL extensions and the SVA algebra are illustrated through two Web resources, namely, the DBLP Bibliography and the SIGMOD Anthology. SQL extensions include clauses for propagating input tuple importance scores to output tuples during query processing, clauses that specify query stopping conditions, threshold predicates (a type of approximate similarity predicates for text comparisons), and user-defined-function-based predicates. The propagated importance scores are then used to rank and return a small number of output tuples. The query stopping conditions are propagated to SVA operators during query processing. We show that our SQL extensions are well-defined, meaning that, given a database and a query Q, under any query processing scheme, the output tuples of Q and their importance scores stay the same. To process the SQL extensions, we discuss two sideway value algebra operators, namely, sideway value algebra join and topic closure, give their implementation algorithms, and report their experimental evaluations.",Score management for Web applications,Algebra; Algorithms; Design; Formal languages; Metadata; Query languages; Semantics; Experimentation; Querying; Score management for web applications; Sideway value algebra (SVA); Database systems
Managing uncertainty in moving objects databases,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-7544226852&doi=10.1145%2f1016028.1016030&partnerID=40&md5=1baddce3de889daa0986d93c44c48150,"This article addresses the problem of managing Moving Objects Databases (MODs) which capture the inherent imprecision of the information about the moving object's location at a given time. We deal systematically with the issues of constructing and representing the trajectories of moving objects and querying the MOD. We propose to model an uncertain trajectory as a three-dimensional (3D) cylindrical body and we introduce a set of novel but natural spatio-temporal operators which capture the uncertainty and are used to express spatio-temporal range queries. We devise and analyze algorithms for processing the operators and demonstrate that the model incorporates the uncertainty in a manner which enables efficient querying, thus striking a balance between the modeling power and computational efficiency. We address some implementation aspects which we experienced in our DOMINO project, as a part of which the operators that we introduce have been implemented. We also report on some experimental observations of a practical relevance. Moving Objects Databases.",,Algebra; Algorithms; Computer science; Information analysis; Mathematical models; Personal computers; Problem solving; Trajectories; Computing devices; Location based services; Miniaturization; Moving objects databases (MOD); Database systems
Exploiting k-constraints to reduce memory overhead in continuous queries over data streams,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-7544219531&doi=10.1145%2f1016028.1016032&partnerID=40&md5=982914aa5453c730bb639614200b76c2,"Continuous queries often require significant run-time state over arbitrary data streams. However, streams may exhibit certain data or arrival patterns, or constraints, that can be detected and exploited to reduce state considerably without compromising correctness. Rather than requiring constraints to be satisfied precisely, which can be unrealistic in a data streams environment, we introduce k-constraints, where k is an adherence parameter specifying how closely a stream adheres to the constraint. (Smaller k's are closer to strict adherence and offer better memory reduction.) We present a query processing architecture, called k-Mon, that detects useful k-constraints automatically and exploits the constraints to reduce run-time state for a wide range of continuous queries. Experimental results showed dramatic state reduction, while only modest computational overhead was incurred for our constraint monitoring and query execution algorithms.",Constraints; Continuous queries; Data streams,Algorithms; Data reduction; Internet; Optimization; Parameter estimation; Problem solving; Constraints; Continuous queries; Data streams; Network monitoring; Database systems
Strong functional dependencies and their application to normal forms in XML,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-7544221362&doi=10.1145%2f1016028.1016029&partnerID=40&md5=e83db74f5ef7c034596bf7760bb9ef73,"In this article, we address the problem of how to extend the definition of functional dependencies (FDs) in incomplete relations to XML documents (called XFDs) using the well-known strong satisfaction approach. We propose a syntactic definition of strong XFD satisfaction in an XML document and then justify it by showing that, similar to the case in relational databases, for the case of simple paths, keys in XML are a special case of XFDs. We also propose a normal form for XML documents based on our definition of XFDs and provide a formal justification for it by proving that it is a necessary and sufficient condition for the elimination of redundancy in an XML document.",Functional dependency; Normalization; XML,Algorithms; Computer science; Information theory; Problem solving; Relational database systems; Axiomatization; Functional dependency; Normalization; XML Documents (XMLD); XML
A declarative approach to optimize bulk loading into databases,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142547351&doi=10.1145%2f1005566.1005567&partnerID=40&md5=0ed3729911780d38b7df2567bedfd269,A general optimization framework developed for bulk loading based on a declarative approach that follows the old database principle of logical and physical independence was discussed. The algebra developed was used to abstract the program for capturing processing capabilities of the source and the target systems and enable the application of algebraic rewritings to optimize the loading process. A search strategy was developed to split the loading process into several programs to control failures during loading. A declarative relational-to-object view language to define class and attribute creation from source relations was also designed.,Algebra; Declarative bulk loading; Recovery; Side-effects,Costs; Data transfer; Data warehouses; High level languages; Linear algebra; Mathematical models; Optimization; Reliability; Robustness (control systems); Societies and institutions; Cluet and Moerkotte (CO); Cost models; Data volumes; Declarative bulk loading; Side-effects; Target systems; Database systems
A compressed accessibility map for XML,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142624792&doi=10.1145%2f1005566.1005570&partnerID=40&md5=f5f8b51fa18f658cff8922e7d333fd35,"XML is the undisputed standard for data representation and exchange. As companies transact business over the Internet, letting authorized customers directly access, and even modify, XML data offers many advantages in terms of cost, accuracy, and timeliness. Given the complex business relationships between companies, and the sensitive nature of information, access must be provided selectively, using sophisticated access control specifications. Using the specification directly to determine if a user has access to an XML data item can be extremely inefficient. The alternative of fully materializing, for each data item, the users authorized to access it can be space-inefficient. In this article, we introduce a compressed accessibility map (CAM) as a space- and time-efficient solution to the access control problem for XML data. A CAM compactly identifies the XML data items to which a user has access, by exploiting structural locality of accessibility in tree-structured data. We present a CAM lookup algorithm for determining if a user has access to a data item that takes time proportional to the product of the depth of the item in the XML data and logarithm of the CAM size. We develop an algorithm for building an optimal size CAM that takes time linear in the size of the XML data set. While optimality cannot be preserved incrementally under data item updates, we provide an algorithm for incrementally maintaining near-optimality. Finally, we experimentally demonstrate the effectiveness of the CAM for multiple users on a variety of real and synthetic data sets.",Access control; Structural locality; XML,Administrative data processing; Algorithms; Costs; Customer satisfaction; Data reduction; Information analysis; Internet; Access control; Compressed accessibility maps (CAM); Experimentations; Structural locality; XML
Evaluating top-k queries over web-accessible databases,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142592508&doi=10.1145%2f1005566.1005569&partnerID=40&md5=0c58341a20155892fb2ce7c8df9d2495,"A query to a web search engine usually consists of a list of keywords, to which the search engine responds with the best or ""top"" k pages for the query. This top-k query model is prevalent over multimedia collections in general, but also over plain relational data for certain applications. For example, consider a relation with information on available restaurants, including their location, price range for one diner, and overall food rating. A user who queries such a relation might simply specify the user's location and target price range, and expect in return the best 10 restaurants in terms of some combination of proximity to the user, closeness of match to the target price range, and overall food rating. Processing top-k queries efficiently is challenging for a number of reasons. One critical such reason is that, in many web applications, the relation attributes might not be available other than through external web-accessible form interfaces, which we will have to query repeatedly for a potentially large set of candidate objects. In this article, we study how to process top-k queries efficiently in this setting, where the attributes for which users specify target values might be handled by external, autonomous sources with a variety of access interfaces. We present a sequential algorithm for processing such queries, but observe that any sequential top-k query processing strategy is bound to require unnecessarily long query processing times, since web accesses exhibit high and variable latency. Fortunately, web sources can be probed in parallel, and each source can typically process concurrent requests, although sources may impose some restrictions on the type and number of probes that they are willing to accept. We adapt our sequential query processing technique and introduce an efficient algorithm that maximizes source-access parallelism to minimize query response time, while satisfying source-access constraints.",Parallel query processing; Query optimization; Top-k query processing; Web databases,Algorithms; Cost effectiveness; Interfaces (computer); Program processors; Random access storage; Relational database systems; Response time (computer systems); Search engines; World Wide Web; Parallel query processing; Query optimization; Top -k query processing; Web databases; Query languages
Expressing and optimizing sequence queries in database systems,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142592507&doi=10.1145%2f1005566.1005568&partnerID=40&md5=4652256aa1a9f8afd3f1c8f8428db97f,"The need to search for complex and recurring patterns in database sequences is shared by many applications. In this paper, we investigate the design and optimization of a query language capable of expressing and supporting efficiently the search for complex sequential patterns in database systems. Thus, we first introduce SQL-TS, an extension of SQL to express these patterns, and then we study how to optimize the queries for this language. We take the optimal text search algorithm of Knuth, Morris and Pratt, and generalize it to handle complex queries on sequences. Our algorithm exploits the interdependencies between the elements of a pattern to minimize repeated passes over the same data. Experimental results on typical sequence queries, such as double bottom queries, confirm that substantial speedups are achieved by our new optimization techniques.",Query optimization; Searching; Sequences; Time series,Algorithmic languages; Algorithms; Database systems; Online searching; Optimization; Software prototyping; Time series analysis; Query optimization; Searching; Sequences; Time series; Query languages
Proxy-based acceleration of dynamically generated content on the World Wide Web: An approach and implementation,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142547348&doi=10.1145%2f1005566.1005571&partnerID=40&md5=2c95ba5c6c5d57377b9942488f27ab64,"A fine-grained, proxy-based approach for caching dynamic content, deployable in both reverse proxy or forward proxy mode is proposed. The dynamic proxy caching technique enables granular, proxy-based caching in a fully distributed mode and allows both the content and layout to be dynamic. The approach is found to be capable of providing significant reductions in bandwidth and response times. The proposed dynamic proxy caching technique provides up to 3x reductions in bandwidth and response times in real-world dynamic Web applications.",Caching dynamically generated content; Edge caching; Fragment caching; Implementation; Proxy caching; World wide web,Algorithmic languages; Bandwidth; Computer software reusability; Cost effectiveness; Data structures; Response time (computer systems); Servers; Virtual storage; World Wide Web; Caching dynamically generated content; Edge caching; Fragment caching; Implementation; Internet traffic; Proxy caching; Buffer storage
ACM Transactions on Database Systems: Foreword,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042685207&doi=10.1145%2f974750.974751&partnerID=40&md5=f2a0894111b9ee9388b0cae061b5d6dd,[No abstract available],,
A normal form for XML documents,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042647135&doi=10.1145%2f974750.974757&partnerID=40&md5=1a31ba8ed57d0a503b5b3029a9e9f58d,"This article takes a first step towards the design and normalization theory for XML documents. We show that, like relational databases, XML documents may contain redundant information, and may be prone to update anomalies. Furthermore, such problems are caused by certain functional dependencies among paths in the document. Our goal is to find a way of converting an arbitrary DTD into a well-designed one, that avoids these problems. We first introduce the concept of a functional dependency for XML, and define its semantics via a relational representation of XML. We then define an XML normal form, XNF, that avoids update anomalies and redundancies. We study its properties, and show that XNF generalizes BCNF; we also discuss the relationship between XNF and normal forms for nested relations. Finally, we present a lossless algorithm for converting any DTD into one in XNF.",Design; DTDs; Functional dependencies; Normal form; XML data,Algorithms; Computer programming; Computer programming languages; Context free grammars; Database systems; Graph theory; Program documentation; World Wide Web; DTDs; Functional dependencies; Normal forms; XML data; XML
Accelerating XPath evaluation in any RDBMS,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042554070&doi=10.1145%2f974750.974754&partnerID=40&md5=20aaaf2acfa209d3d7e4b29f1f181a09,"This article is a proposal for a database index structure, the XPath accelerator, that has been specifically designed to support the evaluation of XPath path expressions. As such, the index is capable to support all XPath axes (including ancestor, following, preceding-sibling, descendant-or-self, etc.). This feature lets the index stand out among related work on XML indexing structures which had a focus on the child and descendant axes only. The index has been designed with a close eye on the XPath semantics as well as the desire to engineer its internals so that it can be supported well by existing relational database query processing technology: the index (a) permits set-oriented (or, rather, sequence-oriented) path evaluation, and (b) can be implemented and queried using well-established relational index structures, notably B-trees and R-trees. We discuss the implementation of the XPath accelerator on top of different database backends and show that the index performs well on all levels of the memory hierarchy, including disk-based and main-memory based database systems.",Main-memory databases; XML; XML indexing; XPath,Computer programming languages; Encoding (symbols); Feedback; Hierarchical systems; Optimization; Program compilers; Program documentation; Semantics; Trees (mathematics); XML; Data exchange formats; Main-memory databases; XML indexing; XPath; Database systems
Extracting predicates from mining models for efficient query evaluation,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-7544239870&doi=10.1145%2f1016028.1016031&partnerID=40&md5=dfaace57e46bdb9986a0d43cddd842e7,"Modern relational database systems are beginning to support ad hoc queries on mining models. In this article, we explore novel techniques for optimizing queries that contain predicates on the results of application of mining models to relational data. For such queries, we use the internal structure of the mining model to automatically derive traditional database predicates. We present algorithms for deriving such predicates for a large class of popular discrete mining models: decision trees, naive Bayes, clustering and linear support vector machines. Our experiments on Microsoft SQL Server demonstrate that these derived predicates can significantly reduce the cost of evaluating such queries. Complex predicate optimization, simpler rules from, complex predictive functions.",,Algorithms; Approximation theory; Computer programming; Data mining; Data warehouses; Decision making; Mathematical models; Optimization; Complex predicate optimization; Mining models; Query evaluation; Simpler rules from complex predictive functions; Relational database systems
Probabilistic wavelet synopses,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042590040&doi=10.1145%2f974750.974753&partnerID=40&md5=22467609700ce620bc396ac5ec5dab09,"Recent work has demonstrated the effectiveness of the wavelet decomposition in reducing large amounts of data to compact sets of wavelet coefficients (termed ""wavelet synopses"") that can be used to provide fast and reasonably accurate approximate query answers. A major shortcoming of these existing wavelet techniques is that the quality of the approximate answers they provide varies widely, even for identical queries on nearly identical values in distinct parts of the data. As a result, users have no way of knowing whether a particular approximate answer is highly-accurate or off by many orders of magnitude. In this article, we introduce Probabilistic Wavelet Synopses, the first wavelet-based data reduction technique optimized for guaranteed accuracy of individual approximate answers. Whereas previous approaches rely on deterministic thresholding for selecting the wavelet coefficients to include in the synopsis, our technique is based on a novel, probabilistic thresholding scheme that assigns each coefficient a probability of being included based on its importance to the reconstruction of individual data values, and then flips coins to select the synopsis. We show how our scheme avoids the above pitfalls of deterministic thresholding, providing unbiased, highly accurate answers for individual data values in a data vector. We propose several novel optimization algorithms for tuning our probabilistic thresholding scheme to minimize desired error metrics. Experimental results on real-world and synthetic data sets evaluate these algorithms, and demonstrate the effectiveness of our probabilistic wavelet synopses in providing fast, highly accurate answers with improved quality guarantees.",Approximate query processing; Data synopses; Randomized rounding; Wavelets,Algorithms; Approximation theory; Data mining; Data reduction; Database systems; Decision support systems; Optimization; Probability; Query languages; Approximate query processing; Data synopses; Randomized rounding; Wavelets; Wavelet transforms
Selection conditions in main memory,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042547933&doi=10.1145%2f974750.974755&partnerID=40&md5=47b49b21872ba1d8f02bfc32fcf1ff31,"We consider the fundamental operation of applying a compound filtering condition to a set of records. With large main memories available cheaply, systems may choose to keep the data entirely in main memory, in order to improve query and/or update performance. The design of a data-intensive algorithm in main memory needs to take into account the architectural characteristics of modern processors, just as a disk-based method needs to consider the physical characteristics of disk devices. An important architectural feature that influences the performance of main memory algorithms is the branch misprediction penalty. We demonstrate that branch misprediction has a substantial impact on the performance of an algorithm for applying selection conditions. We describe a space of ""query plans"" that are logically equivalent, but differ in terms of performance due to variations in their branch prediction behavior. We propose a cost model that takes branch prediction into account, and develop a query optimization algorithm that chooses a plan with optimal estimated cost for conjunctive conditions. We also develop an efficient heuristic optimization algorithm. We also show how records can be ordered to further reduce branch misprediction effects. We provide experimental results for a case study based on an event notification system. Our results show the effectiveness of the proposed optimization techniques. Our results also demonstrate that significant improvements in performance can be obtained by applying a methodology that takes branch misprediction latency into account.",Branch misprediction,Algorithms; Codes (symbols); Constraint theory; Database systems; Dynamic programming; Heuristic methods; Mathematical models; Mathematical operators; Mathematical transformations; Optimization; Query languages; Set theory; Branch misprediction; Condition codes; Optimization algorithms; Query optimization; Data storage equipment
Characterizing memory requirements for queries over continuous data streams,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042554067&doi=10.1145%2f974750.974756&partnerID=40&md5=be4395f4620d77560403cf8687e5f1bc,"This article deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams.",Continuous queries; Memory requirement; Streams,Algorithms; Computer science; Data storage equipment; Digital libraries; Mathematical models; Program processors; Query languages; Continuous queries; Evaluation algorithms; Memory requirements; Streams; Database systems
Archiving scientific data,2004,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042693735&doi=10.1145%2f974750.974752&partnerID=40&md5=fec21f7cffcb4d79634f77fef014b2da,"Archiving is important for scientific data, where it is necessary to record all past versions of a database in order to verify findings based upon a specific version. Much scientific data is held in a hierachical format and has a key structure that provides a canonical identification for each element of the hierarchy. In this article, we exploit these properties to develop an archiving technique that is both efficient in its use of space and preserves the continuity of elements through versions of the database, something that is not provided by traditional minimum-edit-distance diff approaches. The approach also uses timestamps. All versions of the data are merged into one hierarchy where an element appearing in multiple versions is stored only once along with a timestamp. By identifying the semantic continuity of elements and merging them into one data structure, our technique is capable of providing meaningful change descriptions, the archive allows us to easily answer certain temporal queries such as retrieval of any specific version from the archive' and finding the history of an element. This is in contrast with approaches that store a sequence of deltas where such operations may require undoing a large number of changes or significant reasoning with the deltas. A suite of experiments also demonstrates that our archive does not incur any significant space overhead when contrasted with diff approaches. Another useful property of our approach is that we use XML format to represent hierarchical data and the resulting archive is also in XML. Hence, XML tools can be directly applied on our archive. In particular, we apply an XML compressor on our archive, and our experiments show that our compressed archive outperforms compressed diff-based repositories in space efficiency. We also show how we can extend our archiving tool to an external memory archiver for higher scalability and describe various index structures that can further improve the efficiency of some temporal queries on our archive.",Keys for XML,Algorithms; Data structures; Hierarchical systems; Program documentation; Query languages; Semantics; Web browsers; World Wide Web; XML; Data elements; Hierarchical formats; Hierarchical structures; Keys for XML; Database systems
Are quorums an alternative for data replication?,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-28444478854&doi=10.1145%2f937598.937601&partnerID=40&md5=05e4ace220d0a036774affe51803b4f8,"Data replication is playing an increasingly important role in the design of parallel information systems. In particular, the widespread use of cluster architectures often requires to replicate data for performance and availability reasons. However, maintaining the consistency of the different replicas is known to cause severe scalability problems. To address this limitation, quorums are often suggested as a way to reduce the overall overhead of replication. In this article, we analyze several quorum types in order to better understand their behavior in practice. The results obtained challenge many of the assumptions behind quorum based replication. Our evaluation indicates that the conventional read-one/write-all-available approach is the best choice for a large range of applications requiring data replication. We believe this is an important result for anybody developing code for computing clusters as the read-one/write-all- available strategy is much simpler to implement and more flexible than quorum-based approaches. In this article, we show that, in addition, it is also the best choice using a number of other selection criteria. © 2003 ACM.",,Codes (symbols); Computational complexity; Computer architecture; Data processing; Information analysis; Parallel processing systems; Computing clusters; Data replication; Parallel information systems; Quorums; Database systems
Removing permissions in the flexible authorization framework,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-20444492231&doi=10.1145%2f937598.937599&partnerID=40&md5=73d77ce76f1bca6fb44d46a99fc72441,"The Flexible Authorization Framework (FAF) defined by Jajodia et al. [2001] provides a policy-neutral framework for specifying access control policies that is expressive enough to specify many known access control policies. Although the original formulation of FAF indicated how rules could be added to or deleted from a FAF specification, it did not address the removal of access permissions from users. We present two options for removing permissions in FAF and provide details on the option which is representation independent. © 2003 ACM.",Access control policy; Authorization; Logic programming,Database systems; Logic programming; User interfaces; Access control policy; Authorization; Flexible Authorization Framework (FAF); Control systems
Preference formulas in relational queries,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142632034&doi=10.1145%2f958942.958946&partnerID=40&md5=90e095329f32cc43ad731be1ab38bfc9,"The handling of user preferences is becoming an increasingly important issue in present-day information systems. Among others, preferences are used for information filtering and extraction to reduce the volume of data presented to the user. They are also used to keep track of user profiles and formulate policies to improve and automate decision making. We propose here a simple, logical framework for formulating preferences as preference formulas. The framework does not impose any restrictions on the preference relations, and allows arbitrary operation and predicate signatures in preference formulas. It also makes the composition of preference relations straightforward. We propose a simple, natural embedding of preference formulas into relational algebra (and SQL) through a single winnow operator parameterized by a preference formula. The embedding makes possible the formulation of complex preference queries, for example, involving aggregation, by piggybacking on existing SQL constructs, It also leads in a natural way to the definition of further, preference-related concepts like ranking. Finally, we present general algebraic laws governing the winnow operator and its interactions with other relational algebra operators. The preconditions on the applicability of the laws are captured by logical formulas. The laws provide a formal foundation for the algebraic optimization of preference queries. We demonstrate the usefulness of our approach through numerous examples.",Preference queries; Preferences; Query optimization; Relational algebra,Algorithms; Artificial intelligence; Costs; Data reduction; Database systems; Decision making; Logic programming; Optimization; Nonmonotonic reasoning; Preference queries; Query optimization; Relational algebra; Query languages
Effective page refresh policies for Web crawlers,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142549047&doi=10.1145%2f958942.958945&partnerID=40&md5=7ea9ca3c85e7a13ce9f460c79b9d9d2a,"In this article, we study how we can maintain local copies of remote data sources ""fresh,"" when the source data is updated autonomously and independently. In particular, we study the problem of Web crawlers that maintain local copies of remote Web pages for Web search engines. In this context, remote data sources (Websites) do not notify the copies (Web crawlers) of new changes, so we need to periodically poll the sources to maintain the copies up-to-date. Since polling the sources takes significant time and resources, it is very difficult to keep the copies completely up-to-date. This article proposes various refresh policies and studies their effectiveness. We first formalize the notion of ""freshness"" of copied data by defining two freshness metrics, and we propose a Poisson process as the change model of data sources. Based on this framework, we examine the effectiveness of the proposed refresh policies analytically and experimentally. We show that a Poisson process is a good model to describe the changes of Web pages and we also show that our proposed refresh policies improve the ""freshness"" of data very significantly. In certain cases, we got orders of magnitude improvement from existing policies.",Page refresh; Web crawlers; Web search engines; World-wide web,Algorithms; Data reduction; Database systems; Information retrieval systems; Search engines; World Wide Web; Data sources; Page refresh; Poisson process; Web crawlers; Websites
Index-driven similarity search in metric spaces,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041664272&doi=10.1145%2f958942.958948&partnerID=40&md5=5661a9f50fefe971b976c1a2e0a70a0c,"Similarity search is a very important operation in multimedia databases and other database applications involving complex objects, and involves finding objects in a data set S similar to a query object q, based on some similarity measure. In this article, we focus on methods for similarity search that make the general assumption that similarity is represented with a distance metric d. Existing methods for handling similarity search in this setting typically fall into one of two classes. The first directly indexes the objects based on distances (distance-based indexing), while the second is based on mapping to a vector space (mapping-based approach). The main part of this article is dedicated to a survey of distance-based indexing methods, but we also briefly outline how search occurs in mapping-based methods. We also present a general framework for performing search based on distances, and present algorithms for common types of queries that operate on an arbitrary ""search hierarchy."" These algorithms can be applied on each of the methods presented, provided a suitable search hierarchy is defined.",Distance-based indexing; Hiearchical metric data structures; Nearest neighbor queries; Range queries; Ranking; Similarity searching,Algorithms; Data reduction; DNA sequences; Hierarchical systems; Information retrieval systems; Multimedia systems; Query languages; Distance based indexing; Hierarchical metric data structures; Range queries; Similarity searching; Database systems
Analysis of predictive spatio-temporal queries,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442427084&doi=10.1145%2f958942.958943&partnerID=40&md5=48cd6a58f839700f6690757db5e8cb78,"Given a set of objects S, a spatio-temporal window query q retrieves the objects of S that will intersect the window during the (future) interval qT. A nearest neighbor query q retrieves the objects of S closest to q during qT. Given a threshold d, a spatio-temporal join retrieves the pairs of objects from two datasets that will come within distance d from each other during qT. In this article, we present probabilistic cost models that estimate the selectivity of spatio-temporal window queries and joins, and the expected distance between a query and its nearest neighbor(s). Our models capture any query/object mobility combination (moving queries, moving objects or both) and any data type (points and rectangles) in arbitrary dimensionality. In addition, we develop specialized spatio-temporal histograms, which take into account both location and velocity information, and can be incrementally maintained. Extensive performance evaluation verifies that the proposed techniques produce highly accurate estimation on both uniform and non-uniform data.",Database; Histogram; Nearest distance; Selectivity; Spatio-temporal,Algorithms; Costs; Data reduction; Database systems; Error analysis; Information retrieval systems; Velocity measurement; Histogram; Nearest distance; Selectivity coefficient; Spatio temporal window query (STWQ); Query languages
Efficient dynamic mining of Constrained Frequent sets,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142594274&doi=10.1145%2f958942.958944&partnerID=40&md5=891e04d43bdcd6d764b30b8f2f79888d,"Data mining is supposed to be an iterative and exploratory process. In this context, we are working on a project with the overall objective of developing a practical computing environment for the human-centered exploratory mining of frequent sets. One critical component of such an environment is the support for the dynamic mining of constrained frequent sets of items. Constraints enable users to impose a certain focus on the mining process; dynamic means that, in the middle of the computation, users are able to (i) change (such as tighten or relax) the constraints and/or (ii) change the minimum support threshold, thus having a decisive influence on subsequent computations. In a real-life situation, the available buffer space may be limit thus adding another complication to the problem. In this article, we develop an algorithm, called DGF, for Dynamic Constrained Frequent-set computation. This algorithm is enhanced with a few optimizations, exploiting a lightweight structure called a segment support map. It enables DGF to (i) obtain sharper bounds on the support of sets of items, and to (ii) better exploit properties of constraints. Furthermore, when handling dynamic changes to constraints, DGF relies on the concept of a delta member generating function, which generates precisely the sets of items that satisfy the new but not the old constraints. Our experimental results show the effectiveness of these enhancements.",Association rules; Constraints; Data mining; Dynamic changes; Frequent sets; Limited buffer space,Algorithms; Computational methods; Constraint theory; Correlation methods; Database systems; Optimization; Association rules; Dynamic changes; Frequent sets; Limited buffers space; Data mining
Path sharing and predicate evaluation for high-performance XML filtering,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042746680&doi=10.1145%2f958942.958947&partnerID=40&md5=4e0d799eddc9f6baa5fcfa046b9a8212,"XML altering systems aim to provide fast, on-the-fly matching of XML-encoded data to large numbers of query specifications containing constraints on both structure and content. It is now well accepted that approaches using event-based parsing and Finite State Machines (FSMs) can provide the basis for highly scalable structure-oriented XML filtering systems. The XFilter system [Altinel and Franklin 2000] was the first published FSM-based XML filtering approach. XFilter used a separate FSM per path query and a novel indexing mechanism to allow all of the FSMs to be executed simultaneously during the processing of a document. Building on the insights of the XFilter work, we describe a new method, called ""YFilter"" that combines all of the path queries into a single Nondeterministic Finite Automaton (NFA). YFilter exploits commonality among queries by merging common prefixes of the query paths such that they are processed at most once. The resulting shared processing provides tremendous improvements in structure matching performance but complicates the handling of value-based predicates. In this article, we first describe the XFilter and YFilter approaches and present results of a detailed performance comparison of structure matching for these algorithms as well as a hybrid approach. The results show that the path sharing employed by YFilter can provide order-of-magnitude performance benefits. We then propose two alternative techniques for extending YFilter's shared structure matching with support for value-based predicates, and compare the performance of these two techniques. The results of this latter study demonstrate some key differences between shared XML filtering and traditional database query processing. Finally, we describe how the YFilter approach is extended to handle more complicated queries containing nested path expressions.",Content-based matching; Nested path expressions; Nondeterministic Finite Automaton; Path sharing; Predicate evaluation; Structure matching; XML filtering,Algorithms; Data reduction; Database systems; Information retrieval systems; Motion planning; Query languages; Finite state machine (FSM); Nondeterministic finite automation (NFA); Path sharing; XML filtering systems; XML
Iterative spatial join,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-19544371805&doi=10.1145%2f937598.937600&partnerID=40&md5=a5e9f925c2d270b2e974d2cb8069ac38,"The key issue in performing spatial joins is finding the pairs of intersecting rectangles. For unindexed data sets, this is usually resolved by partitioning the data and then performing a plane sweep on the individual partitions. The resulting join can be viewed as a two-step process where the partition corresponds to a hash-based join while the plane-sweep corresponds to a sort-merge join. In this article, we look at extending the idea of the sort-merge join for one-dimensional data to multiple dimensions and introduce the Iterative Spatial Join. As with the sort-merge join, the Iterative Spatial Join is best suited to cases where the data is already sorted. However, as we show in the experiments, the Iterative Spatial Join performs well when internal memory is limited, compared to the partitioning methods. This suggests that the Iterative Spatial Join would be useful for very large data sets or in situations where internal memory is a shared resource and is therefore limited, such as with todays database engines which share internal memory amongst several queries. Furthermore, the performance of the Iterative Spatial Join is predictable and has no parameters which need to be tuned, unlike other algorithms. The Iterative Spatial Join is based on a plane sweep algorithm, which requires the entire data set to fit in internal memory. When internal memory overflows, the Iterative Spatial Join simply makes additional passes on the data, thereby exhibiting only a gradual performance degradation. To demonstrate the use and efficacy of the Iterative Spatial Join, we first examine and analyze current approaches to performing spatial joins, and then give a detailed analysis of the Iterative Spatial Join as well as present the results of extensive testing of the algorithm, including a comparison with partitioning-based spatial join methods. These tests show that the Iterative Spatial Join overcomes the performance limitations of the other algorithms for data sets of all sizes as well as differing amounts of internal memory. © 2003 ACM.",External memory algorithms; Plane-sweep; Spatial databases; Spatial join,Algorithms; Iterative methods; Query languages; Resource allocation; Sorting; Storage allocation (computer); External memory algorithms; Plane-sweep; Spatial databases; Spatial join; Database systems
On the Computation of Relational View Complements,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346329829&doi=10.1145%2f777943.777946&partnerID=40&md5=f2308549558ca8c0a1f1a122bcabd49f,"Views as a means to describe parts of a given data collection play an important role in many database applications. In dynamic environments where data is updated, not only information provided by views, but also information provided by data sources yet missing from views turns out to be relevant: Previously, this missing information has been characterized in terms of view complements; recently, it has been shown that view complements can be exploited in the context of data warehouses to guarantee desirable warehouse properties such as independence and self-maintainability. As the complete source information is a trivial complement for any view, a natural interest for ""small"" or even ""minimal"" complements arises. However, the computation of minimal complements is still not very well understood. In this article, it is shown how to compute reasonably small (and in special cases even minimal) complements for a large class of relational views.",Data warehouses; Minimal complements; Relational algebra; Self-maintainability; View complements; Views,Algebra; Computer programming languages; Data acquisition; Database systems; Relational algebra; Self-maintainability; Data warehouses
Spatial Queries in Dynamic Environments,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346329832&doi=10.1145%2f777943.777944&partnerID=40&md5=c0c17ec544d8047ee34b823322a6fe4c,"Conventional spatial queries are usually meaningless in dynamic environments since their results may be invalidated as soon as the query or data objects move. In this paper we formulate two novel query types, time parameterized and continuous queries, applicable in such environments. A time-parameterized query retrieves the actual result at the time when the query is issued, the expiry time of the result given the current motion of the query and database objects, and the change that causes the expiration. A continuous query retrieves tuples of the form <result, interval>, where each result is accompanied by a future interval, during which it is valid. We study time-parameterized and continuous versions of the most common spatial queries (i.e., window queries, nearest neighbors, spatial joins), proposing efficient processing algorithms and accurate cost models.",Continuous; Database; Spatio-temporal; Time-parameterized,Algorithms; Data structures; Database systems; Electronic commerce; Mobile telecommunication systems; Dynamic environments; Spatial queries; Query languages
Discovering All Most Specific Sentences,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348220549&doi=10.1145%2f777943.777945&partnerID=40&md5=a253f39c994d7877ce4c0b0325a4861e,"Data mining can be viewed, in many instances, as the task of computing a representation of a theory of a model or a database, in particular by finding a set of maximally specific sentences satisfying some property. We prove some hardness results that rule out simple approaches to solving the problem. The a priori algorithm is an algorithm that has been successfully applied to many instances of the problem. We analyze this algorithm, and prove that is optimal when the maximally specific sentences are ""small"". We also point out its limitations. We then present a new algorithm, the Dualize and Advance algorithm, and prove worst-case complexity bounds that are favorable in the general case. Our results use the concept of hypergraph transversals. Our analysis shows that the a priori algorithm can solve the problem of enumerating the transversals of a hypergraph, improving on previously known results in a special case. On the other hand, using results for the general case of the hypergraph transversal enumeration problem, we can show that the Dualize and Advance algorithm has worst-case running time that is sub-exponential to the output size (i.e., the number of maximally specific sentences). We further show that the problem of finding maximally specific sentences is closely related to the problem of exact learning with membership queries studied in computational learning theory.",Association rules; Data mining; Learning with membership queries; Maximal frequent sets; Minimal keys,Algorithms; Computational complexity; Database systems; Problem solving; Set theory; Maximal frequent sets; Minimal keys; Data mining
Description Logics for Semantic Query Optimization in Object-Oriented Database Systems,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346329828&doi=10.1145%2f762471.762472&partnerID=40&md5=4950827df10392923b4671d1589223c3,"Semantic query optimization uses semantic knowledge (i.e., integrity constraints) to transform a query into an equivalent one that may be answered more efficiently. This article proposes a general method for semantic query optimization in the framework of Object-Oriented Database Systems. The method is effective for a large class of queries, including conjunctive recursive queries expressed with regular path expressions and is based on three ingredients. The first is a Description Logic, ODLRE, providing a type system capable of expressing: class descriptions, queries, views, in-tegrity constraint rules and inference techniques, such as incoherence detection and subsumption computation. The second is a semantic expansion function for queries, which incorporates restrictions logically implied by the query and the schema (classes + rules) in one query. The third is an optimal rewriting method of a query with respect to the schema classes that rewrites a query into an equivalent one, by determining more specialized classes to be accessed and by reducing the number of factors. We implemented the method in a tool providing an ODMG-compliant interface that allows a full interaction with OQL queries, wrapping underlying Description Logic representation and techniques to the user.",Description logics; Integrity constraints rules; Query rewriting method; Semantic expansion of a query; Semantic query optimization; Subsumption,Algorithms; Formal logic; Object oriented programming; Query languages; Recursive functions; Semantics; Integrity constraint rules; Query rewriting method; Semantic query optimization; Relational database systems
A Simple Algorithm for Finding Frequent Elements in Streams and Bags,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348252034&doi=10.1145%2f762471.762473&partnerID=40&md5=b531b0ad0d803bed7b53b9bc474910e7,"We present a simple, exact algorithm for identifying in a multiset the items with frequency more than a threshold θ. The algorithm requires two passes, linear time, and space 1/θ. The first pass is an on-line algorithm, generalizing a well-known algorithm for finding a majority element, for identifying a set of at most 1/θ items that includes, possibly among others, all items with frequency greater than θ.",Data stream; Frequent elements,Algorithms; Congestion control (communication); Data structures; Theorem proving; World Wide Web; Data streams; Frequency elements; Database systems
Adaptive Algorithms for Set Containment Joins,2003,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348220548&doi=10.1145%2f762471.762474&partnerID=40&md5=6cb9749329076888223373d825e36faa,"A set containment join is a join between set-valued attributes of two relations, whose join condition is specified using the subset (⊆) operator. Set containment joins are deployed in many database applications, even those that do not support set-valued attributes. In this article, we propose two novel partitioning algorithms, called the Adaptive Pick-and-Sweep Join (APSJ) and the Adaptive Divide-and-Conquer Join (ADCJ), which allow computing set containment joins efficiently. We show that APSJ outperforms previously suggested algorithms for many data sets, often by an order of magnitude. We present a detailed analysis of the algorithms and study their performance on real and synthetic data using an implemented testbed.",Algorithms; Experimentation; Performance,Adaptive algorithms; Functions; Personnel; Personnel rating; Data sets; Partitioning algorithms; Query languages
SilkRoute: A Framework for Publishing Relational Data in XML,2002,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-19044367677&doi=10.1145%2f582410.582413&partnerID=40&md5=0b9404c2059b7777afbb95b07f8cca4a,"XML is the ""lingua franca"" for data exchange between interenterprise applications. In this work, we describe SilkRoute, a framework for publishing relational data in XML. In SilkRoute, relational data is published in three steps: the relational tables are presented to the database administrator in a canonical XML view; the database administrator defines in the XQuery query language a public, virtual XML view over the canonical XML view; and an application formulates an XQuery query over the public view. SilkRoute composes the application query with the public-view query, translates the result into SQL, executes this on the relational engine, and assembles the resulting tuple streams into an XML document. This work makes some key contributions to XML query processing. First, it describes an algorithm that translates an XQuery expression into SQL. The translation depends on a query representation that separates the structure of the output XML document from the computation that produces the document's content. The second contribution addresses the optimization problem of how to decompose an XML view over a relational database into an optimal set of SQL queries. We define formally the optimization problem, describe the search space, and propose a greedy, cost-based optimization algorithm, which obtains its cost estimates from the relational engine. Experiments confirm that the algorithm produces queries that are nearly optimal.",Experimentation; Languages; Standardization; XML; XML storage systems; XQuery,
Distributed Query Evaluation on Semistructured Data,2002,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346387172&doi=10.1145%2f507234.507235&partnerID=40&md5=4ce01c634f2c3fa3d6961ad01acd6507,"Semistructured data is modeled as a rooted, labeled graph. The simplest kinds of queries on such data are those which traverse paths described by regular path expressions. More complex queries combine several regular path expressions, with complex data restructuring, and with subqueries. This article addresses the problem of efficient query evaluation on distributed, semistructured databases. In our setting, the nodes of the database are distributed over a fixed number of sites, and the edges are classified into local (with both ends in the same site) and cross edges (with ends in two distinct sites). Efficient evaluation in this context means that the number of communication steps is fixed (independent on the data or the query), and that the total amount of data sent depends only on the number of cross links and of the size of the query's result. We give such algorithms in three different settings. First, for the simple case of queries consisting of a single regular expression; second, for all queries in a calculus for graphs based on structural recursion which in addition to regular path expressions can perform nontrivial restructuring of the graph; and third, for a class of queries we call select-where queries that combine pattern matching and regular path expressions with data restructuring and subqueries. This article also includes a discussion on how these methods can be used to derive efficient view maintenance algorithms.",Distributed evaluation; Nested queries; Parallel complexity; Regular expressions; Semistructured data,
A Logical Foundation for Deductive Object-Oriented Databases,2002,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242387741&doi=10.1145%2f507234.507237&partnerID=40&md5=9fb7ba2a569cd8229b53c3cb84ca3e2a,"Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.",Declarative semantics; Deductive databases; Nonmonotonic multiple inheritance; Object-oriented databases; Rule-based languages,
Understanding the Global Semantics of Referential Actions using Logic Rules,2002,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-19044391395&doi=10.1145%2f582410.582411&partnerID=40&md5=94d6d6b9d9c08e85cb047897108f4fb0,"Referential actions are specialized triggers for automatically maintaining referential integrity in databases. While the local effects of referential actions can be grasped easily, it is far from obvious what the global semantics of a set of interacting referential actions should be. In particular, when using procedural execution models, ambiguities due to the execution ordering can occur. No global, declarative semantics of referential actions has yet been defined. We show that the well-known logic programming semantics provide a natural global semantics of referential actions that is based on their local characterization: To capture the global meaning of a set RA of referential actions, we first define their abstract (but non-constructive) intended semantics. Next, we formalize RA as a logic program PRA. The declarative, logic programming semantics of PRA then provide the constructive, global semantics of the referential actions. So, we do not define a semantics for referential actions, but we show that there exists a unique natural semantics if one is ready to accept (i) the intuitive local semantics of local referential actions, (ii) the formalization of those and of the local ""effect-propagating"" rules, and (iii) the well-founded or stable model semantics from logic programming as ""reasonable"" global semantics for local rules. We first focus on the subset of referential actions for deletions only. We prove the equivalence of the logic programming semantics and the abstract semantics via a game-theoretic characterization, which provides additional insight into the meaning of interacting referential actions. In this case a unique maximal admissible solution exists, computable by a PTIME algorithm. Second, we investigate the general case - including modifications. We show that in this case there can be multiple maximal admissible subsets and that all maximal admissible subsets can be characterized as 3-valued stable models of PRA. We show that for a given set of user requests, in the presence of referential actions of the form ON UPDATE CASCADE, the admissibility check and the computation of the subsequent database state, and (for non-admissible updates) the derivation of debugging hints all are in PTIME. Thus, full referential actions can be implemented efficiently.",Algorithms; Database theory; Game theory; Logic programming; Referential actions; Referential integrity; Relational databases; Theory,
Searching in Metric Spaces with User-Defined and Approximate Distances,2002,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745602932&doi=10.1145%2f582410.582412&partnerID=40&md5=43384fb5db402d1ac949797447a0523a,"Novel database applications, such as multimedia, data mining, e-commerce, and many others, make intensive use of similarity queries in order to retrieve the objects that better fit a user request. Since the effectiveness of such queries improves when the user is allowed to personalize the similarity criterion according to which database objects are evaluated and ranked, the development of access methods able to efficiently support user-defined similarity queries becomes a basic requirement. In this article we introduce the first index structure, called the QIC-M-tree, that can process userdefined queries in generic metric spaces, that is, where the only information about indexed objects is their relative distances. The QIC-M-tree is a metric access method that can deal with several distinct distances at a time: (1) a query (user-defined) distance, (2) an index distance (used to build the tree), and (3) a comparison (approximate) distance (used to quickly discard from the search uninteresting parts of the tree). We develop an analytical cost model that accurately characterizes the performance of the QIC-M-tree and validate such model through extensive experimentation on real metric data sets. In particular, our analysis is able to predict the best evaluation strategy (i.e., which distances to use) under a variety of configurations, by properly taking into account relevant factors such as the distribution of distances, the cost of computing distances, and the actual index structure. We also prove that the overall saving in CPU search costs when using an approximate distance can be estimated by using information on the data set only (thus such measure is independent of the underlying access method) and show that performance results are closely related to a novel “indexing” error measure. © 2002, ACM. All rights reserved.",Algorithms; Design; Distance metrics; Experimentation; Performance; user-defined queries,
Searching for dependencies at multiple abstraction levels,2002,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041902248&doi=10.1145%2f581751.581752&partnerID=40&md5=ba6fd29312b31f08d2adb78948028c8a,The notion of roll-up dependency (RUD) extends functional dependencies with generalization hierarchies. RUDs can be applied in OLAP and database design. The problem of discovering RUDs in large databases is at the center of this paper. An algorithm is provided that relies on a number of theoretical results. The algorithm has been implemented; results on two real-life datasets are given. The extension of functional dependency (FD) with roll-ups turns out to capture meaningful rules that are outside the scope of classical FD mining. Performance figures show that RUDs can be discovered in linear time in the number of tuples of the input dataset.,Algorithms; Experimentation; F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems; H.2.4 [Database Management]: Systems; I.2.6 [Artificial Intelligence]: Learning - Knowledge Acquisition; Management; Performance,Abstracting; Algorithms; Data mining; Decision making; Decision support systems; Hierarchical systems; Online searching; Theorem proving; Functional dependencies; Knowledge discovery; Roll-up dependency (RUD); Database systems
Fast incremental maintenance of approximate histograms,2002,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041401239&doi=10.1145%2f581751.581753&partnerID=40&md5=bd900488b26f7e5d1f3f6db8e499876c,"Many commercial database systems maintain histograms to summarize the contents of large relations and permit efficient estimation of query result sizes for use in query optimizers. Delaying the propagation of database updates to the histogram often introduces errors into the estimation. This article presents new sampling-based approaches for incremental maintenance of approximate histograms. By scheduling updates to the histogram based on the updates to the database, our techniques are the first to maintain histograms effectively up to date at all times and avoid computing overheads when unnecessary. Our techniques provide highly accurate approximate histograms belonging to the equidepth and Compressed classes. Experimental results show that our new approaches provide orders of magnitude more accurate estimation than previous approaches. An important aspect employed by these new approaches is a backing sample, an up-to-date random sample of the tuples currently in a relation. We provide efficient solutions for maintaining a uniformly random sample of a relation in the presence of updates to the relation. The backing sample techniques can be used for any other application that relies on random samples of data.",Algorithms; Approximation; Experimentation; H.2.4 [Database Management]: Systems - Query processing; Histograms; Incremental maintenance; Performance; Query optimization; Sampling,Algorithms; Data reduction; Parameter estimation; Problem solving; Query languages; Theorem proving; Histograms; Incremental maintenance; Query optimization; Relational database systems
Top-k Selection Queries over Relational Databases: Mapping Strategies and Performance Evaluation,2002,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347649247&doi=10.1145%2f568518.568519&partnerID=40&md5=41cadc58cd127a2f7849f3445367023d,"In many applications, users specify target values for certain attributes, without requiring exact matches to these values in return. Instead, the result to such queries is typically a rank of the ""top k"" tuples that best match the given attribute values. In this paper, we study the advantages and limitations of processing a top-k query by translating it into a single range query that a traditional relational database management system (RDBMS) can process efficiently. In particular, we study how to determine a range query to evaluate a top-k query by exploiting the statistics available to an RDBMS, and the impact of the quality of these statistics on the retrieval efficiency of the resulting scheme. We also report the first experimental evaluation of the mapping strategies over a real RDBMS, namely over Microsoft's SQL Server 7.0. The experiments show that our new techniques are robust and significantly more efficient than previously known strategies requiring at least one sequential scan of the data sets. Categories and Subject Descriptors: H.2.4 [Database Management]: Systems - query processing; relational databases; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - retrieval models; search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - performance evaluation.",Algorithms; Measurement; Multidimensional histograms; Performance; Top-k query processing,
Locally Adaptive Dimensionality Reduction for Indexing Large Time Series Databases,2002,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347649244&doi=10.1145%2f568518.568520&partnerID=40&md5=fe7b64fa233145ca3bed8b97c16d0c7e,"Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority. The work of E. Keogh was done while he was a Ph.D. student at the University of California at Irvine.",,
Cost models for overlapping and multiversion structures,2002,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041401233&doi=10.1145%2f581751.581754&partnerID=40&md5=1891323f2f22174b5c28bd849a8c1bf7,"Overlapping and multiversion techniques are two popular frameworks that transform an ephemeral index into a multiple logical-tree structure in order to support versioning databases. Although both frameworks have produced numerous efficient indexing methods, their performance analysis is rather limited; as a result there is no clear understanding about the behavior of the alternative structures and the choice of the best one, given the data and query characteristics. Furthermore, query optimization based on these methods is currently impossible. These are serious problems due to the incorporation of overlapping and multiversion techniques in several traditional (e.g., financial) and emerging (e.g., spatiotemporal) applications. In this article, we reduce performance analysis of overlapping and multiversion structures to that of the corresponding ephemeral structures, thus simplifying the problem significantly. This reduction leads to accurate cost models that predict the sizes of the trees, the node/page accesses, and selectivity of queries. Furthermore, the models offer significant insight into the behavior of the structures and provide guidelines about the selection of the most appropriate method in practice. Extensive experimentation proves that the proposed models yield errors below 5 and 15% for uniform and nonuniform data, respectively.",Database; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing - Indexing Methods; Index; Overlapping and multiversion structures; Spatiotemporal; Temporal; Theory,Data structures; Problem solving; Trees (mathematics); Logic-tree structures; Multiversion structures; Overlapping structures; Query languages
Atomicity and Isolation for Transactional Processes,2002,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0345903651&doi=10.1145%2f507234.507236&partnerID=40&md5=3e871719376c804de0b70589ffbccede,"Processes are increasingly being used to make complex application logic explicit. Programming using processes has significant advantages but it poses a difficult problem from the system point of view in that the interactions between processes cannot be controlled using conventional techniques. In terms of recovery, the steps of a process are different from operations within a transaction. Each one has its own termination semantics and there are dependencies among the different steps. Regarding concurrency control, the flow of control of a process is more complex than in a flat transaction. A process may, for example, partially roll back its execution or may follow one of several alternatives. In this article, we deal with the problem of atomicity and isolation in the context of processes. We propose a unified model for concurrency control and recovery for processes and show how this model can be implemented in practice, thereby providing a complete framework for developing middleware applications using processes.",Advanced transaction models; Business process management; Electronic commerce; Execution guarantees; Locking; Rocesses; Semantically rich transactions; Transactional workflows; Unified theory of concurrency control and recovery,
SchemaSQL - An Extension to SQL for Multidatabase Interoperability,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347664868&doi=10.1145%2f503099.503102&partnerID=40&md5=e348a43e4fa0132e4362803ba3f1d4bb,"We provide a principled extension of SQL, called SchemaSQL, that offers the capability of uniform manipulation of data and schema in relational multidatabase systems. We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavor of SQL while supporting querying of both data and schema. (2) It can be used to transform data in a database in a structure substantially different from original database, in which data and schema may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits ""horizontal"" aggregation and even aggregation over more general ""blocks"" of information. (5) SchemaSQL provides a useful facility for interoperability and data/schema manipulation in relational multidatabase systems. We provide many examples to illustrate our claims. We clearly spell out the formal semantics of SchemaSQL that accounts for all these features. We describe an architecture for the implementation of SchemaSQL and develop implementation algorithms based on available database technology that allows for powerful integration of SQL based relational DBMS. We also discuss the applicability of SchemaSQL for handling semantic heterogeneity arising in a multidatabase system.",H.2.3 [Database Management]: Languages - query languages; H.2.4 [Database Management]: Systems - relational databases; H.2.5 [Database Management]: Heterogeneous Databases; Query processing,
Editorial,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025413943&doi=10.1145%2f503099.505055&partnerID=40&md5=720fe1e7c993a72fd79b597151f211bd,[No abstract available],,
A Case for Dynamic View Management,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347034065&doi=10.1145%2f503099.503100&partnerID=40&md5=48a5fe930061e0f5a72d3cc05c32b238,"Materialized aggregate views represent a set of redundant entities in a data warehouse that are frequently used to accelerate On-Line Analytical Processing (OLAP). Due to the complex structure of the data warehouse and the different profiles of the users who submit queries, there is need for tools that will automate and ease the view selection and management processes. In this article we present DynaMat, a system that manages dynamic collections of materialized aggregate views in a data warehouse. At query time, DynaMat utilizes a dedicated disk space for storing computed aggregates that are further engaged for answering new queries. Queries are executed independently or can be bundled within a multiquery expression. In the latter case, we present an execution mechanism that exploits dependencies among the queries and the materialized set to further optimize their execution. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window. We show how to derive an efficient update plan with respect to the available maintenance window, the different update policies for the views and the dependencies that exist among them.",Data cube; Data warehousing; H.2.7 [Database Management]: Database administration -data warehouse and repository; H.4.m [Information Systems Applications]: Miscellaneous; Management; Materialized views; OLAP,
Multiway Spatial Joins,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347236452&doi=10.1145%2f503099.503101&partnerID=40&md5=d386e0e407750d255ae147157cdff345,"Due to the evolution of Geographical Information Systems, large collections of spatial data having various thematic contents are currently available. As a result, the interest of users is not limited to simple spatial selections and joins, but complex query types that implicate numerous spatial inputs become more common. Although several algorithms have been proposed for computing the result of pairwise spatial joins, limited work exists on processing and optimization of multiway spatial joins. In this article, we review pairwise spatial join algorithms and show how they can be combined for multiple inputs. In addition, we explore the application of synchronous traversal (ST), a methodology that processes synchronously all inputs without producing intermediate results. Then, we integrate the two approaches in an engine that includes ST and pairwise algorithms, using dynamic programming to determine the optimal execution plan. The results show that, in most cases, multiway spatial joins are best processed by combining ST with pairwise methods. Finally, we study the optimization of very large queries by employing randomized search algorithms.",Algorithms; H.2.8 [Database Management]: Database Application - spatial databases and GIS; Multiway joins; Query processing; Spatial joins,
Data Mining with Optimized Two-Dimensional Association Rules,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346606741&doi=10.1145%2f383891.383893&partnerID=40&md5=0a73c83234bb67632076bfc285beaadd,"We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, ""Age"" and ""Balance"" are two numeric attributes, and ""CardLoan"" is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form ((Age, Balance) ∈ P) ⇒ (CardLoan = Yes), which implies that bank customers whose ages and balances fall within a planar region P tend to take out credit card loans with a high probability. We consider two classes of regions, rectangles and admissible (i.e., connected and x-monotone) regions. For each class, we propose efficient algorithms for computing the regions that give optimal association rules for gain, support, and confidence, respectively. We have implemented the algorithms for admissible regions as well as several advanced functions based on them in our data mining system named SONAR (System for Optimized Numeric Association Rules), where the rules are visualized by using a graphic user interface to make it easy for users to gain an intuitive understanding of rules.",F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems-geometrical problems and computations; G.2.1 [Discrete Mathematics]: Combinatorics-combinatorial algorithms,
ACM Transactions on Database Systems: Editorial,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346963842&partnerID=40&md5=fd2e0f159ede340f322ddd17af71321d,[No abstract available],,
Answering Queries with Useful Bindings,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142228522&doi=10.1145%2f502030.502032&partnerID=40&md5=47e15c3c6af30fbfe5bb49b34e659813,"In information-integration systems, sources may have diverse and limited query capabilities. To obtain maximum information from these restrictive sources to answer a query, one can access sources that are not specified in the query (i.e., off-query sources). In this article, we propose a query-planning framework to answer queries in the presence of limited access patterns. In the framework, a query and source descriptions are translated to a recursive datalog program. We then solve optimization problems in this framework, including how to decide whether accessing off-query sources is necessary, how to choose useful sources for a query, and how to test query containment. We develop algorithms to solve these problems, and thus construct an efficient program to answer a query.",Algorithms; Datalog programs; H.2.4 [Information Systems]: Systems - query processing; H.2.5 [Information Systems]: Heterogeneous Databases; Information-integration systems; Limited source capabilities; Performance; Query containment,Algorithms; Computer program listings; Information management; Information retrieval; Internet; Datalog programs; Information integration systems; Query containment; Database systems
Querying ATSQL Databases with Temporal Logic,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346977642&doi=10.1145%2f383891.383892&partnerID=40&md5=8070aafaf10fb6393dd32b7a64cabc2e,"We establish a correspondence between temporal logic and a subset of ATSQL, a temporal extension of SQL-92. In addition, we provide an effective translation from temporal logic to ATSQL that enables a user to write high-level queries which are then evaluated against a space-efficient representation of the database. A reverse translation, also provided in this paper, characterizes the expressive power of a syntactically defined subset of ATSQL queries.",ATSQL; F.4.1 [Mathematical Logic and Formal Languages]: Mathematical Logic-temporal logic; First-order temporal logic; H.2.3 [Database Management]: Languages-query languages; H.2.4 [Database Management]: Systems-query processing; Languages; Theory,
Probabilistic Object Bases,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003742607&doi=10.1145%2f502030.502031&partnerID=40&md5=192208b902ceda17dc944f3ef926fad4,"Although there are many applications where an object-oriented data model is a good way of representing and querying data, current object database systems are unable to handle objects whose attributes are uncertain. In this article, we extend previous work by Kornatzky and Shimony to develop an algebra to handle object bases with uncertainty. We propose concepts of consistency for such object bases, together with an NP-completeness result, and classes of probabilistic object bases for which consistency is polynomially checkable. In addition, as certain operations involve conjunctions and disjunctions of events, and as the probability of conjunctive and disjunctive events depends both on the probabilities of the primitive events involved as well as on what is known (if anything) about the relationship between the events, we show how all our algebraic operations may be performed under arbitrary probabilistic conjunction and disjunction strategies. We also develop a host of equivalence results in our algebra, which may be used as rewrite rules for query optimization. Last but not least, we have developed a prototype probabilistic object base server on top of ObjectStore. We describe experiments to assess the efficiency of different possible rewrite rules.",H.2.1 [Database Management]: Logical Design - data models; H.2.3 [Database Management]: Languages - query languages; H.2.4 [Database Management]: Systems - object-oriented databases,Algorithms; Object oriented programming; Polynomials; Probability; Query languages; Servers; Logical design; Object oriented databases; Query optimization; Database systems
An Extension of the Relational Data Model to Incorporate Ordered Domains,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347710442&doi=10.1145%2f502030.502033&partnerID=40&md5=414635f745f8aba69da03af7c4931d81,"We extend the relational data model to incorporate partial orderings into data domains, which we call the ordered relational model. Within the extended model, we define the partially ordered relational algebra (the PORA) by allowing the ordering predicate (square image of or equal to) to be used in formulae of the selection operator (σ). The PORA expresses exactly the set of all possible relations that are invariant under order-preserving automorphism of databases. This result characterizes the expressiveness of the PORA and justifies the development of Ordered SQL (OSQL) as a query language for ordered databases. OSQL provides users with the capability of capturing the semantics of ordered data in many advanced applications, such as those having temporal or incomplete information. Ordered functional dependencies (OFDs) on ordered databases are studied, based on two possible extensions of domain orderings: pointwise ordering and lexicographical ordering. We present a sound and complete axiom system for OFDs in the first case and establish a set of sound and complete chase rules for OFDs in the second. Our results suggest that the implication problems for both cases of OFDs are decidable and that the enforcement of OFDs in ordered relations are practically feasible. In a wider perspective, the proposed model explores an important area of object-relational databases, since ordered domains can be viewed as a general kind of data type.",Axiom system; Design; H.2.1 [Database Management]: Logical design - data models; H.2.3 [Database Management]: Languages - query languages; H.2.4 [Database Management]: Systems - relational databases; Languages; Theory,Algebra; Data structures; Object oriented programming; Query languages; Semantics; Lexicographical ordering; Partially ordered relational algebra; Valuation mapping; Relational database systems
Flexible Support for Multiple Access Control Policies,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001619596&doi=10.1145%2f383891.383894&partnerID=40&md5=55a71aae55667986fd915268cad9994b,"Although several access control policies can be devised for controlling access to information, all existing authorization models, and the corresponding enforcement mechanisms, are based on a specific policy (usually the closed policy). As a consequence, although different policy choices are possible in theory, in practice only a specific policy can actually be applied within a given system. In this paper, we present a unified framework that can enforce multiple access control policies within a single system. The framework is based on a language through which users can specify security policies to be enforced on specific accesses. The language allows the specification of both positive and negative authorizations and incorporates notions of authorization derivation, conflict resolution, and decision strategies. Different strategies may be applied to different users, groups, objects, or roles, based on the needs of the security policy. The overall result is a flexible and powerful, yet simple, framework that can easily capture many of the traditional access control policies as well as protection requirements that exist in real-world applications, but are seldom supported by existing systems. The major advantage of our approach is that it can be used to specify different access control policies that can all coexist in the same system and be enforced by the same security server.","Access control policy; Authorization; H.2.7 [Database Management]: Database Administration-security, integrity, and protection; K.6.5 [Management of Computing and Information Systems]: Security and Protection; Logic programming; Management; Security",
I/O reference behavior of production database workloads and the TPC benchmarks - An analysis at the logical level,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037548421&doi=10.1145%2f383734.383737&partnerID=40&md5=8991d6371e9f6a5053ed14adb5197dd3,"As improvements in processor performance continue to far outpace improvements in storage performance, I/O is increasingly the bottleneck in computer systems, especially in large database systems that manage huge amounts of data. The key to achieving good I/O performance is to thoroughly understand its characteristics. In this article we present a comprehensive analysis of the logical I/O reference behavior of the peak production database workloads from ten of the world's largest corporations. In particular, we focus on how these workloads respond to different techniques for caching, prefetching, and write buffering. Our findings include several broadly applicable rules of thumb that describe how effective the various I/O optimization techniques are for the production workloads. For instance, our results indicate that the buffer pool miss ratio tends to be related to the ratio of buffer pool size to data size by an inverse square root rule. A similar fourth root rule relates the write miss ratio and the ratio of buffer pool size to data size. In addition, we characterize the reference characteristics of workloads similar to the Transaction Processing Performance Council (TPC) benchmarks C (TPC-C) and D (TPC-D), which are de facto standard performance measures for online transaction processing (OLTP) systems and decision support systems (DSS), respectively. Since benchmarks such as TPC-C and TPC-D can only be used effectively if their strengths and limitations are understood, a major focus of our analysis is identifying aspects of the benchmarks that stress the system differently than the production workloads. We discover that for the most part, the reference behavior of TPC-C and TPC-D fall within the range of behavior exhibited by the production workloads. However, there are some noteworthy exceptions that affect well-known I/O optimization techniques such as caching (LRU is further from the optimal for TPC-C, while there is little sharing of pages between transactions for TPC-D), prefetching (TPC-C exhibits no significant sequentially), and write buffering (write buffering is less effective for the TPC benchmarks). While the two TPC benchmarks generally complement one another in reflecting the characteristics of the production workloads, there remain aspects of the real workloads that are not represented by either of the benchmarks.",Algorithms; Caching; Design; I/O; Installation Management - Benchmarks; Locality; Performance; Performance of Systems; Prefetchi; Storage Management - Storage hierarchies; Systems and Software - Performance evaluation (efficiency and effectiveness),
Applying an update method to a set of receivers,2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040152048&doi=10.1145%2f383734.383735&partnerID=40&md5=f479e0bfd4fba6d10990229b0a25926e,"In the context of object databases, we study the application of an update method to a collection of receivers rather than to a single one. The obvious strategy of applying the update to the receivers one after the other, in some arbitrary order, brings up the problem of order independence. On a very general level, we investigate how update behavior can be analyzed in terms of certain schema annotations, called colorings. We are able to characterize those colorings that always describe order-independent updates. We also consider a more specific model of update methods implemented in the relational algebra. Order-independence of such algebraic methods is undecidable in general, but decidable if the expressions used are positive. Finally, we consider an alternative parallel strategy for set-oriented application of algebraic update methods and compare and relate it to the sequential strategy.",Algorithms; Database Management; Database update; Languages; Order independence; Parallel update; Relational algebra; Schema coloring; Theory; Verification,
"Probabilistic temporal databases, I: Algebra",2001,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038967918&doi=10.1145%2f383734.383736&partnerID=40&md5=e4f08bd3c5f9760b9f4dff707c22a4b4,"Dyreson and Snodgrass have drawn attention to the fact that, in many temporal database applications, there is often uncertainty about the start time of events, the end time of events, and the duration of events. When the granularity of time is small (e.g., milliseconds), a statement such as ""Packet p was shipped sometime during the first 5 days of January, 1998"" leads to a massive amount of uncertainty (5 × 24 × 60 × 60 × 1000) possibilities. As noted in Zaniolo et al. [1997], past attempts to deal with uncertainty in databases have been restricted to relatively small amounts of uncertainty in attributes. Dyreson and Snodgrass have taken an important first step towards solving this problem. In this article, we first introduce the syntax of Temporal-Probabilistic (TP) relations and then show how they can be converted to an explicit, significantly more space-consuming form, called Annotated Relations. We then present a theoretical annotated temporal algebra (TATA). Being explicit, TATA is convenient for specifying how the algebraic operations should behave, but is impractical to use because annotated relations are overwhelmingly large. Next, we present a temporal probabilistic algebra (TPA). We show that our definition of the TP-algebra provides a correct implementation of TATA despite the fact that it operates on implicit, succinct TP-relations instead of the overwhelmingly large annotated relations. Finally, we report on timings for an implementation of the TP-Algebra built on top of ODBC.",Knowledge Representation Formalisms and Methods; Theory,
Temporal Statement Modifiers,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042260804&doi=10.1145%2f377674.377665&partnerID=40&md5=ebf29a700ae8229653eef9a633fbd9bf,"A wide range of database applications manage time-varying data. Many temporal query languages have been proposed, each one the result of many carefully made yet subtly interacting design decisions. In this article we advocate a different approach to articulating a set of requirements, or desiderata, that directly imply the syntactic structure and core semantics of a temporal extension of an (arbitrary) nontemporal query language. These desiderata facilitate transitioning applications from a nontemporal query language and data model, which has received only scant attention thus far. The paper then introduces the notion of statement modifiers that provide a means of systematically adding temporal support to an existing query language. Statement modifiers apply to all query language statements, for example, queries, cursor definitions, integrity constraints assertions, views, and data manipulation statements. We also provide a way to systematically add temporal support to an existing implementation. The result is a temporal query language syntax, semantics, and implementation that derives from first principles. We exemplify this approach by extending SQL-92 with statement modifiers. This extended language, termed ATSQL, is formally defined via a denotational-semantics-style mapping of temporal statements to expressions using a combination of temporal and conventional relational algebraic operators.",ATSQL; Data description languages (DDL); Data manipulation languages (DML); H.2.3 [Database management]: Languages - Query languages; H.2.4 [Database management]: Systems - Relational databases; Languages; Query processing; Statement modifiers; Theory,
A Foundation for Representing and Querying Moving Objects,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037506734&doi=10.1145%2f352958.352963&partnerID=40&md5=8572fbb6115f40858a7b81352a97682a,"Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to represent such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a ""moving real"" to represent the time-dependent distance of two moving points. It then becomes crucial to achieve (i) orthogonality in the design of the type system, i.e., type constructors can be applied uniformly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goals leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension. Categories and Subject Descriptors: H.2.3 [Database Management]: Languages - Query languages; H.2.8 [Database Management]: Database applications - Spatial databases and GIS General Terms: Languages, Theory.",Abstract data types; Algebra; Moving objects; Moving point; Moving region; Spatio-temporal data types; Spatio-temporal databases,
Iterative Dynamic Programming: A New Class of Query Optimization Algorithms,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002103528&doi=10.1145%2f352958.352982&partnerID=40&md5=9fa61bc08d3610e1f01c770ce61e93ef,"The query optimizer is one of the most important components of a database system. Most commercial query optimizers today are based on a dynamic-programming algorithm, as proposed in Selinger et al. [1979]. While this algorithm produces good optimization results (i.e., good plans), its high complexity can be prohibitive if complex queries need to be processed, new query execution techniques need to be integrated, or in certain programming environments (e.g., distributed database systems). In this paper, we present and thoroughly evaluate a new class of query optimization algorithms that are based on a principle that we call iterative dynamic programming, or IDP for short. IDP has several important advantages: First, IDP-algorithms produce the best plans of all known algorithms in situations in which dynamic programming is not viable because of its high complexity. Second, some IDP variants are adaptive and produce as good plans as dynamic programming if dynamic programming is viable and as-good-as possible plans if dynamic programming turns out to be not viable. Three, all IDP-algorithms can very easily be integrated into an existing optimizer which is based on dynamic programming. Categories and Subject Descriptors: F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems; H.2.4 [Database Management]: Systems -Distributed databases; Relational databases General Terms: Algorithms, Performance.",Dynamic programming; Greedy algorithm; Iterative dynamic programming; Plan evaluation function; Query optimization; Randomized optimization,
A new approach to developing and implementing eager database replication protocols,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001745084&doi=10.1145%2f363951.363955&partnerID=40&md5=4fc2a2fffd57062be1f7e523d1700fbb,"Database replication is traditionally seen as a way to increase the availability and performance of distributed databases. Although a large number of protocols providing data consistency and fault-tolerance have been proposed, few of these ideas have ever been used in commercial products due to their complexity and performance implications. Instead, current products allow inconsistencies and often resort to centralized approaches which eliminates some of the advantages of replication. As an alternative, we propose a suite of replication protocols that addresses the main problems related to database replication. On the one hand, our protocols maintain data consistency and the same transactional semantics found in centralized systems. On the other hand, they provide flexibility and reasonable performance. To do so, our protocols take advantage of the rich semantics of group communication primitives and the relaxed isolation guarantees provided by most databases. This allows us to eliminate the possibility of deadlocks, reduce the message overhead and increase performance. A detailed simulation study shows the feasibility of the approach and the flexibility with which different types of bottlenecks can be circumvented.",Algorithms; C.2.4 [Computer-Communication Networks]: Distributed systems; C.4 [Computer Systems Organization]: Performance of systems; Distributed databases; H.2.4 [Database Management]: Systems - Concurrency; Management; Performance; Transaction processing,
A Model for Compound Type Changes Encountered in Schema Evolution,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002295999&doi=10.1145%2f352958.352983&partnerID=40&md5=4123f1da111c5bc5eb8461526e052306,"Schema evolution is a problem that is faced by long-lived data. When a schema changes, existing persistent data can become inaccessible unless the database system provides mechanisms to access data created with previous versions of the schema. Most existing systems that support schema evolution focus on changes local to individual types within the schema, thereby limiting the changes that the database maintainer can perform. We have developed a model of type changes incorporating changes local to individual types as well as compound changes involving multiple types. The model describes both type changes and their impact on data by defining derivation rules to initialize new data based on the existing data. The derivation rules can describe local and nonlocal changes to types to capture the intent of a large class of type change operations. We have built a system called Tess (Type Evolution Software System) that uses this model to recognize type changes by comparing schemas and then produces a transformer that can update data in a database to correspond to a newer version of the schema. Categories and Subject Descriptors: H.2.m [Database Management]: Miscellaneous; H.2.3 [Database Management]: Languages - Database (persistent) programming languages; D.2.7 [Software Engineering]: Distribution, Maintenance, and Enhancement-Restructuring, reverse engineering, and reengineering General Terms: Algorithms, Languages.",Persistent programming languages; Schema evolution,
Cache Investment: Integrating Query Optimization and Distributed Data Placement,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008486362&doi=10.1145%2f377674.377677&partnerID=40&md5=f4ad8e25444feb9cadad3b535965e85e,"Emerging distributed query-processing systems support flexible execution strategies in which each query can be run using a combination of data shipping and query shipping. As in any distributed environment, these systems can obtain tremendous performance and availability benefits by employing dynamic data caching. When flexible execution and dynamic caching are combined, however, a circular dependency arises: Caching occurs as a by-product of query operator placement, but query operator placement decisions are based on (cached) data location. The practical impact of this dependency is that query optimization decisions that appear valid on a per-query basis can actually cause suboptimal performance for all queries in the long run. To address this problem, we developed Cache Investment - a novel approach for integrating query optimization and data placement that looks beyond the performance of a single query. Cache Investment sometimes intentionally generates a ""suboptimal"" plan for a particular query in the interest of effecting a better data placement for subsequent queries. Cache Investment can be integrated into a distributed database system without changing the internals of the query optimizer. In this paper, we propose Cache Investment mechanisms and policies and analyze their performance. The analysis uses results from both an implementation on the SHORE storage manager and a detailed simulation model. Our results show that Cache Investment can significantly improve the overall performance of a system and demonstrate the trade-offs among various alternative policies.",,
A cost model for query processing in high dimensional data spaces,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039301636&doi=10.1145%2f357775.357776&partnerID=40&md5=352f2420c31a21a0c9788b98cf9b8b04,"During the last decade, multimedia databases have become increasingly important in many application areas such as medicine, CAD, geography, and molecular biology. An important research topic in multimedia databases is similarity search in large data sets. Most current approaches that address similarity search use the feature approach, which transforms important properties of the stored objects into points of a high-dimensional space (feature vectors). Thus, similarity search is transformed into a neighborhood search in feature space. Multidimensional index structures are usually applied when managing feature vectors. Query processing can be improved substantially with optimization techniques such as blocksize optimization, data space quantization, and dimension reduction. To determine optimal parameters, an accurate estimate of index-based query processing performance is crucial. In this paper we develop a cost model for index structures for point databases such as the R *-tree and the X-tree. It provides accurate estimates of the number of data page accesses for range queries and nearest-neighbor queries under a Euclidean metric and a maximum metric. The problems specific to high-dimensional data spaces, called boundary effects, are considered. The concept of the fractal dimension is used to take the effects of correlated data into account.",Cost model; H.2.8 [Database Management]: Database applications; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing; Multidimensional index; Performance; Theory,Algorithms; Digital storage; Fractals; Heuristic methods; Indexing (of information); Information retrieval; Multimedia systems; Vectors; Content analysis; Cost model; Data sets; Multidimensional index; Query languages
Optimizing Object Queries Using an Effective Calculus,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010385667&doi=10.1145%2f377674.377676&partnerID=40&md5=b990d9642e7d7462a0ad57b6782649f3,"Object-oriented databases (OODBs) provide powerful data abstractions and modeling facilities, but they generally lack a suitable framework for query processing and optimization. The development of an effective query optimizer is one of the key factors for OODB systems to successfully compete with relational systems, as well as to meet the performance requirements of many nontraditional applications. We propose an effective framework with a solid theoretical basis for optimizing OODB query languages. Our calculus, called the monoid comprehension calculus, captures most features of ODMG OQL, and is a good basis for expressing various optimization algorithms concisely. This article concentrates on query unnesting (also known as query decorrelation), an optimization that, even though it improves performance considerably, is not treated properly (if at all) by most OODB systems. Our framework generalizes many unnesting techniques proposed recently in the literature, and is capable of removing any form of query nesting using a very simple and efficient algorithm. The simplicity of our method is due to the use of the monoid comprehension calculus as an intermediate form for OODB queries. The monoid comprehension calculus treats operations over multiple collection types, aggregates, and quantifiers in a similar way, resulting in a uniform method of unnesting queries, regardless of their type of nesting.",Design; Experimentation; H.2.1 [Database management]: Logical design; Nested relations; Object-oriented databases; Performance; Query decorrelation; Query optimization,
Theory of dependence values,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001754925&doi=10.1145%2f363951.363956&partnerID=40&md5=61ac5983e28cf7c140e8596aba5c23af,"A new model to evaluate dependencies in data mining problems is presented and discussed. The well-known concept of the association rule is replaced by the new definition of dependence value, which is a single real number uniquely associated with a given itemset. Knowledge of dependence values is sufficient to describe all the dependencies characterizing a given data mining problem. The dependence value of an itemset is the difference between the occurrence probability of the itemset and a corresponding ""maximum independence estimate."" This can be determined as a function of joint probabilities of the subsets of the itemset being considered by maximizing a suitable entropy function. So it is possible to separate in an itemset of cardinality k the dependence inherited from its subsets of cardinality (k - 1) and the specific inherent dependence of that itemset. The absolute value of the difference between the probability P (i) of the event i that indicates the presence of the itemset {a, b, . . . }and its maximum independence estimate is constant for any combination of values of (a, b, . . . ). In addition, the Boolean function specifying the combinations of values for which the dependence is positive is a parity function. So the determination of such combinations is immediate. The model appears to be simple and powerful.",1.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods; H.1.1 [Models and Principles]: Systems and Information Theory - Information theory; H.2.8 [Database Management]: Database applications - Data mining; Statistical databases,
Emancipating instances from the tyranny of classes in information modeling,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039893758&doi=10.1145%2f357775.357778&partnerID=40&md5=6b858a45e64d1f5f8ccafec578801cb4,"Database design commonly assumes, explicitly or implicitly, that instances must belong to classes. This can be termed the assumption of inherent classification We argue that the extent and complexity of problems in schema integration, schema evolution, and interoperability are, to a large degree, consequences of inherent classification. Furthermore, we make the case that the assumption of inherent classification violates philosophical and cognitive guidelines on classification and is, therefore, inappropriate in view of the role of data modeling in representing knowledge about application domains. As an alternative, we propose a layered approach to modeling in which information about instances is separated from any particular classification. Two data modeling layers are proposed: (1) an instance model consisting of an instance base (i.e., information about instances and properties) and operations to populate, use, and maintain it; and (2) a class model consisting of a class base (i.e., information about classes defined in terms of properties) and operations to populate, use, and maintain it. The two-layered model provides class independence. This is analogous to the arguments of data independence offered by the relational model in comparison to hierarchical and network models. We show that a two-layered approach yields several advantages. In particular, schema integration is shown to be partially an artifact of inherent classification that can be greatly simplified in designing a database based on a layered model; schema evolution is supported without the complexity of operations currently required by class-based models; and the difficulties associated with interoperability among heterogeneous databases are reduced because there is no need to agree on the semantics of classes among independent databases. We conclude by considering the adequacy of a two-layered approach, outlining possible implementation strategies, and drawing attention to some practical considerations.",H.2.1 [Database Management]: Logical Design - Data,Cognitive systems; Computer simulation; Data structures; Interoperability; Logic design; Research; Semantics; Conceptual modeling; Data translation; Database design; Ontology; Schema evolution; Schema integration; Database systems
Tracing the lineage of view data in a warehousing environment,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000278897&doi=10.1145%2f357775.357777&partnerID=40&md5=1feb096b185710243a666533fe4daea1,"We consider the view data lineage problem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formally define the lineage problem, develop lineage tracing algorithms for relational views with aggregation, and propose mechanisms for performing consistent lineage tracing in a multisource data warehousing environment. Our results can form the basis of a tool that allows analysts to browse warehouse data, select view tuples of interest, and then ""drill-through"" to examine the exact source tuples that produced the view tuples of interest.",Algorithms; Data warehouse; Derivation; Design; H.2.4 [Database Management]: Systems - Database Management; Lineage; Materialized views,Algorithms; Data mining; Data reduction; Legacy systems; Mathematical operators; Online systems; Theorem proving; Data lineage; Data warehouse; Derivation; Online analytical processing (OLAP); Database systems
Erratum: Distance Browsing in Spatial Databases (ACM Transactions on Database Systems (June 1999) 24:2 (265-318),1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346963772&partnerID=40&md5=2e3e7e9b9d48b92603f04cef87a3f4d0,[No abstract available],,
An algebraic approach to static analysis of active database rules,2000,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000096346&doi=10.1145%2f363951.363954&partnerID=40&md5=c5898ae9e957a410937a7bd06ccb92f1,"Rules in active database systems can be very difficult to program due to the unstructured and unpredictable nature of rule processing. We provide static analysis techniques for predicting whether a given rule set is guaranteed to terminate and whether rule execution is confluent (guaranteed to have a unique final state). Our methods are based on previous techniques for analyzing rules in active database systems. We improve considerably on the previous techniques by providing analysis criteria that are much less conservative: our methods often determine that a rule set will terminate or is confluent when previous methods could not make this determination. Our improved analysis is based on a ""propagation"" algorithm, which uses an extended relational algebra to accurately determine when the action of one rule can affect the condition of another, and determine when rule actions commute. We consider both condition-action rules and event-condition-action rules, making our approach widely applicable to relational active database rule languages and to the trigger language in the SQL:1999 standard.",Active database systems; Confluence; Database rule processing; Database trigger processing; Design; H.2.3 [Database Management]: Languages - SQL; H.2.4 [Database Management]: Systems - Rule-based databases; Termination; Verification,
Optimization of queries with user-defined predicates,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040364310&doi=10.1145%2f320248.320249&partnerID=40&md5=e873f1c7a61b4e6e84c1bbf6522b4845,"Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The naive optimization algorithm is very general, and therefore is most widely applicable. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of user-defined predicates (for a given number of relations). We also propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. We discuss how, depending on application requirements, to determine the algorithm of choice. It should be emphasized that our optimization algorithms handle user-defined selections as well as user-defined join predicates uniformly. We present complexity analysis and experimental comparison of the algorithms.",Algorithms; Database Management; Dynamic programming; H.2 [Information Systems]; Management; Performance; Query optimization; User-defined predicates,Algorithms; Computational complexity; Dynamic programming; Heuristic methods; Optimization; Query languages; User interfaces; Query optimization; User defined predicates; Relational database systems
Security of Random Data Perturbation Methods,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346396080&doi=10.1145%2f331983.331986&partnerID=40&md5=778fde228518cd09e870d9c3295a7751,"Statistical databases often use random data perturbation (RDP) methods to protect against disclosure of confidential numerical attributes. One of the key requirements of RDP methods is that they provide the appropriate level of security against snoopers who attempt to obtain information on confidential attributes through statistical inference. In this study, we evaluate the security provided by three methods of perturbation. The results of this study allow the database administrator to select the most effective RDP method that assures adequate protection against disclosure of confidential information.",Bias; Covariance; Noise addition; Random data perturbation,Data privacy; Mathematical transformations; Perturbation techniques; Security of data; Statistical methods; Database administration; Random data perturbation; Database systems
Broadcast Protocols to Support Efficient Retrieval from Databases by Mobile Users,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346207638&doi=10.1145%2f310701.310710&partnerID=40&md5=a17af7c8c46b6436a5016be84db0565d,"Mobile computing has the potential for managing information globally. Data management issues in mobile computing have received some attention in recent times, and the design of adaptive broadcast protocols has been posed as an important problem. Such protocols are employed by database servers to decide on the content of broadcasts dynamically, in response to client mobility and demand patterns. In this paper we design such protocols and also propose efficient retrieval strategies that may be employed by clients to download information from broadcasts. The goal is to design cooperative strategies between server and client to provide access to information in such a way as to minimize energy expenditure by clients. We evaluate the performance of our protocols both analytically and through simulation. General Terms: Algorithms, Performance.",Adaptive broadcast protocols; Client-server computing; Energy conservation; Mobile databases,
Inverted files versus signature files for text indexing,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032268976&doi=10.1145%2f296854.277632&partnerID=40&md5=e0f05fcf58ea7d8b80c0323ebd9b604c,"Two well-known indexing methods are inverted files and signature files. We have undertaken a detailed comparison of these two approaches in the context of text indexing, paying particular attention to query evaluation speed and space requirements. We have examined their relative performance using both experimentation and a refined approach to modeling of signature files, and demonstrate that inverted files are distinctly superior to signature files. Not only can inverted files be used to evaluate typical queries in less time than can signature files, but inverted files require less space and provide greater functionality. Our results also show that a synthetic text database can provide a realistic indication of the behavior of an actual text database. The tools used to generate the synthetic database have been made publicly available.",,Computer systems programming; Data reduction; Data structures; Distributed database systems; Indexing (of information); Query languages; Response time (computer systems); Inverted files; Signature files; Text databases; Text indexing; Text processing
An Ontological Analysis of the Relationship Construct in Conceptual Modeling,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001657012&doi=10.1145%2f331983.331989&partnerID=40&md5=96cb0e97526b59306439a9313b8b683a,"Conceptual models or semantic data models were developed to capture the meaning of an application domain as perceived by someone. Moreover, concepts employed in semantic data models have recently been adopted in object-oriented approaches to systems analysis and design. To employ conceptual modeling constructs effectively, their meanings have to be defined rigorously. Often, however, rigorous definitions of these constructs are missing. This situation occurs especially in the case of the relationship construct. Empirical evidence shows that use of relationships is often problematical as a way of communicating the meaning of an application domain. For example, users of conceptual modeling methodologies are frequently confused about whether to show an association between things via a relationship, an entity, or an attribute. Because conceptual models are intended to capture knowledge about a real-world domain, we take the view that the meaning of modeling constructs should be sought in models of reality. Accordingly, we use ontology, which is the branch of philosophy dealing with models of reality, to analyze the meaning of common conceptual modeling constructs. Our analysis provides a precise definition of several conceptual modeling constructs. Based on our analysis, we derive rules for the use of relationships in entity-relationship conceptual modeling. Moreover, we show how the rules resolve ambiguities that exist in current practice and how they can enrich the capacity of an entity-relationship conceptual model to capture knowledge about an application domain.",Conceptual modeling; Database design; Entity-relationship model; Object-oriented modeling; Ontology; Semantic data modeling,Computer simulation; Data handling; Object oriented programming; Semantics; Information systems; Ontology; Database systems
GIOSS: Text-source discovery over the Internet,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001511080&doi=10.1145%2f320248.320252&partnerID=40&md5=b7d97a29eed009229e70adcc4e6adad7,"The dramatic growth of the Internet has created a new problem for users: location of the relevant sources of documents. This article presents a framework for (and experimentally analyzes a solution to) this problem, which we call the text-source discovery problem. Our approach consists of two phases. First, each text source exports its contents to a centralized service. Second, users present queries to the service, which returns an ordered list of promising text sources. This article describes GlOSS, Glossary of Servers Server, with two versions: bGlOSS, which provides a Boolean query retrieval model, and vGlOSS, which provides a vector-space retrieval model. We also present hGlOSS, which provides a decentralized version of the system. We extensively describe the methodology for measuring the retrieval effectiveness of these systems and provide experimental evidence, based on actual data, that all three systems are highly effective in determining promising text sources for a given query.",Digital libraries; Distributed information retrieval; H.3 [Information Systems]; Information Storage and Retrieval; Internet search and retrieval; Measurement; Performance; Text databases,Boolean algebra; Glossaries; Information retrieval systems; Mathematical models; Problem solving; Query languages; Servers; Statistical methods; Text processing; User interfaces; Vectors; Distributed information retrieval; Text databases; Internet
Database Design for Incomplete Relations,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003369368&doi=10.1145%2f310701.310712&partnerID=40&md5=54de3f54077e59a34d6e12fdff180376,"Although there has been a vast amount of research in the area of relational database design, to our knowledge, there has been very little work that considers whether this theory is still valid when relations in the database may be incomplete. When relations are incomplete and thus contain null values the problem of whether satisfaction is additive arises. Additivity is the property of the equivalence of the satisfaction of a set of functional dependencies (FDs) F with the individual satisfaction of each member of F in an incomplete relation. It is well known that, in general, satisfaction of FDs is not additive. Previously we have shown that satisfaction is additive if and only if the set of FDs is monodependent. We conclude that monodependence is a fundamental desirable property of a set of FDs when considering incomplete information in relational database design. We show that, when the set of FDs F either satisfies the intersection property or the split-freeness property, then the problem of finding an optimum cover of F can be solved in polynomial time in the size of F; in general, this problem is known to be NP-complete. We also show that when F satisfies the split-freeness property then deciding whether there is a superkey of cardinality k or less can be solved in polynomial time in the size of F, since all the keys have the same cardinality. If F only satisfies the intersection property then this problem is NP-complete, as in the general case. Moreover, we show that when F either satisfies the intersection property or the split-freeness property then deciding whether an attribute is prime can be solved in polynomial time in the size of F; in general, this problem is known to be NP-complete. Assume that a relation schema R is in an appropriate normal form with respect to a set of FDs F. We show that when F satisfies the intersection property then the notions of second normal form and third normal form are equivalent. We also show that when R is in Boyce-Codd Normal Form (BCNF), then F is monodependent if and only if either there is a unique key for R, or for all keys X for R, the cardinality of X is one less than the number of attributes associated with R. Finally, we tackle a long-standing problem in relational database theory by showing that when a set of FDs F over R satisfies the intersection property, it also satisfies the split-freeness property (i.e., is monodependent), if and only if every lossless join decomposition of R with respect to F is also dependency preserving. As a corollary of this result we are able to show that when F satisfies the intersection property, it also satisfies the split-freeness property (i.e., is monodependent), if and only if every lossless join decomposition of R, which is in BCNF, is also dependency preserving. Our final result is that when F is monodependent, then there exists a unique optimum lossless join decomposition of R, which is in BCNF, and is also dependency preserving. Furthermore, this ultimate decomposition can be attained in polynomial time in the size of F. General Terms: Design, Theory.",Additivity problem; Complexity; Dependency preserving decomposition; Incomplete information; Intersection property; Lossless join decomposition; Monodependence; Normal forms; Null functional dependencies; Optimum cover; Prime attribute problem,
Information gathering in the World Wide Web: The W3QL query language and the W3QS system,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032256889&doi=10.1145%2f296854.277639&partnerID=40&md5=89bba3ee2c59dbac6da975d287b71628,"The World Wide Web (WWW) is a fast growing global information resource. It contains an enormous amount of information and provides access to a variety of services. Since there is no central control and very few standards of information organization or service offering, searching for information and services is a widely recognized problem. To some degree this problem is solved by `search services,' also known as `indexers,' such as Lycos, AltaVista, Yahoo, and others. These sites employ search engines known as `robots' or `knowbots' that scan the network periodically and form text-based indices. These services are limited in certain important aspects. First, the structural information, namely, the organization of the document into parts pointing to each other, is usually lost. Second, one is limited by the kind of textual analysis provided by the `search service.' Third, search services are incapable of navigating `through' forms. Finally, one cannot prescribe a complex database-like search. We view the WWW as a huge database. We have designed a high-level SQL-like language called W3QL to support effective and flexible query processing, which addresses the structure and content of WWW nodes and their varied sorts of data. We have implemented a system called W3QS to execute W3QL queries. In W3QS, query results are declaratively specified and continuously maintained as views when desired. The current architecture of W3QS provides a server that enables users to pose queries as well as integrate their own data analysis tools. The system and its query language set a framework for the development of database-like tools over the WWW. A significant contribution of this article is in formalizing the WWW and query processing over it.",,Computer architecture; Data acquisition; Data reduction; Distributed database systems; HTML; HTTP; Information services; Query languages; Data analysis tools; World Wide Web
Temporal FDs on Complex Objects,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0004730750&doi=10.1145%2f310701.310715&partnerID=40&md5=d0fe2e89bfc51c9f5b0146566d59024b,"Temporal functional dependencies (TFD) are defined for temporal databases that include object identity. It is argued that object identity can overcome certain semantic difficulties with existing temporal relational data models. Practical applications of TFDs in object bases are discussed. Reasoning about TFDs is at the center of this paper. It turns out that the distinction between acyclic and cyclic schemas is significant. For acyclic schemas, a complete axiomatization for finite implication is given and an algorithm for deciding finite implication provided. The same axiomatization is proven complete for unrestricted implication in unrestricted schemas, which can be cyclic. An interesting result is that there are cyclic schemas for which unrestricted and finite implication do not coincide. TFDs relate and extend some earlier work on dependency theory in temporal databases. Throughout this paper, the construct of TFD is compared with the notion of temporal FD introduced by Wang et al. [1997]. A comparison with other related work is provided at the end of the article. General Terms: Design, Theory.",Database constraints; Functional dependency; Object-identity; Temporal databases; Time granularity,
Specification and Implementation of Exceptions in Workflow Management Systems,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000865747&doi=10.1145%2f328939.328996&partnerID=40&md5=e92d711ccce3b4cbe5efe6efb5f7827c,"Although workflow management systems are most applicable when an organization follows standard business processes and routines, any of these processes faces the need for handling exceptions, i.e., asynchronous and anomalous situations that fall outside the normal control flow. In this paper we concentrate upon anomalous situations that, although unusual, are part of the semantics of workflow applications, and should be specified and monitored coherently; in most real-life applications, such exceptions affect a significant fraction of workflow cases. However, very few workflow management systems are integrated with a highly expressive language for specifying this kind of exception and with a system component capable of handling it. We present Chimera-Exc, a language for the specification of exceptions for workflows based on detached active rules, and then describe the architecture of a system, called FAR, that implements Chimera-Exc and integrates it with a commercial workflow management system and database server. We discuss the main issues that were solved by our implementation, and report on the performance of FAR. We also discuss design criteria for exceptions in light of the formal properties of their execution. Finally, we focus on the portability of FAR and on its unbundling to a generic architecture with detached active rules.",Active rules; Asynchronous events; Design; Exceptions; H.2.4 [Database Management]: Systems; Rule-based databases; Languages; Management; Performance; Workflow management systems,Computer software portability; Interoperability; Management information systems; Semantics; Servers; Societies and institutions; Specifications; Active rules; Asynchronous events; Exceptions; Rule-based databases; Workflow management systems (WfMS); Database systems
Towards a theory of cost management for digital libraries and electronic commerce,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032284833&doi=10.1145%2f296854.277641&partnerID=40&md5=86d5281660e8ae2e5b3a5d18ed7f405f,"One of the features that distinguishes digital libraries from traditional databases is new cost models for client access to intellectual property. Clients will pay for accessing data items in digital libraries, and we believe that optimizing these costs will be as important as optimizing performance in traditional databases. In this article we discuss cost models and protocols for accessing digital libraries, with the objective of determining the minimum cost protocol for each model. We expect that in the future information appliances will come equipped with a cost optimizer, in the same way that computers today come with a built-in operating system. This article makes the initial steps towards a theory and practice of intellectual property cost management.",,Algorithms; Computer operating systems; Computer systems programming; Data acquisition; Data recording; Data reduction; Data structures; Electronic commerce; Information services; Network protocols; Online systems; Response time (computer systems); Average case analysis; Digital libraries; Worst case analysis; Database systems
Distance browsing in spatial databases,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000701994&doi=10.1145%2f320248.320255&partnerID=40&md5=9127750e12ea3c1a363aac8f49e9ed61,"We compare two different techniques for browsing through a collection of spatial objects stored in an R-tree spatial data structure on the basis of their distances from an arbitrary spatial query object. The conventional approach is one that makes use of a k-nearest neighbor algorithm where k is known prior to the invocation of the algorithm. Thus if m > k neighbors are needed, the k-nearest neighbor algorithm has to be reinvoked for m neighbors, thereby possibly performing some redundant computations. The second approach is incremental in the sense that having obtained the k nearest neighbors, the k + 1st neighbor can be obtained without having to calculate the k + 1 nearest neighbors from scratch. The incremental approach is useful when processing complex queries where one of the conditions involves spatial proximity (e.g., the nearest city to Chicago with population greater than a million), in which case a query engine can make use of a pipelined strategy. We present a general incremental nearest neighbor algorithm that is applicable to a large class of hierarchical spatial data structures. This algorithm is adapted to the R-tree and its performance is compared to an existing k-nearest neighbor algorithm for R-trees [Roussopoulos et al. 1995]. Experiments show that the incremental nearest neighbor algorithm significantly outperforms the k-nearest neighbor algorithm for distance browsing queries in a spatial database that uses the R-tree as a spatial index. Moreover, the incremental nearest neighbor algorithm usually outperforms the k-nearest neighbor algorithm when applied to the k-nearest neighbor problem for the R-tree, although the improvement is not nearly as large as for distance browsing queries. In fact, we prove informally that at any step in its execution the incremental nearest neighbor algorithm is optimal with respect to the spatial data structure that is employed. Furthermore, based on some simplifying assumptions, we prove that in two dimensions the number of distance computations and leaf nodes accesses made by the algorithm for finding k neighbors is O (k+ k).",Algorithms; Data Structures; Database applications; Distance browsing; E.1 [Data]; H.2.8 [Database Management]; Hierarchical spatial data structures; Nearest neighbors; Performance; R-trees; Ranking; Spatial databases and GIS; Trees,Algorithms; Computational methods; Computer aided design; Data structures; Geographic information systems; Input output programs; Trees (mathematics); Web browsers; Distance browsing; Distance computation; Hierarchical spatial data structures; Ranking; Query languages
Indexing Large Metric Spaces for Similarity Search Queries,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001728087&doi=10.1145%2f328939.328959&partnerID=40&md5=ef4c23a8eaf546318069a2f6681c4ae4,"One of the common queries in many database applications is finding approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance-based index structures are proposed for applications where the distance computations between objects of the data domain are expensive (such as high-dimensional data) and the distance function is metric. In this paper we consider using distance-based index structures for similarity queries on large metric spaces. We elaborate on the approach that uses reference points (vantage points) to partition the data space into spherical shell-like regions in a hierarchical manner. We introduce the multivantage point tree structure (mvp-tree) that uses more than one vantage point to partition the space into spherical cuts at each level. In answering similarity-based queries, the mvp-tree also utilizes the precomputed (at construction time) distances between the data points and the vantage points. We summarize the experiments comparing mvp-trees to vp-trees that have a similar partitioning strategy, but use only one vantage point at each level and do not make use of the precomputed distances. Empirical studies show that the mvp-tree outperforms the vp-tree by 20% to 80% for varying query ranges and different distance distributions. Next, we generalize the idea of using multiple vantage points and discuss the results of experiments we have made to see how varying the number of vantage points in a node affects search performance and how much is gained in performance by making use of precomputed distances. The results show that, after all, it may be best to use a large number of vantage points in an internal node in order to end up with a single directory node and keep as many of the precomputed distances as possible to provide more efficient filtering during search operations. Finally, we provide some experimental results that compare mvp-trees with M-trees, which is a dynamic distance-based index structure for metric domains.",Algorithms; E.1 [Data]: Data Structures - Trees; Experimentation; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing - Indexing methods; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process,Algorithms; Data storage equipment; Data structures; Indexing (of information); Information retrieval; Online searching; Semantics; Content analysis; Data domain; Distance-based index structures; Query languages
Type-Checking OQL Queries in the ODMG Type Systems,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001298897&doi=10.1145%2f328939.328943&partnerID=40&md5=9e93aa36d4f453e703bbfa2b36581396,"Several negative results are proved about the ability to type-check queries in the only existing proposed standard for object-oriented databases. The first of these negative results is that it is not possible to type-check OQL queries in the type system underlying the ODMG object model and its definition language ODL. The second negative result is that OQL queries cannot be type-checked in the type system of the Java binding of the ODMG standard either. A solution proposed in this paper is to extend the ODMG object model with explicit support for parametric polymorphism (universal type quantification). These results show that Java™ cannot be a viable database programming language unless extended with parametric polymorphism. This is why type-checking OQL queries presents no problem for the type system of the C++ binding of the ODMG standard. However, a type system that is strictly more powerful than any of the type systems of the ODMG standard is required in order to properly type ordered collections and indices. The required form of polymorphism is bounded type quantification (constrained genericity) and even F-bounded polymorphism. A further result is that neither static nor the standard dynamic object-oriented type-checking is possible for Java OQL, in spite of the fact that Java OQL combines features of two strongly and mostly statically-typed languages. Contrary to one of the promises of object-oriented database technology, this result shows that the impedance mismatch does not disappear in the ODMG standard. A type-safe reflective technique is proposed for overcoming this mismatch.",H.2.3 [Database Management]: Languages - Query languages; Data description languages (DDL); Database (persistent) programming languages; H.2.4 [Database Management]: Systems - Object-oriented databases,C (programming language); Data description; Java programming language; Object oriented programming; Standardization; Standards; Data description languages (DDL); Inheritance; Language constructs; Object-oriented databases; Parametric polymorphism; Type systems; Query languages
Improving Database Design through the Analysis of Relationships,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008537248&doi=10.1145%2f331983.331984&partnerID=40&md5=9902bf6c83fb09dd0099de85e7fbb0b2,"Much of the work on conceptual modeling involves the use of an entity-relationship model in which binary relationships appear as associations between two entities. Relationships involving more than two entities are considered rare and, therefore, have not received adequate attention. This research provides a general framework for the analysis of relationships in which binary relationships simply become a special case. The framework helps a designer to identify ternary and other higher-degree relationships that are commonly represented, often inappropriately, as either entities or binary relationships. Generalized rules are also provided for representing higher-degree relationships in the relational model. This uniform treatment of relationships should significantly ease the burden on a designer by enabling him or her to extract more information from a real-world situation and represent it properly in a conceptual design.",Conceptual model; ER model; Integrity constraint; Min-max cardinality; Relationship degree; Weak relationship,Computer simulation; Data handling; Information management; Security of data; Database design; Database technology; Database systems
The SIFT Information Dissemination System,1999,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001563073&doi=10.1145%2f331983.331992&partnerID=40&md5=9933c13e162ca466c8d8447ad821214f,"Information dissemination is a powerful mechanism for finding information in wide-area environments. An information dissemination server accepts long-term user queries, collects new documents from information sources, matches the documents against the queries, and continuously updates the users with relevant information. This paper is a retrospective of the Stanford Information Filtering Service (SIFT), a system that as of April 1996 was processing over 40,000 worldwide subscriptions and over 80,000 daily documents. The paper describes some of the indexing mechanisms that were developed for SIFT, as well as the evaluations that were conducted to select a scheme to implement. It also describes the implementation of SIFT, and experimental results for the actual system. Finally, it also discusses and experimentally evaluates techniques for distributing a service such as SIFT for added performance and availability.",Boolean queries; Dissemination; Filtering; Indexing; Vector space queries,Computer software; Information dissemination; Information retrieval; Query languages; Servers; Boolean queries; Vector space queries; Database systems
Erratum: Database Design with Common Sense Business Reasoning and Learning (ACM Transactions on Database Systems (Dec. 1997) 22:4 (471-512)),1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0345134571&partnerID=40&md5=2e5d39f3a7bd5052a8c7dce5a707d049,[No abstract available],,
Ensuring Consistency in Multidatabases by Preserving Two-Level Serializability,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032083692&doi=10.1145%2f292481.277629&partnerID=40&md5=5d76eb5b261e950c4915ef1ff3098587,"The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency. In this article, we introduce a systematic approach to relaxing the serializability requirement in MDBS environments. Our approach exploits the structure of the integrity constraints and the nature of transaction programs to ensure consistency without requiring executions to be serializable. We develop a simple yet powerful classification of MDBSs based on the nature of integrity constraints and transaction programs. For each of the identified models we show how consistency can be preserved by ensuring that executions are two-level serializable (2LSR). 2LSR is a correctness criterion for MDBS environments weaker than serializability. What makes our approach interesting is that unlike global serializability, ensuring 2LSR in MDBS environments is relatively simple and protocols to ensure 2LSR permit a high degree of concurrency. Furthermore, we believe the range of models we consider cover many practical MDBS environments to which the results of this article can be applied to preserve database consistency.",Beyond serializability; Concurrency control; Database consistency; H.2.4 [Database Management]: Systems - Concurrency; distributed databases; transaction processing; Management; Multidatabases; Theory,Algorithms; Computer software; Concurrency control; Network protocols; Problem solving; Semantics; Multiprocessing systems; Database consistency; Multidatabases (MDBS); Serializability; Transaction processing; Multidatabase systems (MDBS); Two level serializability; Database systems
An Access Control Model Supporting Periodicity Constraints and Temporal Reasoning,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032153903&doi=10.1145%2f293910.293151&partnerID=40&md5=cfe5854d4fa462f53076f9d0d9df92ae,"Access control models, such as the ones supported by commercial DBMSs, are not yet able to fully meet many application needs. An important requirement derives from the temporal dimension that permissions have in many real-world situations. Permissions are often limited in time or may hold only for specific periods of time. In this article, we present an access control model in which periodic temporal intervals are associated with authorizations. An authorization is automatically granted in the specified intervals and revoked when such intervals expire. Deductive temporal rules with periodicity and order constraints are provided to derive new authorizations based on the presence or absence of other authorizations in specific periods of time. We provide a solution to the problem of ensuring the uniqueness of the global set of valid authorizations derivable at each instant, and we propose an algorithm to compute this set. Moreover, we address issues related to the efficiency of access control by adopting a materialization approach. The resulting model provides a high degree of flexibility and supports the specification of several protection requirements that cannot be expressed in traditional access control models.","Access control; H.2.7 [Information Systems]: Database Administration - security, integrity, and protection; Periodic authorization; Security; Temporal constraints; Time management",Algorithms; Computer software; Data privacy; Logic programming; Public policy; Societies and institutions; Constraint theory; Data acquisition; Data structures; Security of data; Access control; Periodic authorization; Temporal constraints; Time management; Database systems
An Efficient Method for Checking Object-Oriented Database Schema Correctness,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032153868&doi=10.1145%2f293910.293152&partnerID=40&md5=90495b7b8ba0576c0e960e6f6bb50fd3,"Inheritance is introduced in object-oriented systems to enhance code reuse and create more compact and readable software. Powerful object models adopt multiple inheritance, allowing a type (or class) definition to inherit from more than one supertype. Unfortunately, in applying this powerful modeling mechanism, inheritance conflicts may be generated, which arise when the same property or operation is defined in more than one supertype. Inheritance conflicts identification and resolution is the key issue of this article. In strongly typed object-oriented systems the resolution of inheritance conflicts depends on the compatibility of the types of the conflicting definitions. In case of incompatible types, a contradiction arises. This article focuses on object-oriented databases (ODBs), providing a method aimed at supporting the designer in the construction of correct ODB schemas. The first necessary condition for schema correctness is the absence of contradictions. A second cause of schema incorrectness is due to the presence of structurally recursive types that, when defined within certain hierarchical patterns, cause the nontermination of the inheritance process. In the article, after the formal definition of a correct schema, two graph-theoretic methods aimed at verifying ODB schema correctness are analyzed. Although the first method is intuitive but inefficient, the second allows schema correctness to be checked in polynomial time, in the size of the schema. The results of this study are included in the implementation of Mosaico, an environment for ODB application design.",D.3.3 [Programming Languages]: Language Constructs and Features - data types and structures; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - mechanical verification,Computer programming languages; Computer software; Graph theory; Mathematical models; Object oriented programming; Codes (symbols); Computer systems programming; Data structures; Polynomials; Object-oriented databases (ODB); Object-oriented systems; Object oriented databases (ODB); Database systems
Optimization Techniques for Queries with Expensive Methods,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032083881&doi=10.1145%2f292481.277627&partnerID=40&md5=498c98ec61d05a36febc3d1ba51ecabb,"Object-relational database management systems allow knowledgeable users to define new data types as well as new methods (operators) for the types. This flexibility produces an attendant complexity, which must be handled in new ways for an object-relational database management system to be efficient. In this article we study techniques for optimizing queries that contain time-consuming methods. The focus of traditional query optimizers has been on the choice of join methods and orders; selections have been handled by ""pushdown"" rules. These rules apply selections in an arbitrary order before as many joins as possible, using the assumption that selection takes no time. However, users of object-relational systems can embed complex methods in selections. Thus selections may take significant amounts of time, and the query optimization model must be enhanced. In this article we carefully define a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial object-relational database management system Illustra, and discuss practical issues that affect our earlier assumptions. We compare Predicate Migration to a variety of simpler optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we present may be useful for constrained workloads.",Algorithms; Expensive methods; Extensibility; H.2.4 [Database Management]: Systems - query processing; Object-relational databases; Performance; Predicate Migration; Predicate placement; Query optimization; Theory,Algorithms; Object oriented programming; Optimization; Data types; Object-relational systems; Predicate migration; Query optimizers; Predicate management algorithm; Query languages; Relational database systems
Conceptual Schema Analysis: Techniques and Applications,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032153830&doi=10.1145%2f293910.293150&partnerID=40&md5=230ffd694af5821539487992d0eadb6d,"The problem of analyzing and classifying conceptual schemas is becoming increasingly important due to the availability of a large number of schemas related to existing applications. The purposes of schema analysis and classification activities can be different: to extract information on intensional properties of legacy systems in order to restructure or migrate to new architectures; to build libraries of reference conceptual components to be used in building new applications in a given domain; and to identify information flows and possible replication of data in an organization. This article proposes a set of techniques for schema analysis and classification to be used separately or in combination. The techniques allow the analyst to derive significant properties from schemas, with human intervention limited as far as possible. In particular, techniques for associating descriptors with schemas, for abstracting reference conceptual schemas based on schema clustering, and for determining schema similarity are presented. A methodology for systematic schema analysis is illustrated, with the purpose of identifying and abstracting into reference components the similar and poten-tially reusable parts of a set of schemas. Experiences deriving from the application of the proposed techniques and methodology on a large set of Entity-Relationship conceptual schemas of information systems in the Italian Public Administration domain are described.",Conceptual modeling; Design; Documentation; H.2.1 [Database Management]: Logical Design; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing; Management; Reference components; Schema classification; Schema similarity,Data reduction; Data structures; Indexing (of information); Legacy systems; Classification (of information); Management information systems; Societies and institutions; System program documentation; Conceptual schema analysis; Conceptual modeling; Database systems
Safe Stratified Datalog with Integer Order Does Not Have Syntax,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010351&doi=10.1145%2f288086.288089&partnerID=40&md5=e9203419c17b626355d1f36a564c624b,"Stratified Datalog with integer (gap)-order (or Datalog¬, <z) is considered. A Datalog¬, <z -program is said to be safe iff its bottom-up processing terminates on all valid inputs. We prove that safe Datalog¬, <z -programs do not have effective syntax in the sense that there is no recursively enumerable set S of safe Datalog¬, <z -programs such that every safe Datalog¬, <z -program is equivalent to a program in S.",H.2.1 [Database Management]: Logical Design; H.2.3 [Database Management]: Languages; Languages; Theory; Verification,Computational complexity; Computer simulation; Logic programming; Theorem proving; Distributed database systems; Integer programming; Z transforms; Stratified datalog language; Syntax; Turning machines; Stratified datalog; Database systems; Query languages
Multiview Access Protocols for Large-Scale Replication,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032083996&doi=10.1145%2f292481.277628&partnerID=40&md5=a24633f02363aa7c0ede5ed85fa3d242,"This article proposes a scalable protocol for replication management in large-scale replicated systems. The protocol organizes sites and data replicas into a tree-structured, hierarchical cluster architecture. The basic idea of the protocol is to accomplish the complex task of updating replicated data with a very large number of replicas by a set of related but independently committed transactions. Each transaction is responsible for updating replicas in exactly one cluster and invoking additional transactions for member clusters. Primary copies (one from each cluster) are updated by a cross-cluster transaction. Then each cluster is independently updated by a separate transaction. This decoupled update propagation process results in possible multiple views of replicated data in a cluster. Compared to other replicated data management protocols, the proposed protocol has several unique advantages. First, thanks to a smaller number of replicas each transaction needs to atomically update in a cluster, the protocol significantly reduces the transaction abort rate, which tends to soar in large transactional systems. Second, the protocol improves user-level transaction response time as top-level update transactions are allowed to commit before all replicas have been updated. Third, read-only queries have the flexibility to see database views of different degrees of consistency and data currency. This ranges from global, most up to date, and consistent views, to local, consistent, but potentially old views, to local, nearest to users but potentially inconsistent views. Fourth, the protocol maintains its scalability by allowing dynamic system reconfiguration as it grows by splitting a cluster into two or more smaller ones. Fifth, autonomy of the clusters is preserved as no specific protocol is required to update replicas within the same cluster. Clusters are, therefore, free to use any valid replication or concurrency control protocols.","Algorithms; C.2.4 [Computer-Communication Networks]: Distributed Systems - distributed databases; Design; Experimentation; H.2.4 [Database Management]: Systems - distributed systems, transaction processing; Management; Measurement; Performance",Algorithms; Computer simulation; Database systems; Distributed computer systems; Large scale systems; Servers; Telecommunication networks; Computer architecture; Computer networks; Data communication systems; Network protocols; Data replication; Multiview access (MVA) protocols; Transaction processing; Large-scale replication; Multiview access protocols; Network protocols; Database systems
Supporting Valid-Time Indeterminacy,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010257&doi=10.1145%2f288086.288087&partnerID=40&md5=6916f7ba1b2e21e6d2fc18d756340d5a,"In valid-time indeterminacy it is known that an event stored in a database did in fact occur, but it is not known exactly when. In this paper we extend the SQL data model and query language to support valid-time indeterminacy. We represent the occurrence time of an event with a set of possible instants, delimiting when the event might have occurred, and a probability distribution over that set. We also describe query language constructs to retrieve information in the presence of indeterminacy. These constructs enable users to specify their credibility in the underlying data and their plausibility in the relationships among that data. A denotational semantics for SQL's select statement with optional credibility and plausibility constructs is given. We show that this semantics is reliable, in that it never produces incorrect information, is maximal, in that if it were extended to be more informative, the results may not be reliable, and reduces to the previous semantics when there is no indeterminacy. Although the extended data model and query language provide needed modeling capabilities, these extensions appear initially to carry a significant execution cost. A contribution of this paper is to demonstrate that our approach is useful and practical. An efficient representation of valid-time indeterminacy and efficient query processing algorithms are provided. The cost of support for indeterminacy is empirically measured, and is shown to be modest. Finally, we show that the approach is general, by applying it to the temporal query language constructs being proposed for SQL3.",Algorithms; Data models; H.2.1 [Database Management]: Logical Design; H.2.3 [Database Management]: Languages; H.2.4 [Database Management]: Systems; Incomplete information; Indeterminacy; Languages; Probabilistic information; Query languages; Query processing,Algorithms; Graph theory; Information analysis; Probabilistic logics; Probability distributions; Semantics; Theorem proving; Computational linguistics; Data reduction; Data structures; Distributed database systems; Planar graphs; Structured query languages (SQL); Timestamping; Valid-time indeterminancy; Valid-time indeterminacy; Query languages
Safe Query Languages for Constraint Databases,1998,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010247&doi=10.1145%2f288086.288088&partnerID=40&md5=fdc1e6be75249865ada957606d65fd09,In the database framework of Kanellakis et al. [1990] it was argued that constraint query languages should take constraint databases as input and give other constraint databases that use the same type of atomic constraints as output. This closed-form requirement has been difficult to realize in constraint query languages that contain the negation symbol. This paper describes a general approach to restricting constraint query languages with negation to safe subsets that contain only programs that are evaluable in closed-form on any valid constraint database input.,Algorithms; H.2.1 [Database Management]: Logical Design; H.2.3 [Database Management]: Languages - Datalog; Languages; Query languages; Theory; Verification,Algorithms; Constraint theory; Distributed database systems; Algorithmic languages; Computer simulation; Logic programming; Constraint databases; Constraint query languages; Stratified datalog language; Turning machines; Constraint databases; Query languages
Database Design with Common Sense Business Reasoning and Learning,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031313817&doi=10.1145%2f278245.278246&partnerID=40&md5=87d98c640062c7c2e395515d32924657,"Automated database design systems embody knowledge about the database design process. However, their lack of knowledge about the domains for which databases are being developed significantly limits their usefulness. A methodology for acquiring and using general world knowledge about business for database design has been developed and implemented in a system called the Common Sense Business Reasoner, which acquires facts about application domains and organizes them into a hierarchical, context-dependent knowledge base. This knowledge is used to make intelligent suggestions to a user about the entities, attributes, and relationships to include in a database design. A distance function approach is employed for integrating specific facts, obtained from individual design sessions, into the knowledge base (learning) and for applying the knowledge to subsequent design problems (reasoning).",D.2.1 [Software Engineering]: Requirements/Specifications; H.2.1 [Database Management]: Logical Design; H.2.8 [Database Management]: Database Applications; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods,Artificial intelligence; Formal logic; Hierarchical systems; Knowledge based systems; Knowledge representation; Learning systems; Software engineering; Common sense business reasoning; Database systems
"Adaptive, Fine-Grained Sharing in a Client-Server OODBMS: A Callback-Based Approach",1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031313818&doi=10.1145%2f278245.278249&partnerID=40&md5=ec0f6d4b6683003d17bd412aa9f59479,"For reasons of simplicity and communication efficiency, a number of existing object-oriented database management systems are based on page server architectures; data pages are their minimum unit of transfer and client caching. Despite their efficiency, page servers are often criticized as being too restrictive when it comes to concurrency, as existing systems use pages as the minimum locking unit as well. In this paper we show how to support object-level locking in a page-server context. Several approaches are described, including an adaptive granularity approach that uses page-level locking for most pages but switches to object-level locking when finer-grained sharing is demanded. Each of the approaches is based on extending the idea of callback locking. We study the performance of these approaches, comparing them to both a pure page server and a pure object server. For the range of workloads that we have examined, our results indicate that the adaptive page server provides very good performance, usually outperforming the pure page server and the other page-server variants as well. In addition, the adaptive page server is often preferable to the pure object server; our results provide insight into when each approach is likely to perform better.",Algorithms; Cache coherency; Cache consistency; Client-server databases; Design; Distributed systems; Fine-grained sharing; H.2.4 [Database Management]: Systems - concurrency; Object-oriented databases; Performance; Performance analysis; Transaction processing,Adaptive control systems; Buffer storage; Concurrency control; Data structures; Object oriented programming; Online systems; Callback based methods; Client server systems; Fine grained sharing; Object oriented database management systems (OODBMS); Distributed database systems
Transactional Client-Server Cache Consistency: Alternatives and Performance,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031236717&doi=10.1145%2f261124.261125&partnerID=40&md5=99cbc8f65563a7ac1286f58dc51b2ea7,"Client-server database systems based on a data shipping model can exploit client memory resources by caching copies of data items across transaction boundaries. Caching reduces the need to obtain data from servers or other sites on the network. In order to ensure that such caching does not result in the violation of transaction semantics, a transactional cache consistency maintenance algorithm is required. Many such algorithms have been proposed in the literature and, as all provide the same functionality, performance is a primary concern in choosing among them. In this article we present a taxonomy that describes the design space for transactional cache consistency maintenance algorithms and show how proposed algorithms relate to one another. We then investigate the performance of six of these algorithms, and use these results to examine the tradeoffs inherent in the design choices identified in the taxonomy. The results show that the interactions among dimensions of the design space can impact performance in many ways, and that classifications of algorithms as simply ""pessimistic"" or ""optimistic"" do not accurately characterize the similarities and differences among the many possible cache consistency algorithms.",C.2.4 [Computer-Communication Networks]: Distributed Systems-Distributed databases; C.4 [Computer Systems Organization]: Performance of Systems; D.4.8 [Operating Systems]: Performance,Algorithms; Concurrency control; Distributed database systems; Performance; Systems analysis; Transactional client server cache consistency; Buffer storage
Disjunctive Datalog,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031237090&doi=10.1145%2f261124.261126&partnerID=40&md5=9911aff65080b40ab2878840f7b4e6a5,"We consider disjunctive Datalog, a powerful database query language based on disjunctive gic programming. Briefly, disjunctive Datalog is a variant of Datalog where disjunctions may appear in the rule heads; advanced versions also allow for negation in the bodies, which can be handled according to a semantics for negation in disjunctive logic programming. In particular, we investigate three different semantics for disjunctive Datalog: the minimal model semantics, the perfect model semantics, and the stable model semantics. For each of these semantics, the expressive power and complexity are studied. We show that the possibility variants of these semantics express the same set of queries. In fact, they precisely capture the complexity class Σp2. Thus, unless the Polynomial Hierarchy collapses, disjunctive Datalog is more expressive than normal logic programming with negation. These results are not only of theoretical interest; we demonstrate that problems relevant in practice such as computing the optimal tour value in the Traveling Salesman Problem and eigenvector computations can be handled in disjunctive Datalog, but not Datalog with negation (unless the Polynomial Hierarchy collapses). In addition, we study modularity properties of disjunctive Datalog and investigate syntactic restrictions of the formalisms.",1.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods; D. 1.6 [Programming Techniques]: Logic Programming; H.2.3 [Database Management]: Languages-query languages; I.2.3 [Artificial Intelligence]: Deduction and Theorem Proving,Algorithms; Computational complexity; Computational linguistics; Formal logic; Knowledge representation; Logic programming; Theorem proving; Disjunctive datalog; Polynomial hierarchy; Query languages
ProbView: A Flexible Probabilistic Database System,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031233999&doi=10.1145%2f261124.261131&partnerID=40&md5=2a03f6fd7b5ca2821a6d10abbe00adef,"Probability theory is mathematically the best understood paradigm for modeling and manipulating uncertain information. Probabilities of complex events can be computed from those of basic events on which they depend, using any of a number of strategies. Which strategy is appropriate depends very much on the known interdependencies among the events involved. Previous work on probabilistic databases has assumed a fixed and restrictive combination strategy (e.g., assuming all events are pairwise independent). In this article, we characterize, using postulates, whole classes of strategies for conjunction, disjunction, and negation, meaningful from the viewpoint of probability theory. (1) We propose a probabilistic relational data model and a generic probabilistic relational algebra that neatly captures various strategies satisfying the postulates, within a single unified framework. (2) We show that as long as the chosen strategies can be computed in polynomial time, queries in the positive fragment of the probabilistic relational algebra have essentially the same data complexity as classical relational algebra. (3) We establish various containments and equivalences between algebraic expressions, similar in spirit to those in classical algebra. (4) We develop algorithms for maintaining materialized probabilistic views. (5) Based on these ideas, we have developed a prototype probabilistic database system called ProbView on top of Dbase V.0. We validate our complexity results with experiments and show that rewriting certain types of queries to other equivalent forms often yields substantial savings.",Algebra; Data complexity; H.2.1 [Database Management]: Logical Design-data models; H.2.3 [Database Management]: Languages-query languages; H.2.4 [Database Management]: Systems; Performance evaluation; Probabilistic databases; View maintenance,Algorithms; Data structures; Probabilistic logics; Probability; Query languages; Probabilistic database system; Probabilistic relational algebra; Probabilistic relational data model; Database systems
On the Semantics of “Now” in Databases,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025410371&doi=10.1145%2f249978.249980&partnerID=40&md5=3de67e4730b89246d12399aa9d1a18bf,"Although “now” is expressed in SQL as CURRENT_TIMESTAMP within queries, this value cannot be stored in the database. However, this notion of an ever-increasing current-time value has been reflected in some temporal data models by inclusion of database-resident variables, such as “now”, “until-changed,” “⧜” “@,” and “-”. Time variables are very desirable, but their use also leads to a new type of database, consisting of tuples with variables, termed a variable database. This article proposes a framework for defining the semantics of the variable databases of the relational and temporal relational data models. A framework is presented because several reasonable meanings may be given to databases that use some of the specific temporal variables that have appeared in the literature. Using the framework, the article defines a useful semantics for such databases. Because situations occur where the existing time variables are inadequate, two new types of modeling entities that address these shortcomings, timestamps that we call now-relative and now-relative indeterminate, are introduced and defined within the framework. Moreover, the article provides a foundation, using algebraic bind operators, for the querying of variable databases via existing query languages. This transition to variable databases presented here requires minimal change to the query processor. Finally, to underline the practical feasibility of variable databases, we show that database variables can be precisely specified and efficiently implemented in conventional query languages, such as SQL, and in temporal query languages, such as TSQL2. © 1997, ACM. All rights reserved.",Indeterminacy; Languages; Now; now-relative value; Performance; SQL; temporal query language; TSQL2,
Object Normal Forms and Dependency Constraints for Object-Oriented Schemata,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031328161&doi=10.1145%2f278245.278247&partnerID=40&md5=01b63588fc278f8dcebbc2c81ae79fa2,"We address the development of a normalization theory for object-oriented data models that have common features to support object identity and complex objects. We first provide an extension of functional dependencies to cope with the richer semantics of relationships between objects, called path dependency, local dependency, and global dependency constraints. Using these dependency constraints, we provide normal forms for object-oriented data models based on the notions of user interpretation (user-specified dependency constraints) and object model. In contrast to conventional data models in which a normalized object has a unique interpretation, in object-oriented data models, an object may have many multiple interpretations that form the model for that object. An object will then be in a normal form if and only if the user's interpretation is derivable from the model of the object. Our normalization process is by nature iterative, in which objects are restructured until their models reflect the user's interpretation.",Data model; Design; Functional and multivalued dependencies; H.2.1 [Database Management]: Logical Design - Normal forms; H.2.4 [Database Management]: Systems; Normal forms; Object-oriented paradigm; Theory,Computational linguistics; Constraint theory; Data structures; Object oriented programming; Functional dependencies; Multivalued dependencies; Object oriented schemata; Relational database systems
An Adaptive Data Replication Algorithm,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000721930&doi=10.1145%2f249978.249982&partnerID=40&md5=215fedbd088d7ab64581075045a1a14d,"This article addresses the performance of distributed database systems. Specifically, we present an algorithm for dynamic replication of an object in distributed systems. The algorithm is adaptive in the sense that it changes the replication scheme of the object (i.e., the set of processors at which the object is replicated) as changes occur in the read-write pattern of the object (i.e., the number of reads and writes issued by each processor). The algorithm continuously moves the replication scheme towards an optimal one. We show that the algorithm can be combined with the concurrency control and recovery mechanisms of a distributed database management system. The performance of the algorithm is analyzed theoretically and experimentally. On the way we provide a lower bound on the performance of any dynamic replication algorithm.","C.2.4 [Computer-Communication Networks]: Distributed systems - distributed applications, distributed databases; C.4 [Computer Systems Organization]; H.2.4 [Database Management]: Systems - distributed systems, transaction processing",
Logical Design for Temporal Databases with Multiple Granularities,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000238056&doi=10.1145%2f249978.249979&partnerID=40&md5=4615ecaf9aa1950347a788d4d7fc1838,"The purpose of good database logical design is to eliminate data redundancy and insertion and deletion anomalies. In order to achieve this objective for temporal databases, the notions of temporal types, which formalize time granularities, and temporal functional dependencies (TFDs) are introduced. A temporal type is a monotonic mapping from ticks of time (represented by positive integers) to time sets (represented by subsets of reals) and is used to capture various standard and user-defined calendars. A TFD is a proper extension of the traditional functional dependency and takes the form X →μ Y, meaning that there is a unique value for Y during one tick of the temporal type μ for one particular X value. An axiomatization for TFDs is given. Because a finite set of TFDs usually implies an infinite number of TFDs, we introduce the notion of and give an axiomatization for a finite closure to effectively capture a finite set of implied TFDs that are essential to the logical design. Temporal normalization procedures with respect to TFDs are given. Specifically, temporal Boyce-Codd normal form (TBCNF) that avoids all data redundancies due to TFDs, and temporal third normal form (T3NF) that allows dependency preservation, are defined. Both normal forms are proper extensions of their traditional counterparts, BCNF and 3NF. Decomposition algorithms are presented that give lossless TBCNF decompositions and lossless, dependency-preserving, T3NF decompositions.",Algorithms; Boyce-Codd normal form; Design; Granularity; H.2.1 [Database Management]: Logical Design-normal forms; Normalization; Temporal databases; Temporal modules; Temporal relations; Theory; Third normal form,
Outerjoin Simplification and Reordering for Query Optimization,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031099920&doi=10.1145%2f244810.244812&partnerID=40&md5=8140912e71815aa47a6b051670f3a32d,"Conventional database optimizers take full advantage of associativity and commutativity properties of join to implement efficient and powerful optimizations on select/project/join queries. However, only limited optimization is performed on other binary operators. In this article, we present the theory and algorithms needed to generate alternative evaluation orders for the optimization of queries containing outerjoins. Our results include both a complete set of transformation rules, suitable for new-generation, transformation-based optimizers, and a bottom-up join enumeration algorithm compatible with those used by traditional optimizers.",Algorithms; G.2.2 [Discrete Mathematics]: Graph theory - graph algorithms; H.2.4 [Database Management]: Systems - query processing; Outerjoins; Query optimization; Query reordering; Theory; Verification,Algorithms; Optimization; Outerjoins; Query reordering; Query languages
An Axiomatic Model of Dynamic Schema Evolution in Objectbase Systems,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031098676&doi=10.1145%2f244810.244813&partnerID=40&md5=a4ebafc250abb49392ea8e7e4d578348,"The schema of a database system consists of the constructs that model the entities of data. Schema evolution is the timely change of the schema and the consistent management of these changes. Dynamic schema evolution (DSE) is the management of schema changes while a database management system is in operation. DSE is a necessary facility of objectbase systems (OBSs) because of the volatile application domains that OBSs support. We propose a sound and complete axiomatic model for DSE in OBSs that supports the fundamental concepts of object-oriented computing such as subtyping and property inheritance. The model can infer all schema relationships from two identified input sets associated with each type called the essential supertypes and essential properties. These sets are typically specified by schema designers, but can be automatically supplied within an OBS. The inference mechanism performed by the model has a proven termination. The axiomatic model is a formal treatment of DSE in OBSs, which distinguishes it from other approaches that informally define a number of schema invariants and the rules that enforce them. An informal approach leads to multiple DSE mechanisms because of the differences in object models and the choices made by system designers. The lack of a common object model makes comparison of OBSs more difficult. The axiomatic model provides a solution for DSE in OBSs by serving as a common, formal underlying foundation for describing DSE of existing systems, which makes comparison of these systems much easier. A design space for OBSs based on the inclusion/exclusion of axioms is developed and can be used to classify, compare, and differentiate the features of OBSs. To test the expressibility of the model, the DSE of several OBSs are reduced to the axiomatic model and compared.",Algorithms; Design; Dynamic schema evolution; H.2.1 [Database Management]: Logical design - schema and subschema; Management; Object database management systems; Theory,Algorithms; Object oriented programming; Axiomatic model; Dynamic schema evolution (DSE); Objectbase system (OBS); Database systems
Applying Formal Methods to Semantic-Based Decomposition of Transactions,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000484157&doi=10.1145%2f249978.249981&partnerID=40&md5=6061187b2e36323b487bacd8e1647635,"In some database applications the traditional approach of serializability, in which transactions appear to execute atomically and in isolation on a consistent database state, fails to satisfy performance requirements. Although many researchers have investigated the process of decomposing transactions into steps to increase concurrency, such research typically focuses on providing algorithms necessary to implement a decomposition supplied by the database application developer and pays relatively little attention to what constitutes a desirable decomposition or how the developer should obtain one. We focus on the decomposition itself. A decomposition generates proof obligations whose discharge ensures desirable properties with respect to the original collection of transactions. We introduce the notion of semantic histories to formulate and prove the necessary properties, and the notion of successor sets to describe efficiently the correct interleavings of steps. The successor set constraints use information about conflicts between steps so as to take full advantage of conflict serializability at the level of steps. We propose a mechanism based on two-phase locking to generate correct stepwise serializable histories.",D.2.1 [Software Engineering]: Requirements/Specifications-methodologies; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs-invariants; H.2.4 [Database Management]: System-transaction processing,
Extended Ephemeral Logging: Log Storage Management for Applications with Long-Lived Transactions,1997,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031099991&doi=10.1145%2f244810.244811&partnerID=40&md5=6b182ddeeab47044a5876934e9c47c2e,"Extended ephemeral logging (XEL) is a new technique for managing a log of database activity subject to the general assumption that the lifetimes of an application's transactions may be statistically distributed over a wide range. The log resides on nonvolatile disk storage and provides fault tolerance to system failures (in which the contents of volatile main memory storage may be lost). XEL segments a log into a chain of fixed-size FIFO queues and performs generational garbage collection on records in the log. Log records that are no longer necessary for recovery purposes are ""thrown away"" when they reach the head of a queue; only records that are still needed for recovery are forwarded from the head of one queue to the tail of the next. XEL does not require checkpoints, permits fast recovery after a crash and is well suited for applications that have a wide distribution of transaction lifetimes. Quantitative evaluation of XEL via simulation indicates that it can significantly reduce the disk space required for the log, at the expense of slightly higher bandwidth for log information and more main memory; the reduced size of the log permits much faster recovery after a crash as well as cost savings. XEL can significantly reduce both the disk space and the disk bandwidth required for log information in a system that has been augmented with a nonvolatile region of main memory.",Algorithms; Design; H.2.2 [Database Management]: Physical Design - recovery and restart; H.2.4 [Database Management]: Systems - transaction processing; H.2.7 [Database Management]: Database Administration - logging and recovery; Performance; Reliability,Algorithms; Computer system recovery; Cost effectiveness; Fault tolerant computer systems; Magnetic disk storage; Extended ephemeral logging; Database systems
Model and Verification of a Data Manager Based on ARIES,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030407533&doi=10.1145%2f236711.236712&partnerID=40&md5=82eb3bec06ed7307a79ec72f00473413,"In this article, we model and verify a data manager whose algorithm is based on ARIES. The work uses the I/O automata method as the formal model and the definition of correctness is defined on the interface between the scheduler and the data manager.","F.3.1 [Logics and Meanings of Programs]: Specifying and Reasoning about Programs - assertions, invariants, pre- and postconditions; H.2.2 [Data Management]: Physical Design - restart recovery; H.2.4 [Data Management]: Systems - transaction processing",Algorithms; Automata theory; Input output programs; Interfaces (computer); Database management system (DBMS); Software package ARIES; Database systems
"LH*-A Scalable, Distributed Data Structure",1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030400983&doi=10.1145%2f236711.236713&partnerID=40&md5=837f9f80f8302e159b76e61f03b4b338,"We present a scalable distributed data structure called LH*. LH* generalizes Linear Hashing (LH) to distributed RAM and disk files. An LH* file can be created from records with primary keys, or objects with OIDs, provided by any number of distributed and autonomous clients. It does not require a central directory, and grows gracefully, through splits of one bucket at a time, to virtually any number of servers. The number of messages per random insertion is one in general, and three in the worst case, regardless of the file size. The number of messages per key search is two in general, and four in the worst case. The file supports parallel operations, e.g., hash joins and scans. Performing a parallel operation on a file of M buckets costs at most 2M + 1 messages, and between 1 and O(log2 M) rounds of messages. We first describe the basic LH* scheme where a coordinator site manages bucket splits, and splits a bucket every time a collision occurs. We show that the average load factor of an LH* file is 65-70% regardless of file size, and bucket capacity. We then enhance the scheme with load control, performed at no additional message cost. The average load factor then increases to 80-95%. These values are about that of LH, but the load factor for LH* varies more. We next define LH* schemes without a coordinator. We show that insert and search costs are the same as for the basic scheme. The splitting cost decreases on the average, but becomes more variable, as cascading splits are needed to prevent file overload. Next, we briefly describe two variants of splitting policy, using parallel splits and presplitting that should enhance performance for high-performance applications. All together, we show that LH* files can efficiently scale to files that are orders of magnitude larger in size than single-site files. LH* files that reside in main memory may also be much faster than single-site disk files. Finally, LH* files can be more efficient than any distributed file with a centralized directory, or a static parallel or distributed hash file.",Algorithms; Data structures; Design; Distributed access methods; E.1 [Data]: Data Structures; E.2 [Data]: Data Storage Representations - hash table representations; Extensible hashing; H.2.1 [Database Management]: Logical Design; Linear hashing; Performance,Algorithms; Database systems; Distributed computer systems; Random access storage; Distributed access methods; Hash table representations; Linear hashing (LH); Data structures
Magic Conditions,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030102241&doi=10.1145%2f227604.227624&partnerID=40&md5=2de4b6934190db6cbd94528c6075cc5d,"Much recent work has focused on the bottom-up evaluation of Datalog programs [Bancilhon and Ramakrishnan 1988]. One approach, called magic-sets, is based on rewriting a logic program so that bottom-up fixpoint evaluation of the program avoids generation of irrelevant facts [Bancilhon et al. 1986; Beeri and Ramakrishnan 1987; Ramakrishnan 1991]. It was widely believed for some time that the principal application of the magic-sets technique is to restrict computation in recursive queries using equijoin predicates. We extend the magic-sets transformation to use predicates other than equality (X > 10, for example) in restricting computation. The resulting ground magic-sets transformation is an important step in developing an extended magic-sets transformation that has practical utility in ""real"" relational databases, not only for recursive queries, but for nonrecursive queries as well [Mumick et al. 1990b; Mumick 1991].",Algorithms; Bottom-up evaluation; Constraint logic programming; Constraints; Deductive databases; H.2.4 [Database Management]: Systems-query processing; Magic sets; Measurement; Query optimization; Relational databases; Starburst; Theory; Verification,Algorithms; Constraint theory; Logic programming; Query languages; Relational database systems; Bottom-up evaluation; Constraint logic programming; Deductive databases; Magic sets; Query optimization; Database systems
Declustering of Key-Based Partitioned Signature Files,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030247602&doi=10.1145%2f232753.232755&partnerID=40&md5=1fd008e62809d1cd70cfa28b8735e22f,"Access methods based on signature files can largely benefit from possibilities offered by parallel environments. To this end, an effective declustering strategy that would distribute signatures over a set of parallel independent disks has to be combined with a synergic clustering which is employed to avoid searching the whole signature file while executing a query. This article proposes two parallel signature file organizations, Hamming Filter (HF) and Hamming+ Filter (H+F), whose common declustering strategy is based on error correcting codes, and where clustering is achieved by organizing signatures into fixed-size buckets, each containing signatures sharing the same key value. HF allocates signatures on disks in a static way and works well if a correct relationship holds between the parameters of the code and the size of the file. H+F is a generalization of HF suitable to manage highly dynamic files. It uses a dynamic declustering, obtained through a sequence of codes, and organizes a smooth migration of signatures between disks so that high performance levels are retained regardless of current file size. Theoretical analysis characterizes the best-case, expected, and worst-case behaviors of these organizations. Analytical results are verified by experiments on prototype systems.",E.4 [Data]: Coding and Information Theory; E.5 [Data]: Files - Organization/structure; H.2.2 [Database Management]: Physical Design - Access methods; H.2.4 [Database Management]: Systems - Query processing,Codes (symbols); Database systems; Error correction; File organization; Information retrieval; Performance; Software prototyping; Error correcting codes; Organizing signatures; Parallel environments; Parallel independent disk; Partial match queries; Prototype systems; Superimposed coding; Parallel processing systems
A Probabilistic Relational Model and Algebra,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030245427&doi=10.1145%2f232753.232796&partnerID=40&md5=33bf5092e6c4e2b734c21da9a89248b1,"Although the relational model for databases provides a great range of advantages over other data models, it lacks a comprehensive way to handle incomplete and uncertain data. Uncertainty in data values, however, is pervasive in all real-world environments and has received much attention in the literature. Several methods have been proposed for incorporating uncertain data into relational databases. However, the current approaches have many shortcomings and have not established an acceptable extension of the relational model. In this paper, we propose a consistent extension of the relational model. We present a revised relational structure and extend the relational algebra. The extended algebra is shown to be closed, a consistent extension of the conventional relational algebra, and reducible to the latter.",F.4.3 [Mathematical Logic and Formal Languages]: Formal Languages - Algebraic language theory; G.3 [Probability and Statistics]: Statistical Computing; H.2.1 [Database Management]: Logical Design - Data models,Algebra; Artificial intelligence; Data structures; Formal languages; Formal logic; Mathematical models; Probabilistic logics; Query languages; Theorem proving; Data incompleteness; Data uncertainty; Probabilistic relation; Probability calculus; Relational algebra; Relational model; Relational database systems
Tail Recursion Elimination in Deductive Databases,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030173750&doi=10.1145%2f232616.232628&partnerID=40&md5=82937da91bb528d39681a63af466cc44,"We consider an optimization technique for deductive and relational databases. The optimization technique is an extension of the magic templates rewriting, and it can improve the performance of query evaluation by not materializing the extension of intermediate views. Standard relational techniques, such as unfolding embedded view definitions, do not apply to recursively defined views, and so alternative techniques are necessary. We demonstrate the correctness of our rewriting. We define a class of ""nonrepeating"" view definitions, and show that for certain queries our rewriting performs at least as well as magic templates on nonrepeating views, and often much better. A syntactically recognizable property, called ""weak right-linearity,"" is proposed. Weak right-linearity is a sufficient condition for nonrepetition, and is more general than right-linearity. Our technique gives the same benefits as right-linear evaluation of right-linear views, while applying to a significantly more general class of views.",Algorithms; Deductive databases; H.2.3 [Database Management]: Languages - query languages; H.2.4 [Database Management]: Systems - query processing; I.2.3 [Artificial Intelligence]: Deduction and Theorem Proving-logic programming; Magic sets,Algorithms; Logic programming; Optimization; Performance; Query languages; Theorem proving; Deductive database; Magic sets; Query optimization; Tail recursion; Relational database systems
The Building Blocks for Specifying Communication Behavior of Complex Objects: An Activity-Driven Approach,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030171984&doi=10.1145%2f232616.232622&partnerID=40&md5=dff135074e586c18e864bb49b4cf2105,"Communication behavior represents dynamic evolution and cooperation of a group of objects in accomplishing a task. It is an important feature in object-oriented systems. We propose the concept of activity as a basic building block for declarative specification of communication behavior in object-oriented database systems, including the temporal ordering of message exchanges within object communication and the behavioral relationships between activity executions. We formally introduce two kinds of activity composition mechanisms: activity specialization and activity aggregation for abstract implementation of communication behavior. The former is suited for behavioral refinement of existing activities into specialized activities. The latter is used for behavioral composition of simpler activities into complex activities, and ultimately, into the envisaged database system. We use first-order temporal logic as an underlying formalism for specification of communication constraints. The well-known Air-traffic-control case is used as a running example to highlight the underlying concepts, to illustrate the usefulness, and to assess the effectiveness of the activity model for declarative specification of communication behavior in the relevant universe of discourse. We also propose a methodological framework for integrating activity schema with entity schema in an object-oriented design environment.",D.3.1 [Programming Languages]: Formal Definitions and Theory - semantics; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - specification techniques; H.2.1 [Database Management]: Logical Design,Air traffic control; Computational linguistics; Computer hardware description languages; Formal logic; Information theory; Logic design; Synchronization; Activity aggregation; Activity approach; Activity specialization; Communication behavior; First order temporal logic; Object oriented databases; Database systems
Heraclitus: Elevating Deltas to be First-Class Citizens in a Database Programming Language,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030246818&doi=10.1145%2f232753.232801&partnerID=40&md5=bbbcb343962ee0e3407faf3485c272b5,"Traditional database systems provide a user with the ability to query and manipulate one database state, namely the current database state. However, in several emerging applications the ability to analyze ""what-if"" scenarios in order to reason about the impact of an update (before committing that update) is of paramount importance. Example applications include hypothetical database access, active database management systems, and version management, to name a few. The central thesis of the Heraclitus paradigm is to provide flexible support for applications such as these by elevating deltas, which represent updates proposed against the current database state, to be first-class citizens. Heraclitus[Alg, C] is a database programming language that extends C to incorporate the relational algebra and deltas Operators are provided that enable the programmer to explicitly construct combine, and access deltas. Most interesting is the when operator, that supports hypothetical access to a delta: the expression E when δ yields the value that side effect free expression E would have if the value of delta expression δ were applied to the current database state This article presents a broad overview of the philosophy underlying the Heraclitus paradigm and describes the design and prototype implementation of Heraclitus[Alg, C]. A model-independent formalism for the Heraclitus paradigm is also presented. To illustrate the utility of Heraclitus, the article presents an in-depth discussion of how Heraclitus[Alg, C] can be used to specify, and thereby implement, a wide range of execution models for rule application in active databases; this includes both prominent execution models presented in the literature and more recent ""customized"" execution models with novel features.",D.3.3 [Programming Languages]: Language Constructs and Feature; H.2.3 [Database Management]: Languages - Database (persistent) programming languages; H.2.4 [Database Management]: Systems - Query processing,Algebra; C (programming language); Computer programming languages; Mathematical models; Query languages; Software prototyping; Database programming language; Query processing; Relational algebra; Database systems
Semantics for Update Rule Programs and Implementation in a Relational Database Management System,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030396927&doi=10.1145%2f236711.236714&partnerID=40&md5=266322135a011a18e189b6e4f06d7ccb,"In this paper we present our research on defining a correct semantics for a class of update rule (UR) programs, and discuss implementing these programs in a DBMS environment. Update rules execute by updating relations in a database which may cause the further execution of rules. A correct semantics must guarantee that the execution of the rules will terminate and that it will produce a minimal updated database. The class of UR programs is syntactically identified, based upon a concept that is similar to stratification. We extend the strict definition of stratification and allow a relaxed criterion for partitioning of the rules in the UR program. This relaxation allows a limited degree of nondeterminism in rule execution. We define an execution semantics based upon a monotonic fixpoint operator TUR, resulting in a set of fixpoints for UR. The monotonicity of the operator is maintained by explicitly representing the effect of asserting and retracting tuples in the database. A declarative semantics for the update rule program is obtained by associating a normal logic program UR to represent the UR program. We use the stable model semantics which characterize a normal logic program by a set of minimal models which are called stable models. We show the equivalence between the set of fixpoints for UR and the set of stable models for UR. We briefly discuss implementing the fixpoint semantics of the UR program in a DBMS environment. Relations that can be updated by the rules are updatable relations and they are extended with two flags. An update rule is represented by a database query, which queries the updatable relations as well as database relations, i.e., those relations which are not updated by rules. We describe an algorithm to process the queries and compute a fixpoint in the DBMS environment and obtain a final database.","F.4.1 [Mathematical Logic and Formal Languages]: Mathematical Logic - model theory; H.2.3 [Database Management]: Languages - query languages, DDL",Algorithms; Formal languages; Formal logic; Knowledge representation; Query languages; Database management systems (DBMS); Monotonic fixpoint operators; Stable models; Relational database systems
Polymorphism and Type Inference in Database Programming,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030103193&doi=10.1145%2f227604.227609&partnerID=40&md5=7e1ab705bd0d56746b8c90db27859a0a,"In order to find a static type system that adequately supports database languages, we need to express the most general type of a program that involves database operations. This can be achieved through an extension to the type system of ML that captures the polymorphic nature of field selection, together with a technique that generalizes relational operators to arbitrary data structures. The combination provides a statically typed language in which generalized relational databases may be cleanly represented as typed structures. As in ML types are inferred, which relieves the programmer of making the type assertions that may be required in a complex database environment. These extensions may also be used to provide static polymorphic typechecking in object-oriented languages and databases. A problem that arises with object-oriented databases is the apparent need for dynamic typechecking when dealing with queries on heterogeneous collections of objects. An extension of the type system needed for generalized relational operations can also be used for manipulating collections of dynamically typed values in a statically typed language. A prototype language based on these ideas has been implemented. While it lacks a proper treatment of persistent data, it demonstrates that a wide variety of database structures can be cleanly represented in a polymorphic programming language.","D.3.1 [Programming Languages]: Formal Definitions and Theory; D.3.2 [Programming Languages]: Language Classification-applicative languages; D.3.3 [Programming Languages]: Language Constructs and Features-data types and construct, abstract data types",Data processing; Data structures; Object oriented programming; Relational database systems; Generalized relational algebra; Inheritance; Object-oriented databases; Polymorphism; Database systems
Solving Satisfiability and Implication Problems in Database Systems,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030164849&doi=10.1145%2f232616.232692&partnerID=40&md5=ffe5af8f6d35899cf580fc0eb98acbdf,"Satisfiability, implication, and equivalence problems involving conjunctive inequalities are important and widely encountered database problems that need to be efficiently and effectively processed. In this article we consider two popular types of arithmetic inequalities, (X op Y) and (X op C), where X and Y are attributes, C is a constant of the domain or X, and op ∈ {<, ≤, =, ≠, >, ≥}. These inequalities are most frequently used in a database system, inasmuch as the former type of inequality represents a θ - join, and the latter is a selection. We study the satisfiability and implication problems under the integer domain and the real domain, as well as under two different operator sets ({<, ≤, =, ≥, >} and {<, ≤, =, ≠, ≥, >}). Our results show that solutions under different domains and/or different operator sets are quite different. Out of these eight cases, excluding two cases that had been shown to be NP-hard, we either report the first necessary and sufficient conditions for these problems as well as their efficient algorithms with complexity analysis (for four cases), or provide an improved algorithm (for two cases). These iff conditions and algorithms are essential to database designers, practitioners, and researchers. These algorithms have been implemented and an experimental study comparing the proposed algorithms and those previously known is conducted. Our experiments show that the proposed algorithms are more efficient than previously known algorithms even for small input.",H.2.4 [Database Management]: Systems - query processing; I.1.2. [Algebraic Manipulation]: Algorithms - analysis of algorithms; I.2.3 [Artificial Intelligence]: Deduction and Theorem Proving - deduction,Algorithms; Computational complexity; Equivalence classes; Mathematical operators; Query languages; Theorem proving; Deduction; Implication; Reasoning; Satisfiability; Database systems
Implementing Deductive Databases by Mixed Integer Programming,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030164696&doi=10.1145%2f232616.232691&partnerID=40&md5=6e6e9364406e6ffd0e93a674b83c5ad9,"Existing and past generations of Prolog compilers have left deduction to run-time and this may account for the poor run-time performance of existing Prolog systems. Our work tries to minimize run-time deduction by shifting the deductive process to compile-time. In addition, we offer an alternative inferencing procedure based on translating logic to mixed integer programming. This makes available for research and implementation in deductive databases, all the theorems, algorithms, and software packages developed by the operations research community over the past 50 years. The method keeps the same query language as for disjunctive deductive databases, only the inferencing procedure changes. The language is purely declarative, independent of the order of rules in the program, and independent of the order in which literals occur in clause bodies. The technique avoids Prolog's problem of infinite looping. It saves run-time by doing primary inferencing at compile-time. Furthermore, it is incremental in nature. The first half of this article translates disjunctive clauses, integrity constraints, and database facts into Boolean equations, and develops procedures to use mixed integer programming methods to compute - least models of definite deductive databases, and - minimal models and the Generalized Closed World Assumption of disjunctive deductive databases. These procedures are sound and complete. The second half of the article proposes a query processing system based on mixed integer programming compilation, and describes our (implemented) prototype compiler. Experimental results using this compiler are reported. These results suggest that our compilation-based mixed integer programming paradigm is a promising approach to practical implementation of query systems for definition and disjunctive databases.",Deductive databases; F.4.1 [Mathematical Logic and Formal Languages]: Mathematical Logic; H.2.3 [Database Management]: Languages; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods; Languages; Theory; Verification,Algorithms; Boolean functions; Computer software; Integer programming; Query languages; Deductive databases; Mixed integer programming; Database systems
Modularization Techniques for Active Rules Design,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030103715&doi=10.1145%2f227604.227605&partnerID=40&md5=fc653ba63c347fd201402567fa1c0899,"Active database systems can be used to establish and enforce data management policies. A large amount of the semantics that normally needs to be coded in application programs can be abstracted and assigned to active rules. This trend is sometimes called ""knowledge independence""; a nice consequence of achieving full knowledge independence is that data management policies can then effectively evolve just by modifying rules instead of application programs. Active rules, however, may be quite complex to understand and manage: rules react to arbitrary event sequences, they trigger each other, and sometimes the outcome of rule processing may depend on the order in which events occur or rules are scheduled. Although reasoning on a large collection of rules is very difficult, the task becomes more manageable when the rules are few. Therefore we are convinced that modularization, similar to what happens in any software development process, is the key principle for designing active rules; however, this important notion has not been addressed so far. This article introduces a modularization technique for active rules called stratification; it presents a theory of stratification and indicates how stratification can be practically applied. The emphasis of this article is on providing a solution to a very concrete and practical problem; therefore, our approach is illustrated by several examples.",Active database systems; D.2.1 [Software Engineering]: Requirements/Specifications-methodologies; D.2.2 [Software Engineering]: Tools and Techniques-modules and interfaces; Database rule processing; Design; H.2.m [Database Management]: Miscellaneous,Management information systems; Software engineering; Active database systems; Database rule processing; Modularization; Static analysis; Termination; Database systems
A Normal Form for Precisely Characterizing Redundancy in Nested Relations,1996,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030103716&doi=10.1145%2f227604.227612&partnerID=40&md5=f7e8abfb3f42de26c2866e88602ccefb,"We give a straightforward definition for redundancy in individual nested relations and define a new normal form that precisely characterizes redundancy for nested relations. We base our definition of redundancy on an arbitrary set of functional and multivalued dependencies, and show that our definition of nested normal form generalizes standard relational normalization theory. In addition, we give a condition that can prevent an unwanted structural anomaly in nested relations, namely, embedded nested relations with at most one tuple. Like other normal forms, our nested normal form can serve as a guide for database design.","Data redundancy; Database design; Design,; Functional and multivalued dependencies; H.2.1 [Database Management]: Logical Design-data models, normal forms; Nested normal form; Nested relations; Normalization theory; Scheme trees; Theory",Data description; Data processing; Data structures; Data redundancy; Nested normal form; Nested relations; Normalization theory; Scheme trees; Database systems
"COST-BENEFIT DECISION MODEL: ANALYSIS, COMPARISON, AND SELECTION OF DATA MANAGEMENT SYSTEMS.",1987,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023418617&doi=10.1145%2f27629.33403&partnerID=40&md5=03390c6de35be2a461e2b41933d56982,"This paper describes a general cost-benefit decision model that is applicable to evaluation, comparison, and selection of alternative products with a multiplicity of features, such as complex computer systems. The application of this model is explained and illustrated using the selection of data management systems as an example. The cost model incorporates an aggregation of costs which may be estimated over different time horizons and discounted at appropriate discount rates. A procedure to establish an overall ranking of alternative systems based on their global preference scores and global costs is also discussed. (Edited author abstact)",,Mathematical models; COST-BENEFIT DECISION MODEL; DATA MANAGEMENT SYSTEMS; Database systems
PERFORMANCE ANALYSIS OF SEVERAL BACK-END DATABASE ARCHITECTURES.,1986,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022681079&doi=10.1145%2f5236.5242&partnerID=40&md5=76b60b2adad33e51d1a371ad71451fe5,"The growing acceptance of database systems makes their performance increasingly more important. One way to gain performance is to off-load some of the functions of the database system to a back-end computer. The problem is what functions should be off-loaded to maximize the benefits of distributed processing. Our approach to this database consited of constructing several variants of an existing relational dabase system, INGRES, that partition the database system software into two parts, and assigning these two parts to two computers conneted by a local area network. Six different variants of the database software were constructed to test the six most interesting functional subdivisions. Each variant was then benchmarked using two different databases and query streams. The communication medium and the communication software were also benchmarked. Various conclusions were reached about the viability of the configurations, the desirable properties of the communications mechanisms to be used, the operating system interface and overhead, the performance of the database system.",,"COMPUTER NETWORKS - Local Networks; COMPUTER SYSTEMS, DIGITAL - Distributed; BACK-END DATABASE ARCHITECTURES; INGRES DATABASE SYSTEM; RELATIONAL DATABASES; DATABASE SYSTEMS"
SELECTED PAPERS FROM THE 1985 ACM SIGMOD CONFERENCE.,1985,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022130474&doi=10.1145%2f3979.3980&partnerID=40&md5=50c9f1bb5b9976cbca8fd50301fe3bdf,This issue contains 2 conference papers. The topics covered are: logical query languages for databases; and modeling concepts for VLSI CAD objects.,,"INTEGRATED CIRCUITS, VLSI - Computer Aided Design; CAD; DESIGN AUTOMATION; EIREV; LOGICAL QUERY LANGUAGES; RELATIONAL DATABASES; DATABASE SYSTEMS"
LIMITATIONS OF CONCURRENCY IN TRANSACTION PROCESSING.,1985,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022026345&doi=10.1145%2f3148.3160&partnerID=40&md5=4509cf132b9c19d5b73f7e7cdac8367a,"The authors analyze the fundamental limitation of concurrency for various general classes of scheduling policies independently of any given transaction processing system. Using a random graph model of transaction processing systems, they bound the effective level of concurrency for three general classes of concurrency control policies, examples of which are (1) the usual locking method, which they call standard locking; (2) strict priority scheduling, with priorities assigned independently of conflicts; and (3) optimistic methods. These classes are shown to be fundamentally different. The results of the random graph model analysis are confirmed by simulations of an abstract transaction processing system.",,"COMPUTER SYSTEMS, DIGITAL - Multiprocessing; MATHEMATICAL TECHNIQUES - Graph Theory; CONCURRENCY CONTROL; TRANSACTION PROCESSING; DATABASE SYSTEMS"
ANALYSIS OF INDEX-SEQUENTIAL FILES WITH OVERFLOW CHAINING.,1981,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019687844&doi=10.1145%2f319628.319665&partnerID=40&md5=a6f1e74edeb6838fe0c85289c47c28e0,The gradual performance deterioration caused by deletions from and insertions into an index-sequential file after loading is analyzed. The model developed assumes that overflow records are handled by chaining. Formulas for computing the expected number of overflow records and the expected number of additional accesses caused by the overflow records for both successful and unsuccessful searches are derived.,,Data processing
OBJECT-ORIENTED APPROACH TO DATABASE SYSTEM IMPLEMENTATION.,1981,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019680495&doi=10.1145%2f319628.319645&partnerID=40&md5=3299db625ef6f7418c693c122fce35d9,"Object-oriented programming is examined as an implementation technique for database systems. The object-oriented approach encapsulates the representations of database entities and relationships with the procedures that manipulate them. A generic procedure model of database implementation techniques is presented and discussed. Several current database system implementation techniques are illustrated as examples of this model, followed by a critical analysis of our implementation technique based on the use of objects. It is demonstrated that the object-oriented approach has advantages of data independence, run-time efficiency due to eliminating access to system descriptors,and support for low-level views.",,Computer programming; Data base systems
HEURISTICS FOR TRIE INDEX MINIMIZATION.,1979,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018517049&partnerID=40&md5=1db5971b012401a55ad27413be5a5df9,"A trie is a digital search tree in which leaves correspond to records in a file. Searching proceeds from the root to a leaf, where the edge taken at each node depends on the value of an attribute in the query. Trie implementations have the advantage of being fast, but the disadvantage of achieving that speed at great expense in storage space. Of primary concern in making a trie practical, therefore, is the problem of minimizing storage requirements. One method for reducing the space required is to reorder attribute testing. Unfortunately, the problem of finding an ordering which guarantees a minimum-size trie is NP-complete. Several heuristics for reordering attributes are investigated, and bounds are derived on the sizes of the worst tries produced by them in terms of the underlying file. Although the analysis is presented for a binary file, extensions to files of higher degree are shown. Another alternative for reducing the space required by a trie is an implementation, called an O-trie, in which the order of attribute testing is contained in the trie itself. We show that for most applications, O-tries are smaller than other implementation of tries, even when heuristics for improving storage requirements are employed.",,DATA PROCESSING
Proximity Queries on Terrain Surface,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146416642&doi=10.1145%2f3563773&partnerID=40&md5=566631d36b75b824ce660ed46ada303d,"Due to the advance of the geo-spatial positioning and the computer graphics technology, digital terrain data has become increasingly popular nowadays. Query processing on terrain data has attracted considerable attention from both the academic and the industry communities. Proximity queries such as the shortest path/distance query, k nearest/farthest neighbor query, and top-k closest/farthest pairs query are fundamental and important queries in the context of the terrain surfaces, and they have a lot of applications in Geographical Information System, 3D object feature vector construction, and 3D object data mining. In this article, we first study the most fundamental type of query, namely, shortest distance and path query, which is to find the shortest distance and path between two points of interest on the surface of the terrain. As observed by existing studies, computing the exact shortest distance/path is very expensive. Some existing studies proposed ϵ-approximate distance and path oracles, where ϵ is a non-negative real-valued error parameter. However, the best-known algorithm has a large oracle construction time, a large oracle size, and a large query time. Motivated by this, we propose a novel ϵ-approximate distance and path oracle called the Space Efficient distance and path oracle (SE), which has a small oracle construction time, a small oracle size, and a small distance and path query time, thanks to its compactness of storing concise information about pairwise distances between any two points-of-interest. Then, we propose several algorithms for the k nearest/farthest neighbor and top-k closest/farthest pairs queries with the assistance of our distance and path oracle SE. Our experimental results show that the oracle construction time, the oracle size, and the distance and path query time of SE are up to two, three, and five orders of magnitude faster than the best-known algorithm, respectively. Besides, our algorithms for other proximity queries including k nearest/farthest neighbor queries and top-k closest/farthest pairs queries significantly outperform the state-of-the-art algorithms by up to two orders of magnitude. © 2022 Association for Computing Machinery.",proximity queries; Shortest distance queries; shortest path queries; spatial database; terrain,Computer graphics; Data handling; Data mining; Graph theory; Landforms; Query languages; Construction time; Near-far; Path queries; Proximity queries; Query time; Short distance query; Shortest path queries; Spatial database; Terrain; Terrain surfaces; Query processing
Deciding Robustness for Lower SQL Isolation Levels,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146416389&doi=10.1145%2f3561049&partnerID=40&md5=cdd55905c455c64d22a51fe4c49f2ac4,"While serializability always guarantees application correctness, lower isolation levels can be chosen to improve transaction throughput at the risk of introducing certain anomalies. A set of transactions is robust against a given isolation level if every possible interleaving of the transactions under the specified isolation level is serializable. Robustness therefore always guarantees application correctness with the performance benefit of the lower isolation level. While the robustness problem has received considerable attention in the literature, only sufficient conditions have been obtained. The most notable exception is the seminal work by Fekete where he obtained a characterization for deciding robustness against SNAPSHOT ISOLATION. In this article, we address the robustness problem for the lower SQL isolation levels READ UNCOMMITTED and READ COMMITTED, which are defined in terms of the forbidden dirty write and dirty read patterns. The first main contribution of this article is that we characterize robustness against both isolation levels in terms of the absence of counter-example schedules of a specific form (split and multi-split schedules) and by the absence of cycles in interference graphs that satisfy various properties. A critical difference with Fekete's work, is that the properties of cycles obtained in this article have to take the relative ordering of operations within transactions into account as READ UNCOMMITTED and READ COMMITTED do not satisfy the atomic visibility requirement. A particular consequence is that the latter renders the robustness problem against READ COMMITTED coNP-complete. The second main contribution of this article is the coNP-hardness proof. For READ UNCOMMITTED, we obtain LOGSPACE-completeness. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Concurrency control; SQL isolation levels,Condition; Counter examples; Interference graphs; Interleavings; Isolation level; Performance benefits; Property; Serializability; SQL isolation level; Transaction throughput; Concurrency control
Conjunctive Queries: Unique Characterizations and Exact Learnability,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146419207&doi=10.1145%2f3559756&partnerID=40&md5=e65e9d6893af91447633934cbbdaa133,"We answer the question of which conjunctive queries are uniquely characterized by polynomially many positive and negative examples and how to construct such examples efficiently. As a consequence, we obtain a new efficient exact learning algorithm for a class of conjunctive queries. At the core of our contributions lie two new polynomial-time algorithms for constructing frontiers in the homomorphism lattice of finite structures. We also discuss implications for the unique characterizability and learnability of schema mappings and of description logic concepts. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Conjunctive queries; description logic; exact learnability; frontiers; homomorphisms; schema mappings; unique characterizations,Computer circuits; Formal languages; Polynomial approximation; Query processing; Conjunctive query; Description logic; Exact learnability; Frontier; Homomorphism; Learnability; Negative examples; Positive examples; Schema mappings; Unique characterization; Data description
On Finding Rank Regret Representatives,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137931071&doi=10.1145%2f3531054&partnerID=40&md5=d5d230fb49e027aad64e11286283e934,"Selecting the best items in a dataset is a common task in data exploration. However, the concept of “best” lies in the eyes of the beholder: Different users may consider different attributes more important and, hence, arrive at different rankings. Nevertheless, one can remove “dominated” items and create a “representative” subset of the data, comprising the “best items” in it. A Pareto-optimal representative is guaranteed to contain the best item of each possible ranking, but it can be a large portion of data. A much smaller representative can be found if we relax the requirement of including the best item for each user and instead just limit the users’ “regret.” Existing work defines regret as the loss in score by limiting consideration to the representative instead of the full dataset, for any chosen ranking function. However, the score is often not a meaningful number, and users may not understand its absolute value. Sometimes small ranges in score can include large fractions of the dataset. In contrast, users do understand the notion of rank ordering. Therefore, we consider items’ positions in the ranked list in defining the regret and propose the rank-regret representative as the minimal subset of the data containing at least one of the top-k of any possible ranking function. This problem is polynomial time solvable in two-dimensional space but is NP-hard on three or more dimensions. We design a suite of algorithms to fulfill different purposes, such as whether relaxation is permitted on k, the result size, or both, whether a distribution is known, whether theoretical guarantees or practical efficiency is important, and so on. Experiments on real datasets demonstrate that we can efficiently find small subsets with small rank-regrets. © 2022 Association for Computing Machinery.",computational geometry; epsilon-nets; Rank regret representatives; spatial databases; top-k search,Information retrieval; Large dataset; Pareto principle; Polynomial approximation; Absolute values; Data exploration; Epsilon-net; Minimal subset; Pareto-optimal; Rank ordering; Rank regret representative; Ranking functions; Spatial database; Top-k search; Computational geometry
Persistent Summaries,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137644441&doi=10.1145%2f3531053&partnerID=40&md5=1a97af66b19ddef295e2c6117c6284b5,"A persistent data structure, also known as a multiversion data structure in the database literature, is a data structure that preserves all its previous versions as it is updated over time. Every update (inserting, deleting, or changing a data record) to the data structure creates a new version, while all the versions are kept in the data structure so that any previous version can still be queried.Persistent data structures aim at recording all versions accurately, which results in a space requirement that is at least linear to the number of updates. In many of today's big data applications, in particular, for high-speed streaming data, the volume and velocity of the data are so high that we cannot afford to store everything. Therefore, streaming algorithms have received a lot of attention in the research community, which uses only sublinear space by sacrificing slightly on accuracy.All streaming algorithms work by maintaining a small data structure in memory, which is usually called a sketch, summary, or synopsis. The summary is updated upon the arrival of every element in the stream, thus it is ephemeral, meaning that it can only answer queries about the current status of the stream. In this article, we aim at designing persistent summaries, thereby giving streaming algorithms the ability to answer queries about the stream at any prior time. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",sketches; streaming algorithms; Summaries,Database systems; Query processing; Big data applications; High Speed; Multi-version; Research communities; Sketch; Space requirements; Streaming algorithm; Streaming data; Sublinear; Summary; Data structures
Answering (Unions of) Conjunctive Queries using Random Access and Random-Order Enumeration,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137938801&doi=10.1145%2f3531055&partnerID=40&md5=47d058b72e7483783a6f96b25726ce7c,"As data analytics becomes more crucial to digital systems, so grows the importance of characterizing the database queries that admit a more efficient evaluation. We consider the tractability yardstick of answer enumeration with a polylogarithmic delay after a linear-time preprocessing phase. Such an evaluation is obtained by constructing, in the preprocessing phase, a data structure that supports polylogarithmic-delay enumeration. In this article, we seek a structure that supports the more demanding task of a “random permutation”: polylogarithmic-delay enumeration in truly random order. Enumeration of this kind is required if downstream applications assume that the intermediate results are representative of the whole result set in a statistically meaningful manner. An even more demanding task is that of “random access”: polylogarithmic-time retrieval of an answer whose position is given. We establish that the free-connex acyclic CQs are tractable in all three senses: enumeration, random-order enumeration, and random access; and in the absence of self-joins, it follows from past results that every other CQ is intractable by each of the three (under some fine-grained complexity assumptions). However, the three yardsticks are separated in the case of a union of CQs (UCQ): while a union of free-connex acyclic CQs has a tractable enumeration, it may (provably) admit no random access. We identify a fragment of such UCQs where we can guarantee random access with polylogarithmic access time (and linear-time preprocessing) and a more general fragment where we can guarantee tractable random permutation. For general unions of free-connex acyclic CQs, we devise two algorithms with relaxed guarantees: one has logarithmic delay in expectation, and the other provides a permutation that is almost uniformly distributed. Finally, we present an implementation and an empirical study that show a considerable practical superiority of our random-order enumeration approach over state-of-the-art alternatives. © 2022 Association for Computing Machinery.",complexity; enumeration; Unions of conjunctive queries,Query languages; Query processing; Complexity; Data analytics; Digital system; Enumeration; Linear time; Polylogarithmic; Preprocessing phase; Random access; Random permutations; Union of conjunctive query; Data Analytics
Influence Maximization Revisited: Efficient Sampling with Bound Tightened,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137844512&doi=10.1145%2f3533817&partnerID=40&md5=34c9abb77cc45fed0fef9d6ef856768f,"Given a social network G with n nodes and m edges, a positive integer k, and a cascade model C, the influence maximization (IM) problem asks for k nodes in G such that the expected number of nodes influenced by the k nodes under cascade model C is maximized. The state-of-the-art approximate solutions run in O(k(n + m) log n/ϵ2) expected time while returning a (1 − 1/e − ϵ) approximate solution with at least 1 − 1/n probability. A key phase of these IM algorithms is the random reverse reachable (RR) set generation, and this phase significantly affects the efficiency and scalability of the state-of-the-art IM algorithms. In this article, we present a study on this key phase and propose an efficient random RR set generation algorithm under IC model. With the new algorithm, we show that the expected running time of existing IM algorithms under IC model can be improved to O(k · n log n/ϵ2), when for any node v, the total weight of its incoming edges is no larger than a constant. For the general IC model where the weights are skewed, we present a sampling algorithm SKIP. To the best of our knowledge, it is the first index-free algorithm that achieves the optimal time complexity of the sorted subset sampling problem. Moreover, existing approximate IM algorithms suffer from scalability issues in high influence networks where the size of random RR sets is usually quite large. We tackle this challenging issue by reducing the average size of random RR sets without sacrificing the approximation guarantee. The proposed solution is orders of magnitude faster than states of the art as shown in our experiment. Besides, we investigate the issues of forward propagation and derive its time complexity with our proposed subset sampling techniques. We also present a heuristic condition to indicate when the forward propagation approach should be utilized to estimate the expected influence of a given seed set. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Influence maximization; sampling,Complex networks; Integrated circuits; Approximate solution; Cascade modeling; Efficient sampling; Forward propagation; IC-Models; Influence maximizations; Maximization algorithm; Positive integers; Reachable set; State of the art; Scalability
Incremental Graph Computations: Doable and Undoable,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133494491&doi=10.1145%2f3500930&partnerID=40&md5=0b1834c3f4817b9ac4bc44ee56fdf085,"The incremental problem for a class of graph queries aims to compute, given a query , graph G, answers Q(G) to Q in G and updates δG to G as input, changes δO to output Q(G) such that Q(Gg•δG) = Q(G)g•δO. It is called bounded if its cost can be expressed as a polynomial function in the sizes of Q, δG and δO, which reduces the computations on possibly big G to small δG and δO. No matter how desirable, however, our first results are negative: For common graph queries such as traversal, connectivity, keyword search, pattern matching, and maximum cardinality matching, their incremental problems are unbounded. In light of the negative results, we propose two characterizations for the effectiveness of incremental graph computation: (a) localizable, if its cost is decided by small neighbors of nodes in δG instead of the entire G; and (b) bounded relative to a batch graph algorithm , if the cost is determined by the sizes of δG and changes to the affected area that is necessarily checked by any algorithms that incrementalize . We show that the incremental computations above are either localizable or relatively bounded by providing corresponding incremental algorithms. That is, we can either reduce the incremental computations on big graphs to small data, or incrementalize existing batch graph algorithms by minimizing unnecessary recomputation. Using real-life and synthetic data, we experimentally verify the effectiveness of our incremental algorithms.  © 2022 Association for Computing Machinery.",Incremntal graph computation; Relative boundedness,Graph theory; Search engines; Boundedness; Graph algorithms; Graph G; Graph queries; Incremental algorithm; Incremental computation; Incremntal graph computation; Polynomial functions; Query graph; Relative boundedness; Pattern matching
Optimal Joins Using Compressed Quadtrees,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130565823&doi=10.1145%2f3514231&partnerID=40&md5=9d383418f9159c6b52a6303cea00919e,"Worst-case optimal join algorithms have gained a lot of attention in the database literature. We now count several algorithms that are optimal in the worst case, and many of them have been implemented and validated in practice. However, the implementation of these algorithms often requires an enhanced indexing structure: To achieve optimality one either needs to build completely new indexes or must populate the database with several instantiations of indexes such as B-Trees. Either way, this means spending an extra amount of storage space that is typically one or two orders of magnitude more than what is required to store the raw data.We show that worst-case optimal algorithms can be obtained directly from a representation that regards the relations as point sets in variable-dimensional grids, without the need of any significant extra storage. Our representation is a compressed quadtreefor the static indexes and a quadtreebuilt on the fly that shares subtrees (which we dub a qdag) for intermediate results. We develop a compositional algorithm to process full join queries under this representation, which simulates navigation of the quadtreeof the output, and show that the running time of this algorithm is worst-case optimal in data complexity.We implement our index and compare it experimentally with state-of-The-Art alternatives. Our experiments show that our index uses even less space than what is needed to store the data in raw form (and replaces it) and one or two orders of magnitude less space than the other indexes. At the same time, our query algorithm is competitive in time, even sharply outperforming other indexes in various cases.Finally, we extend our framework to evaluate more expressive queries from relational algebra, including not only joins and intersections but also unions and negations. To obtain optimality on those more complex formulas, we introduce a lazy version of qdagswe dub lqdags, which allow us navigate over the quadtreerepresenting the output of a formula while only evaluating what is needed from its components. We show that the running time of our query algorithms on this extended set of operations is worst-case optimal under some constraints. Moving to full relational algebra, we also show that lqdagscan handle selections and projections. While worst-case optimality is no longer guaranteed, we introduce a partial materialization scheme that extends results from Deep and Koutris regarding compressed representation of query results.  © 2022 Association for Computing Machinery.",AGM bound; Compact data structures; Join algorithms; Quadtrees,Algebra; Optimization; Query processing; Trees (mathematics); AGM bound; Compact data structure; Indexing structures; Join algorithm; Optimality; Orders of magnitude; Quad trees; Query algorithms; Relational algebra; Running time; Digital storage
Conjunctive Regular Path Queries with Capture Groups,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133418627&doi=10.1145%2f3514230&partnerID=40&md5=56ec768a79604649d83cac8b78d3c3a2,"In practice, regular expressions are usually extended by so-called capture groups or capture variables, which allow to capture a subexpression by a variable that can be referenced in the regular expression in order to describe repetitions of subwords. We investigate how this concept could be used for pattern-based graph querying; i.e., we investigate conjunctive regular path queries (CRPQs) that are extended by capture variables.If capture variables are added to CRPQs in a completely unrestricted way, then Boolean evaluation becomes PSPACE-hard in data complexity, even for single-edge graph patterns. On the other hand, if capture variables do not occur under a Kleene star, then the data complexity drops to NL-completeness. Combined complexity is in EXPSPACE but drops to PSPACE-completeness if the depth (i.e., the nesting depth of capture variables) is bounded, and it drops to NP-completeness if the size of the images of capture variables is bounded by a constant (regardless of the depth or of whether capture variables occur under a Kleene star).In the application of regular expressions as string searching tools, references to capture variables only describe exact repetitions of subwords (i.e., they implement the equality relation on strings). Following recent developments in graph database research, we also study CRPQs with capture variables that describe arbitrary regular relations. We show that if the expressions have depth 0, or if the size of the images of capture variables is bounded by a constant, then we can allow arbitrary regular relations while staying in the same complexity bounds. We also investigate the problems of checking whether a given tuple is in the solution set and computing the whole solution set.On the conceptual side, we add capture variables to CRPQs in such a way that they can be defined in an expression on one arc of the graph pattern but also referenced in expressions on other arcs. Hence, they add to CRPQs the possibility to define inter-dependencies between different paths, which is a relevant feature of pattern-based graph querying.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Conjunctive regular path queries; Graph databases; Regular expressions with backreferences,Graph Databases; Pattern matching; Query processing; Stars; Conjunctive regular path query; Data complexity; Graph database; Graph patterns; Kleene star; Regular expression with backreference; Regular expressions; Regular path queries; Regular relations; Sub words; Drops
Mining Order-preserving Submatrices under Data Uncertainty: A Possible-world Approach and Efficient Approximation Methods,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133447101&doi=10.1145%2f3524915&partnerID=40&md5=230abf823d1b82258b2308312127be83,"Given a data matrix , a submatrix of is an order-preserving submatrix (OPSM) if there is a permutation of the columns of , under which the entry values of each row in are strictly increasing. OPSM mining is widely used in real-life applications such as identifying coexpressed genes and finding customers with similar preference. However, noise is ubiquitous in real data matrices due to variable experimental conditions and measurement errors, which makes conventional OPSM mining algorithms inapplicable. No previous work on OPSM has ever considered uncertain value intervals using the well-established possible world semantics.We establish two different definitions of significant OPSMs based on the possible world semantics: (1) expected support-based and (2) probabilistic frequentness-based. An optimized dynamic programming approach is proposed to compute the probability that a row supports a particular column permutation, with a closed-form formula derived to efficiently handle the special case of uniform value distribution and an accurate cubic spline approximation approach that works well with any uncertain value distributions. To efficiently check the probabilistic frequentness, several effective pruning rules are designed to efficiently prune insignificant OPSMs; two approximation techniques based on the Poisson and Gaussian distributions, respectively, are proposed for further speedup. These techniques are integrated into our two OPSM mining algorithms, based on prefix-projection and Apriori, respectively. We further parallelize our prefix-projection-based mining algorithm using PrefixFPM, a recently proposed framework for parallel frequent pattern mining, and we achieve a good speedup with the number of CPU cores. Extensive experiments on real microarray data demonstrate that the OPSMs found by our algorithms have a much higher quality than those found by existing approaches.  © 2022 Association for Computing Machinery.",Data mining; Expected support; OPSM; Order-preserving submatrices; Possible world semantics; Probabilistic frequentness,Dynamic programming; Matrix algebra; Poisson distribution; Semantics; Data matrix; Expected support; Mining algorithms; Mining order; Order-preserving submatrix; Possible world semantics; Probabilistic frequentness; Probabilistics; Value distribution; Data mining
Unified Route Planning for Shared Mobility: An Insertion-based Framework,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129874030&doi=10.1145%2f3488723&partnerID=40&md5=4a64152e35f9d78db1bf1d5961c299c7,"There has been a dramatic growth of shared mobility applications such as ride-sharing, food delivery, and crowdsourced parcel delivery. Shared mobility refers to transportation services that are shared among users, where a central issue is route planning. Given a set of workers and requests, route planning finds for each worker a route, i.e., a sequence of locations to pick up and drop off passengers/parcels that arrive from time to time, with different optimization objectives. Previous studies lack practicability due to their conflicted objectives and inefficiency in inserting a new request into a route, a basic operation called insertion. In addition, previous route planning solutions fail to exploit the appearance patterns of future requests hidden in historical data for optimization. In this paper, we present a unified formulation of route planning called URPSM. It has a well-defined parameterized objective function which eliminates the contradicted objectives in previous studies and enables flexible multi-objective route planning for shared mobility. We propose two insertion-based frameworks to solve the URPSM problem. The first is built upon the plain-insertion widely used in prior studies, which processes online requests only, whereas the second relies on a new insertion operator called prophet-insertion that handles both online and predicted requests. Novel dynamic programming algorithms are designed to accelerate both insertions to only linear time. Theoretical analysis shows that no online algorithm can have a constant competitive ratio for the URPSM problem under the competitive analysis model, yet our prophet-insertion-based framework can achieve a constant optimality ratio under the instance-optimality model. Extensive experimental results on real datasets show that our insertion-based solutions outperform the state-of-the-art algorithms in both effectiveness and efficiency by a large margin (e.g., up to 30 more effective in the objective and up to 20 faster).  © 2022 Association for Computing Machinery.",Dynamic programming; Insertion; Ride-sharing; Route planning,Large dataset; Transportation routes; Basic operation; Food delivery; Historical data; Insertion; Optimisations; Parcel delivery; Ride-sharing; Route planning; Transportation services; Workers'; Dynamic programming
The Space-Efficient Core of Vadalog,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129797967&doi=10.1145%2f3488720&partnerID=40&md5=692fe9d0aa4c79708a665077067c3e91,"Vadalog is a system for performing complex reasoning tasks such as those required in advanced knowledge graphs. The logical core of the underlying Vadalog language is the warded fragment of tuple-generating dependencies (TGDs). This formalism ensures tractable reasoning in data complexity, while a recent analysis focusing on a practical implementation led to the reasoning algorithm around which the Vadalog system is built. A fundamental question that has emerged in the context of Vadalog is whether we can limit the recursion allowed by wardedness in order to obtain a formalism that provides a convenient syntax for expressing useful recursive statements, and at the same time achieves space-efficiency. After analyzing several real-life examples of warded sets of TGDs provided by our industrial partners, as well as recent benchmarks, we observed that recursion is often used in a restricted way: the body of a TGD contains at most one atom whose predicate is mutually recursive with a predicate in the head. We show that this type of recursion, known as piece-wise linear in the Datalog literature, is the answer to our main question. We further show that piece-wise linear recursion alone, without the wardedness condition, is not enough as it leads to undecidability. We also study the relative expressiveness of the query languages based on (piece-wise linear) warded sets of TGDs. Finally, we give preliminary experimental evidence for the practical effect of piece-wise linearity on Vadalog.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",complexity; Datalog; expressive power; query answering; Reasoning; tuple-generating dependencies,Knowledge graph; Query processing; Complexity; Datalog; Expressive power; Piecewise linear; Query answering; Reasoning; Reasoning tasks; Recursions; Space efficient; Tuple-generating dependencies; Query languages
Height Optimized Tries,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129811058&doi=10.1145%2f3506692&partnerID=40&md5=078bd206031c8b86ef5d307588de68b3,"We present the Height Optimized Trie (HOT), a fast and space-efficient in-memory index structure. The core algorithmic idea of HOT is to dynamically vary the number of bits considered at each node, which enables a consistently high fanout and thereby good cache efficiency. For a fixed maximum node fanout, the overall tree height is minimal and its structure is deterministically defined. Multiple carefully engineered node implementations using SIMD instructions or lightweight compression schemes provide compactness and fast search and optimize HOT structures for different usage scenarios. Our experiments, which use a wide variety of workloads and data sets, show that HOT outperforms other state-of-the-art index structures for string keys both in terms of search performance and memory footprint, while being competitive for integer keys.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Height optimized trie; index structure; main memory; SIMD,Algorithmic ideas; Cache efficiency; Fan Out; Height optimized trie; Index structure; Main-memory; Memory index; SIMD; Space efficient; Tree height; Structure (composition)
Sampling a Near Neighbor in High Dimensions-Who is the Fairest of Them All?,2022,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129852069&doi=10.1145%2f3502867&partnerID=40&md5=5d62b3313851e803d84387c3cf2fad6f,"Similarity search is a fundamental algorithmic primitive, widely used in many computer science disciplines. Given a set of points S and a radius parameter r > 0, the r-near neighbor (r-NN) problem asks for a data structure that, given any query point q, returns a point p within distance at most r from q. In this paper, we study the r-NN problem in the light of individual fairness and providing equal opportunities: all points that are within distance r from the query should have the same probability to be returned. In the low-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the theoretically strongest approach to similarity search in high dimensions, does not provide such a fairness guarantee.In this work, we show that LSH based algorithms can be made fair, without a significant loss in efficiency. We propose several efficient data structures for the exact and approximate variants of the fair NN problem. Our approach works more generally for sampling uniformly from a sub-collection of sets of a given collection and can be used in a few other applications. We also develop a data structure for fair similarity search under inner product that requires nearly-linear space and exploits locality sensitive filters. The paper concludes with an experimental evaluation that highlights the unfairness of state-of-the-art NN data structures and shows the performance of our algorithms on real-world datasets.  © 2022 Association for Computing Machinery.",fairness; locality sensitive hashing; near neighbor; sampling; Similarity search,Infrared devices; Algorithmics; Equal opportunity; Fairness; Higher dimensions; Locality sensitive hashing; Low dimensional; Nearest-neighbour; Query points; Science disciplines; Similarity search; Data structures
Timely Reporting of Heavy Hitters Using External Memory,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121288710&doi=10.1145%2f3472392&partnerID=40&md5=2966f3e2028c4584d1292d2a92e86adb,"Given an input stream S of size N, a φ-heavy hitter is an item that occurs at least φN times in S. The problem of finding heavy-hitters is extensively studied in the database literature.We study a real-time heavy-hitters variant in which an element must be reported shortly after we see its T = φN-th occurrence (and hence it becomes a heavy hitter). We call this the Timely Event Detection (TED) Problem. The TED problem models the needs of many real-world monitoring systems, which demand accurate (i.e., no false negatives) and timely reporting of all events from large, high-speed streams with a low reporting threshold (high sensitivity).Like the classic heavy-hitters problem, solving the TED problem without false-positives requires large space (ω (N) words). Thus in-RAM heavy-hitters algorithms typically sacrifice accuracy (i.e., allow false positives), sensitivity, or timeliness (i.e., use multiple passes).We show how to adapt heavy-hitters algorithms to external memory to solve the TED problem on large high-speed streams while guaranteeing accuracy, sensitivity, and timeliness. Our data structures are limited only by I/O-bandwidth (not latency) and support a tunable tradeoff between reporting delay and I/O overhead. With a small bounded reporting delay, our algorithms incur only a logarithmic I/O overhead.We implement and validate our data structures empirically using the Firehose streaming benchmark. Multi-threaded versions of our structures can scale to process 11M observations per second before becoming CPU bound. In comparison, a naive adaptation of the standard heavy-hitters algorithm to external memory would be limited by the storage device's random I/O throughput, i.e., ≈100K observations per second.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Dictionary data structure; external-memory algorithms; streaming algorithms,Database systems; Random access storage; Real time systems; Virtual storage; Detection problems; Dictionary data structure; Events detection; External memory; External memory algorithms; False positive; Heavy-hitter; High Speed; Input streams; Streaming algorithm; Data structures
Balancing Expressiveness and Inexpressiveness in View Design,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121332277&doi=10.1145%2f3488370&partnerID=40&md5=79a1febbbbf40e95f97b099c2363d233,"We study the design of data publishing mechanisms that allow a collection of autonomous distributed data sources to collaborate to support queries. A common mechanism for data publishing is via views: Functions that expose derived data to users, usually specified as declarative queries. Our autonomy assumption is that the views must be on individual sources, but with the intention of supporting integrated queries. In deciding what data to expose to users, two considerations must be balanced. The views must be sufficiently expressive to support queries that users want to ask-the utility of the publishing mechanism. But there may also be some expressiveness restrictions. Here, we consider two restrictions, a minimal information requirement, saying that the views should reveal as little as possible while supporting the utility query, and a non-disclosure requirement, formalizing the need to prevent external users from computing information that data owners do not want revealed. We investigate the problem of designing views that satisfy both expressiveness and inexpressiveness requirements, for views in a restricted information systems-query languages (conjunctive queries), and for arbitrary views.  © 2021 Association for Computing Machinery.",determinacy; non-disclosure; Views,Search engines; Data publishing; Derived data; Determinacy; Distributed data sources; Information requirement; Minimal information; Non-disclosure; View; Query languages
On Directed Densest Subgraph Discovery,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121298710&doi=10.1145%2f3483940&partnerID=40&md5=af34d9bee727128f6c32089cad4b7022,"Given a directed graph G, the directed densest subgraph (DDS) problem refers to the finding of a subgraph from G, whose density is the highest among all the subgraphs of G. The DDS problem is fundamental to a wide range of applications, such as fraud detection, community mining, and graph compression. However, existing DDS solutions suffer from efficiency and scalability problems: On a 3,000-edge graph, it takes three days for one of the best exact algorithms to complete. In this article, we develop an efficient and scalable DDS solution. We introduce the notion of [x, y]-core, which is a dense subgraph for G, and show that the densest subgraph can be accurately located through the [x, y]-core with theoretical guarantees. Based on the [x, y]-core, we develop exact and approximation algorithms. We further study the problems of maintaining the DDS over dynamic directed graphs and finding the weighted DDS on weighted directed graphs, and we develop efficient non-trivial algorithms to solve these two problems by extending our DDS algorithms. We have performed an extensive evaluation of our approaches on 15 real large datasets. The results show that our proposed solutions are up to six orders of magnitude faster than the state-of-the-art.  © 2021 Association for Computing Machinery.",densest subgraph discovery; Directed graph,Approximation algorithms; Large dataset; Community mining; Dense sub-graphs; Dense subgraph discovery; Densest subgraph problems; Exact algorithms; Fraud detection; Graph compressions; Graph G; Scalability problems; Subgraphs; Directed graphs
A Formal Framework for Complex Event Recognition,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119392533&doi=10.1145%2f3485463&partnerID=40&md5=adf238a34171e63bdd96d1a53dd0825d,"Complex event recognition (CER) has emerged as the unifying field for technologies that require processing and correlating distributed data sources in real time. CER finds applications in diverse domains, which has resulted in a large number of proposals for expressing and processing complex events. Existing CER languages lack a clear semantics, however, which makes them hard to understand and generalize. Moreover, there are no general techniques for evaluating CER query languages with clear performance guarantees.In this article, we embark on the task of giving a rigorous and efficient framework to CER. We propose a formal language for specifying complex events, called complex event logic (CEL), that contains the main features used in the literature and has a denotational and compositional semantics. We also formalize the so-called selection strategies, which had only been presented as by-design extensions to existing frameworks. We give insight into the language design trade-offs regarding the strict sequencing operators of CEL and selection strategies.With a well-defined semantics at hand, we discuss how to efficiently process complex events by evaluating CEL formulas with unary filters. We start by introducing a formal computational model for CER, called complex event automata (CEA), and study how to compile CEL formulas with unary filters into CEA. Furthermore, we provide efficient algorithms for evaluating CEA over event streams using constant time per event followed by output-linear delay enumeration of the results.  © 2021 Association for Computing Machinery.",complex event processing; Complex event recognition; constant delay enumeration; streaming evaluation,Computation theory; Economic and social effects; Formal languages; Query languages; Complex event processing; Complex event recognition; Complex events; Constant delay enumeration; Constant delays; Event logic; Event Processing; Event recognition; Streaming evaluation; Semantics
Stream Data Cleaning under Speed and Acceleration Constraints,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116468345&doi=10.1145%2f3465740&partnerID=40&md5=41fb0eff44e17f3b4832d87b4878dc5d,"Stream data are often dirty, for example, owing to unreliable sensor reading or erroneous extraction of stock prices. Most stream data cleaning approaches employ a smoothing filter, which may seriously alter the data without preserving the original information. We argue that the cleaning should avoid changing those originally correct/clean data, a.k.a. the minimum modification rule in data cleaning. To capture the knowledge about what is clean, we consider the (widely existing) constraints on the speed and acceleration of data changes, such as fuel consumption per hour, daily limit of stock prices, or the top speed and acceleration of a car. Guided by these semantic constraints, in this article, we propose the constraint-based approach for cleaning stream data. It is notable that existing data repair techniques clean (a sequence of) data as a whole and fail to support stream computation. To this end, we have to relax the global optimum over the entire sequence to the local optimum in a window. Rather than the commonly observed NP-hardness of general data repairing problems, our major contributions include (1) polynomial time algorithm for global optimum, (2) linear time algorithm towards local optimum under an efficient median-based solution, and (3) experiments on real datasets demonstrate that our method can show significantly lower L1 error than the existing approaches such as smoother. © 2021 Copyright held by the owner/author(s).",acceleration constraints; Data repairing; speed constraints,Acceleration; Cleaning; Clustering algorithms; Data mining; Electronic trading; Financial markets; Polynomial approximation; Repair; Semantics; Acceleration constraint; Data cleaning; Data repairing; Globaloptimum; Local optima; Sensor readings; Smoothing filters; Speed constraints; Stock price; Stream data; Costs
Bag Query Containment and Information Theory,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116448356&doi=10.1145%2f3472391&partnerID=40&md5=18f093f1afd950c3484cd4a3170a90ea,"The query containment problem is a fundamental algorithmic problem in data management. While this problem is well understood under set semantics, it is by far less understood under bag semantics. In particular, it is a long-standing open question whether or not the conjunctive query containment problem under bag semantics is decidable. We unveil tight connections between information theory and the conjunctive query containment under bag semantics. These connections are established using information inequalities, which are considered to be the laws of information theory. Our first main result asserts that deciding the validity of a generalization of information inequalities is many-one equivalent to the restricted case of conjunctive query containment in which the containing query is acyclic; thus, either both these problems are decidable or both are undecidable. Our second main result identifies a new decidable case of the conjunctive query containment problem under bag semantics. Specifically, we give an exponential-time algorithm for conjunctive query containment under bag semantics, provided the containing query is chordal and admits a simple junction tree. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bag semantics; entropy; information theory; Query containment,Computability and decidability; Entropy; Information management; Semantics; Trees (mathematics); Algorithmic problems; Bag semantics; Conjunctive-query containment; Exponential time algorithm; Generalisation; Junction trees; Query containment; Query information; Set semantics; Simple junction; Information theory
SkinnerDB: Regret-bounded Query Evaluation via Reinforcement Learning,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116399263&doi=10.1145%2f3464389&partnerID=40&md5=c1c2702af9f9625e14f56ffd915a2da1,"SkinnerDB uses reinforcement learning for reliable join ordering, exploiting an adaptive processing engine with specialized join algorithms and data structures. It maintains no data statistics and uses no cost or cardinality models. Also, it uses no training workloads nor does it try to link the current query to seemingly similar queries in the past. Instead, it uses reinforcement learning to learn optimal join orders from scratch during the execution of the current query. To that purpose, it divides the execution of a query into many small time slices. Different join orders are tried in different time slices. SkinnerDB merges result tuples generated according to different join orders until a complete query result is obtained. By measuring execution progress per time slice, it identifies promising join orders as execution proceeds.Along with SkinnerDB, we introduce a new quality criterion for query execution strategies. We upper-bound expected execution cost regret, i.e., the expected amount of execution cost wasted due to sub-optimal join order choices. SkinnerDB features multiple execution strategies that are optimized for that criterion. Some of them can be executed on top of existing database systems. For maximal performance, we introduce a customized execution engine, facilitating fast join order switching via specialized multi-way join algorithms and tuple representations.We experimentally compare SkinnerDB's performance against various baselines, including MonetDB, Postgres, and adaptive processing methods. We consider various benchmarks, including the join order benchmark, TPC-H, and JCC-H, as well as benchmark variants with user-defined functions. Overall, the overheads of reliable join ordering are negligible compared to the performance impact of the occasional, catastrophic join order choice. © 2021 Copyright held by the owner/author(s).",adaptive processing; Query optimization; reinforcement learning,Database systems; Engines; Processing; Reinforcement learning; 'current; Adaptive processing; Execution costs; Execution strategies; Join algorithm; Join orderings; Performance; Queries optimization; Query evaluation; Time slice; Query processing
Error Bounded Line Simplification Algorithms for Trajectory Compression: An Experimental Evaluation,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116443807&doi=10.1145%2f3474373&partnerID=40&md5=e934ae884f6911fd1c8f53221400012d,"Nowadays, various sensors are collecting, storing, and transmitting tremendous trajectory data, and it is well known that the storage, network bandwidth, and computing resources could be heavily wasted if raw trajectory data is directly adopted. Line simplification algorithms are effective approaches to attacking this issue by compressing a trajectory to a set of continuous line segments, and are commonly used in practice. In this article, we first classify the error bounded line simplification algorithms into different categories and review each category of algorithms. We then study the data aging problem of line simplification algorithms and distance metrics from the views of aging friendliness and aging errors. Finally, we present a systematic experimental evaluation of representative error bounded line simplification algorithms, including both compression optimal and sub-optimal methods, in terms of commonly adopted perpendicular Euclidean, synchronous Euclidean, and direction-aware distances. Using real-life trajectory datasets, we systematically evaluate and analyze the performance (compression ratio, average error, running time, aging friendliness, and query friendliness) of error bounded line simplification algorithms with respect to distance metrics, trajectory sizes, and error bounds. Our study provides a full picture of error bounded line simplification algorithms, which leads to guidelines on how to choose appropriate algorithms and distance metrics for practical applications. © 2021 Association for Computing Machinery.",batch algorithms; line simplification; one-pass algorithms; online algorithms; Trajectory compression,Digital storage; Error analysis; Batch algorithms; Distance metrics; Experimental evaluation; Line simplification; On-line algorithms; One-pass; One-pass algorithm; Simplification algorithms; Trajectories datum; Trajectory compression; Trajectories
Graph Indexing for Efficient Evaluation of Label-constrained Reachability Queries,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108440470&doi=10.1145%2f3451159&partnerID=40&md5=657db93fe54c7274b715064067fca06c,"Given a directed edge labeled graph G, to check whether vertex v is reachable from vertex u under a label set S is to know if there is a path from u to v whose edge labels across the path are a subset of S. Such a query is referred to as a label-constrained reachability (LCR) query. In this article, we present a new approach to store a compressed transitive closure of G in the form of intervals over spanning trees (forests). The basic idea is to associate each vertex v with two sequences of some other vertices: one is used to check reachability from v to any other vertex, by using intervals, while the other is used to check reachability to v from any other vertex. We will show that such sequences are in general much shorter than the number of vertices in G. Extensive experiments have been conducted, which demonstrates that our method is much better than all the previous methods for this problem in all the important aspects, including index construction times, index sizes, and query times. © 2021 ACM.",Label constraint reachability; Labeled directed graphs; Recursive graph decomposition; Spanning trees; Tree labeling,Directed graphs; Directed edges; Graph indexing; Index construction; Labeled graphs; New approaches; Reachability queries; Spanning tree; Transitive closure; Query processing
Optimizing One-time and Continuous Subgraph Queries using Worst-case Optimal Joins,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108428467&doi=10.1145%2f3446980&partnerID=40&md5=ef34f351fd5ac226734aec36b67ea966,"We study the problem of optimizing one-time and continuous subgraph queries using the new worst-case optimal join plans. Worst-case optimal plans evaluate queries by matching one query vertex at a time using multiway intersections. The core problem in optimizing worst-case optimal plans is to pick an ordering of the query vertices to match. We make two main contributions: 1. A cost-based dynamic programming optimizer for one-time queries that (i) picks efficient query vertex orderings for worst-case optimal plans and (ii) generates hybrid plans that mix traditional binary joins with worst-case optimal style multiway intersections. In addition to our optimizer, we describe an adaptive technique that changes the query vertex orderings of the worst-case optimal subplans during query execution for more efficient query evaluation. The plan space of our one-time optimizer contains plans that are not in the plan spaces based on tree decompositions from prior work. 2. A cost-based greedy optimizer for continuous queries that builds on the delta subgraph query framework. Given a set of continuous queries, our optimizer decomposes these queries into multiple delta subgraph queries, picks a plan for each delta query, and generates a single combined plan that evaluates all of the queries. Our combined plans share computations across operators of the plans for the delta queries if the operators perform the same intersections. To increase the amount of computation shared, we describe an additional optimization that shares partial intersections across operators. Our optimizers use a new cost metric for worst-case optimal plans called intersection-cost. When generating hybrid plans, our dynamic programming optimizer for one-time queries combines intersection-cost with the cost of binary joins. We demonstrate the effectiveness of our plans, adaptive technique, and partial intersection sharing optimization through extensive experiments. Our optimizers are integrated into GraphflowDB. © 2021 ACM.",generic join; Subgraph queries; worst-case optimal joins,Database systems; Information systems; Adaptive technique; Continuous queries; Core problems; Efficient query evaluation; Query execution; Query framework; Tree decomposition; Vertex ordering; Dynamic programming
On the Enumeration Complexity of Unions of Conjunctive Queries,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108441362&doi=10.1145%2f3450263&partnerID=40&md5=58000db7c29c2d3a5797b94265626b7f,"We study the enumeration complexity of Unions of Conjunctive Queries (UCQs). We aim o identify the UCQs that are tractable in the sense that the answer tuples can be enumerated with a linear preprocessing phase and a constant delay between every successive tuples. It has been established that, in the absence of self-joins and under conventional complexity assumptions, the CQs that admit such an evaluation are precisely the free-connex ones. A union of tractable CQs is always tractable. We generalize the notion of free-connexity from CQs to UCQs, thus showing that some unions containing intractable CQs are, in fact, tractable. Interestingly, some unions consisting of only intractable CQs are tractable too. We show how to use the techniques presented in this article also in settings where the database contains cardinality dependencies (including functional dependencies and key constraints) or when the UCQs contain disequalities. The question of finding a full characterization of the tractability of UCQs remains open. Nevertheless, we prove that, for several classes of queries, free-connexity fully captures the tractable UCQs. © 2021 ACM.",complexity; constant delay; enumeration; Unions of conjunctive queries,Information systems; Cardinalities; Complexity assumptions; Conjunctive queries; Constant delays; Functional dependency; Key constraints; Preprocessing phase; Self-join; Database systems
Embedded Functional Dependencies and Data-completeness Tailored Database Design,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108423539&doi=10.1145%2f3450518&partnerID=40&md5=bd38b04d03d34999dd484127817f5d35,"We establish a principled schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing redundant data value occurrences that may cause problems with processing data that meets the requirements. We establish axiomatic, algorithmic, and logical foundations for reasoning about embedded functional dependencies. These foundations enable us to introduce generalizations of Boyce-Codd and Third normal forms that avoid processing difficulties of any application data, or minimize these difficulties across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements, and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate the effectiveness of our framework for the acquisition of the constraints, the schema design process, and the performance of the schema designs in terms of updates and join queries. © 2021 ACM.",Boyce-Codd normal form; database design; decomposition; functional dependency; key; missing value; normal form; redundancy; synthesis; third normal form; updates,Benchmarking; Design; Application data; Data completeness; Database design; Fit for purpose; Functional dependency; Generalized normal forms; Integrity requirements; Logical foundations; Data handling
Scotty,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104179948&doi=10.1145%2f3433675&partnerID=40&md5=21c6dbd59e48a3a7fc9e457f0cafaff5,"Window aggregation is a core operation in data stream processing. Existing aggregation techniques focus on reducing latency, eliminating redundant computations, or minimizing memory usage. However, each technique operates under different assumptions with respect to workload characteristics, such as properties of aggregation functions (e.g., invertible, associative), window types (e.g., sliding, sessions), windowing measures (e.g., time-or count-based), and stream (dis)order. In this article, we present Scotty, an efficient and general open-source operator for sliding-window aggregation in stream processing systems, such as Apache Flink, Apache Beam, Apache Samza, Apache Kafka, Apache Spark, and Apache Storm. One can easily extend Scotty with user-defined aggregation functions and window types. Scotty implements the concept of general stream slicing and derives workload characteristics from aggregation queries to improve performance without sacrificing its general applicability. We provide an in-depth view on the algorithms of the general stream slicing approach. Our experiments show that Scotty outperforms alternative solutions. © 2021 ACM.",aggregate sharing; aggregation; Apache Beam; Apache Flink; Apache Kafka Streams; Apache Samza; Apache Spark; Apache Storm; open-source; Scotty; session window; sliding-window; stream processing; tumbling window; Window,Open systems; Aggregation functions; Aggregation queries; Aggregation techniques; Alternative solutions; Data stream processing; Redundant computation; Stream processing systems; Workload characteristics; Data streams
An Empirical Study of Moment Estimators for Quantile Approximation,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104191477&doi=10.1145%2f3442337&partnerID=40&md5=5f71b9ea9355b90472965e38864e26ee,"We empirically evaluate lightweight moment estimators for the single-pass quantile approximation problem, including maximum entropy methods and orthogonal series with Fourier, Cosine, Legendre, Chebyshev and Hermite basis functions. We show how to apply stable summation formulas to offset numerical precision issues for higher-order moments, leading to reliable single-pass moment estimators up to order 15. Additionally, we provide an algorithm for GPU-Accelerated quantile approximation based on parallel tree reduction. Experiments evaluate the accuracy and runtime of moment estimators against the state-of-The-Art KLL quantile estimator on 14,072 real-world datasets drawn from the OpenML database. Our analysis highlights the effectiveness of variants of moment-based quantile approximation for highly space efficient summaries: Their average performance using as few as five sample moments can approach the performance of a KLL sketch containing 500 elements. Experiments also illustrate the difficulty of applying the method reliably and showcases which moment-based approximations can be expected to fail or perform poorly. © 2021 ACM.",data streams; Density estimation; quantiles,Approximation algorithms; Fourier analysis; Fourier series; Maximum entropy methods; Orthogonal functions; Approximation problems; Hermite basis functions; Higher order moments; Moment based approximations; Numerical precision; Quantile estimators; Real-world datasets; Summation formula; Trees (mathematics)
Constant-Delay Enumeration for Nondeterministic Document Spanners,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104186572&doi=10.1145%2f3436487&partnerID=40&md5=f1afd03e2d672187b3d5affb408b1aa7,"We consider the information extraction framework known as document spanners and study the problem of efficiently computing the results of the extraction from an input document, where the extraction task is described as a sequential variable-set automaton (VA). We pose this problem in the setting of enumeration algorithms, where we can first run a preprocessing phase and must then produce the results with a small delay between any two consecutive results. Our goal is to have an algorithm that is tractable in combined complexity, i.e., in the sizes of the input document and the VA, while ensuring the best possible data complexity bounds in the input document size, i.e., constant delay in the document size. Several recent works at PODS'18 proposed such algorithms but with linear delay in the document size or with an exponential dependency in size of the (generally nondeterministic) input VA. In particular, Florenzano et al. suggest that our desired runtime guarantees cannot be met for general sequential VAs. We refute this and show that, given a nondeterministic sequential VA and an input document, we can enumerate the mappings of the VA on the document with the following bounds: The preprocessing is linear in the document size and polynomial in the size of the VA, and the delay is independent of the document and polynomial in the size of the VA. The resulting algorithm thus achieves tractability in combined complexity and the best possible data complexity bounds. Moreover, it is rather easy to describe, particularly for the restricted case of so-called extended VAs. Finally, we evaluate our algorithm empirically using a prototype implementation. © 2021 ACM.",constant delay enumeration; Documents spanners; information extraction,Information systems; Combined complexity; Constant delays; Data complexity; Enumeration algorithms; Preprocessing phase; Prototype implementations; Runtimes; Database systems
Evaluation of Machine Learning Algorithms in Predicting the Next SQL Query from the Future,2021,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104204807&doi=10.1145%2f3442338&partnerID=40&md5=2fd8ead1ef14a9910c073b409b453c6b,"Prediction of the next SQL query from the user, given her sequence of queries until the current timestep, during an ongoing interaction session of the user with the database, can help in speculative query processing and increased interactivity. While existing machine learning-(ML) based approaches use recommender systems to suggest relevant queries to a user, there has been no exhaustive study on applying temporal predictors to predict the next user issued query. In this work, we experimentally compare ML algorithms in predicting the immediate next future query in an interaction workload, given the current user query or the sequence of queries in a user session thus far. As a part of this, we propose the adaptation of two powerful temporal predictors: (a) Recurrent Neural Networks (RNNs) and (b) a Reinforcement Learning approach called Q-Learning that uses Markov Decision Processes. We represent each query as a comprehensive set of fragment embeddings that not only captures the SQL operators, attributes, and relations but also the arithmetic comparison operators and constants that occur in the query. Our experiments on two real-world datasets show the effectiveness of temporal predictors against the baseline recommender systems in predicting the structural fragments in a query w.r.t. both quality and time. Besides showing that RNNs can be used to synthesize novel queries, we find that exact Q-Learning outperforms RNNs despite predicting the next query entirely from the historical query logs. © 2021 ACM.",Query prediction; recommender systems; recurrent neural networks; schema-Aware SQL embeddings,Forecasting; Learning systems; Markov processes; Query languages; Query processing; Real time systems; Recommender systems; Recurrent neural networks; Reinforcement learning; Comparison operators; Historical queries; Markov Decision Processes; Real-world datasets; Recurrent neural network (RNNs); Reinforcement learning approach; Relevant query; Structural fragments; Learning algorithms
Flexible Skylines: Dominance for Arbitrary Sets of Monotone Functions,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097907120&doi=10.1145%2f3406113&partnerID=40&md5=13344692a240924607fdb15ef5916f0a,"Skyline and ranking queries are two popular, alternative ways of discovering interesting data in large datasets. Skyline queries are simple to specify, as they just return the set of all non-dominated tuples, thereby providing an overall view of potentially interesting results. However, they are not equipped with any means to accommodate user preferences or to control the cardinality of the result set. Ranking queries adopt, instead, a specific scoring function to rank tuples, and can easily control the output size. While specifying a scoring function allows one to give different importance to different attributes by means of, e.g., weight parameters, choosing the ""right""weights to use is known to be a hard problem. In this article, we embrace the skyline approach by introducing an original framework able to capture user preferences by means of constraints on the weights used in a scoring function, which is typically much easier than specifying precise weight values. To this end, we introduce the novel concept of F-dominance, i.e., dominance with respect to a family of scoring functions F: A tuple t is said to F-dominate tuple s when t is always better than or equal to s according to all the functions in F. Based on F-dominance, we present two flexible skyline (F-skyline) operators, both returning a subset of the skyline: nd, characterizing the set of non-F-dominated tuples; po, referring to the tuples that are also potentially optimal, i.e., best according to some function in F. While nd and po coincide and reduce to the traditional skyline when F is the family of all monotone scoring functions, their behaviors differ when subsets thereof are considered. We discuss the formal properties of these new operators, show how to implement them efficiently, and evaluate them on both synthetic and real datasets.  © 2020 ACM.",monotone functions; ranking queries; Skyline queries,Database systems; Information systems; Arbitrary sets; Formal properties; Large datasets; Monotone functions; Ranking queries; Real data sets; Scoring functions; Weight parameters; Large dataset
Functional Aggregate Queries with Additive Inequalities,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097900867&doi=10.1145%2f3426865&partnerID=40&md5=6df477fb86fa62246c7f49ead8cc4218,"Motivated by fundamental applications in databases and relational machine learning, we formulate and study the problem of answering functional aggregate queries (FAQ) in which some of the input factors are defined by a collection of additive inequalities between variables. We refer to these queries as FAQ-AI for short. To answer FAQ-AI in the Boolean semiring, we define relaxed tree decompositions and relaxed submodular and fractional hypertree width parameters. We show that an extension of the InsideOut algorithm using Chazelle's geometric data structure for solving the semigroup range search problem can answer Boolean FAQ-AI in time given by these new width parameters. This new algorithm achieves lower complexity than known solutions for FAQ-AI. It also recovers some known results in database query answering. Our second contribution is a relaxation of the set of polymatroids that gives rise to the counting version of the submodular width, denoted by subw. This new width is sandwiched between the submodular and the fractional hypertree widths. Any FAQ and FAQ-AI over one semiring can be answered in time proportional to subw and respectively to the relaxed version of subw. We present three applications of our FAQ-AI framework to relational machine learning: k-means clustering, training linear support vector machines, and training models using non-polynomial loss. These optimization problems can be solved over a database asymptotically faster than computing the join of the database relations.  © 2020 ACM.",Functional aggregate queries; in-database machine learning; inequality joins; non-polynomial loss,Additives; Computational complexity; Database systems; K-means clustering; Query processing; Support vector machines; Database relations; Fractional hypertree widths; Functional aggregates; Geometric data structures; Linear Support Vector Machines; Optimization problems; Range search problems; Tree decomposition; Learning systems
Incremental and Approximate Computations for Accelerating Deep CNN Inference,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091107504&doi=10.1145%2f3397461&partnerID=40&md5=c8cf9b27eaaf9d695d6479c55fc67cea,"Deep learning now offers state-of-The-Art accuracy for many prediction tasks. A form of deep learning called deep convolutional neural networks (CNNs) are especially popular on image, video, and time series data. Due to its high computational cost, CNN inference is often a bottleneck in analytics tasks on such data. Thus, a lot of work in the computer architecture, systems, and compilers communities study how to make CNN inference faster. In this work, we show that by elevating the abstraction level and re-imagining CNN inference as queries, we can bring to bear database-style query optimization techniques to improve CNN inference efficiency. We focus on tasks that perform CNN inference repeatedly on inputs that are only slightly different. We identify two popular CNN tasks with this behavior: occlusion-based explanations (OBE) and object recognition in videos (ORV). OBE is a popular method for ""explaining""CNN predictions. It outputs a heatmap over the input to show which regions (e.g., image pixels) mattered most for a given prediction. It leads to many re-inference requests on locally modified inputs. ORV uses CNNs to identify and track objects across video frames. It also leads to many re-inference requests. We cast such tasks in a unified manner as a novel instance of the incremental view maintenance problem and create a comprehensive algebraic framework for incremental CNN inference that reduces computational costs. We produce materialized views of features produced inside a CNN and connect them with a novel multi-query optimization scheme for CNN re-inference. Finally, we also devise novel OBE-specific and ORV-specific approximate inference optimizations exploiting their semantics. We prototype our ideas in Python to create a tool called Krypton that supports both CPUs and GPUs. Experiments with real data and CNNs show that Krypton reduces runtimes by up to 5× (respectively, 35×) to produce exact (respectively, high-quality approximate) results without raising resource requirements.  © 2020 ACM.",convolutional neural network explainability; Incremental view maintenance; multi-query optimization; systems for machine learning,Deep learning; Deep neural networks; Forecasting; Krypton; Object recognition; Program processors; Query languages; Query processing; Semantics; Software prototyping; Algebraic framework; Approximate computation; Approximate inference; Computational costs; Incremental view maintenance; Inference efficiency; Multiquery optimization; Resource requirements; Convolutional neural networks
MobilityDB: A Mobility Database Based on PostgreSQL and PostGIS,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097256875&doi=10.1145%2f3406534&partnerID=40&md5=3e20690e1e9b6018a553647bc8ed6ffc,"Despite two decades of research in moving object databases and a few research prototypes that have been proposed, there is not yet a mainstream system targeted for industrial use. In this article, we present MobilityDB, a moving object database that extends the type system of PostgreSQL and PostGIS with abstract data types for representing moving object data. The types are fully integrated into the platform to reuse its powerful data management features. Furthermore, MobilityDB builds on existing operations, indexing, aggregation, and optimization framework. This is all made accessible via the SQL query interface.  © 2020 Owner/Author.",mobility data management; Moving object databases; spatiotemporal data management; SQL,Industrial research; Information management; Object-oriented databases; Query processing; Fully integrated; Industrial use; Moving object database; Moving objects; Optimization framework; PostgreSQL; Research prototype; Type systems; Abstract data types
Discovering graph functional dependencies,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092408590&doi=10.1145%2f3397198&partnerID=40&md5=50c8da0ec4af1ea3f4e84de3ff8a0b0f,"This article studies discovery of Graph Functional Dependencies (GFDs), a class of functional dependencies defined on graphs. We investigate the fixed-parameter tractability of three fundamental problems related to GFD discovery. We show that the implication and satisfiability problems are fixed-parameter tractable, but the validation problem is co-W[1]-hard in general. We introduce notions of reduced GFDs and their topological support, and formalize the discovery problem for GFDs. We develop algorithms for discovering GFDs and computing their covers. Moreover, we show that GFD discovery is feasible over large-scale graphs, by providing parallel scalable algorithms that guarantee to reduce running time when more processors are used. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms.  © 2020 ACM.",Discovery; Functional dependencies; Graphs; Implication; Validation,Database systems; Information systems; Fixed-parameter tractability; Functional dependency; Running time; Satisfiability problems; Scalable algorithms; Synthetic data; Validation problem; Graph algorithms
Synthesis of incremental linear algebra programs,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092364799&doi=10.1145%2f3385398&partnerID=40&md5=98a0a9be95e43fb97f4f9f1a602dee86,"This article targets the Incremental View Maintenance (IVM) of sophisticated analytics (such as statistical models, machine learning programs, and graph algorithms) expressed as linear algebra programs. We present LAGO, a unified framework for linear algebra that automatically synthesizes efficient incremental trigger programs, thereby freeing the user from error-prone manual derivations, performance tuning, and low-level implementation details. The key technique underlying our framework is abstract interpretation, which is used to infer various properties of analytical programs. These properties give the reasoning power required for the automatic synthesis of efficient incremental triggers. We evaluate the effectiveness of our framework on a wide range of applications from regression models to graph computations.  © 2020 ACM.",Abstract interpretation; Compilation; Domain-specific languages; Incremental linear algebra; Incremental view maintenance (IVM); Materialized views,Graph algorithms; Linear algebra; Regression analysis; Abstract interpretations; Automatic synthesis; Error prones; Incremental view maintenance; Performance tuning; Regression model; Unified framework; Machine learning
Editorial,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092335025&doi=10.1145%2f3417730&partnerID=40&md5=80b242527a4daf311e5f0db16c637fdb,[No abstract available],,
Maintaining triangle queries under updates,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092385375&doi=10.1145%2f3396375&partnerID=40&md5=21915eb406c6681206d5fe22a5d556b7,"We consider the problem of incrementally maintaining the triangle queries with arbitrary free variables under single-tuple updates to the input relations. We introduce an approach called IVMϵ that exhibits a trade-off between the update time, the space, and the delay for the enumeration of the query result, such that the update time ranges from the square root to linear in the database size while the delay ranges from constant to linear time. IVMϵ achieves Pareto worst-case optimality in the update-delay space conditioned on the Online Matrix-Vector Multiplication conjecture. It is strongly Pareto optimal for the triangle queries with no or three free variables and weakly Pareto optimal for the remaining triangle queries with one or two free variables. IVMϵ recovers prior work such as the suboptimal classical view maintenance approach that uses delta query processing and the worst-case optimal approach that computes all triangles in a static database.  © 2020 ACM.",Amortized update time; Complexity trade-off; Enumeration delay; Incremental view maintenance; Pareto worst-case optimality,Economic and social effects; Query processing; Vector spaces; Database size; Delay spaces; Free variable; Online matrix; Optimal approaches; Pareto-optimal; Query results; View maintenance; Pareto principle
Efficient discovery of matching dependencies,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092395161&doi=10.1145%2f3392778&partnerID=40&md5=84cb2589fc9ff4bc637f9e02fe1dad90,"Matching dependencies (MDS) are data profiling results that are often used for data integration, data cleaning, and entity matching. They are a generalization of functional dependencies (FDs) matching similar rather than same elements. As their discovery is very difficult, existing profiling algorithms find either only small subsets of all MDS or their scope is limited to only small datasets. We focus on the efficient discovery of all interesting MDS in real-world datasets. For this purpose, we propose HyMD, a novel MD discovery algorithm that finds all minimal, non-trivial MDS within given similarity boundaries. The algorithm extracts the exact similarity thresholds for the individual MDS from the data instead of using predefined similarity thresholds. For this reason, it is the first approach to solve the MD discovery problem in an exact and truly complete way. If needed, the algorithm can, however, enforce certain properties on the reported MDS, such as disjointness and minimum support, to focus the discovery on such results that are actually required by downstream use cases. HyMD is technically a hybrid approach that combines the two most popular dependency discovery strategies in related work: lattice traversal and inference from record pairs. Despite the additional effort of finding exact similarity thresholds for all MD candidates, the algorithm is still able to efficiently process large datasets, e.g., datasets larger than 3 GB.  © 2020 ACM.",Data matching; Data profiling; Dependency discovery; Entity resolution; Functional dependencies; Matching dependencies; Similarity measures,Large dataset; Dependency discovery; Discovery algorithm; Entity matching; Functional dependency; Matching dependencies; Minimum support; Real-world datasets; Similarity threshold; Data integration
"Packing r-trees with space-filling curves: Theoretical optimality, empirical efficiency, and bulk-loading parallelizability",2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091111015&doi=10.1145%2f3397506&partnerID=40&md5=48819238ba65ed9973bb685d423e1eb8,"The massive amount of data and large variety of data distributions in the big data era call for access methods that are efficient in both query processing and index management, and over both practical and worst-case workloads. To address this need, we revisit two classic multidimensional access methods - the R-tree and the space-filling curve. We propose a novel R-tree packing strategy based on space-filling curves. This strategy produces R-trees with an asymptotically optimal I/O complexity for window queries in the worst case. Experiments show that our R-trees are highly efficient in querying both real and synthetic data of different distributions. The proposed strategy is also simple to parallelize, since it relies only on sorting. We propose a parallel algorithm for R-tree bulk-loading based on the proposed packing strategy and analyze its performance under the massively parallel communication model. To handle dynamic data updates, we further propose index update algorithms that process data insertions and deletions without compromising the optimal query I/O complexity. Experimental results confirm the effectiveness and efficiency of the proposed R-tree bulk-loading and updating algorithms over large data sets.  © 2020 ACM.",Logarithmic method; R-trees; Rank space; Window queries,Decision trees; Efficiency; Filling; Forestry; Trees (mathematics); Asymptotically optimal; Different distributions; Effectiveness and efficiencies; Empirical efficiency; Massively parallels; Multidimensional access methods; Space-filling curve; Updating algorithm; Loading
On the language of nested tuple generating dependencies,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091955266&doi=10.1145%2f3369554&partnerID=40&md5=4407fef6621e95b6cbdba81f9334f4f8,"During the past 15 years, schema mappings have been extensively used in formalizing and studying such critical data interoperability tasks as data exchange and data integration. Much of the work has focused on GLAV mappings, i.e., schema mappings specified by source-To-Target tuple-generating dependencies (s-T tgds), and on schema mappings specified by second-order tgds (SO tgds), which constitute the closure of GLAV mappings under composition. In addition, nested GLAV mappings have also been considered, i.e., schema mappings specified by nested tgds, which have expressive power intermediate between s-T tgds and SO tgds. Even though nested GLAV mappings have been used in data exchange systems, such as IBM's Clio, no systematic investigation of this class of schema mappings has been carried out so far. In this article, we embark on such an investigation by focusing on the basic reasoning tasks, algorithmic problems, and structural properties of nested GLAV mappings. One of our main results is the decidability of the implication problem for nested tgds. We also analyze the structure of the core of universal solutions with respect to nested GLAV mappings and develop useful tools for telling apart SO tgds from nested tgds. By discovering deeper structural properties of nested GLAV mappings, we show that also the following problem is decidable: Given a nested GLAV mapping, is it logically equivalent to a GLAV mapping?  © 2020 ACM.",data exchange; data integration; nested dependencies; Schema mappings; second-order dependencies,Computability and decidability; Data integration; Electronic data interchange; Structural properties; Algorithmic problems; Data exchange system; Expressive power; Following problem; Implication problem; Schema mappings; Tuple-generating dependencies; Universal solutions; Mapping
Adaptive asynchronous parallelization of graph algorithms,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091916802&doi=10.1145%2f3397491&partnerID=40&md5=73c0379b9079f05f66df127c5599a469,"This article proposes an Adaptive Asynchronous Parallel (AAP) model for graph computations. As opposed to Bulk Synchronous Parallel (BSP) and Asynchronous Parallel (AP) models, AAP reduces both stragglers and stale computations by dynamically adjusting relative progress of workers. We show that BSP, AP, and Stale Synchronous Parallel model (SSP) are special cases of AAP. Better yet, AAP optimizes parallel processing by adaptively switching among these models at different stages of a single execution. Moreover, employing the programming model of GRAPE, AAP aims to parallelize existing sequential algorithms based on simultaneous fixpoint computation with partial and incremental evaluation. Under a monotone condition, AAP guarantees to converge at correct answers if the sequential algorithms are correct. Furthermore, we show that AAP can optimally simulate MapReduce, PRAM, BSP, AP, and SSP. Using real-life and synthetic graphs, we experimentally verify that AAP outperforms BSP, AP, and SSP for a variety of graph computations.  © 2020 ACM.",convergence; Graph computations; parallel graph query engines; parallelizing sequential algorithms; simulation,Database systems; Information systems; Asynchronous parallel; Bulk synchronous parallel; Fixpoint computations; Incremental evaluation; Parallel processing; Programming models; Sequential algorithm; Synthetic graphs; Graph algorithms
Learning models over relational data using sparse tensors and functional dependencies,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091926959&doi=10.1145%2f3375661&partnerID=40&md5=bf338bf73b26ea10499de4e454bd30a1,"Integrated solutions for analytics over relational databases are of great practical importance as they avoid the costly repeated loop data scientists have to deal with on a daily basis: select features from data residing in relational databases using feature extraction queries involving joins, projections, and aggregations; export the training dataset defined by such queries; convert this dataset into the format of an external learning tool; and train the desired model using this tool. These integrated solutions are also a fertile ground of theoretically fundamental and challenging problems at the intersection of relational and statistical data models. This article introduces a unified framework for training and evaluating a class of statistical learning models over relational databases. This class includes ridge linear regression, polynomial regression, factorization machines, and principal component analysis. We show that, by synergizing key tools from database theory such as schema information, query structure, functional dependencies, recent advances in query evaluation algorithms, and from linear algebra such as tensor and matrix operations, one can formulate relational analytics problems and design efficient (query and data) structure-Aware algorithms to solve them. This theoretical development informed the design and implementation of the AC/DC system for structure-Aware learning. We benchmark the performance of AC/DC against R, MADlib, libFM, and TensorFlow. For typical retail forecasting and advertisement planning applications, AC/DC can learn polynomial regression models and factorization machines with at least the same accuracy as its competitors and up to three orders of magnitude faster than its competitors whenever they do not run out of memory, exceed 24-hour timeout, or encounter internal design limitations.  © 2020 ACM.",functional aggregate queries; functional dependencies; In-database analytics; model reparameterization; tensors,Benchmarking; Factorization; Feature extraction; Learning systems; Query processing; Relational database systems; Structural design; Tensors; Design and implementations; Factorization machines; Functional dependency; Polynomial regression models; Query evaluation algorithm; Statistical data model; Theoretical development; Three orders of magnitude; Polynomial regression
Catching numeric inconsistencies in graphs,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091944580&doi=10.1145%2f3385031&partnerID=40&md5=aabe34f67475885b4eae5f3faa567db3,"Numeric inconsistencies are common in real-life knowledge bases and social networks. To catch such errors, we extend graph functional dependencies with linear arithmetic expressions and built-in comparison predicates, referred to as numeric graph dependencies (NGDs). We study fundamental problems for NGDs. We show that their satisfiability, implication, and validation problems are ςp2-complete, p2-complete, and coNP-complete, respectively. However, if we allow non-linear arithmetic expressions, even of degree at most 2, the satisfiability and implication problems become undecidable. In other words, NGDs strike a balance between expressivity and complexity. To make practical use of NGDs, we develop an incremental algorithm IncDect to detect errors in a graph G using NGDs in response to updates ""G to G. We show that the incremental validation problem is coNP-complete. Nonetheless, algorithm IncDect is localizable, i.e., its cost is determined by small neighbors of nodes in ""G instead of the entire G. Moreover, we parallelize IncDect such that it guarantees to reduce running time with the increase of processors. In addition, to strike a balance between the efficiency and accuracy, we also develop polynomial-Time parallel algorithms for detection and incremental detection of top-ranked inconsistencies. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the algorithms.  © 2020 ACM.",graph dependencies; incremental validation; Numeric errors,Efficiency; Formal logic; Polynomial approximation; Functional dependency; Implication problem; Incremental algorithm; Incremental validation; Knowledge basis; Linear arithmetic; Synthetic graphs; Validation problem; Graph algorithms
Succinct range filters,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091907675&doi=10.1145%2f3375660&partnerID=40&md5=3c8e0607d9bbad28c21fa8c7b898bd86,"We present the Succinct Range Filter (SuRF), a fast and compact data structure for approximate membership tests. Unlike traditional Bloom filters, SuRF supports both single-key lookups and common range queries: open-range queries, closed-range queries, and range counts. SuRF is based on a new data structure called the Fast Succinct Trie (FST) that matches the point and range query performance of state-of-The-Art order-preserving indexes, while consuming only 10 bits per trie node. The false-positive rates in SuRF for both point and range queries are tunable to satisfy different application needs. We evaluate SuRF in RocksDB as a replacement for its Bloom filters to reduce I/O by filtering requests before they access on-disk data structures. Our experiments on a 100-GB dataset show that replacing RocksDB's Bloom filters with SuRFs speeds up open-seek (without upper-bound) and closed-seek (with upper-bound) queries by up to 1.5× and 5× with a modest cost on the worst-case (all-missing) point query throughput due to slightly higher false-positive rate.  © 2020 ACM.",LSM-Tree; Range filter; succinct data structure; trie,Database systems; Information systems; Bloom filters; Compact data structure; False positive rates; Lookups; Order preserving; Range query; State of the art; Upper Bound; Data structures
Editorial: Updates to the editorial board,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081203761&doi=10.1145%2f3381020&partnerID=40&md5=b4e966ab792ace3326703528568ab2ec,[No abstract available],,
Computing optimal repairs for functional dependencies,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081137551&doi=10.1145%2f3360904&partnerID=40&md5=24c37baa4a8ea1fef0985a9213c0c30f,"We investigate the complexity of computing an optimal repair of an inconsistent database, in the case where integrity constraints are Functional Dependencies (FDs).We focus on two types of repairs: an optimal subset repair (optimal S-repair), which is obtained by a minimum number of tuple deletions, and an optimal update repair (optimal U-repair), which is obtained by a minimum number of value (cell) updates. For computing an optimal S-repair, we present a polynomial-time algorithm that succeeds on certain sets of FDs and fails on others. We prove the following about the algorithm. When it succeeds, it can also incorporate weighted tuples and duplicate tuples. When it fails, the problem is NP-hard and, in fact, APX-complete (hence, cannot be approximated better than some constant). Thus, we establish a dichotomy in the complexity of computing an optimal S-repair. We present general analysis techniques for the complexity of computing an optimal Urepair, some based on the dichotomy for S-repairs. We also draw a connection to a past dichotomy in the complexity of finding a most probable database that satisfies a set of FDs with a single attribute on the left-hand side; the case of general FDs was left open, and we show how our dichotomy provides the missing generalization and thereby settles the open problem. © 2020 American Institute of Physics Inc.. All rights reserved.",Approximation; Cardinality repairs; Database cleaning; Dichotomy; Functional dependencies; Inconsistent databases; Optimal repairs; Value repairs,Database systems; Polynomial approximation; Approximation; Cardinalities; Dichotomy; Functional dependency; Inconsistent database; Optimization
ϵKtelo: A framework for defining differentially private computations,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079779805&doi=10.1145%2f3362032&partnerID=40&md5=970a7625aa555044df89308bc7f40a54,"The adoption of differential privacy is growing, but the complexity of designing private, efficient, and accurate algorithms is still high. We propose a novel programming framework and system, ϵktelo, for implementing both existing and new privacy algorithms. For the task of answering linear counting queries, we show that nearly all existing algorithms can be composed from operators, each conforming to one of a small number of operator classes. While past programming frameworks have helped to ensure the privacy of programs, the novelty of our framework is its significant support for authoring accurate and efficient (as well as private) programs. After describing the design and architecture of the ϵktelo system, we show that ϵktelo is expressive, allows for safer implementations through code reuse, and allows both privacy novices and experts to easily design algorithms. We provide a number of novel implementation techniques to support the generality and scalability of ϵktelo operators. These include methods to automatically compute lossless reductions of the data representation, implicit matrices that avoid materialized state but still support computations, and iterative inference implementations that generalize techniques from the privacy literature. We demonstrate the utility of ϵktelo by designing several new state-of-the-art algorithms, most of which result from simple re-combinations of operators defined in the framework. We study the accuracy and scalability of ϵktelo plans in a thorough empirical evaluation. © 2020 Association for Computing Machinery.",Differential privacy; Privacy-preserving query answering; Programming framework,Scalability; Data representations; Differential privacies; Empirical evaluations; Implementation techniques; Novices and experts; Programming framework; Query answering; State-of-the-art algorithms; Iterative methods
Efficient enumeration algorithms for regular document spanners,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079807779&doi=10.1145%2f3351451&partnerID=40&md5=fb82ddd1369a0bee6d8a2c6da04d0fc6,"Regular expressions and automata models with capture variables are core tools in rule-based information extraction. These formalisms, also called regular document spanners, use regular languages to locate the data that a user wants to extract from a text document and then store this data into variables. Since document spanners can easily generate large outputs, it is important to have efficient evaluation algorithms that can generate the extracted data in a quick succession, and with relatively little precomputation time. Toward this goal, we present a practical evaluation algorithm that allows output-linear delay enumeration of a spanner's result after a precomputation phase that is linear in the document. Although the algorithm assumes that the spanner is specified in a syntactic variant of variable-set automata, we also study how it can be applied when the spanner is specified by general variable-set automata, regex formulas, or spanner algebras. Finally, we study the related problem of counting the number of outputs of a document spanner and provide a fine-grained analysis of the classes of document spanners that support efficient enumeration of their results. © 2020 Association for Computing Machinery.",Automata; Capture variables; Enumeration delay; Information extraction; Spanners,Artificial intelligence; Automata theory; Information retrieval; Pattern matching; Automata; Capture variables; Enumeration algorithms; Enumeration delay; Evaluation algorithm; Fine-grained analysis; Regular expressions; Spanners; Data mining
A game-theoretic approach to data interaction,2020,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079808767&doi=10.1145%2f3351450&partnerID=40&md5=28a68d3c50afac78c7aa7a42b79bdbc6,"As most users do not precisely know the structure and/or the content of databases, their queries do not exactly reflect their information needs. The database management system (DBMS) may interact with users and use their feedback on the returned results to learn the information needs behind their queries. Current query interfaces assume that users do not learn and modify the way they express their information needs in the form of queries during their interaction with the DBMS. Using a real-world interaction workload, we show that users learn and modify how to express their information needs during their interactions with the DBMS and their learning is accurately modeled by a well-known reinforcement learning mechanism. As current data interaction systems assume that users do not modify their strategies, they cannot discover the information needs behind users' queries effectively. We model the interaction between the user and the DBMS as a game with identical interest between two rational agents whose goal is to establish a common language for representing information needs in the form of queries. We propose a reinforcement learning method that learns and answers the information needs behind queries and adapts to the changes in users' strategies and proves that it improves the effectiveness of answering queries, stochastically speaking. We propose two efficient implementations of this method over large relational databases. Our extensive empirical studies over real-world query workloads indicate that our algorithms are efficient and effective. © 2020 Association for Computing Machinery.",Collaborative interaction; Database querying; Game theory; Reinforcement learning; User and database interaction,Game theory; Learning systems; Query languages; Query processing; Reinforcement learning; Collaborative interaction; Data interactions; Database interactions; Database querying; Efficient implementation; Empirical studies; Reinforcement learning method; Relational Database; Information management
General temporally biased sampling schemes for online model management,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077746433&doi=10.1145%2f3360903&partnerID=40&md5=7761ebb8acafe7104fa25ce3b209287e,"To maintain the accuracy of supervised learning models in the presence of evolving data streams, we provide temporally biased sampling schemes that weight recent data most heavily, with inclusion probabilities for a given data item decaying over time according to a specified “decay function.” We then periodically retrain the models on the current sample. This approach speeds up the training process relative to training on all of the data. Moreover, time-biasing lets the models adapt to recent changes in the data while—unlike in a sliding-window approach—still keeping some old data to ensure robustness in the face of temporary fluctuations and periodicities in the data values. In addition, the sampling-based approach allows existing analytic algorithms for static data to be applied to dynamic streaming data essentially without change. We provide and analyze both a simple sampling scheme (Targeted-Size Time-Biased Sampling (T-TBS)) that probabilistically maintains a target sample size and a novel reservoir-based scheme (Reservoir-Based Time-Biased Sampling (R-TBS)) that is the first to provide both control over the decay rate and a guaranteed upper bound on the sample size. If the decay function is exponential, then control over the decay rate is complete, and R-TBS maximizes both expected sample size and sample-size stability. For general decay functions, the actual item inclusion probabilities can be made arbitrarily close to the nominal probabilities, and we provide a scheme that allows a tradeoff between sample footprint and sample-size stability. R-TBS rests on the notion of a “fractional sample” and allows for data arrival rates that are unknown and time varying (unlike T-TBS). The R-TBS and T-TBS schemes are of independent interest, extending the known set of unequal-probability sampling schemes. We discuss distributed implementation strategies; experiments in Spark illuminate the performance and scalability of the algorithms, and show that our approach can increase machine learning robustness in the face of evolving data. © 2019 Association for Computing Machinery.",Distributed sampling; Fractional sample; Latent sample; Model retraining; Model robustness; Online model management; Reservoir sampling; Temporal decay; Temporally biased sampling; Time-biased sampling; Unequal-probability sampling,Exponential functions; Machine learning; Probability; Reservoir management; Robustness (control systems); Sampling; Biased sampling; Distributed samplings; Model robustness; Online modeling; Reservoir samplings; Temporal decay; Unequal probability sampling; Decay (organic)
Design and evaluation of an RDMA-aware data shuffling operator for parallel database systems,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077734090&doi=10.1145%2f3360900&partnerID=40&md5=08b931ee5378d9d12536af4bc2fcb0db,"The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space that includes (1) the number of open connections, (2) the contention for the shared network interface, (3) the RDMA transport function, and (4) how much memory should be reserved to exchange data between nodes during query processing. We contribute eight designs that capture salient tradeoffs in this design space as well as an adaptive algorithm to dynamically manage RDMA-registered memory. We comprehensively evaluate how transport-layer decisions impact the query performance of a database system for different generations of InfiniBand. We find that a shuffling operator that uses the RDMA Send/Receive transport function over the Unreliable Datagram transport service can transmit data up to 4× faster than an RDMA-capable MPI implementation in a 16-node cluster. The response time of TPC-H queries improves by as much as 2×. © 2019 Copyright held by the owner/author(s).",Data shuffling; Parallel database systems; RDMA,Adaptive algorithms; Function evaluation; Query processing; Analytical performance; Data shuffling; Design and evaluations; High-performance networking; Parallel database systems; RDMA; Shuffling operators; Unreliable datagram; Database systems
Efficient algorithms for approximate single-source personalized pagerank queries,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075592699&doi=10.1145%2f3360902&partnerID=40&md5=fb304370db199ec3471eed6d088a7d49,"Given a graph G, a source node s, and a target node t, the personalized PageRank (PPR) of t with respect to s is the probability that a random walk starting from s terminates at t . An important variant of the PPR query is single-source PPR (SSPPR), which enumerates all nodes in G and returns the top-k nodes with the highest PPR values with respect to a given source s. PPR in general and SSPPR in particular have important applications in web search and social networks, e.g., in Twitter'sWho-To-Follow recommendation service. However, PPR computation is known to be expensive on large graphs and resistant to indexing. Consequently, previous solutions either use heuristics, which do not guarantee result quality, or rely on the strong computing power of modern data centers, which is costly. Motivated by this, we propose effective index-free and index-based algorithms for approximate PPR processing, with rigorous guarantees on result quality. We first present FORA, an approximate SSPPR solution that combines two existing methods-Forward Push (which is fast but does not guarantee quality) and Monte Carlo Random Walk (accurate but slow)-in a simple and yet non-trivial way, leading to both high accuracy and efficiency. Further, FORA includes a simple and effective indexing scheme, as well as a module for top-k selection with high pruning power. Extensive experiments demonstrate that the proposed solutions are orders of magnitude more efficient than their respective competitors. Notably, on a billion-edge Twitter dataset, FORA answers a top-500 approximate SSPPR query within 1s, using a single commodity server. © 2019 Association for Computing Machinery.",Forward push; Personalized PageRank; Random walk,Indexing (of information); Monte Carlo methods; Random processes; Social networking (online); Computing power; Effective index; Forward push; Index based algorithm; Indexing scheme; Orders of magnitude; Personalized PageRank; Random Walk; Graph theory
Dichotomies for evaluating simple regular path queries,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073684448&doi=10.1145%2f3331446&partnerID=40&md5=480e3c665c2a4510a4c9deb948e96bba,"Regular path queries (RPQs) are a central component of graph databases. We investigate decision and enumeration problems concerning the evaluation of RPQs under several semantics that have recently been considered: Arbitrary paths, shortest paths, paths without node repetitions (simple paths), and paths without edge repetitions (trails). Whereas arbitrary and shortest paths can be dealt with efficiently, simple paths and trails become computationally difficult already for very small RPQs. We study RPQ evaluation for simple paths and trails from a parameterized complexity perspective and define a class of simple transitive expressions that is prominent in practice and for which we can prove dichotomies for the evaluation problem. We observe that, even though simple path and trail semantics are intractable for RPQs in general, they are feasible for the vast majority of RPQs that are used in practice. At the heart of this study is a result of independent interest: The two disjoint paths problem in directed graphs is W[1]-hard if parameterized by the length of one of the two paths. © 2019 Association for Computing Machinery.",Graph databases; Parameterized complexity; Regular languages; Regular path queries,Formal languages; Graph Databases; Parameterization; Semantics; Central component; Disjoint paths problem; Enumeration problems; Evaluation problems; Parameterized; Parameterized complexity; Regular path queries; Shortest path; Directed graphs
ChronicleDB: A high-performance event store,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073696340&doi=10.1145%2f3342357&partnerID=40&md5=1e481327150aebf8fb5002e1ba431ae7,"Reactive security monitoring, self-driving cars, the Internet of Things (IoT), and many other novel applications require systems for both writing events arriving at very high and fluctuating rates to persistent storage as well as supporting analytical ad hoc queries. As standard database systems are not capable of delivering the required write performance, log-based systems, key-value stores, and other write-optimized data stores have emerged recently. However, the drawbacks of these systems are a fair query performance and the lack of suitable instant recovery mechanisms in case of system failures. In this article, we present ChronicleDB, a novel database system with a storage layout tailored for high write performance under fluctuating data rates and powerful indexing capabilities to support a variety of queries. In addition, ChronicleDB offers low-cost fault tolerance and instant recovery within milliseconds. Unlike previous work, ChronicleDB is designed either as a serverless library to be tightly integrated in an application or as a standalone database server. Our results of an experimental evaluation with real and synthetic data reveal that ChronicleDB clearly outperforms competing systems with respect to both write and query performance. © 2019 Association for Computing Machinery.",Aggregation queries; Event processing; Indexing; Recovery; Storage layout; Time travel queries,Digital storage; Fault tolerance; Indexing (materials working); Indexing (of information); Internet of things; Query languages; Query processing; Recovery; Systems engineering; Aggregation queries; Event Processing; Experimental evaluation; Internet of thing (IOT); Recovery mechanisms; Security monitoring; Storage layouts; Time travel; Search engines
On the expressive power of query languages for matrices,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073692681&doi=10.1145%2f3331445&partnerID=40&md5=84d95d2d1d01d7ed75078a0dab0eb0d0,"We investigate the expressive power of MATLANG, a formal language for matrix manipulation based on common matrix operations and linear algebra. The language can be extended with the operation inv for inverting amatrix. In MATLANG + inv, we can compute the transitive closure of directed graphs, whereas we show that this is not possible without inversion. Indeed, we show that the basic language can be simulated in the relational algebra with arithmetic operations, grouping, and summation. We also consider an operation eigen for diagonalizing a matrix. It is defined such that for each eigenvalue a set of mutually orthogonal eigenvectors is returned that span the eigenspace of that eigenvalue. We show that inv can be expressed in MATLANG + eigen. We put forward the open question whether there are Boolean queries about matrices, or generic queries about graphs, expressible in MATLANG + eigen but not in MATLANG + inv. Finally, the evaluation problem for MATLANG + eigen is shown to be complete for the complexity class ∃R. © 2019 Association for Computing Machinery.",Graph queries; Matrix query languages; Query evaluation problem; Relational algebra with aggregates,Directed graphs; Eigenvalues and eigenfunctions; Formal languages; Query languages; Arithmetic operations; Evaluation problems; Graph queries; Matrix manipulation; Matrix operations; Query evaluation; Relational algebra; Transitive closure; Matrix algebra
From a Comprehensive Experimental Survey to a Cost-based selection strategy for lightweight integer compression algorithms,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067793305&doi=10.1145%2f3323991&partnerID=40&md5=4c4cfead5f3c5c6905329b91274f9b1d,"Lightweight integer compression algorithms are frequently applied in in-memory database systems to tackle the growing gap between processor speed and main memory bandwidth. In recent years, the vectorization of basic techniques such as delta coding and null suppression has considerably enlarged the corpus of available algorithms. As a result, today there is a large number of algorithms to choose from, while different algorithms are tailored to different data characteristics. However, a comparative evaluation of these algorithms with different data and hardware characteristics has never been sufficiently conducted in the literature. To close this gap, we conducted an exhaustive experimental survey by evaluating several state-of-the-art lightweight integer compression algorithms as well as cascades of basic techniques. We systematically investigated the influence of data as well as hardware properties on the performance and the compression rates. The evaluated algorithms are based on publicly available implementations as well as our own vectorized reimplementations. We summarize our experimental findings leading to several new insights and to the conclusion that there is no single-best algorithm. Moreover, in this article, we also introduce and evaluate a novel cost model for the selection of a suitable lightweight integer compression algorithm for a given dataset. © 2019 Association for Computing Machinery.",Compression algorithm selection; Cost modeling; Experiment and analysis; Lightweight data compression; SIMD; Vectorization,Cost benefit analysis; Surveys; Compression algorithms; Cost modeling; Experiment and analysis; SIMD; Vectorization; Bandwidth compression
Inferring insertion times and optimizing error penalties in time-decaying bloom filters,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065799192&doi=10.1145%2f3284552&partnerID=40&md5=0431c9a9b027cef2e2534f7c28ce899f,"Current Bloom Filters tend to ignore Bayesian priors as well as a great deal of useful information they hold, compromising the accuracy of their responses. Incorrect responses cause users to incur penalties that are both application- and item-specific, but current Bloom Filters are typically tuned only for static penalties. Such shortcomings are problematic for all Bloom Filter variants, but especially so for Time-decaying Bloom Filters, in which the memory of older items decays over time, causing both false positives and false negatives. We address these issues by introducing inferential filters, which integrate Bayesian priors and information latent in filters to make penalty-optimal, query-specific decisions. We also show how to properly infer insertion times in such filters. Our methods are general, but here we illustrate their application to inferential time-decaying filters to support novel query types and sliding window queries with dynamic error penalties. We present inferential versions of the Timing Bloom Filter and Generalized Bloom Filter. Our experiments on real and synthetic datasets show that our methods reduce penalties for incorrect responses to sliding-window queries in these filters by up to 70% when penalties are dynamic. © 2019 Copyright held by the owner/author(s).",Bayesian priors; Bloom filters; Inferential bloom filters; Sliding windows; Statistical analysis,Decoding; Statistical methods; Bayesian priors; Bloom filters; Dynamic error; False negatives; False positive; Query types; Sliding Window; Synthetic datasets; Data structures
Output-optimal massively parallel algorithms for similarity joins,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065758036&doi=10.1145%2f3311967&partnerID=40&md5=cd8b759c11573e61a07826606f64d01d,"Parallel join algorithms have received much attention in recent years due to the rapid development of massively parallel systems such as MapReduce and Spark. In the database theory community, most efforts have been focused on studying worst-case optimal algorithms. However, the worst-case optimality of these join algorithms relies on the hard instances having very large output sizes. In the case of a two-relation join, the hard instance is just a Cartesian product, with an output size that is quadratic in the input size. In practice, however, the output size is usually much smaller. One recent parallel join algorithm by Beame et al. has achieved output-optimality (i.e., its cost is optimal in terms of both the input size and the output size), but their algorithm only works for a 2-relation equi-join and has some imperfections. In this article, we first improve their algorithm to true optimality. Then we design output-optimal algorithms for a large class of similarity joins. Finally, we present a lower bound, which essentially eliminates the possibility of having output-optimal algorithms for any join on more than two relations. © 2019 Association for Computing Machinery.",Output-sensitive algorithms; Parallel computation; Similarity joins,Computation theory; Cartesian Products; Data base theory; Join algorithm; Massively parallel systems; Optimal algorithm; Output-sensitive algorithm; Parallel Computation; Similarity join; Optimization
A survey of spatial crowdsourcing,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065778310&doi=10.1145%2f3291933&partnerID=40&md5=e4f7e95a0550e57c4c4926c139458337,"Widespread use of advanced mobile devices has led to the emergence of a new class of crowdsourcing called spatial crowdsourcing. Spatial crowdsourcing advances the potential of a crowd to perform tasks related to real-world scenarios involving physical locations, which were not feasible with conventional crowdsourcing methods. The main feature of spatial crowdsourcing is the presence of spatial tasks that require workers to be physically present at a particular location for task fulfillment. Research related to this new paradigm has gained momentum in recent years, necessitating a comprehensive survey to offer a bird's-eye view of the current state of spatial crowdsourcing literature. In this article, we discuss the spatial crowdsourcing infrastructure and identify the fundamental differences between spatial and conventional crowdsourcing. Furthermore, we provide a comprehensive view of the existing literature by introducing a taxonomy, elucidate the issues/challenges faced by different components of spatial crowdsourcing, and suggest potential research directions for the future. © 2019 Association for Computing Machinery.",Algorithms; Incentive mechanism; Location privacy; Quality assurance; Rewards; Spatial crowdsourcing; Spatial databases; Task assignment; Task matching; Task scheduling,Algorithms; Location; Mobile telecommunication systems; Quality assurance; Surveys; Incentive mechanism; Location privacy; Rewards; Spatial database; Task assignment; Task matching; Task-scheduling; Crowdsourcing
Dependencies for graphs,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062239044&doi=10.1145%2f3287285&partnerID=40&md5=135be6611a61a17b9d68203c870f1376,"This article proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is defined as a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs can express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities (vertices) in a graph. We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound, complete and independent axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication, and validation problems for these extensions. © 2019 Association for Computing Machinery.",Axiom system; Built-in predicates; Conditional functional dependencies; Disjunction; EGDs; Graph dependencies; Implication; Keys; Satisfiability; TGDs; Validation,Database systems; Information systems; Axiom system; Built-in predicates; Conditional functional dependencies; Disjunction; EGDs; Graph dependencies; Implication; Keys; Satisfiability; TGDs; Validation; Formal logic
A unified framework for frequent sequence mining with subsequence constraints,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067176206&doi=10.1145%2f3321486&partnerID=40&md5=c12dbd2274946316a23bc815076a1172,"Frequent sequence mining methods often make use of constraints to control which subsequences should be mined. A variety of such subsequence constraints has been studied in the literature, including length, gap, span, regular-expression, and hierarchy constraints. In this article, we show that many subsequence constraints—including and beyond those considered in the literature—can be unified in a single framework. A unified treatment allows researchers to study jointly many types of subsequence constraints (instead of each one individually) and helps to improve usability of pattern mining systems for practitioners. In more detail, we propose a set of simple and intuitive “pattern expressions” to describe subsequence constraints and explore algorithms for efficiently mining frequent subsequences under such general constraints. Our algorithms translate pattern expressions to succinct finite-state transducers, which we use as computational model, and simulate these transducers in a way suitable for frequent sequence mining. Our experimental study on real-world datasets indicates that our algorithms—although more general—are efficient and, when used for sequence mining with prior constraints studied in literature, competitive to (and in some cases superior to) state-of-the-art specialized methods. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Data mining; Finite-state transducers; Frequent sequence mining; Hierarchies; Sequential pattern mining; Subsequence constraints,Transducers; Finite state transducers; Frequent sequences; Hierarchies; Sequential-pattern mining; Subsequence constraints; Data mining
Verification of hierarchical artifact systems,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067197335&doi=10.1145%2f3321487&partnerID=40&md5=53da29350077910d4d1f44944095df2f,"Data-driven workflows, of which IBM’s Business Artifacts are a prime exponent, have been successfully deployed in practice, adopted in industrial standards, and have spawned a rich body of research in academia, focused primarily on static analysis. The present work represents a significant advance on the problem of artifact verification by considering a much richer and more realistic model than in previous work, incorporating core elements of IBM’s successful Guard-Stage-Milestone model. In particular, the model features task hierarchy, concurrency, and richer artifact data. It also allows database key and foreign key dependencies, as well as arithmetic constraints. The results show decidability of verification and establish its complexity, making use of novel techniques including a hierarchy of Vector Addition Systems and a variant of quantifier elimination tailored to our context. © 2019 Association for Computing Machinery.",Business process management; Data-centric workflows; Temporal logic; Verification,Enterprise resource management; Industrial research; Temporal logic; Vectors; Verification; Arithmetic constraints; Business Artifacts; Business process management; Data centric workflows; Industrial standards; Modeling features; Quantifier elimination; Vector addition systems; Hierarchical systems
Interactive mapping specification with exemplar tuples,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067190525&doi=10.1145%2f3321485&partnerID=40&md5=886651639c05df71e31c0345d73e89ef,"While schema mapping specification is a cumbersome task for data curation specialists, it becomes unfeasible for non-expert users, who are unacquainted with the semantics and languages of the involved transformations. In this article, we present an interactive framework for schema mapping specification suited for non-expert users. The underlying key intuition is to leverage a few exemplar tuples to infer the underlying mappings and iterate the inference process via simple user interactions under the form of Boolean queries on the validity of the initial exemplar tuples. The approaches available so far are mainly assuming pairs of complete universal data examples, which can be solely provided by data curation experts, or are limited to poorly expressive mappings. We present a quasi-lattice-based exploration of the space of all possible mappings that satisfy arbitrary user exemplar tuples. Along the exploration, we challenge the user to retain the mappings that fit the user’s requirements at best and to dynamically prune the exploration space, thus reducing the number of user interactions. We prove that after the refinement process, the obtained mappings are correct and complete. We present an extensive experimental analysis devoted to measure the feasibility of our interactive mapping strategies and the inherent quality of the obtained mappings. © 2019 Association for Computing Machinery.",Data integration; Mapping refinement; User interactions,Data curation; Data integration; Semantics; Specifications; Boolean queries; Experimental analysis; Inference process; Interactive mapping; Lattice-based; Refinement process; Schema mappings; User interaction; Mapping
Representations and optimizations for embedded parallel dataflow languages,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061210087&doi=10.1145%2f3281629&partnerID=40&md5=babc2e9d88342eefde8c155d5d513a46,"Parallel dataflow engines such as Apache Hadoop, Apache Spark, and Apache Flink are an established alternative to relational databases for modern data analysis applications. A characteristic of these systems is a scalable programming model based on distributed collections and parallel transformations expressed by means of second-order functions such as map and reduce. Notable examples are Flink's DataSet and Spark's RDD programming abstractions. These programming models are realized as EDSLs-domain specific languages embedded in a general-purpose host language such as Java, Scala, or Python. This approach has several advantages over traditional external DSLs such as SQL or XQuery. First, syntactic constructs from the host language (e.g., anonymous functions syntax, value definitions, and fluent syntax via method chaining) can be reused in the EDSL. This eases the learning curve for developers already familiar with the host language. Second, it allows for seamless integration of library methods written in the host language via the function parameters passed to the parallel dataflow operators. This reduces the effort for developing analytics dataflows that go beyond pure SQL and require domain-specific logic. At the same time, however, state-of-the-art parallel dataflow EDSLs exhibit a number of shortcomings. First, one of the main advantages of an external DSL such as SQL-the high-level, declarative Select-From-Where syntax-is either lost completely or mimicked in a non-standard way. Second, execution aspects such as caching, join order, and partial aggregation have to be decided by the programmer. Optimizing them automatically is very difficult due to the limited program context available in the intermediate representation of the DSL. In this article, we argue that the limitations listed above are a side effect of the adopted type-based embedding approach. As a solution, we propose an alternative EDSL design based on quotations. We present a DSL embedded in Scala and discuss its compiler pipeline, intermediate representation, and some of the enabled optimizations. We promote the algebraic type of bags in union representation as a model for distributed collections and its associated structural recursion scheme and monad as a model for parallel collection processing. At the source code level, Scala's comprehension syntax over a bag monad can be used to encode Select-From-Where expressions in a standard way. At the intermediate representation level, maintaining comprehensions as a first-class citizen can be used to simplify the design and implementation of holistic datafow optimizations that accommodate for nesting and control-flow. The proposed DSL design therefore reconciles the benefits of embedded parallel dataflow DSLs with the declarativity and optimization potential of external DSLs like SQL. © 2019 Copyright held by the owner/author(s).",MapReduce; Monad comprehensions; Parallel dataflows,Computer software; Digital subscriber lines; Distributed computer systems; Problem oriented languages; Syntactics; XML; Data flow; Design and implementations; Domain specific languages; Intermediate representations; Map-reduce; Monad comprehensions; Programming abstractions; Structural recursion schemes; Data flow analysis
Wander join and XDB: Online aggregation via random walks,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061205068&doi=10.1145%2f3284551&partnerID=40&md5=d34895409ee66f1441a16e42d1a8d69b,"Joins are expensive, and online aggregation over joins was proposed to mitigate the cost, which offers users a nice and flexible tradeoff between query efficiency and accuracy in a continuous, online fashion. However, the state-of-the-art approach, in both internal and external memory, is based on ripple join, which is still very expensive and even needs unrealistic assumptions (e.g., tuples in a table are stored in random order). This article proposes a new approach, the wander join algorithm, to the online aggregation problem by performing random walks over the underlying join graph. We also design an optimizer that chooses the optimal plan for conducting the random walks without having to collect any statistics a priori. Compared with ripple join, wander join is particularly efficient for equality joins involving multiple tables, but also supports θ-joins. Selection predicates and group-by clauses can be handled as well. To demonstrate the usefulness of wander join, we have designed and implemented XDB (approXimate DB) by integrating wander join into various systems including PostgreSQL, Spark, and a stand-alone plug-in version using PL/SQL. The design and implementation of XDB has demonstrated wander join's practicality in a full-fledged database system. Extensive experiments using the TPC-H benchmark have demonstrated the superior performance of wander join over ripple join. © 2019 Association for Computing Machinery.",Join; Online aggregation; Random walk,Benchmarking; Joining; Query processing; Design and implementations; External memory; On-line fashion; Online aggregations; Query efficiency; Random Walk; State-of-the-art approach; Tpc-h benchmarks; Random processes
Historic moments discovery in sequence data,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061200576&doi=10.1145%2f3276975&partnerID=40&md5=938e2c405b557cd27630f4ae6dc9d81e,"Many emerging applications are based on finding interesting subsequences from sequence data. Finding ""prominent streaks,"" a set of the longest contiguous subsequences with values all above (or below) a certain threshold, from sequence data is one of that kind that receives much attention. Motivated from real applications, we observe that prominent streaks alone are not insightful enough but require the discovery of something we coined as ""historic moments"" as companions. In this article, we present an algorithm to efficiently compute historic moments from sequence data. The algorithm is incremental and space optimal, meaning that when facing new data arrival, it is able to efficiently refresh the results by keeping minimal information. Case studies show that historic moments can significantly improve the insights offered by prominent streaks alone. Furthermore, experiments show that our algorithm can outperform the baseline in both time and space. © 2019 Association for Computing Machinery.",Historic moments; Prominent streaks; Sequence data; Space optimal,Information systems; Case-studies; Emerging applications; Historic moments; Minimal information; Prominent streaks; Real applications; Sequence data; Space optimal; Database systems
Scalable analytics on fast data,2019,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061189821&doi=10.1145%2f3283811&partnerID=40&md5=4dd53d1389328b08ae221c403f9e3721,"Today's streaming applications demand increasingly high event throughput rates and are often subject to strict latency constraints. To allow for more complex workloads, such as window-based aggregations, streaming systems need to support stateful event processing. This introduces new challenges for streaming engines as the state needs to be maintained in a consistent and durable manner and simultaneously accessed by complex queries for real-time analytics. Modern streaming systems, such as Apache Flink, do not allow for efficiently exposing the state to analytical queries. Thus, data engineers are forced to keep the state in external data stores, which significantly increases the latencies until events become visible to analytical queries. Proprietary solutions have been created to meet data freshness constraints. These solutions are expensive, error-prone, and difficult to maintain. Main-memory database systems, such as HyPer, achieve extremely low query response times while maintaining high update rates, which makes them well-suited for analytical streaming workloads. In this article, we explore extensions to database systems to match the performance and usability of streaming systems. © 2019 Copyright held by the owner/author(s).",Event processing; Multi-version concurrency control; Real-time analytics; User-space networking,Database systems; Query processing; Event Processing; Latency constraints; Main memory database systems; Multi-version; Proprietary solutions; Real-time analytics; Streaming applications; User spaces; Concurrency control
Parallelizing sequential graph computations,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061216592&doi=10.1145%2f3282488&partnerID=40&md5=1e8c474cc6170f255631fc1c62511472,"This article presents GRAPE, a parallel GRAPh Engine for graph computations. GRAPE differs from prior systems in its ability to parallelize existing sequential graph algorithms as a whole, without the need for recasting the entire algorithm into a new model. Underlying GRAPE are a simple programming model and a principled approach based on fixpoint computation that starts with partial evaluation and usesanincremental function as the intermediate consequence operator. We show that users can devise existing sequential graph algorithms with minor additions, and GRAPE parallelizes the computation. Under a monotonic condition, the GRAPE parallelization guarantees to converge at correct answers as long as the sequential algorithms are correct. Moreover, we show that algorithms in MapReduce, BSP, and PRAM can be optimally simulated on GRAPE. In addition to the ease of programming, we experimentally verify that GRAPE achieves comparable performance to the state-of-the-art graph systems using real-life and synthetic graphs. © 2018 Association for Computing Machinery.",Convergence; Graph computations; Parallel graph query engines; Parallelizing sequential algorithms; Simulation,Database systems; Information systems; Convergence; Fixpoint computations; Graph algorithms; Graph queries; Partial evaluation; Programming models; Sequential algorithm; Simulation; Engines
Optimal bloom filters and adaptive merging for LSM-trees,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061209141&doi=10.1145%2f3276980&partnerID=40&md5=ec8ab06696d2da26422127de71b7be5f,"In this article, we show that key-value stores backed by a log-structured merge-tree (LSM-tree) exhibit an intrinsic tradeoff between lookup cost, update cost, and main memory footprint, yet all existing designs expose a suboptimal and dificult to tune tradeoff among these metrics. We pinpoint the problem to the fact that modern key-value stores suboptimally co-tune the merge policy, the buffer size, and the Bloom filters' false-positive rates across the LSM-tree's different levels. We present Monkey, an LSM-tree based key-value store that strikes the optimal balance between the costs of updates and lookups with any given main memory budget. The core insight is that worst-case lookup cost is proportional to the sum of the false-positive rates of the Bloom filters across all levels of the LSM-tree. Contrary to state-of-the-art key-value stores that assign a fixed number of bits-per-element to all Bloom filters, Monkey allocates memory to filters across different levels so as to minimize the sum of their false-positive rates. We show analytically that Monkey reduces the asymptotic complexity of the worst-case lookup I/O cost, and we verify empirically using an implementation on top of RocksDB that Monkey reduces lookup latency by an increasing margin as the data volume grows (50-80% for the data sizes we experimented with). Furthermore, we map the design space onto a closed-form model that enables adapting the merging frequency and memory allocation to strike the best tradeoff among lookup cost, update cost and main memory, depending on the workload (proportion of lookups and updates), the dataset (number and size of entries), and the underlying hardware (main memory available, disk vs. flash). We show how to use this model to answer what-if design questions about how changes in environmental parameters impact performance and how to adapt the design of the key-value store for optimal performance. © 2018 Association for Computing Machinery.",Bloom filters; Key-value stores; LSM-tree; NoSQL; System design,Adaptive filtering; Blooms (metal); Budget control; Data structures; Systems analysis; Asymptotic complexity; Bloom filters; Environmental parameter; False positive rates; Key-value stores; Log structured merge trees; LSM-tree; NoSQL; Merging
MacroBase: Prioritizing atention in fast data,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061197497&doi=10.1145%2f3276463&partnerID=40&md5=6b169d8e0a9b319cb2657deb24cae655,"As data volumes continue to rise, manual inspection is becoming increasingly untenable. In response, we present MacroBase, a data analytics engine that prioritizes end-user attention in high-volume fast data streams. MacroBase enables eficient, accurate, and modular analyses that highlight and aggregate important and unusual behavior, acting as a search engine for fast data. MacroBase is able to deliver order-of-magnitude speedups over alternatives by optimizing the combination of explanation (i.e., feature selection) and classification tasks and by leveraging a new reservoir sampler and heavy-hitters sketch specialized for fast data streams. As a result, MacroBase delivers accurate results at speeds of up to 2M events per second per query on a single core. The system has delivered meaningful results in production, including at a telematics company monitoring hundreds of thousands of vehicles. © 2018 Association for Computing Machinery.",Analytics; Database; Streaming,Acoustic streaming; Data Analytics; Data mining; Database systems; Query processing; Analytics; Classification tasks; Data volume; Heavy-hitter; High volumes; Manual inspection; Modular analysis; Unusual behaviors; Search engines
Learning from qery-answers: A scalable approach to belief updating and parameter learning,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061183508&doi=10.1145%2f3277503&partnerID=40&md5=0c84478b1a32cd733379ee70e22e4141,"Tuple-independent and disjoint-independent probabilistic databases (TI- and DI-PDBs) represent uncertain data in a factorized form as a product of independent random variables that represent either tuples (TI-PDBs) or sets of tuples (DI-PDBs). When the user submits a query, the database derives the marginal probabilities of each output-tuple, exploiting the underlying assumptions of statistical independence. While query processing in TI- and DI-PDBs has been studied extensively, limited research has been dedicated to the problems of updating or deriving the parameters from observations of query results. Addressing this problem is the main focus of this article. We first introduce Beta Probabilistic Databases (B-PDBs), a generalization of TI-PDBs designed to support both (i) belief updating and (ii) parameter learning in a principled and scalable way. The key idea of B-PDBs is to treat each parameter as a latent, Beta-distributed random variable. We show how this simple expedient enables both belief updating and parameter learning in a principled way, without imposing any burden on regular query processing. Building on B-PDBs, we then introduce Dirichlet Probabilistic Databases (D-PDBs), a generalization of DI-PDBs with similar properties. We provide the following key contributions for both B- and D-PDBs: (i) We study the complexity of performing Bayesian belief updates and devise efficient algorithms for certain tractable classes of queries; (ii) we propose a soft-EM algorithm for computing maximum-likelihood estimates of the parameters; (iii) we present an algorithm for eficiently computing conditional probabilities, allowing us to eficiently implement B- and D-PDBs via a standard relational engine; and (iv) we support our conclusions with extensive experimental results. © 2018 Association for Computing Machinery.",Bayesian updates; Maximum likelihood estimation; Parameter learning; Probabilistic databases; Uncertain data management,Data mining; Database systems; Information management; Maximum likelihood estimation; Query processing; Random variables; Uncertainty analysis; Bayesian; Conditional probabilities; Distributed random variables; Independent random variables; Maximum likelihood estimate; Parameter learning; Probabilistic database; Statistical independence; Parameter estimation
Distributed joins and data placement for minimal network traffic,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061252515&doi=10.1145%2f3241039&partnerID=40&md5=1dbf341bb5086537be69104dfc05b1fc,"Network communication is the slowest component of many operators in distributed parallel databases deployed for large-scale analytics. Whereas considerable work has focused on speeding up databases on modern hardware, communication reduction has received less attention. Existing parallel DBMSs rely on algorithms designed for disks with minor modifications for networks. A more complicated algorithm may burden the CPUs but could avoid redundant transfers of tuples across the network. We introduce track join, a new distributed join algorithm that minimizes network traffic by generating an optimal transfer schedule for each distinct join key. Track join extends the trade-off options between CPU and network. Track join explicitly detects and exploits locality, also allowing for advanced placement of tuples beyond hash partitioning on a single attribute. We propose a novel data placement algorithm based on track join that minimizes the total network cost of multiple joins across different dimensions in an analytical workload. Our evaluation shows that track join outperforms hash join on the most expensive queries of real workloads regarding both network traffic and execution time. Finally, we show that our data placement optimization approach is both robust and effective in minimizing the total network cost of joins in analytical workloads. © 2018 Association for Computing Machinery.",Data placement; Distributed joins; Network optimization,Program processors; Advanced placements; Communication reduction; Data placement; Data placement algorithms; Large-scale analytics; Network communications; Network optimization; Optimal transfers; Economic and social effects
Expressive languages for querying the semantic web,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061245563&doi=10.1145%2f3238304&partnerID=40&md5=8897639c20e82a4645cfa46c5b7d84f2,"The problem of querying RDF data is a central issue for the development of the Semantic Web. The query language SPARQL has become the standard language for querying RDF since its W3C standardization in 2008. However, the 2008 version of this language missed some important functionalities: reasoning capabilities to deal with RDFS and OWL vocabularies, navigational capabilities to exploit the graph structure of RDF data, and a general form of recursion much needed to express some natural queries. To overcome these limitations, a new version of SPARQL, called SPARQL 1.1, was released in 2013, which includes entailment regimes for RDFS and OWL vocabularies, and a mechanism to express navigation patterns through regular expressions. Unfortunately, there are a number of useful navigation patterns that cannot be expressed in SPARQL 1.1, and the language lacks a general mechanism to express recursive queries. To the best of our knowledge, no efficient RDF query language that combines the above functionalities is known. It is the aim of this work to fill this gap. To this end, we focus on a core fragment of the OWL 2 QL profile of OWL 2 and show that every SPARQL query enriched with the above features can be naturally translated into a query expressed in a language that is based on an extension of Datalog, which allows for value invention and stratified negation. However, the query evaluation problem for this language is highly intractable, which is not surprising since it is expressive enough to encode some inherently hard queries. We identify a natural fragment of it, and we show it to be tractable and powerful enough to define SPARQL queries enhanced with the desired functionalities. © 2018 Association for Computing Machinery.",Datalog-based languages; Query answering; RDF; Semantic web; SPARQL,Birds; Navigation; Query languages; Datalog; Navigation patterns; Query answering; RDF query language; Reasoning capabilities; Regular expressions; SPARQL; Stratified negation; Semantic Web
A relational framework for classifier engineering,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061293178&doi=10.1145%2f3268931&partnerID=40&md5=875cab74b71031ac4c3025c062adfb1b,"In the design of analytical procedures and machine learning solutions, a critical and time-consuming task is that of feature engineering, for which various recipes and tooling approaches have been developed. In this article, we embark on the establishment of database foundations for feature engineering. We propose a formal framework for classification in the context of a relational database. The goal of this framework is to open the way to research and techniques to assist developers with the task of feature engineering by utilizing the database's modeling and understanding of data and queries and by deploying the well-studied principles of database management. As a first step, we demonstrate the usefulness of this framework by formally defining three key algorithmic challenges. The first challenge is that of separability, which is the problem of determining the existence of feature queries that agree with the training examples. The second is that of evaluating the VC dimension of the model class with respect to a given sequence of feature queries. The third challenge is identifiability, which is the task of testing for a property of independence among features that are represented as database queries. We give preliminary results on these challenges for the case where features are defined by means of conjunctive queries, and, in particular, we study the implication of various traditional syntactic restrictions on the inherent computational complexity. © 2018 Association for Computing Machinery.",Classifiers; Conjunctive queries; Feature engineering; Machine learning; Relational databases,Classification (of information); Classifiers; Learning systems; Machine learning; Analytical procedure; Conjunctive queries; Database foundations; Database management; Database queries; Feature engineerings; Relational Database; Time-consuming tasks; Query languages
Dynamic complexity under definable changes,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055181312&doi=10.1145%2f3241040&partnerID=40&md5=2b422796d3d61422ccc1e4f82c78282c,"In the setting of dynamic complexity, the goal of a dynamic program is to maintain the result of a fixed query for an input database that is subject to changes, possibly using additional auxiliary relations. In other words, a dynamic program updates a materialized view whenever a base relation is changed. The update of query result and auxiliary relations is specified using first-order logic or, equivalently, relational algebra. The original framework by Patnaik and Immerman only considers changes to the database that insert or delete single tuples. This article extends the setting to definable changes, also specified by first-order queries on the database, and generalizes previous maintenance results to these more expressive change operations. More specifically, it is shown that the undirected reachability query is first-order maintainable under singletuple changes and first-order defined insertions, likewise the directed reachability query for directed acyclic graphs is first-order maintainable under insertions defined by quantifier-free first-order queries. These results rely on bounded bridge properties, which basically say that, after an insertion of a defined set of edges, for each connected pair of nodes there is some path with a bounded number of new edges. While this bound can be huge, in general, it is shown to be small for insertion queries defined by unions of conjunctive queries. To illustrate that the results for this restricted setting could be practically relevant, they are complemented by an experimental study that compares the performance of dynamic programs with complex changes, dynamic programs with single changes, and with recomputation from scratch. The positive results are complemented by several inexpressibility results. For example, it is shown that-unlike for single-tuple insertions-dynamic programs that maintain the reachability query under definable, quantifier-free changes strictly need update formulas with quantifiers. Finally, further positive results unrelated to reachability are presented: It is shown that for changes definable by parameter-free first-order formulas, all LOGSPACE-definable (and even AC1-definable) queries can be maintained by first-order dynamic programs. © 2018 Association for Computing Machinery.",Dynamic descriptive complexity; Dynamic programs; SQL updates,Algebra; Directed graphs; Formal logic; Query languages; Conjunctive queries; Descriptive complexity; Directed acyclic graph (DAG); Dynamic program update; Dynamic programs; First-order formulas; Reachability queries; SQL updates; Query processing
Efficient evaluation and static analysis for well-designed pattern trees with projection,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053470665&doi=10.1145%2f3233983&partnerID=40&md5=a3219af636272cadcf173c0f4fcae2d5,"Conjunctive queries (CQs) fail to provide an answer when the pattern described by the query does not exactly match the data. CQs might thus be too restrictive as a querying mechanism when data is semistructured or incomplete. The semantic web therefore provides a formalism-known as (projected) well-designed pattern trees (pWDPTs)-that tackles this problem: pWDPTs allow us to formulate queries that match parts of the query over the data if available, but do not ignore answers of the remaining query otherwise. Here we abstract away the specifics of semantic web applications and study pWDPTs over arbitrary relational schemas. Since the language of pWDPTs subsumes CQs, their evaluation problem is intractable. We identify structural properties of pWDPTs that lead to (fixed-parameter) tractability of various variants of the evaluation problem. We also show that checking if a pWDPT is equivalent to one in our tractable class is in 2EXPTIME. As a corollary, we obtain fixed-parameter tractability of evaluation for pWDPTs with such good behavior. Our techniques also allow us to develop a theory of approximations for pWDPTs. © 2018 ACM.",Approximations; Containment; Hyper-treewidth; Query answering; RDF; SPARQL; Subsumption; Treewidth; Well-designed pattern trees,Approximation theory; Equivalence classes; Parameter estimation; Petroleum reservoir evaluation; Query processing; Trees (mathematics); Approximations; Containment; Pattern trees; Query answering; SPARQL; Subsumption; Tree-width; Semantic Web
Answering FO+MOD queries under updates on bounded degree databases,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053453671&doi=10.1145%2f3232056&partnerID=40&md5=82441655b5cd94b9f92554f5ecf3cb58,"We investigate the query evaluation problem for fixed queries over fully dynamic databases, where tuples can be inserted or deleted. The task is to design a dynamic algorithm that immediately reports the new result of a fixed query after every database update. We consider queries in first-order logic (FO) and its extension with modulo-counting quantifiers (FO+MOD) and show that they can be efficiently evaluated under updates, provided that the dynamic database does not exceed a certain degree bound. In particular, we construct a data structure that allows us to answer a Boolean FO+MOD query and to compute the size of the result of a non-Boolean query within constant time after every database update. Furthermore, after every database update, we can update the data structure in constant time such that afterwards we are able to test within constant time for a given tuple whether or not it belongs to the query result, to enumerate all tuples in the new query result, and to enumerate the difference between the old and the new query result with constant delay between the output tuples. The preprocessing time needed to build the data structure is linear in the size of the database. Our results extend earlier work on the evaluation of first-order queries on static databases of bounded degree and rely on an effective Hanf normal form for FO+MOD recently obtained by Heimberg, Kuske, and Schweikardt (LICS 2016). © 2018 ACM.",Counting problem; Dynamic databases; First-order logic with modulo-counting quantifiers; Hanf locality; Query enumeration,Computer circuits; Data structures; Formal logic; Query languages; Counting problems; Counting quantifiers; Dynamic database; Hanf locality; Query enumeration; Query processing
K-regret queries using multiplicative utility functions,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053449355&doi=10.1145%2f3230634&partnerID=40&md5=cc2a16302ef2dc67f09d591aad15020f,"The k-regret query aims to return a size-k subset S of a database D such that, for any query user that selects a data object from this size-k subset S rather than from database D, her regret ratio is minimized. The regret ratio here is modeled by the relative difference in the optimality between the locally optimal object in S and the globally optimal object in D. The optimality of a data object in turn is modeled by a utility function of the query user. Unlike traditional top-k queries, the k-regret query does not minimize the regret ratio for a specific utility function. Instead, it considers a family of infinite utility functions F, and aims to find a size-k subset that minimizes the maximum regret ratio of any utility function in F . Studies on k-regret queries have focused on the family of additive utility functions, which have limitations in modeling individuals' preferences and decision-making processes, especially for a common observation called the diminishing marginal rate of substitution (DMRS). We introduce k-regret queries with multiplicative utility functions, which are more expressive in modeling the DMRS, to overcome those limitations. We propose a query algorithm with bounded regret ratios. To showcase the applicability of the algorithm, we apply it to a special family of multiplicative utility functions, the Cobb-Douglas family of utility functions, and a closely related family of utility functions, the Constant Elasticity of Substitution family of utility functions, both of which are frequently used utility functions in microeconomics. After a further study of the query properties, we propose a heuristic algorithm that produces even smaller regret ratios in practice. Extensive experiments on the proposed algorithms confirm that they consistently achieve small maximum regret ratios. © 2018 ACM.",K-regret; Maximum regret ratio; Multiplicative utility functions; Skyline,Decision making; Economics; Heuristic algorithms; Optimization; Data objects; Decision making process; Elasticity of substitution; Marginal rate of substitutions; Maximum regrets; Query algorithms; Skyline; Utility functions; Query processing
Lightweight monitoring of distributed streams,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051841436&doi=10.1145%2f3226113&partnerID=40&md5=90afbb3676313ed5b442af3f2f6dd99f,"As data becomes dynamic, large, and distributed, there is increasing demand for what have become known as distributed stream algorithms. Since continuously collecting the data to a central server and processing it there is infeasible, a common approach is to define local conditions at the distributed nodes, such that-as long as they are maintained-some desirable global condition holds. Previous methods derived local conditions focusing on communication efficiency. While proving very useful for reducing the communication volume, these local conditions often suffer from heavy computational burden at the nodes. The computational complexity of the local conditions affects both the runtime and the energy consumption. These are especially critical for resource-limited devices like smartphones and sensor nodes. Such devices are becoming more ubiquitous due to the recent trend toward smart cities and the Internet of Things. To accommodate for high data rates and limited resources of these devices, it is crucial that the local conditions be quickly and efficiently evaluated. Here we propose a novel approach, designated CB (for Convex/Concave Bounds). CB defines local conditions using suitably chosen convex and concave functions. Lightweight and simple, these local conditions can be rapidly checked on the fly. CB's superiority over the state-of-the-art is demonstrated in its reduced runtime and power consumption, by up to six orders of magnitude in some cases. As an added bonus, CB also reduced communication overhead in all the tested application scenarios. ©2018 ACM.",Continuous Distributed Monitoring; Disributed Stream Mining,Energy utilization; Application scenario; Communication efficiency; Communication overheads; Computational burden; Continuous distributed monitoring; Orders of magnitude; Resource-limited devices; Stream mining; Sensor nodes
Estimating the impact of unknown unknowns on aggregate qery results,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043760184&doi=10.1145%2f3167970&partnerID=40&md5=486b28120fa99c2ccefda5cc49db8c90,"It is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) Is the integrated data set complete? and (2) What is the impact of any unknown (i.e., unobserved) data on query results? In this work, we develop and analyze techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key idea is that the overlap between different data sources enables us to estimate the number and values of the missing data items. Our main techniques are parameterfree and do not assume prior knowledge about the distribution; we also propose a parametric model that can be used instead when the data sources are imbalanced. Through a series of experiments, we show that estimating the impact of unknown unknowns is invaluable to better assess the results of aggregate queries over integrated data sources. © 2018 ACM.",Aggregate query processing; Crowdsourcing; Species estimation; Unknown unknowns,Crowdsourcing; Information systems; Aggregate queries; Aggregate query processing; Common practices; Integrated data; Parametric modeling; Prior knowledge; Query results; Unknown unknowns; Database systems
Practical private range search in depth,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043790813&doi=10.1145%2f3167971&partnerID=40&md5=572c1d324ecd1f421e4440c2dc2bb4f0,"We consider a data owner that outsources its dataset to an untrusted server. The owner wishes to enable the server to answer range queries on a single attribute, without compromising the privacy of the data and the queries. There are several schemes on ""practical"" private range search (mainly in database venues) that attempt to strike a trade-off between efficiency and security. Nevertheless, these methods either lack provable security guarantees or permit unacceptable privacy leakages. In this article, we take an interdisciplinary approach, which combines the rigor of security formulations and proofs with efficient data management techniques. We construct a wide set of novel schemes with realistic security/performance trade-offs, adopting the notion of Searchable Symmetric Encryption (SSE), primarily proposed for keyword search. We reduce range search to multi-keyword search using range-covering techniques with tree-like indexes, and formalize the problem as Range Searchable Symmetric Encryption (RSSE). We demonstrate that, given any secure SSE scheme, the challenge boils down to (i) formulating leakages that arise from the index structure and (ii) minimizing false positives incurred by some schemes under heavy data skew. We also explain an important concept in the recent SSE bibliography, namely locality, and design generic and specialized ways to attribute locality to our RSSE schemes. Moreover, we are the first to devise secure schemes for answering range aggregate queries, such as range sums and range min/max. We analytically detail the superiority of our proposals over prior work and experimentally confirm their practicality. © 2018 ACM.",Private range search; Searchable encryption,Economic and social effects; Information management; Query processing; Search engines; Data management techniques; Privacy leakages; Provable security; Range aggregate queries; Range search; Searchable encryptions; Symmetric encryption; Untrusted server; Cryptography
Editorial: Updates to the editorial board,2018,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043770161&doi=10.1145%2f3183376&partnerID=40&md5=286d9acab37b568c66fefcb5adc8caad,[No abstract available],,
"Linear time membership in a class of regular expressions with counting, interleaving, and unordered concatenation",2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037818643&doi=10.1145%2f3132701&partnerID=40&md5=f60bd90dedf7137eb085b626253a6566,"Regular Expressions (REs) are ubiquitous in database and programming languages. While many applications make use of REs extended with interleaving (shuffle) and unordered concatenation operators, this extension badly affects the complexity of basic operations, and, especially, makes membership NP-hard, which is unacceptable in most practical scenarios. In this article, we study the problem of membership checking for a restricted class of these extended REs, called conflict-free REs, which are expressive enough to cover the vast majority of real-world applications.We present several polynomial algorithms for membership checking over conflict-free REs. The algorithms are all polynomial and differ in terms of adopted optimization techniques and in the kind of supported operators. As a particular application, we generalize the approach to check membership of Extensible Markup Language trees into a class of EDTDs (Extended Document Type Definitions) that models the crucial aspects of DTDs (Document Type Definitions) and XSD (XML Schema Definitions) schemas. Results about an extensive experimental analysis validate the efficiency of the presented membership checking techniques. © 2017 ACM.",Regular expressions; Word membership; XML,Hypertext systems; Object oriented programming; Optimization; Pattern matching; XML; Basic operation; Document type definition; Experimental analysis; Optimization techniques; Polynomial algorithm; Regular expressions; Word membership; Xml schema definitions; Computer programming languages
Designing a qery language for rdf: Marrying open and closed worlds,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032913452&doi=10.1145%2f3129247&partnerID=40&md5=1a00c90362fdd9de7099fd1badab04d6,"When querying an Resource Description Framework (RDF) graph, a prominent feature is the possibility of extending the answer to a query with optional information. However, the defnition of this feature in SPARQL-the standard RDF query language-has raised some important issues. Most notably, the use of this feature increases the complexity of the evaluation problem, and its closed-world semantics is in con?ict with the underlying open-world semantics of RDF. Many approaches for fxing such problems have been proposed, the most prominent being the introduction of the semantic notion of weakly monotone SPARQL query. Weakly monotone SPARQL queries have shaped the class of queries that conform to the open-world semantics of RDF. Unfortunately, fnding an e?ective way of restricting SPARQL to the fragment of weakly monotone queries has proven to be an elusive problem. In practice, the most widely adopted fragment for writing SPARQL queries is based on the syntactic notion of well designedness. This notion has proven to be a good approach for writing SPARQL queries, but its expressive power has yet to be fully understood. The starting point of this article is to understand the relation between well-designed queries and the semantic notion of weak monotonicity. It is known that every well-designed SPARQL query is weakly monotone; as our frst contribution we prove that the converse does not hold, even if an extension of this notion based on the use of disjunction is considered. Given this negative result, we embark on the task of defning syntactic fragments that are weakly monotone and have higher expressive power than the fragment of well-designed queries. To this end, we move to a more general scenario where infnite RDF graphs are also allowed, so interpolation techniques studied for frst-order logic can be applied. With the use of these techniques, we are able to defne a new operator for SPARQL that gives rise to a query language with the desired properties (over fnite and infnite RDF graphs). It should be noticed that every query in this fragment is weakly monotone if we restrict the semantics to fnite RDF graphs. Moreover, we use this result to provide a simple characterization of the class of monotone CONSTRUCT queries, that is, the class of SPARQL queries that produce RDF graphs as output. Finally, we pinpoint the complexity of the evaluation problem for the query languages identifed in the article. © 2017 Copyright is held by the owner/author(s).",Monotonicity; Open-world assumption; Query languages; RDF; Semantic Web; SPARQL,Graphic methods; Query languages; Semantic Web; Semantics; Syntactics; Evaluation problems; Interpolation techniques; Monotonicity; Open world assumption; Prominent features; RDF query language; Resource description framework; SPARQL; Query processing
Blazes: Coordination analysis and placement for distributed programs,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032951961&doi=10.1145%2f3110214&partnerID=40&md5=c8c70b44cd3895fcdeab87e58c4f46dd,"Distributed consistency is perhaps the most-discussed topic in distributed systems today. Coordination protocols can ensure consistency, but in practice they cause undesirable performance unless used judiciously. Scalable distributed architectures avoid coordination whenever possible, but under-coordinated systems can exhibit behavioral anomalies under fault, which are often extremely difcult to debug. This raises signifcant challenges for distributed system architects and developers. In this article, we present Blazes, a cross-platform program analysis framework that (a) identifes program locations that require coordination to ensure consistent executions, and (b) automatically synthesizes application-specifc coordination code that can signifcantly outperform general-purpose techniques. We present two case studies, one using annotated programs in the Twitter Storm system and another using the Bloom declarative language. © 2017 Copyright is held by the owner/author(s).",Consistency via Coordination,Application programs; Consistency via Coordination; Coordinated system; Coordination analysis; Coordination protocols; Declarative Languages; Distributed architecture; Distributed program; Distributed systems; Program debugging
Priv bayes: Private data release via Bayesian networks,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032887124&doi=10.1145%2f3134428&partnerID=40&md5=17fca00586b07b61ac190b2fcdef4467,"Privacy-preserving data publishing is an important problem that has been the focus of extensive study. The state-of-the-art solution for this problem is differential privacy, which offers a strong degree of privacy protection without making restrictive assumptions about the adversary. Existing techniques using differential privacy, however, cannot effectively handle the publication of high-dimensional data. In particular, when the input dataset contains a large number of attributes, existing methods require injecting a prohibitive amount of noise compared to the signal in the data, which renders the published data next to useless. To address the defciency of the existing methods, this paper presents PrivBayes, a differentially private method for releasing high-dimensional data. Given a dataset D, PrivBayes frst constructs a Bayesian network N, which (i) provides a succinct model of the correlations among the attributes in D and (ii) allows us to approximate the distribution of data in D using a set P of low-dimensional marginals of D. After that, PrivBayes injects noise into each marginal in P to ensure differential privacy and then uses the noisy marginals and the Bayesian network to construct an approximation of the data distribution in D. Finally, PrivBayes samples tuples from the approximate distribution to construct a synthetic dataset, and then releases the synthetic data. Intuitively, PrivBayes circumvents the curse of dimensionality, as it injects noise into the low-dimensional marginals in P instead of the high-dimensional dataset D. Private construction of Bayesian networks turns out to be signifcantly challenging, and we introduce a novel approach that uses a surrogate function for mutual information to build the model more accurately. We experimentally evaluate PrivBayes on real data and demonstrate that it signifcantly outperforms existing solutions in terms of accuracy. © 2017 Copyright is held by the owner/author(s).",Bayesian network; Differential privacy; Synthetic data generation,Clustering algorithms; Data privacy; Publishing; Curse of dimensionality; Differential privacies; High dimensional data; High-dimensional dataset; Mutual informations; Privacy Preserving Data Publishing; Surrogate function; Synthetic data generations; Bayesian networks
Declarative probabilistic programming with datalog,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032969151&doi=10.1145%2f3132700&partnerID=40&md5=6623fa625ac15246d85f9da183d66d33,"Probabilistic programming languages are used for developing statistical models. They typically consist of two components: a specifcation of a stochastic process (the prior) and a specifcation of observations that restrict the probability space to a conditional subspace (the posterior). Use cases of such formalisms include the development of algorithms in machine learning and artifcial intelligence. In this article, we establish a probabilistic-programming extension of Datalog that, on the one hand, allows for defning a rich family of statistical models, and on the other hand retains the fundamental properties of declarativity. Our proposed extension provides mechanisms to include common numerical probability functions; in particular, conclusions of rules may contain values drawn from such functions. The semantics of a program is a probability distribution over the possible outcomes of the input database with respect to the program. Observations are naturally incorporated by means of integrity constraints over the extensional and intensional relations. The resulting semantics is robust under different chases and invariant to rewritings that preserve logical equivalence. © 2017 Copyright is held by the owner/author(s).",Chase; Datalog; Declarative; Probabilistic programming; Probability measure space,Computer programming languages; Learning systems; Object oriented programming; Probability; Random processes; Semantics; Stochastic systems; Chase; Datalog; Declarative; Probabilistic programming; Probability measures; Probability distributions
Empty headed: A relational engine for graph processing,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032908414&doi=10.1145%2f3129246&partnerID=40&md5=2647587148e08833874a7dcb7d4af4ff,"There are two types of high-performance graph processing engines: low-and high-level engines. Low-level engines (Galois, PowerGraph, Snap) provide optimized data structures and computation models but require users to write low-level imperative code, hence ensuring that efciency is the burden of the user. In high-level engines, users write in query languages like datalog (SociaLite) or SQL (Grail). High-level engines are easier to use but are orders of magnitude slower than the low-level graph engines. We present EmptyHeaded, a highlevel engine that supports a rich datalog-like query language and achieves performance comparable to that of low-level engines. At the core of EmptyHeaded's design is a new class of join algorithms that satisfy strong theoretical guarantees, but have thus far not achieved performance comparable to that of specialized graph processing engines. To achieve high performance, EmptyHeaded introduces a new join engine architecture, including a novel query optimizer and execution engine that leverage single-instruction multiple data (SIMD) parallelism. With this architecture, EmptyHeaded outperforms high-level approaches by up to three orders of magnitude on graph pattern queries, PageRank, and Single-Source Shortest Paths (SSSP) and is an order of magnitude faster than many low-level baselines. We validate that EmptyHeaded competes with the bestof-breed low-level engine (Galois), achieving comparable performance on PageRank and at most 3× worse performance on SSSP. Finally, we show that the EmptyHeaded design can easily be extended to accommodate a standard resource description framework (RDF) workload, the LUBM benchmark. On the LUBM benchmark, we show that EmptyHeaded can compete with and sometimes outperform two high-level, but specialized RDF baselines (TripleBit and RDF-3X), while outperforming MonetDB by up to three orders of magnitude and LogicBlox by up to two orders of magnitude. © 2017 Copyright is held by the owner/author(s).",Generalized hypertree decomposition; GHD; Graph processing; SIMD; Single-instruction multiple data; Worst-case optimal join,Engines; Graph theory; Optimization; Query languages; Search engines; Graph processing; Hypertree decomposition; Resource description framework; SIMD; Single instruction multiple data; Single source shortest paths; Theoretical guarantees; Three orders of magnitude; High level languages
BonXai: Combining the simplicity of DTD with the expressiveness of XML schema,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029792629&doi=10.1145%2f3105960&partnerID=40&md5=bf3323c6905ecb8d0212221e0f9b1ebb,"While the migration from DTD to XML Schema was driven by a need for increased expressivity and flexibility, the latter was also significantly more complex to use and understand. Whereas DTDs are characterized by their simplicity, XML Schema Documents are notoriously difficult. In this article, we introduce the XML specification language BonXai, which incorporates many features of XML Schema but is arguably almost as easy to use as DTDs. In brief, the latter is achieved by sacrificing the explicit use of types in favor of simple patterns expressing contexts for elements. The goal of BonXai is not to replace XML Schema but rather to provide a simpler alternative for users who want to go beyond the expressiveness and features of DTD but do not need the explicit use of types. Furthermore, XML Schema processing tools can be used as a backend for BonXai, since BonXai can be automatically converted into XML Schema. A particularly strong point of BonXai is its solid foundation rooted in a decade of theoretical work around pattern-based schemas. We present a formal model for a core fragment of BonXai and the translation algorithms to and from a core fragment of XML Schema. We prove that BonXai and XML Schema can be converted back-and-forth on the level of tree languages and we formally study the size trade-offs between the two languages. © 2017 ACM.",BonXai; Schema languages; XML; XML Schema,Economic and social effects; Specification languages; BonXai; Formal model; Processing tools; Schema language; Translation algorithms; Tree languages; XML schemas; XML specifications; XML
"Qery nesting, assignment, and aggregation in SPARQL 1.1",2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028539806&doi=10.1145%2f3083898&partnerID=40&md5=e354cb20fd4e33e01559c50ffb201244,"Answering aggregate queries is a key requirement of emerging applications of Semantic Technologies, such as data warehousing, business intelligence, and sensor networks. To fulfil the requirements of such applications, the standardization of SPARQL 1.1 led to the introduction of a wide range of constructs that enable value computation, aggregation, and query nesting. In this article, we provide an in-depth formal analysis of the semantics and expressive power of these new constructs as defined in the SPARQL 1.1 specification, and hence lay the necessary foundations for the development of robust, scalable, and extensible query engines supporting complex numerical and analytics tasks. © 2017 ACM.",SPARQL,Data warehouses; Semantics; Sensor networks; Aggregate queries; Emerging applications; Expressive power; Formal analysis; Query engines; Semantic technologies; SPARQL; SPARQL 1.1; Query processing
Detecting inclusion dependencies on very many tables,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026919805&doi=10.1145%2f3105959&partnerID=40&md5=c6c6c97386fc85d65eca67cdc0c21ff7,"Detecting inclusion dependencies, the prerequisite of foreign keys, in relational data is a challenging task. Detecting them among the hundreds of thousands or even millions of tables on the web is daunting. Still, such inclusion dependencies can help connect disparate pieces of information on the Web and reveal unknown relationships among tables. With the algorithm Many, we present a novel inclusion dependency detection algorithm, specialized for the very many-but typically small-tables found on the Web. We make use of Bloom filters and indexed bit-vectors to show the feasibility of our approach. Our evaluation on two corpora of Web tables shows a superior runtime over known approaches and its usefulness to reveal hidden structures on the Web.",Data profiling; Foreign key discovery; Inclusion dependency discovery; Web data management,
On the hardness and approximation of euclidean DBSCAN,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026922299&doi=10.1145%2f3083897&partnerID=40&md5=87d7aa5873cd38cfedc810b5de5be847,"DBSCAN is a method proposed in 1996 for clustering multi-dimensional points, and has received extensive applications. Its computational hardness is still unsolved to this date. The original KDD'96 paper claimed an algorithm of O(n logn) ""average runtime complexity"" (where n is the number of data points) without a rigorous proof. In 2013, a genuineO(n logn)-time algorithm was found in 2D space under Euclidean distance. Thehardness of dimensionality d ≥ 3 has remained open ever since. This article considers the problem of computing DBSCAN clusters from scratch (assuming no existing indexes) under Euclidean distance.We prove that, for d ≥ 3, the problem requires Ω(n4/3) time to solve, unless very significant breakthroughs-ones widely believed to be impossible-could be made in theoretical computer science. Motivated by this, we propose a relaxed version of the problem called ρ-approximate DBSCAN, which returns the same clusters as DBSCAN, unless the clusters are ""unstable"" (i.e., they change once the input parameters are slightly perturbed). The ρ-approximate problem can be settled in O(n) expected time regardless of the constant dimensionality d. The article also enhances the previous result on the exact DBSCAN problem in 2D space. We show that, if the n data points have been pre-sorted on each dimension (i.e., one sorted list per dimension), the problem an be settled in O(n) worst-case time. As a corollary, when all the coordinates are integers, the 2D DBSCAN problem can be solved in O(n log logn) time deterministically, improving the existing O(n logn) bound. © 2017 ACM.",Algorithms; Computational geometry; DBSCAN; Density-based clustering; Hopcroft hard,
"DBSCAN revisited, revisited: Why and how you should (still) use DBSCAN",2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026903118&doi=10.1145%2f3068335&partnerID=40&md5=a19827026f2d7cc8a5e26a1a7f5b182b,"At SIGMOD 2015, an article was presented with the title ""DBSCAN Revisited: Mis-Claim, Un-Fixability, and Approximation"" that won the conference's best paper award. In this technical correspondence, we want to point out some inaccuracies in the way DBSCAN was represented, and why the criticism should have been directed at the assumption about the performance of spatial index structures such as R-trees and not at an algorithm that can use such indexes. We will also discuss the relationship of DBSCAN performance and the indexability of the dataset, and discuss some heuristics for choosing appropriate DBSCAN parameters. Some indicators of bad parameters will be proposed to help guide future users of this algorithm in choosing parameters such as to obtain both meaningful results and good performance. In new experiments, we show that the new SIGMOD 2015 methods do not appear to offer practical benefits if the DBSCAN parameters are well chosen and thus they are primarily of theoretical interest. In conclusion, the original DBSCAN algorithm with effective indexes and reasonably chosen parameter values performs competitively compared to the method proposed by Gan and Tao. © 2017 ACM.",DBSCAN; Density-based clustering; Range-search complexity,
Efficient SimRank-based similarity join,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026905010&doi=10.1145%2f3083899&partnerID=40&md5=bc19ac69dbcda9d0fd665b790ee27033,"Graphs have been widely used to model complex data in many real-world applications. Answering vertex join queries over large graphs is meaningful and interesting, which can benefit friend recommendation in social networks and link prediction, and so on. In this article, we adopt ""SimRank"" [13] to evaluate the similarity between two vertices in a large graph because of its generality. Note that ""Simank"" is purely structure dependent, and it does not rely on the domain knowledge. Specifically, we define a SimRank-based join (SRJ) query to find all vertex pairs satisfying the threshold from two sets of vertices U and V. To reduce the search space, we propose a shortest-path-distance-based upper bound for SimRank scores to prune unpromising vertex pairs. In the verification, we propose a novel index, called h-go cover+, to efficiently compute the SimRank score of any single vertex pair. Given a graphG, we only materialize the SimRank scores of a small proportion of vertex pairs (i.e., the h-go cover+ vertex pairs), based on which the SimRank score of any vertex pair can be computed easily. To find the h-go cover+ vertex pairs, we propose an efficient method without building the vertex-pair graph. Hence, large graphs can be dealt with easily. Extensive experiments over both real and synthetic datasets confirm the efficiency of our solution. © 2016 ACM.",Index-based compution; Similarity; SimRank; Upper bound,
Fast and accurate time-series clustering,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048741828&doi=10.1145%2f3044711&partnerID=40&md5=ff2463b69e653a6fd786ed186a843d67,"The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data-mining methods, not only due to its exploratory power but also because it is often a preprocessing step or subroutine for other techniques. In this article, we present k-Shape and k-MultiShapes (k-MS), two novel algorithms for time-series clustering. k-Shape and k-MS rely on a scalable iterative refinement procedure. As their distance measure, k-Shape and k-MS use shape-based distance (SBD), a normalized version of the cross-correlation measure, to consider the shapes of time series while comparing them. Based on the properties of SBD, we develop two new methods, namely ShapeExtraction (SE) and MultiShapesExtraction (MSE), to compute cluster centroids that are used in every iteration to update the assignment of time series to clusters. k-Shape relies on SE to compute a single centroid per cluster based on all time series in each cluster. In contrast, k-MS relies on MSE to compute multiple centroids per cluster to account for the proximity and spatial distribution of time series in each cluster. To demonstrate the robustness of SBD, k-Shape, and k-MS, we perform an extensive experimental evaluation on 85 datasets against state-of-the-art distance measures and clustering methods for time series using rigorous statistical analysis. SBD, our efficient and parameter-free distance measure, achieves similar accuracy to Dynamic Time Warping (DTW), a highly accurate but computationally expensive distance measure that requires parameter tuning. For clustering, we compare k-Shape and kMS against scalable and non-scalable partitional, hierarchical, spectral, density-based, and shapelet-based methods, with combinations of the most competitive distance measures. k-Shape outperforms all scalable methods in terms of accuracy. Furthermore, k-Shape also outperforms all non-scalable approaches, with one exception, namely k-medoids with DTW, which achieves similar accuracy. However, unlike k-Shape, this approach requires tuning of its distance measure and is significantly slower than k-Shape. k-MS performs similarly to k-Shape in comparison to rival methods, but k-MS is significantly more accurate than k-Shape. Beyond clustering, we demonstrate the effectiveness of k-Shape to reduce the search space of one-nearest-neighbor classifiers for time series. Overall, SBD, k-Shape, and k-MS emerge as domain-independent, highly accurate, and efficient methods for time-series comparison and clustering with broad applications. © 2017 ACM 0362-5915/2017/06-ART8 $15.00",Distance measures; Time-series classification; Time-series clustering,Clustering algorithms; Data mining; Iterative methods; Nearest neighbor search; Scalability; Time series; Distance measure; Dynamic time warping; Experimental evaluation; Iterative refinement; Nearest Neighbor classifier; Time series classifications; Time series clustering; Time-series comparisons; Time series analysis
COMPRESS: A comprehensive framework of trajectory compression in road networks,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019852961&doi=10.1145%2f3015457&partnerID=40&md5=c13125e28b539f61b27d000fece16797,"More and more advanced technologies have become available to collect and integrate an unprecedented amount of data from multiple sources, including GPS trajectories about the traces of moving objects. Given the fact that GPS trajectories are vast in size while the information carried by the trajectories could be redundant, we focus on trajectory compression in this article. As a systematic solution, we propose a comprehensive framework, namely, COMPRESS (Comprehensive Paralleled Road-Network-Based Trajectory Compression), to compress GPS trajectory data in an urban road network. In the preprocessing step, COMPRESS decomposes trajectories into spatial paths and temporal sequences, with a thorough justification for trajectory decomposition. In the compression step, COMPRESS performs spatial compression on spatial paths, and temporal compression on temporal sequences in parallel. It introduces two alternative algorithms with different strengths for lossless spatial compression and designs lossy but error-bounded algorithms for temporal compression. It also presents query processing algorithms to support error-bounded location-based queries on compressed trajectories without full decompression. All algorithms under COMPRESS are efficient and have the time complexity of O(|T|), where |T| is the size of the input trajectory T. We have also conducted a comprehensive experimental study to demonstrate the effectiveness of COMPRESS, whose compression ratio is significantly better than related approaches. © 2017 ACM.",Algorithms; Design; Performance,Algorithms; Design; Motor transportation; Query languages; Roads and streets; Transportation; Advanced technology; Alternative algorithms; Performance; Pre-processing step; Query processing algorithms; Spatial compression; Temporal compressions; Urban road networks; Trajectories
An indexing framework for queries on probabilistic graphs,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019644218&doi=10.1145%2f3044713&partnerID=40&md5=f68881b69ea357b1e472c1cfe0d5ed97,"Information in many applications, such as mobile wireless systems, social networks, and road networks, is captured by graphs. In many cases, such information is uncertain. We study the problem of querying a probabilistic graph, in which vertices are connected to each other probabilistically. In particular, we examine ""source-To-Target"" queries (ST-queries), such as computing the shortest path between two vertices. The major difference with the deterministic setting is that query answers are enriched with probabilistic annotations. Evaluating ST-queries over probabilistic graphs is #P-hard, as it requires examining an exponential number of ""possible worlds""-database instances generated from the probabilistic graph. Existing solutions to the STquery problem, which sample possible worlds, have two downsides: (i) a possible world can be very large and (ii) many samples are needed for reasonable accuracy. To tackle these issues, we study the ProbTree, a data structure that stores a succinct, or indexed, version of the possible worlds of the graph. Existing ST-query solutions are executed on top of this structure, with the number of samples and sizes of the possible worlds reduced. We examine lossless and lossy methods for generating the ProbTree, which reflect the tradeoff between the accuracy and efficiency of query evaluation. We analyze the correctness and complexity of these approaches. Our extensive experiments on real datasets show that the ProbTree is fast to generate and small in size. It also enhances the accuracy and efficiency of existing ST-query algorithms significantly. © 2017 ACM.",Reachability; Shortest path; SPQR; Tree decomposition; Treewidth; Triconnected component; Uncertain graph,Efficiency; Query languages; Query processing; Trees (mathematics); Wireless networks; Reachability; Shortest path; SPQR; Tree decomposition; Tree-width; Triconnected component; Uncertain graphs; Graph theory
Consistent query answering for self-join-free conjunctive queries under primary key constraints,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027006807&doi=10.1145%2f3068334&partnerID=40&md5=3c7a4614d82b83306327c8248246b654,"A relational database is said to be uncertain if primary key constraints can possibly be violated. A repair (or possible world) of an uncertain database is obtained by selecting a maximal number of tuples without ever selecting two distinct tuples with the same primary key value. For any Boolean query q, CERTAINTY(q) is the problem that takes an uncertain database db as input and asks whether q is true in every repair of db. The complexity of this problem has been particularly studied for q ranging over the class of self-join-free Boolean conjunctive queries. A research challenge is to determine, given q, whether CERTAINTY(q) belongs to complexity classes FO, P, or coNP-complete. In this article, we combine existing techniques for studying this complexity classification task. We show that, for any self-join-free Boolean conjunctive query q, it can be decided whether or not CERTAINTY(q) is in FO. We additionally show how to construct a single SQL query for solving CERTAINTY(q) if it is in FO. Further, for any self-join-free Boolean conjunctive query q, CERTAINTY(q) is either in P or coNP-complete and the complexity dichotomy is effective. This settles a research question that has been open for 10 years. © 2017 ACM.",Attack graph; Complexity dichotomy; Conjunctive queries; Consistent query answering; Primary keys,Query languages; Repair; Attack graph; Complexity dichotomies; Conjunctive queries; Consistent query answering; Primary keys; Query processing
Outlier detection over massive-scale trajectory streams,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018735073&doi=10.1145%2f3013527&partnerID=40&md5=eb19e0f1a6f6ce679650393e577c01a2,"The detection of abnormal moving objects over high-volume trajectory streams is critical for real-time applications ranging from military surveillance to transportation management. Yet this outlier detection problem, especially along both the spatial and temporal dimensions, remains largely unexplored. In this work, we propose a rich taxonomy of novel classes of neighbor-based trajectory outlier definitions that model the anomalous behavior of moving objects for a large range of real-time applications. Our theoretical analysis and empirical study on two real-world datasets-the Beijing Taxi trajectory data and the Ground Moving Target Indicator data stream- and one generated Moving Objects dataset demonstrate the effectiveness of our taxonomy in effectively capturing different types of abnormal moving objects. Furthermore, we propose a general strategy for efficiently detecting these new outlier classes called the minimal examination (MEX) framework. The MEX framework features three core optimization principles, which leverage spatiotemporal as well as the predictability properties of the neighbor evidence to minimize the detection costs. Based on this foundation, we design algorithms that detect the outliers based on these classes of new outlier semantics that successfully leverage our optimization principles. Our comprehensive experimental study demonstrates that our proposed MEX strategy drives the detection costs 100-fold down into the practical realm for applications that analyze high-volume trajectory streams in near real time. © 2017 ACM.",Moving objects; Outlier detection; Trajectory stream,Data handling; Data mining; Digital storage; Military applications; Semantics; Statistics; Taxicabs; Taxonomies; Tracking (position); Trajectories; Ground moving target indicator; Military surveillance; Moving objects; Optimization principle; Outlier Detection; Real-time application; Trajectory streams; Transportation management; Object detection
Approximation algorithms for schema-mapping discovery from data examples,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018774726&doi=10.1145%2f3044712&partnerID=40&md5=2673b99cec12adf641313f94c722f4d0,"In recent years, data examples have been at the core of several different approaches to schema-mapping design. In particular, Gottlob and Senellart introduced a framework for schema-mapping discovery from a single data example, in which the derivation of a schema mapping is cast as an optimization problem. Our goal is to refine and study this framework in more depth. Among other results, we design a polynomial-time log(n)-approximation algorithm for computing optimal schema mappings from a given set of data examples (where nis the combined size of the given data examples) fora restricted class of schema mappings; moreover, we show that this approximation ratio cannot be improved. In addition to the complexity-theoretic results, we implemented the aforementioned log(n)-approximation algorithm and carried outanexperimental evaluation in a real-world mapping scenario. © 2017 ACM.",Approximation algorithms; Data examples; Schema mappings,Optimization; Polynomial approximation; Approximation ratios; Data examples; Mapping scenarios; Optimization problems; Polynomial-time; Real-world; Schema mappings; Approximation algorithms
Exact model counting of query expressions: Limitations of propositional methods,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015669296&doi=10.1145%2f2984632&partnerID=40&md5=f17561f9fe90326dd7d4a4b3a9d4ab34,"We prove exponential lower bounds on the running time of the state-of-the-art exact model counting algorithms-algorithms for exactly computing the number of satisfying assignments, or the satisfying probability, of Boolean formulas. These algorithms can be seen, either directly or indirectly, as building Decision-Decomposable Negation Normal Form (decision-DNNF) representations of the input Boolean formulas. Decision-DNNFs are a special case of d-DNNFs where d stands for deterministic. We show that any knowledge compilation representations from a class (called DLDDs in this article) that contain decision- DNNFs can be converted into equivalent Free Binary Decision Diagrams (FBDDs), also known as Read-Once Branching Programs, with only a quasi-polynomial increase in representation size. Leveraging known exponential lower bounds for FBDDs, we then obtain similar exponential lower bounds for decision-DNNFs, which imply exponential lower bounds for model-counting algorithms.We also separate the power of decision- DNNFs from d-DNNFs and a generalization of decision-DNNFs known as AND-FBDDs. We then prove new lower bounds for FBDDs that yield exponential lower bounds on the running time of these exact model counters when applied to the problem of query evaluation in tuple-independent probabilistic databases-computing the probability of an answer to a query given independent probabilities of the individual tuples in a database instance. This approach to the query evaluation problem, in which one first obtains the lineage for the query and database instance as a Boolean formula and then performs weighted model counting on the lineage, is known as grounded inference. A second approach, known as lifted inference or extensional query evaluation, exploits the high-level structure of the query as a first-order formula. Although it has been widely believed that lifted inference is strictly more powerful than grounded inference on the lineage alone, no formal separation has previously been shown for query evaluation. In this article, we show such a formal separation for the first time. In particular, we exhibit a family of database queries for which polynomial-time extensional query evaluation techniques were previously known but for which query evaluation via grounded inference using the state-of-the-art exact model counters requires exponential time. © 2017 ACM.",DNNF; FBDD; Knowledge compilation; Lower bounds; Model counting; Probabilistic databases; Read-once branching programs,Binary decision diagrams; Boolean algebra; Boolean functions; Database systems; Equivalence classes; Polynomial approximation; Probability; Query languages; Separation; DNNF; FBDD; Knowledge compilation; Lower bounds; Model Counting; Probabilistic database; Read-once branching programs; Query processing
Editorial: Updates to the editorial board,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015427091&doi=10.1145%2f3041040&partnerID=40&md5=67f64ccf4c6a579293d37715958b248c,[No abstract available],,
Computational fact checking through query perturbations,2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011350060&doi=10.1145%2f2996453&partnerID=40&md5=1b55240c7907e9cf71e7fef2c73436d9,"Our media is saturated with claims of ""facts"" made from data. Database research has in the past focused on how to answer queries, but has not devotedmuch attention to discerningmore subtle qualities of the resulting claims, for example, is a claim ""cherry-picking""? This article proposes a framework that models claims based on structured data as parameterized queries. Intuitively, with its choice of the parameter setting, a claim presents a particular (and potentially biased) view of the underlying data. A key insight is that we can learn a lot about a claim by ""perturbing"" its parameters and seeing how its conclusion changes. For example, a claim is not robust if small perturbations to its parameters can change its conclusions significantly. This framework allows us to formulate practical fact-checking tasks-reverse-engineering vague claims, and countering questionable claims-as computational problems. Along with the modeling framework, we develop an algorithmic framework that enables efficient instantiations of ""meta"" algorithms by supplying appropriate algorithmic building blocks.We present real-world examples and experiments that demonstrate the power of our model, efficiency of our algorithms, and usefulness of their results. © 2017 AC.",Computational journalism; Fact checking; Sensitivity analysis,Natural language processing systems; Query languages; Reverse engineering; Sensitivity analysis; Algorithmic framework; Computational journalism; Computational problem; Database research; Model framework; Parameter setting; Small perturbations; Structured data; Query processing
"Response to ""differential dependencies revisited""",2017,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011340331&doi=10.1145%2f2983602&partnerID=40&md5=59468add110aa277329bd423451d1daf,"A recent article [Vincent et al. 2015] concerns the correctness of several results in reasoning about differential dependencies (DDs), originally reported in Song and Chen [2011]. The major concern by Vincent et al. [2015] roots from assuming a type of infeasible differential functions in the given DDs for consistency and implication analysis, which are not allowed in Song and Chen [2011]. A differential function is said to be infeasible if there is no tuple pair with values that can satisfy the specified distance constraints. For example, [price(〈2, 〉4)] requires the difference of two price values to be 〈 2 and 〉 4 at the same time, which is clearly impossible. Although DDs involving infeasible differential functions may be syntactically interesting, they are semantically meaningless and would neither be specified by domain experts nor discovered from data. For these reasons, infeasible differential functions are not considered [Song and Chen 2011] and the results in Song and Chen [2011] are correct, in contrast to what is claimed in Vincent et al. [2015]. © 2017 ACM.",Data dependencies; Differential dependencies,Database systems; Information systems; Data dependencies; Differential dependencies; Differential functions; Distance constraints; Domain experts; nocv1; Price values; Aluminum
Exact and approximate maximum inner product search with LEMP,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007017172&doi=10.1145%2f2996452&partnerID=40&md5=c6b291cabee84039349078bfe6e87235,"We study exact and approximate methods for maximum inner product search, a fundamental problem in a number of data mining and information retrieval tasks. We propose the LEMP framework, which supports both exact and approximate search with quality guarantees. At its heart, LEMP transforms a maximum inner product search problem over a large database of vectors into a number of smaller cosine similarity search problems. This transformation allows LEMP to prune large parts of the search space immediately and to select suitable search algorithms for each of the remaining problems individually. LEMP is able to leverage existing methods for cosine similarity search, but we also provide a number of novel search algorithms tailored to our setting. We conducted an extensive experimental study that provides insight into the performance of many state-of-the-art techniques - including LEMP - on multiple real-world datasets. We found that LEMP often was significantly faster or more accurate than alternative methods. © 2016 ACM.",,Cosine transforms; Learning algorithms; Approximate methods; Cosine similarity; Large database; Number of datum; Real-world datasets; Search Algorithms; Search problem; State-of-the-art techniques; Data mining
Guarded-based disjunctive tuple-generating dependencies,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996618788&doi=10.1145%2f2976736&partnerID=40&md5=86573f1de6f3172eedb7c486c529a99f,"We perform an in-depth complexity analysis of query answering under guarded-based classes of disjunctive tuple-generating dependencies (DTGDs), focusing on (unions of) conjunctive queries ((U)CQs).We show that the problem under investigation is very hard, namely 2EXPTIME-complete, even for fixed sets of dependencies of a very restricted form. This is a surprising lower bound that demonstrates the enormous impact of disjunction on query answering under guarded-based tuple-generating dependencies, and also reveals the source of complexity for expressive logics such as the guarded fragment of first-order logic. We then proceed to investigate whether prominent subclasses of (U)CQs (i.e., queries of bounded treewidth and hypertreewidth, and acyclic queries) have a positive impact on the complexity of the problem under consideration. We show that queries of bounded treewidth and bounded hypertree-width do not reduce the complexity of our problem, even if we focus on predicates of bounded arity or on fixed sets of DTGDs. Regarding acyclic queries, although the problem remains 2EXPTIME-complete in general, in some relevant settings the complexity reduces to EXPTIME-complete. Finally, with the aim of identifying tractable cases, we focus our attention on atomic queries. We show that atomic queries do not make the query answering problem easier under classes of guarded-based DTGDs that allow more than one atom to occur in the body of the dependencies. However, the complexity significantly decreases in the case of dependencies that can have only one atom in the body. In particular, we obtain a PTIME-completeness if we focus on predicates of bounded arity, and AC0-membership when the set of dependencies and the query are fixed. Interestingly, our results can be used as a generic tool for establishing complexity results for query answering under various description logics. © 2016 ACM.",Computational complexity; Conjunctive queries; Disjunction; Existential rules; Guardedness; Query answering; Tuple-generating dependencies,Computational complexity; Data description; Formal logic; Conjunctive queries; Disjunction; Existential rules; Guardedness; Query answering; Tuple-generating dependencies; Atoms
"Smart meter data analytics: Systems, algorithms, and benchmarking",2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997502455&doi=10.1145%2f3004295&partnerID=40&md5=6c0e60e2764c6ea4e5ffd1b73f27c7bc,"Smart electricity meters have been replacing conventional meters worldwide, enabling automated collection of fine-grained (e.g., every 15 minutes or hourly) consumption data. A variety of smart meter analytics algorithms and applications have been proposed, mainly in the smart grid literature. However, the focus has been on what can be done with the data rather than how to do it efficiently. In this article, we examine smart meter analytics from a software performance perspective. First, we design a performance benchmark that includes common smart meter analytics tasks. These include offline feature extraction and model building as well as a framework for online anomaly detection that we propose. Second, since obtaining real smart meter data is difficult due to privacy issues, we present an algorithm for generating large realistic datasets from a small seed of real data. Third, we implement the proposed benchmark using five representative platforms: a traditional numeric computing platform (Matlab), a relational DBMS with a built-in machine learning toolkit (PostgreSQL/MADlib), a main-memory column store (""System C""), and two distributed data processing platforms (Hive and Spark/Spark Streaming). We compare the five platforms in terms of application development effort and performance on a multicore machine as well as a cluster of 16 commodity servers. © 2016 ACM.",Data analytics; Hadoop; Performance benchmarking; Smart meters; Spark,Artificial intelligence; Benchmarking; Data handling; Distributed computer systems; Electric sparks; Feature extraction; Learning systems; MATLAB; Application development; Data analytics; Distributed data processing; Hadoop; Offline feature extraction; Online anomaly detection; Performance benchmarking; Software performance; Smart meters
Skycube materialization using the topmost skyline or functional dependencies,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996614639&doi=10.1145%2f2955092&partnerID=40&md5=76f4b79957487b6598880afa97de0455,"Given a table T(Id, D1, . . . , Dd), the skycube of T is the set of skylines with respect to to all nonempty subsets (subspaces) of the set of all dimensions {D1, . . . , Dd}. To optimize the evaluation of any skyline query, the solutions proposed so far in the literature either (i) precompute all of the skylines or (ii) use compression techniques so that the derivation of any skyline can be done with little effort. Even though solutions (i) are appealing because skyline queries have optimal execution time, they suffer from time and space scalability because the number of skylines to be materialized is exponential with respect to d. On the other hand, solutions (ii) are attractive in terms of memory consumption, but as we show, they also have a high time complexity. In this article, we make contributions to both kinds of solutions. We first observe that skyline patterns are monotonic. This property leads to a simple yet efficient solution for full and partial skycube materialization when the skyline with respect to all dimensions, the topmost skyline, is small. On the other hand, when the topmost skyline is large relative to the size of the input table, it turns out that functional dependencies, a fundamental concept in databases, uncover a monotonic property between skylines. Equipped with this information, we show that closed attributes sets are fundamental for partial and full skycube materialization. Extensive experiments with real and synthetic datasets show that our solutions generally outperform state-of-the-art algorithms. © 2016 ACM.",Implementation; Materialization,Query processing; Compression techniques; Functional dependency; Fundamental concepts; Implementation; Materialization; Monotonic properties; State-of-the-art algorithms; Synthetic datasets; Indexing (of information)
Joins via geometric resolutions: Worst case and beyond,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996865594&doi=10.1145%2f2967101&partnerID=40&md5=777d998a44d7c2166d0b07b0fbcf62c1,"We present a simple geometric framework for the relational join. Using this framework, we design an algorithm that achieves the fractional hypertree-width bound, which generalizes classical and recent worstcase algorithmic results on computing joins. In addition, we use our framework and the same algorithm to show a series of what are colloquially known as beyond worst-case results. The framework allows us to prove results for data stored in BTrees, multidimensional data structures, and even multiple indices per table. A key idea in our framework is formalizing the inference one does with an index as a type of geometric resolution, transforming the algorithmic problem of computing joins to a geometric problem. Our notion of geometric resolution can be viewed as a geometric analog of logical resolution. In addition to the geometry and logic connections, our algorithm can also be thought of as backtracking search with memoization. © 2016 ACM.",Beyond worst-case analysis; Bounded-width join queries; Indices; Relational join; Resolution,Computation theory; Optical resolving power; Algorithmic problems; Backtracking search; Bounded-width join queries; Fractional hypertree widths; Geometric resolution; Indices; Multidimensional data structure; Worst-case analysis; Geometry
Exploiting integrity constraints for cleaning trajectories of RFID-monitored objects,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996721523&doi=10.1145%2f2939368&partnerID=40&md5=f4011cb5a4f4e6e2a9e548a7d263ab1f,"A probabilistic framework for cleaning the data collected by Radio-Frequency IDentification (RFID) tracking systems is introduced. What has to be cleaned is the set of trajectories that are the possible interpretations of the readings: a trajectory in this set is a sequence whose generic element is a location covered by the reader(s) that made the detection at the corresponding time point. The cleaning is guided by integrity constraints and consists of discarding the inconsistent trajectories and assigning to the others a suitable probability of being the actual one. The probabilities are evaluated by adopting probabilistic conditioning that logically consists of the following steps. First, the trajectories are assigned a priori probabilities that rely on the independence assumption between the time points. Then, these probabilities are revised according to the spatio-temporal correlations encoded by the constraints. This is done by conditioning the a priori probability of each trajectory to the event that the constraints are satisfied: this means taking the ratio of this a priori probability to the sum of the a priori probabilities of all the consistent trajectories. Instead of performing these steps by materializing all the trajectories and their a priori probabilities (which is infeasible, owing to the typically huge number of trajectories), our approach exploits a data structure called conditioned trajectory graph (ct-graph) that compactly represents the trajectories and their conditioned probabilities, and an algorithm for efficiently constructing the ct-graph, which progressively builds it while avoiding the construction of components encoding inconsistent trajectories. © 2016 ACM.",Data cleaning; RFID data,Cleaning; Probability; Radio frequency identification (RFID); A-priori probabilities; Data cleaning; Independence assumption; Integrity constraints; Probabilistic framework; Spatiotemporal correlation; Time points; Tracking system; Trajectories
Extending the kernel of a relational DBMS with comprehensive support for sequenced temporal queries,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996957469&doi=10.1145%2f2967608&partnerID=40&md5=6d80024d4f622dc79438bd666c86f1eb,"Many databases contain temporal, or time-referenced, data and use intervals to capture the temporal aspect. While SQL-based database management systems (DBMSs) are capable of supporting the management of interval data, the support they offer can be improved considerably. A range of proposed temporal data models and query languages offer ample evidence to this effect. Natural queries that are very difficult to formulate in SQL are easy to formulate in these temporal query languages. The increased focus on analytics over historical data where queries are generally more complex exacerbates the difficulties and thus the potential benefits of a temporal query language. Commercial DBMSs have recently started to offer limited temporal functionality in a step-by-step manner, focusing on the representation of intervals and neglecting the implementation of the query evaluation engine. This article demonstrates how it is possible to extend the relational database engine to achieve a fullfledged, industrial-strength implementation of sequenced temporal queries, which intuitively are queries that are evaluated at each time point. Our approach reduces temporal queries to nontemporal queries over data with adjusted intervals, and it leaves the processing of nontemporal queries unaffected. Specifically, the approach hinges on three concepts: interval adjustment, timestamp propagation, and attribute scaling. Interval adjustment is enabled by introducing two new relational operators, a temporal normalizer and a temporal aligner, and the latter two concepts are enabled by the replication of timestamp attributes and the use of so-called scaling functions. By providing a set of reduction rules, we can transform any temporal query, expressed in terms of temporal relational operators, to a query expressed in terms of relational operators and the two new operators.We prove that the size of a transformed query is linear in the number of temporal operators in the original query. An integration of the new operators and the transformation rules, along with query optimization rules, into the kernel of PostgreSQL is reported. Empirical studies with the resulting temporal DBMS are covered that offer insights into pertinent design properties of the article's proposal. The new system is available as open-source software. © 2016 ACM.",Attribute value scaling; Change preservation; Interval adjustment; Query processing; Sequenced semantics; Temporal databases; Timestamp propagation,Engines; Information dissemination; Information management; Mathematical transformations; Open source software; Open systems; Query processing; Relational database systems; Semantics; Software engineering; Attribute values; Interval adjustment; Sequenced semantics; Temporal Database; Time-stamp; Query languages
Building a hybrid warehouse: Efficient joins between data stored in HDFS and enterprise warehouse,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996841798&doi=10.1145%2f2972950&partnerID=40&md5=fc5e1b7b105e18657b8ad9b929915e36,"The Hadoop Distributed File System (HDFS) has become an important data repository in the enterprise as the center for all business analytics, from SQL queries and machine learning to reporting. At the same time, enterprise data warehouses (EDWs) continue to support critical business analytics. This has created the need for a new generation of a special federation between Hadoop-like big data platforms and EDWs, which we call the hybrid warehouse. There are many applications that require correlating data stored in HDFS with EDW data, such as the analysis that associates click logs stored in HDFS with the sales data stored in the database. All existing solutions reach out to HDFS and read the data into the EDW to perform the joins, assuming that the Hadoop side does not have efficient SQL support. In this article, we show that it is actually better to do most data processing on the HDFS side, provided that we can leverage a sophisticated execution engine for joins on the Hadoop side. We identify the best hybrid warehouse architecture by studying various algorithms to join database and HDFS tables. We utilize Bloom filters to minimize the data movement and exploit the massive parallelism in both systems to the fullest extent possible. We describe a new zigzag join algorithm and show that it is a robust join algorithm for hybrid warehouses that performs well in almost all cases. We further develop a sophisticated cost model for the various join algorithms and show that it can facilitate query optimization in the hybrid warehouse to correctly choose the right algorithm under different predicate and join selectivities. © 2016 ACM.",Bloom filter; Cost model; Distributed join; Federation; Hybrid warehouse; Join on Hadoop; Query push-down; SQL-on-Hadoop,Artificial intelligence; Data handling; Data structures; Data warehouses; File organization; Learning systems; Optimization; Query processing; Bloom filters; Cost modeling; Federation; Query push-down; SQL-on-Hadoop; Big data
The goal behind the action: Toward goal-aware systems and applications,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996922115&doi=10.1145%2f2934666&partnerID=40&md5=0c4ce7635e150a194bfe122281ce4298,"Human activity is almost always intentional, be it in a physical context or as part of an interaction with a computer system. By understanding why user-generated events are happening and what purposes they serve, a system can offer a significantly improved and more engaging experience. However, goals cannot be easily captured. Analyzing user actions such as clicks and purchases can reveal patterns and behaviors, but understanding the goals behind these actions is a different and challenging issue. Our work presents a unified, multidisciplinary viewpoint for goal management that covers many different cases where goals can be used and techniques with which they can be exploited. Our purpose is to provide a common reference point to the concepts and challenging tasks that need to be formally defined when someone wants to approach a data analysis problem from a goal-oriented point of view. This work also serves as a springboard to discuss several open challenges and opportunities for goal-oriented approaches in data management, analysis, and sharing systems and applications. © 2016 ACM.",Actions; Goal-aware; Intention; Interaction; Interactive systems; Operationalization; Plans; Queries; Recommenders; Retrieval,Formal concept analysis; Human computer interaction; Actions; Goal-aware; Intention; Interaction; Interactive system; Operationalization; Plans; Queries; Recommenders; Retrieval; Information management
DBMS metrology: Measuring query time,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997241532&doi=10.1145%2f2996454&partnerID=40&md5=ae8d6a2314cadc4d098db13cf2460022,"It is surprisingly hard to obtain accurate and precise measurements of the time spent executing a query because there are many sources of variance. To understand these sources, we review relevant per-process and overall measures obtainable from the Linux kernel and introduce a structural causal model relating these measures. A thorough correlational analysis provides strong support for this model. We attempted to determine why a particular measurement wasn't repeatable and then to devise ways to eliminate or reduce that variance. This enabled us to articulate a timing protocol that applies to proprietary DBMSes, that ensures the repeatability of a query, and that obtains a quite accurate query execution time while dropping very few outliers. This resulting query time measurement procedure, termed the Tucson Timing Protocol Version 2 (TTPv2), consists of the following steps: (i) perform sanity checks to ensure data validity; (ii) drop some query executions via clearly motivated predicates; (iii) drop some entire queries at a cardinality, again via clearly motivated predicates; (iv) for those that remain, compute a single measured time by a carefully justified formula over the underlying measures of the remaining query executions; and (v) perform post-analysis sanity checks. The result is a mature, general, robust, self-checking protocol that provides a more precise and more accurate timing of the query. The protocol is also applicable to other operating domains in which measurements of multiple processes each doing computation and I/O is needed. © 2016 ACM.",Accuracy; Database ergalics; Repeatability; Tucson timing protocol,Computer operating systems; Drops; Relational database systems; Accuracy; Correlational analysis; Measurement procedures; Measurements of; Multiple process; Precise measurements; Query execution time; Repeatability; Query processing
UniAD: A unified Ad hoc data processing system,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997343236&doi=10.1145%2f3009957&partnerID=40&md5=c3b45ff59b60f426e0877a420e40dd96,"Instead of constructing complex declarative queries, many users prefer to write their programs using procedural code embedded with simple queries. Since many users are not expert programmers or the programs are written in a rush, these programs usually exhibit poor performance in practice and it is a challenge to automatically and efficiently optimize these programs. In this article, we present UniAD, which stands for Unified execution for Ad hoc Data processing, a system designed to simplify the programming of data processing tasks and provide efficient execution for user programs. We provide the background of program semantics and propose a novel intermediate representation, called Unified Intermediate Representation (UniIR), which utilizes a simple and expressive mechanism HOQ to describe the operations performed in programs. By combining both procedural and declarative logics with the proposed intermediate representation, we can perform various optimizations across the boundary between procedural and declarative code. We propose a transformation-based optimizer to automatically optimize programs and implement the UniAD system. The extensive experimental results on various benchmarks demonstrate that our techniques can significantly improve the performance of a wide range of data processing programs. © 2016 ACM.",Ad hoc data processing; Design; F.3.2 [logics and meanings of programs]: semantics of programming languages - program analysis; H.2.4 [database management]: systems - query processing; Languages; Performance; Program semantics; Query optimization,Benchmarking; Computer programming; Design; Information management; Query languages; Query processing; Search engines; Semantics; XML; Performance; Program semantics; Query optimization; Semantics of programming languages; Systems-query processing; Data handling
Private and scalable execution of SQL aggregates on a secure decentralized architecture,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981517762&doi=10.1145%2f2894750&partnerID=40&md5=ef422168380018948af8f2982f2a3661,"Current applications, from complex sensor systems (e.g., quantified self) to online e-markets, acquire vast quantities of personal information that usually end up on central servers where they are exposed to prying eyes. Conversely, decentralized architectures that help individuals keep full control of their data complexify global treatments and queries, impeding the development of innovative services. This article aims precisely at reconciling individual's privacy on one side and global benefits for the community and business perspectives on the other. It promotes the idea of pushing the security to secure hardware devices controlling the data at the place of their acquisition. Thanks to these tangible physical elements of trust, secure distributed querying protocols can reestablish the capacity to perform global computations, such as Structured Query Language (SQL) aggregates, without revealing any sensitive information to central servers. This article studies how to secure the execution of such queries in the presence of honest-but-curious and malicious attackers. It also discusses how the resulting querying protocols can be integrated in a concrete decentralized architecture. Cost models and experiments on SQL/Asymmetric Architecture (AA), our distributed prototype running on real tamper-resistant hardware, demonstrate that this approach can scale to nationwide applications. © 2016 ACM.",Decentralized architecture; Parallel computing; Trusted hardware,Aggregates; Computer hardware; Hardware; Network architecture; Online systems; Parallel processing systems; Query languages; Reconfigurable hardware; Business perspective; Decentralized architecture; Personal information; Physical elements; Sensitive informations; Structured query languages; Tamper resistant; Trusted hardwares; Hardware security
Scalable atomic visibility with ramp transactions,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057775592&doi=10.1145%2f2909870&partnerID=40&md5=91d75dde866a0d1f431ba36426bc3444,"Databases can provide scalability by partitioning data across several servers. However, multipartition, mul-tioperation transactional access is often expensive, employing coordination-intensive locking, validation, or scheduling mechanisms. Accordingly, many real-world systems avoid mechanisms that provide useful semantics for multipartition operations. This leads to incorrect behavior for a large class of applications including secondary indexing, foreign key enforcement, and materialized view maintenance. In this work, we identify a new isolation model'Read Atomic (RA) isolation'that matches the requirements of these use cases by ensuring atomic visibility: either all or none of each transaction's updates are observed by other transactions. We present algorithms for Read Atomic Multipartition (RAMP) transactions that enforce atomic visibility while offering excellent scalability, guaranteed commit despite partial failures (via coordination-free execution), and minimized communication between servers (via partition independence). These RAMP transactions correctly mediate atomic visibility of updates and provide readers with snapshot access to database state by using limited multiversioning and by allowing clients to independently resolve nonatomic reads. We demonstrate that, in contrast with existing algorithms, RAMP transactions incur limited overhead'even under high contention'and scale linearly to 100 servers. © 2016 ACM 0362-5915/2016/07-ART15 $15.00",Atomic visibility; Materialized views; NoSQL; Secondary indexing; Transaction processing,Data handling; Indexing (materials working); Indexing (of information); Locks (fasteners); Scalability; Semantics; Visibility; Access to database; High contentions; Materialized view; Materialized view maintenance; NoSQL; Real-world system; Scheduling mechanism; Transaction processing; Atoms
Monadic datalog and regular tree pattern queries,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978129443&doi=10.1145%2f2925986&partnerID=40&md5=8f2db2ff02376db335231206c0b64b18,"Containment of monadic datalog programs over trees is decidable. The situation is more complex when tree nodes carry labels from an infinite alphabet that can be tested for equality. It then matters whether the descendant relation is allowed or not: the descendant relation can be eliminated easily from monadic programs only when label equalities are not used. With descendant, even containment of linear monadic programs in unions of conjunctive queries is undecidable, and positive results are known only for boundeddepth trees. We show that without descendant, containment of connected monadic programs is decidable over ranked trees, but over unranked trees it is so only for linear programs. With descendant, it becomes decidable over unranked trees under restriction to downward programs: each rule only moves down from the node in the head. This restriction is motivated by regular tree pattern queries, a recent formalism in the area of ActiveXML, which we show to be equivalent to linear downward programs. © 2016 ACM.",Containment problem; Recursive queries; Semistructured data; Trees,Computability and decidability; Forestry; Linear programming; Conjunctive queries; Containment problem; Datalog programs; Descendant relation; Linear programs; Recursive queries; Semi structured data; Trees; Trees (mathematics)
Editorial: The dark citations of TODS papers and what to do about it-or: Cite the journal paper,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978123629&doi=10.1145%2f2946798&partnerID=40&md5=d7c047989293e41de257e34c233ceff9,[No abstract available],,
Sloth: Being lazy is a virtue (when issuing database queries),2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975254967&doi=10.1145%2f2894749&partnerID=40&md5=a7235f89ecc364d8aa519a4e682fc763,"Many web applications store persistent data in databases. During execution, such applications spend a significant amount of time communicating with the database for retrieval and storing of persistent data over the network. These network round-trips represent a significant fraction of the overall execution time for many applications (especially those that issue a lot of database queries) and, as a result, increase their latency. While there has been prior work that aims to eliminate round-trips by batching queries, they are limited by (1) a requirement that developers manually identify batching opportunities, or (2) the fact that they employ static program analysis techniques that cannot exploit many opportunities for batching, as many of these opportunities require knowing precise information about the state of the running program. In this article, we present SLOTH, a new system that extends traditional lazy evaluation to expose query batching opportunities during application execution, even across loops, branches, and method boundaries. Many such opportunities often require expensive and sophisticated static analysis to recognize from the application source code. Rather than doing so, SLOTH instead makes use of dynamic analysis to capture information about the program state and, based on that information, decides how to batch queries and when to issue them to the database. We formalize extended lazy evaluation and prove that it preserves program semantics when executed under standard semantics. Furthermore, we describe our implementation of SLOTH and our experience in evaluating SLOTH using over 100 benchmarks from two large-scale open-source applications, in which SLOTH achieved up to a 3× reduction in page load time by delaying computation using extended lazy evaluation. © 2016 ACM.",Application performance; Lazy evaluation; Query optimization,Benchmarking; Computer programming; Database systems; Open source software; Query processing; Semantics; Static analysis; Application execution; Application performance; Lazy evaluation; Open source application; Overall execution; Program semantics; Query optimization; Static program analysis; Query languages
Bounded repairability for regular tree languages,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978069657&doi=10.1145%2f2898995&partnerID=40&md5=f3dc6e909eb3193d701a789aeb4c9359,"We study the problem of bounded repairability of a given restriction tree language R into a target tree language T. More precisely, we say that R is bounded repairable with respect to T if there exists a bound on the number of standard tree editing operations necessary to apply to any tree in R to obtain a tree in T. We consider a number of possible specifications for tree languages: bottom-up tree automata (on curry encoding of unranked trees) that capture the class of XML schemas and document type definitions (DTDs). We also consider a special case when the restriction language R is universal (i.e., contains all trees over a given alphabet). We give an effective characterization of bounded repairability between pairs of tree languages represented with automata. This characterization introduces two tools-synopsis trees and a coverage relation between them-allowing one to reason about tree languages that undergo a bounded number of editing operations. We then employ this characterization to provide upper bounds to the complexity of deciding bounded repairability and show that these bounds are tight. In particular, when the input tree languages are specified with arbitrary bottom-up automata, the problem is CONEXP-complete. The problem remains CONEXP-complete even if we use deterministic nonrecursive DTDs to specify the input languages. The complexity of the problem can be reduced if we assume that the alphabet, the set of node labels, is fixed: the problem becomes PSPACE-complete for nonrecursive DTDs and CONP-complete for deterministic nonrecursive DTDs. Finally, when the restriction tree language R is universal, we show that the bounded repairability problem becomes EXP-complete if the target language is specified by an arbitrary bottom-up tree automaton and becomes tractable (P-complete, in fact) when a deterministic bottom-up automaton is used. © 2016 ACM.",DTD; Edit distance; Repair; XML; XML schema,Automata theory; Characterization; Computational linguistics; Repair; XML; Coverage relations; Document type definition; Edit distance; Editing operations; PSPACE-complete; Regular tree languages; Target language; XML schemas; Forestry
Plan bouquets: A fragrant approach to robust query processing,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971291462&doi=10.1145%2f2901738&partnerID=40&md5=2f0d8be9f5b729bbfead4bd6e144c33b,"Identifying efficient execution plans for declarative OLAP queries typically entails estimation of several predicate selectivities. In practice, these estimates often differ significantly from the values actually encountered during query execution, leading to poor plan choices and grossly inflated response times. We propose here a conceptually new approach to address this classical problem, wherein the compile-time estimation process is completely eschewed for error-prone selectivities. Instead, from the set of optimal plans in the query's selectivity error space, a limited subset, called the ""plan bouquet,"" is selected such that at least one of the bouquet plans is 2-optimal at each location in the space. Then, at run time, a sequence of cost-budgeted executions from the plan bouquet is carried out, eventually finding a plan that executes to completion within its assigned budget. The duration and switching of these executions is controlled by a graded progression of isosurfaces projected onto the optimal performance profile. We prove that this construction results, for the first time, in guarantees on worst-case performance sub-optimality. Moreover, it ensures repeatable execution strategies across different invocations of a query. We then present a suite of enhancements to the basic plan bouquet algorithm, including randomized variants, that result in significantly stronger performance guarantees. An efficient isosurface identification algorithm is also introduced to curtail the bouquet construction overheads. The plan bouquet approach has been empirically evaluated on both PostgreSQL and a commercial DBMS, over the TPC-H and TPC-DS benchmark environments. Our experimental results indicate that it delivers substantial improvements in the worst-case behavior, without impairing the average-case performance, as compared to the native optimizers of these systems.Moreover, it can be implemented using existing optimizer infrastructure, making it relatively easy to incorporate in current database engines. Overall, the plan bouquet approach provides novel performance guarantees that open up new possibilities for robust query processing. © 2016 ACM.",Plan bouquets; Robust query processing; Selectivity estimation,Algorithms; Budget control; Database systems; Query languages; Average case performance; Execution strategies; Identification algorithms; Performance guarantees; Plan bouquets; Selectivity estimation; Worst-case behaviors; Worst-case performance; Query processing
Synchronization of queries and views upon schema evolutions: A survey,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969963099&doi=10.1145%2f2903726&partnerID=40&md5=17c9cedf290027aba7d1414b3bc58a27,"One of the problems arising upon the evolution of a database schema is that some queries and views defined on the previous schema version might no longer work properly. Thus, evolving a database schema entails the redefinition of queries and views to adapt them to the new schema. Although this problem has been mainly raised in the context of traditional information systems, solutions to it are also advocated in other database-related areas, such as Data Integration, Web Data Integration, and Data Warehouses. The problem is a critical one, since industrial organizations often need to adapt their databases and data warehouses to frequent changes in the real world. In this article, we provide a survey of existing approaches and tools to the problem of adapting queries and views upon a database schema evolution; we also propose a classification framework to enable a uniform comparison method among many heterogeneous approaches and tools.",Db schema evolution; Query synchronization; View synchronization,Classification (of information); Data warehouses; Query languages; Query processing; Surveys; Synchronization; Classification framework; Comparison methods; Database schemas; Industrial organization; Real-world; Schema evolution; View synchronizations; Web data integration; Data integration
Random walk with restart on large graphs using block elimination,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971290048&doi=10.1145%2f2901736&partnerID=40&md5=a87ca9c9cb2f65a5e31762871d06aec5,"Given a large graph, how can we calculate the relevance between nodes fast and accurately? Random walk with restart (RWR) provides a good measure for this purpose and has been applied to diverse data mining applications including ranking, community detection, link prediction, and anomaly detection. Since calculating RWR from scratch takes a long time, various preprocessing methods, most of which are related to inverting adjacency matrices, have been proposed to speed up the calculation. However, these methods do not scale to large graphs because they usually produce large dense matrices that do not fit into memory. In addition, the existing methods are inappropriate when graphs dynamically change because the expensive preprocessing task needs to be computed repeatedly. In this article, we propose BEAR, a fast, scalable, and accurate method for computing RWR on large graphs. BEAR has two versions: a preprocessing method BEARS for static graphs and an incremental update method BEARD for dynamic graphs. BEARS consists of the preprocessing step and the query step. In the preprocessing step, BEARS reorders the adjacency matrix of a given graph so that it contains a large and easy-to-invert submatrix, and precomputes several matrices including the Schur complement of the submatrix. In the query step, BEARS quickly computes the RWR scores for a given query node using a block elimination approach with the matrices computed in the preprocessing step. For dynamic graphs, BEARD efficiently updates the changed parts in the preprocessed matrices of BEARS based on the observation that only small parts of the preprocessed matrices change when few edges are inserted or deleted. Through extensive experiments, we show that BEARS significantly outperforms other state-of-the-art methods in terms of preprocessing and query speed, space efficiency, and accuracy. We also show that BEARD quickly updates the preprocessed matrices and immediately computes queries when the graph changes. © 2016 ACM.",,Data mining; Graphic methods; Random processes; Community detection; Data mining applications; Incremental updates; Pre-processing method; Pre-processing step; Random walk with restart; Space efficiencies; State-of-the-art methods; Matrix algebra
The exact complexity of the first-order logic definability problem,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970023344&doi=10.1145%2f2886095&partnerID=40&md5=43c40c02581e5ad950a6ec0abe86b0f1,"We study the definability problem for first-order logic, denoted by FO-DEF. The input of FO-DEF is a relational database instance I and a relation R; the question to answer is whether there exists a first-order query Q (or, equivalently, a relational algebra expression Q) such that Q evaluated on I gives R as an answer. Although the study of FO-DEF dates back to 1978, when the decidability of this problem was shown, the exact complexity of FO-DEF remains as a fundamental open problem. In this article, we provide a polynomial-time algorithm for solving FO-DEF that uses calls to a graph-isomorphism subroutine (or oracle). As a consequence, the first-order definability problem is found to be complete for the class GI of all problems that are polynomial-time Turing reducible to the graph isomorphism problem, thus closing the open question about the exact complexity of this problem. The technique used is also applied to a generalized version of the problem that accepts a finite set of relation pairs, and whose exact complexity was also open; this version is also found to be GI-complete. © 2016 ACM.",Definability problem; Expressiveness; First-order logic; Relational algebra,Algebra; Algorithms; Formal logic; Polynomial approximation; Query processing; Reconfigurable hardware; Set theory; Definability problems; Expressiveness; First order definability; First order logic; Graph isomorphism problem; Polynomial-time algorithms; Relational algebra; Relational Database; Computer circuits
Capturing missing tuples and missing values,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970016935&doi=10.1145%2f2901737&partnerID=40&md5=aca2db7670703c4316ae4a2864cf4beb,"Databases in real life are often neither entirely closed-world nor entirely open-world. Databases in an enterprise are typically partially closed, in which a part of the data is constrained by master data that contains complete information about the enterprise in certain aspects. It has been shown that, despite missing tuples, such a database may turn out to have complete information for answering a query. This article studies partially closed databases from which both tuples and attribute values may be missing. We specify such a database in terms of conditional tables constrained by master data, referred to as c-instances. We first propose three models to characterize whether a c-instance T is complete for a query Q relative to master data. That is, depending on how missing values in T are instantiated, the answer to Q in T remains unchanged when new tuples are added. We then investigate three problems, to determine (a) whether a given c-instance is complete for a query Q, (b) whether there exists a c-instance that is complete for Q relative to master data available, and (c) whether a c-instance is a minimal-size database that is complete for Q. We establish matching lower and upper bounds on these problems for queries expressed in a variety of languages in each of the three models for specifying relative completeness. © 2016 ACM.",Complexity; Incomplete information; Master data management; Partially closed databases; Relative completeness,Information management; Query languages; Query processing; Attribute values; Complete information; Complexity; Conditional tables; Incomplete information; Lower and upper bounds; Master data management; Relative completeness; C (programming language)
The complexity of learning tree patterns from example graphs,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969931952&doi=10.1145%2f2890492&partnerID=40&md5=bef3f535e5641f1f234e5bb20230d062,"This article investigates the problem of learning tree patterns that return nodes with a given set of labels, from example graphs provided by the user. Example graphs are annotated by the user as being either positive or negative. The goal is then to determine whether there exists a tree pattern returning tuples of nodes with the given labels in each of the positive examples, but in none of the negative examples, and furthermore, to find one such pattern if it exists. These are called the satisfiability and learning problems, respectively. This article thoroughly investigates the satisfiability and learning problems in a variety of settings. In particular, we consider example sets that (1) may contain only positive examples, or both positive and negative examples, (2) may contain directed or undirected graphs, and (3) may have multiple occurrences of labels or be uniquely labeled (to some degree). In addition, we consider tree patterns of different types that can allow, or prohibit, wildcard labeled nodes and descendant edges. We also consider two different semantics for mapping tree patterns to graphs. The complexity of satisfiability is determined for the different combinations of settings. For cases in which satisfiability is polynomial, it is also shown that learning is polynomial. (This is nontrivial as satisfying patterns may be exponential in size.) Finally, the minimal learning problem, that is, that of finding a minimal-sized satisfying pattern, is studied for cases in which satisfiability is polynomial. © 2016 ACM.",Graphs; Learning; Tree patterns,Directed graphs; Forestry; Formal logic; Graphic methods; Polynomials; Semantics; Graphs; Learning; Learning problem; Negative examples; Positive examples; Satisfiability; Tree pattern; Undirected graph; Trees (mathematics)
BEVA: An efficient Query processing algorithm for error-tolerant autocompletion,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968919554&doi=10.1145%2f2877201&partnerID=40&md5=3575f726444fa4486a24092244054ea0,"Query autocompletion has become a standard feature in many search applications, especially for search engines. A recent trend is to support the error-tolerant autocompletion, which increases the usability significantly by matching prefixes of database strings and allowing a small number of errors. In this article, we systematically study the query processing problem for error-tolerant autocompletion with a given edit distance threshold. We propose a general framework that encompasses existing methods and characterizes different classes of algorithms and the minimum amount of information they need to maintain under different constraints. We then propose a novel evaluation strategy that achieves the minimum active node size by eliminating ancestor-descendant relationships among active nodes entirely. In addition, we characterize the essence of edit distance computation by a novel data structure named edit vector automaton (EVA). It enables us to compute new active nodes and their associated states efficiently by table lookups. In order to support large distance thresholds, we devise a partitioning scheme to reduce the size and construction cost of the automaton, which results in the universal partitioned EVA (UPEVA) to handle arbitrarily large thresholds. Our extensive evaluation demonstrates that our proposed method outperforms existing approaches in both space and time efficiencies. © 2016 ACM.",Edit distance; Edit vector automaton; Error-tolerant autocompletion; Query optimization; Query processing,Algorithms; Errors; Optimization; Search engines; Table lookup; Amount of information; Edit distance; Edit vector automaton; Error tolerant; Evaluation strategies; Processing problems; Query optimization; Query processing algorithms; Query processing
Inferring social strength from spatiotemporal data,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968835334&doi=10.1145%2f2877200&partnerID=40&md5=e1b4ccc0bfc44279b8813dc763a91d62,"The advent of geolocation technologies has generated unprecedented rich datasets of people's location information at a very high fidelity. These location datasets can be used to study human behavior; for example, social studies have shown that people who are seen together frequently at the same place and same time are most probably socially related. In this article, we are interested in inferring these social connections by analyzing people's location information; this is useful in a variety of application domains, from sales and marketing to intelligence analysis. In particular, we propose an entropy-based model (EBM) that not only infers social connections but also estimates the strength of social connections by analyzing people's cooccurrences in space and time. We examine two independent methods: diversity and weighted frequency, through which co-occurrences contribute to the strength of a social connection. In addition, we take the characteristics of each location into consideration in order to compensate for cases where only limited location information is available. We also study the role of location semantics in improving our computation of social strength.We develop a parallel implementation of our algorithm usingMapReduce to create a scalable and efficient solution for online applications. We conducted extensive sets of experiments with real-world datasets including both people's location data and their social connections, where we used the latter as the ground truth to verify the results of applying our approach to the former.We show that our approach is valid across different networks and outperforms the competitors. © 2016 ACM.",data mining; geospatial; social computing; Social network; social strength; spatial; spatiotemporal,Computational efficiency; Data mining; Location; Semantics; Social networking (online); Social sciences; Social sciences computing; Geo-spatial; Social computing; social strength; spatial; spatiotemporal; Behavioral research
SQL's three-valued logic and certain answers,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964764019&doi=10.1145%2f2877206&partnerID=40&md5=847b78661d07b52075057a896106c251,"The goal of the article is to bridge the difference between theoretical and practical approaches to answering queries over databases with nulls. Theoretical research has long ago identified the notion of correctness of query answering over incomplete data: one needs to find certain answers, which are true regardless of how incomplete information is interpreted. This serves as the notion of correctness of query answering, but carries a huge complexity tag. In practice, on the other hand, query answering must be very efficient, and to achieve this, SQL uses three-valued logic for evaluating queries on databases with nulls. Due to the complexity mismatch, the two approaches cannot coincide, but perhaps they are related in some way. For instance, does SQL always produce answers we can be certain about? This is not so: SQL's and certain answers semantics could be totally unrelated. We show, however, that a slight modification of the three-valued semantics for relational calculus queries can provide the required certainty guarantees. The key point of the new scheme is to fully utilize the three-valued semantics, and classify answers not into certain or noncertain, as was done before, but rather into certainly true, certainly false, or unknown. This yields relatively small changes to the evaluation procedure, which we consider at the level of both declarative (relational calculus) and procedural (relational algebra) queries. These new evaluation procedures give us certainty guarantees even for queries returning tuples with null values. © 2016 ACM.",Certain answers; Incomplete information; Null values; Query evaluation; Three-valued logic,Algebra; Calculations; Computer circuits; Query processing; Reconfigurable hardware; Semantics; Certain answers; Incomplete information; Null value; Query evaluation; Three-valued logic; Many valued logics
Materialization optimizations for feature selection workloads,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960954918&doi=10.1145%2f2877204&partnerID=40&md5=d7d8914e73cfd41af5e04a0412b89a53,"There is an arms race in the data management industry to support statistical analytics. Feature selection, the process of selecting a feature set that will be used to build a statistical model, is widely regarded as the most critical step of statistical analytics. Thus, we argue that managing the feature selection process is a pressing data management challenge. We study this challenge by describing a feature selection language and a supporting prototype system that builds on top of current industrial R-integration layers. From our interactions with analysts, we learned that feature selection is an interactive human-in-The-loop process, which means that feature selection workloads are rife with reuse opportunities. Thus, we study how to materialize portions of this computation using not only classical database materialization optimizations but also methods that have not previously been used in database optimization, including structural decomposition methods (like QR factorization) and warmstart. These new methods have no analogue in traditional SQL systems, but they may be interesting for array and scientific database applications. On a diverse set of datasets and programs, we find that traditional database-style approaches that ignore these new opportunities are more than two orders of magnitude slower than an optimal plan in this new trade-off space across multiple R backends. Furthermore, we show that it is possible to build a simple cost-based optimizer to automatically select a near-optimal execution plan for feature selection. © 2016 ACM.",declarative language; Feature selection; machine learning; materialization; optimization; R; statistical analytics,Artificial intelligence; Computational linguistics; Database systems; Economic and social effects; Information management; Learning systems; Optimization; Structural optimization; Data management challenges; Database optimization; Declarative Languages; materialization; Scientific database; statistical analytics; Statistical modeling; Structural decomposition methods; Feature extraction
Editorial: Updates to the editorial board,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964434756&doi=10.1145%2f2893581&partnerID=40&md5=9a5a33d13358af31c8d0d678dda3db0c,[No abstract available],,
Dichotomies for queries with negation in probabilistic databases,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960977011&doi=10.1145%2f2877203&partnerID=40&md5=e9f0e839874ce7d751c3f4d8dfa10318,"This article charts the tractability frontier of two classes of relational algebra queries in tuple-independent probabilistic databases. The first class consists of queries with join, projection, selection, and negation but without repeating relation symbols and union. The second class consists of quantified queries that express the following binary relationships among sets of entities: set division, set inclusion, set equivalence, and set incomparability. Quantified queries are expressible in relational algebra using join, projection, nested negation, and repeating relation symbols. Each query in the two classes has either polynomial-Time or #P-hard data complexity and the tractable queries can be recognised efficiently. Our result for the first query class extends a known dichotomy for conjunctive queries without self-joins to such queries with negation. For quantified queries, their tractability is sensitive to their outermost projection operator: They are tractable if no attribute representing set identifiers is projected away and #P-hard otherwise. © 2016 ACM.",Complexity dichotomy; knowledge compilation; probabilistic databases,Algebra; Database systems; Polynomial approximation; Query languages; Binary relationships; Complexity dichotomies; Conjunctive queries; Knowledge compilation; Polynomial-time; Probabilistic database; Projection Operator; Relational algebra; Equivalence classes
Declarative cleaning of inconsistencies in information extraction,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974604331&doi=10.1145%2f2877202&partnerID=40&md5=b75b7b17e0b6d885f7852b9bbab06e08,"The population of a predefined relational schema from textual content, commonly known as Information Extraction (IE), is a pervasive task in contemporary computational challenges associated with Big Data. Since the textual content varies widely in nature and structure (from machine logs to informal natural language), it is notoriously difficult to write IE programs that unambiguously extract the sought information. For example, during extraction, an IE program could annotate a substring as both an address and a person name. When this happens, the extracted information is said to be inconsistent, and some way of removing inconsistencies is crucial to compute the final output. Industrial-strength IE systems like GATE and IBM SystemT therefore provide a built-in collection of cleaning operations to remove inconsistencies from extracted relations. These operations, however, are collected in an ad hoc fashion through use cases. Ideally, we would like to allow IE developers to declare their own policies. But existing cleaning operations are defined in an algorithmic way, and hence it is not clear how to extend the built-in operations without requiring low-level coding of internal or external functions. We embark on the establishment of a framework for declarative cleaning of inconsistencies in IE through principles of database theory. Specifically, building upon the formalism of document spanners for IE, we adopt the concept of prioritized repairs, which has been recently proposed as an extension of the traditional database repairs to incorporate priorities among conflicting facts. We show that our framework captures the popular cleaning policies, as well as the POSIX semantics for extraction through regular expressions. We explore the problem of determining whether a cleaning declaration is unambiguous (i.e., always results in a single repair) and whether it increases the expressive power of the extraction language. We give both positive and negative results, some of which are general and some of which apply to policies used in practice. © 2016 ACM.",Information extraction,Big data; Cleaning; Computation theory; Computational linguistics; Information analysis; Information retrieval; Population statistics; Semantics; Cleaning operations; Computational challenges; Data base theory; Expressive power; Industrial strength; Natural languages; Regular expressions; Relational schemas; Data mining
ENFrame: A framework for processing probabilistic data,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964701662&doi=10.1145%2f2877205&partnerID=40&md5=9ae32c3aafc13b3c675c885470c00a73,"This article introduces ENFrame, a framework for processing probabilistic data. Using ENFrame, users can write programs in a fragment of Python with constructs such as loops, list comprehension, aggregate operations on lists, and calls to external database engines. Programs are then interpreted probabilistically by ENFrame. We exemplify ENFrame on three clustering algorithms (k-means, k-medoids, and Markov clustering) and one classification algorithm (k-nearest-neighbour). A key component of ENFrame is an event language to succinctly encode correlations, trace the computation of user programs, and allow for computation of discrete probability distributions for program variables. We propose a family of sequential and concurrent, exact, and approximate algorithms for computing the probability of interconnected events. Experiments with k-medoids clustering and k-nearest-neighbour show orders-of-magnitude improvements of exact processing using ENFrame over näive processing in each possible world, of approximate over exact, and of concurrent over sequential processing. © 2016 ACM.",Data mining; Data processing; Probabilistic inference,Computer software; Data handling; Data mining; Data processing; Nearest neighbor search; Probability distributions; Approximate algorithms; Classification algorithm; Discrete probability distribution; K-medoids clustering; K-nearest neighbours; Orders of magnitude; Probabilistic inference; Sequential processing; Clustering algorithms
Generating plans from proofs,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963967589&doi=10.1145%2f2847523&partnerID=40&md5=e81068dcb7376f5ef8b6df72cfce7943,"We present algorithms for answering queries making use of information about source integrity constraints, access restrictions, and access costs. Ourmethod can exploit the integrity constraints to find plans even when there is no direct access to relations appearing in the query. We look at different kinds of plans, depending on the kind of relational operators that are permitted within their commands. To each type of plan, we associate a semantic property that is necessary for having a plan of that type. The key idea of our method is to move from a search for a plan to a search for a proof of the corresponding semantic property, and then generate a plan from a proof. We provide algorithms for converting proofs to plans and show that they will find a plan of the desired type whenever such a plan exists. We show that while discovery of one proof allows us to find a single plan that answers the query, we can explore alternative proofs to find lower-cost plans. © 2016 ACM.",Access methods; Hidden web; Optimization,Algorithms; Optimization; Semantics; Access cost; Access methods; Access restriction; Answering queries; Hidden web; Integrity constraints; Relational operator; Semantic properties; Information use
Processing top-k dominating queries in metric spaces,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963831343&doi=10.1145%2f2847524&partnerID=40&md5=1db61d38c874e749b7b451fa863fb527,"Top-k dominating queries combine the natural idea of selecting the k best items with a comprehensive ""goodness"" criterion based on dominance. A point p1 dominates p2 if p1 is as good as p2 in all attributes and is strictly better in at least one. Existing works address the problem in settings where data objects are multidimensional points. However, there are domains where we only have access to the distance between two objects. In cases like these, attributes reflect distances from a set of input objects and are dynamically generated as the input objects change. Consequently, prior works from the literature cannot be applied, despite the fact that the dominance relation is still meaningful and valid. For this reason, in this work, we present the first study for processing top-k dominating queries over distance-based dynamic attribute vectors, defined over a metric space.We propose four progressive algorithms that utilize the properties of the underlying metric space to efficiently solve the problem and present an extensive, comparative evaluation on both synthetic and real-world datasets. © 2016 ACM.",Distance computation; Dominating queries; Metric spaces,Set theory; Topology; Comparative evaluations; Distance computation; Dominance relation; Dominating queries; Dynamic attributes; Metric spaces; Real-world datasets; Top-k dominating queries; Vector spaces
Faster random walks by rewiring online social networks on-the-fly,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963958030&doi=10.1145%2f2847526&partnerID=40&md5=505b7da35756f17124833d72cc0e7cd2,"Many online social networks feature restrictive web interfaces that only allow the query of a user's local neighborhood. To enable analytics over such an online social network through its web interface, many recent efforts use Markov Chain Monte Carlo (MCMC) methods such as random walks to sample users in the social network and thereby support analytics based on the samples. The problem with such an approach, however, is the large amount of queries often required for a random walk to converge to a desired (stationary) sampling distribution. In this article, we consider a novel problem of enabling a faster random walk over online social networks by ""rewiring"" the social network on-the-fly. Specifically, we develop a Modified TOpology Sampling (MTO-Sampling) scheme that, by using only information exposed by the restrictive web interface, constructs a ""virtual"" random-walk-friendly overlay topology of the social network while performing a random walk and ensures that the random walk follows the modified overlay topology rather than the original one. We describe in this article instantiations of MTO-Sampling for various types of random walks, such as Simple Random Walk (MTO-SRW), Metropolis-Hastings Random Walk (MTO-MHRW), and General Random Walk (MTO-GRW). We not only rigidly prove that MTO-Sampling improves the efficiency of sampling, but we also demonstrate the significance of such improvement through experiments on real-world online social networks such as Google Plus, Epinion, Facebook, etc. © 2016 ACM.",Graph sampling; Random walk,Markov processes; Monte Carlo methods; Online systems; Random processes; Topology; Websites; Graph samplings; Local neighborhoods; Markov chain Monte Carlo method; Metropolis Hastings; On-line social networks; Overlay topologies; Random Walk; Sampling distribution; Social networking (online)
Negative factor: Improving regular-expression matching in strings,2016,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963792001&doi=10.1145%2f2847525&partnerID=40&md5=b2990a37c599da237cad59344ad7178f,"The problem of finding matches of a regular expression (RE) on a string exists in many applications, such as text editing, biosequence search, and shell commands. Existing techniques first identify candidates using substrings in the RE, then verify each of them using an automaton. These techniques become inefficient when there are many candidate occurrences that need to be verified. In this article, we propose a novel technique that prunes false negatives by utilizing negative factors, which are substrings that cannot appear in an answer. A main advantage of the technique is that it can be integrated with many existing algorithms to improve their efficiency significantly. We present a detailed description of this technique. We develop an efficient algorithm that utilizes negative factors to prune candidates, then improve it by using bit operations to process negative factors in parallel. We show that negative factors, when used with necessary factors (substrings that must appear in each answer), can achieve much better pruning power.We analyze the large number of negative factors, and develop an algorithm for finding a small number of high-quality negative factors. We conducted a thorough experimental study of this technique on real datasets, including DNA sequences, proteins, and text documents, and show significant performance improvement of the state-of-theart tools by an order of magnitude. © 2016 Copyright held by the owner/author(s).",Long sequence; Regular expression,DNA sequences; Bit-operations; False negatives; Long sequences; Novel techniques; Performance improvements; Real data sets; Regular expressions; Regular-expression matching; Pattern matching
Supporting Better Insights of Data Science Pipelines with Fine-grained Provenance,2024,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193523259&doi=10.1145%2f3644385&partnerID=40&md5=c04327fa8388fa072d98d494f2b7dbec,"Successful data-driven science requires complex data engineering pipelines to clean, transform, and alter data in preparation for machine learning, and robust results can only be achieved when each step in the pipeline can be justified, and its effect on the data explained. In this framework, we aim at providing data scientists with facilities to gain an in-depth understanding of how each step in the pipeline affects the data, from the raw input to training sets ready to be used for learning. Starting from an extensible set of data preparation operators commonly used within a data science setting, in this work we present a provenance management infrastructure for generating, storing, and querying very granular accounts of data transformations, at the level of individual elements within datasets whenever possible. Then, from the formal definition of a core set of data science preprocessing operators, we derive a provenance semantics embodied by a collection of templates expressed in PROV, a standard model for data provenance. Using those templates as a reference, our provenance generation algorithm generalises to any operator with observable input/output pairs. We provide a prototype implementation of an application-level provenance capture library to produce, in a semiautomatic way, complete provenance documents that account for the entire pipeline. We report on the ability of that reference implementation to capture provenance in real ML benchmark pipelines and over TCP-DI synthetic data. We finally show how the collected provenance can be used to answer a suite of provenance benchmark queries that underpin some common pipeline inspection questions, as expressed on the Data Science Stack Exchange. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data preparation; data science; preprocessing; Provenance,Data Science; Query processing; Semantics; Complex data; Data driven; Data engineering; Data preparation; Fine grained; In-depth understanding; Machine-learning; Preprocessing; Provenance; Training sets; Pipelines
Sharing Queries with Nonequivalent User-defined Aggregate Functions,2024,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193473079&doi=10.1145%2f3649133&partnerID=40&md5=aedd598cd0fe0124d03b8acd9dc4fa71,"This article presents Sharing User-Defined Aggregate Function (SUDAF), a declarative framework that allows users to write User-defined Aggregate Functions (UDAFs) as mathematical expressions and use them in Structured Query Language statements. SUDAF rewrites partial aggregates of UDAFs using built-in aggregate functions and supports efficient dynamic caching and reusing of partial aggregates. Our experiments show that rewriting UDAFs using built-in functions can significantly speed up queries with UDAFs, and the proposed sharing approach can yield up to two orders of magnitude improvement in query execution time. The article studies also an extension of SUDAF to support sharing partial results between arbitrary queries with UDAFs. We show a connection with the problem of query rewriting using views and introduce a new class of rewritings, called SUDAF rewritings, which enables to use views that have aggregate functions different from the ones used in the input query. We investigate the underlying rewriting-checking and rewriting-existing problem. Our main technical result is a reduction of these problems to, respectively, rewriting-checking and rewriting-existing of the so-called aggregate candidates, a class of rewritings that has been deeply investigated in the literature. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesQuery processing; query rewriting; user-defined aggregate functions,Query languages; Additional key word and phrasesquery processing; Aggregate function; Dynamic caching; Key words; Language statements; Mathematical expressions; Query rewritings; Structured Query Language; User-defined aggregate function; User-defined aggregates; Aggregates
Database Repairing with Soft Functional Dependencies,2024,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193505547&doi=10.1145%2f3651156&partnerID=40&md5=58b1dd1abef175ea0ee87daddd0fb7d3,"A common interpretation of soft constraints penalizes the database for every violation of every constraint, where the penalty is the cost (weight) of the constraint. A computational challenge is that of finding an optimal subset: a collection of database tuples that minimizes the total penalty when each tuple has a cost of being excluded. When the constraints are strict (i.e., have an infinite cost), this subset is a ""cardinality repair""of an inconsistent database; in soft interpretations, this subset corresponds to a ""most probable world""of a probabilistic database, a ""most likely intention""of a probabilistic unclean database, and so on. Within the class of functional dependencies, the complexity of finding a cardinality repair is thoroughly understood. Yet, very little is known about the complexity of finding an optimal subset for the more general soft semantics. The work described in this manuscript makes significant progress in that direction. In addition to general insights about the hardness and approximability of the problem, we present algorithms for two special cases (and some generalizations thereof): a single functional dependency, and a bipartite matching. The latter is the problem of finding an optimal ""almost matching""of a bipartite graph where a penalty is paid for every lost edge and every violation of monogamy. For these special cases, we also investigate the complexity of additional computational tasks that arise when the soft constraints are used as a means to represent a probabilistic database in the case of a probabilistic unclean database. © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesDatabase inconsistency; database repairs; functional dependencies; integrity constraints; soft constraints,Repair; Semantics; Set theory; Additional key word and phrasesdatabase inconsistency; Cardinalities; Database repairs; Functional dependency; Integrity constraints; Key words; Optimal subsets; Probabilistic database; Probabilistics; Soft constraint; Database systems
Identifying the Root Causes of DBMS Suboptimality,2024,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189094733&doi=10.1145%2f3636425&partnerID=40&md5=f60e5e462b8028dec8cb78c303eaac26,"The query optimization phase within a database management system (DBMS) ostensibly finds the fastest query execution plan from a potentially large set of enumerated plans, all of which correctly compute the same result of the specified query. Sometimes the cost-based optimizer selects a slower plan, for a variety of reasons. Previous work has focused on increasing the performance of specific components, often a single operator, within an individual DBMS. However, that does not address the fundamental question: from where does this suboptimality arise, across DBMSes generally? In particular, the contribution of each of many possible factors to DBMS suboptimality is currently unknown. To identify the root causes of DBMS suboptimality, we first introduce the notion of empirical suboptimality of a query plan chosen by the DBMS, indicated by the existence of a query plan that performs more efficiently than the chosen plan, for the same query. A crucial aspect is that this can be measured externally to the DBMS, and thus does not require access to its source code. We then propose a novel predictive model to explain the relationship between various factors in query optimization and empirical suboptimality. Our model associates suboptimality with the factors of complexity of the schema, of the underlying data on which the query is evaluated, of the query itself, and of the DBMS optimizer. The model also characterizes concomitant interactions among these factors. This model induces a number of specific hypotheses that were tested on multiple DBMSes. We performed a series of experiments that examined the plans for thousands of queries run on four popular DBMSes. We tested the model on over a million of these query executions, using correlational analysis, regression analysis, and causal analysis, specifically Structural Equation Modeling (SEM). We observed that the dependent construct of empirical suboptimality prevalence correlates positively with nine specific constructs characterizing four identified factors that explain in concert much of the variance of suboptimality of two extensive benchmarks, across these disparate DBMSes. This predictive model shows that it is the common aspects of these DBMSes that predict suboptimality, not the particulars embedded in the inordinate complexity of each of these DBMSes. This paper thus provides a new methodology to study mature query optimizers, identifies underlying DBMS-independent causes for the observed suboptimality, and quantifies the relative contribution of each of these causes to the observed suboptimality. This work thus provides a roadmap for fundamental improvements of cost-based query optimizers.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesQuery optimization; database performance evaluation; empirical studies; query suboptimality,Codes (symbols); Query processing; Regression analysis; Search engines; Additional key word and phrasesquery optimization; Database performance; Database performance evaluation; Empirical studies; Key words; Optimisations; Performances evaluation; Query suboptimality; Root cause; Suboptimality; Database systems
The Ring: Worst-case Optimal Joins in Graph Databases using (Almost) No Extra Space,2024,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192926996&doi=10.1145%2f3644824&partnerID=40&md5=ea9f59c5dae33dd24a3a274f0e3997cd,"We present an indexing scheme for triple-based graphs that supports join queries in worst-case optimal (wco) time within compact space. This scheme, called a ring, regards each triple as a cyclic string of length 3. Each rotation of the triples is lexicographically sorted and the values of the last attribute are stored as a column, so we obtain the order of the next column by stably re-sorting the triples by its attribute. We show that, by representing the columns with a compact data structure called a wavelet tree, this ordering enables forward and backward navigation between columns without needing pointers. These wavelet trees further support wco join algorithms and cardinality estimations for query planning. While traditional data structures such as B-Trees, tries, and so on, require 6 index orders to support all possible wco joins over triples, we can use one ring to index them all. This ring replaces the graph and uses only sublinear extra space, thus supporting wco joins in almost no space beyond storing the graph itself. Experiments querying a large graph (Wikidata) in memory show that the ring offers nearly the best overall query times while using only a small fraction of the space required by several state-of-the-art approaches.We then turn our attention to some theoretical results for indexing tables of arity d higher than 3 in such a way that supports wco joins. While a single ring of length d no longer suffices to cover all d! orders, we need much fewer rings to index them all: O(2d) rings with a small constant. For example, we need 5 rings instead of 120 orders for d=5. We show that our rings become a particular case of what we dub order graphs, whose nodes are attribute orders and where stably sorting by some attribute leads us from an order to another, thereby inducing an edge labeled by the attribute. The index is then the set of columns associated with the edges, and a set of rings is just one possible graph shape. We show that other shapes, like for example a single ring instead of several ones of length d, can lead us to even smaller indexes, and that other more general shapes are also possible. For example, we handle d=5 attributes within space equivalent to 4 rings. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesWorst-case optimal joins; column stores; graph databases; graph indexing; graph patterns; wavelet trees,Data structures; Indexing (of information); Optimization; Sorting; Trees (mathematics); A-RINGS; Additional key word and phrasesbad-case optimal join; Columnstores; Graph database; Graph indexing; Graph patterns; Indexing scheme; Key words; Optimal time; Wavelet tree; Graph Databases
Ad Hoc Transactions through the Looking Glass: An Empirical Study of Application-Level Transactions in Web Applications,2024,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189100423&doi=10.1145%2f3638553&partnerID=40&md5=65be99f20adec320d05f814d36873837,"Many transactions in web applications are constructed ad hoc in the application code. For example, developers might explicitly use locking primitives or validation procedures to coordinate critical code fragments. We refer to database operations coordinated by application code as ad hoc transactions. Until now, little is known about them. This paper presents the first comprehensive study on ad hoc transactions. By studying 91 ad hoc transactions among eight popular open-source web applications, we found that (i) every studied application uses ad hoc transactions (up to 16 per application), 71 of which play critical roles; (ii) compared with database transactions, concurrency control of ad hoc transactions is much more flexible; (iii) ad hoc transactions are error-prone-53 of them have correctness issues, and 33 of them are confirmed by developers; and (iv) ad hoc transactions have the potential for improving performance in contentious workloads by utilizing application semantics such as access patterns. Based on these findings, we discuss the implications of ad hoc transactions to the database research community.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAd hoc transactions,Concurrency control; Open source software; Semantics; Additional key word and phrasesad hoc transaction; Application codes; Application level; Code fragments; Critical codes; Empirical studies; Key words; Open-source; WEB application; Web applications; Database systems
Linking Entities across Relations and Graphs,2024,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189067094&doi=10.1145%2f3639363&partnerID=40&md5=77b59832e6cd7cd2a40adb4d5d25d0ab,"This article proposes a notion of parametric simulation to link entities across a relational database and a graph G. Taking functions and thresholds for measuring vertex closeness, path associations, and important properties as parameters, parametric simulation identifies tuples t in and vertices v in G that refer to the same real-world entity, based on both topological and semantic matching. We develop machine learning methods to learn the parameter functions and thresholds. We show that parametric simulation is in quadratic-Time by providing such an algorithm. Moreover, we develop an incremental algorithm for parametric simulation; we show that the incremental algorithm is bounded relative to its batch counterpart, i.e., it incurs the minimum cost for incrementalizing the batch algorithm. Putting these together, we develop HER, a parallel system to check whether (t, v) makes a match, find all vertex matches of t in G, and compute all matches across and G, all in quadratic-Time; moreover, HER supports incremental computation of these in response to updates to and G. Using real-life and synthetic data, we empirically verify that HER is accurate with F-measure of 0.94 on average, and is able to scale with database and graph G for both batch and incremental computations.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEntity resolution; Incremental algorithm; Knowledge graph; Parallelization; Relational database; Relative boundedness,Graph theory; Knowledge graph; Learning systems; Semantics; Additional key word and phrasesentity resolution; Boundedness; Graph G; Incremental algorithm; Key words; Knowledge graphs; Parallelizations; Parametric simulations; Relational Database; Relative boundedness; Parameter estimation
Fast Parallel Hypertree Decompositions in Logarithmic Recursion Depth,2024,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189106116&doi=10.1145%2f3638758&partnerID=40&md5=3b425754a8582abbf0cddfc7a162d4d9,"Various classic reasoning problems with natural hypergraph representations are known to be tractable if a hypertree decomposition (HD) of low width exists. The resulting algorithms are attractive for practical use in fields like databases and constraint satisfaction. However, algorithmic use of HDs relies on the difficult task of first computing a decomposition of the hypergraph underlying a given problem instance, which is then used to guide the algorithm for this particular instance. The performance of purely sequential methods for computing HDs is inherently limited, yet the problem is, theoretically, amenable to parallelisation. In this article, we propose the first algorithm for computing hypertree decompositions that is well suited for parallelisation. The newly proposed algorithm log-k-decomp requires only a logarithmic number of recursion levels and additionally allows for highly parallelised pruning of the search space by restriction to so-called balanced separators. We provide a detailed experimental evaluation over the HyperBench benchmark and demonstrate that log-k-decomp outperforms the current state of the art significantly.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHypergraph decomposition; hypertree width; parallel algorithms,Additional key word and phraseshypergraph decomposition; Hypergraph representations; Hypertree decomposition; Hypertree width; Hypertrees; Key words; Parallelizations; Practical use; Reasoning problems; Recursions
Cost-based Data Prefetching and Scheduling in Big Data Platforms over Tiered Storage Systems,2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183076629&doi=10.1145%2f3625389&partnerID=40&md5=049e6d40f97bdfc7498c5079dec2ff58,"The use of storage tiering is becoming popular in data-intensive compute clusters due to the recent advancements in storage technologies. The Hadoop Distributed File System, for example, now supports storing data in memory, SSDs, and HDDs, while OctopusFS and hatS offer fine-grained storage tiering solutions. However, current big data platforms (such as Hadoop and Spark) are not exploiting the presence of storage tiers and the opportunities they present for performance optimizations. Specifically, schedulers and prefetchers will make decisions only based on data locality information and completely ignore the fact that local data are now stored on a variety of storage media with different performance characteristics. This article presents Trident, a scheduling and prefetching framework that is designed to make task assignment, resource scheduling, and prefetching decisions based on both locality and storage tier information. Trident formulates task scheduling as a minimum cost maximum matching problem in a bipartite graph and utilizes two novel pruning algorithms for bounding the size of the graph, while still guaranteeing optimality. In addition, Trident extends YARN's resource request model and proposes a new storage-tier-aware resource scheduling algorithm. Finally, Trident includes a cost-based data prefetching approach that coordinates with the schedulers for optimizing prefetching operations. Trident is implemented in both Spark and Hadoop and evaluated extensively using a realistic workload derived from Facebook traces as well as an industry-validated benchmark, demonstrating significant benefits in terms of application performance and cluster efficiency.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesDistributed file systems; data prefetching; task scheduling; tiered storage,Benchmarking; Digital storage; File organization; Multitasking; Scheduling algorithms; Additional key word and phrasesdistributed file system; Data platform; Data pre-fetching; Data scheduling; Data-prefetching; Filesystem; Key words; Prefetching; Tasks scheduling; Tiered storage; Big data
Partial Order Multiway Search,2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183080018&doi=10.1145%2f3626956&partnerID=40&md5=97f13a04f71fc462af5a31333ccf5480,"Partial order multiway search (POMS) is a fundamental problem that finds applications in crowdsourcing, distributed file systems, software testing, and more. This problem involves an interaction between an algorithm and an oracle, conducted on a directed acyclic graph known to both parties. Initially, the oracle selects a vertex t in called the target. Subsequently, must identify the target vertex by probing reachability. In each probe, selects a set Q of vertices in , the number of which is limited by a pre-agreed value k. The oracle then reveals, for each vertex q g Q, whether q can reach the target in . The objective of is to minimize the number of probes. We propose an algorithm to solve POMS in probes, where n represents the number of vertices in , and d denotes the largest out-degree of the vertices in . The probing complexity is asymptotically optimal. Our study also explores two new POMS variants: The first one, named taciturn POMS, is similar to classical POMS but assumes a weaker oracle, and the second one, named EM POMS, is a direct extension of classical POMS to the external memory (EM) model. For both variants, we introduce algorithms whose performance matches or nearly matches the corresponding theoretical lower bounds.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPartial order; data structures; graph algorithms; lower bounds,Application programs; Directed graphs; File organization; Software testing; Acyclic graphs; Additional key word and phrasespartial order; Distributed file systems; Graph algorithms; Key words; Low bound; Multi-way searches; Partial order; Software testings; System softwares; Probes
DomainNet: Homograph Detection and Understanding in Data Lake Disambiguation,2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171732893&doi=10.1145%2f3612919&partnerID=40&md5=2a0fe9b9c1938a39da747fecbfef2b4c,"Modern data lakes are heterogeneous in the vocabulary that is used to describe data. We study a problem of disambiguation in data lakes: How can we determine if a data value occurring more than once in the lake has different meanings and is therefore a homograph?While word and entity disambiguation have been well studied in computational linguistics, data management, and data science, we show that data lakes provide a new opportunity for disambiguation of data values, because tables implicitly define a massive network of interconnected values. We introduce DomainNet, which efficiently represents this network, and investigate to what extent it can be used to disambiguate values without requiring any supervision.DomainNet leverages network-centrality measures on a bipartite graph whose nodes represent data values and attributes to determine if a value is a homograph. A thorough experimental evaluation demonstrates that state-of-the-art techniques in domain discovery cannot be re-purposed to compete with our method. Specifically, using a domain discovery method to identify homographs achieves an F1-score of 0.38 versus 0.69 for DomainNet, which separates homographs well from data values that have a unique meaning. On a real data lake, our top-100 precision is 93%. Given a homograph, we also present a novel method for determining the number of meanings of the homograph and for assigning its data lake attributes to a meaning. We show the influence of homographs on two downstream tasks: entity-matching and domain discovery.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesData Discovery; homograph detection; network-centrality measures,Graph theory; Information management; Additional key word and phrasesdata discovery; Centrality measures; Data values; Domain discovery; Entity disambiguation; Homograph detection; Key words; Linguistic data; Network centralities; Network-centrality measure; Lakes
Model Counting Meets F0 Estimation,2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171777546&doi=10.1145%2f3603496&partnerID=40&md5=ecb4f838b90e6b5d1336caddf1ff6814,"Constraint satisfaction problems (CSPs) and data stream models are two powerful abstractions to capture a wide variety of problems arising in different domains of computer science. Developments in the two communities have mostly occurred independently and with little interaction between them. In this work, we seek to investigate whether bridging the seeming communication gap between the two communities may pave the way to richer fundamental insights. To this end, we focus on two foundational problems: model counting for CSP's and computation of zeroth frequency moments (F0) for data streams.Our investigations lead us to observe a striking similarity in the core techniques employed in the algorithmic frameworks that have evolved separately for model counting and F0 computation. We design a recipe for translating algorithms developed for F0 estimation to model counting, resulting in new algorithms for model counting. We also provide a recipe for transforming sampling algorithm over streams to constraint sampling algorithms. We then observe that algorithms in the context of distributed streaming can be transformed into distributed algorithms for model counting. We next turn our attention to viewing streaming from the lens of counting and show that framing F0 estimation as a special case of #DNF counting allows us to obtain a general recipe for a rich class of streaming problems, which had been subjected to case-specific analysis in prior works. In particular, our view yields an algorithm for multidimensional range efficient F0 estimation with a simpler analysis.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesModel counting; DNF counting; F<sub>0</sub>-computation; streaming algorithms,Computer hardware description languages; Learning algorithms; Additional key word and phrasesmodel counting; Constraint-satisfaction problems; Data stream model; Different domains; DNF counting; F0-computation; Key words; Model Counting; Sampling algorithm; Streaming algorithm; Constraint satisfaction problems
Enabling Timely and Persistent Deletion in LSM-Engines,2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171781372&doi=10.1145%2f3599724&partnerID=40&md5=361c9d441d0cc8c8a49c90b7abf09da8,"Data-intensive applications have fueled the evolution of log-structured merge (LSM) based key-value engines that employ the out-of-place paradigm to support high ingestion rates with low read/write interference. These benefits, however, come at the cost of treating deletes as second-class citizens. A delete operation inserts a tombstone that invalidates older instances of the deleted key. State-of-the-art LSM-engines do not provide guarantees as to how fast a tombstone will propagate to persist the deletion. Further, LSM-engines only support deletion on the sort key. To delete on another attribute (e.g., timestamp), the entire tree is read and re-written, leading to undesired latency spikes and increasing the overall operational cost of a database. Efficient and persistent deletion is key to support: (i) streaming systems operating on a window of data, (ii) privacy with latency guarantees on data deletion, and (iii) en masse cloud deployment of data systems.Further, we document that LSM-based key-value engines perform suboptimally in the presence of deletes in a workload. Tombstone-driven logical deletes, by design, are unable to purge the deleted entries in a timely manner, and retaining the invalidated entries perpetually affects the overall performance of LSM-engines in terms of space amplification, write amplification, and read performance. Moreover, the potentially unbounded latency for persistent deletes brings in critical privacy concerns in light of the data privacy protection regulations, such as the right to be forgotten in EU's GDPR, the right to delete in California's CCPA and CPRA, and deletion right in Virginia's VCDPA. Toward this, we introduce the delete design space for LSM-trees and highlight the performance implications of the different classes of delete operations.To address these challenges, in this article, we build a new key-value storage engine, Lethe+, that uses a very small amount of additional metadata, a set of new delete-aware compaction policies, and a new physical data layout that weaves the sort and the delete key order. We show that Lethe+ supports any user-defined threshold for the delete persistence latency offering higher read throughput (1.17× -1.4×) and lower space amplification (2.1× -9.8×), with a modest increase in write amplification (between 4% and 25%) that can be further amortized to less than 1%. In addition, Lethe+ supports efficient range deletes on a secondary delete key by dropping entire data pages without sacrificing read performance or employing a costly full tree merge.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesKey-value stores; data deletion; data privacy; LSM-trees,Engines; Trees (mathematics); Virtual storage; Additional key word and phraseskey-value store; Data deletion; Data-intensive application; Key values; Key words; Log structured; Log structured merge trees; Performance; Read performance; Write amplifications; Data privacy
Efficient Bi-objective SQL Optimization for Enclaved Cloud Databases with Differentially Private Padding,2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164279147&doi=10.1145%2f3597021&partnerID=40&md5=5a5e89da942c0d8b001ba9a9b6670454,"Hardware-enabled enclaves have been applied to efficiently enforce data security and privacy protection in cloud database services. Such enclaved systems, however, are reported to suffer from I/O-size (also referred to as communication-volume)-based side-channel attacks. Albeit differentially private padding has been exploited to defend against these attacks as a principle method, it introduces a challenging bi-objective parametric query optimization (BPQO) problem and current solutions are still not satisfactory. Concretely, the goal in BPQO is to find a Pareto-optimal plan that makes a tradeoff between query performance and privacy loss; existing solutions are subjected to poor computational efficiency and high cloud resource waste. In this article, we propose a two-phase optimization algorithm called TPOA to solve the BPQO problem. TPOA incorporates two novel ideas: divide-And-conquer to separately handle parameters according to their types in optimization for dimensionality reduction; on-demand-optimization to progressively build a set of necessary Pareto-optimal plans instead of seeking a complete set for saving resources. Besides, we introduce an acceleration mechanism in TPOA to improve its efficiency, which prunes the non-optimal candidate plans in advance. We theoretically prove the correctness of TPOA, numerically analyze its complexity, and formally give an end-To-end privacy analysis. Through a comprehensive evaluation on its efficiency by running baseline algorithms over synthetic and test-bed benchmarks, we can conclude that TPOA outperforms all benchmarked methods with an overall efficiency improvement of roughly two orders of magnitude; moreover, the acceleration mechanism speeds up TPOA by 10-200×. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEnclaved database; bi-objective parametric query optimization; differential privacy; SGX; SQL queries; tradeoff between performance and privacy loss,Data privacy; Pareto principle; Query languages; Query processing; Side channel attack; Additional key word and phrasesenclaved database; Bi objectives; Bi-objective parametric query optimization; Differential privacies; Key words; Performance; Queries optimization; SGX; SQL query; Tradeoff between performance and privacy loss; Computational efficiency
Proportionality on Spatial Data with Context,2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164282139&doi=10.1145%2f3588434&partnerID=40&md5=b91772967f033b040b7f3ee227619f2d,"More often than not, spatial objects are associated with some context, in the form of text, descriptive tags (e.g., points of interest, flickr photos), or linked entities in semantic graphs (e.g., Yago2, DBpedia). Hence, location-based retrieval should be extended to consider not only the locations but also the context of the objects, especially when the retrieved objects are too many and the query result is overwhelming. In this article, we study the problem of selecting a subset of the query result, which is the most representative. We argue that objects with similar context and nearby locations should proportionally be represented in the selection. Proportionality dictates the pairwise comparison of all retrieved objects and hence bears a high cost. We propose novel algorithms which greatly reduce the cost of proportional object selection in practice. In addition, we propose pre-processing, pruning, and approximate computation techniques that their combination reduces the computational cost of the algorithms even further. We theoretically analyze the approximation quality of our approaches. Extensive empirical studies on real datasets show that our algorithms are effective and efficient. A user evaluation verifies that proportional selection is more preferable than random selection and selection based on object diversification. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesProportionality; diversity; fairness; keyword search; Ptolemy's spatial diversity; ranking; spatial data,Cost reduction; Geographic information systems; Location based services; Search engines; Semantics; Additional key word and phrasesproportionality; Diversity; Fairness; Key words; Keyword search; Ptolemies; Ptolemy spatial diversity; Ranking; Spatial data; Spatial diversity; Location
Reversible Database Watermarking Based on Order-preserving Encryption for Data Sharing,2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164276801&doi=10.1145%2f3589761&partnerID=40&md5=9d35d4f6fc91a65eeb8d89df5cd84fa0,"In the era of big data, data sharing not only boosts the economy of the world but also brings about problems of privacy disclosure and copyright infringement. The collected data may contain users' sensitive information; thus, privacy protection should be applied to the data prior to them being shared. Moreover, the shared data may be re-shared to third parties without the consent or awareness of the original data providers. Therefore, there is an urgent need for copyright tracking. There are few works satisfying the requirements of both privacy protection and copyright tracking. The main challenge is how to protect the shared data and realize copyright tracking while not undermining the utility of the data. In this article, we propose a novel solution of a reversible database watermarking scheme based on order-preserving encryption. First, we encrypt the data using order-preserving encryption and adjust an encryption parameter within an appropriate interval to generate a ciphertext with redundant space. Then, we leverage the redundant space to embed robust reversible watermarking. We adopt grouping and K-means to improve the embedding capacity and the robustness of the watermark. Formal theoretical analysis proves that the proposed scheme guarantees correctness and security. Results of extensive experiments show that OPEW has 100% data utility, and the robustness and efficiency of OPEW are better than existing works. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesData sharing; copyright; data utility; efficiency; order-preserving encryption; robustness; security; security; watermark,Copyrights; K-means clustering; Privacy-preserving techniques; Sensitive data; Watermarking; Additional key word and phrasesdata sharing; Data utilities; Database watermarking; Key words; Order preserving; Order-preserving encryption; Robustness; Security; Watermark; Efficiency
Tractable Orders for Direct Access to Ranked Answers of Conjunctive Queries,2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152591290&doi=10.1145%2f3578517&partnerID=40&md5=64edfb08bbf8fa1ebfd563b7a61f1c66,"We study the question of when we can provide direct access to the k-th answer to a Conjunctive Query (CQ) according to a specified order over the answers in time logarithmic in the size of the database, following a preprocessing step that constructs a data structure in time quasilinear in database size. Specifically, we embark on the challenge of identifying the tractable answer orderings, that is, those orders that allow for such complexity guarantees. To better understand the computational challenge at hand, we also investigate the more modest task of providing access to only a single answer (i.e., finding the answer at a given position), a task that we refer to as the selection problem, and ask when it can be performed in quasilinear time. We also explore the question of when selection is indeed easier than ranked direct access. We begin with lexicographic orders. For each of the two problems, we give a decidable characterization (under conventional complexity assumptions) of the class of tractable lexicographic orders for every CQ without self-joins. We then continue to the more general orders by the sum of attribute weights and establish the corresponding decidable characterizations, for each of the two problems, of the tractable CQs without self-joins. Finally, we explore the question of when the satisfaction of Functional Dependencies (FDs) can be utilized for tractability and establish the corresponding generalizations of our characterizations for every set of unary FDs.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",answer orderings; Conjunctive queries; direct access; query classification; ranking function,Computability and decidability; Query processing; Answer ordering; Conjunctive query; Direct access; Functional dependency; Lexicographic order; Pre-processing step; Quasi-linear; Query classification; Ranking functions; Self-join; Database systems
Efficiently Cleaning Structured Event Logs: A Graph Repair Approach,2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152600466&doi=10.1145%2f3571281&partnerID=40&md5=c3557d06c1aa9a8387f9164252a4e2e6,"Event data are often dirty owing to various recording conventions or simply system errors. These errors may cause serious damage to real applications, such as inaccurate provenance answers, poor profiling results, or concealing interesting patterns from event data. Cleaning dirty event data is strongly demanded. While existing event data cleaning techniques view event logs as sequences, structural information does exist among events, such as the task passing relationships between staffs in workflow or the invocation relationships among different micro-services in monitoring application performance. We argue that such structural information enhances not only the accuracy of repairing inconsistent events but also the computation efficiency. It is notable that both the structure and the names (labeling) of events could be inconsistent. In real applications, while an unsound structure is not repaired automatically (which requires manual effort from business actors to handle the structure error), it is highly desirable to repair the inconsistent event names introduced by recording mistakes. In this article, we first prove that the inconsistent label repairing problem is NP-complete. Then, we propose a graph repair approach for (1) detecting unsound structures, and (2) repairing inconsistent event names. Efficient pruning techniques together with two heuristic solutions are also presented. Extensive experiments over real and synthetic datasets demonstrate both the effectiveness and efficiency of our proposal.  © 2023 Copyright held by the owner/author(s).",data cleaning; Event data; event label repairing,Cleaning; Errors; Repair; Data cleaning; Dirty events; Event data; Event label repairing; Event logs; Micro services; Real applications; Structural information; System errors; Work-flows; Efficiency
Robust and Efficient Sorting with Offset-value Coding,2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148095936&doi=10.1145%2f3570956&partnerID=40&md5=b93d5df22290195c2f048c99e9d929b4,"Sorting and searching are large parts of database query processing, e.g., in the forms of index creation, index maintenance, and index lookup, and comparing pairs of keys is a substantial part of the effort in sorting and searching. We have worked on simple, efficient implementations of decades-old, neglected, effective techniques for fast comparisons and fast sorting, in particular offset-value coding. In the process, we happened upon its mutually beneficial relationship with prefix truncation in run files as well as the duality of compression techniques in row- and column-format storage structures, namely prefix truncation and run-length encoding of leading key columns. We also found a beneficial relationship with consumers of sorted streams, e.g., merging parallel streams, in-stream aggregation, and merge join. We report on our implementation in the context of Google's Napa and F1 Query systems as well as an experimental evaluation of performance and scalability.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",duplicate removal; grouping; merging; offset-value coding; priority queue; Sorting; tree-of-losers,Codes (symbols); Petroleum reservoir evaluation; Query processing; Database query processing; Duplicate removal; Grouping; Index creation; Large parts; Offset values; Offset-value coding; Priority queues; Sorting and searching; Tree-of-loser; Merging
"Efficient Sorting, Duplicate Removal, Grouping, and Aggregation",2023,ACM Transactions on Database Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146354416&doi=10.1145%2f3568027&partnerID=40&md5=cc466886c2b5de9755af3a59ed0ed367,"Database query processing requires algorithms for duplicate removal, grouping, and aggregation. Three algorithms exist: in-stream aggregation is most efficient by far but requires sorted input; sort-based aggregation relies on external merge sort; and hash aggregation relies on an in-memory hash table plus hash partitioning to temporary storage. Cost-based query optimization chooses which algorithm to use based on several factors, including the sort order of the input, input and output sizes, and the need for sorted output. For example, hash-based aggregation is ideal for output smaller than the available memory (e.g., Query 1 of TPC-H), whereas sorting the entire input and aggregating after sorting are preferable when both aggregation input and output are large and the output needs to be sorted for a subsequent operation such as a merge join. Unfortunately, the size information required for a sound choice is often inaccurate or unavailable during query optimization, leading to sub-optimal algorithm choices. In response, this article introduces a new algorithm for sort-based duplicate removal, grouping, and aggregation. The new algorithm always performs at least as well as both traditional hash-based and traditional sort-based algorithms. It can serve as a system's only aggregation algorithm for unsorted inputs, thus preventing erroneous algorithm choices. Furthermore, the new algorithm produces sorted output that can speed up subsequent operations. Google's F1 Query uses the new algorithm in production workloads that aggregate petabytes of data every day. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",aggregation; b-tree; duplicate removal; early aggregation; grouping; in-memory index; replacement selection; Sorting; wide merging,Data structures; Digital storage; Query processing; B trees; Duplicate removal; Early aggregation; Grouping; In-memory index; Input and outputs; Memory index; Queries optimization; Replacement selections; Wide merging; Sorting
