Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
"The uncertain web: Concepts, challenges, and current solutions",2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953211299&doi=10.1145%2f2847252&partnerID=40&md5=933eb5cb79911d75789d6d0975b541b6,"Uncertainty, incompleteness, and imprecision are common characteristics of the data and knowledge that users deal with in a wide range of domains and applications. Exploiting these uncertain data sources and services to their full potential raises important research challenges that relate to the different phases of their lifecycle. Modeling and representing the semantics of Web data sources and services is the first step toward the automation of their different related activities such as data and service querying, selection, integration, ranking, and composition. As the uncertainly of data and services could impact these activities, it should be considered as an integral part of data and service descriptions. The existing Web modeling languages and standards should be extended and used to represent the uncertainty of data and services.",,Semantic Web; Semantics; Uncertainty analysis; Integral part; Research challenges; Service description; Uncertain datas; Web data sources; Web modeling languages; Modeling languages
Introduction to theme section on trust in social networks and systems,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954222030&doi=10.1145%2f2835510&partnerID=40&md5=35223d4d5ee64b4eaf34a0991e9d2c97,[No abstract available],,
The relevance of categories for trusting information sources,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954319854&doi=10.1145%2f2803175&partnerID=40&md5=299ab9e38223ff12bdaacf3cf0c3eaea,"In this article, we are interested in the fact that relevance and trustworthiness of information acquired by an agent X from a source F strictly depends and derives from X's trust in F with respect to the kind of information. In particular, we are interested in analyzing the relevance of F's category as indicator for its trustworthiness with respect to the specific informative goals of X. In this article, we analyze an interactive cognitive model for searching information in a world in which each agent can be considered as belonging to a specific agent's category. We also consider variability within the canonical categorical behavior and consequent influence on the trustworthiness of provided information. The introduced interactive cognitive model also allows evaluation of the trustworthiness of a source both on the basis of its category and on past direct experience with it, thus selecting the more adequate source with respect to the informative goals to achieve. We present a computational approach based on fuzzy sets and some selected simulation scenarios together with the discussion of their more interesting results. Copyright © 2015 ACM.",Cognitive model; Social simulation; Trust,Cognitive model; Computational approach; Direct experience; Information sources; Social simulations; Trust; Internet
Chasing offensive conduct in social networks: A reputation-based practical approach for Frisber,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954308227&doi=10.1145%2f2797139&partnerID=40&md5=cbcede2103639b77924cd1587f64b509,"Social network users take advantage of anonymity to share rumors or gossip about others, making it important to provide means to report offensive conduct. This article presents a proposal to automatically manage these reports. We consider not only the users' public behavior, but also private messages between users. The automatic approach is based, in both cases, on the reporters' reputation along with other metrics intrinsic to social networks. Promising results from adopting the proposed reporting methods on Frisber, a geolocalized social network in production, are presented as well as some experiments based on real data extracted from Frisber. Copyright © 2015 ACM.",Abusive user; Offensive content; Reporting system; Social networks; Trust assessment,Internet; Abusive user; Automatic approaches; Offensive content; Reporting systems; Trust assessments; Social networking (online)
On selecting recommenders for trust evaluation in online social networks,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948955639&doi=10.1145%2f2807697&partnerID=40&md5=f0e5a9b07c0815582701513ae2cfba7b,"Trust is a central component of social interactions among humans. Many applications motivate the consideration of trust evaluation in online social networks (OSNs). Some work has been proposed based on a trusted graph. However, it is still an open challenge to construct a trusted graph, especially in terms of selecting proper recommenders, which can be used to predict the trustworthiness of an unknown target efficiently and effectively. Based on the intuition that people who are close to and influential to us can make more proper and acceptable recommendations, we present the idea of recommendation-aware trust evaluation (RATE). We further model the recommender selection problem as an optimization problem, with the objectives of higher accuracy, lower risk (uncertainty), and lower cost. Four metrics: trustworthiness, expertise, uncertainty, and cost, are identified to measure and adjust the quality of recommenders. We focus on a 1-hop recommender selection, for which we propose the FluidTrust model to better illustrate the trust-decisionmaking process of a user. We also discuss the extension of multihop scenarios and multitarget scenarios. Experimental results, with the real social network datasets of Epinions and Advogato, validate the effectiveness of RATE: it can predict trust with higher accuracy (it gains about 20% higher accuracy in Epinions), lower risk, and less cost (about a 30% improvement). © 2015 ACM 1533-5399/2015/11-ART16 $15.00.",Online social networks (OSNs); Recommendation-aware; Recommender selection; Trust evaluation,Costs; Optimization; Decision making process; On-line social networks; Online social networks (OSNs); Optimization problems; Recommendation-aware; Recommender selection; Social interactions; Trust evaluation; Social networking (online)
Modelling the role of trust in social relationships,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948994369&doi=10.1145%2f2815620&partnerID=40&md5=0a6d996d589ffd61f0e92179780eb942,"A social trust model is presented for investigating social relationships and social networks in the real world and in social media. The results demonstrate that multilevel social structures, with a few strong relationships, more medium ties, and large numbers of weak ties, emerge in an evolutionary simulation when well-being and alliances are rewarded with high levels of social interaction. 'Favour-the-few' trust strategies were more competitive than others under a wide range of fitness conditions, suggesting that the development of complex, multilevel social structures depends on capacity for high investment in social time and preferential social interaction strategies. © 2015 ACM 1533-5399/2015/11-ART16 $15.00.",Social brain hypothesis; Social media; Social relationships; Trust,Complex networks; Social networking (online); Social sciences; Evolutionary simulations; Social brains; Social interactions; Social media; Social relationships; Social structure; Trust; Trust modeling; Social aspects
Progressive batch medical image retrieval processing in mobile wireless networks,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941568790&doi=10.1145%2f2783437&partnerID=40&md5=7e0780f74ed4a64eef76706b192626b8,"This article addresses a multi-query optimization problem for distributed medical image retrieval in mobile wireless networks by exploiting the dependencies in the derivation of a retrieval evaluation plan. To the best of our knowledge, this is the first work investigating batch medical image retrieval (BMIR) processing in a mobile wireless network environment. Four steps are incorporated in our BMIR algorithm. First, when a number of retrieval requests (i.e., m retrieval images and m radii) are simultaneously submitted by users, then a cost-based dynamic retrieval (CDRS) scheduling procedure is invoked to efficiently and effectively identify the correlation among the retrieval spheres (requests) based on a cost model. Next, an index-based image set reduction (ISR) is performed at the execution-node level in parallel. Then, a refinement processing of the candidate images is conducted to get the answers. Finally, at the transmissionnode level, the corresponding image fragment (IF) replicas are chosen based on an adaptive multi-resolution (AMR)-based IF replicas selection scheme, and transmitted to the user-node level by a priority-based IF replicas transmission (PIFT) scheme. The experimental results validate the efficiency and effectiveness of the algorithm in minimizing the response time and increasing the parallelism of I/O and CPU. © 2015 ACM.",Medical image; Mobile wireless network; Multi-resolution,Content based retrieval; Medical imaging; Wireless networks; Adaptive multi resolutions; Dynamic retrieval; Image fragments; Mobile wireless network; Multiquery optimization; Priority-based; Retrieval evaluation; Selection scheme; Medical image processing
P-DONAS: A P2P-based domain name system in access networks,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946069524&doi=10.1145%2f2808229&partnerID=40&md5=714795bdca3bfe32f4054186a448ed65,"The domain name system (DNS) includes infrastructures deployed by Internet service providers (ISPs) and third-party suppliers to ensure high responsiveness, resilience, and load sharing. This equipment implies high effort and energy for 24/7 operation. To facilitate cost reductions in this regard, P-DONAS-a peer-topeer (P2P)-based DNS-organizes access nodes (ANs) of an ISP's access network, which possess available resources, into a decentralized, self-organizing distributed hash table'based P2P network. Each AN acts as traditional DNS server and solely stores a piece of DNS data. DNS requests issued to an AN are resolved via P2P lookups while maintaining full compatibility with traditional DNS. The article discusses the application of P-DONAS as both a complement and an alternative to traditional DNS. Results from both simulations and a practical test arrangement prove P-DONAS' high scalability and its performance comparable to that of a commercial DNS name server relieving this name server by 53% to 75% of DNS traffic.",Domain name system; Embedded systems; Network architectures; Peerto-peer,Cost reduction; Embedded systems; Internet protocols; Internet service providers; Network architecture; Access network; Distributed Hash Table; DNS traffics; Domain name system; High scalabilities; Peer to peer; Practical tests; Third parties; Peer to peer networks
JCloudScale: Closing the gap between IaaS and PaaS,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941553393&doi=10.1145%2f2792980&partnerID=40&md5=e5b132c0e457b0bd8610387c09aaee43,"Building Infrastructure-as-a-Service (IaaS) applications today is a complex, repetitive, and error-prone endeavor, as IaaS does not provide abstractions on top of virtual machines. This article presents JCLOUDSCALE, a Java-based middleware for moving elastic applications to IaaS clouds, with minimal adjustments to the application code. We discuss the architecture and technical features, as well as evaluate our system with regard to user acceptance and performance overhead. Our user study reveals that JCLOUDSCALE allows many participants to build IaaS applications more efficiently, compared to industrial Platform-as-a-Service (PaaS) solutions. Additionally, unlike PaaS, JCLOUDSCALE does not lead to a control loss and vendor lock-in. © 2015 ACM.",Cloud computing; Jcloudscale; Middleware; Programming,Cloud computing; Java programming language; Mathematical programming; Middleware; Platform as a Service (PaaS); Application codes; Elastic applications; Error prones; Iaas clouds; Jcloudscale; Technical features; User acceptance; Virtual machines; Infrastructure as a service (IaaS)
Bottom-up fault management in service-based systems,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934754768&doi=10.1145%2f2739045&partnerID=40&md5=8c4599e4f9c8ac6dd32430aec154e968,"Service Oriented Architecture (SOA) enables the creation of distributed applications from independently developed and deployed services. As with any component-based system, the overall performance and quality of the system is an aggregate function of its component services. In this article, we present a novel approach for managing bottom-up faults in service-based systems. Bottom-up faults are a special case of system-wide exceptions that are defined as abnormal conditions or defects occurring in component services, which if not detected and/or managed, may lead to runtime failures. Examples of bottom-up faults include network outage, server disruption, and changes to service provisioning (e.g., new operation parameter required) that may have an impact on the way component services are consumed. We propose a soft-state signaling-based approach to propagate these faults from participants to composite services. Soft-state refers to a class of protocols where the state of a service is constantly refreshed by periodic messages, and user/service takes up the responsibility of communicating and maintaining its state. Soft-state-based protocols have a number of advantages including implicit error recovery and easier fault management, resulting in high availability for systems. Although soft-state has been widely used in various Internet protocols, this work is the first (to the best of our knowledge) to adopt soft-state for fault management in composite services. The proposed approach includes protocols for fault propagation (pure soft-state and soft-state with explicit removal) and fault reaction (rule-based). We also present experiment results to assess the performance and applicability of our approach. © 2015 ACM.",Advanced services invocation framework; Collaboration exchange protocol; Collaborative services delivery platform; Web services interchangeability,Aggregates; Failure analysis; Information services; Internet protocols; Quality of service; Service oriented architecture (SOA); Software engineering; Advanced services invocation framework; Collaboration exchange protocol; Collaborative services delivery platform; Component based systems; Distributed applications; Operation parameters; Service provisioning; Service-based systems; Web services
The effect of data caps upon ISP service tier design and users,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934759021&doi=10.1145%2f2774973&partnerID=40&md5=4dc0bfed288d26ee03caf5ccb3d22a71,"We model the design and impact of Internet pricing plans with data caps. We consider a monopoly ISP that maximizes its profit by setting tier prices, tier rates, network capacity, data caps, and overage charges. We show that when data caps are used to maximize profit, a monopoly ISP will keep the basic tier price the same, increase the premium tier rate, and decrease the premium tier price and the basic tier rate. We give analytical and numerical results to illustrate the increase in ISP profit, and the corresponding changes in user tier choices, user surplus, and social welfare. © 2015 ACM.",Business models; Charging; Pricing,Charging (furnace); Competition; Costs; Profitability; Business models; Internet pricing; Network Capacity; Numerical results; Social welfare; Internet service providers
Interaction-based recommendations for online communities,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934754559&doi=10.1145%2f2774974&partnerID=40&md5=94503ec34774b75b20fefb6ec0e8580a,"A key challenge in online communities is that of keeping a community active and alive. All online communities work hard to keep their members through various initiatives, such as personalisation and recommendation technologies. In online communities aimed at supporting behavioural change, that is, in domains such as diet, lifestyle, or the environment, the main reason for participation is not to connect with real-world friends for sharing and communicating, but to meet and gain support from like-minded people in an online environment. Introducing personalisation and recommendation features in these networks is challenging, as traditional approaches leverage the densely populated friendship relations found in typical social networks, and these are not present in these new community types. We address this challenge by looking beyond the articulated friendships of a community for evidence of relationships. In particular, we look at the interactions of members of an online community with other members and resources. In this article, we present a social behaviour model and apply it to two types of recommendation systems, a people recommender and a content recommender system. We evaluate our systems using the interaction logs of an online diet and lifestyle community in which 5,000 Australians participated in a 12-week programme. Our results show that our social behaviour-based recommendation algorithms outperform baselines, friendship-based, and link-prediction algorithms. © 2015 ACM.",Filtering; Online communities; Personalisation; Recommendation system; Social behaviour,Filtration; Recommender systems; Social networking (online); Behavioural changes; On-line communities; Online environments; Personalisation; Recommendation algorithms; Recommendation technologies; Social behaviour; Traditional approaches; Online systems
"P2P-based, multi-attribute resource discovery under real-world resources and queries",2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925302925&doi=10.1145%2f2729139&partnerID=40&md5=2850a3161a16fddf0bd14ecec9d48a7a,"Collaborative peer-to-peer (P2P), grid, and cloud computing rely on resource discovery (RD) solutions to aggregate groups of multi-attribute, dynamic, and distributed resources. However, specific characteristics of real-world resources and queries, and their impact on P2P-based RD, are largely unknown. We analyze the characteristics of resources and queries using data from four real-world systems. These characteristics are then used to qualitatively and quantitatively evaluate the fundamental design choices for P2P-based multi-attribute RD. The datasets exhibit several noteworthy features that affect the performance. For example, compared to uniform queries, real-world queries are relatively easier to resolve using unstructured, superpeer, and single-attribute-dominated query-based structured P2P solutions, as queries mostly specify only a small subset of the available attributes and large ranges of attribute values. However, all the solutions are prone to significant load balancing issues, as the resources and queries are highly skewed and correlated. The implications of our findings for improving RD solutions are also discussed. © 2015 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Multi-attribute queries; Multi-attribute resources; Peer-to-peer; Resource discovery; Simulation,Multi-attribute queries; Multi-attributes; Peer to peer; Resource discovery; Simulation; Internet
"Net neutrality: Discrimination, competition, and innovation in the UK and US",2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925295477&doi=10.1145%2f2700055&partnerID=40&md5=5231f55ea51c88b0514162f606c5a216,"We analyze UK and US experiences as they relate to two central net neutrality questions: (1) whether competition serves as a deterrent to the discriminatory treatment of Internet traffic, and (2) whether discrimination creates barriers to application development and innovation. Relying on consumer switching behavior to provide more comprehensive competitive discipline was insufficient for a variety of reasons, including the presence of switching costs. The process of correcting errors in the technology used for application-specific management revealed that such management creates costs for application developers and innovators, regardless of whether their products are targeted for traffic management.",Economics; Legal aspects,Economics; Internet; Application developers; Application development; Application specific; Internet traffic; Legal aspects; Net neutralities; Switching behaviors; Traffic management; Consumer behavior
Strategic formation of credit networks,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925305858&doi=10.1145%2f2700058&partnerID=40&md5=cd6a666ea3ca615fbe22f7d9d3e6181c,"Credit networks are an abstraction for modeling trust among agents in a network. Agents who do not directly trust each other can transact through exchange of IOUs (obligations) along a chain of trust in the network. Credit networks are robust to intrusion, can enable transactions between strangers in exchange economies, and have the liquidity to support a high rate of transactions. We study the formation of such networks when agents strategically decide how much credit to extend each other. We find strong positive network formation results for the simplest theoretical model. When each agent trusts a fixed set of other agents and transacts directly only with those it trusts, all pure-strategy Nash equilibria are social optima. However, when we allow transactions over longer paths, the price of anarchy may be unbounded. On the positive side, when agents have a shared belief about the trustworthiness of each agent, simple greedy dynamics quickly converge to a star-shaped network, which is a social optimum. Similar star-like structures are found in equilibria of heuristic strategies found via simulation studies. In addition, we simulate environments where agents may have varying information about each others' trustworthiness based on their distance in a social network. Empirical game analysis of these scenarios suggests that star structures arise only when defaults are relatively rare, and otherwise, credit tends to be issued over short social distances conforming to the locality of information. Overall, we find that networks formed by self-interested agents achieve a high fraction of available value, as long as this potential value is large enough to enable any network to form. © 2015 ACM 1533-5399/2015/02-ART3 $15.00.",Credit networks; Empirical gametheoretic analysis; Strategic network formation; Trust,Optimization; Credit networks; Empirical gametheoretic analysis; Self-interested agents; Simulation studies; Star-shaped networks; Strategic network formations; Theoretical modeling; Trust; Stars
Enabling social applications via decentralized social data management,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925334725&doi=10.1145%2f2700057&partnerID=40&md5=ba1e38eebe8ce771ffaf975c1cc0e0a4,"An unprecedented information wealth produced by online social networks, further augmented by location/collocation data, is currently fragmented across different proprietary services. Combined, it can accurately represent the social world and enable novel socially aware applications. We present Prometheus, a socially aware peer-to-peer service that collects social information from multiple sources into a multigraph managed in a decentralized fashion on user-contributed nodes, and exposes it through an interface implementing nontrivial social inferences while complying with user-defined access policies. Simulations and experiments on PlanetLab with emulated application workloads show the system exhibits good end-to-end response time, low communication overhead, and resilience to malicious attacks. © 2015 ACM 1533-5399/2015/02-ART1 $15.00.",Decentralized social graph; Distributed systems; P2P networks; Social inferences; Social sensors; Socially aware data management,Distributed computer systems; Distributed database systems; Network security; Peer to peer networks; Social aspects; Social networking (online); Distributed systems; P2P network; Social graphs; Social inferences; Social sensors; Information management
Aggregate characterization of user behavior in Twitter and analysis of the retweet graph,2015,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925339904&doi=10.1145%2f2700060&partnerID=40&md5=199e51b4cfce34d043babb5c517f341f,"Most previous analysis of Twitter user behavior has focused on individual information cascades and the social followers graph, in which the nodes for two users are connected if one follows the other. We instead study aggregate user behavior and the retweet graph with a focus on quantitative descriptions. We find that the lifetime tweet distribution is a type-II discrete Weibull stemming from a power law hazard function, that the tweet rate distribution, although asymptotically power law, exhibits a lognormal cutoff over finite sample intervals, and that the inter-tweet interval distribution is a power law with exponential cutoff. The retweet graph is small-world and scale-free, like the social graph, but less disassortative and has much stronger clustering. These differences are consistent with it better capturing the real-world social relationships of and trust between users than the social graph. Beyond just understanding and modeling human communication patterns and social networks, applications for alternative, decentralized microblogging systems-both predicting real-word performance and detecting spam-are discussed. © 2015 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Decentralized network architectures; Microblogging systems; Social network analysis,Aggregates; Network architecture; Sampling; Social networking (online); Weibull distribution; Decentralized network architecture; Exponential cutoff; Human communications; Information cascades; Microblogging; Quantitative description; Rate distributions; Social relationships; Behavioral research
Ontology-based query answering with group preferences,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920003703&doi=10.1145%2f2677207&partnerID=40&md5=195a6fd97c970f5d73eeaad2b5734f57,"The Web has recently been evolving into a system that is in many ways centered on social interactions and is now more and more becoming what is called the Social Semantic Web. One of the many implications of such an evolution is that the ranking of search results no longer depends solely on the structure of the interconnections amongWeb pages-instead, the social components must also come into play. In this article, we argue that such rankings can be based on ontological background knowledge and on user preferences. Another aspect that has become increasingly important in recent times is that of uncertainty management, since uncertainty can arise due to many uncontrollable factors. To combine these two aspects, we propose extensions of the Datalog+/- family of ontology languages that both allow for the management of partially ordered preferences of groups of users as well as uncertainty, which is represented via a probabilistic model. We focus on answering k-rank queries in this context, presenting different strategies to compute group preferences as an aggregation of the preferences of a collection of single users. We also study merging operators that are useful for combining the preferences of the users with those induced by the values obtained from the probabilistic model. We then provide algorithms to answer k-rank queries for DAQs (disjunctions of atomic queries) under these group preferences and uncertainty that generalizes top-k queries based on the iterative computation of classical skyline answers. We show that such DAQ answering in Datalog+/- can be done in polynomial time in the data complexity, under certain reasonable conditions, as long as query answering can also be done in polynomial time (in the data complexity) in the underlying classical ontology. Finally, we present a prototype implementation of the query answering system, as well as experimental results (on the running time of our algorithms and the quality of their results) obtained from real-world ontological data and preference models, derived from information gathered from real users, showing in particular that our approach is feasible in practice.",Datalog; Group preferences; Knowledge representation; Preference modeling; Ranking; Social choice; Social semantic web,Iterative methods; Knowledge representation; Learning to rank; Ontology; Polynomial approximation; Query processing; Semantic Web; Datalog; Group preferences; Preference modeling; Ranking; Social choice; Social semantic webs; Search engines
Integrating social and auxiliary semantics for multifaceted topic modeling in twitter,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919953989&doi=10.1145%2f2651403&partnerID=40&md5=5770472b902de97e322b1468e251f77d,"Microblogging platforms, such as Twitter, have already played an important role in recent cultural, social and political events. Discovering latent topics from social streams is therefore important for many downstream applications, such as clustering, classification or recommendation. However, traditional topic models that rely on the bag-of-words assumption are insufficient to uncover the rich semantics and temporal aspects of topics in Twitter. In particular, microblog content is often influenced by external information sources, such as Web documents linked from Twitter posts, and often focuses on specific entities, such as people or organizations. These external sources provide useful semantics to understand microblogs and we generally refer to these semantics as auxiliary semantics. In this article, we address the mentioned issues and propose a unified framework for Multifaceted Topic Modeling from Twitter streams. We first extract social semantics from Twitter by modeling the social chatter associated with hashtags. We further extract terms and named entities from linked Web documents to serve as auxiliary semantics during topic modeling. The Multifaceted Topic Model (MfTM) is then proposed to jointly model latent semantics among the social terms from Twitter, auxiliary terms from the linked Web documents and named entities. Moreover, we capture the temporal characteristics of each topic. An efficient online inference method for MfTM is developed, which enables our model to be applied to large-scale and streaming data. Our experimental evaluation shows the effectiveness and efficiency of our model compared with state-of-the-art baselines. We evaluate each aspect of our framework and show its utility in the context of tweet clustering.",Semantic enrichment; Social media; Topic model; Unsupervised learning,Semantics; Social networking (online); Unsupervised learning; Downstream applications; Effectiveness and efficiencies; Experimental evaluation; Micro-blogging platforms; Semantic enrichment; Social media; Temporal characteristics; Topic Modeling; Semantic Web
Introduction to the special issue on foundations of social computing,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920046296&doi=10.1145%2f2680536&partnerID=40&md5=fa82c6999d4b28834a6b26982523f2d4,[No abstract available],,
Revealing the city that we cannot see,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920011486&doi=10.1145%2f2677208&partnerID=40&md5=315502e63359c03aab0d3b5c7316a423,"We here investigate the potential of participatory sensor networks derived from location sharing systems, such as Foursquare, to understand the human dynamics of cities. We propose the City Image visualization technique, which builds a transition graph mapping people's movements between location categories, and demonstrate its use to identify similarities and differences of human dynamics across cities by clustering cities according to their citizens' routines. We also analyze centrality metrics of the transition graphs built for different cities, considering transitions between specific venues. We show that these metrics complement the City Image technique, contributing to a deeper understanding of city dynamics.",City dynamics; Location based social media; Participatory sensing; Urban-computing,Location; Sensor networks; Human dynamics; Image visualization; Location sharing; Participatory Sensing; Participatory sensor networks; Social media; Transition graphs; Urban computing; Dynamics
On the dynamics of social media popularity: A you tube case study,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920052636&doi=10.1145%2f2665065&partnerID=40&md5=ab5f7999c77f5ff29327d3a6f7f5e3db,"Understanding the factors that impact the popularity dynamics of social media can drive the design of effective information services, besides providing valuable insights to content generators and online advertisers. Taking YouTube as case study, we analyze how video popularity evolves since upload, extracting popularity trends that characterize groups of videos. We also analyze the referrers that lead users to videos, correlating them, features of the video and early popularity measures with the popularity trend and total observed popularity the video will experience. Our findings provide fundamental knowledge about popularity dynamics and its implications for services such as advertising and search.",Characterization; Popularity growth; Referrers; Social media; You Tube,Dynamics; Information services; Lead users; Referrers; Social media; YouTube; Social networking (online)
A commitment-based infrastructure for programming socio-technical systems,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920027142&doi=10.1145%2f2677206&partnerID=40&md5=f0f15a928fb409d112d2e6dd9a3ce361,"Socio-Technical Systems demand an evolution of computing into social computing, with a transition from an individualistic to a societal view. As such, they seem particularly suitable to realize multiparty, crossorganizational systems. Multi-Agent Systems are a natural candidate to realize Socio-Technical Systems. However, while Socio-Technical Systems envisage an explicit layer that contains the regulations that all parties must respect in their interaction, and thus preserve the agents' autonomy, current frameworks and platforms require to hard-code the coordination requirements inside the agents. We propose to explicitly represent the missing layer of Socio-Technical Systems in terms of social relationships among the involved parties, that is, in terms of a set of normatively defined relationships among two or more parties, subject to social control by monitoring the observable behaviour. In our proposal, social relationships are resources, available to agents, who use them in their practical reasoning. Both agents and social relationships are firstclass entities of the model. The work also describes 2COMM4JADE, a framework that realizes the proposal by extending the well-known JADE and CArtAgO. The impact of the approach on programming is explained both conceptually and with the help of an example. © 2014 ACM.",Artifacts; Commitments; Infrastructures; Interaction protocols; Middleware; Social relations; Sociotechnical systems,Autonomous agents; Middleware; Patient monitoring; Silicate minerals; Social aspects; Social computing; Artifacts; Commitments; Infrastructures; Inter-action protocols; Social relations; Sociotechnical systems; Multi agent systems
SESAME: Mining user digital footprints for fine-grained preference-aware social media search,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920033455&doi=10.1145%2f2677209&partnerID=40&md5=534434a82adcd020ced18c479e515552,"With the recent popularity of social network services, a significant volume of heterogeneous social media data is generated by users, in the form of texts, photos, videos and collections of points of interest, etc. Such social media data provides users with rich resources for exploring content, such as looking for an interesting video or a favorite point of interest. However, the rapid growth of social media causes difficulties for users to efficiently retrieve their desired media items. Fortunately, users' digital footprints on social networks such as comments massively reflect individual's fine-grained preference on media items, that is, preference on different aspects of the media content, which can then be used for personalized social media search. In this article, we propose SESAME, a fine-grained preference-aware social media search framework leveraging user digital footprints on social networks. First, we collect users' direct feedback on media content from their social networks. Second, we extract users' sentiment about the media content and the associated keywords from their comments to characterize their fine-grained preference. Third, we propose a parallel multituple based ranking tensor factorization algorithm to perform the personalized media item ranking by incorporating two unique features, viz., integrating an enhanced bootstrap sampling method by considering user activeness and adopting stochastic gradient descent parallelization techniques. We experimentally evaluate the SESAME framework using two datasets collected from Foursquare and YouTube, respectively. The results show that SESAME can subtly capture user preference on social media items and consistently outperform baseline approaches by achieving better personalized ranking quality.",Fine-grained user preference; Parallelization; Personalized search; Sentiment analysis; Social media; Tensor factorization,Factorization; Gradient methods; Sentiment analysis; Stochastic systems; Tensors; Fine grained; Parallelizations; Personalized search; Social media; Tensor factorization; Social networking (online)
Regulation of off-network pricing in a nonneutral network,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908582690&doi=10.1145%2f2663491&partnerID=40&md5=dbee90739452e9e4345ee6a8af7db7d4,"Representatives of several Internet service providers (ISPs) have expressed their wish to see a substantial change in the pricing policies of the Internet. In particular, they would like to see content providers (CPs) pay for use of the network, given the large amount of resources they use. This would be in clear violation of the ""network neutrality"" principle that had characterized the development of the wireline Internet. Our first goal in this article is to propose and study possible ways of implementing such payments and of regulating their amount. We introduce a model that includes the users' behavior, the utilities of the ISP and of the CPs, and, the monetary flow that involves the content users, the ISP and CP, and, in particular, the CP's revenues from advertisements. We consider various game models and study the resulting equilibria; they are all combinations of a noncooperative game (in which the ISPs and CPs determine how much they will charge the users) with a ""cooperative"" one on how the CP and the ISP share the payments.We include in our model a possible asymmetric weighting parameter (that varies between zero to one).We also study equilibria that arise when one of the CPs colludes with the ISP. We also study two dynamic game models as well as the convergence of prices to the equilibrium values. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Games; Network neutrality; Off-network pricing; Proportional sharing; Telecommunications policy; Two-sided market,Costs; Economics; Games; Network neutralities; Network pricing; Proportional sharing; Telecommunications policy; Two-sided markets; Internet service providers
Economic model-driven cloud service composition,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908565712&doi=10.1145%2f2651420&partnerID=40&md5=c21d2930154914c933fffa8d32712086,"This article considers cloud service composition from a decision analysis perspective. Traditional QoS-aware composition techniques usually consider the qualities available at the time of the composition because compositions are usually immediately consumed. This is fundamentally different in the cloud environment where the cloud service composition typically lasts for a relatively long period of time. The two most important drivers when composing cloud service are the long-term nature of the composition and the economic motivation for outsourcing tasks to the cloud. We propose an economic model, which we represent as a Bayesian network, to select and compose cloud services. We then leverage influence diagrams to model the cloud service composition. We further extend the traditional influence diagram problem to a hybrid one and adopt an extended Shenoy-Shafer architecture to solve such hybrid influence diagrams that include deterministic chance nodes. In addition, analytical and simulation results are presented to show the performance of the proposed composition approach. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Cloud service; Economic model; Service composition,Bayesian networks; Cloud environments; Cloud service compositions; Cloud services; Composition technique; Economic modeling; Influence diagram; QoS-aware; Service compositions; Quality of service
Secure team composition to thwart insider threats and cyber-espionage,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908592787&doi=10.1145%2f2663499&partnerID=40&md5=9b22eb2fc19b8993f0fd06529a785af5,We develop a formal nondeterministic game model for secure team composition to counter cyber-espionage and to protect organizational secrets against an attacker who tries to sidestep technical securitymechanisms by offering a bribe to a project team member. The game captures the adversarial interaction between the attacker and the project manager who has a secret she wants to protect but must share with a team of individuals selected from within her organization. Our interdisciplinary work is important in the face of the multipronged approaches utilized by well-motivated attackers to circumvent the fortifications of otherwise well-defended targets. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.,Access control; Cyber-espionage; Game theory; Human factor; Insider threat; Management of information security,Access control; Game theory; Human engineering; Cyber-espionage; Game models; Insider Threat; Interdisciplinary work; Project managers; Project team; Team composition; Human resource management
Revenue guarantees in the generalized second price auction,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908568656&doi=10.1145%2f2663497&partnerID=40&md5=4ec6536d9a92e9887759bb55b38f6a9c,"Sponsored search auctions are the main source of revenue for search engines. In such an auction, a set of utility maximizing advertisers competes for a set of ad slots. The assignment of advertisers to slots depends on the bids they submit; these bids may be different than the true valuations of the advertisers for the slots. Variants of the celebrated VCG auction mechanism guarantee that advertisers act truthfully and, under some assumptions, lead to revenue or social welfare maximization. Still, the sponsored search industry mostly uses generalized second price (GSP) auctions; these auctions are known to be nontruthful and suboptimal in terms of social welfare and revenue. In an attempt to explain this tradition, we study a Bayesian setting wherein the valuations of advertisers are drawn independently from a common regular probability distribution. In this setting, it is well known from the work of Myerson [1981] that the optimal revenue is obtained by the VCG mechanism with a particular reserve price that depends on the probability distribution. We show that, by appropriately setting the reserve price, the revenue over any Bayes-Nash equilibrium of the game induced by the GSP auction is at most a small constant factor away from the optimal revenue, improving previous results of Lucier et al. [2012]. Our analysis is based on the Bayes- Nash equilibrium conditions and the improved results are obtained by bounding the utility of each player at equilibrium using infinitely many deviating bids and also by developing novel prophet-like inequalities. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Generalized second price auction; Incomplete information games; Sponsored search auction design,Commerce; Computation theory; Game theory; Search engines; Bayes-Nash equilibrium; Constant factors; Incomplete information games; Second-price auction; Social welfare; Social welfare maximization; Sponsored search auctions; Sponsored searches; Probability distributions
Recommendation and weaving of reusable mashup model patterns for assisted development,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908582707&doi=10.1145%2f2663500&partnerID=40&md5=9f2fccc7dd21fa77a590c458ae7c13c4,"With this article, we give an answer to one of the open problems of mashup development that users may face when operating a model-driven mashup tool, namely the lack of modeling expertise. Although commonly considered simple applications, mashups can also be complex software artifacts depending on the number and types of Web resources (the components) they integrate. Mashup tools have undoubtedly simplified mashup development, yet the problem is still generally nontrivial and requires intimate knowledge of the components provided by the mashup tool, its underlying mashup paradigm, and of how to apply such to the integration of the components. This knowledge is generally neither intuitive nor standardized across different mashup tools and the consequent lack of modeling expertise affects both skilled programmers and end-user programmers alike. In this article, we show how to effectively assist the users of mashup tools with contextual, interactive recommendations of composition knowledge in the form of reusable mashup model patterns. We design and study three different recommendation algorithms and describe a pattern weaving approach for the one-click reuse of composition knowledge. We report on the implementation of three pattern recommender plugins for different mashup tools and demonstrate via user studies that recommending and weaving contextual mashup model patterns significantly reduces development times in all three cases. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Mashup patterns; Mashups; Pattern recommendation; Pattern weaving,Internet; Complex software; End user programmers; Mashup patterns; Mashup tools; Mashups; Pattern recommendation; Recommendation algorithms; Web resources; Application programs
Demand-invariant price relationships and market outcomes in competitive private commons,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908592754&doi=10.1145%2f2663495&partnerID=40&md5=b10a2f81462ac5b0f6c196f83c686837,"We introduce a private commons model that consists of network providers who serve a fixed primary demand and strategically price to improve their revenues from an additional secondary demand. For general forms of secondary demand, we establish the existence and uniqueness of two characteristic prices: the break-even price and the market sharing price. We show that the market sharing price is always greater than the break-even price, leading to a price interval in which a provider is both profitable and willing to share the demand. Making use of this result, we give insight into the nature of market outcomes. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Competitivemarket analysis; Dynamic spectrum sharing; Game theory; Pricing and profitability,Costs; Game theory; Profitability; Break-even prices; Competitivemarket analysis; Dynamic spectrum sharing; Existence and uniqueness; Market outcomes; Network provider; Commerce
Understanding quota dynamics in wireless networks,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908592755&doi=10.1145%2f2663494&partnerID=40&md5=98f03d8eed9615ae362c39bed4a14e5a,"In designing new service plans, network service providers need to understand how consumption of voice or data service will change in response to pricing signals. It is difficult to acquire such information from customer usage data because voice minutes and data bandwidth are typically sold in the form of large quotas. We address this issue by studying how end-users consume their quotas, both in a prepaid setting (where users pay in advance and refill as needed) and a postpaid setting (where users pay each month for a fixed amount of quota). Our presentation has three main parts. In the first we present data on quota usage for prepaid voice/text services and show that users reduce their voice usage when their balances become low. Moreover, when balances are low there is a tendency to shift from voice to SMS. In the second part, we provide descriptive models of both prepaid and postpaid services. The main feature of these models is that there is a background level of potential demand and the rate at which this potential demand is realized depends on the amount of quota balance available. In the third part, we propose utility maximizing models that can account for this type of behavior. In the prepaid case the main feature of the model is a discount function that represents the perceived cost to the user of a quota refill that will occur sometime in the future. In the postpaid case, where the end-user is attempting to get the maximum amount of utility from his monthly quota, we present a dynamic programming formulation in which utility functions are time varying and not known to the user in advance. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Network pricing; Quota dynamics; Wireless communications,Costs; Wireless networks; Background level; Descriptive Model; Network pricing; Network service providers; Perceived costs; Potential demand; Utility functions; Wireless communications; Dynamic programming
Adoption of bundled services with network externalities and correlated affinities,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908577671&doi=10.1145%2f2663493&partnerID=40&md5=ab754118c83a68875ab5241efefeff8c,"The goal of this article is to develop a principled understanding of when it is beneficial to bundle technologies or services whose value is heavily dependent on the size of their user base, that is, exhibits positive exernalities. Of interest is how the joint distribution, and in particular the correlation, of the values users assign to components of a bundle affect its odds of success. The results offer insight and guidelines for deciding when bundling new Internet technologies or services can help improve their overall adoption. In particular, successful outcomes appear to require a minimum level of value correlation. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Adoption; Correlation; Internet services; User valuation,Correlation methods; Internet; Adoption; Bundled service; Internet services; Internet technology; Joint distributions; Network externality; Web services
Is the price of anarchy the right measure for load-balancing games?,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908587448&doi=10.1145%2f2663498&partnerID=40&md5=56c6b190d49881fe5338c899510dc070,"Price of anarchy is an oft-used worst-case measure of the inefficiency of noncooperative decentralized architectures. For a noncooperative load-balancing game with two classes of servers and for a finite or infinite number of dispatchers, we show that the price of anarchy is an overly pessimistic measure that does not reflect the performance obtained in most instances of the problem. We explicitly characterize the worst-case traffic conditions for the efficiency of noncooperative load-balancing schemes and show that, contrary to a common belief, the worst inefficiency is in general not achieved in heavy traffic. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Atomic games; Game theory; Inefficiency; Load balancing; Price of anarchy,Internet; Resource allocation; Atomic games; Decentralized architecture; Heavy traffics; Inefficiency; Infinite numbers; Load-balancing schemes; Price of anarchy; Traffic conditions; Game theory
Special issue on pricing and incentives in networks and systems: Guest Editors' introduction,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908592758&doi=10.1145%2f2665064&partnerID=40&md5=d1e25e759dd52a4c8d1ce7d754be17c4,[No abstract available],,
Influence of search neutrality on the economics of advertisement-financed content,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908582689&doi=10.1145%2f2663490&partnerID=40&md5=2a497859f8b739282ac5703b6339d5b3,"The search neutrality debate questions the ranking methods of search engines. We analyze the issue when content providers offer content for free, but get revenues from advertising.We investigate the noncooperative game among competing content providers under different ranking policies. When the search engine is not involved with high-quality content providers, it should adopt neutral ranking, also maximizing user quality-of-experience. If the search engine controls high-quality content, favoring its ranking and adding advertisement yield a larger revenue. Though user perceived quality may not be impaired, the advertising revenues of the other content providers drastically decrease. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Computational advertising; Content providers; Game theory; Search engines; Search neutrality,Computation theory; Economics; Quality of service; Search engines; User experience; Advertising revenues; Computational advertisings; Content providers; Noncooperative game; Perceived quality; Quality of experience (QoE); Ranking methods; Search neutrality; Game theory
Economic viability of Paris metro pricing for digital services,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908565592&doi=10.1145%2f2663492&partnerID=40&md5=b19d3bf238d9162125c6b6b87354b37d,"Nowadays digital services, such as cloud computing and network access services, allow dynamic resource allocation and virtual resource isolation. This trend can create a new paradigm of flexible pricing schemes. A simple pricing scheme is to allocate multiple isolated service classes with differentiated prices, namely Paris Metro Pricing (PMP). The benefits of PMP are its simplicity and applicability to a wide variety of general digital services, without considering specific performance guarantees for different service classes. The central issue of our study is whether PMP is economically viable, namely whether it will produce more profit for the service provider and whether it will achieve more social welfare. Prior studies had only considered specific models and arrived at conflicting conclusions. In this article, we identify unifying principles in a general setting and derive general sufficient conditions that can guarantee the viability of PMP. We further apply the results to analyze various examples of digital services. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Cloud computing services; Internet economics; Pricing; Service classes,Cloud computing; Costs; Rail motor cars; Cloud computing services; Different services; Dynamic resource allocations; Economic viability; Economically viable; Internet economics; Performance guarantees; Service class; Economics
Approximation algorithms for secondary spectrum auctions,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908568641&doi=10.1145%2f2663496&partnerID=40&md5=e9c598c83358620e6c2cca7b04438ab6,"We study combinatorial auctions for secondary spectrum markets, where short-term communication licenses are sold to wireless nodes. Channels can be assigned to multiple bidders according to interference constraints captured by a conflict graph. We suggest a novel approach to such combinatorial auctions using a graph parameter called inductive independence number. We achieve good approximation results by showing that interference constraints for wireless networks imply a bounded inductive independence number. For example, in the physical model the factor becomes O(√klog2 n) for n bidders and k channels. Our algorithms can be turned into incentive-compatible mechanisms for bidders with arbitrary valuations. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Combinatorial auctions; Inductive independence number; SINR,Commerce; Approximation results; Combinatorial auction; Graph parameters; Incentive compatible mechanisms; Independence number; Interference constraints; Secondary spectrums; SINR; Approximation algorithms
Distributed content curation on the web,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908577103&doi=10.1145%2f2663489&partnerID=40&md5=83ec84c64a8f49ce519663fc4ca37b42,"In recent years there has been an explosive growth of digital content in the form of news feeds, videos, and original content on online platforms such as blogs and social networks. Indeed, such platforms have been used as a means of sharing and republishing information, leading to a large collection of content that users must sift through.We consider the problem of curating this vast catalogue of content such that aggregators or publishers can offer readers content that is of interest to them, with minimal spam. Under a game-theoretic model we obtain several results on the optimal content selection and on the efficiency of distributed curation. © 2014 ACM 1533-5399/2014/10-ART10 $15.00.",Budget of attention; Content curation; Game theory; Social networks,Budget control; Game theory; Budget of attention; Content curation; Digital contents; Distributed content; Explosive growth; Game-theoretic model; Online platforms; Optimal content; Social networking (online)
Event recognition challenges and techniques: Guest editors' introduction,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940323709&doi=10.1145%2f2632220&partnerID=40&md5=944856d04d8936a2168c53187847556f,"Event recognition refers to the detection of events that are considered relevant for processing, thereby providing the opportunity to implement reactive measures. There are five research challenges of event recognition, namely, multiscale temporal aggregation, uncertainty, distribution, pattern learning, and event forecasting. The July 2014 Special Issue of ACM Transactions on Internet Technology introduces six articles on the challenges of event recognition. Approximate Semantic Matching of Events for The Internet of Things, PADUA: Parallel Architecture to Detect Unexplained Activities, Adaptive Speculative Processing of Out-of-Order Event Streams, Decentralised Fault-Tolerant Event Correlation, MCEP: A Mobility-Aware Complex Event Processing System, and Efficient Stream Provenance via Operator Instrumentation are the articles discussed in this issue.",,Parallel architectures; Semantics; Complex event processing; Event correlation; Event recognition; Internet technology; Pattern Learning; Research challenges; Semantic matching; Temporal aggregation; Semantic Web
Efficient stream provenance via operator instrumentation,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906509305&doi=10.1145%2f2633689&partnerID=40&md5=18c14472fb7f712f8692ffd021464817,"Managing fine-grained provenance is a critical requirement for data stream management systems (DSMS), not only for addressing complex applications that require diagnostic capabilities and assurance, but also for providing advanced functionality, such as revision processing or query debugging. This article introduces a novel approach that uses operator instrumentation, that is, modifying the behavior of operators, to generate and propagate fine-grained provenance through several operators of a query network. In addition to applying this technique to compute provenance eagerly during query execution, we also study how to decouple provenance computation from query processing to reduce runtime overhead and avoid unnecessary provenance retrieval. Our proposals include computing a concise superset of the provenance (to allow lazily replaying a query and reconstruct its provenance) as well as lazy retrieval (to avoid unnecessary reconstruction of provenance). We develop stream-specific compression methods to reduce the computational and storage overhead of provenance generation and retrieval. Ariadne, our provenance-aware extension of the Borealis DSMS implements these techniques. Our experiments confirm that Ariadne manages provenance with minor overhead and clearly outperforms query rewrite, the current state of the art. © 2014 ACM.",Annotation; Data streams; Experiments; Provenance,Complex networks; Data communication systems; Experiments; Program diagnostics; Query languages; Annotation; Complex applications; Compression methods; Data stream; Data stream management systems; Diagnostic capabilities; Provenance; Runtime overheads; Cost reduction
Approximate semantic matching of events for the internet of things,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906487044&doi=10.1145%2f2633684&partnerID=40&md5=e39baed6997f06c19ab09c31e4c380d7,"Event processing follows a decoupled model of interaction in space, time, and synchronization. However, another dimension of semantic coupling also exists and poses a challenge to the scalability of event processing systems in highly semantically heterogeneous and dynamic environments such as the Internet of Things (IoT). Current state-of-the-art approaches of content-based and concept-based event systems require a significant agreement between event producers and consumers on event schema or an external conceptual model of event semantics. Thus, they do not address the semantic coupling issue. This article proposes an approach where participants only agree on a distributional statistical model of semantics represented in a corpus of text to derive semantic similarity and relatedness. It also proposes an approximate model for relaxing the semantic coupling dimension via an approximation-enabled rule language and an approximate event matcher. The model is formalized as an ensemble of semantic and top-k matchers along with a probability model for uncertainty management. The model has been empirically validated on large sets of events and subscriptions synthesized from real-world smart city and energy management systems. Experiments show that the proposed model achieves more than 95% F1Score of effectiveness and thousands of events/sec of throughput formedium degrees of approximation while not requiring users to have complete prior knowledge of event semantics. In semantically loosely-coupled environments, one approximate subscription can compensate for hundreds of exact subscriptions to cover all possibilities in environments which require complete prior knowledge of event semantics. Results indicate that approximate semantic event processing could play a promising role in the IoT middleware layer. © 2014 ACM.",Approximate matching; Distributional semantics; Event processing; Internet of Things; Semantic matching; Uncertainty,Energy management systems; Internet of things; Middleware; Approximate matching; Distributional semantics; Event Processing; Semantic matching; Uncertainty; Semantics
MCEP: A mobility-aware complex event processing system,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906517574&doi=10.1145%2f2633688&partnerID=40&md5=8cdd8b6c9f327f1affa8f70f3f0eb948,"With the proliferation of mobile devices and sensors, complex event proceesing (CEP) is becoming increasingly important to scalably detect situations in real time. Current CEP systems are not capable of dealing efficiently with highly dynamic mobile consumers whose interests change with their location. We introduce the distributed mobile CEP (MCEP) system which automatically adapts the processing of events according to a consumer's location. MCEP significantly reduces latency, network utilization, and processing overhead by providing on-demand and opportunistic adaptation algorithms to dynamically assign event streams and computing resources to operators of the MCEP system. © 2014 ACM.",Complex event processing; Migration; Mobility; Moving range queries,Carrier mobility; Mobile devices; Adaptation algorithms; Complex event processing; Complex events; Computing resource; Migration; Moving range queries; Net work utilization; Processing overhead; Complex networks
PADUA: Parallel architecture to detect unexplained activities,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906497580&doi=10.1145%2f2633685&partnerID=40&md5=62cb510a8119ab1d0eef62c8e297df32,"There are numerous applications (e.g., video surveillance, fraud detection, cybersecurity) in which we wish to identify unexplained sets of events. Most related past work has been domain-dependent (e.g., video surveillance, cybersecurity) and has focused on the valuable class of statistical anomalies in which statistically unusual events are considered. In contrast, suppose there is a set A of known activity models (both harmless and harmful) and a log L of time-stamped observations. We define a part L′ ⊆ L of the log to represent an unexplained situation when none of the known activity models can explain L′ with a score exceeding a userspecified threshold. We represent activities via probabilistic penalty graphs (PPGs) and show how a set of PPGs can be combined into one Super-PPG for which we define an index structure. Given a compute cluster of (K+1) nodes (one of which is a master node), we show how to split a Super-PPG into K subgraphs, each of which can be independently processed by a compute node. We provide algorithms for the individual compute nodes to ensure seamless handoffs that maximally leverage parallelism. PADUA is domain-independent and can be applied to many domains (perhaps with some specialization). We conducted detailed experiments with PADUA on two real-world datasets-the ITEA CANDELA video surveillance dataset and a network traffic dataset appropriate for cybersecurity applications. PADUA scales extremely well with the number of processors and significantly outperforms past work both in accuracy and time. Thus, PADUA represents the first parallel architecture and algorithm for identifying unexplained situations in observation data, offering both scalability and accuracy. © 2014 ACM.",Activity detection; Parallel computation; Temporal stochastic automata; Unexplained activities,Monitoring; Network architecture; Security systems; Stochastic systems; Activity detection; Domain independents; Parallel Computation; Real-world datasets; Stochastic Automata; Time-stamped observations; Unexplained activities; Video surveillance; Parallel architectures
Decentralized fault-tolerant event correlation,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906510866&doi=10.1145%2f2633687&partnerID=40&md5=0ec0ca59f59d1fca0a7c7a9b729b474d,"Despite the prognosed use of event correlation techniques for monitoring critical complex infrastructures or dealing with disasters in the physical world, little work exists on making event correlation systems themselves tolerant to failure. Existing systems either provide no guarantees on event deliveries, do not support multicast and thus provide no guarantees across individual processes, or then rely on centralized components or strong assumptions on the infrastructure. The FAIDECS system attempts to reconcile strong guarantees with practical performance in the presence of process crash failures. To that end, the FAIDECS system uses an overlay network with specific guarantees aligned with its proposed correlation language and guarantees. However, the language proposed lacks expressivity, and the system itself supports only very specific rigid semantics, incapable of supporting even fundamental features like sliding windows. After providing a comprehensive overview of the FAIDECS model and system, this article bridges the gap between strong guarantees and more established correlation languages and systems in several steps. First, we propose alternative semantics for several modules of the FAIDECS matching engine and revisit guarantees. Second, we pinpoint which guarantees are contradicted by which combinations of semantic options. Third, we investigate four correlation languages-StreamSQL, EQL, CEL, and TESLA-showing which semantic options their respective features correspond to in our model, and thus, ultimately, which guarantees of FAIDECS are maintained by which language features. © 2014 ACM.",Agreement; Correlation; Event; Fault tolerance; Guarantee; Order,Contracts; Correlation methods; Fault tolerance; Mobile telecommunication systems; Overlay networks; Alternative Semantics; Centralized components; Complex infrastructures; Event; Guarantee; Languages and systems; Order; Process crash failure; Semantics
Adaptive speculative processing of out-of-order event streams,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906502226&doi=10.1145%2f2633686&partnerID=40&md5=221c128d300ef88913be32b4ab935293,"Distributed event-based systems are used to detect meaningful events with low latency in high data-rate event streams that occur in surveillance, sports, finances, etc. However, both known approaches to dealing with the predominant out-of-order event arrival at the distributed detectors have their shortcomings: buffering approaches introduce latencies for event ordering, and stream revision approaches may result in system overloads due to unbounded retraction cascades. This article presents an adaptive speculative processing technique for out-of-order event streams that enhances typical buffering approaches. In contrast to other stream revision approaches developed so far, our novel technique encapsulates the event detector, uses the buffering technique to delay events but also speculatively processes a portion of it, and adapts the degree of speculation at runtime to fit the available system resources so that detection latency becomes minimal. Our technique outperforms known approaches on both synthetical data and real sensor data from a realtime locating system (RTLS) with several thousands of out-of-order sensor events per second. Speculative buffering exploits system resources and reduces latency by 40% on average. © 2014 ACM.",Distributed event processing; Low latency; Message-oriented middleware; Out-of-order event processing; Publish/subscribe; Speculative processing,Distributed event processing; Event Processing; Low latency; Message oriented middleware; Publish/subscribe; Internet
Personalizing top-k processing online in a peer-to-peer social tagging network,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905261555&doi=10.1145%2f2602572&partnerID=40&md5=23bbf874d87a768f9d1bf89ffb9f145e,"The rapidly increasing amount of user-generated content in social tagging systems provides a huge source of information. Yet, performing effective search in these systems is very challenging, especially when we seek the most appropriate items that match a potentially ambiguous query. Collaborative filtering-based personalization is appealing in this context, as it limits the search within a small network of participants with similar preferences. Offline personalization, which consists in maintaining, for every user, a network of similar participants based on their tagging behaviors, is effective for queries that are close to the querying user's tagging profile but performs poorly when the queries, reflecting emerging interests, have little correlation with the querying user's profile. We present P2TK2, the first protocol to personalize query processing in social tagging systems online. P2TK2 is completely decentralized, and this design choice stems from the observation that the evolving social tagging systems naturally resemble P2P systems where users are both producers and consumers. This design exploits the power of the crowd and prevents any central authority from controlling personal information. P2TK2 is gossip-based and probabilistic. It dynamically associates each user with social acquaintances sharing similar tagging behaviors. Appropriate users for answering a query are discovered at query time with the help of social acquaintances. This is achieved according to the hybrid interest of the querying user, taking into account both her tagging behavior and her query. Results are iteratively refined and returned to the querying user. We evaluate P2TK2 on CiteULike and Delicious traces involving up to 50,000 users. We highlight the advantages of online personalization compared to offline personalization, as well as its efficiency, scalability, and inherent ability to cope with user departure and interest evolution in P2P systems. © 2014 ACM.",Gossip; Online; Peer-to-peer systems; Personalization; Social tagging networks; Top-k processing,Distributed computer systems; Query processing; Social networking (online); Gossip; Online; Peer-to-Peer system; Personalizations; Social tagging networks; Top-k processing; User interfaces
Web service compositions with fuzzy preferences: A graded dominance relationship-based approach,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905217669&doi=10.1145%2f2576231&partnerID=40&md5=a2e1f0307a3bdf0c8974b3d5b71703c4,"Data-driven Web services build on service-oriented technologies to provide an interoperable method of interacting with data sources on top of the Web. Data Web services composition has emerged as a flexible solution to answer users' complex queries on the fly. However, as the number of Web services on the Web grows quickly, a large number of candidate compositions that would use different (most likely competing) services may be used to answer the same query. User preferences are a key factor that can be used to rank candidate services/compositions and retain only the best ones. In this article, we present a novel approach for computing the top-k data service compositions based on user preferences. In our approach, we model user preferences using fuzzy sets and incorporate them into the composition query. We use an efficient RDF query rewriting algorithm to determine the relevant services that may be used to answer the composition query. We match the (fuzzy) constraints of the relevant services to those of the query and determine their matching degrees using a set of matching methods. We then rank-order the candidate services based on a fuzzification of Pareto dominance and compute the top-k data service compositions. In addition, we introduce a new method for increasing the diversity of returned top-k compositions while maintaining as much as possible the compositions with the highest scores. Finally, we describe the architecture of our system and present a thorough experimental study of our proposed techniques and algorithms. The experimental study demonstrates the efficiency and the effectiveness of our techniques in different settings. © 2014 ACM.",Diversity; Preference queries; Service composition; Top-k; Web services,Fuzzy sets; Quality of service; Query processing; Web services; Websites; Diversity; Fuzzy preferences; Matching methods; Pareto dominance; Preference queries; Service compositions; Top-k; Web service composition; Interoperability
Equipping IDEs with XML-path reasoning capabilities,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905235196&doi=10.1145%2f2602573&partnerID=40&md5=7f1325f669477c8a2d10378aeba028a7,"One of the challenges in Web development is to achieve a good level of quality in terms of code size and runtime performance for popular domain-specific languages such as XQuery, XSLT, and XML Schema. We present the first IDE augmented with static detection of inconsistent XPath expressions that assists the programmer with simplifying development and debugging of any application involving XPath expressions. The tool is based on newly developed formal verification techniques based on expressive modal logics, which are now mature enough to be introduced in the process of software development. We further develop this idea in the context of XQuery for which we introduce an analysis for identifying and eliminating dead code automatically. This proof of concept aims at illustrating the benefits of equipping modern IDEs with reasoning capabilities. © 2014 ACM.",Analysis; Compile time; Environment; Path; Programming; Query; Reasoning; Schema; XML,Mathematical programming; Problem oriented languages; Software design; Analysis; Compile time; Environment; Path; Query; Reasoning; Schema; XML
Dissecting darknets: Measurement and performance analysis,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901932989&doi=10.1145%2f2611527&partnerID=40&md5=7afde67aa82fa114deb28b5c99615f32,"BitTorrent (BT) plays an important role in Internet content distribution. Because public BTs suffer from the free-rider problem, Darknets are becoming increasingly popular, which use Sharing Ratio Enforcement to increase their efficiency. We crawled and traced 17 Darknets from September 2009 to February 2011, and obtained datasets about over 5 million torrents. We conducted a broad range of measurements, including traffic, sites, torrents, and users activities. We found that some of the features of Darknets are noticeably different from public BTs. The results of our study reflect both macroscopic and microscopic aspects of the overall ecosystem of BitTorrent Darknets.",Analysis; BitTorrent; Darknets; Peer-to-peer networks; Private tracker,Peer to peer networks; Analysis; Bit torrents; Darknets; Free-rider problem; Internet content; Macroscopic and microscopic; Performance analysis; Private tracker; Distributed computer systems
Cheating in online games: A social network perspective,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901917719&doi=10.1145%2f2602570&partnerID=40&md5=262b7385b2cf6b02faf52949259db120,"Online gaming is a multi-billion dollar industry that entertains a large, global population. One unfortunate phenomenon, however, poisons the competition and spoils the fun: cheating. The costs of cheating span from industry-supported expenditures to detect and limit it, to victims' monetary losses due to cyber crime. This article studies cheaters in the Steam Community, an online social network built on top of the world's dominant digital game delivery platform. We collected information about more than 12 million gamers connected in a global social network, of which more than 700 thousand have their profiles flagged as cheaters. We also observed timing information of the cheater flags, as well as the dynamics of the cheaters' social neighborhoods. We discovered that cheaters are well embedded in the social and interaction networks: their network position is largely indistinguishable from that of fair players. Moreover, we noticed that the number of cheaters is not correlated with the geographical, real-world population density, or with the local popularity of the Steam Community. Also, we observed a social penalty involved with being labeled as a cheater: cheaters lose friends immediately after the cheating label is publicly applied. Most importantly, we observed that cheating behavior spreads through a social mechanism: the number of cheater friends of a fair player is correlated with the likelihood of her becoming a cheater in the future. This allows us to propose ideas for limiting cheating contagion.",Cheating in online games; Contagion; Diffusion; Social networks,Diffusion; Internet; Population dynamics; Population statistics; Cheating in online games; Contagion; Global population; Interaction networks; On-line gaming; On-line social networks; Population densities; Timing information; Social networking (online)
Cost-aware cloud bursting for enterprise applications,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901911448&doi=10.1145%2f2602571&partnerID=40&md5=d4f0284452ad903847a90814546b387b,"The high cost of provisioning resources to meet peak application demands has led to the widespread adoption of pay-as-you-go cloud computing services to handle workload fluctuations. Some enterprises with existing IT infrastructure employ a hybrid cloud model where the enterprise uses its own private resources for the majority of its computing, but then ""bursts"" into the cloud when local resources are insufficient. However, current commercial tools rely heavily on the system administrator's knowledge to answer key questions such as when a cloud burst is needed and which applications must be moved to the cloud. In this article, we describe Seagull, a system designed to facilitate cloud bursting by determining which applications should be transitioned into the cloud and automating the movement process at the proper time. Seagull optimizes the bursting of applications using an optimization algorithm as well as a more efficient but approximate greedy heuristic. Seagull also optimizes the overhead of deploying applications into the cloud using an intelligent precopying mechanism that proactively replicates virtualized applications, lowering the bursting time from hours to minutes. Our evaluation shows over 100% improvement compared to solutions but produces more expensive solutions compared to ILP. However, the scalability of our greedy algorithm is dramatically better as the number of VMs increase. Our evaluation illustrates scenarios where our prototype can reduce cloud costs by more than 45% when bursting to the cloud, and that the incremental cost added by precopying applications is offset by a burst time reduction of nearly 95%.",Hybrid clouds; Live migration; Prototype; Resource management,Algorithms; Cloud computing; Industry; Optimization; Cloud computing services; Enterprise applications; Hybrid cloud modeling; Hybrid clouds; Live migrations; Optimization algorithms; Prototype; Resource management; Costs
Microcomputations as micropayments in web-based services,2014,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901915970&doi=10.1145%2f2611526&partnerID=40&md5=ccdcb45ed0773f4f90ae925f1c1b565e,"In this article, we propose a new micropayment model for nonspecialized commodity web-services based on microcomputations. In our model, a user that wishes to access online content (offered by a website) does not need to register or pay to access the website; instead, he will accept to run microcomputations on behalf of the service provider in exchange for access to the content. These microcomputations can, for example, support ongoing computing projects that have clear social benefits (e.g., projects relating to medical research) or can contribute towards commercial computing projects. We analyze the security and privacy of our proposal and we show that it preserves the privacy of users. We argue that this micropayment model is economically and technically viable and that it can be integrated in existing distributed computing frameworks (e.g., the BOINC platform). In this respect, we implement a prototype of a system based on our model and we deploy our prototype on Amazon Mechanical Turk to evaluate its performance and usability given a large number of users. Our results show that our proposed scheme does not affect the browsing experience of users and is likely to be used by a non-trivial proportion of users. Finally, we empirically show that our scheme incurs comparable bandwidth and CPU consumption to the resource usage incurred by online advertisements featured in popular websites.",Distributed computing; Microcomputations; Micropayments; Monetization; Privacy,Data privacy; Distributed computer systems; Internet; Amazon mechanical turks; Computing projects; Distributed computing frameworks; Microcomputations; Micropayments; Monetization; Online advertisements; Security and privacy; Websites
A graph analytical approach for topic detection,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891823783&doi=10.1145%2f2542214.2542215&partnerID=40&md5=2d0abef37476cd91b377065f88b2e518,"Topic detection with large and noisy data collections such as social media must address both scalability and accuracy challenges. KeyGraph is an efficient method that improves on current solutions by considering keyword cooccurrence. We show that KeyGraph has similar accuracy when compared to state-of-the-art approaches on small, well-annotated collections, and it can successfully filter irrelevant documents and identify events in large and noisy social media collections. An extensive evaluation using Amazon's Mechanical Turk demonstrated the increased accuracy and high precision of KeyGraph, as well as superior runtime performance compared to other solutions. © 2013 ACM.",Community detection; KeyGraphbased Topic Detection; Network analysis; Topic detection,Electric network analysis; Amazon's mechanical turks; Analytical approach; Co-occurrence; Community detection; Run-time performance; Social media; State-of-the-art approach; Topic detection; Internet
LAKE: A server-side authenticated key-establishment with low computational workload,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891795189&doi=10.1145%2f2542214.2542216&partnerID=40&md5=0a1221c1c2d3d5a3d96161280c2b6b41,"Server-side authenticated key-establishment protocols are characterized by placing a heavy workload on the server. We propose LAKE: a new protocol that enables amortizing servers' workload peaks by moving most of the computational burden to the clients. We provide a formal analysis of the LAKE protocol under the Canetti-Krawczyk model and prove it to be secure. To the best of our knowledge, this is the most computationally efficient authenticated key-establishment ever proposed in the literature. © 2013 ACM.",Experimentation; Performance; Security,Internet; Canetti-Krawczyk model; Computational burden; Computational workload; Computationally efficient; Experimentation; Formal analysis; Performance; Security; Lakes
A multimedia recommender system,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890387806&doi=10.1145%2f2532640&partnerID=40&md5=7906c9240b9b80fb15b5e2fca2f44525,"The extraordinary technological progress we have witnessed in recent years has made it possible to generate and exchange multimedia content at an unprecedented rate. As a consequence, massive collections of multimedia objects are now widely available to a large population of users. As the task of browsing such large collections could be daunting, Recommender Systems are being developed to assist users in finding items that match their needs and preferences. In this article, we present a novel approach to recommendation in multimedia browsing systems, based on modeling recommendation as a social choice problem. In social choice theory, a set of voters is called to rank a set of alternatives, and individual rankings are aggregated into a global ranking. In our formulation, the set of voters and the set of alternatives both coincide with the set of objects in the data collection. We first define what constitutes a choice in the browsing domain and then define a mechanism to aggregate individual choices into a global ranking. The result is a framework for computing customized recommendations by originally combining intrinsic features of multimedia objects, past behavior of individual users, and overall behavior of the entire community of users. Recommendations are ranked using an importance ranking algorithm that resembles the well-known PageRank strategy. Experiments conducted on a prototype of the proposed system confirm the effectiveness and efficiency of our approach. © 2013 ACM 1533-5399/2013/11-ART3 $15.00.",Multimedia browsing; Recommender systems,Recommender systems; Effectiveness and efficiencies; Intrinsic features; Multimedia browsing; Multimedia contents; Multimedia recommender; Social choice problems; Social choice theory; Technological progress; Aggregates
A context-based approach to reconciling data interpretation conflicts in web services composition,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890371035&doi=10.1145%2f2532638&partnerID=40&md5=23fdc79ccacca497b9ffd9092c784fc7,"We present a comprehensive classification of data misinterpretation problems and develop an approachto automatic detection and reconciliation of data interpretation conflicts in Web services composition. The approach uses a lightweight ontology augmented with modifiers, contexts, and atomic conversions between the contexts. The WSDL descriptions of Web services are annotated to establish correspondences to the ntology. Given the naive Business Process Execution Language (BPEL) specification of the desired Web services composition with data interpretation conflicts, the approach can automatically detect the conflicts and produce the corresponding mediated BPEL. Finally, we develop a prototype to validate and evaluate the approach. © 2013 ACM 1533-5399/2013/11-ART1 $15.00.",Composition; Context; Mediation; Semantics; Web services,Chemical analysis; Data processing; Semantics; Websites; Automatic Detection; Business Process Execution Language; Classification of data; Context; Data interpretation; Lightweight ontology; Mediation; Web services composition; Web services
Investigating Users' perspectives of web single sign-on: Conceptual gaps and acceptance model,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890415233&doi=10.1145%2f2532639&partnerID=40&md5=b46284a659cad01f53cd5b91f41ec455,"OpenID and OAuth are open and simple Web SSO protocols that have been adopted by major service providers, and millions of supporting Web sites. However, the average user's perception of Web SSO is still poorly understood. Through several user studies, this work investigates users' perceptions and concerns when using Web SSO for authentication. We found that our participants had several misconceptions and concerns that impeded their adoption. This ranged from their inadequate mental models of Web SSO, to their concerns about personal data exposure, and a reduction in perceived Web SSO value due to the employment of password management practices. Informed by our findings, we offer a Web SSO technology acceptance model, and suggest design improvements. © 2013 ACM 1533-5399/2013/11-ART2 $15.00.",OAuth; OpenID; Usable security; Web single sign-on,Authentication; Design improvements; OAuth; OpenID; Password management; Single sign on; Technology acceptance model; Usable security; User's perceptions; Internet protocols
"A proxy view of quality of domain name service, poisoning attacks and survival strategies",2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878524483&doi=10.1145%2f2461321.2461324&partnerID=40&md5=0fd095ac4cee8c3aaa23e0d25e3db315,"The Domain Name System (DNS) provides a critical service for the Internet -mapping of user-friendly domain names to their respective IP addresses. Yet, there is no standard set of metrics quantifying the Quality of Domain Name Service (QoDNS), let alone a thorough evaluation of it. This article attempts to fill this gap from the perspective of a DNS proxy/cache, which is the bridge between clients and authoritative servers. We present an analytical model of DNS proxy operations that offers insights into the design tradeoffs of DNS infrastructure and the selection of critical DNS parameters. Due to the critical role DNS proxies play in QoDNS, they are the focus of attacks including cache poisoning attack. We extend the analytical model to study DNS cache poisoning attacks and their impact on QoDNS metrics. This analytical study prompts us to present Domain Name Cross-Referencing (DoX), a peer-to-peer systems for DNS proxies to cooperatively defend cache poisoning attacks. Based on QoDNS, we compare DoX with the cryptography-based DNS Security Extension (DNSSEC) to understand their relative merits.© 2013 ACM.",Cache; DNS; Poisoning; Proxy; QoDNS,Analytical models; Catalyst poisoning; Critical infrastructures; Models; Proxy caches; Cache; Cache poisoning attacks; DNS; DNS cache poisoning attacks; Domain name service; Peer-to-Peer system; Proxy; QoDNS; Internet protocols
Providing users' anonymity in mobile hybrid networks,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878595251&doi=10.1145%2f2461321.2461322&partnerID=40&md5=d41fd5f62d7eb1ce3d5c79838be24cc3,"We present a novel hybrid communication protocol that guarantees mobile users' anonymity against a widerange of adversaries by exploiting the capability of handheld devices to connect to both WiFi and cellular networks. Unlike existing anonymity schemes, we consider all parties that can intercept communications between a mobile user and a server as potential privacy threats. We formally quantify the privacy exposure and the protection of our system in the presence of malicious neighboring peers, global WiFi eavesdroppers, and omniscient mobile network operators, which possibly collude to breach user's anonymity or disrupt the communication. We also describe how a micropayment scheme that suits our mobile scenario can provide incentives for peers to collaborate in the protocol. Finally, we evaluate the network overhead and attack resiliency of our protocol using a prototype implementation deployed in Emulab and Orbit, and our probabilistic model.© 2013 ACM.",Anonymity; Mobile hybrid networks; Privacy,Computer crime; Data privacy; Global system for mobile communications; Wi-Fi; Anonymity; Hybrid network; Micro-payment schemes; Mobile network operators; Mobile scenarios; Network overhead; Probabilistic models; Prototype implementations; Peer to peer networks
Modeling decentralized reputation-based trust for initial transactions in digital environments,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878607795&doi=10.1145%2f2461321.2461323&partnerID=40&md5=8d4e08804590a75651adfb112b5f3407,"The advent of digital environments has generated significant benefits for businesses, organizations, governments, academia and societies in general. Today, over millions of transactions take place on the Internet. Although the widespread use of digital environments has generally provided opportunities for societies, a number of threats have limited their adoption. The de-facto standard today is for certification authorities to authenticate the identity of service providers while trust on the provided services is implied. This approach has certain shortcomings, for example, single point of failure, implied trust rather than explicit trust and others. One approach for minimizing such threats is to introduce an effective and resilient trust mechanism that is capable of determining the trustworthiness of service providers in providing their services. Determining the trustworthiness of services reduces invalid transactions in digital environments and further encourages collaborations. Evaluating trustworthiness of a service provider without any prior historical transactions (i.e. the initial transaction) pose a number of challenging issues. This article presents TIDE - a decentralized reputation trust mechanism that determines the initial trustworthiness of entities in digital environments. TIDE improves the precision of trust computation by considering raters' feedback, number of transactions, credibility, incentive to encourage raters' participation, strategy for updating raters' category, and safeguards against dynamic personalities. Furthermore, TIDE classifies raters into three categories and promotes the flexibility and customization through its parameters. Evaluation of TIDE against several attack vectors demonstrates its accuracy, robustness and resilience.© 2013 ACM.",Digital environments online communities; Reputation; Trust; Trust models,Internet; Certification authorities; De facto standard; Digital environment; On-line communities; Reputation; Trust; Trust computations; Trust models; Societies and institutions
When amazon meets google: Product visualization by exploring multiple web sources,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896928890&doi=10.1145%2f2492690&partnerID=40&md5=dad6344b5dcf5b54d6cf0ae7c1856435,"Product visualization is able to help users easily get knowledge about the visual appearance of a product. It is useful in many application and commercialization scenarios. However, the existing product image search on e-commerce Web sites or general search engines usually get insufficient search results or return images that are redundant and not relevant enough. In this article, we present a novel product visualization approach that automatically collects a set of diverse and relevant product images by exploring multiple Web sources. Our approach simultaneously leverages Amazon and Google image search engines, which represent domain-specific knowledge resource and general Web information collection, respectively. We propose a conditional clustering approach that is formulated as an affinity propagation problem regarding the Amazon examples as information prior. The ranking information of Google image search results is also explored. In this way, a set of exemplars can be found from the Google search results and they are provided together with the Amazon example images for product visualization. Experiments demonstrate the feasibility and effectiveness of our approach. © 2013 ACM.",Image search; Product visualization; Web source,Internet; Websites; Affinity propagation; Clustering approach; Domain-specific knowledge; Image search; Image search engine; Product visualization; Visual appearance; Web sources; Search engines
Vision for TOIT,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896953416&doi=10.1145%2f2499926.2499929&partnerID=40&md5=bf9096580272136db7b6732ebac76a35,[No abstract available],,
Resolvers revealed: Characterizing DNS resolvers and their clients,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896918844&doi=10.1145%2f2499926.2499928&partnerID=40&md5=31e570157e1e4cd6a4d08211508b2a1d,"The Domain Name System (DNS) allows clients to use resolvers, sometimes called caches, to query a set of authoritative servers to translate host names into IP addresses. Prior work has proposed using the interaction between these DNS resolvers and the authoritative servers as an access control mechanism. However, while prior work has examined the DNS from many angles, the resolver component has received little scrutiny. Essential factors for using a resolver in an access control system, such as whether a resolver is part of an ISP's infrastructure or running on an end-user's system, have not been examined. In this study, we examine DNS resolver behavior and usage, from query patterns and reactions to nonstandard responses to passive association techniques to pair resolvers with their client hosts. In doing so, we discover evidence of security protocol support, misconfigured resolvers, techniques to fingerprint resolvers, and features for detecting automated clients. These measurements can influence the implementation and design of these resolvers and DNS-based access control systems. © 2013 ACM.",DNS resolvers; Security,Association reactions; Access control mechanism; DNS resolvers; Domain name system; End users; IP addresss; Query patterns; Security; Security protocols; Internet protocols
User and ISP rights of device attachment and device management,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891774932&doi=10.1145%2f2513227&partnerID=40&md5=2d969ac72f1be4c1911c29f6bfa2a6df,"Internet research often assumes users may connect devices without consent by their service providers. However, in many networks the service provider only allows use of devices obtained directly from the provider. We review how United States communications law addresses the rights of users to connect devices of their choice. We explicate a set of user and service provider rights. We propose legal requirements for attachment andmanagement of devices.We illustrate how these proposed regulations would affect the services currently offered on telephone, cable, satellite, video networks, and cellular networks, as well as on the Internet. © 2013 ACM.",Economics; Legal Aspects; Management,Cellular telephone systems; Cellular telephones; Economics; Laws and legislation; Management; Cellular network; Device management; Internet research; Legal aspects; Legal requirements; Service provider; Video networks; Internet service providers
Safe V chat: A system for obscene content detection in online video chat services,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896962606&doi=10.1145%2f2499926.2499927&partnerID=40&md5=ef7c586ce4f25724a654bd9fc86da29b,"Online video chat services such as Chatroulette, Omegle, and vChatter that randomly match pairs of users in video chat sessions are quickly becoming very popular, with over a million users per month in the case of Chatroulette. A key problem encountered in such systems is the presence of flashers and obscene content. This problem is especially acute given the presence of underage minors in such systems. This article presents SafeVchat, a novel solution to the problem of flasher detection that employs an array of image detection algorithms. A key contribution of the article concerns how the results of the individual detectors are fused together into an overall decision classifying a user as misbehaving or not, based on Dempster- Shafer theory. The article introduces a novel, motion-based skin detection method that achieves significantly higher recall and better precision. The proposed methods have been evaluated over real-world data and image traces obtained from Chatroulette.com. SafeVchat has been deployed in Chatroulette. A combination of SafeVchat with human moderation has resulted in banning as many as 50,000 inappropriate users per day on Chatoulette. Furthermore, offensive content on Chatoulette has dropped significantly from 33.08% (before SafeVchat installation) to 3.49% (after SafeVchat installation). © 2013 ACM.",Dempster-Shafer theory; Obscene content detection; Online video chat system,Internet; Chat system; Content detection; Dempster-shafer; Dempster-Shafer theory; Image detection; Online video; Real-world; Skin Detection; Image analysis
TOIT administrative updates,2013,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896985959&doi=10.1145%2f2499926.2499930&partnerID=40&md5=e0a0706ce81b797ca654577a47a43968,[No abstract available],,
Exploiting and maintaining materialized views for XML keyword queries,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878550906&doi=10.1145%2f2390209.2390212&partnerID=40&md5=f61e6eee1376e5b890c43364c318eefd,"Keyword query is a user-friendly mechanism for retrieving useful information from XML data in Web and scientific applications. Inspired by the performance benefits of exploiting materialized views when processing structured queries, we investigate the feasibility and present a general framework for answering XML keyword queries using materialized views. Then we develop an XML keyword search engine that leverages materialized views for query evaluation and maintains materialized views incrementally upon XML data update. Experimental evaluation demonstrates the significance and efficiency of our approach. © 2012 ACM.",Keyword search; Materialized view; XML,Query processing; Search engines; Experimental evaluation; Keyword search; Materialized view; Performance benefits; Query evaluation; Scientific applications; Structured queries; Xml keyword searches; XML
Using priced options to solve the exposure problem in sequential auctions,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878542706&doi=10.1145%2f2390209.2390211&partnerID=40&md5=668c9954d2fb59cdfb06d70bd5bf3115,"We propose a priced options model for solving the exposure problem of bidders with valuation synergies participating in a sequence of online auctions. We consider a setting in which complementary-valued items are offered sequentially by different sellers, who have the choice of either selling their item directly or through a priced option. In our model, the seller fixes the exercise price for this option, and then sells it through a first-price auction. We analyze this model from a decision-theoretic perspective and we show, for a setting where the competition is formed by local bidders (which desire a single item), that using options can increase the expected profit for both sides. Furthermore, we derive the equations that provide minimum and maximum bounds between which the bids of the synergy buyer are expected to fall, in order for both sides of the market to have an incentive to use the options mechanism. Next, we perform an experimental analysis of a market in which multiple synergy buyers are active simultaneously. We show that, despite the extra competition, some synergy buyers may benefit, because sellers are forced to set their exercise prices for options at levels which encourage participation of all buyers. © 2012 ACM.",Auction theory; Priced options,Commerce; Costs; Mergers and acquisitions; Profitability; Auction theory; Decision-theoretic; Expected profits; Experimental analysis; Exposure problem; Online auctions; Priced options; Sequential auctions; Sales
Minersoft: Software retrieval in grid and cloud computing infrastructures,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864867245&doi=10.1145%2f2220352.2220354&partnerID=40&md5=db5f4d0c25b8354c847ed5e2ea9ac342,"One of the main goals of Cloud and Grid infrastructures is to make their services easily accessible and attractive to end-users. In this article we investigate the problem of supporting keyword-based searching for the discovery of software files that are installed on the nodes of large-scale, federated Grid and Cloud computing infrastructures. We address a number of challenges that arise from the unstructured nature of software and the unavailability of software-related metadata on large-scale networked environments. We present Minersoft, a harvester that visits Grid/Cloud infrastructures, crawls their file systems, identifies and classifies software files, and discovers implicit associations between them. The results of Minersoft harvesting are encoded in a weighted, typed graph, called the Software Graph. A number of information retrieval (IR) algorithms are used to enrich this graph with structural and content associations, to annotate software files with keywords and build inverted indexes to support keyword-based searching for software. Using a real testbed, we present an evaluation study of our approach, using data extracted from productionquality Grid and Cloud computing infrastructures. Experimental results show that Minersoft is a powerful tool for software search and discovery. © 2012 ACM.",Cloud computing; Grid computing; Resource management; Software search engine,Cloud computing; Metadata; Search engines; End-users; Evaluation study; File systems; Grid and cloud computing; Grid infrastructures; Inverted indices; Nature of software; Networked environments; Resource management; Software search; Software search engines; Grid computing
Theory and network applications of balanced Kautz tree structures,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864883311&doi=10.1145%2f2220352.2220355&partnerID=40&md5=adbbe55c56c010d5fa39a2e2ae7606d3,"In order to improve scalability and to reduce the maintenance overhead for structured peer-to-peer (P2P) networks, researchers have proposed architectures based on several interconnection networks with a fixeddegree and a logarithmical diameter. Among existing fixed-degree interconnection networks, the Kautz digraph hasmany distinctive topological properties compared to others. It, however, requires that the number of peers have the some given values, determined by peer degree and network diameter. In practice, we cannot guarantee how many peers will join a P2P network at a given time, since a P2P network is typically dynamic with peers frequently entering and leaving. To address such an issue, we propose the balanced Kautz tree and Kautz ring structures. We further design a novel structured P2P system, called BAKE, based on the two structures that has the logarithmical diameter and constant degree, even the number of peers is an arbitrary value. By keeping a total ordering of peers and employing a robust locality-preserved resource placement strategy, resources that are similar in a single or multidimensional attributes space are stored on the same peer or neighboring peers. Through analysis and simulation, we show that BAKE achieves the optimal diameter and as good a connectivity as the Kautz digraph does (almost achieves the Moore bound), and supports the exact as well as the range queries efficiently. Indeed, the structures of balanced Kautz tree and Kautz ring we propose can also be applied to other interconnection networks after minimal modifications, for example, the de Bruijn digraph. © 2012 ACM.",Distributed hash table; Kautz digraph; Kautz tree; Peer-to-peer networks,Algorithms; Computation; Forestry; Neural Networks; Telecommunications; Directed graphs; Distributed computer systems; Fault tolerance; Forestry; Interconnection networks; Telecommunication networks; Topology; Trees (mathematics); Analysis and simulation; Arbitrary values; Constant degree; De Bruijn digraph; Distributed Hash Table; Kautz digraph; Kautz tree; Maintenance overhead; Moore bound; Multidimensional attributes; Network applications; Network diameter; Optimal diameters; P2P network; Proposed architectures; Range query; Resource placement; Ring structures; Structured P2P systems; Structured peer-to-peer; Topological properties; Total ordering; Tree structures; Peer to peer networks
One-time cookies: Preventing session hijacking attacks with stateless authentication tokens,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864876206&doi=10.1145%2f2220352.2220353&partnerID=40&md5=3cb4c48625da02b1b2947816a0888ef1,"HTTP cookies are the de facto mechanism for session authentication in Web applications. However, their inherent security weaknesses allow attacks against the integrity of Web sessions. HTTPS is often recommended to protect cookies, but deploying full HTTPS support can be challenging due to performance and financial concerns, especially for highly distributed applications. Moreover, cookies can be exposed in a variety of ways even when HTTPS is enabled. In this article, we propose one-time cookies (OTC), a more robust alternative for session authentication. OTC prevents attacks such as session hijacking by signing each user request with a session secret securely stored in the browser. Unlike other proposed solutions, OTC does not require expensive state synchronization in the Web application, making it easily deployable in highly distributed systems.We implemented OTC as a plug-in for the popularWordPress platform and as an extension for Firefox and Firefox for mobile browsers. Our extensive experimental analysis shows that OTC introduces a latency of less than 6 ms when compared to cookies-a negligible overhead for most Web applications. Moreover, we show that OTC can be combined with HTTPS to effectively add another layer of security to Web applications. In so doing, we demonstrate that one-time cookies can significantly improve the security of Web applications with minimal impact on performance and scalability. © 2012 ACM.",Highly-distributed Web applications; HTTP cookies; Session hijacking; Web session authentication,Authentication; Web browsers; World Wide Web; Authentication token; Distributed applications; Experimental analysis; Firefox; Mobile Browsers; Plug-ins; Security weakness; Session hijacking; State synchronization; WEB application; Web sessions; HTTP
Firewall policy change-impact analysis,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859409064&doi=10.1145%2f2109211.2109212&partnerID=40&md5=21e00dae92f28595d4823f9c4e20adff,"Firewalls are the cornerstones of the security infrastructure for most enterprises. They have been widely deployed for protecting private networks. The quality of the protection provided by a firewall directly depends on the quality of its policy (i.e., configuration). Due to the lack of tools for analyzing firewall policies, many firewalls used today have policy errors. A firewall policy error either creates security holes that will allow malicious traffic to sneak into a private network or blocks legitimate traffic and disrupts normal business processes, which in turn could lead to irreparable, if not tragic, consequences. A major cause of policy errors are policy changes. Firewall policies often need to be changed as networks evolve and new threats emerge. Users behind a firewall often request the firewall administrator to modify rules to allow or protect the operation of some services. In this article, we first present the theory and algorithms for firewall policy change-impact analysis. Our algorithms take as input a firewall policy and a proposed change, then output the accurate impact of the change. Thus, a firewall administrator can verify a proposed change before committing it. We implemented our firewall change-impact analysis algorithms, and tested them on both real-life and synthetic firewall policies. The experimental results show that our algorithms are effective in terms of ensuring firewall policy correctness and efficient in terms of computing the impact of policy changes. Thus, our tool can be practically used in the iterative process of firewall policy design and maintenance. Although the focus of this article is on firewalls, the change-impact analysis algorithms proposed in this article are not limited to firewalls. Rather, they can be applied to other rule-based systems, such as router access control lists (ACLs), as well. © 2012 ACM 1533-5399/2012/03-ART15 $10.00.",Algorithms; Management; Security,Access control; Algorithms; Computer networks; Computer viruses; Errors; Management; Access control lists; Analysis algorithms; Business Process; Firewall policies; Iterative process; Malicious traffic; Policy changes; Private networks; Security; Security holes; Security infrastructure; Computer system firewalls
A framework for personalizing web search with concept-based user profiles,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859413230&doi=10.1145%2f2109211.2109214&partnerID=40&md5=e0f704648920a4f7584880b13d00cdb1,"Personalized search is an important means to improve the performance of a search engine. In this article, we propose a framework that supports mining a user's conceptual preferences from users' clickthrough data resulting from Web search. The discovered preferences are utilized to adapt a search engine's ranking function. In this framework, an extended set of conceptual preferences was derived for a user based on the concepts extracted from the search results and the clickthrough data. Then, a concept-based user profile (CUP) representing the user profile as a concept ontology tree is generated. Finally, the CUP is input to a support vector machine (SVM) to learn a concept preference vector for adapting a personalized ranking function that reranks the search results. In order to achieve more flexible personalization, the framework allows a user to control the amount of specific CUP ontology information to be exposed to the personalized search engine. We study various parameters, such as conceptual relationships and concept features, arising from CUP that affect the ranking quality. Experiments confirm that our approach is able to significantly improve the retrieval effectiveness for the user. Further, our proposed control parameters of CUP information can adjust the exposed user information more smoothly and maintain better ranking quality than the existing methods. © 2012 ACM 1533-5399/2012/03-ART17 $10.00.",Experimentation; Performance,Experiments; Information retrieval; Search engines; Support vector machines; Clickthrough data; Concept-based; Control parameters; Experimentation; Performance; Personalizations; Personalized search; Ranking functions; Retrieval effectiveness; Search results; Support vector machine (SVM); User information; User profile; Web searches; Websites
Towards the taxonomy-oriented categorization of yellow pages queries,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859387680&doi=10.1145%2f2109211.2109213&partnerID=40&md5=86c1e920fd4b65e805ba540aab542f56,"Yellow pages search is a popular service that provides a means for finding businesses close to particular locations. The efficient search of yellow pages is becoming a rapidly evolving research area. The underlying data maintained in yellow pages search engines are typically labeled according to Standard Industry Classification (SIC) categories, and users can search yellow pages with categories according to their interests. Categorizing yellow pages queries into a subset of topical categories can help to improve search experience and quality. However, yellow pages queries are usually short and ambiguous. In addition, a yellow pages query taxonomy is typically organized by a hierarchy of a fairly large number of categories. These characteristics make automatic yellow pages query categorization difficult and challenging. In this article, we propose a flexible yellow pages query categorization approach. The proposed technique is built based on a TF-IDF similarity taxonomy matching scheme that is able to provide more accurate query categorization than previous keyword-based matching schemes. To further improve the categorization performance, we design several filtering schemes. Through extensive experimentation, we demonstrate encouraging results. We obtain F1 measures of about 0.5 and 0.3 for categorizing yellow pages queries into 19 coarse categories and 244 finer categories, respectively. We investigate different components in the proposed approach and also demonstrate the superiority of our approach over a hierarchical support vector machine classifier. © 2012 ACM 1533-5399/2012/03-ART16 $10.00.",Algorithms; Experimentation,Algorithms; Content based retrieval; Experiments; Search engines; Experimentation; Filtering schemes; Hierarchical support vector machines; Matching scheme; Research areas; Standard industry classification; Yellow pages; Taxonomies
Introduction to special issue on context-aware web services for the future internet,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857873486&doi=10.1145%2f2078316.2078317&partnerID=40&md5=5268f84da0872770c17723cd16789e9e,[No abstract available],,
Juno: A middleware platform for supporting delivery-centric applications,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877942345&doi=10.1145%2f2390209.2390210&partnerID=40&md5=69823e5909020df8f59fcf5eb2075a6e,"This article proposes a new delivery-centric abstraction which extends the existing content-centric networking API. A delivery-centric abstraction allows applications to generate content requests agnostic to location or protocol, with the additional ability to stipulate high-level requirements regarding such things as performance, security, and resource consumption. Fulfilling these requirements, however, is complex as often the ability of a provider to satisfy requirements will vary between different consumers and over time. Therefore, we argue that it is vital to manage this variance to ensure an application fulfils its needs. To this end, we present the Juno middleware, which implements delivery-centric support using a reconfigurable software architecture to: (i) discover multiple sources of an item of content; (ii) model each source's ability to provide the content; then (iii) adapt to interact with the source(s) that can best fulfil the application's requirements. Juno therefore utilizes existing providers in a backwards compatible way, supporting immediate deployment. This article evaluates Juno using Emulab to validate its ability to adapt to its environment. © 2012 ACM.",Content delivery; Content-centric; Middleware,Abstracting; Middleware; Reconfigurable architectures; Content centric; Content delivery; Content-centric networkings; Middleware platforms; Multiple source; Reconfigurable software; Resource consumption; Application programs
Context-sensitive user interfaces for semantic services,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863262489&doi=10.1145%2f2078316.2078322&partnerID=40&md5=011e05f88c448b192d85f4295df4bc70,"Service-centric solutions usually require rich context to fully deliver and better reflect on the underlying applications. We present a novel use of context in the form of customized user interface services with the concept of User Interface as a Service (UIaaS). UIaaS takes user profiles as input to generate contextaware interface services. Such interface services can be used as context to augment semantic services with contextual information leading to UIaaS as a Context (UIaaSaaC). The added serendipitous benefit of the proposed concept is that the composition of a customized user interface with the requested service is performed by the service composition engine, as is the case with any other services. We use a specialpurpose language (called User Interface Description Language (UIDL)) to model and realize user interfaces as services. We use a real-life e-government application, human services delivery for the citizens, as a proof-of-concept. We also present a comprehensive evaluation of the proposed approach using a functional evaluation and a nonfunctional evaluation consisting of an end user usability test and expert usability reviews. © 2012 ACM.",Context-aware; Semantic service; User interface,Semantics; Comprehensive evaluation; Context-Aware; Context-sensitive; Contextual information; E-Government applications; End users; Functional evaluation; Human services; Proof of concept; Semantic service; Service compositions; Usability tests; User interface description languages; User profile; User interfaces
Context-aware web search in ubiquitous sensor environments,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857846214&doi=10.1145%2f2078316.2078320&partnerID=40&md5=b44d58b26e76dffb8d4eec85779a40f2,"This article proposes a new concept for a context-aware Web search method that automatically retrieves a webpage related to the daily activity that a user currently is engaged in and displays the page on nearby Internet-connected home appliances such as televisions. For example, when a user is washing a coffeemaker, a webpage is retrieved that includes tips such as ""cleaning a coffee maker with vinegar removes stains well,"" and the page is displayed on a nearby appliance. In this article, we design and implement a Web search method that employs ubiquitous sensors to monitor a user's daily life. Our proposed method automatically searches for a webpage related to a daily activity by using a query constructed from the use of daily objects employed in the activity that is detected with object-attached sensors. We evaluate the search method with real datasets collected from vast numbers of sensors and achieve very accurate webpage retrieval. We then investigate the usefulness and effectiveness of a daily life Web search with Wizard-of-Oz (WOz)-like experiments. We confirm that the presentation of webpages related to daily activities improves participants' future daily lives and triggers communication among the participants in the experiment. © 2012 ACM.",Daily living; Experiment; Web search; Wireless sensor node,Acetic acid; Data processing; Domestic appliances; Electric network synthesis; Experiments; Information retrieval; Query processing; Sensors; Washing; Context-Aware; Daily activity; Daily lives; Daily living; Daily object; Real data sets; Search method; Ubiquitous sensor; Web searches; Web-page; Wireless sensor node; Wizard of Oz; Websites
Vulnerabilities and countermeasures in context-aware social rating services,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863231998&doi=10.1145%2f2078316.2078319&partnerID=40&md5=d71c31eb62e1206f50af31f98722d833,"Social trust and recommendation services are the most popular social rating systems today for service providers to learn about the social opinion or popularity of a product, item, or service, such as a book on Amazon, a seller on eBay, a story on Digg or amovie on Netflix. Such social rating systems are very convenient and offer alternative learning environments for decision makers, but they open the door for attackers to manipulate the social rating systems by selfishly promoting or maliciously demoting certain items. Although a fair amount of effort has been made to understand various risks and possible defense mechanisms to counter such attacks, most of the existing work to date has been devoted to studying specific types of attacks and their countermeasures. In this article, we argue that vulnerabilities in social rating systems and their countermeasures should be examined and analyzed in a systematic manner. We first give an overview of the common vulnerabilities and attacks observed in some popular social rating services. Next, we describe three types of attack strategies in two types of social rating systems, including a comprehensive theoretical analysis of their attack effectiveness and attack costs. Three context-aware countermeasures are then presented: (i) hiding user-item relationships, (ii) using confidence weight to distinguish popular and unpopular items, and (iii) incorporating time windows in trust establishment. We also provide an in-depth discussion on how these countermeasures can be used effectively to improve the robustness and trustworthiness of the social rating services. © 2012 ACM.",Rating system; Recommender system; Reputation system; Trust mechanism,Internet; Recommender systems; Attack strategies; Context-Aware; Decision makers; Defense mechanism; Learning environments; Rating system; Reputation systems; Service provider; Social ratings; Time windows; Trust establishment; Trust mechanism; Rating
"Context-aware services engineering: Models, transformations, and verification",2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857820806&doi=10.1145%2f2078316.2078318&partnerID=40&md5=7207fd63fda6d571b438632b054c9943,"Context-aware Web services are identified as an important technology to support new applications on the future Internet. Context information has several qualities that make the development of these services challenging, compared to conventional, Web services. Therefore, sound software engineering practices are needed during their development and execution. This article discusses a novel software engineering-based approach, which leverages the benefits of model-driven architecture, aspect-oriented modeling, and formal model checking, for modeling and verifying context-aware services. The approach is explored using a realworld case study in intelligent transport. An evaluation framework is established to validate the main methods and tools employed. © 2012 ACM.",Aspect-oriented modeling; Context-aware services; Model checking; Model-driven development; Software architecture; Software engineering,Information services; Model checking; Software architecture; Software engineering; Web services; Websites; Aspect oriented modeling; Context aware services; Context information; Context-Aware; Evaluation framework; Formal model; Future internet; Intelligent transport; Model driven architectures; Model driven development; New applications; Software engineering practices; Models
A smart web service based on the context of things,2012,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863264856&doi=10.1145%2f2078316.2078321&partnerID=40&md5=4bf9dc4b0d03330ba10056f827f62e8a,"Combining the Semantic Web and the Ubiquitous Web, Web 3.0 is for things. The Semantic Web enables human knowledge to be machine-readable and the Ubiquitous Web allows Web services to serve any thing, forming a bridge between the virtual world and the real world. By using context, Web services can become smarter-that is, aware of the target things' or applications' physical environments, or situations and respond proactively and intelligently. Existing methods for implementing context-aware Web services on Web 2.0 mainly enumerate different implementations corresponding to different attribute values of the context, in order to improve the Quality of Services (QoS). However, things in the physical world are extremely diverse, which poses new problems for Web services: it is difficult to unify the context of things and to implement a flexible smart Web service for things. This article proposes a novel smart Web service based on the context of things, which is implemented using a REpresentational State Transfer for Things (Thing-REST) style, to tackle the two problems. In a smart Web service, the user's description (semantic context) and sensor reports (sensing context) are two channels for acquiring the context of things which are then employed by ontology services to make the context of things machine-readable. With guidance of domain knowledge services, event detection services can analyze things' needs particularly, well through the context of things. We then propose a Thing-REST style to manage the context of things and user context, and to mashup Web services through three structures (i.e., chain, select, and merge) to implement smart Web services. A smart plant watering-service application demonstrates the effectiveness of our method. © 2012 ACM.",Context of things; Context-awareness; REpresentational State Transfer (REST); Smart Web services,Electronic document exchange; Semantic Web; Websites; Context of things; Context- awareness; Context-aware web services; Domain knowledge; Ontology services; Physical environments; Representational state transfer; Service applications; Web services
ACCENT: Cognitive cryptography plugged compression for ssl/tls-based cloud computing services,2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855228699&doi=10.1145%2f2049656.2049659&partnerID=40&md5=b7598acd74fe08950bc0bc7888d01064,"Emerging cloud services, including mobile offices, Web-based storage services, and content delivery services, run diverse workloads under various device platforms, networks, and cloud service providers. They have been realized on top of SSL/TLS, which is the de facto protocol for end-to-end secure communication over the Internet. In an attempt to achieve a cognitive SSL/TLS with heterogeneous environments (device, network, and cloud) and workload awareness, we thoroughly analyze SSL/TLS-based data communication and identify three critical mismatches in a conventional SSL/TLS-based data transmission. The first mismatch is the performance of loosely coupled encryption-compression and communication routines that lead to underutilized computation and communication resources. The second mismatch is that the conventional SSL/TLS only provides a static compression mode, irrespective of the dynamically changing status of each SSL/TLS connection and the computing power gap between the cloud service provider and diverse device platforms. The third is the memory allocation overhead due to frequent compression switching in the SSL/TLS. As a remedy to these rudimentary operations, we present a system called an Adaptive Cryptography Plugged Compression Network (ACCENT) for SSL/TLS-based cloud services. It is comprised of the following three novel mechanisms, each of which aims to provide an optimal SSL/TLS communication and maximize the network transfer performance of an SSL/TLS protocol stack: tightly-coupled threaded SSL/TLS coding, floating scale-based adaptive compression negotiation, and unified memory allocation for seamless compression switching. We implemented and tested the mechanisms in OpenSSL-1.0.0. ACCENT is integrated into the Web-interface layer and SSL/TLS-based secure storage service within a real cloud computing service, called iCubeCloud, as the key primitive for SSL/TLS-based data delivery over the Internet © 2011 ACM.",Communication system security; Compression; Cryptography; SSL/TLS,Communication systems; Compaction; Cryptography; Distributed database systems; Internet; Internet protocols; Memory architecture; Web services; Adaptive compression; Cloud services; Communication resources; Communication system security; Computing power; Computing services; Content delivery services; Data delivery; Data-communication; Diverse devices; Heterogeneous environments; Mobile offices; Network transfers; Secure communications; Secure storage; SSL/TLS; SSL/TLS protocol; Static compression; Storage services; Tightly-coupled; Cloud computing
User-Perceived quality assessment of streaming media using reduced feature sets,2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855223316&doi=10.1145%2f2049656.2049660&partnerID=40&md5=68bb8d12df928b417beb88f3491e92bc,"While subjective measurements are the most natural for assessing the user-perceived quality of a media stream, there are issues with their scalability and their context accuracy. We explore techniques to select application-layer measurements, collected by an instrumented media player, that most accurately predict the subjective quality rating that a user would assign to a stream. We consider three feature subset selection techniques that reduce the number of features (measurements) under consideration to ones most relevant to user-perceived stream quality. Two of the three techniques mathematically consider stream characteristics when selecting measurements, while the third is based on observation. We apply the reduced feature sets to two nearest-neighbor algorithms for predicting user-perceived stream quality. Our results demonstrate that there are clear strategies for estimating the quality rating that work well in specific circumstances such as video-on-demand services. The results also demonstrate that neither of the mathematically-based feature subset selection techniques identify a single set of features that is unambiguously influential on userperceived stream quality, but that ultimately a combination of retransmitted and/or lost application-layer packets is most accurate for predicting stream quality. © 2011 ACM.",Multimedia applications; Multimedia measurement; Quality of Experience (QoE); Quality of Service (QoS); Streaming media; Subjective quality; Video quality assessment (VQA),Feature extraction; Forecasting; Quality of service; Rating; Multimedia applications; Quality of Experience (QoE); Quality of Service (QoS); Streaming media; Subjective quality; Video quality assessment; Media streaming
"Distributed application configuration, management, and visualization with plush",2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855245439&doi=10.1145%2f2049656.2049658&partnerID=40&md5=848e52b58c457e5ec195dba48ed27069,"Support for distributed application management in large-scale networked environments remains in its early stages. Although a number of solutions exist for subtasks of application deployment, monitoring, and maintenance in distributed environments, few tools provide a unified framework for application management. Many of the existing tools address the management needs of a single type of application or service that runs in a specific environment, and these tools are not adaptable enough to be used for other applications or platforms. To this end, we present the design and implementation of Plush, a fully configurable application management infrastructure designed to meet the general requirements of several different classes of distributed applications. Plush allows developers to specifically define the flow of control needed by their computations using application building blocks. Through an extensible resource management interface, Plush supports execution in a variety of environments, including both live deployment platforms and emulated clusters. Plush also uses relaxed synchronization primitives for improving fault tolerance and liveness in failure-prone environments. To gain an understanding of how Plush manages different classes of distributed applications, we take a closer look at specific applications and evaluate how Plush provides support for each. © 2011 ACM.",Application management; PlanetLab,Fault tolerance; Management; Visualization; Application deployment; Application management; Building blockes; Configurable; Distributed applications; Distributed environments; Flow of control; Liveness; Networked environments; Other applications; PlanetLab; Resource management; Subtasks; Synchronization primitive; Unified framework; Environmental management
Comparing ingress and egress detection to secure interdomain routing: An experimental analysis,2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855252117&doi=10.1145%2f2049656.2049657&partnerID=40&md5=273f557077e16bbcc09a99e9786803a5,"The global economy and society increasingly depends on computer networks linked together by the Internet. The importance of computer networks reaches far beyond the telecommunications sector since they have become a critical factor for many other crucial infrastructures and markets. With threats mounting and security incidents becoming more frequent, concerns about network security grow. It is an acknowledged fact that some of the most fundamental network protocols that make the Internet work are exposed to serious threats.One of them is the BorderGateway Protocol (BGP)which determines how Internet traffic is routed through the topology of administratively independent networks that the Internet is comprised of. Despite the existence of a steadily growing number of BGP security proposals, to date none of them has been adopted. Using a precise definition of BGP robustness we experimentally show that the degree of robustness is distributed unequally across the administrative domains of the Internet, the so-called Autonomous Systems (ASes). The experiments confirm the intuition that the contribution ASes are able to make towards securing the correct working of the inter-domain routing infrastructure by deploying countermeasures against routing attacks differ depending on their position in the AS topology. We also show that the degree of this asymmetry can be controlled by the choice of the security strategy. We compare the strengths and weaknesses of two fundamentally different approaches in increasing BGP's robustness which we termed ingress and egress detection of false route advertisements and indicate their implications. Our quantitative results have important implications for Internet security policy, in particular with respect to the crucial question where to start the deployment of which type of security scheme in order to maximize the Internet's robustness to routing attacks. © 2011 ACM.",Adoption; BGP security; Inter-domain routing; Policy,Critical infrastructures; Dense wavelength division multiplexing; Internet; Internet protocols; Network protocols; Public policy; Security systems; Telecommunication networks; Topology; Adoption; Autonomous systems; BGP security; Critical factors; Degree of robustness; Experimental analysis; Global economies; Inter-domain routing; Internet security policy; Internet traffic; Precise definition; Quantitative result; Routing attacks; Security incident; Security scheme; Security strategies; Network security
Impact of XML Schema evolution,2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051945758&doi=10.1145%2f1993083.1993087&partnerID=40&md5=3ec89ff63edff526b2ec6151dd5dd6c8,"We consider the problem of XML Schema evolution. In the ever-changing context of the web, XML schemas continuously change in order to cope with the natural evolution of the entities they describe. Schema changes have important consequences. First, existing documents valid with respect to the original schema are no longer guaranteed to fulfill the constraints described by the evolved schema. Second, the evolution also impacts programs, manipulating documents whose structure is described by the original schema. We propose a unifying framework for determining the effects of XML Schema evolution both on the validity of documents and on queries. The system is very powerful in analyzing various scenarios in which forward/backward compatibility of schemas is broken, and in which the result of a query may no longer be what was expected. Specifically, the system offers a predicate language that allows one to formulate properties related to schema evolution. The system then relies on exact reasoning techniques to perform a fine-grained analysis. This yields either a formal proof of the property or a counter-example that can be used for debugging purposes. The system has been fully implemented and tested with real-world use cases, in particular with the main standard document formats used on the web, as defined by W3C. The system precisely identifies compatibility relations between document formats. In case these relations do not hold, the system can identify queries that must be reformulated in order to produce the expected results across successive schema versions. © 2011 ACM .",Queries; Schema evolution; Schemas; Web document formats; XML,Printing machinery; User interfaces; Compatibility relation; Document formats; Fine-grained analysis; Formal proofs; Natural evolution; Queries; Reasoning techniques; Schema changes; Schema evolution; Schemas; Standard documents; Web document; XML schemas; XML
Exploiting service usage information for optimizing server resource management,2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051929389&doi=10.1145%2f1993083.1993084&partnerID=40&md5=19bd6c27926d67eaf4376af38b79438c,"It is often difficult to tune the performance of modern component-based Internet services because: (1) component middleware are complex software systems that expose several independently tuned server resource management mechanisms; (2) session-oriented client behavior with complex data access patterns makes it hard to predict what impact tuning these mechanisms has on application behavior; and (3) component-based Internet services themselves exhibit complex structural organization with requests of different types having widely ranging execution complexity. In this article we show that exposing and using detailed information about how clients use Internet services enables mechanisms that achieve two interconnected goals: (1) providing improved QoS to the service clients, and (2) optimizing server resource utilization. To differentiate among levels of service usage (service access) information, we introduce the notion of the service access attribute and identify four related groups of service access attributes, encompassing different aspects of service usage information, ranging from the high-level structure of client web sessions to low-level fine-grained information about utilization of server resources by different requests. To show how the identified service usage information can be collected, we implement a request profiling infrastructure in the JBoss Java application server. In the context of four representative service management problems, we show how collected service usage information is used to improve service performance, optimize server resource utilization, or to achieve other problem-specific service management goals. © 2011 ACM.",Client behavior; Component middleware; Internet application; Optimization; Quality-of-service; Server resource management; Service usage information,Information use; Internet; Java programming language; Middleware; Natural resources management; Optimization; Quality of service; Resource allocation; Telecommunication networks; User interfaces; Web services; Client behavior; Component middleware; Internet application; Server resource management; Service usage; Information management
Characterizing intelligence gathering and control on an edge network,2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051916430&doi=10.1145%2f1993083.1993085&partnerID=40&md5=70b5b5fbd9ec900654ef6a01037e2d05,"There is a continuous struggle for control of resources at every organization that is connected to the Internet. The local organization wishes to use its resources to achieve strategic goals. Some external entities seek direct control of these resources, for purposes such as spamming or launching denial-of-service attacks. Other external entities seek indirect control of assets (e.g., users, finances), but provide services in exchange for them. Using a year-long trace from an edge network, we examine what various external organizations know about one organization. We compare the types of information exposed by or to external organizations using either active (reconnaissance) or passive (surveillance) techniques. We also explore the direct and indirect control external entities have on local IT resources. © 2011 ACM .",Workload Characterization,Denial of service attacks; Direct control; EDGE Networks; Indirect control; Intelligence gathering; IT resources; Strategic goals; Workload characterization
Achieving data consistency by contextualization in web-based collaborative applications,2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953240781&doi=10.1145%2f1944339.1944340&partnerID=40&md5=e0fb1b1ad48d9482e8ed8f48a04b5ef6,"Recent years have witnessed the emergence and rapid development of collaborative Web-based applications exemplified by Web-based office productivity applications. One major challenge in building these applications is maintaining data consistency while meeting the requirements of fast local response, total work preservation, unconstrained interaction, and customizable collaboration mode. These requirements are important in determining users' experiences in interaction and collaboration, and in meeting users' diverse needs under complex and dynamic collaboration and networking environments; but none of existing solutions is able to meet all of them. In this article, we present a data consistency maintenance solution capable of meeting these requirements for collaborative Web-based applications. Major technical contributions include an efficient sequence-based operation transformation control algorithm based on the concept of contextualization, an operation broadcast protocol for supporting a variety of collaboration modes, an operation replaying algorithm for ensuring fast local response and efficient remote operation replay, and a set of communication protocols for managing the integrity of collaborative Web-based sessions. The proposed solution has been implemented in a prototype collaborative Web-based editor WRACE and the correctness of the solution is formally verified in the article. © 2011 ACM.",Asymmetric collaboration; Collaborative Web-based applications; Contextualization; Data consistency; Operational transformation,Algorithms; Communication; Historic preservation; Metadata; Web services; Asymmetric collaboration; Collaborative Web-based applications; Contextualization; Data consistency; Operational transformation; Internet protocols
Properties and evolution of internet traffic networks from anonymized flow data,2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953246500&doi=10.1145%2f1944339.1944342&partnerID=40&md5=1e1b8146f2b00c7210412d36c5e508b8,"Many projects have tried to analyze the structure and dynamics of application overlay networks on the Internet using packet analysis and network flow data. While such analysis is essential for a variety of network management and security tasks, it is infeasible on many networks: either the volume of data is so large as to make packet inspection intractable, or privacy concerns forbid packet capture and require the dissociation of network flows from users' actual IP addresses. Our analytical framework permits useful analysis of network usage patterns even under circumstances where the only available source of data is anonymized flow records. Using this data, we are able to uncover distributions and scaling relations in host-to-host networks that bear implications for capacity planning and network application design. We also show how to classify network applications based entirely on topological properties of their overlay networks, yielding a taxonomy that allows us to accurately identify the functions of unknown applications. We repeat this analysis on a more recent dataset, allowing us to demonstrate that the aggregate behavior of users is remarkably stable even as the population changes. © 2011 ACM.",Application identification; Application networks; Behavioral networks; Evolution of networks; Functional networks; Internet usage; Latitudinal analysis; Network flows; Power-law networks; Traffic statistics,Computer aided network analysis; Data flow analysis; Information management; Internet; Internet protocols; Network security; Overlay networks; Topology; Traffic surveys; Application identification; Application networks; Behavioral networks; Evolution of networks; Functional network; Internet usage; Latitudinal analysis; Network flows; Power-law networks; Traffic statistics; Network management
Novelty and Diversity in top-N recommendation-Analysis and evaluation,2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953246287&doi=10.1145%2f1944339.1944341&partnerID=40&md5=1bc8a8bc5f96a02377173147e27e8fe2,"For recommender systems that base their product rankings primarily on a measure of similarity between items and the user query, it can often happen that products on the recommendation list are highly similar to each other and lack diversity. In this article we argue that the motivation of diversity research is to increase the probability of retrieving unusual or novel items which are relevant to the user and introduce a methodology to evaluate their performance in terms of novel item retrieval. Moreover, noting that the retrieval of a set of items matching a user query is a common problem across many applications of information retrieval, we formulate the trade-off between diversity and matching quality as a binary optimization problem, with an input control parameter allowing explicit tuning of this trade-off. We study solution strategies to the optimization problem and demonstrate the importance of the control parameter in obtaining desired system performance. The methods are evaluated for collaborative recommendation using two datasets and case-based recommendation using a synthetic dataset constructed from the public-domain Travel dataset. © 2011 ACM.",Casebased recommendation; Collaborative filtering; Diversity; Novelty; Recommender systems,Optimization; Analysis and evaluation; Binary optimization; Casebased recommendation; Collaborative filtering; Collaborative recommendation; Common problems; Control parameters; Data sets; Diversity; Novelty; Optimization problems; Public domains; Solution strategy; User query; Recommender systems
Load balancing and range queries in P2P systems using P-ring,2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953246712&doi=10.1145%2f1944339.1944343&partnerID=40&md5=1e890b62ad81c77d5405afc60a72b787,"In peer-to-peer (P2P) systems, computers from around the globe share data and can participate in distributed computation. P2P became famous, and infamous, due to file-sharing systems like Napster. However, the scalability and robustness of these systems make them appealing to a wide range of applications. This article introduces P-Ring, a new peer-to-peer index structure. P-Ring is fully distributed, fault tolerant, and provides load balancing and logarithmic search performance while supporting both equality and range queries. Our theoretical analysis as well as experimental results, obtained both in a simulated environment and on PlanetLab, show the performance of our system. © 2011 ACM.",Indexing; Load balancing; Peer-to-peer systems; Range queries,Indexing (of information); Parallel architectures; Peer to peer networks; Distributed computations; Fault-tolerant; File-sharing system; Index structure; Indexing; Load balancing; Logarithmic Search; Napster; P2P system; Peer to peer; Peer-to-Peer system; PlanetLab; Range query; Simulated environment; Distributed computer systems
Anomaly detection in dynamic systems using weak estimators,2011,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051919849&doi=10.1145%2f1993083.1993086&partnerID=40&md5=2c4bb8c3f9672bd0bfc39d9301cf7bdf,"Anomaly detection involves identifying observations that deviate from the normal behavior of a system. One of the ways to achieve this is by identifying the phenomena that characterize ""normal"" observations. Subsequently, based on the characteristics of data learned from the ""normal"" observations, new observations are classified as being either ""normal"" or not. Most state-of-the-art approaches, especially those which belong to the family of parameterized statistical schemes, work under the assumption that the underlying distributions of the observations are stationary. That is, they assume that the distributions that are learned during the training (or learning) phase, though unknown, are not time-varying. They further assume that the same distributions are relevant even as new observations are encountered. Although such a ""stationarity"" assumption is relevant for many applications, there are some anomaly detection problems where stationarity cannot be assumed. For example, in network monitoring, the patterns which are learned to represent normal behavior may change over time due to several factors such as network infrastructure expansion, new services, growth of user population, and so on. Similarly, in meteorology, identifying anomalous temperature patterns involves taking into account seasonal changes of normal observations. Detecting anomalies or outliers under these circumstances introduces several challenges. Indeed, the ability to adapt to changes in nonstationary environments is necessary so that anomalous observations can be identified even with changes in what would otherwise be classified as ""normal"" behavior. In this article we propose to apply a family of weak estimators for anomaly detection in dynamic environments. In particular, we apply this theory to spam email detection. Our experimental results demonstrate that our proposal is both feasible and effective for the detection of such anomalous emails. © 2011 ACM .",Anomaly detection; Dynamic systems; Weak estimator,Dynamical systems; Electronic mail; Anomaly detection; Dynamic environments; Network infrastructure; Non-stationary environment; State-of-the-art approach; Temperature patterns; Underlying distribution; Weak estimators; Population statistics
Forecasting click-through rates based on sponsored search advertiser bids and intermediate variable regression,2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049335091&doi=10.1145%2f1852096.1852099&partnerID=40&md5=afcfaa1ec96ff19fbb61293cc8aea9f3,"To participate in sponsored search online advertising, an advertiser bids on a set of keywords relevant to his/her product or service. When one of these keywords matches a user search string, the ad is then considered for display among sponsored search results. Advertisers compete for positions in which their ads appear, as higher slots typically result in more user clicks. All existing position allocating mechanisms charge more per click for a higher slot. Therefore, an advertiser must decide whether to bid high and receive more, but more expensive, clicks. In this work, we propose a novel methodology for building forecasting landscapes relating an individual advertiser bid to the expected click-through rate and/or the expected daily click volume. Displaying such landscapes is currently offered as a service to advertisers by all major search engine providers. Such landscapes are expected to be instrumental in helping the advertisers devise their bidding strategies. We propose a triply monotone regression methodology. We start by applying the current stateofthe-art monotone regression solution. We then propose to condition on the ad position and to estimate the bid-position and position-click effects separately. While the latter translates into a standard monotone regression problem, we devise a novel solution to the former based on approximate maximum likelihood. We show that our proposal significantly outperforms the standard monotone regression solution, while the latter similarly improves upon routinely used ad-hoc methods. Last, we discuss other e-commerce applications of the proposed intermediate variable regression methodology. © 2010 ACM.",Bidding agents; Clickthrough rate estimation; Electronic commerce; Isotonic regression; Nonparametric regression; Online auctions; Shape constraints; Sponsored search; Statistical inference,Electronic commerce; Intelligent agents; Maximum likelihood estimation; Mobile telecommunication systems; Search engines; Bidding agents; Click-through rate; Isotonic regression; Non-parametric regression; Online auctions; Shape constraints; Sponsored searches; Statistical inference; Regression analysis
A framework for classification of traffic management practices as reasonable or unreasonable,2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049340354&doi=10.1145%2f1852096.1852100&partnerID=40&md5=dec8f6ba219a34471454a7f905e60197,"Traffic management practices of ISPs are an issue of public concern. We propose a framework for classification of traffic management practices as reasonable or unreasonable. We present a survey of traffic management techniques and examples of how these techniques are used by ISPs. We suggest that whether a traffic management practice is reasonable rests on the answers to four questions regarding the techniques and practices used. We propose a framework that classifies techniques as unreasonable if they are unreasonably anticompetitive, cause undue harm to consumers, or unreasonably impair free speech. We propose alternatives to unreasonable or borderline congestion management practices. © 2010 ACM.",,Congestion management; Free speech; Public concern; Traffic management; Management
Peeking through the cloud: Client density estimation via DNS cache probing,2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049321178&doi=10.1145%2f1852096.1852097&partnerID=40&md5=4304d478aa7d5e7f334efe0a455e8b4b,"Reliable network demographics are quickly becoming a much sought-after digital commodity. However, as the need for more refined Internet demographics has grown, so too has the tension between privacy and utility. Unfortunately, current techniques lean too much in favor of functional requirements over protecting the privacy of users. For example, the most prominent proposals for measuring the relative popularity of a Web site depend on the deployment of client-side measurement agents that are generally perceived as infringing on users' privacy, thereby limiting their wide-scale adoption. Moreover, the client-side nature of these techniques also makes them susceptible to various manipulation tactics that undermine the integrity of their results. In this article, we propose a new estimation technique that uses DNS cache probing to infer the density of clients accessing a given service. Compared to earlier techniques, our scheme is less invasive as it does not reveal user-specific traits, and is more robust against manipulation. We demonstrate the flexibility of our approach through two important security applications. First, we illustrate how our scheme can be used as a lightweight technique for measuring and verifying the relative popularity rank of different Web sites. Second, using data from several hundred botnets, we apply our technique to indirectly measure the infected population of this increasing Internet phenomenon. © 2010 ACM.",Botnets; Client density estimation; Measurement; Network security; Web metering,Estimation; Internet; Internet protocols; Population statistics; Botnets; Current techniques; Density estimation; Estimation techniques; Functional requirement; Security application; Web metering; Network security
Using probabilistic confidence models for trust inference in web-based social networks,2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953576532&doi=10.1145%2f1754393.1754397&partnerID=40&md5=c4f49e0d877414556c51ab48f0db810f,"In this article, we describe a new approach that gives an explicit probabilistic interpretation for social networks. In particular, we focus on the observation that many existing Web-based trust-inference algorithms conflate the notions of ""trust"" and ""confidence,"" and treat the amalgamation of the two concepts to compute the trust value associated with a social relationship. Unfortunately, the result of such an algorithm that merges trust and confidence is not a trust value, but rather a new variable in the inference process. Thus, it is hard to evaluate the outputs of such an algorithm in the context of trust inference. This article first describes a formal probabilistic network model for social networks that allows us to address that issue. Then we describe SUNNY, a new trust inference algorithm that uses probabilistic sampling to separately estimate trust information and our confidence in the trust estimate and use the two values in order to compute an estimate of trust based on only those information sources with the highest confidence estimates. We present an experimental evaluation of SUNNY. In our experiments, SUNNY produced more accurate trust estimates than the well-known trust inference algorithm TIDALTRUST, demonstrating its effectiveness. Finally, we discuss the implications these results will have on systems designed for personalizing content and making recommendations. © 2010 ACM.",Bayesian networks; Social networks; Trust,Algorithms; Distributed parameter networks; Inference engines; Intelligent networks; Metals; Social networking (online); Confidence model; Experimental evaluation; Inference algorithm; Inference process; Information sources; New approaches; Probabilistic interpretation; Probabilistic network; Probabilistic sampling; Social Networks; Social relationships; Trust values; Bayesian networks
BogusBiter: A transparent protection against phishing attacks,2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953587426&doi=10.1145%2f1754393.1754395&partnerID=40&md5=df914a28f733468ad766f5a10c375852,"Many anti-phishing mechanisms currently focus on helping users verify whether a Web site is genuine. However, usability studies have demonstrated that prevention-based approaches alone fail to effectively suppress phishing attacks and protect Internet users from revealing their credentials to phishing sites. In this paper, instead of preventing human users from ""biting the bait,"" we propose a new approach to protect against phishing attacks with ""bogus bites."" We develop BogusBiter, a unique client-side anti-phishing tool, which transparently feeds a relatively large number of bogus credentials into a suspected phishing site. BogusBiter conceals a victim's real credential among bogus credentials, and moreover, it enables a legitimate Web site to identify stolen credentials in a timely manner. Leveraging the power of client-side automatic phishing detection techniques, BogusBiter is complementary to existing preventive anti-phishing approaches. We implemented BogusBiter as an extension to the Firefox 2 Web browser, and evaluated its efficacy through real experiments on both phishing and legitimate Web sites. Our experimental results indicate that it is promising to use BogusBiter to transparently protect against phishing attacks. © 2010 ACM.",Credential theft; Phishing; Security; Usability; Web spoofing,Web browsers; World Wide Web; Anti-phishing; Detection technique; Firefox; Human users; Internet users; New approaches; Phishing; Phishing attacks; Transparent protection; Usability studies; Web spoofing; Security of data
Detecting visually similar web pages: Application to phishing detection,2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953636660&doi=10.1145%2f1754393.1754394&partnerID=40&md5=de39ad98a9a19ad64a6e04be607bb776,"We propose a novel approach for detecting visual similarity between two Web pages. The proposed approach applies Gestalt theory and considers a Web page as a single indivisible entity. The concept of supersignals, as a realization of Gestalt principles, supports our contention that Web pages must be treated as indivisible entities. We objectify, and directly compare, these indivisible supersignals using algorithmic complexity theory. We illustrate our approach by applying it to the problem of detecting phishing scams. Via a large-scale, real-world case study, we demonstrate that 1) our approach effectively detects similar Web pages; and 2) it accuractely distinguishes legitimate and phishing pages. © 2010 ACM.",Algorithmic complexity theory; Antiphishing technologies; Gestalt theory; Web page similarity,Algorithms; Computational complexity; Parallel processing systems; Algorithmic complexity theory; Anti-phishing; Antiphishing technologies; Gestalt theory; Web page; World Wide Web
Teaching johnny not to fall for phish,2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953569639&doi=10.1145%2f1754393.1754396&partnerID=40&md5=3097e0a2e2ea92c7903b8dfc07c7c2c2,"Phishing attacks, in which criminals lure Internet users to Web sites that spoof legitimate Web sites, are occurring with increasing frequency and are causing considerable harm to victims. While a great deal of effort has been devoted to solving the phishing problem by prevention and detection of phishing emails and phishing Web sites, little research has been done in the area of training users to recognize those attacks. Our research focuses on educating users about phishing and helping them make better trust decisions. We identified a number of challenges for end-user security education in general and anti-phishing education in particular: users are not motivated to learn about security; for most users, security is a secondary task; it is difficult to teach people to identify security threats without also increasing their tendency to misjudge nonthreats as threats. Keeping these challenges in mind, we developed an email-based anti-phishing education system called ""PhishGuru"" and an online game called ""Anti-Phishing Phil"" that teaches users how to use cues in URLs to avoid falling for phishing attacks. We applied learning science instructional principles in the design of PhishGuru and Anti-Phishing Phil. In this article we present the results of PhishGuru and Anti-Phishing Phil user studies that demonstrate the effectiveness of these tools. Our results suggest that, while automated detection systems should be used as the first line of defense against phishing attacks, user education offers a complementary approach to help people better recognize fraudulent emails and websites. © 2010 ACM.",Email; Embedded training; Instructional principles; Learning science; Phishing; Situated learning; Usable privacy and security,Education; Electronic mail; World Wide Web; Instructional principles; Learning science; Phishing; Situated learning; Usable privacy; Security of data
"A programmable network address translator: Design, implementation, and performance",2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849092941&doi=10.1145%2f1667067.1667070&partnerID=40&md5=801c71851306bbbc40a453bdb93a01d6,"Network Address Translation (NAT) alleviates the shortage of IPv4 addresses but incurs peer-to-peer communication, application functionality and packet integrity problems. To date, no approach has yet been proposed to solve these three problems. By exploiting mobile agent and active networking technologies, we propose a Programmable Network Address Translation (PNAT) implementation that enables peer-to-peer communication while maintaining application functionality and packet integrity. For peer-to-peer communication, our proposed PNAT approach works for various NAT types (including the Symmetric NAT) with simple APIs supported by our proposed NAT design. For application functionality, the PNAT uses the mobile code to update protocol information in packet payloads according to different application needs. For packet integrity, the PNAT allows applications to delay their data encryption until NAT begins to translate addresses and ports in packet headers. To validate our proposed PNAT approach, we implemented the PNAT design on Windows 2000, and we present an empirical performance evaluation of the implemented design. © 2010 ACM.",Application functionality; NAT; Packet integrity; Peer-to-peer communication,Communication; Cryptography; Design; Distributed computer systems; Mobile agents; Programmed control systems; Quality assurance; Query languages; Wireless networks; Active networking; Data encryption; Empirical performance; Mobile codes; NAT; Network address translations; Packet header; Packet payloads; Peer-to-peer communications; Programmable network; Three problems; Update protocol; Windows 2000; Peer to peer networks
Semantic integrity in large-scale online simulations,2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849096100&doi=10.1145%2f1667067.1667069&partnerID=40&md5=382b918fa9c01d092ddb0c5b10742178,"As large-scale online simulations such as Second Life and World of Warcraft are gaining economic significance, there is a growing incentive for attacks against such simulation software. We focus on attacks against the semantic integrity of the simulation. This class of attacks exploits the client-server architecture and is specific to online simulations which, for performance reasons, have to delegate the detailed rendering of the simulated world to the clients. Attacks against semantic integrity often compromise the physical laws of the simulated world-enabling the user's simulation persona to fly, walk through walls, or to run faster than anybody else. We introduce the Secure Semantic Integrity Protocol (SSIP), which enables the simulation provider to audit the client computations. Then we analyze the security and scalability of SSIP. First, we show that under standard cryptographic assumptions SSIP will detect semantic integrity attacks. Second, we analyze the network overhead, and determine the optimum tradeoff between cost of bandwidth and audit frequency for our protocol. © 2010 ACM.",Cryptographic protocols; Networked virtual environments; Semantic integrity,Client server computer systems; Computer software; Cryptography; Network security; Quality assurance; Semantics; Virtual reality; Client-server architectures; Cryptographic assumptions; Cryptographic protocols; Network overhead; Networked virtual environments; Online simulation; Physical laws; Second Life; Semantic integrity; Simulation software; Through walls; Network protocols
A market-based bandwidth charging framework,2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849111359&doi=10.1145%2f1667067.1667068&partnerID=40&md5=cf68d8e15793aec856a7dd19ad87e131,"The increasing demand for high-bandwidth applications such as video-on-demand and grid computing is reviving interest in bandwidth reservation schemes. Earlier attempts did not catch on for a number of reasons, notably lack of interest on the part of the bandwidth providers. This, in turn, was partially caused by the lack of an efficient way of charging for bandwidth. Thus, the viability of bandwidth reservation depends on the existence of an efficient market where bandwidth-related transactions can take place. For this market to be effective, it must be efficient for both the provider (seller) and the user (buyer) of the bandwidth. This implies that: (a) the buyer must have a wide choice of providers that operate in a competitive environment, (b) the seller must be assured that a QoS transaction will be paid by the customer, and (c) the QoS transaction establishment must have low overheads so that it may be used by individual customers without a significant burden to the provider. In order to satisfy these requirements, we propose a framework that allows customers to purchase bandwidth using an open market where providers advertise links and capacities and customers bid for these services. The model is close to that of a commodities market that offers both advance bookings (futures) and a spot market. We explore the mechanisms that can support such a model. © 2010 ACM.",Bandwidth reservations; Credentials; Network management; Trust management,Commerce; Customer satisfaction; Grid computing; Sales; A-spots; Bandwidth reservation; Bandwidth reservation scheme; Bandwidth reservations; Bandwidth-related transaction; Commodities market; Competitive environment; High-bandwidth application; Individual customers; Low overhead; Open market; Trust management; Bandwidth
A framework for large-scale detection of Web site defacements,2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049325551&doi=10.1145%2f1852096.1852098&partnerID=40&md5=07598ca513ee24792fd4688fe3f0d686,"Web site defacement, the process of introducing unauthorized modifications to a Web site, is a very common form of attack. In this paper we describe and evaluate experimentally a framework that may constitute the basis for a defacement detection service capable of monitoring thousands of remote Web sites systematically and automatically. In our framework an organization may join the service by simply providing the URLs of the resources to be monitored along with the contact point of an administrator. The monitored organization may thus take advantage of the service with just a few mouse clicks, without installing any software locally or changing its own daily operational processes. Our approach is based on anomaly detection and allows monitoring the integrity of many remote Web resources automatically while remaining fully decoupled from them, in particular, without requiring any prior knowledge about those resources. We evaluated our approach over a selection of dynamic resources and a set of publicly available defacements. The results are very satisfactory: all attacks are detected while keeping false positives to a minimum. We also assessed performance and scalability of our proposal and we found that it may indeed constitute the basis for actually deploying the proposed service on a large scale. © 2010 ACM.",Experimental evaluation; Intrusion detection; Monitoring service; Web site defacement,Web crawler; Websites; Anomaly detection; Dynamic resources; Experimental evaluation; Monitoring services; Operational process; Performance and scalabilities; Unauthorized modification; Web site defacements; Intrusion detection
Privacy-preserving similarity-based text retrieval,2010,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849085704&doi=10.1145%2f1667067.1667071&partnerID=40&md5=273388569216bfa5439902406122ce57,"Users of online services are increasingly wary that their activities could disclose confidential information on their business or personal activities. It would be desirable for an online document service to perform text retrieval for users, while protecting the privacy of their activities. In this article, we introduce a privacy-preserving, similarity-based text retrieval scheme that (a) prevents the server from accurately reconstructing the term composition of queries and documents, and (b) anonymizes the search results from unauthorized observers. At the same time, our scheme preserves the relevance-ranking of the search server, and enables accounting of the number of documents that each user opens. The effectiveness of the scheme is verified empirically with two real text corpora. © 2010 ACM.",Privacy of search queries; Security in text retrieval; Singular value decomposition,Network security; Singular value decomposition; Confidential information; On-line documents; On-line service; Privacy preserving; Search queries; Search results; Security in text retrieval; Text corpora; Text retrieval; Information retrieval
Fast XML document filtering by sequencing twig patterns,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350348206&doi=10.1145%2f1592446.1592447&partnerID=40&md5=7c99cf1373b190105e7823cb23ef9469,"XML-enabled publish-subscribe (pub-sub) systems have emerged as an increasingly important tool for e-commerce and Internet applications. In a typical pub-sub system, subscribed users specify their interests in a profile expressed in the XPath language. Each new data content is then matched against the user profiles so that the content is delivered only to the interested subscribers. As the number of subscribed users and their profiles can grow very large, the scalability of the service is critical to the success of pub-sub systems. In this article, we propose a novel scalable filtering system called iFiST that transforms user profiles of a twig pattern expressed in XPath into sequences using the Prüfer's method. Consequently, instead of breaking a twig pattern into multiple linear paths and matching them separately, FiST performs holistic matching of twig patterns with each incoming document in a bottom-up fashion. FiST organizes the sequences into a dynamic hash-based index for efficient filtering, and exploits the commonality among user profiles to enable shared processing during the filtering phase. We demonstrate that the holistic matching approach reduces filtering cost and memory consumption, thereby improving the scalability of FiST. © 2009 ACM.",Prüfer sequences; Selective dissemination of information; Twig pattern; XML filtering,Electronic commerce; Information dissemination; Information services; Internet; Markup languages; Scalability; XML; Bottom-up fashion; Data contents; Document filtering; E-Commerce; Filtering systems; Internet application; Memory consumption; Publish-subscribe; Selective dissemination of information; Sub-systems; Twig pattern; User profile; XML filtering; Information retrieval systems
Detection of corrupted schema mappings in XML data integration systems,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350337740&doi=10.1145%2f1592446.1592448&partnerID=40&md5=8a5c021c8ad41e0e5bde0abd8601de33,"In modern data integration scenarios, many remote data sources are located on the Web and are accessible only through forms or Web services, and no guarantee is given about their stability. In these contexts the detection of corrupted mappings, as a consequence of a change in the source or in the target schema, is a key problem. A corrupted mapping fails in matching the target or the source schema, hence it is not able to transform data conforming to a schema S into data conforming to a schema T, nor it can be used for effective query reformulation. This article describes a novel technique for maintaining schema mappings in XML data integration systems, based on a notion of mapping correctness relying on the denotational semantics of mappings. © 2009 ACM.",Data exchange; Data integration; Mapping correctness; P2p systems; Type inference; Type systems; XML,Electronic data interchange; Mapping; Semantics; Web services; XML; Denotational semantics; Novel techniques; P2P system; Query reformulation; Remote data sources; Schema mappings; Type inferences; Type systems; Data integration
Controlling entity state updates to maintain remote consistency within a distributed interactive application,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350336643&doi=10.1145%2f1592446.1592449&partnerID=40&md5=2e2cc58f8346550c5bf663694316c899,"One of the ongoing challenges for Distributed Interactive Applications (DIAs) is balancing the quality of service delivered to the end user with the operational costs involved. In particular the resultant network traffic should be minimized without affecting the end user experience where possible. This article proposes the use of remote feedback as a method of maintaining a desired consistency level within a peer-to-peer DIA. Though many existing techniques attempt to maintain consistency within a DIA, they operate in an open-loop manner and do not take error introduced into the system due to transmission delay into consideration. The goal of the work presented in this article is to transform this open-loop scheme into a closed-loop control system utilizing feedback from the remote users. By incorporating remote error into the systems update paradigm, the Protocol Data Unit (PDU) transmission rate can be dynamically altered to reflect changing network conditions. The performance of the resultant closed-loop control system is presented within. © 2009 ACM.",Consistency maintenance; Dead reckoning; Distributed interactive applications (DIAs); Multiplayer games; Prediction mechanisms; Remote feedback,Closed loop control systems; Distributed computer systems; Game theory; Interactive devices; Internet; Maintainability; Navigation; Network protocols; Quality of service; Consistency maintenance; Dead reckoning; Distributed interactive applications (DIAs); Multiplayer games; Prediction mechanisms; Remote feedback; Feedback
Distribution fairness in Internet-scale networks,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350302170&doi=10.1145%2f1592446.1592450&partnerID=40&md5=ef7c39b9f891bc77e10c38417376e0f0,"We address the issue of measuring distribution fairness in Internet-scale networks. This problem has several interesting instances encountered in different applications, ranging from assessing the distribution of load between network nodes for load balancing purposes, to measuring node utilization for optimal resource exploitation, and to guiding autonomous decisions of nodes in networks built with market-based economic principles. Although some metrics have been proposed, particularly for assessing load balancing algorithms, they fall short. We first study the appropriateness of various known and previously proposed statistical metrics for measuring distribution fairness. We put forward a number of required characteristics for appropriate metrics. We propose and comparatively study the appropriateness of the Gini coefficient (G) for this task. Our study reveals as most appropriate the metrics of G, the fairness index (FI), and the coefficient of variation (CV) in this order. Second, we develop six distributed sampling algorithms to estimate metrics online efficiently, accurately, and scalably. One of these algorithms (2-PRWS) is based on two effective optimizations of a basic algorithm, and the other two (the sequential sampling algorithm, LBS-HL, and the clustered sampling one, EBSS) are novel, developed especially to estimate G. Third, we show how these metrics, and especially G, can be readily utilized online by higher-level algorithms, which can now know when to best intervene to correct unfair distributions (in particular, load imbalances). We conclude with a comprehensive experimentation which comparatively evaluates both the various proposed estimation algorithms and the three most appropriate metrics (G, CV, andFI). Specifically, the evaluation quantifies the efficiency (in terms of number of the messages and a latency indicator), precision, and accuracy achieved by the proposed algorithms when estimating the competing fairness metrics. The central conclusion is that the proposed metric, G, can be estimated with a small number of messages and latency, regardless of the skew of the underlying distribution. © 2009 ACM.",Distributed sampling; Distribution fairness; Peer-to-peer networks; The Gini coefficient,Distributed computer systems; Internet; Learning algorithms; Sequential switching; Telecommunication networks; Appropriate metrics; Autonomous decision; Coefficient of variation; Distributed sampling; Distribution fairness; Economic principle; Estimation algorithm; Fairness index; Gini coefficients; In-network; Internet-scale networks; Level algorithms; Load balancing algorithms; Load imbalance; Load-Balancing; Network node; Peer-to-peer networks; Sampling algorithm; Sequential sampling; The Gini coefficient; Underlying distribution; Metric system
An online blog reading system by topic clustering and personalized ranking,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149090858&doi=10.1145%2f1552291.1552292&partnerID=40&md5=6f21b6770952c2f0b88c33146bea02e3,"There is an increasing number of people reading, writing, and commenting on blogs. According to a recent survey made by Technorati, there are about 75,000 new blogs and 1.2 million new posts everyday. However, it is difficult and time consuming for a blog reader to find the most interesting posts in the huge and dynamic blog world. In this article, an online Personalized Blog Reader (PBR) system is proposed, which facilitates blog readers in browsing the coolest and newest blog posts of their interests by automatically clustering the most relevant stories. PBR aims to make a user's potential favorite topics always ranked higher than those nonfavorite ones. This is accomplished in the following steps. First, the system collects and provides a unified incremental index of posts coming from different blogs. Then, an incremental clustering algorithm with a flexible half-bounded window of observation is proposed to satisfy the requirements of online processing. It learns people's personalized reading preferences to present a user with a final reading list. The experimental results show that the proposed incremental clustering algorithm is effective and efficient, and the personalization of the PBR performs well.",Blog; Connected subgraph; Content information; Link information; Personalization; Ranking; Story; Topic,Clustering algorithms; Content based retrieval; Graph theory; Windows; Blog; Connected subgraph; Content information; Link information; Personalization; Ranking; Story; Topic; Internet
An ontology-driven approach for semantic information retrieval on the Web,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149101349&doi=10.1145%2f1552291.1552293&partnerID=40&md5=e1caaf058622b6021b99da269e1eb5f3,"The concept of relevance is a hot topic in the information retrieval process. In recent years the extreme growth of digital documents brought to light the need for novel approaches and more efficient techniques to improve the accuracy of IR systems to take into account real users' information needs. In this article we propose a novel metric to measure the semantic relatedness between words. Our approach is based on ontologies represented using a general knowledge base for dynamically building a semantic network. This network is based on linguistic properties and it is combined with our metric to create a measure of semantic relatedness. In this way we obtain an efficient strategy to rank digital documents from the Internet according to the user's interest domain. The proposed methods, metrics, and techniques are implemented in a system for information retrieval on the Web. Experiments are performed on a test set built using a directory service having information about analyzed documents. The obtained results compared to other similar systems show an effective improvement.",Ontologies; Semantic relatedness metrics; Word Net,Information retrieval; Information services; Knowledge based systems; Metric system; Semantics; Digital Documents; Directory service; Efficient strategy; General knowledge; Information need; Linguistic properties; Semantic information retrieval; Semantic network; Semantic relatedness; Semantic relatedness metrics; Test sets; User's interest; Word Net; Ontology
Web service clustering using multidimensional angles as proximity measures,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149092661&doi=10.1145%2f1552291.1552294&partnerID=40&md5=1d388d2dbad0373911835ff81116c407,"Increasingly, application developers seek the ability to search for existing Web services within large Internet-based repositories. The goal is to retrieve services that match the user's requirements. With the growing number of services in the repositories and the challenges of quickly finding the right ones, the need for clustering related services becomes evident to enhance search engine results with a list of similar services for each hit. In this article, a statistical clustering approach is presented that enhances an existing distributed vector space search engine for Web services with the possibility of dynamically calculating clusters of similar services for each hit in the list found by the search engine. The focus is laid on a very efficient and scalable clustering implementation that can handle very large service repositories. The evaluation with a large service repository demonstrates the feasibility and performance of the approach.",Clustering; Discovery; Search; Search engines; Service discovery; Vector space; Web service,Engines; Information retrieval; Internet; Search engines; Clustering; Discovery; Search; Service discovery; Vector space; Web services
Using a fuzzy classification approach to assess e-commerce Web sites: An empirical investigation,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149093779&doi=10.1145%2f1552291.1552295&partnerID=40&md5=85eb3cd3229c758bf9b048ad6d48ce8e,"E-commerce Web site assessment helps determine whether a corporation's Web site is effectively designed to meet its business needs and whether the investment in Web sites is well justified. Due to the complexity of commercial Web sites that may include hundreds of Web pages for many big corporations, there may inevitably exist uncertainties when human assessors express their subjective judgments in assessing e-commerce Web sites. Fuzzy set theory is widely used to model uncertain and imprecise information in applications. Prior studies in e-commerce Web site assessment identified some key factors to assess commercial Web sites by using a numeric assessment scale that may not be effective and efficient in modeling uncertainty. This study intends to propose an e-commerce Web site assessment framework using a fuzzy classification approach. Based on this framework, a Web-based e-commerce assessment system was designed and developed, which can provide online assessment services to corporations on evaluating their commercial Web sites. An empirical investigation into assessing commercial Web sites of the top 120 Fortune Corporations of the USA was conducted using the developed online assessment system to demonstrate the usefulness of the proposed framework. Research findings and implications are discussed.",E-commerce Web site assessment; E-commerce Web site assessment system; Fuzzy set,Electronic commerce; Fuzzy logic; Fuzzy sets; Internet; Surveys; Uncertainty analysis; Assessment scale; Assessment system; Business needs; E-Commerce; E-commerce Web site assessment; E-commerce Web site assessment system; Empirical investigation; Fuzzy classification; Human assessors; Imprecise information; Key factors; Modeling uncertainties; On-line assessment; Web page; Websites
Implications of Internet architecture on net neutrality,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349742462&doi=10.1145%2f1516539.1516540&partnerID=40&md5=be2b3de8795f8201e55d609ecb186f75,"Net neutrality represents the idea that Internet users are entitled to service that does not discriminate on the basis of source, destination, or ownership of Internet traffic. The United States Congress is considering legislation on net neutrality, and debate over the issue has generated intense lobbying. Congressional action will substantially affect the evolution of the Internet and of future Internet research. In this article, we argue that neither the pro nor anti net neutrality positions are consistent with the philosophy of Internet architecture. We develop a net neutrality policy founded on a segmentation of Internet services into infrastructure services and application services, based on the Internet's layered architecture. Our net neutrality policy restricts an Internet service Provider's ability to engage in anticompetitive behavior while simultaneously ensuring that it can use desirable forms of network management. We illustrate the effect of this policy by discussing acceptable and unacceptable uses of network management. © 2009 ACM.",,Internet service providers; Philosophical aspects; World Wide Web; Future internet; Infrastructure services; Internet architecture; Internet services; Internet traffic; Internet users; Layered architecture; Internet
Retrieving XML data from heterogeneous sources through vague querying,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349368267&doi=10.1145%2f1516539.1516542&partnerID=40&md5=a734046207e12d94eed67c79403e7752,"We propose a framework for querying heterogeneous XML data sources. The framework ensures high autonomy to participating sources as it does not rely on a global schema or on semantic mappings between schemas. The basic intuition is that of extending traditional approaches for approximate query evaluation, by providing techniques for combining partial answers coming from different sources, possibly on the basis of limited knowledge about the local schemas (i.e., key constraints). We define a query language and its associated semantics, that allows us to collect as much information as possible from several heterogeneous XML sources. We provide algorithms for query evaluation and characterize the complexity of the query language. Finally, we validate the approach in a medical application scenario. © 2009 ACM.",Heterogeneous databases; Querying; XML,Linguistics; Markup languages; Medical applications; Semantics; XML; Approximate query evaluation; Global schemas; Heterogeneous databases; Heterogeneous sources; Key constraints; Query evaluation; Querying; Schemas; Semantic mapping; XML data; XML data sources; Query languages
Analysis of single and networked auctions,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69849089032&doi=10.1145%2f1516539.1516543&partnerID=40&md5=9a4da1472a363a08e01a699ea6f5b193,"Web-based computerized auctions are increasingly present in the Internet. We can imagine that in the future this trend will actually be extended to situations where virtual buyer and seller agents will conduct automated transactions across the network, and that large sectors of the economy may be strucured in this manner. The purpose of this article is to model automated bidders and sellers which interact through a network. We model the bidding process as a random arrival process while the price attained by a good is modeled as a discrete random variable. We obtain analytical solutions allowing us to compute the income from a single auction, or the income per unit time from a repeated sequence of auctions. A variety of single-auction models are studied, including English and Vickrey auctions, and the income per unit time is derived as a function of other parameters, including the rate of arrival of bids, the seller's decision time, the value of the good, and the rest time of the seller between successive auctions. We illustrate the results via numerical examples. We also introduce a model for networked auctions where bidders can circulate among a set of interconnected auctions which we call the Mobile Bidder Model (MBM). We obtain an analytical solution for the MBM under the assumption,which we call the active bidders assumption, that activities that are internal to an auction (bids and sales) are much more frequent than changes that occur in the number of bidders at each auction. © 2009 ACM.",Automated auctions; Autonomic systems; E-commerce; Internet technologies,Automation; Electronic commerce; Internet; Random processes; Random variables; Analytical solutions; Arrival process; Auction model; Automated auctions; Autonomic systems; Bidding process; Buyer and seller agents; Decision time; Discrete random variables; E-commerce; Internet technologies; Numerical example; Per unit; Repeated sequences; Rest time; Vickrey auction; Wireless networks
On the state of IP spoofing defense,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349754408&doi=10.1145%2f1516539.1516541&partnerID=40&md5=89364f547b1fd2bc92623cd1eeddbee0,"IP source address spoofing has plagued the Internet for many years. Attackers spoof source addresses to mount attacks and redirect blame. Researchers have proposed many mechanisms to defend against spoofing, with varying levels of success. With the defense mechanisms available today, where do we stand How do the various defense mechanisms compare This article first looks into the current state of IP spoofing, then thoroughly surveys the current state of IP spoofing defense. It evaluates data from the Spoofer Project, and describes and analyzes host-based defense methods, router-based defense methods, and their combinations. It further analyzes what obstacles stand in the way of deploying those modern solutions and what areas require further research. © 2009 ACM.",IP spoofing; Packet filtering; Spoofing defense; Spoofing packet,Defense mechanism; First look; Host-based; IP spoofing; Packet filtering; Source address; Spoofing defense; Spoofing packet; Internet protocols
Resource overbooking and application profiling in a shared internet hosting platform,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149087382&doi=10.1145%2f1462159.1462160&partnerID=40&md5=b63574fdfe9c7c78573085731405bfb8,"In this article, we present techniques for provisioning CPU and network resources in shared Internet hosting platforms running potentially antagonistic third-party applications. The primary contribution of our work is to demonstrate the feasibility and benefits of overbooking resources in shared Internet platforms. Since an accurate estimate of an application's resource needs is necessary when overbooking resources, we present techniques to profile applications on dedicated nodes, possibly while in service, and use these profiles to guide the placement of application components onto shared nodes. We then propose techniques to overbook cluster resources in a controlled fashion. We outline an empirical appraoch to determine the degree of overbooking that allows a platform to achieve improvements in revenue while providing performance guarantees to Internet applications. We show how our techniques can be combined with commonly used QoS resource allocation mechanisms to provide application isolation and performance guarantees at run-time. We implement our techniques in a Linux cluster and evaluate them using common server applications. We find that the efficiency (and consequently revenue) benefits from controlled overbooking of resources can be dramatic. Specifically, we find that overbooking resources by as little as 1% we can increase the utilization of the cluster by a factor of two, and a 5% overbooking yields a 300 - 500% improvement, while still providing useful resource guarantees to applications. © 2009 ACM.",Capsule; Dedicated hosting platform; High percentile; Internet application; Placement; Profile; Quality-of-service; Resource overbooking; Shared hosting platform; Yield management,Economics; Internet; Planning; Resource allocation; Capsule; Dedicated hosting platform; High percentile; Internet application; Placement; Profile; Quality-of-service; Resource overbooking; Shared hosting platform; Yield management; Applications
Flexible provisioning of web service workflows,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149107347&doi=10.1145%2f1462159.1462161&partnerID=40&md5=503d3b34f06b56bc82c33f13984dd195,"Web services promise to revolutionize the way computational resources and business processes are offered and invoked in open, distributed systems, such as the Internet. These services are described using machine-readable metadata, which enables consumer applications to automatically discover and provision suitable services for their workflows at run-time. However, current approaches have typically assumed service descriptions are accurate and deterministic, and so have neglected to account for the fact that services in these open systems are inherently unreliable and uncertain. Specifically, network failures, software bugs and competition for services may regularly lead to execution delays or even service failures. To address this problem, the process of provisioning services needs to be performed in a more flexible manner than has so far been considered, in order to proactively deal with failures and to recover workflows that have partially failed. To this end, we devise and present a heuristic strategy that varies the provisioning of services according to their predicted performance. Using simulation, we then benchmark our algorithm and show that it leads to a 700% improvement in average utility, while successfully completing up to eight times as many workflows as approaches that do not consider service failures. © 2009 ACM.",Semantic Web services; Service composition; Service provisioning; Service-oriented computing; Web services; Workflows,Benchmarking; Computer software; Information theory; Metadata; Open systems; Program debugging; Quality of service; Semantic Web; Semantics; Semantic Web services; Service composition; Service provisioning; Service-oriented computing; Workflows; Web services
A model of process documentation to determine provenance in mash-ups,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62249091004&doi=10.1145%2f1462159.1462162&partnerID=40&md5=d009ebac3073826a3856ca10e9f4e7ae,"Through technologies such as RSS (Really Simple Syndication), Web Services, and AJAX (Asynchronous JavaScript and XML), the Internet has facilitated the emergence of applications that are composed from a variety of services and data sources. Through tools such as Yahoo Pipes, these mash-ups can be composed in a dynamic, just-in-time manner from components provided by multiple institutions (i.e., Google, Amazon, your neighbor). However, when using these applications, it is not apparent where data comes from or how it is processed. Thus, to inspire trust and confidence in mash-ups, it is critical to be able to analyze their processes after the fact. These trailing analyses, in particular the determination of the provenance of a result (i.e., the process that led to it), are enabled by process documentation, which is documentation of an application's past process created by the components of that application at execution time. In this article, we define a generic conceptual data model that supports the autonomous creation of attributable, factual process documentation for dynamic multi-institutional applications. The data model is instantiated using two Internet formats, OWL and XML, and is evaluated with respect to questions about the provenance of results generated by a complex bioinformatics mash-up. © 2009 ACM.",Concept maps; Data model; Mash-ups; Process; Process documentation; Provenance,Applications; Bioinformatics; Conformal mapping; Internet; Java programming language; Markup languages; Query processing; XML; Concept maps; Data model; Mash-ups; Process; Process documentation; Provenance; RSS
A peer-to-peer recommender system based on spontaneous affinities,2009,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149116834&doi=10.1145%2f1462159.1462163&partnerID=40&md5=4bb420b49b769b0091bde5773ffeded9,"Network analysis has proved to be very useful in many social and natural sciences, and in particular Small World topologies have been exploited in many application fields. In this article, we focus on P2P file sharing applications, where spontaneous communities of users are studied and analyzed. We define a family of structures that we call Affinity Networks (or even Graphs) that show self-organized interest-based clusters. Empirical evidence proves that affinity networks are small worlds and shows scale-free features. The relevance of this finding is augmented with the introduction of a proactive recommendation scheme, namely DeHinter, that exploits this natural feature. The intuition behind this scheme is that a user would trust her network of elective affinities more than anonymous and generic suggestions made by impersonal entities. The accuracy of the recommendation is evaluated by way of a 10-fold cross validation, and a prototype has been implemented for further feedbacks from the users. © 2009 ACM.",Complex networks; File sharing systems; Peer-to-Peer; Recommender system; Social networks,Electric network analysis; Complex networks; File sharing systems; Peer-to-Peer; Recommender system; Social networks; Distributed computer systems
Security and identification indicators for browsers against spoofing and phishing attacks,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-54049142548&doi=10.1145%2f1391949.1391950&partnerID=40&md5=1ea41a9dd5b10668b78c8c7217f4148f,"In spite of the use of standard Web security measures (SSL/TLS), users enter sensitive information such as passwords into fake Web sites. Such fake sites cause substantial damages to individuals and corporations. In this work, we identify several vulnerabilities of browsers, focusing on security and identification indicators. We present improved security and identification indicators, as we implemented in TrustBar, a browser extension we developed. With TrustBar, users can assign a name or logo to identify SSL/TLS-protected sites; if users did not assign a name or logo, TrustBar identifies protected sites by the name or logo of the site, and by the certificate authority (CA) who identified the site. We present usability experiments which compared TrustBar's indicators to the basic indicators available in most browsers (padlock, URL, and https prefix), and some relevant secure-usability principles. © 2008 ACM.",Human-computer interaction; Phishing; Secure usability; Web spoofing,Knowledge management; Certificate authorities; Phishing; Phishing attacks; Secure usability; Sensitive informations; Ssl/tls; Trustbar; Web securities; Web sites; Web spoofing; Human computer interaction
"Resource space model, OWL and database: Mapping and integration",2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849094835&doi=10.1145%2f1391949.1391954&partnerID=40&md5=50ba7d7c9f05a3a5ffe560bab0ecd7f4,"Semantics exhibits diversity in the real world, mental abstraction world, document world, and machine world. Studying mappings between different forms of semantics helps unveil the uniformity in the diversity. This article investigates the mappings between three typical semantic models: the Web ontology language (OWL), relational database model, and resource space model (a classification-based semantic model). By establishing mappings between the semantic primitives of the three models, we study the mapping from OWL description onto resource space and analyze the normal forms of the generated resource space. Mapping back from resource space onto OWL description is then discussed. Further, we investigate the mapping between OWL description and relational database, as well as the mapping between relational database and resource space. Normal forms of the generated relational tables are analyzed. To support advanced applications on the future Web, we suggest integrating the resource space, OWL, and databases to form a powerful semantic platform that enables different semantic models to enhance each other. © 2008 ACM.",Integration; Mapping; Relational database model; Resource space model; Semantic link network; Semantic web; Web ontology language,Conformal mapping; Information theory; Integration; Ontology; Semantics; Mapping; Relational database model; Resource space model; Semantic link network; Semantic web; Web ontology language; Database systems
Improving Web search using image snippets,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849121299&doi=10.1145%2f1391949.1391955&partnerID=40&md5=a452235fde36b4ea8fd3fce5b97c5e6b,"The Web has become the largest information repository in the world; thus, effectively and efficiently searching the Web becomes a key challenge. Interactive Web search divides the search process into several rounds, and for each round the search engine interacts with the user for more knowledge of the user's information requirement. Previous research mainly uses the text information on Web pages, while little attention is paid to other modalities. This article shows that Web search performance can be significantly improved if imagery is considered in interactive Web search. Compared with text, imagery has its own advantage: the time for ""reading"" an image is as little as that for reading one or two words, while the information brought by an image is as much as that conveyed by a whole passage of text. In order to exploit the advantages of imagery, a novel interactive Web search framework is proposed, where image snippets are first extracted from Web pages and then provided, along with the text snippets, to the user for result presentation and relevance feedback, as well as being presented alone to the user for image suggestion. User studies show that it is more convenient for the user to identify the Web pages he or she expects and to reformulate the initial query. Further experiments demonstrate the promise of introducing multimodal techniques into the proposed interactive Web search framework. © 2008 ACM.",Image snippet; Image suggestion; Interactive Web search; Multimodality; Relevance feedback; Term suggestion,Control theory; Feedback; Helium; Information retrieval; Search engines; Websites; Image snippet; Image suggestion; Interactive Web search; Multimodality; Relevance feedback; Term suggestion; World Wide Web
Design and implementation trade-offs for wide-area resource discovery,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53949109600&doi=10.1145%2f1391949.1391952&partnerID=40&md5=b146a2f5934aa85cf867acc9ae237754,"We describe the design and implementation of SWORD, a scalable resource discovery service for wide-area distributed systems. In contrast to previous systems, SWORD allows users to describe desired resources as a topology of interconnected groups with required intragroup, intergroup, and per-node characteristics, along with the utility that the application derives from specified ranges of metric values. This design gives users the flexibility to find geographically distributed resources for applications that are sensitive to both node and network characteristics, and allows the system to rank acceptable configurations based on their quality for that application. Rather than evaluating a single implementation of SWORD, we explore a variety of architectural designs that deliver the required functionality in a scalable and highly available manner. We discuss the trade-offs of using a centralized architecture as compared to a fully decentralized design to perform wide-area resource discovery. To summarize our results, we found that a centralized architecture based on 4-node server cluster sites at network-peering facilities outperforms a decentralized DHT-based resource discovery infrastructure with respect to query latency for all but the smallest number of sites. However, although a centralized architecture shows significant promise in stable environments, we find that our decentralized implementation has acceptable performance and also benefits from the DHT's self-healing properties in more volatile environments. We evaluate the advantages and disadvantages of centralized and distributed resource discovery architectures on 1000 hosts in emulation and on approximately 200 PlanetLab nodes spread across the Internet. © 2008 ACM.",PlanetLab; Resource discovery,Applications; Commerce; Design; Network protocols; Sensor networks; Servers; Centralized architecture; Decentralized design; Distributed resource discovery; Geographically distributed resources; Highly Available; Metric values; Network characteristics; PlanetLab; Planetlab nodes; Query latency; Resource discovery; Self-healing properties; Server clusters; Volatile environments; Wide-area; Wide-area distributed systems; Architectural design
Security analysis of Internet technology components enabling globally distributed workplaces - A framework,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53949123562&doi=10.1145%2f1391949.1391951&partnerID=40&md5=4e951ca1482ca19c283692a2fa1727a1,"As organizations increasingly operate, compete, and cooperate in a global context, business processes are also becoming global to propagate the benefits from coordination and standardization across geographical boundaries. In this context, security has gained significance due to increased threats, as well as legislation and compliance issues. This article presents a framework for assessing the security of Internet technology components that support a globally distributed workplace. Four distinct information flow and design architectures are identified based on location sensitivities and placements of the infrastructure components. Using a combination of scenarios, architectures, and technologies, the article presents the framework of a development tool for information security officers to evaluate the security posture of an information system. To aid managers in better understanding their options to improve security of the system, we also propose a three-dimensional representation, based on the framework, for embedding solution alternatives. To demonstrate its use in a real-world context, the article also applies the framework to assess a globally distributed workforce application at a northeast financial institution. © 2008 ACM.",Globally distributed workforce; Internet applications; Risk management; Security analysis,Industrial engineering; Internet; Sensitivity analysis; Societies and institutions; Three dimensional; Globally distributed workforce; Internet applications; Risk management; Security analysis; Arsenic
Semantics-based composition-oriented discovery of Web services,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53949108740&doi=10.1145%2f1391949.1391953&partnerID=40&md5=6ea1e0ac4ad232ddac4768d15e81c4f8,"Service discovery and service aggregation are two crucial issues in the emerging area of service-oriented computing (SOC). We propose a new technique for the discovery of (Web) services that accounts for the need of composing several services to satisfy a client query. The proposed algorithm makes use of OWL-S ontologies, and explicitly returns the sequence of atomic process invocations that the client must perform in order to achieve the desired result. When no full match is possible, the algorithm features a flexible matching by returning partial matches and by suggesting additional inputs that would produce a full match. © 2008 ACM.",Matchmaking algorithms; OWL-S ontologies; Web service composition; Web service discovery,Agglomeration; Information theory; Ontology; Matchmaking algorithms; OWL-S ontologies; Web service composition; Web service discovery; Boolean functions
Conformance checking of service behavior,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149089736&doi=10.1145%2f1361186.1361189&partnerID=40&md5=9136fbbd07db8b2ba81aa88268e5432f,"A service-oriented system is composed of independent software units, namely services, that interact with one another exclusively through message exchanges. The proper functioning of such system depends on whether or not each individual service behaves as the other services expect it to behave. Since services may be developed and operated independently, it is unrealistic to assume that this is always the case. This article addresses the problem of checking and quantifying how much the actual behavior of a service, as recorded in message logs, conforms to the expected behavior as specified in a process model. We consider the case where the expected behavior is defined using the BPEL industry standard (Business Process Execution Language for Web Services). BPEL process definitions are translated into Petri nets and Petri net-based conformance checking techniques are applied to derive two complementary indicators of conformance: fitness and appropriateness. The approach has been implemented in a toolset for business process analysis and mining, namely ProM, and has been tested in an environment comprising multiple Oracle BPEL servers. © 2008 ACM.",BPEL; Conformance; Petri nets; ProM; Web services,"Computer networks; Graph theory; Industry; Information services; Marine biology; Petri nets; Standards; Web services; (e ,3e) process; (extended) Petri nets; Applied (CO); Business process analysis; Business process Execution Language for web services (BPEL4WS); Conformance checking; Individual (PSS 544-7); Industry standards; Message exchanging; Process modelling; Service oriented systems; Tool sets; Model checking"
Pattern-based design of a service-oriented middleware for remote object federations,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849144337&doi=10.1145%2f1361186.1361191&partnerID=40&md5=034c4f420e7e9e7771d00fcdb6a343c7,"Service-oriented middleware architectures should enable the rapid realization of loosely coupled services. Unfortunately, existing technologies used for service-oriented middleware architectures, such as Web services, P2P systems, coordination and cooperation technologies, and spontaneous networking, do not fully support all requirements in the realm of loosely coupled business services yet. Typical problems that arise in many business domains are for instance missing central control, complex cooperation models, complex lookup models, or issues regarding dynamic deployment. We used a pattern-based approach to identify the well working solutions in the different technologies for loosely coupled services. Then we reused this design knowledge in our concept for a service-oriented middleware. This concept is centered around a controlled environment, called a federation. Each remote object (a peer service) is controlled in one or more federations, but within this environment peers can collaborate in a simple-to-use, loosely coupled, and ad hoc style of communication. A semantic lookup service is used to let the peers publish rich metadata about themselves to their fellow peers. © 2008 ACM.",Middleware; Service-oriented architecture; Software patterns,Architectural design; Coordination reactions; Distributed computer systems; Dynamic models; Dynamical systems; Fiber optic sensors; Information services; Information theory; Metadata; Metropolitan area networks; Middleware; Multi agent systems; Network protocols; business domains; business services; Central control; Controlled environment; cooperation models; Design knowledge; Dynamic deployment; lookup services; Loosely-coupled services; P2p systems; Remote object; Service-oriented middleware; Spontaneous networking; Technology
Guest editorial: Service-oriented computing,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849095124&doi=10.1145%2f1361186.1361187&partnerID=40&md5=707da74dbf38ca36e3352cb3e02ec800,[No abstract available],,
A framework for service-oriented computing with C and C++ Web service components,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849110560&doi=10.1145%2f1361186.1361188&partnerID=40&md5=65ec693f0122e002396099b266f4aead,"Service-oriented architectures use loosely coupled softwareservices to support the requirements of business processes andsoftware users. Several software engineering challenges have to beovercome to expose legacy C and C++ applications and specializedsystem resources as XML-based software services. It is critical todevise effective bindings between XML and C/C++ data to efficientlyinteroperate with other XML-based services. Binding applicationdata to XML has many software solutions, ranging from genericdocument object models to idiosyncratic type mappings. A safebinding must conform to XML validation constraints, guarantee typesafety, and should preserve the structural integrity ofcommunicated application data. However, tight XML bindings imposemapping constraints that can hamper interoperability betweenservices. This paper presents a framework for constructing looselycoupled C/C++ services based on a programming model that integratesXML bindings into the C and C++ syntax. The concepts behind thebindings are generic, which makes the approach applicable to otherprogramming languages. © 2008 ACM.",Service-oriented computing; Web services standards,Canning; Computer programming languages; Information management; Information services; Markup languages; Mathematical models; Quality assurance; Reliability; Reusability; Software engineering; Technology; XML; Application data; Business processes (BP); C++ syntax; engineering challenges; Languages (traditional); object modelling; programming models; Service oriented architectures (SOAs); Service oriented computing; software services; Software solutions; Web service components; Computer software
QoS-Aware service management for component-based distributed applications,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849135479&doi=10.1145%2f1361186.1361190&partnerID=40&md5=053014a268d63a0a55f741d0959034f5,"Component-based software development has evolved from a tightly coupled style to a loosely coupled style in the recent few years. The paradigm shift will eventually allow heterogeneous systems to interoperate in open networks such as the Internet and will make software development more of a management task than a development task. Envisioning that future applications may comprise dynamically aggregated component services possibly distributed widely, we develop a Quality of Service (QoS)-aware service management framework in the middleware layer to make the component services infrastructure transparent to the applications. Specifically, we manage services not only as individuals, but more importantly as meaningful aggregated entities based on the logical compositional needs coming from the applications, by composing services properly according to QoS requirements at application setup time, and performing continuous maintenance at application runtime seamlessly. Our service management framework is scalable in two dimensions: network size and application's client population size. Specifically, the framework employs a decentralized management solution that scales to large network size, and explores resource sharing in one-to-many group-based applications by means of multicasting mechanisms. Moreover, it incorporates local adaptation operations and distributed failure detection, reporting, and recovery mechanisms to deal with runtime resource fluctuations and failures. © 2008 ACM.",Application-level routing; Fault tolerance; Multicast; Overlay networks; QoS; Service composition; Service management; SOA,Ad hoc networks; Computer networks; Computer software maintenance; Failure (mechanical); Information technology; Internet; Management; Management science; Mechanisms; Metropolitan area networks; Middleware; Multitasking; Network protocols; Population statistics; Quality management; Component based software development (CBSE); Component-based; Continuous maintenance; Decentralized management; Distributed applications (DA); failure detection; future applications; Heterogeneous (hybrid) systems; Local adaptation; Middleware layer; Network sizes; open networks; paradigm shifts; population sizes; QOS requirements; Quality of service (QoS) aware; recovery mechanisms; Resource sharing; Run time; Service Management; Set-up time; software development; Tightly coupled; Two dimensions; Quality of service
A multimedia broker to support accessible and mobile learning through learning objects adaptation,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049102859&doi=10.1145%2f1323651.1323655&partnerID=40&md5=97049bc188ee0fd1c6748f87129268d0,"The large diffusion of e-learning technologies represents a great opportunity for underserved segments of population. This is particularly true for people with disabilities for whom digital barriers should be overstepped with the aim of reengaging them back into society to education. In essence, before a mass of learners can be engaged in a collective educational process, each single member should be put in the position to enjoy accessible and customized educational experiences, regardless of the wide diversity of their personal characteristics and technological equipment. To respond to this demand, we developed LOT (Learning Object Transcoder), a distributed PHP-based service-oriented system designed to deliver flexible and customized educational services for a multitude of learners, each with his/her own diverse preferences and needs. The main novelty of LOT amounts to a broking service able to manage the transcoding activities needed to convert multimedia digital material into the form which better fits a given student profile. Transcoding activities are performed based on the use of Web service technologies. Experimental results gathered from several field trials with LOT (available online at http://137.204.74.83/~lot/) have confirmed the viability of our approach. © 2008 ACM.",Accessibility; Content transcoding; Device profiling; E-learning; Mobile-learning; Multimedia adaptation; User profiling,Digital signal processing; Encoding (symbols); Multimedia systems; Online systems; User interfaces; Accessibility; Content transcoding; Device profiling; Mobile learning; Multimedia adaptation; User profiling; E-learning
Technology supports for distributed and collaborative learning over the internet,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049104243&doi=10.1145%2f1323651.1323656&partnerID=40&md5=e49a1931e03b1da84fa79614f1dc0c51,"With the advent of Internet and World Wide Web (WWW) technologies, distance education (e-learning or Web-based learning) has enabled a new era of education. There are a number of issues that have significant impact on distance education, including those from educational, sociological, and psychological perspectives. Rather than attempting to cover exhaustively all the related perspectives, in this survey article, we focus on the technological issues. A number of technology issues are discussed, including distributed learning, collaborative learning, distributed content management, mobile and situated learning, and multimodal interaction and augmented devices for e-learning. Although we have tried to include the state-of-the-art technologies and systems here, it is anticipated that many new ones will emerge in the near future. As such, we point out several emerging issues and technologies that we believe are promising, for the purpose of highlighting important directions for future research. © 2008 ACM.",Collaborative learning; Distance learning technologies; Distributed content management; Distributed learning,Computer supported cooperative work; Distributed computer systems; Information management; Internet protocols; Mobile devices; Psychology computing; Collaborative learning; Distance learning technologies; Distributed content management; Distributed learning; E-learning
Personalizing access to learning networks,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049111706&doi=10.1145%2f1323651.1323654&partnerID=40&md5=202bbe64e321cbd5c4e0defd3cfc9041,"In this article, we describe a Smart Space for Learning™ (SS4L) framework and infrastructure that enables personalized access to distributed heterogeneous knowledge repositories. Helping a learner to choose an appropriate learning resource or activity is a key problem which we address in this framework, enabling personalized access to federated learning repositories with a vast number of learning offers. Our infrastructure includes personalization strategies both at the query and the query results level. Query rewriting is based on learning and language preferences; rule-based and ranking-based personalization improves these results further. Rule-based reasoning techniques are supported by formal ontologies we have developed based on standard information models for learning domains; ranking-based recommendations are supported through ensuring minimal sets of predicates appearing in query results. Our evaluation studies show that the implemented solution enables learners to find relevant learning resources in a distributed environment and through goal-based personalization improves relevancy of results. © 2008 ACM.",Learning networks; Ontologies; Personalization; Personalized access; Semantic Web,Access control; Distributed computer systems; Knowledge based systems; Ontology; Query languages; Semantic Web; Learning networks; Personalization strategies; Personalized access; Query rewriting; Learning systems
Introduction to special issue internet technologies for distance education,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049099457&doi=10.1145%2f1323651.1323652&partnerID=40&md5=0316bb0aa00bc0601275cfd7f493254b,[No abstract available],,
Nearcast: A locality-aware P2P live streaming approach for distance education,2008,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049112611&doi=10.1145%2f1323651.1323653&partnerID=40&md5=29c189bb9abbf7f92893ffbff0743bed,"Peer-to-peer (P2P) live video streaming has been widely used in distance education applications to deliver the captured video courses to a large number of online students. By allowing peers serving each other in the network, P2P technology overcomes many limitations in the traditional client-server paradigm to achieve user and bandwidth scalabilities. However, existing systems do not perform well when the number of online students increases, and the system performance degrades seriously. One of the reasons is that the construction of the peer overlay in existing P2P systems has not considered the underlying physical network topology and can cause serious topology mismatch between the P2P overlay network and the physical network. The topology mismatch problem brings great link stress (unnecessary traffic) in the Internet infrastructure and greatly degrades the system performance. In this article, we address this problem and propose a locality-aware P2P overlay construction method, called Nearcast, which builds an efficient overlay multicast tree by letting each peer node choose physically closer nodes as its logical children. We have conducted extensive simulations to evaluate the performance of Nearcast in comparison with the existing RTT and NICE protocols. Also, Nearcast has been deployed on a wide-area network testbed to delivery video coursed to about 7200 users distributed across 100 collages in 32 cities in China. The experimental results show that Nearcast leads to lower link stress and shorter end-to-end latencies compared with the RTT and NICE protocols. © 2008 ACM.",Distance education; Live streaming; Peer-to-peer; Video,Client server computer systems; E-learning; Multicasting; Students; Systems science; Client-server paradigm; Online students; Peer-to-peer (P2P) live video streaming; Physical networks; Video streaming
Supporting intelligent Web search,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35349023786&doi=10.1145%2f1278366.1278369&partnerID=40&md5=25c45c65a6dd26cb86a190badf450974,"Search engines continue to struggle to provide everyday users with a service capable of delivering focussed results that are relevant to their information needs. Moreover, traditional search engines really only provide users with a starting point for their information search. That is, upon selecting a page from a search result list, the interaction between user and search engine is effectively over and the user must continue their search alone. In this article, we argue that a comprehensive search service needs to provide the user with more help, both at the result list level and beyond, and we outline some recommendations for intelligent Web search support. We introduce the SearchGuide Web search support system and we describe how it fulfils the requirements for a search support system, providing evaluation results where applicable. © 2007 ACM.",Collaborative search; Explanation; Interaction history; Personalization; Visualisation; Web search,Information dissemination; Online searching; Search engines; User interfaces; Web services; Collaborative search; Explanation; Interaction history; Personalization; Web search; Artificial intelligence
Generating semantically enriched user profiles for Web personalization,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35349020030&doi=10.1145%2f1278366.1278371&partnerID=40&md5=94c97b724c0a3cf12dcf1e7297deb860,"Traditional collaborative filtering generates recommendations for the active user based solely on ratings of items by other users. However, most businesses today have item ontologies that provide a useful source of content descriptors that can be used to enhance the quality of recommendations generated. In this article, we present a novel approach to integrating user rating vectors with an item ontology to generate recommendations. The approach is novel in measuring similarity between users in that it first derives factors, referred to as impacts, driving the observed user behavior and then uses these factors within the similarity computation. In doing so, a more comprehensive user model is learned that is sensitive to the context of the user visit. An evaluation of our recommendation algorithm was carried out using data from an online retailer of movies with over 94,000 movies, 44,000 actors, and 10,000 directors within the item knowledge base. The evaluation showed a statistically significant improvement in the prediction accuracy over traditional collaborative filtering. Additionally, the algorithm was shown to generate recommendations for visitors that belong to sparse sections of the user space, areas where traditional collaborative filtering would generally fail to generate accurate recommendations. © 2007 ACM.",Collaborative filtering; Evaluation; Implicit ratings; Personalization; Similarity metric,Algorithms; Information dissemination; Ontology; User interfaces; Web services; Collaborative filtering; Evaluation; Implicit ratings; Personalization; Similarity metric; Semantics
Introduction to special issue on semantic Web services,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949009264&doi=10.1145%2f1294148.1294149&partnerID=40&md5=34bb1e23d2b192d9f3c36547dd150034,[No abstract available],,
A context-based mediation approach to compose semantic web services,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949019167&doi=10.1145%2f1294148.1294152&partnerID=40&md5=bb66c930b2e68697511a4f5177a0f29f,"Web services composition is a keystone in the development of interoperable systems. However, despite the widespread adoption of Web services, several obstacles still hinder their smooth automatic semantic reconciliation when being composed. Consistent understanding of data exchanged between composed Web services is hampered by various implicit modeling assumptions and representations. Our contribution in this article revolves around context and how it enriches data exchange between Web services. In particular, a context-based mediation approach to solve semantic heterogeneities between composed Web services is presented. © 2007 ACM.",Composition; Context; Mediation; Semantics; Web services,Electronic data interchange; Semantic Web; Semantics; Context-based mediation; Interoperable systems; Web services
Model-driven design and development of semantic Web service applications,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949019170&doi=10.1145%2f1294148.1294151&partnerID=40&md5=b69d1962e0208e7adae8f9dbff5cd453,"This article proposes a model-driven methodology to design and develop semantic Web service applications and their components, described according to the emerging WSMO standard. In particular, we show that business processes and Web engineering models have sufficient expressive power to support the semiautomatic extraction of semantic descriptions (i.e., WSMO ontologies, goals, Web services, and mediators), thus partially hiding the complexity of dealing with semantics. Our method is based on existing models for the specification of business processes (BPMN) combined with Web engineering models for designing and developing semantically rich Web applications (WebML). The proposed approach leads from an abstract view of the business needs to a concrete implementation of the application by means of several design steps; high-level models are transformed into software components. Our framework increases the efficiency of the whole design process, yielding to the construction of semantic Web service applications spanning over several enterprises. © 2007 ACM.",Semantic Web service; WebML; WSMO,Computer software; Ontology; Semantic Web; Semantic Web service; WebML; Web services
A semantic approach to approximate service retrieval,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949002783&doi=10.1145%2f1294148.1294150&partnerID=40&md5=236f096c53cd47408c6276690149dc51,"Web service discovery is one of the main applications of semantic Web services, which extend standard Web services with semantic annotations. Current discovery solutions were developed in the context of automatic service composition. Thus, the client of the discovery procedure is an automated computer program rather than a human, with little, if any, tolerance to inexact results. However, in the real world, services which might be semantically distanced from each other are glued together using manual coding. In this article, we propose a new retrieval model for semantic Web services, with the objective of simplifying service discovery for human users. The model relies on simple and extensible keyword-based query language and enables efficient retrieval of approximate results, including approximate service compositions. Since representing all possible compositions and all approximate concept references can result in an exponentially-sized index, we investigate clustering methods to provide a scalable mechanism for service indexing. Results of experiments, designed to evaluate our indexing and query methods, show that satisfactory approximate search is feasible with efficient processing time. © 2007 ACM.",Ontology; Semantic web; Service retrieval; Web service,Mathematical models; Ontology; Query languages; Semantic Web; Web services; Query methods; Service retrieval; Information retrieval
Introduction to intelligent techniques for Web personalization,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35548983131&doi=10.1145%2f1278366.1278367&partnerID=40&md5=79fbec13330b3b1342ee468c6842c467,"Various studies conducted in the field of intelligent techniques for Web personalization achieved through the implementation of different phases of data mining are presented. A study focuses on use of clickthrough data to learn user preferences to adapt search engine result ranking, and concludes that the approach to search engine personalization provides data accuracy. SearchGuide, a Web search support system derives search results to a user by considering the collective interests of a group of users through collaborative Web search. The integration of navigation trails with data about the hyperlink structure of a Web site improves the quality of personalization achieved. The studies also focus on generating semantic user profiles for use in personalization and improve the accuracy of the resulting personalization.",,Data mining; Intelligent agents; Search engines; Semantics; Data accuracy; Hyperlink structure; Web search support system; World Wide Web
Web services discovery in secure collaboration environments,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949011422&doi=10.1145%2f1294148.1294153&partnerID=40&md5=6ed814dc6e481428b824a97db422ddf3,"Multidomain application environments where distributed domains interoperate with each other is a reality in Web-services-based infrastructures. Collaboration enables domains to effectively share resources; however, it introduces several security and privacy challenges. In this article, we use the current web service standards such as SOAP and UDDI to enable secure interoperability in a service-oriented mediator-free environment. We propose a multihop SOAP messaging protocol that enables domains to discover secure access paths to access roles in different domains. Then we propose a path authentication mechanism based on the encapsulation of SOAP messages and the SOAP-DISG standard. Furthermore, we provide a service discovery protocol that enables domains to discover service descriptions stored in private UDDI registries. © 2007 ACM.",Encapsulated SOAP; Private UDDI registries; Protocols; Secure access paths; Secure collaboration; Services,Authentication; Interoperability; Resource allocation; Standards; Encapsulated SOAP; Private UDDI registries; Secure access paths; Secure collaboration; Web services
Web site personalization based on link analysis and navigational patterns,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348999613&doi=10.1145%2f1278366.1278370&partnerID=40&md5=3ca95e76c4502798dfda1c2a09c066d8,"The continuous growth in the size and use of the World Wide Web imposes new methods of design and development of online information services. The need for predicting the users' needs in order to improve the usability and user retention of a Web site is more than evident and can be addressed by personalizing it. Recommendation algorithms aim at proposing next pages to users based on their current visit and past users' navigational patterns. In the vast majority of related algorithms, however, only the usage data is used to produce recommendations, disregarding the structural properties of the Web graph. Thus important - -in terms of PageRank authority score - -pages may be underrated. In this work, we present UPR, a PageRank-style algorithm which combines usage data and link analysis techniques for assigning probabilities to Web pages based on their importance in the Web site's navigational graph. We propose the application of a localized version of UPR (l-UPR) to personalized navigational subgraphs for online Web page ranking and recommendation. Moreover, we propose a hybrid probabilistic predictive model based on Markov models and link analysis for assigning prior probabilities in a hybrid probabilistic model. We prove, through experimentation, that this approach results in more objective and representative predictions than the ones produced from the pure usage-based approaches. © 2007 ACM.",Link analysis; Markov models; Recommendations; Usage-based PageRank; Web personalization,Algorithms; Data reduction; Information services; Markov processes; Online systems; Pattern recognition; User interfaces; Websites; Link analysis; Markov models; Recommendations; Usage based PageRank; Web graph; Web personalization; Artificial intelligence
Toward trustworthy recommender systems: An analysis of attack models and algorithm robustness,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348969366&doi=10.1145%2f1278366.1278372&partnerID=40&md5=0a1ef5be402444e030b8481afe7c3923,"Publicly accessible adaptive systems such as collaborative recommender systems present a security problem. Attackers, who cannot be readily distinguished from ordinary users, may inject biased profiles in an attempt to force a system to adapt in a manner advantageous to them. Such attacks may lead to a degradation of user trust in the objectivity and accuracy of the system. Recent research has begun to examine the vulnerabilities and robustness of different collaborative recommendation techniques in the face of profile injection attacks. In this article, we outline some of the major issues in building secure recommender systems, concentrating in particular on the modeling of attacks and their impact on various recommendation algorithms. We introduce several new attack models and perform extensive simulation-based evaluations to show which attacks are most successful and practical against common recommendation techniques. Our study shows that both user-based and item-based algorithms are highly vulnerable to specific attack models, but that hybrid algorithms may provide a higher degree of robustness. Using our formal characterization of attack models, we also introduce a novel classification-based approach for detecting attack profiles and evaluate its effectiveness in neutralizing attacks. © 2007 ACM.",Attack detection; Collaborative filtering; Profile injection attacks; Recommender systems; Shilling,Algorithms; Robustness (control systems); Security systems; User interfaces; Attack detection; Collaborative filtering; Profile injection attacks; Recommender systems; Adaptive systems
Mining user preference using Spy voting for search engine personalization,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35349013130&doi=10.1145%2f1278366.1278368&partnerID=40&md5=23142d0d51bb8e9040c587900cf3ed1d,"This article addresses search engine personalization. We present a new approach to mining a user's preferences on the search results from clickthrough data and using the discovered preferences to adapt the search engine's ranking function for improving search quality. We develop a new preference mining technique called SpyNB, which is based on the practical assumption that the search results clicked on by the user reflect the user's preferences but does not draw any conclusions about the results that the user did not click on. As such, SpyNB is still valid even if the user does not follow any order in reading the search results or does not click on all relevant results. Our extensive offline experiments demonstrate that SpyNB discovers many more accurate preferences than existing algorithms do. The interactive online experiments further confirm that SpyNB and our personalization approach are effective in practice. We also show that the efficiency of SpyNB is comparable to existing simple preference mining algorithms. © 2007 ACM.",Clickthrough data; Personalization; Search engine; User preferences,Algorithms; Online searching; Search engines; User interfaces; Clickthrough data; Mining algorithms; Personalization; Spy voting; User preferences; Data mining
Preserving data privacy in outsourcing data aggregation services,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548264581&doi=10.1145%2f1275505.1275510&partnerID=40&md5=cfe0f6c1e60d6cfad8f64e900cc02a91,"Advances in distributed service-oriented computing and Internet technology have formed a strong technology push for outsourcing and information sharing. There is an increasing need for organizations to share their data across organization boundaries both within the country and with countries that may have lesser privacy and security standards. Ideally, we wish to share certain statistical data and extract the knowledge from the private databases without revealing any additional information of each individual database apart from the aggregate result that is permitted. In this article, we describe two scenarios for outsourcing data aggregation services and present a set of decentralized peer-to-peer protocols for supporting data sharing across multiple private databases while minimizing the data disclosure among individual parties. Our basic protocols include a set of novel probabilistic computation mechanisms for important primitive data aggregation operations across multiple private databases such as max, min, and top k selection. We provide an analytical study of our basic protocols in terms of precision, efficiency, and privacy characteristics. Our advanced protocols implement an efficient algorithm for performing kNN classification across multiple private databases. We provide a set of experiments to evaluate the proposed protocols in terms of their correctness, efficiency, and privacy characteristics. © 2007 ACM.",Classification; Confidentiality; Outsourcing; Privacy,Classification (of information); Data privacy; Database systems; Internet; Optimization; Outsourcing; Aggregation services; Confidentiality; Data sharing; Security standards; Web services
Guest editorial: The Internet and outsourcing,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548270873&doi=10.1145%2f1275505.1275506&partnerID=40&md5=d20dd69f80da1dd95424540f71bbfc4c,[No abstract available],,
24-hour knowledge factory: Using Internet technology to leverage spatial and temporal separations,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548221840&doi=10.1145%2f1275505.1275507&partnerID=40&md5=c3110d91c7ccbb0abc7ba1d1ecbc3d18,"Several of the outsourcing endeavors of today will gradually converge to a hybrid outsourcing model that will involve a team spread across three or more strategically-located centers interconnected by Internet technology. White-collar professionals in the US, Australia, and Poland, for example, could each work on a standard 9 - 5 basis, transfer the activity to a colleague in the next center, thereby enabling work to be performed on a round-the-clock basis. The effective use of sequential workers in such a 24-hour knowledge factory requires that professional tasks be broken down to the level where individuals can work on them with minimal interaction with their peers, and where new approaches can be employed to reduce the effort involved in transitioning from one employee to the next. This article describes an Internet-based prototype system that uses a Web-based interactive approach, coupled with a unique data model, to optimize collection and storage of design rationale and history from stakeholders and workers. The idea of multiple individuals acting as one composite persona is explored in the context of facilitating tasks and knowledge to be shared across the Internet in a seamless manner. The article also describes related activities in the commercial arena. © 2007 ACM.",Geographic boundaries; Global teams; Knowledge management; Knowledge sharing; Outsourcing; Temporal boundaries,Data transfer; Internet; Large scale systems; Outsourcing; Software prototyping; Temporal logic; Geographic boundaries; Global teams; Knowledge sharing; Temporal boundaries; Knowledge management
The Internet's role in offshored services: A case study of India,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548238097&doi=10.1145%2f1275505.1275508&partnerID=40&md5=1b046bca6d19ccfdcc708e37ff35f8ea,"Using India as a case study, this article analyzes how the Internet influenced its export-oriented software industry. We show that prior to the Internet, domestic entrepreneurship was the key factor for the industry's origin and growth. The industry suffered from relatively low value-addition. As a result, domestic firms, though they were industry leaders within India, were followers of the global leadership provided by transnational firms. With the arrival of the Internet, there was a rise in the level of domain expertise. We show that the Internet facilitated the transfer of domain expertise for foreign firms more than it helped the acquisition of domain expertise by domestic firms. While the value-addition of the industry increased as a result, industry leadership began to pass to foreign firms. The strategic lesson for other countries trying to rapidly develop an export-oriented software industry is unambiguous: exclusive reliance on domestic entrepreneurship will usually result in the domestic industry falling behind its global competitors, while granting unrestricted entry to transnational firms will lead to the domestic firms losing industry leadership in most cases. © 2007 ACM.",Globalization; India; Services; Software,Data transfer; Domain decomposition methods; Global system for mobile communications; Internet; Telecommunication industry; Domain expertise; Entrepreneurship; Low value-addition; Software industry; Web services
Impact of Internet-based distributed monitoring systems on offshore sourcing of services,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548224093&doi=10.1145%2f1275505.1275509&partnerID=40&md5=fb9a7c339386cb6bbea57df04f941f98,"The use of Internet-based distributed monitoring systems has allowed firms to source services globally that were hitherto thought of as being too risky or complex to execute offshore. These systems enable buyers of such services (clients) to monitor the execution of processes in real-time and exert a degree of managerial control over information workers of another firm located in a distant region. Our research shows that process codifiability plays a key role in determining output quality. Further, we show that the efforts made by the client and the provider in monitoring work via Internet-based monitoring mechanisms have a significant impact on output quality. Finally, we show that these monitoring mechanisms enable clients and providers of services to focus on those processes that are best managed by each entity. This scheme of optimal allocation of monitoring effort is termed by us as the efficient monitoring frontier. © 2007 ACM.",Efficient monitoring frontier; Internet-enabled monitoring systems; Knowledge continuum; Offshore outsourcing; Real-time monitoring,Computer control; Internet; Knowledge management; Real time systems; Web services; Efficient monitoring frontiers; Internet-enabled monitoring systems; Knowledge continuum; Offshore outsourcing; Distributed computer systems
XQueC: A query-conscious compressed XML database,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250195749&doi=10.1145%2f1239971.1239974&partnerID=40&md5=bbff80ea0fac97481b4c2747ed036042,"XML compression has gained prominence recently because it counters the disadvantage of the verbose representation XML gives to data. In many applications, such as data exchange and data archiving, entirely compressing and decompressing a document is acceptable. In other applications, where queries must be run over compressed documents, compression may not be beneficial since the performance penalty in running the query processor over compressed data outweighs the data compression benefits. While balancing the interests of compression and query processing has received significant attention in the domain of relational databases, these results do not immediately translate to XML data. In this article, we address the problem of embedding compression into XML databases without degrading query performance. Since the setting is rather different from relational databases, the choice of compression granularity and compression algorithms must be revisited. Query execution in the compressed domain must also be rethought in the framework of XML query processing due to the richer structure of XML data. Indeed, a proper storage design for the compressed data plays a crucial role here. The XQueC system (XQuery Processor and Compressor) covers a wide set of XQuery queries in the compressed domain and relies on a workload-based cost model to perform the choices of the compression granules and of their corresponding compression algorithms. As a consequence, XQueC provides efficient query processing on compressed XML data. An extensive experimental assessment is presented, showing the effectiveness of the cost model, the compression ratios, and the query execution times. © 2007 ACM.",XML compression; XML data management; XML databases; XQuery,Data compression; Data structures; Electronic data interchange; Information management; Mathematical models; Query processing; Relational database systems; Compression ratios; Data archiving; Query processors; Verbose representation; XML
Optimal design of english auctions with discrete bid levels,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250178068&doi=10.1145%2f1239971.1239976&partnerID=40&md5=870c74731df7f90615e951b6f32d23b7,"This article considers a canonical auction protocol that forms the basis of nearly all current online auctions. Such discrete bid auctions require that the bidders submit bids at predetermined discrete bid levels, and thus, there exists a minimal increment by which the bid price may be raised. In contrast, the academic literature of optimal auction design deals almost solely with continuous bid auctions. As a result, there is little practical guidance as to how an auctioneer, seeking to maximize its revenue, should determine the number and value of these discrete bid levels, and it is this omission that is addressed here. To this end, a model of an ascending price English auction with discrete bid levels is considered. An expression for the expected revenue of this auction is derived and used to determine numerical and analytical solutions for the optimal bid levels in the case of uniform and exponential bidder's valuation distributions. Finally, in order to develop an intuitive understanding of how these optimal bid levels are distributed, the limiting case where the number of discrete bid levels is large is considered, and an analytical expression for their distribution is derived. © 2007 ACM.",Discrete bids; English auction; Optimal auction design,Electronic data interchange; Financial data processing; Mathematical models; Numerical methods; Online systems; Canonical auction protocols; Discrete bid levels; Online auctions; Internet protocols
Characterization of national Web domains,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250194626&doi=10.1145%2f1239971.1239973&partnerID=40&md5=899a54e76af9495622e018719b72f74c,"During the last few years, several studies on the characterization of the public Web space of various national domains have been published. The pages of a country are an interesting set for studying the characteristics of the Web because at the same time these are diverse (as they are written by several authors) and yet rather similar (as they share a common geographical, historical and cultural context). This article discusses the methodologies used for presenting the results of Web characterization studies, including the granularity at which different aspects are presented, and a separation of concerns between contents, links, and technologies. Based on this, we present a side-by-side comparison of the results of 12 Web characterization studies, comprising over 120 million pages from 24 countries. The comparison unveils similarities and differences between the collections and sheds light on how certain results of a single Web characterization study on a sample may be valid in the context of the full Web. © 2007 ACM.",Web characterization; Web measurement,Information dissemination; Information management; Information retrieval; Telecommunication links; Web characterization; Web domains; Web measurement; Websites
Supporting ad-hoc resource sharing on the Web: A peer-to-peer approach to hypermedia link services,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250204131&doi=10.1145%2f1239971.1239975&partnerID=40&md5=d51695083ef50c59ea60592e627621b0,"The key element to support ad-hoc resource sharing on the Web is to discover resources of interest. The hypermedia paradigm provides a way of overlaying a set of resources with additional information in the form of links to help people find other resources. However, existing hypermedia approaches primarily develop mechanisms to enable resource sharing in a fairly static, centralized way. Recent developments in distributed computing, on the other hand, introduced peer-to-peer (P2P) computing that is notable for employing distributed resources to perform a critical function in a more dynamic and ad-hoc scenario. We investigate the feasibility and potential benefits of bringing together the P2P paradigm with the concept of hypermedia link services to implement ad-hoc resource sharing on the Web. This is accomplished by utilizing a web-based Distributed Dynamic Link Service (DDLS) as a testbed and addressing the issues arising from the design, implementation, and enhancement of the service. Our experimental result reveals the behavior and performance of the semantics-based resource discovery in DDLS and demonstrates that the proposed enhancing technique for DDLS, topology reorganization, is appropriate and efficient. © 2007 ACM.",Distributed dynamic link service; Open hypermedia; Peer-to-peer (P2P); Reorganization; Semantic search,Distributed computer systems; Hypermedia systems; Information services; Internet; Semantics; Distributed Dynamic Link Services (DDLS); Hypermedia link services; Semantic search; Topology reorganization; Web services
Requirements for scalable access control and security management architectures,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250159086&doi=10.1145%2f1239971.1239972&partnerID=40&md5=9911509dc81558df2dcd76b98264f074,"Maximizing local autonomy by delegating functionality to end nodes when possible (the end-to-end design principle) has led to a scalable Internet. Scalability and the capacity for distributed control have unfortunately not extended well to resource access-control policies and mechanisms. Yet management of security is becoming an increasingly challenging problem in no small part due to scaling up of measures such as number of users, protocols, applications, network elements, topological constraints, and functionality expectations. In this article, we discuss scalability challenges for traditional access-control mechanisms at the architectural level and present a set of fundamental requirements for authorization services in large-scale networks. We show why existing mechanisms fail to meet these requirements and investigate the current design options for a scalable access-control architecture. We argue that the key design options to achieve scalability are the choice of the representation of access control policy, the distribution mechanism for policy, and the choice of the access-rights revocation scheme. Although these ideas have been considered in the past, current access-control systems in use continue to use simpler but restrictive architectural models. With this article, we hope to influence the design of future access-control systems towards more decentralized and scalable mechanisms. © 2007 ACM.",Access control; Authorization; Credentials; Delegation; Distributed systems; Large-scale systems; Security policy; Trust management,Computer architecture; Distributed computer systems; Internet; Large scale systems; Mathematical models; Authorization; Delegation; Security policy; Trust management; Access control
Adapting Web information extraction knowledge via mining site-invariant and site-dependent features,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847732258&doi=10.1145%2f1189740.1189746&partnerID=40&md5=900fc61879dddcebba9a1089a72ea553,"We develop a novel framework that aims at automatically adapting previously learned information extraction knowledge from a source Web site to a new unseen target site in the same domain. Two kinds of features related to the text fragments from the Web documents are investigated. The first type of feature is called, a site-invariant feature. These features likely remain unchanged in Web pages from different sites in the same domain. The second type of feature is called a site-dependent feature. These features are different in the Web pages collected from different Web sites, while they are similar in the Web pages originating from the same site. In our framework, we derive the site-invariant features from previously learned extraction knowledge and the items previously collected or extracted from the source Web site. The derived site-invariant features will be exploited to automatically seek a new set of training examples in the new unseen target site. Both the site-dependent features and the site-invariant features of these automatically discovered training examples will be considered in the learning of new information extraction knowledge for the target site. We conducted extensive experiments on a set of real-world Web sites collected from three different domains to demonstrate the performance of our framework. For example, by just providing training examples from one online book catalog Web site, our approach can automatically extract information from ten different book catalog sites achieving an average precision and recall of 71.9% and 84.0% respectively without any further manual intervention. © 2007 ACM.",Machine learning; Text mining; Web mining; Wrapper adaptation,Data acquisition; Data mining; Learning systems; World Wide Web; Information extraction; Text mining; Web mining; Wrapper adaptation; Information analysis
Equipping smart devices with public key signatures,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847710745&doi=10.1145%2f1189740.1189743&partnerID=40&md5=2e883120c8bfae21f5603eb8f90bfb41,"One of the major recent trends in computing has been towards so-called smart devices, such as PDAs, cell phones and sensors. Such devices tend to have a feature in common: limited computational capabilities and equally limited power, as most operate on batteries. This makes them ill-suited for public key signatures. This article explores practical and conceptual implications of using Server-Aided Signatures (SAS) for these devices. SAS is a signature method that relies on partially-trusted servers for generating (normally expensive) public key signatures for regular users. Although the primary goal is to aid small, resource-limited devices in signature generation, SAS also offers fast certificate revocation, signature causality and reliable timestamping. It also has some interesting features such as built-in attack detection for users and DoS resistance for servers. Our experimental results also validate the feasibility of deploying SAS on smart devices. © 2007 ACM.",Digital signatures; Public key infrastructure,Computer crime; Electronic document identification systems; Personal digital assistants; Public key cryptography; Sensors; Servers; Built-in attack detection; Certificate revocation; Resource-limited devices; Server-Aided Signatures (SAS); Smart devices; Intelligent systems
Model-driven development of context-aware Web applications,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847727047&doi=10.1145%2f1189740.1189742&partnerID=40&md5=4c215b7519301b1dc25a31b2663b4c27,"Context-aware, multi-channel Web applications are more and more gaining consensus among both content providers and consumers, but very few proposals exist for their conceptual modeling. This article illustrates a conceptual framework that provides modeling facilities for context-aware, multichannel Web applications; it also shows how high-level modeling constructs can drive the application development process through automatic code generation. Our work stresses the importance of user-independent, context-triggered adaptation actions, in which the context plays the role of a first class actor, operating independently of users on the same hypertext the users navigate. Modeling concepts are based on WebML (Web Modeling Language), an already established conceptual model for data-intensive Web applications, which is also accompanied by a development method and a CASE tool. However, given their general validity, the concepts of this article shape up a complete framework that can be adopted independently of the chosen model, method, and tool. © 2007 ACM.",Adaptive hypertext; Conceptual modeling; Context; Context-aware Web applications; Context-awareness; WebML,Codes (symbols); Computer aided software engineering; Computer programming languages; Computer simulation; Information science; Mathematical models; Adaptive hypertext; Conceptual modeling; Context-aware Web applications; Context-awareness; WebML; World Wide Web
Defeating DDoS attacks by fixing the incentive chain,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847729500&doi=10.1145%2f1189740.1189745&partnerID=40&md5=37809891423b467c050c79d72aa3981b,"Cooperative technological solutions for Distributed Denial-of-Service (DDoS) attacks are already available, yet organizations in the best position to implement them lack incentive to do so, and the victims of DDoS attacks cannot find effective methods to motivate them. In this article we discuss two components of the technological solutions to DDoS attacks: cooperative filtering and cooperative traffic smoothing by caching. We then analyze the broken incentive chain in each of these technological solutions. As a remedy, we propose usage-based pricing and Capacity Provision Networks, which enable victims to disseminate enough incentive along attack paths to stimulate cooperation against DDoS attacks. © 2007 ACM.",Denial-of-service; Incentive; Pricing; Security,Cache memory; Distributed computer systems; Security of data; Capacity Provision Networks; Denial-of-service; Incentive; Computer crime
Provisioning servers in the application tier for E-commerce systems,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847736302&doi=10.1145%2f1189740.1189747&partnerID=40&md5=c1ac690b41d44e1640811fd050b531de,"Server providers that support e-commerce applications as a service for multiple e-commerce Web sites traditionally use a tiered server architecture. This architecture includes an application tier to process requests for dynamically generated content. How this tier is provisioned can significantly impact a provider's profit margin. In this article we study methods to provision servers in the application serving tier that increase a server provider's profits. First, we examine actual traces of request arrivals to the application tier of an e-commerce site, and show that the arrival process is effectively Poisson. Next, we construct an optimization problem in the context of a set of application servers modeled as M/G/1/PS queueing systems, and derive three simple methods that approximate the allocation that maximizes profits. Simulation results demonstrate that our approximation methods achieve profits that are close to optimal, and are significantly higher than those achieved via simple heuristics. © 2007 ACM.",Eletronic commerce; Server provisioning,Computer simulation; Heuristic methods; Optimization; Queueing networks; Servers; Websites; Queueing systems; Server provisioning; Electronic commerce
Internet content filtering using isotonic separation on content category ratings,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847717905&doi=10.1145%2f1189740.1189741&partnerID=40&md5=46e4d74c362540c225a74d87dc1ab9f2,"The World Wide Web has enabled anybody with a low cost Internet connection to access vast information repositories. Some of these repositories contain information (e.g., hate speech and pornography) that is considered objectionable, especially for children to view. Several efforts - -legal and technical - -are underway to protect children and the generic public from accessing this type of content. We propose a technical approach utilizing a recently proposed technique called isotonic separation for filtering with content metadata if they satisfy monotone conditions. We illustrate this approach using a category rating method of PICS. In essence, we formulate the Internet content filtering problem as a classification problem on content metadata and report on experiments we conducted with the isotonic separation technique. © 2007 ACM.",Internet content filtering; Isotonic separation; PICS,Information systems; Metadata; Problem solving; World Wide Web; Classification problem; Internet content filtering; Isotonic separation; PICS; Internet
The Web as a graph: How far we are,2007,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847706952&doi=10.1145%2f1189740.1189744&partnerID=40&md5=4fb9c9c27b623dec4279bdd8ba1d9154,"In this article we present an experimental study of the properties of webgraphs. We study a large crawl from 2001 of 200M pages and about 1.4 billion edges, made available by the WebBase project at Stanford, as well as several synthetic ones generated according to various models proposed recently. We investigate several topological properties of such graphs, including the number of bipartite cores and strongly connected components, the distribution of degrees and PageRank values and some correlations; we present a comparison study of the models against these measures.Our findings are that (i) the WebBase sample differs slightly from the (older) samples studied in the literature, and (ii) despite the fact that these models do not catch all of its properties, they do exhibit some peculiar behaviors not found, for example, in the models from classical random graph theory.Moreover we developed a software library able to generate and measure massive graphs in secondary memory; this library is publicy available under the GPL licence. We discuss its implementation and some computational issues related to secondary memory graph algorithms. © 2007 ACM.",Graph structure; Models; World-wide-Web,Algorithms; Computational methods; Graph theory; Mathematical models; Graph structure; Secondary memory; Webgraphs; World Wide Web
Automated gathering of Web information: An in-depth examination of agents interacting with search engines,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845463355&doi=10.1145%2f1183463.1183468&partnerID=40&md5=51e99399adfc81c6e3f2581cdb0b5a00,"The Web has become a worldwide repository of information which individuals, companies, and organizations utilize to solve or address various information problems. Many of these Web users utilize automated agents to gather this information for them. Some assume that this approach represents a more sophisticated method of searching. However, there is little research investigating how Web agents search for online information. In this research, we first provide a classification for information agent using stages of information gathering, gathering approaches, and agent architecture. We then examine an implementation of one of the resulting classifications in detail, investigating how agents search for information on Web search engines, including the session, query, term, duration and frequency of interactions. For this temporal study, we analyzed three data sets of queries and page views from agents interacting with the Excite and AltaVista search engines from 1997 to 2002, examining approximately 900,000 queries submitted by over 3,000 agents. Findings include: (1) agent sessions are extremely interactive, with sometimes hundreds of interactions per second (2) agent queries are comparable to human searchers, with little use of query operators, (3) Web agents are searching for a relatively limited variety of information, wherein only 18% of the terms used are unique, and (4) the duration of agent-Web search engine interaction typically spans several hours. We discuss the implications for Web information agents and search engines. © 2006 ACM.",Agent searching; Search engines; Web searching,Computer architecture; Database systems; Information retrieval; Query languages; Search engines; World Wide Web; Agent searching; Web searching; Web users; Information technology
Inferring binary trust relationships in Web-based social networks,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845467053&doi=10.1145%2f1183463.1183470&partnerID=40&md5=c7abc7fabdbda760ee88e76cf1952f93,"The growth of Web-based social networking and the properties of those networks have created great potential for producing intelligent software that integrates a user's social network and preferences. Our research looks particularly at assigning trust in Web-based social networks and investigates how trust information can be mined and integrated into applications. This article introduces a definition of trust suitable for use in Web-based social networks with a discussion of the properties that will influence its use in computation. We then present two algorithms for inferring trust relationships between individuals that are not directly connected in the network. Both algorithms are shown theoretically and through simulation to produce calculated trust values that are highly accurate.. We then present TrustMail, a prototype email client that uses variations on these algorithms to score email messages in the user's inbox based on the user's participation and ratings in a trust network. © 2006 ACM.",Online communities; Semantic Web; Small worlds; Social networks; Trust,Algorithms; Computer software; Intelligent agents; Online systems; Semantics; Online communities; Semantic Web; Social networks; World Wide Web
A mobile computing middleware for location- and context-aware internet data services,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845406318&doi=10.1145%2f1183463.1183465&partnerID=40&md5=cfbf06127d11b5b5d95381340cd70b9d,"The widespread diffusion of mobile computing calls for novel services capable of providing results that depend on both the current physical position of users (location) and the logical set of accessible resources, subscribed services, preferences, and requirements (context). Leaving the burden of location/context management to applications complicates service design and development. In addition, traditional middleware solutions tend to hide location/context visibility to the application level and are not suitable for supporting novel adaptive services for mobile computing scenarios. The article proposes a flexible middleware for the development and deployment of location/context-aware services for heterogeneous data access in the Internet. A primary design choice is to exploit a high-level policy framework to simplify the specification of services that the middleware dynamically adapts to the client location/context. In addition, the middleware adopts the mobile agent technology to effectively support autonomous, asynchronous, and local access to data resources, and is particularly suitable for temporarily disconnected clients. The article also presents the case study of a museum guide assistant service that provides visitors with location/context-dependent artistic data. The case study points out the flexibility and usability of the proposed middleware that permits automatic service reconfiguration with no impact on the implementation of the application logic. © 2006 ACM.",Adaptive services; Context awareness; Location awareness; Middleware; Mobile agents; Mobile computing; Policies,Data structures; Database systems; Internet; Middleware; Adaptive services; Context awareness; Location awareness; Mobile agents; Mobile computing
Computing customized page ranks,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845449121&doi=10.1145%2f1183463.1183466&partnerID=40&md5=dd7b1b05a9be71f9e6035ccb7080d616,"In this article, we present a new approach to page ranking. The page rank of a collection of Web pages can be represented in a parameterized model, and the user requirements can be represented by a set of constraints. For a particular parameterization, namely, a linear combination of the page ranks produced by different forcing functions, and user requirements represented by a set of linear constraints, the problem can be solved using a quadratic programming method. The solution to this problem produces a set of parameters which can be used for ranking all pages in the Web. We show that the method is suitable for building customized versions of PageRank which can be readily adapted to the needs of a vertical search engine or that of a single user. © 2006 ACM.",Interface personalization; PageRank; Search engines; Web page scoring systems,Constraint theory; Interfaces (computer); Mathematical models; Quadratic programming; Search engines; Customized versions; Parameterized models; Web page scoring systems; Websites
A heuristic bidding strategy for buying multiple goods in multiple English auctions,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745601981&doi=10.1145%2f1183463.1183469&partnerID=40&md5=8ca43f0a178b779eff88fdecdabd4427,"This paper presents the design, implementation, and evaluation of a novel bidding algorithm that a software agent can use to obtain multiple goods from multiple overlapping English auctions. Specifically, an Earliest Closest First heuristic algorithm is proposed that uses neurofuzzy techniques to predict the expected closing prices of the auctions and to adapt the agent's bidding strategy to reflect the type of environment in which it is situated. This algorithm first identifies the set of auctions that are most likely to give the agent the best return and then, according to its attitude to risk, it bids in some other auctions that have approximately similar expected returns, but which finish earlier than those in the best return set. We show through empirical evaluation against a number of methods proposed in the multiple auction literature that our bidding strategy performs effectively and robustly in a wide range of scenarios. © 2006 ACM.",Bidding strategy; e-commerce; Intelligent agents; Multiple English auctions; Online auctions,Algorithms; Heuristic programming; Intelligent agents; Online systems; Bidding strategy; Multiple English auctions; Online auctions; Electronic commerce
Searching for experts on the Web: A review of contemporary expertise locator systems,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745357081&doi=10.1145%2f1183463.1183464&partnerID=40&md5=d163e60cb27237a49d84da66d10182e1,"This article presents the role of ontologies and Web mining techniques in the construction and maintenance of experts' profiles. This article also discusses the development of contemporary expertise-locator knowledge management systems and, specifically, the implementation details of two such systems: the Searchable Answer Generating Engine (SAGE), and Expert Seeker. SAGE is a Web-based expertise locator system, which searches for researchers in universities in Florida based on specified criteria, including expertise in a specific domain. Expert Seeker's purpose is to search for experts in one of the best-known knowledge organizations, the National Aeronautics and Space Administration. Implementation details, results to date, and future plans for these systems are also presented. © 2006 ACM.",Expert finders; Expert recommenders; Expertise management; Expertise modeling; Expertise-locator systems; Information retrieval; Knowledge management systems; Web content mining; Web mining,Computer simulation; Data mining; Data structures; Information retrieval; Knowledge based systems; Mathematical models; Expert finders; Expert recommenders; Expertise management; Expertise modeling; Expertise-locator systems; Knowledge management systems; Web content mining; World Wide Web
Online information disclosure: Motivators and measurements,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845388608&doi=10.1145%2f1183463.1183467&partnerID=40&md5=c1e9301692376ea011ce6722f7a83af6,"To increase their revenue from electronic commerce, more and more Internet businesses are soliciting personal information from consumers in order to target products and services at the right consumers. But when deciding whether to disclose their personal information to Internet businesses, consumers may weigh the concerns of giving up information privacy against the benefits of information disclosure. This article examines how Internet businesses can motivate consumers to disclose their personal information. Based on a synthesis of the literature, the article identifies seven types of extrinsic or intrinsic benefits that Internet businesses can provide when soliciting personal information from consumers. Through comprehensive conceptual and empirical validation processes, the article develops an instrument that allows Internet businesses to gauge the preference of consumers for the various types of benefits. By testing a set of nomological networks, some ideas are presented to Internet businesses about what types of benefits may be more effective given the personality traits of particular consumer populations. Besides providing a foundation for efforts aimed at developing theories on information, privacy and information disclosure, the results of this research provide useful suggestions to Internet businesses on how best to solicit personal information from consumers. Implications for research and practical application are discussed. © 2006 ACM.",Information disclosure; Information privacy; Internet business; Measurement; Motivators; Personality,Data privacy; Information technology; Internet; Online systems; Information disclosure; Information privacy; Internet business; Electronic commerce
PageRank revisited,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747876638&doi=10.1145%2f1151087.1151090&partnerID=40&md5=f6cf6418387b588957f92a348cf242ee,"PageRank, one part of the search engine Google, is one of the most prominent link-based rankings of documents in the World Wide Web. Usually it is described as a Markov chain modeling a specific random surfer. In this article, an alternative representation as a power series is given. Nonetheless, it is possible to interpret the values as probabilities in a random surfer setting, differing from the usual one. Using the new description we restate and extend some results concerning the convergence of the standard iteration used for PageRank. Furthermore we take a closer look at sinks and sources, leading to some suggestions for faster implementations. © 2006 ACM.",Dynamical update; Link-analysis; Markov chain; Pagerank; Personalization; Random surfer; Ranking algorithm; Web graph; Web page scoring; Web search; World Wide Web,Algorithms; Graph theory; Information retrieval systems; Markov processes; Online searching; Search engines; Dynamical update; Pagerank; Personalization; Random surfer; Ranking algorithm; Web page scoring; Web search; Websites
A wide-area distribution network for free software,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747890456&doi=10.1145%2f1151087.1151089&partnerID=40&md5=9d0f9626f82f46d615c282906c7254d1,"The Globe Distribution Network (GDN) is an application for the efficient, worldwide distribution of freely redistributable software packages. Distribution is made efficient by encapsulating the software into special distributed objects which efficiently replicate themselves near to the downloading clients. The Globe Distribution Network takes a novel, optimistic approach to stop the illegal distribution of copyrighted and illicit material via the network. Instead of having moderators check the packages at upload time, illegal content is removed and its uploader's access to the network permanently revoked only when the violation is discovered. Other protective measures defend the GDN against internal and external attacks to its availability. By exploiting the replication of the software and using fault-tolerant server software, the Globe Distribution Network achieves high availability. A prototype implementation of the GDN is available from http://www.es.vu.nl/globe/. © 2006 ACM.",Copyright; Distributed objects; File sharing; Middleware; Software distribution; Traceable content; Wide-area networks,Computer software; Copyrights; Distributed computer systems; Middleware; Software engineering; Distributed objects; File sharing; Globe Distribution Network; Traceable content; Wide area networks
A stochastic model for the evolution of the Web allowing link deletion,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21744438723&doi=10.1145%2f1149121.1149122&partnerID=40&md5=f0c4a70ed4e0265eb6335c13d7a6c6e2,"Recently several authors have proposed stochastic evolutionary models for the growth of the Web graph and other networks that give rise to power-law distributions. These models are based on the notion of preferential attachment, leading to the ""rich get richer"" phenomenon. We present a generalization of the basic model by allowing deletion of individual links and show that it also gives rise to a power-law distribution. We derive the mean-field equations for this stochastic model and show that, by examining a snapshot of the distribution at the steady state of the model, we are able to determine the extent to which link deletion has taken place and estimate the probability of deleting a link. Applying our model to actual Web graph data provides evidence of the extent to which link deletion has occurred. We also discuss a problem that frequently arises in estimating the power-law exponent from empirical data and a few possible methods for dealing with this, indicating our preferred approach. Using this approach our analysis of the data suggests a power-law exponent of approximately 2.15 for the distribution of inlinks in the Web graph, rather than the widely published value of 2.1. © 2006 ACM.",Evolutionary models; Link deletion; Power-law distribution; Preferential attachment; Web graph,Approximation theory; Evolutionary algorithms; Graph theory; Mathematical models; Probability; World Wide Web; Evolutionary models; Link deletion; Power-law distribution; Preferential attachment; Web graph; Random processes
Stanford WebBase components and applications,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747096982&doi=10.1145%2f1149121.1149124&partnerID=40&md5=246ce46c85fb0a216b6adb6d4bd5458b,"We describe the design and performance of WebBase, a tool for Web research. The system includes a highly customizable crawler, a repository for collected Web pages, an indexer for both text and link-related page features, and a high-speed content distribution facility. The distribution module enables researchers world-wide to retrieve pages from WebBase, and stream them across the Internet at high speed. The advantage for the researchers is that they need not all crawl the Web before beginning their research. WebBase has been used by scores of research and teaching organizations world-wide, mostly for investigations into Web topology and linguistic content analysis. After describing the system's architecture, we explain our engineering decisions for each of the WebBase components, and present respective performance measurements. © 2006 ACM.",Distribution; Hyperlink indexing; Site crawling; WebBase Web crawler,Database systems; Decision theory; Information retrieval; Linguistics; Distribution; Hyperlink indexing; Linguistic content analysis; Site crawling; WebBase Web crawlers; Websites
Behavior-based modeling and its application to Email analysis,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747144912&doi=10.1145%2f1149121.1149125&partnerID=40&md5=02ef248607e2856e7619c5a611c6289a,"The Email Mining Toolkit (EMT) is a data mining system that computes behavior profiles or models of user email accounts. These models may be used for a multitude of tasks including forensic analyses and detection tasks of value to law enforcement and intelligence agencies, as well for as other typical tasks such as virus and spam detection. To demonstrate the power of the methods, we focus on the application of these models to detect the early onset of a viral propagation without ""content-based"" (or signature-based) analysis in common use in virus scanners. We present several experiments using real email from 15 users with injected simulated viral emails and describe how the combination of different behavior models improves overall detection rates. The performance results vary depending upon parameter settings, approaching 99% true positive (TP) (percentage of viral emails caught) in general cases and with 0.38% false positive (FP) (percentage of emails with attachments that are mislabeled as viral). The models used for this study are based upon volume and velocity statistics of a user's email rate and an analysis of the user's (social) cliques revealed in the person's email behavior. We show by way of simulation that virus propagations are detectable since viruses may emit emails at rates different than human behavior suggests is normal, and email is directed to groups of recipients in ways that violate the users' typical communications with their social groups. © 2006 ACM.",Anomaly detection; Behavior profiling; Email virus propagations,Computer viruses; Electronic communities; Electronic mail; Intelligent agents; Law enforcement; Spamming; User interfaces; Anomaly detection; Behavior profiling; Email virus propagations; User's (social) cliques; Data mining
Core algorithms in the CLEVER system,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747117346&doi=10.1145%2f1149121.1149123&partnerID=40&md5=0daf81147df96a6ae334acaae800ac8b,"This article describes the CLEVER search system developed at the IBM Almaden Research Center. We present a detailed and unified exposition of the various algorithmic components that make up the system, and then present results from two user studies. © 2006 ACM.",Anchortext; Hyperlinks; Linear algebra; Link analysis; Web search,Graph theory; Linear algebra; Search engines; World Wide Web; Anchortext; Hyperlinks; Link analysis; Web search; Algorithms
Performance and overhead of semantic cache management,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747875873&doi=10.1145%2f1151087.1151091&partnerID=40&md5=a9db1f00d9b5de2a13b6c682696b187c,"The emergence of query-based online data services and e-commerce applications has prompted much recent research on data caching. This article studies semantic caching, a caching architecture for such applications, that caches the results of selection queries. The primary contribution of this article is to revisit the performance and overhead of semantic caching using a modern database server and modern hardware. Initially, the performance study focuses on simple workloads and demonstrates several benefits of semantic caching, including low overhead, insensitivity to the physical layout of the database, reduced network traffic, and the ability to answer some queries without contacting the server. With moderately complex workloads, careful coding of remainder queries is required to maintain efficient query processing at the server. Using very complex workloads, we demonstrate that semantic caching works well in a range of applications, especially in network-constrained environments. © 2006 ACM.",Data caching; e-commerce applications; Online data services; Semantic caching; Wireless networks,Computer architecture; Database systems; Electronic commerce; Information management; Semantics; Servers; Wireless telecommunication systems; Data caching; e-commerce applications; Online data services; Semantic cache management; Wireless networks; Buffer storage
"A compressor for effective archiving, retrieval, and updating of XML documents",2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747884624&doi=10.1145%2f1151087.1151088&partnerID=40&md5=8d859aca751ae036a1cfc384fc00916f,"Like HTML, many XML documents are resident on native file systems. Since XML data is irregular and verbose, the disk space and the network bandwidth are wasted. To overcome the verbosity problem, research on compressors for XML data has been conducted. Some XML compressors do not support querying compressed data, while other XML compressors which support querying compressed data blindly encode tags and data values using predefined encoding methods. Existing XML compressors do not provide the facility for updates on compressed XML data. In this article, we propose XPRESS, an XML compressor which supports direct updates and efficient evaluations of queries on compressed XML data. XPRESS adopts a novel encoding method called reverse arithmetic encoding, which encodes label paths of XML data and applies diverse encoding methods depending on the types of data values. Experimental results with real-life data sets show that XPRESS achieves significant improvements on query performance for compressed XML data and reasonable compression ratios. On average, the query performance of XPRESS is 2.13 times better than that of an existing XML compressor, and the compression ratio of XPRESS is about 71%. Additionally, we demonstrate the efficiency of the updates performed directly on compressed XML data. © 2006 ACM.",,Bandwidth; Data compression; Encoding (symbols); Information retrieval; Information retrieval systems; Problem solving; Query languages; Arithmetic encoding; Compression ratio; Query performance; XML compressors; XML
Effective web browsing through content delivery adaptation,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745268190&doi=10.1145%2f1111627.1111628&partnerID=40&md5=96e458942f3059cba943e45ac7ecc724,"This article presents a Web content adaptation and delivery mechanism based on application-level quality of service (QoS) policies. To realize effective Web content delivery for users, two kinds of application-level QoS policies, transmission time and transmission order of inline objects, are introduced. Next, we define a language to specify these policies. We show that transmission order control can be implemented using HTTP/1.1 pipelined requests in which a client recognizes the transmission order description in a Web page and simulates parallel transmission of inline objects by HTTP/1.1 range requests. Experimental results show that our proposed mechanism realizes effective content delivery to a diverse group of Internet users. Finally, we introduce two methods to specify application-level QoS policies, one by content authors, and the other by end users. © 2005 ACM.",Content adaptation; Hypertext; World Wide Web,Client server computer systems; Computer simulation; Content based retrieval; Data communication systems; HTTP; Quality of service; Content adaptation; Hypertext; Inline objects; Transmission time; Web browsers
Portlet syndication: Raising variability concerns,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745259007&doi=10.1145%2f1111627.1111630&partnerID=40&md5=35f6022743171e29d852eed3329e77ab,"A Portlet is a multistep, user-facing application delivered through a Web application (e.g., a portal). OASIS approved standard, WSRP, is an attempt to standardize the interface between the provider and the consumer of the Portlet. This initiative promotes Portlet interoperability, componentware practices, and the existence of a Portlet market. This work argues that the diversity of the settings where a Portlet might be syndicated recommends that Portlets be instrumented for variability, and this, in turn, demands a product-line approach. This work introduces a new source of variability, the ""interaction lifecycle"", a description of the visible flow of a Portlet, and shows how this feature can be adapted to cater to the idiosyncrasies of the hosting application. Distinct variants are identified that permit the consumer to customize the presentation, content, and links of the Portlet markup in a controlled way. The use of product-line techniques allow the consumer to cope with this variability in a cost-effective manner. The article ends by illustrating how the extensible capabilities of WSRP are used to accomodate this process. © 2005 ACM.",Portal applications; Portlet; Product lines,Computer applications; Cost effectiveness; Interoperability; Portals; World Wide Web; Portal applications; Portlet; Product lines; User interfaces
IDN server proxy architecture for internationalized domain name resolution and experiences with providing web services,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745229240&doi=10.1145%2f1125274.1125275&partnerID=40&md5=7adcff4a475140233de8cec3312b4007,"The composition of traditional domain names are restricted to ASCII letters, digits, and hyphens (abbreviated as LDH). This makes it difficult for many to use their native language to name and access their Internet hosts. The IETF IDN (Internationalized Domain Name) Working Group proposes a mechanism, IDNA (Internationalizing Domain Names in Applications), for internationalized access to multilingual domain names. The proposal uses a preparation process that converts a Unicode IDN into an ACE (ASCII Compatible Encoding) string that uses only LDH. Thus, applications can look up the ACE string by using the existing DNS infrastructure. However, some of the domain name strings embedded in multilingual content do not have any charset tag so they cannot appropriately be converted into ACE strings. We noticed that many Internet applications allow users to use non-ASCII domain names. We were motivated to design an architecture for IDN resolution as well as to minimize the cost of modifying legacy Internet applications. We specifically focus on designing an IDN server proxy, which is located on the domain name server side, to handle domain names in multiple encodings. In this article, we study several architecture design issues including detection of charset encoding, routing of non-ACE IDN lookup requests, and so on. With respect to these design issues, we present an IDN server proxy architecture which stores ACE IDNs in domain name servers. Note that traditional domain name servers can be used without modification. An IDN server proxy, called Octopus, is employed on the domain name server side. Octopus converts a non-ACE IDN string into ACE upon receiving an IDN lookup request from remote users or autonomous systems. The ACE string is then forwarded to backend domain name servers (where the traditional domain names and ACE IDNs are stored) for further processing. This allows Internet users to access IDNs without having to upgrade their software. Based on the design and implementation of Octopus, we deployed a CDN (Chinese domain name) trial in July 2002. In this article, we present the results of testing Octopus IDN lookup functions as well as our experiences in providing CDN Web services. Several types of errors can occur if applications are unable to handle IDNs adequately. For example, a Web browser may erroneously parse an IDN within a URL. Many legacy Web servers are unable to process the IDN of a virtual host. Web application servers may have trouble completing some actions such as redirecting Web pages to alternative Web pages. Our studies help service providers understand potential problems when non-ASCII domain names are used and the best common practice at this stage. As well, the experiences give some guidance for software developers to develop IDNA-compliant Internet applications. © 2006 ACM.","Application server, multilingual; DNS server proxy; Internationalized domain name","Computer architecture; Formal languages; Internet; Optical resolving power; Signal encoding; World Wide Web; Application server, multilingual; DNS server proxy; Internationalized domain name; Servers"
Web servers under overload: How scheduling can help,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745251650&doi=10.1145%2f1125274.1125276&partnerID=40&md5=529275f93f81d517ad045e437bf3fb99,"This article provides a detailed implementation study on the behavior of web serves that serve static requests where the load fluctuates over time (transient overload). Various external factors are considered, including WAN delays and losses and different client behavior models. We find that performance can be dramatically improved via a kernel-level modification to the web server to change the scheduling policy at the server from the standard FAIR (processor-sharing) scheduling to SRPT (shortest-remaining-processing-time) scheduling. We find that SRPT scheduling induces no penalties. In particular, throughput is not sacrificed and requests for long files experience only negligibly higher response times under SRPT than they did under the original FAIR scheduling. © 2006 ACM.",Overload; Scheduling; SRPT; Starvation; Unfairness; Web server,Mathematical models; Response time (computer systems); Scheduling; Wide area networks; World Wide Web; Overload; SRPT; Starvation; Unfairness; Web server; Servers
Supporting complex queries on multiversion XML documents,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745267046&doi=10.1145%2f1125274.1125277&partnerID=40&md5=6efc5180697d5dd4bd58d5582f5b8f2b,"Managing multiple versions of XML documents represents a critical requirement for many applications. Recently, there has been much work on supporting complex queries on XML data (e.g., regular path expressions, structural projections, etc.). In this article, we examine the problem of implementing efficiently such complex queries on multiversion XML documents. Our approach relies on a numbering scheme, whereby durable node numbers (DNNs) are used to preserve the order among the nodes of the XML tree while remaining invariant with respect to updates. Using the document's DNNs, we show that many complex queries are reduced to combinations of range version retrieval queries. We thus examine three alternative storage organizations/indexing schemes to efficiently evaluate range version retrieval queries in this environment. A thorough performance analysis is then presented to reveal the advantages of each scheme. © 2006 ACM.",Multiversion; Query support; Version retrieval; XML document,Computer applications; Indexing (of information); Random number generation; Trees (mathematics); XML; Multiversion; Query support; Version retrieval; XML document; Query languages
Characteristics of streaming media stored on the web,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745229699&doi=10.1145%2f1111627.1111629&partnerID=40&md5=3be8c6635513ff60daf35c0895b461bb,"Despite the growth in multimedia, there have been few studies that focus on characterizing streaming audio and video stored on the Web. This investigation used a customized Web crawler to traverse 17 million Web pages from diverse geographic locations and identify nearly 30,000 streaming audio and video clips available for analysis. Using custom-built extraction tools, these streaming media objects were analyzed to determine attributes such as media type, encoding format, playout duration, bitrate, resolution, and codec. The streaming media content encountered is dominated by proprietary audio and video formats with the top four commercial products being RealPlayer, Windows Media Player, MP3 and QuickTime. The distribution of the stored playout durations of streaming audio and video clips are long-tailed. More than half of the streaming media clips encountered are video, encoded primarily for broadband connections and at resolutions considerably smaller than the resolutions of typical monitors. © 2005 ACM.",Apple QuickTime; Long-tailed; Microsoft Windows Media Player; Multimedia; RealNetworks RealPlayer; Self-similarity; Streaming,Bit error rate; Broadband networks; Data storage equipment; Feature extraction; Geographic information systems; World Wide Web; Apple QuickTime; Long-tailed; Microsoft Windows Media Player; RealNetworks RealPlayer; Self-similarity; Streaming; Multimedia systems
Design and implementation of the web-enabled nist design repository,2006,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745250485&doi=10.1145%2f1125274.1125278&partnerID=40&md5=f7fdfb894cee2aa9538408a962e989a3,"This article describes the design and development of a design repository software system. This system is a prototype implementation intended to demonstrate the role of design repositories as part of a vision for the next generation of product development software systems. This research involves not only the creation of a prototype software system, but is part of a broader effort that also includes the development of a core product knowledge representation and that seeks to address terminological and semantic issues associated with computer-aided product development. This article focuses on the interfaces that have been developed to support authoring and navigation of the product models stored in design repositories as well as the software architecture and associated rationale that provide the framework on which the system is built. © 2006 ACM.",Management,Computer aided design; Interfaces (computer); Product development; Semantics; Software prototyping; Terminology; Design repository software; Product models; Prototype software; Software engineering
Taxonomy of XML schema languages using formal language theory,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644699458&doi=10.1145%2f1111627.1111631&partnerID=40&md5=3232299b8b4c5ced19f6c33247fca5ca,"On the basis of regular tree grammars, we present a formal framework for XML schema languages. This framework helps to describe, compare, and implement such schema languages in a rigorous manner. Our main results are as follows: (1) a simple framework to study three classes of tree languages (local, single-type, and regular); (2) classification and comparison of schema languages (DTD, W3C XML Schema, and RELAX NG) based on these classes; (3) efficient document validation algorithms for these classes; and (4) other grammatical concepts and advanced validation algorithms relevant to an XML model (e.g., binarization, derivative-based validation). © 2005 ACM.",Interpretation; Schema; Tree automaton; Validation; XML,Algorithms; Classification (of information); Computational grammars; Formal languages; Mathematical models; Trees (mathematics); Interpretation; Schema; Tree automaton; Validation; XML
Characterizing a national community Web,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27144499527&doi=10.1145%2f1084772.1084775&partnerID=40&md5=a28959c6dd2fc515e2a07f1e61128207,"This article presents a characterization of the community Web of the people of Portugal. We defined criteria for delimiting this Web based on our past experience of crawling pages related to Portugal and collected over 3.2 million documents from 46,000 sites satisfying those criteria. Our characterization was derived from this crawl. We describe the rules that we established for defining the boundaries of this community Web and the methodology used to gather statistics. Statistics cover the number and domain distribution of sites; the number, type and size distribution of text documents; and the linkage structure of this Web. We also show how crawling constraints and abnormal situations on the Web can influence the statistics. © 2005 ACM.",Portuguese Web; Web characterization; Web communities; Web measurements,Constraint theory; Information retrieval systems; Logic programming; Number theory; Statistical methods; Portuguese web; Web characterization; Web communities; Web measurements; World Wide Web
Optimal methods for coordinated enroute Web caching for tree networks,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27144472748&doi=10.1145%2f1084772.1084774&partnerID=40&md5=4aca1ec4449565348cfd7b608c8fe04f,"Web caching is an important technology for improving the scalability of Web services. One of the key problems in coordinated enroute Web caching is to compute the locations for storing copies of an object among the enroute caches so that some specified objectives are achieved. In this article, we address this problem for tree networks, and formulate it as a maximization problem. We consider this problem for both unconstrained and constrained cases. The constrained case includes constraints on the cost gain per node and on the number of object copies to be placed. We present dynamic programming-based solutions to this problem for different cases and theoretically show that the solutions are either optimal or convergent to optimal solutions. We derive efficient algorithms that produce these solutions. Based on our mathematical model, we also present a solution to coordinated enroute Web caching for autonomous systems as a natural extension of the solution for tree networks. We implement our algorithms and evaluate our model on different performance metrics through extensive simulation experiments. The implementation results show that our methods outperform the existing algorithms of either coordinated enroute Web caching for linear topology or object placement (replacement) at individual nodes only. © 2005 ACM.",Autonomous system (AS); Dynamic programming; Object placement (replacement); Performance evaluation; Tree network; Web caching,Algorithms; Buffer storage; Computational methods; Computer simulation; Constraint theory; Dynamic programming; Mathematical models; Problem solving; Trees (mathematics); Autonomous system (AS); Object placement (replacement); Performance evaluation; Tree network; Web caching; World Wide Web
"Erratum: ""Graphical Query Interfaces for Semistructured Data: The QURSED System"" (ACM Transcations on Internet Technology (2005) 5:2 (390-438))",2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27144468056&doi=10.1145%2f1084772.1084777&partnerID=40&md5=5aa71a531d713089e37557bbff994681,[No abstract available],,
Flash crowd mitigation via adaptive admission control based on application-level observations,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27144457682&doi=10.1145%2f1084772.1084776&partnerID=40&md5=0744891aaaf0c2cec209cd3ff239740b,"We design an adaptive admission control mechanism, network early warning system (NEWS), to protect servers and networks from flash crowds and maintain high performance for end-users. NEWS detects flash crowds from performance degradation in responses and mitigates flash crowds by admitting incoming requests adaptively. We evaluate NEWS performance with both simulations and testbed experiments. We first investigate a network-limited scenarion in simulations. We find that NEWS detects flash crowds within 20 seconds. By discarding 32% of incoming requests, NEWS protects the target server and networks from overloading, reducing the response packet drop rate from 25% to 2%. For admitted requests, NEWS increases their response rate by two times. This performance is similar to the best static rate limiter deployed in the same scenario. We also investigate the impact of detection intervals on NEWS performance, showing it affects both detection delay and false alarm rate. We further consider a server memory-limited scenario in testbed experiments, confirming that NEWS is also effective in this case. We also examine the runtime cost of NEWS traffic monitoring in practice and find that it consumes little CPU time and relatively small memory. Finally, we show NEWS effectively protects bystander traffic from flash crowds. © 2005 ACM.",Admission control; Experimentation with testbeds; Flash crowds; Simulations,Alarm systems; Computer simulation; Costs; Servers; Admission control; Experimentation with testbeds; Flash crowds; Network early warning systems (NEWS); Flash memory
Model-driven design and deployment of service-enabled web applications,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27144478032&doi=10.1145%2f1084772.1084773&partnerID=40&md5=0541611b9594f659debaa27da94ccb33,"Significant effort is currently invested in application integration, enabling business processes of different companies to interact and form complex multiparty processes. Web service standards, based on WSDL (Web Service Definition Language), have been adopted as process-to-process communication paradigms. However, the conceptual modeling of applications using Web services has not yet been addressed. Interaction with Web services is often specified at the level of the source code; thus, Web service interfaces are buried within a programmatic specification. In this article, we argue that Web services should be considered first-class citizens in the specification of Web applications. Thus, service-enabled Web applications should benefit from the high-level modeling and automatic code generation techniques that have long been advocated for Web application design and implementation. To this end, we extend a declarative model for specifying data-intensive Web applications in two directions: (i) high-level modeling of Web services and their interactions with the Web applications which use them, and (ii) modeling and specification of Web applications implementing new, complex Web services. Our approach is fully implemented within a CASE tool allowing the high-level modeling and automatic deployment of service-enabled Web applications. © 2005 ACM.",Modeling; UML; Web application; Web services; WebML,Computer aided software engineering; Computer simulation; Industrial management; Integration; Interfaces (computer); Mathematical models; Modeling; UML; Web application; Web services; WebML; World Wide Web
"Moderately hard, memory-bound functions",2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21644461880&doi=10.1145%2f1064340.1064341&partnerID=40&md5=ddb09a25aae2ddc71d522f8550319058,"A resource may be abused if its users incur little or no cost. For example, e-mail abuse is rampant because sending an e-mail has negligible cost for the sender. It has been suggested that such abuse may be discouraged by introducing an artificial cost in the form of a moderately expensive computation. Thus, the sender of an e-mail might be required to pay by computing for a few seconds before the e-mail is accepted. Unfortunately, because of sharp disparities across computer systems, this approach may be ineffective against malicious users with high-end systems, prohibitively slow for legitimate users with low-end systems, or both. Starting from this observation, we research moderately hard functions that most recent systems will evaluate at about the same speed. For this purpose, we rely on memory-bound computations. We describe and analyze a family of moderately hard, memory-bound functions, and we explain how to use them for protecting against abuses. © 2005 ACM.",Spam,Algorithms; Computer simulation; Computer systems; Costs; Data storage equipment; Electronic mail; Security of data; Telecommunication networks; Telecommunication traffic; Computer-communication networks; Hash functions; Memory latency; Memory-bound computations; Spamming
A fragment-based approach for efficiently creating dynamic Web content,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21644458123&doi=10.1145%2f1064340.1064343&partnerID=40&md5=678fe6dcecc1d0a2351c4dd20ad0c471,"This article presents a publishing system for efficiently creating dynamic Web content. Complex Web pages are constructed from simpler fragments. Fragments may recursively embed other fragments. Relationships between Web pages and fragments are represented by object dependence graphs. We present algorithms for efficiently detecting and updating Web pages affected after one or more fragments change. We also present algorithms for publishing sets of Web pages consistently; different algorithms are used depending upon the consistency requirements. Our publishing system provides an easy method for Web site designers to specify and modify inclusion relationships among Web pages and fragments. Users can update content on multiple Web pages by modifying a template. The system then automatically updates all Web pages affected by the change. Our system accommodates both content that must be proofread before publication and is typically from humans as well as content that has to be published immediately and is typically from automated feeds. We discuss some of our experiences with real deployments of our system as well as its performance. We also quantitatively present characteristics of fragments used at a major deployment of our publishing system including fragment sizes, update frequencies, and inclusion relationships. © 2005 ACM.",Caching; Dynamic content; Fragments; Publishing; Web; Web performance,Algorithms; Cache memory; Computer aided design; Copyrights; Display devices; Electronic publishing; Information theory; Caching; Dynamic content; Fragments; Web performance; Portals
Supporting application development in the Semantic Web,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21644477270&doi=10.1145%2f1064340.1064342&partnerID=40&md5=b60db983e976f2d003c89c24a5bc2080,"The Semantic Web augments the current WWW by giving information a well-defined meaning, better enabling computers and people to work in cooperation. This is done by adding machine understandable content to Web resources. Such added content is called metadata, whose semantics is provided by referring to an ontology-a domain's conceptualization agreed upon by a community. The Semantic Web relies on the complex interaction of several technologies involving ontologies. Therefore, sophisticated Semantic Web applications typically comprise more than one software module. Instead of coming up with proprietary solutions, developers should be able to rely on a generic infrastructure for application development in this context. We call such an infrastructure Application Server for the Semantic Web whose design and development are based on existing Application Servers. However, we apply and augment their underlying concepts for use in the Semantic Web and integrate semantic technology within the server itself. The article discusses requirements and design issues of such a server, presents our implementation KAON SERVER and demonstrates its usefulness by a detailed scenario. © 2005 ACM.",Application server; Extensibility; Interoperation; KAON; KAON SERVER; Middleware; Ontology; Reuse; Semantic middleware; Semantic Web; Wonder-Web,Computer aided design; Computer applications; Computer software; Metadata; Middleware; Online systems; Reusability; Servers; World Wide Web; Application server; Extensibility; Interoperation; Ontology; Proprietary solutions; Semantic web; Wonder web; Semantics
Graphical query interfaces for semistructured data: The QURSED system,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21644461877&doi=10.1145%2f1064340.1064344&partnerID=40&md5=b4ef48b51a573d554af324033bdf39ab,"We describe the QURSED system for the declarative specification and automatic generation of Web-based query forms and reports (QFRs) for semistructured XML data. In QURSED, a QFR is formally described by its query set specification (QSS) which captures the complex query and reporting capabilities of the QFR and the associations of the query set specification with visual elements that implement these capabilities on a Web page. The design-time component of QURSED, called QURSED Editor, semi-automates the development of the query set specification and its association with visual elements by translating intuitive visual actions taken by a developer into appropriate specification fragments. The run-time component of QURSED produces XQuery statements by synthesizing fragments from the query set specification that have been activated during the interaction of the end-user with the QFR and renders the query results in interactive reports as specified by the QSS. We describe the techniques and algorithms employed by QURSED with emphasis on how it accommodates the intricacies introduced by the semistructured nature of the underlying data. We present the formal model of the query set specification, as well as its generation via the QURSED Editor, and focus on the techniques and heuristics the Editor employs for translating visual designer input into meaningful specifications. We also present the algorithms QURSED employs for query generation and report generation. An online demonstration of the system is available at http://www.db.ucsd.edu/qursed/. © 2005 ACM.",Algorithms; Design; Human Factors; Languages,Algorithms; Computer aided design; Computer programming; Computer software; Copyrights; Data storage equipment; Database systems; Graphic methods; Information retrieval; Interfaces (computer); XML; Graphical query; QURSED system; Semistructured data; Visual elements; Data structures
What makes the differences: Benchmarking XML database implementations,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244373337&doi=10.1145%2f1052934.1052940&partnerID=40&md5=adc7c394e56a7317089aceb4eca81dd6,"XML is emerging as a major standard for representing data on the World Wide Web. Recently, many XML storage models have been proposed to manage XML data. In order to assess an XML database's abilities to deal with XML queries, several benchmarks have also been proposed, including XMark and XMach. However, no reported studies using those benchmarks were found that can provide users with insights on the impacts of a variety of storage models on XML query performance. In this article, we report our first set of results on benchmarking a set of XML database implementations using two XML benchmarks. The selected implementations represent a wide range of approaches, including RDBMS-based systems with document-independent and document-dependent XML-relational schema mapping approaches, and XML native engines based on an Object-Oriented Model and the Document Object Model. Comprehensive experiments were conducted to study relative performance of different approaches and the important issues that affect XML query performance, such as path expression query processing, effectiveness of various partitioning, label-path, and indexing structures. © 2005 ACM.",,Benchmarking; Database systems; Indexing (of information); Project management; Query languages; World Wide Web; Document object model; Query performance; Query processing; XML database; XML
A multi-agent infrastructure for developing personalized web-based systems,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244371709&doi=10.1145%2f1052934.1052936&partnerID=40&md5=49c5ff7bde79a559addf3146c95eda52,"Although personalization and ubiquity are key properties for on-line services, they challenge the development of these systems due to the complexity of the required architectures. In particular, the current infrastructures for the development of personalized, ubiquitous services are not flexible enough to accommodate the configuration requirements of the various application domains. To address such issues, highly configurable infrastructures are needed. In this article, we describe Seta2000, an infrastructure for the development of recommender systems that support personalized interactions with their users and are accessible from different types of devices (e.g., desktop computers and mobile phones). The Seta2000 infrastructure offers a built-in recommendation engine, based on a multi-agent architecture. Moreover, the infrastructure supports the integration of heterogeneous software and the development of agents that can be configured to offer specialized facilities within a recommender system, but also to dynamically enable and disable such facilities, depending on the requirements of the application domain. The Seta2000 infrastructure has been exploited to develop two prototypes: SeTA is an adaptive Web store personalizing the recommendation and presentation of products in the Web. INTRIGUE is a personalized, ubiquitous information system suggesting attractions to possibly heterogeneous tourist groups. © 2005 ACM.",Infrastructures for developing personalized recommender systems; Multi-agent architectures; Personalization,Computer architecture; Computer software; Interfaces (computer); Multi agent systems; Online systems; Personal computers; Heterogeneous software; Infrastructures for developing personalized recommender systems; Multi-agent architectures; Personalization; World Wide Web
Inside PageRank,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244392071&doi=10.1145%2f1052934.1052938&partnerID=40&md5=c921f771692baaacf62234c99ce75c84,"Although the interest of a Web page is strictly related to its content and to the subjective readers' cultural background, a measure of the page authority can be provided that only depends on the topological structure of the Web. PageRank is a noticeable way to attach a score to Web pages on the basis of the Web connectivity. In this article, we look inside PageRank to disclose its fundamental properties concerning stability, complexity of computational scheme, and critical role of parameters involved in the computation. Moreover, we introduce a circuit analysis that allows us to understand the distribution of the page score, the way different Web communities interact each other, the role of dangling pages (pages with no outlinks), and the secrets for promotion of Web pages. © 2005 ACM.",Information retrieval; Markov chains; PageRank; Search engines; Searching the Web; Web page scoring,Algorithms; Computational complexity; Computer graphics; Information retrieval; Markov processes; Search engines; PageRank; Problem complexity; Searching the Web; Web page scoring; Websites
Motion prediction for caching and prefetching in mouse-driven DVE navigation,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244410750&doi=10.1145%2f1052934.1052937&partnerID=40&md5=c19fe46a806c4d009c8df81e19ff5710,"A distributed virtual environment (DVE) allows geographically separated users to participate in a shared virtual environment via connected networks. However, when the users are connected by the Internet, bandwidth limitation and network latency may seriously affect the performance and the interactivity of the system. This explains why there are very few DVE applications for the Internet. To address these shortcomings, caching and prefetching techniques are usually employed. Unfortunately, the effectiveness of these techniques depends largely on the accuracy of the prediction method used. Although there are a few methods proposed for predicting 3D motion, most of them are primarily designed for predicting the motion of specific objects by assuming certain object motion behaviors. We notice that in desktop DVE applications, such as virtual walkthrough and network gaming, the 2D mouse is still the most popular device used for navigation input. Through studying the motion behavior of a mouse during 3D navigation, we have developed a hybrid motion model for predicting the mouse motion during such navigation - a linear model for prediction at low-velocity motion and an elliptic model for prediction at high-velocity motion. The predicted mouse motion velocity is then mapped to the 3D environment for predicting the user's 3D motion. We describe how this prediction method can be integrated into the caching and prefetching mechanisms of our DVE prototype. We also demonstrate the effectiveness of the method and the resulting caching and prefetching mechanisms through extensive experiments. © 2005 ACM.",Caching and prefetching; Distributed virtual environments; Mouse motion prediction; Virtual navigation,Algorithms; Bandwidth; Distributed computer systems; Human engineering; Internet; Navigation; Probability; Caching and prefetching; Distributed virtual environments (DVE); Mouse motion prediction; Virtual navigation; Virtual reality
Using certified policies to regulate e-commerce transactions,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244409227&doi=10.1145%2f1052934.1052939&partnerID=40&md5=2093de50bdbbc61746e478aa2dff6af0,"A language for stating contract terms and several formal examples of certified policies (CP) to regulate e-commerce transactions are discussed. The implementation of the enforcement mechanism is described and experimental performance results are presented. It is argued that existent access control mechanisms cannot adequately support large sets of contract regulations. It is believed that the proposed mechanism should be relatively easy and inexpensive to apply since it uses an infrastructure which is already in place, well known and well understood.",Contract terms; Enforcement; Scalability,Computer operating systems; Contracts; Laws and legislation; Public policy; Regulatory compliance; Security of data; Certified policies (CP); Contract terms; Enforcement; Scalability; Electronic commerce
An embedded domain-specific language for type-safe server-side web scripting,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244406191&doi=10.1145%2f1052934.1052935&partnerID=40&md5=09f21595f9314ae3ea7c30e6934d6fa2,"WASH/CGI is an embedded domain-specific language for server-side Web scripting. Due to its reliance on the strongly typed, purely functional programming language Haskell as a host language, it is highly flexible and - at the same time - it provides extensive guarantees due to its pervasive use of type information. WASH/CGI can be structured into a number of sublanguages addressing different aspects of the application. The document sublanguage provides tools for the generation of parameterized XHTML documents and forms. Its typing guarantees that almost all generated documents are valid XHTML documents. The session sublanguage provides a session abstraction with a transparent notion of session state and allows the composition of documents and Web forms to entire interactive scripts. Both are integrated with the widget sublanguage which describes the communication (parameter passing) between client and server. It imposes a simple type discipline on the parameters that guarantees that forms posted by the client are always understood by the server. That is, the server never asks for data not submitted by the client and the data submitted by the client has the type requested by the server. In addition, parameters are received in their typed internal representation, not as strings. Finally, the persistence sublanguage deals with managing shared state on the server side as well as individual state on the client side. It presents shared state as an abstract data type, where the script can control whether it wants to observe mutations due to concurrently executing scripts. It guarantees that states from different interaction threads cannot be confused. © 2005 ACM.",Interactive Web services; Web programming,Computer programming; Data reduction; Embedded systems; Interactive computer systems; Servers; World Wide Web; Domain-specific languages; Interactive Web services; Web forms; Web programming; Computer programming languages
"Link analysis ranking: Algorithms, theory, and experiments",2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244369398&doi=10.1145%2f1052934.1052942&partnerID=40&md5=7f423be1a4c99c01f4b5679bb9fc351f,"The explosive growth and the widespread accessibility of the Web has led to a surge of research activity in the area of information retrieval on the World Wide Web. The seminal papers of Kleinberg [1998, 1999] and Brin and Page [1998] introduced Link Analysis Ranking, where hyperlink structures are used to determine the relative authority of a Web page and produce improved algorithms for the ranking of Web search results. In this article we work within the hubs and authorities framework defined by Kleinberg and we propose new families of algorithms. Two of the algorithms we propose use a Bayesian approach, as opposed to the usual algebraic and graph theoretic approaches. We also introduce a theoretical framework for the study of Link Analysis Ranking algorithms. The framework allows for the definition of specific properties of Link Analysis Ranking algorithms, as well as for comparing different algorithms. We study the properties of the algorithms that we define, and we provide an axiomatic characterization of the INDEGREE heuristic which ranks each node according to the number of incoming links. We conclude the article with an extensive experimental evaluation. We study the quality of the algorithms, and we examine how different structures in the graphs affect their performance. © 2005 ACM.",Bayesian; HITS; Link analysis; Ranking; Web search,Algebra; Data storage equipment; Graph theory; Heuristic methods; Information retrieval; World Wide Web; Bayesian approach; HITS; Link analysis; Ranking; Web search; Algorithms
Learning algorithms for single-instance electronic negotiations using the time-dependent behavioral tactic,2005,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244397698&doi=10.1145%2f1052934.1052941&partnerID=40&md5=d12656fae662c97499992939f7832658,"Negotiator often rely on learning an opponent's behavior and on then using the knowledge gained to arrive at a better deal. However, in an electronic negotiation setting in which the parties involved are often unknown to (and therefore lack information about) each other, this learning has to be accomplished with only the bid offers submitted during an ongoing negotiation. In this article, we consider such a scenario and develop learning algorithms for electronic agents that use a common negotiation tactic, namely, the time-dependent tactic (TDT), in which the values of the negotiating issues are dependent on the time elapsed in the negotiation. Learning algorithms for this tactic have not been proposed in the literature. Our approach is based on using the derivatives of the Taylor's series approximation of the TDT function in a three-phase algorithm that enumerates over a partial discretized version of the solution space. Computational results with our algorithms are encouraging. © 2005 ACM.",Electronic agents; Electronic commerce; Electronic negotiation; Learning; Time-dependent tactic,Approximation theory; Computational methods; Electronic commerce; Estimation; Internet; Learning systems; Electronic agents; Electronic negotiation; Learning; Time-dependent tactic; Learning algorithms
Market-based recommendation: Agents that compete for consumer attention,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10944234573&doi=10.1145%2f1031114.1031118&partnerID=40&md5=d8a76a19a39876fb86221f5bf33e1046,"The amount of attention space available for recommending suppliers to consumers on e-commerce sites is typically limited. We present a competitive distributed recommendation mechanism based on adaptive software agents for efficiently allocating the ""consumer attention space,"" or banners. In the example of an electronic shopping mall, the task is delegated to the individual shops, each of which evaluates the information that is available about the consumer and his or her interests (e.g. keywords, product queries, and available parts of a profile). Shops make a monetary bid in an auction where a limited amount of ""consumer attention space"" for the arriving consumer is sold. Each shop is represented by a software agent that bids for each consumer. This allows shops to rapidly adapt their bidding strategy to focus on consumers interested in their offerings. For various basic and simple models for on-line consumers, shops, and profiles, we demonstrate the feasibility of our system by evolutionary simulations as in the field of agent-based computational economics (ACE). We also develop adaptive software agents that learn bidding-strategies, based on neural networks and strategy exploration heuristics. Furthermore, we address the commercial and technological advantages of this distributed market-based approach. The mechanism we describe is not limited to the example of the electronic shopping mall, but can easily be extended to other domains.",ACE; Agent-based computational economics; Competitive multi-agent systems; Electronic markets; Learning agents; Market-based programming; Recommendation systems,Adaptive control systems; Algorithms; Computational complexity; Data processing; Learning systems; Mathematical models; Multi agent systems; Parameter estimation; Resource allocation; Software agents; Agent-based computational economies (ACE); Competitive multi-agent systems; Electronic markets; Learning agents; Recommendation systems; Electronic commerce
Topical web crawlers: Evaluating adaptive algorithms,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-9744257884&doi=10.1145%2f1031114.1031117&partnerID=40&md5=fa17d19c8d5a5f65beaef1cf36083b3e,"Topical crawlers are increasingly seen as a way to address the scalability limitations of universal search engines, by distributing the crawling process across users, queries, or even client computers. The context available to such crawlers can guide the navigation of links with the goal of efficiently locating highly relevant target pages. We developed a framework to fairly evaluate topical crawling algorithms under a number of performance metrics. Such a framework is employed here to evaluate different algorithms that have proven highly competitive among those proposed in the literature and in our own previous research. In particular we focus on the tradeoff between exploration and exploitation of the cues available to a crawler, and on adaptive crawlers that use machine learning techniques to guide their search. We find that the best performance is achieved by a novel combination of explorative and exploitative bias, and introduce an evolutionary crawler that surpasses the performance of the best nonadaptive crawler after sufficiently long crawls. We also analyze the computational complexity of the various crawlers and discuss how performance and complexity scale with available resources. Evolutionary crawlers achieve high efficiency and scalability by distributing the work across concurrent agents, resulting in the best performance/cost ratio.",Efficiency; Evaluation; Evolution; Exploitation; Exploration; Reinforcement learning; Topical crawlers,Algorithms; Bandwidth; Computational complexity; Data reduction; Information retrieval; Learning systems; Semantics; Websites; World Wide Web; Exploration; Reinforcement learning; Tropical crawlers; Vector spaces; Search engines
Collaborative recommendation: A robustness analysis,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10944236856&doi=10.1145%2f1031114.1031116&partnerID=40&md5=f36a7777318b8b1abab31169b8c33e6f,"Collaborative recommendation has emerged as an effective technique for personalized information access. However, there has been relatively little theoretical analysis of the conditions under which the technique is effective. To explore this issue, we analyse the robustness of collaborative recommendation: the ability to make recommendations despite (possibly intentional) noisy product ratings. There are two aspects to robustness: recommendation accuracy and stability. We formalize recommendation accuracy in machine learning terms and develop theoretically justified models of accuracy. In addition, we present a framework to examine recommendation stability in the context of a widely-used collaborative filtering algorithm. For each case, we evaluate our analysis using several real-world data-sets. Our investigation is both practically relevant for enterprises wondering whether collaborative recommendation leaves their marketing operations open to attack, and theoretically interesting for the light it sheds on a comprehensive theory of collaborative recommendation.",Collaborative recommendation; Machine learning; Robustness,Data storage equipment; Database systems; Electronic commerce; Learning systems; Probability; Robustness (control systems); Theorem proving; Collaborative recommendation; Data sets; Personalization; Computer supported cooperative work
Defending against an Internet-based attack on the physical world,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444317270&doi=10.1145%2f1013202.1013203&partnerID=40&md5=6948b43508110d7d885521ff909c9b8c,"We discuss the dangers that scalable Internet functionality may present to the real world, focusing upon an attack that is simple, yet can have great impact, which we believe may occur quite soon. We offer and critique various solutions to this class of attack and hope to provide a warning to the Internet community of what is currently possible. The attack is, to some degree, a consequence of the availability of private information on the Web, and the increase in the amount of personal information that users must reveal to obtain Web services.",Automated attacks; Cybercrime; Internet threats,Computer crime; Computer system firewalls; Data handling; Data privacy; Search engines; Security of data; Automated attacks; Cybercrime; Internet threats; Internet
Answering queries using views: A KRDB perspective for the semantic web,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444361318&doi=10.1145%2f1013202.1013204&partnerID=40&md5=db0f9d018bd1e8452e52130b908c94c2,"In this article, we investigate a first step towards the long-term vision of the Semantic Web by studying the problem of answering queries posed through a mediated ontology to multiple information sources whose content is described as views over the ontology relations. The contributions of this paper are twofold. We first offer a uniform logical setting which allows us to encompass and to relate the existing work on answering and rewriting queries using views. In particular, we make clearer the connection between the problem of rewriting queries using views and the problem of answering queries using extensions of views. Then we focus on an instance of the problem of rewriting conjunctive queries using views through an ontology expressed in a description logic, for which we exhibit a complete algorithm.",Information integration; Knowledge representation; Semantic web,Algorithms; Data handling; Formal logic; Information retrieval; Knowledge representation; Semantics; Information integration; Ontology; Semantic web; World Wide Web
Recovery guarantees for internet applications,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444313951&doi=10.1145%2f1013202.1013205&partnerID=40&md5=6aaf0dd6e70275450f35e5d550c2e833,"Internet-based e-services require application developers to deal explicitly with failures of the underlying software components, for example web servers, servlets, browser sessions, and so forth. This complicates application programming, and may expose failures to end users. This paper presents a framework for an application-independent infrastructure that provides recovery guarantees and masks almost all system failures, thus relieving the application programmer from having to deal with these failures - by making applications ""stateless."" The main concept is an interaction contract between two components regarding message and state preservation. The framework provides comprehensive recovery encompassing data, messages, and the states of application components. We describe techniques to reduce logging cost, allow effective log truncation, and permit independent recovery for critical components. We illustrate the framework's utility via web-based e-services scenarios. Its feasibility is demonstrated by our prototype implementation of interaction contracts based on the Apache web server and the PHP servlet engine. Finally, we discuss industrial relevance for middleware architectures such as .Net or J2EE.",Application recovery; Communication protocols; Exactly-once execution; Interaction contracts,Database systems; Electronic commerce; Information retrieval; Network protocols; Web browsers; Application recovery; Exactly one execution; Interaction contracts; Internet
Agent trade servers in financial exchange systems,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444384079&doi=10.1145%2f1013202.1013206&partnerID=40&md5=145f268d9d781c5074c65959ec08c608,"New services based on the best-effort paradigm could complement the current deterministic services of an electronic financial exchange. Four crucial aspects of such systems would benefit from a hybrid stance: proper use of processing resources, bandwidth management, fault tolerance, and exception handling. We argue that a more refined view on Quality-of-Service control for exchange systems, in which the principal ambition of upholding a fair and orderly marketplace is left uncompromised, would benefit all interested parties.",Agent programming; Agent server; Financial exchange service; Trading agent,Bandwidth; Computer programming; Electronic commerce; Financial data processing; Servers; Agent programming; Agent servers; Financial exchange services; Trading agents; Internet
PageCluster: Mining conceptual link hierarchies from Web log files for adaptive Web site navigation,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344280878&doi=10.1145%2f990301.990305&partnerID=40&md5=ffb6e99c63a29d84f0753b6cddc9d8c9,"User traversals on hyperlinks between Web pages can reveal semantic relationships between these pages. We use user traversals on hyperlinks as weights to measure semantic relationships between Web pages. On the basis of these weights, we propose a novel method to put Web pages on a Web site onto different conceptual levels in a link hierarchy. We develop a clustering algorithm called PageCluster, which clusters conceptually-related pages on each conceptual level of the link hierarchy based on their in-link and out-link similarities. Clusters are then used to construct a conceptual link hierarchy, which is visualized in a prototype called Online Navigation Explorer (ONE) for adaptive Web site navigation. Our experiments show that our method can put Web pages onto conceptual levels of a link hierarchy more accurately than both the breadth-first search method and the shortest-weighted-path method, and PageCluster can cluster conceptually-related pages more accurately than the bibliographic analysis method. Our user study also shows that the conceptual link hierarchy visualized in ONE can help users find information more effectively and efficiently as the task of finding information becomes less specific and involves more Web pages on multiple conceptual levels.",Bibliographic analysis; Clustering; Conceptual link hierarchies; Link hierarchies; Link similarity; Web site navigation,Adaptive systems; Algorithms; Bibliographies; Hierarchical systems; Learning systems; Navigation; Web browsers; Bibliographic analysis; Clustering; Conceptual link hierarchies; Link hierarchies; Link similarity; Web site navigation; Websites
Learning to find answers to questions on the Web,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344316704&doi=10.1145%2f990301.990303&partnerID=40&md5=3e91ceee5d91c1830a10b879bb60d8c5,"We introduce a method for learning to find documents on the Web that contain answers to a given natural language question. In our approach, questions are transformed into new queries aimed at maximizing the probability of retrieving answers from existing information retrieval systems. The method involves automatically learning phrase features for classifying questions into different types, automatically generating candidate query transformations from a training set of question/answer pairs, and automatically evaluating the candidate transformations on target information retrieval systems such as real-world general purpose search engines. At run-time, questions are transformed into a set of queries, and reranking is performed on the documents retrieved. We present a prototype search engine, Tritus, that applies the method to Web search engines. Blind evaluation on a set of real queries from a Web search engine log shows that the method significantly outperforms the underlying search engines, and outperforms a commercial search engine specializing in question answering. Our methodology cleanly supports combining documents retrieved from different search engines, resulting in additional improvement with a system that combines search results from multiple Web search engines.",Information retrieval; Meta-search; Query expansion; Question answering; Web search,Algorithms; Computer software; Heuristic methods; Information retrieval systems; Knowledge engineering; Learning systems; Natural language processing systems; Search engines; Experimentation; Meta-search; Queries; Query expansion; Query transformations; Question answering; Websites
LinkSelector: A Web mining approach to hyperlink selection for Web portals,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344329225&doi=10.1145%2f990301.990306&partnerID=40&md5=4b24e727ff78e226f2fc7a3259defa22,"As the size and complexity of Web sites expands dramatically, it has become increasingly challenging to design Web sites where Web surfers can easily find the information they seek. In this article, we address the design of the portal page of a Web site, which serves as the homepage of a Web site or a default Web portal. We define an important research problem - hyperlink selection: selecting from a large set of hyperlinks in a given Web site, a limited number of hyperlinks for inclusion in a portal page. The objective of hyperlink selection is to maximize the efficiency, effectiveness, and usage of a Web site's portal page. We propose a heuristic approach to hyperlink selection, LinkSelector, which is based on relationships among hyperlinks - structural relationships that can be extracted from an existing Web site and access relationships that can be discovered from a Web log. We compared the performance of LinkSelector with that of the current practice of hyperlink selection (i.e., manual hyperlink selection by domain experts), using data obtained from the University of Arizona Web site. Results showed that LinkSelector outperformed the current manual selection method.",Web mining,Algorithms; Data acquisition; Data mining; Database systems; Heuristic methods; Matrix algebra; Portals; Search engines; Hyperlink Selection; LinkSelector; Web mining; Web search efficiency; World Wide Web
Selective Markov models for predicting Web page accesses,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344267610&doi=10.1145%2f990301.990304&partnerID=40&md5=96aa6e2627608065750adbc452740295,"The problem of predicting a user's behavior on a Web site has gained importance due to the rapid growth of the World Wide Web and the need to personalize and influence a user's browsing experience. Markov models and their variations have been found to be well suited for addressing this problem. Of the different variations of Markov models, it is generally found that higher-order Markov models display high predictive accuracies on Web sessions that they can predict. However, higher-order models are also extremely complex due to their large number of states, which increases their space and run-time requirements. In this article, we present different techniques for intelligently selecting parts of different order Markov models so that the resulting model has a reduced state complexity, while maintaining a high predictive accuracy.",Markov models; Predicting user behavior; Web mining; World wide web,Computational complexity; Database systems; Mathematical models; Problem solving; Signal filtering and prediction; Web browsers; Websites; Markov models; Predicting user behavior; Web mining; Markov processes
Guest Editorial,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025381640&doi=10.1145%2f990301.990302&partnerID=40&md5=37dc4f6088cc6bfb0bf88c056ca483df,[No abstract available],,
Fine-Grained Control of Security Capabilities,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016749139&doi=10.1145%2f967030.967033&partnerID=40&md5=4b2345fa640dee7aa6a7e1ec53ca28b4,"We present a new approach for fine-grained control over users' security privileges (fast revocation of credentials) centered around the concept of an on-line semi-trusted mediator (SEM). The use of a SEM in conjunction with a simple threshold variant of the RSA cryptosystem (mediated RSA) offers a number of practical advantages over current revocation techniques. The benefits include simplified validation of digital signatures, efficient certificate revocation for legacy systems and fast revocation of signature and decryption capabilities. This paper discusses both the architecture and the implementation of our approach as well as its performance and compatibility with the existing infrastructure. Experimental results demonstrate its practical aspects. © 2004, ACM. All rights reserved.",Algorithms; Certificate Revocation; Digital Signatures; Public Key Infrastructure; Security,
Machine Learning for the Internet,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861874398&doi=10.1145%2f1031114.1031115&partnerID=40&md5=85fd4d3885422ac30c8cf450fc802994,[No abstract available],,
Fighting the Spam Wars: A Remailer Approach with Restrictive Aliasing,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-29544447597&doi=10.1145%2f967030.967031&partnerID=40&md5=546c243fda05a20871dabfe95595d52a,"We present an effective method of eliminating unsolicited electronic mail (so-called spam) and discuss its publicly accessible prototype implementation. Asubscriber to our system is able to obtain an unlimited number of aliases of his/her permanent (protected) E-Mail address to be handed out to parties willing to communicate with the subscriber. It is also possible to set up publishable aliases, which can be used by human correspondents to contact the subscriber, while being useless to harvesting robots and spammers. The validity of an alias can be easily restricted to a specific duration in time, a specific number of received messages, a specific population of senders, and/or in other ways. The system is fully compatible with the existing E-Mail infrastructure and can be immediately accessed via any standard E-Mail client software (MUA). It can be easily deployed at any institution or organization running its private E-Mail server (MTA) with a trivial modification to that server. Our system offers a simple method to salvage the existing population of E-Mail addresses while eliminating all spam aimed at them. © 2004, ACM. All rights reserved.",Algorithms; Human Factors Electronic Mail; Privacy; Security; Spam,
An Open Architecture for Next-Generation Telecommunication Services,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942748958&doi=10.1145%2f967030.967034&partnerID=40&md5=9d63d71036c6dba9c50dbd5bdd6d0a3a,"An open (in the sense of extensible and programmable) architecture for IP telecommunications must be based on a comprehensive strategy for managing feature interaction. We describe our experience with BoxOS, an IP telecommunication platform that implements the DFC technology for feature composition. We present solutions to problems, common to all efforts in IP telecommunications, of feature distribution, interoperability, and media management. We also explain how BoxOS addresses many deficiencies in SIP, including how BoxOS can be used as a SIP application server. © 2004, ACM. All rights reserved.",Component architectures; Design; Electronic mail; Feature interaction; Instant messaging; Intelligent Network architecture; Languages; Multimedia systems; Network addressing; Network interoperation; Network optimization; Network protocols; Service creation; Session Initiation Protocol,
Optimizing Result Prefetching in Web Search Engines with Segmented Indices,2004,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448970629&doi=10.1145%2f967030.967032&partnerID=40&md5=17885d5e7c138e5333119b4540accc70,"We study the process in which search engines with segmented indices serve queries. In particular, we investigate the number of result pages that search engines should prepare during the query processing phase. Search engine users have been observed to browse through very few pages of results for queries that they submit. This behavior of users suggests that prefetching many results upon processing an initial query is not efficient, since most of the prefetched results will not be requested by the user who initiated the search. However, a policy that abandons result prefetching in favor of retrieving just the first page of search results might not make optimal use of system resources either. We argue that for a certain behavior of users, engines should prefetch a constant number of result pages per query. We define a concrete query processing model for search engines with segmented indices, and analyze the cost of such prefetching policies. Based on these costs, we show how to determine the constant that optimizes the prefetching policy. Our results are mostly applicable to local index partitions of the inverted files, but are also applicable to processing short queries in global index architectures. © 2004, ACM. All rights reserved.",Distributed inverted indices; Languages; Prefetching; Search engines; Theory,
Estimating frequency of change,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042814329&doi=10.1145%2f857166.857170&partnerID=40&md5=58ecab8546bf66f0b8555446b04d3863,"Many online data sources are updated autonomously and independently. In this article, we make the case for estimating the change frequency of data to improve Web crawlers, Web caches and to help data mining. We first identify various scenarios, where different applications have different requirements on the accuracy of the estimated frequency. Then we develop several ""frequency estimators"" for the identified scenarios, showing analytically and experimentally how precise they are. In many cases, our proposed estimators predict change frequencies much more accurately and improve the effectiveness of applications. For example, a Web crawler could achieve 35% improvement in ""freshness"" simply by adopting our proposed estimator.",Change frequency estimation; Poisson process,Algorithms; Data mining; Database systems; Mathematical models; Time series analysis; Websites; Change frequency estimation; Data sources; Online stores; Poisson processes; Online systems
Southampton TAC: An adaptive autonomous trading agent,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347240736&doi=10.1145%2f857166.857168&partnerID=40&md5=7ea101376ff90b7616aae4649f813c5e,"Software agents are increasingly being used to represent humans in on-line auctions. Such agents, have the advantages of being able to systematically monitor a wide variety of auctions and then make rapid decisions about what bids to place in what auctions. They can do this continuously and repetitively without losing concentration. Moreover, in complex multiple auction settings, agents may need to modify their behavior in one auction depending on what is happening in another. To provide a means of evaluating and comparing (benchmarking) research methods in this area, the Trading Agent Competition (TAC) was established. This competition involves a number of agents bidding against one another in a number of related auctions (operating different protocols) to purchase travel packages for customers. Against this background, this artcle describes the design, implementation and evaluation of our adaptive autonomous trading agent, SouthamptonTAC, one of the most successful participants in TAC 2002.",Bidding strategy; On-line auctions; Trading agent competition,Algorithms; Artificial intelligence; Decision making; Intelligent agents; Online systems; Bidding strategy; On-line auctions; Trading agent competition; Travel packages; Autonomous agents
Developing a bidding agent for multiple heterogeneous auctions,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1142304988&doi=10.1145%2f857166.857167&partnerID=40&md5=e68b22d583fef34314ef97987f73ad38,"Due to the proliferation of online auctions, there is an increasing need to monitor and bid in multiple auctions in order to procure the best deal for the desired good. To this end, this paper reports on the development of a heuristic decision making framework that an autonomous agent can exploit to tackle the problem of bidding across multiple auctions with varying start and end times and with varying protocols (including English, Dutch and Vickrey). The framework is flexible, configurable, and enables the agent to adopt varying tactics and strategies that attempt to ensure that the desired item is delivered in a manner consistent with the user's preferences. Given this large space of possibilities, we employ a genetic algorithm to search (offline) for effective strategies in common classes of environment. The strategies that emerge from this evolution are then codified into the agent's reasoning behaviour so that it can select the most appropriate strategy to employ in its prevailing circumstances. The proposed framework has been implemented in a simulated marketplace environment and its effectiveness has been empirically demonstrated.",Bidding strategy; Genetic algorithms; Multiple auctions,Decision making; Genetic algorithms; Heuristic methods; Intelligent agents; Mathematical models; Online systems; Probability; Bidding strategy; Multiple auctions; Online auctions; Price distributions; Electronic commerce
Location management for mobile commerce applications in wireless Internet environment,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042774924&doi=10.1145%2f857166.857169&partnerID=40&md5=1848cbc33beb7076f15f943c27c3c4a1,"With recent advances in devices, middleware, applications and networking infrastructure, the wireless Internet is becoming a reality. We believe that some of the major drivers of the wireless Internet will be emerging mobile applications such as mobile commerce. Although many of these are futuristic, some applications including user-and location-specific mobile advertising, location-based services, and mobile financial services are beginning to be commercialized. Mobile commerce applications present several interesting and complex challenges including location management of products, services, devices, and people. Further, these applications have fairly diverse requirements from the underlying wireless infrastructure in terms of location accuracy, response time, multicast support, transaction frequency and duration, and dependability. Therefore, research is necessary to address these important and complex challenges. In this article, we present an integrated location management architecture to support the diverse location requirements of m-commerce applications. The proposed architecture is capable of supporting a range of location accuracies, wider network coverage, wireless multicast, and infrastructure dependability for m-commerce applications. The proposed architecture can also support several emerging mobile applications. Additionally, several interesting research problems and directions in location management for wireless Internet applications are presented and discussed.",Infrastructure dependability; Location management; Mobile applications; Mobile commerce; Satellites; Wireless Internet; Wireless LANs; Wireless multicast,Communication satellites; Local area networks; Multicasting; Servers; Wireless telecommunication systems; Infrastructure dependability; Location management; Mobile commerce; Wireless multicasts; Internet
(How) can mobile agents do secure electronic transactions on untrusted hosts? A survey of the security issues and the current solutions,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1142282727&doi=10.1145%2f643477.643479&partnerID=40&md5=99e5a0a1970cc4c9b8db8a1aa950360a,"This article investigates if and how mobile agents can execute secure electronic transactions on untrusted hosts. An overview of the security issues of mobile agents is first given. The problem of untrusted (i.e., potentially malicious) hosts is one of these issues, and appears to be the most difficult to solve. The current approaches to counter this problem are evaluated, and their relevance for secure electronic transactions is discussed. In particular, a state-of-the-art survey of mobile agent-based secure electronic transactions is presented.",Electronic transactions; Malicious hosts; Mobile agent security,Bandwidth; Cryptography; Electronic commerce; Information management; Problem solving; Security of data; Web browsers; Electronic payments; Electronic transactions; Malicious hosts; Mobile agent security; Internet
The Use of Web Structure and Content to Identify Subjectively Interesting Web Usage Patterns,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14544282729&doi=10.1145%2f767193.767194&partnerID=40&md5=cb85bf77f502519b2b4cf284ca1b7681,"The discipline of Web Usage Mining has grown rapidly in the past few years, despite the crash of the e-commerce boom of the late 1990s. Web Usage Mining is the application of data mining techniques to Web clickstream data in order to extract usage patterns. Yet, with all of the resources put into the problem, claims of success have been limited and are often tied to specific Web site properties that are not found in general. One reason for the limited success has been a component of Web Usage Mining that is often overlooked—the need to understand the content and structure of a Web site. The processing and quantification of a Web sites content and structure for all but completely static and single frame Web sites is arguably one of the most difficult tasks to automate in the Web Usage Mining process. This article shows that, not only is the Web Usage Mining process enhanced by content and structure, it cannot be completed without it. The results of experiments run on data from a large e-commerce site are presented to show that proper preprocessing cannot be completed without the use of Web site content and structure, and that the effectiveness of pattern analysis is greatly enhanced. © 2003, ACM. All rights reserved.",Data mining; Experimentation; Measurement; Web usage mining; World Wide Web,
XDuce: A Statically Typed XML Processing Language,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442286447&doi=10.1145%2f767193.767195&partnerID=40&md5=8a8ed1a01ff0d4bd47cbbba3812348c1,"XDuce is a statically typed programming language for XML processing. Its basic data values are XML documents, and its types (so-called regular expression types) directly correspond to document schemas. XDuce also provides a flexible form of regular expression pattern matching, integrating conditional branching, tag checking, and subtree extraction, as well as dynamic typechecking. We survey the principles of XDuce's design, develop examples illustrating its key features, describe its foundations in the theory of regular tree automata, and present a complete formal definition of its core, along with a proof of type safety. © 2003, ACM. All rights reserved.",Languages; subtyping; Theory; tree automata; Type systems; XML,
Privacy through Pseudonymity in User-Adaptive Systems,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016682337&doi=10.1145%2f767193.767196&partnerID=40&md5=d500f7a61e2e2b13744e846699d49d50,"User-adaptive applications cater to the needs of each individual computer user, taking for example users' interests, level of expertise, preferences, perceptual and motoric abilities, and the usage environment into account. Central user modeling servers collect and process the information about users that different user-adaptive systems require to personalize their user interaction. Adaptive systems are generally better able to cater to users the more data their user modeling systems collect and process about them. They therefore gather as much data as possible and “lay them in stock” for possible future usage. Moreover, data collection usually takes place without users' initiative and sometimes even without their awareness, in order not to cause distraction. Both is in conflict with users' privacy concerns that became manifest in numerous recent consumer polls, and with data protection laws and guidelines that call for parsimony, purpose-orientation, and user notification or user consent when personal data are collected and processed. This article discusses security requirements to guarantee privacy in user-adaptive systems and explores ways to keep users anonymous while fully preserving personalized interaction with them. User anonymization in personalized systems goes beyond current models in that not only users must remain anonymous, but also the user modeling system that maintains their personal data. Moreover, users' trust in anonymity can be expected to lead to more extensive and frank interaction, hence to more and better data about the user, and thus to better personalization. A reference model for pseudonymous and secure user modeling is presented that meets many of the proposed requirements. © 2003, ACM. All rights reserved.",access control; anonymity; Chaum mix; encryption; Human Factors; KQML; personal information; personalization; privacy; pseudonymity; reference model; secrecy; Security; security; User modeling; user-adaptive systems,
Measuring and Characterizing End-to-End Internet Service Performance,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73349089742&doi=10.1145%2f945846.945849&partnerID=40&md5=cb7e88a054e1d4a30ddf4b3bef7d6890,"Fundamental to the design of reliable, high-performance network services is an understanding of the performance characteristics of the service as perceived by the client population as a whole. Understanding and measuring such end-to-end service performance is a challenging task. Current techniques include periodic sampling of service characteristics from strategic locations in the network and instrumenting Web pages with code that reports client-perceived latency back to a performance server. Limitations to these approaches include potentially nonrepresentative access patterns in the first case and determining the location of a performance bottleneck in the second. This paper presents EtE monitor, a novel approach to measuring Web site performance. Our system passively collects packet traces from a server site to determine service performance characteristics. We introduce a two-pass heuristic and a statistical filtering mechanism to accurately reconstruct different client page accesses and to measure performance characteristics integrated across all client accesses. Relative to existing approaches, EtE monitor offers the following benefits: i) a latency breakdown between the network and server overhead of retrieving a Web page, ii) longitudinal information for all client accesses, not just the subset probed by a third party, iii) characteristics of accesses that are aborted by clients, iv) an understanding of the performance breakdown of accesses to dynamic, multitiered services, and v) quantification of the benefits of network and browser caches on server performance. Our initial implementation and performance analysis across three different commercial Web sites confirm the utility of our approach. © 2003, ACM. All rights reserved.",End-to-end service performance; Measurement; network packet traces; passive monitoring; Performance; QoS; reconstruction of web page composition; web site performance,
Efficient Scheduling of Internet Banner Advertisements,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011106926&doi=10.1145%2f945846.945848&partnerID=40&md5=5b5e8bf970f0f9216b16ab9ec0c0aaef,"Despite the slowdown in the economy, advertisement revenue remains a significant source of income for many Internet-based organizations. Banner advertisements form a critical component of this income, accounting for 40 to 50 percent of the total revenue. There are considerable gains to be realized through the efficient scheduling of banner advertisements. This problem has been observed to be intractable via traditional optimization techniques, and has received only limited attention in the literature. This paper presents a procedure to generate advertisement schedules under the most commonly used advertisement pricing scheme—the CPM model. The solution approach is based on Lagrangean decomposition and is seen to provide extremely good advertisement schedules in a relatively short period of time, taking only a few hundred seconds of elapsed time on a 450 MHz PC compared to a few thousand seconds of CPU time on a workstation that other approaches need. Additionally, this approach can be incorporated into an actual implementation with minimal alterations and hence is of particular interest. © 2003, ACM. All rights reserved.",Algorithms; Banner advertising; Management; Scheduling; WWW,
Web mining for Web personalization,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042731750&doi=10.1145%2f643477.643478&partnerID=40&md5=b8d8650e829832a9a209a1a0d8f1d16a,"Web personalization is the process of customizing a Web site to the needs of specific users, taking advantage of the knowledge acquired from the analysis of the user's navigational behavior (usage data) in correlation with other information collected in the Web context, namely, structure, content, and user profile data. Due to the explosive growth of the Web, the domain of Web personalization has gained great momentum both in the research and commercial areas. In this article we present a survey of the use of Web mining for Web personalization. More specifically, we introduce the modules that comprise a Web personalization system, emphasizing the Web usage mining module. A review of the most common methods that are used as well as technical issues that occur is given, along with a brief overview of the most popular tools and applications available from software vendors. Moreover, the most important research initiatives in the Web usage mining and personalization areas are presented.",User profiling; Web personalization; Web usage mining; WWW,Correlation methods; Data mining; Data structures; HTML; Information retrieval; Web browsers; XML; User profiling; Web personalization; Web usage mining; World Wide Web
Design and development of data-intensive Web sites: The araneus approach,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042774921&doi=10.1145%2f643477.643480&partnerID=40&md5=c50fe955154cf17b1da5c3bafec769b0,"Data-intensive Web sites are large sites based on a back-end database, with a fairly complex hypertext structure. The paper develops two main contributions: (a) a specific design methodology for data-intensive Web sites, composed of a set of steps and design transformations that lead from a conceptual specification of the domain of interest to the actual implementation of the site; (b) a tool called HOMER, conceived to support the site design and implementation process, by allowing the designer to move through the various steps of the methodology, and to automate the generation of the code needed to implement the actual site. Our approach to site design is based on a clear separation between several design activities, namely database design, hypertext design, and presentation design. All these activities are carried on by using high-level models, all subsumed by an extension of the nested relational model; the mappings between the models can be nicely expressed using an extended relational algebra for nested structures. Based on the design artifacts produced during the design process, and on their representation in the algebraic framework, HOMER is able to generate all the code needed for the actual generation of the site, in a completely automatic way.",Databases; Development; Internet; World Wide Web; WWW,Automation; Computer programming languages; Data structures; HTML; Information retrieval; Mathematical models; Relational database systems; Sustainable development; Datbases; High-level models; Hypertext design; Relational models; Websites
Architecture and Performance of Server-Directed Transcoding,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1142296549&doi=10.1145%2f945846.945850&partnerID=40&md5=5728fa6b03fd84b6fab8260d8809b274,"Proxy-based transcoding adapts Web content to be a better match for client capabilities (such as screen size and color depth) and last-hop bandwidths. Traditional transcoding breaks the end-toend model of theWeb, because the proxy does not know the semantics of the content. Server-directed transcoding preserves end-to-end semantics while supporting aggressive content transformations. We show how server-directed transcoding can be integrated into the HTTP protocol and into the implementation of a proxy. We discuss several useful transformations for image content, and present measurements of the performance impacts. Our results demonstrate that server-directed transcoding is a natural extension to HTTP, can be implemented without great complexity, and can provide good performance when carefully implemented. © 2003, ACM. All rights reserved.",Design; Experimentation; HTTP; Performance; proxy; transcode; web,
Quality of Service in an Information Economy,2003,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011104435&doi=10.1145%2f945846.945847&partnerID=40&md5=d447025b490078caa180469b20bb45e8,"Accessing and processing distributed data sources have become important factors for businesses today. This is especially true for the emerging virtual enterprises with their data and processing capabilities spread across the Internet. Unfortunately, however, query processing on the Internet is not predictable and robust enough to meet the requirements of many business applications. For instance, the response time of a query can be unexpectedly high; or the monetary cost might be too high if the partners charge for the usage of their data or processing capabilities; or the result of the query might be useless because it is based on outdated data or only on parts (rather than all) of the available data. In this work, we show how a distributed query processor can be extended in order to support quality of service (QoS) guarantees.We proposeways to integrate QoS management into the various phases of query processing: (1) Query optimization uses a multi-dimensional assessment (cost, time and result quality) of query plans, (2) query plan instantiation comprises an admission control for sub-plans, and (3) during query plan execution the QoS of the query is monitored and a fuzzy controller initiates repairing actions if needed. The goal of our work is to provide an initial step towards QoS management in distributed query processing systems and do significantly better than current distributed database systems, which are based on a best-effort policy. © 2003, ACM. All rights reserved.",Design; Performance; Quality of Service,
Affinity-Based Management of Main Memory Database Clusters,2002,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015502069&doi=10.1145%2f604596.604599&partnerID=40&md5=2a75abcbbf03d5f090b843804948c7d9,"We study management strategies for main memory database clusters that are interposed between Internet applications and back-end databases as content caches. The task of management is to allocate data across individual cache databases and to route queries to the appropriate databases for execution. The goal is to maximize effective cache capacity and to minimize synchronization cost. We propose an affinity-based management system for main memory database cLUsters (ALBUM). ALBUM executes each query in two stages in order to take advantage of the query affinity that is observed in a wide range of applications. We evaluate the data/query distribution strategy in ALBUM with a set of trace-based simulations. The results show that ALBUM reduces cache miss ratio by a factor of 1.7 to 9 over alternative strategies.We have implemented a prototype of ALBUM, and compare its performance to that of an existing infrastructure: a fully replicated database with large buffer cache. The results show that ALBUM outperforms the existing infrastructure with the same number of server machines by a factor of 2 to 7, and that ALBUM with only 1=3 to 1=2 of the server machines achieves the same throughput as the existing infrastructure. Copyright © 2002, ACM. All rights reserved.",clustering; database administration; database cluster; Experimentation; file organization; Main memory database; Management; Measurement; Performance; query affinity; scalability,
Literature-Based Discovery on the World Wide Web,2002,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980370084&doi=10.1145%2f604596.604597&partnerID=40&md5=d31fb895fe81cd95dcdf37ae1fadc9d8,"Previous research has shown that researchers can generate medical hypotheses by using computers to analyze several, seemingly unrelated, medical literatures. In this work we suggest broader application for the ideas of literature-based discovery. Specifically, we suggest that literature-based discovery can be fruitful in areas other than medicine; that in addition to finding “cures” for “problems,” literature-based discovery offers the possibility of finding new problems for existing technologies; that the analysis of a single literature may be sufficient for literature-based discovery; and that literature-based discovery can support individuals seeking to draw together ideas from various areas of inquiry, even if such connections have been previously made by others. We describe literature-based discovery experiments conducted on the World Wide Web that support these ideas. Copyright © 2002, ACM. All rights reserved.",Design; Experimentation; Literature-based discovery,
The Platform for Privacy Preference as a Social Protocol: An Examination Within the U.S. Policy Context,2002,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993048403&doi=10.1145%2f604596.604598&partnerID=40&md5=8481aaab869f447b7e3f6e72552324ec,"As a “social protocol” aimed at providing a technological means to address concerns over Internet privacy, the Platform for Privacy Preferences (P3P) has been controversial since its announcement in 1997. In the U.S., critics have decried P3P as an industry attempt to avoid meaningful privacy legislation, while developers have portrayed the proposal as a tool for helping users make informed decisions about the impact of their Web surfing choices. This dispute touches upon the privacy model underlying P3P, the U.S. political context regarding privacy, and the technical components of the protocol. This article presents an examination of these factors, with an eye towards distilling lessons for developers of future social protocols. Copyright © 2002, ACM. All rights reserved.",Human Factors; Legal Aspects; P3P; social protocols; Standardization Privacy,
An Optimal Strategy for Sellers in an Online Auction,2002,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1142305874&doi=10.1145%2f503334.503335&partnerID=40&md5=b061049a844d899cf31b86c54b2d278b,"We consider an online auction setting where the seller attempts to sell an item. Bids arrive over time and the seller has to make an instant decision to either accept this bid and close the auction or reject it and move on to the next bid, with the hope of higher gains. What should be the seller's strategy to maximize gains? Using techniques from convex analysis, we provide an explicit closedform optimal solution (and hence a simple optimum online algorithm) for the seller. Our methodology is attractive to online auction systems that have to make an instant decision, especially when it is not humanly possible to evaluate each bid individually, when the number of bids is large or unknown ahead of time, and when the bidders are unwilling to wait. © 2002, ACM. All rights reserved.",Algorithms; Online Auctions; Random Walks; Theory Convex Analysis,
Principled Design of the Modern Web Architecture,2002,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963744744&doi=10.1145%2f514183.514185&partnerID=40&md5=8ecf89b452804d81577246b6f2077fc6,"The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia application. The modern Web architecture emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. In this article we introduce the Representational State Transfer (REST) architectural style, developed as an abstract model of the Web architecture and used to guide our redesign and definition of the Hypertext Transfer Protocol and Uniform Resource Identifiers. We describe the software engineering principles guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. We then compare the abstract model to the currently deployed Web architecture in order to elicit mismatches between the existing protocols and the applications they are intended to support. © 2002, ACM. All rights reserved.",Design; Network-based applications; Performance; REST; Standardization; World Wide Web,
The <Bigwig> Project,2002,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015157778&doi=10.1145%2f514183.514184&partnerID=40&md5=3032b6f352aa77ee8655a6049216d2d6,"We present the results of the <bigwig> project, which aims to design and implement a high-level domain-specific language for programming interactive Web services. A fundamental aspect of the development of the World Wide Web during the last decade is the gradual change from static to dynamic generation ofWeb pages. GeneratingWeb pages dynamically in dialog with the client has the advantage of providing up-to-date and tailor-made information. The development of systems for constructing such dynamic Web services has emerged as a whole new research area. The <bigwig> language is designed by analyzing its application domain and identifying fundamental aspects ofWeb services inspired by problems and solutions in existingWeb service development languages. The core of the design consists of a session-centered service model together with a flexible template-based mechanism for dynamicWeb page construction. Using specialized program analyses, certain Web-specific properties are verified at compile time, for instance that only valid HTML 4.01 is ever shown to the clients. In addition, the design provides high-level solutions to form field validation, caching of dynamic pages, and temporal-logic based concurrency control, and it proposes syntax macros for making highly domain-specific languages. The language is implemented via widely available Web technologies, such as Apache on the server-side and JavaScript and Java Applets on the client-side. We conclude with experience and evaluation of the project. © 2002, ACM. All rights reserved.",Design; Interactive Web services; Languages; program analysis; Verificatio,
Xlinkit: A Consistency Checking and Smart Link Generation Service,2002,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886843322&doi=10.1145%2f514183.514186&partnerID=40&md5=ebe56758672ad0dbd6084a559ad74657,"xlinkit is a lightweight application service that provides rule-based link generation and checks the consistency of distributed Web content. It leverages standard Internet technologies, notably XML, XPath, and XLink. xlinkit can be used as part of a consistency management scheme or in applications that require smart link generation, including portal construction and management of large document repositories. In this article we show how consistency constraints can be expressed and checked.We describe a novel semantics for first-order logic that produces links instead of truth values and give an account of our content management strategy.We present the architecture of our service and the results of two substantial case studies that use xlinkit for checking course syllabus information and for validating UML models supplied by industrial partners. © 2002, ACM. All rights reserved.",automatic link generation; Consistency management; constraint checking; Languages; Management; Verification; XML,
On Filter Effects in Web Caching Hierarchies,2002,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015440537&doi=10.1145%2f503334.503337&partnerID=40&md5=340dc00a5f5be481fe2a7ba49bf45129,"This article studies the “filter effects” that occur in Web proxy caching hierarchies due to the presence of multiple levels of caches. That is, the presence of one level of cache changes the structural characteristics of the workload presented to the next level of cache, since only the requests that miss in one cache are forwarded to the next cache. Trace-driven simulations, with empirical and synthetic traces, are used to demonstrate the presence and magnitude of the filter effects in a multilevel Web proxy caching hierarchy. Experiments focus on the effects of cache size, cache replacement policy, Zipf slope, and the depth of the Web proxy caching hierarchy. Finally, the article considers novel cache management techniques that can better exploit the changing workload characteristics across a multilevel Web proxy caching hierarchy. Trace-driven simulations are used to evaluate the performance of these approaches. The simulation results demonstrate that size-based partitioning and heterogeneous cache replacement policies each offer improvements in overall caching performance. The sensitivity of the results to the degree of workload overlap among child-level proxy caches is also studied. © 2002, ACM. All rights reserved.",Design; Experimentation; Measurement; Performance; Performance evaluation; Web performance; Web proxy caching hierarchies; World-Wide-Web,
A Self-Configuring and Self-Administering Name System with Dynamic Address Assignment,2002,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142730950&doi=10.1145%2f503334.503336&partnerID=40&md5=f51e8cb8de7869dec63674e730d53ec8,"In this article we present a distributed system that stores name-to-address bindings and provides name resolution to a network of computers. This name system consists of a network of name services that are individually self-configuring and self-administering. The name service consists of an agent program that works in conjunction with the current implementation of the Domain Name System (DNS) program. The DNS agent program automatically configures the Berkeley Internet Name Domain (BIND) process during start-up and dynamically reconfigures and administers the BIND process based on the changing state of the network. The proposed name system offers high scalability and fault-tolerance capabilities and communicates using standard Internet protocols. © 2002, ACM. All rights reserved.",Algorithms; Berkeley Internet Name Domain; Design; dynamic reconfiguration; name-to-name address binding; Performance; self-administering systems; self-configuring systems,
XREL: A Path-Based Approach to Storage and Retrieval of XML Documents Using Relational Databases,2001,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955598924&doi=10.1145%2f383034.383038&partnerID=40&md5=32416ec733ce2b30ff6ad4d080dcc7ff,"This article describes XRel, a novel approach for storage and retrieval of XML documents using relational databases. In this approach, an XML document is decomposed into nodes on the basis of its tree structure and stored in relational tables according to the node type, with path information from the root to each node. XRel enables us to store XML documents using a fixed relational schema without any information about DTDs and also to utilize indices such as the B+-tree and the R-tree supported by database management systems. Thus, XRel does not need any extension of relational databases for storing XML documents. For processing XML queries, we present an algorithm for translating a core subset of XPath expressions into SQL queries. Finally, we demonstrate the effectiveness of this approach through several experiments using actual XML documents. © 2001, ACM. All rights reserved.",Algorithms; Design; Management; Performance; Text markup; Text tagging; Xml query; Xpath,
Characterizing the Scalability of a Large Web-Based Shopping System,2001,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957095980&doi=10.1145%2f383034.383036&partnerID=40&md5=63d431d728867edbe0f1a6b2bc3bfeec,"This article presents an analysis of five days of workload data from a large Web-based shopping system. The multitier environment of this Web-based shopping system includes Web servers, application servers, database servers, and an assortment of load-balancing and firewall appliances. We characterize user requests and sessions and determine their impact on system performance and scalability. The purpose of our study is to assess scalability and support capacity planning exercises for the multitier system. We find that horizontal scalability is not always an adequate mechanism for supporting increased workloads and that personalization and robots can have a significant impact on system scalability. © 2001, ACM. All rights reserved.",Capacity planning; Clustering; Measurement; Performance; Personalization; Scalability analysis; Web-Based systems; Workload characterization,
On Balancing the Load in a Clustered Web Farm,2001,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006543884&doi=10.1145%2f502152.502155&partnerID=40&md5=5229f368a1db8e955a4585befd452021,"In this article we propose a novel, yet practical, scheme which attempts to optimally balance the load on the servers of a clustered Web farm. The goal in solving this performance problem is to achieve minimal average response time for customer requests, and thus ultimately achieve maximal customer throughput. The article decouples the overall problem into two related but distinct mathematical subproblems, one static and one dynamic. We believe this natural decoupling is one of the major contributions of our article. The static component algorithm determines good assignments of sites to potentially overlapping servers. These cluster assignments, which, due to overhead, cannot be changed too frequently, have a major effect on achievable response time. Additionally, these assignments must be palatable to the sites themselves. The dynamic component algorithm is designed to handle real-time load balancing by routing customer requests from the network dispatcher to the servers. This algorithm must react to fluctuating customer request load while respecting the assignments of sites to servers determined by the static component. The static and dynamic components both employ in various contexts the same so-called goal setting algorithm. This algorithm determines the theoretically optimal load on each server, given hypothetical cluster assignments and site activity. We demonstrate the effectiveness of the overall load-balancing scheme via a number of simulation experiments. © 2001, ACM. All rights reserved.",Algorithms; Clusteredweb Farms; Combinatorial Optimization; Load Balancing; Performance; Resource Allocation Problems,
The Architecture of Robust Publishing Systems,2001,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979203636&doi=10.1145%2f502152.502154&partnerID=40&md5=9da4deb32ea7afbf2c5972574ff1c4e3,"The Internet in its present form does not protect content from censorship. It is straightforward to trace any document back to a specific Web server, and usually directly to an individual. As we discuss below, there are valid reasons for publishing a document in a censorship-resistant manner. Unfortunately, few tools exist that facilitate this form of publishing. We describe the architecture of robust systems for publishing content on the Web. The discussion is in the context of Publius, as that system meets the most design goals of currently deployed systems. Publius has the property that it is very difficult for any adversary to censor or modify the content. In addition, the identity of the publisher is protected once the content is posted. The system differs from others in that tools are provided for updating or deleting published content, and users can browse the content in the normal point-and-click manner using a standard Web browser and a client-side proxy. © 2001, ACM. All rights reserved.",Censorship Resistance; Security; Web Publishing,
Rethinking the Design of the Internet: The End-To-End Arguments vs. The Brave New World,2001,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052862897&doi=10.1145%2f383034.383037&partnerID=40&md5=e80e323fecd0487964447619ef6f0c84,"This article looks at the Internet and the changing set of requirements for the Internet as it becomes more commercial, more oriented toward the consumer, and used for a wider set of purposes. We discuss a set of principles that have guided the design of the Internet, called the end-to-end arguments, and we conclude that there is a risk that the range of new requirements now emerging could have the consequence of compromising the Internet's original design principles. Were this to happen, the Internet might lose some of its key features, in particular its ability to support new and unanticipated applications. We link this possible outcome to a number of trends: the rise of new stakeholders in the Internet, in particular Internet service providers; new government interests; the changing motivations of a growing user base; and the tension between the demand for trustworthy overall operation and the inability to trust the behavior of individual users. © 2001, ACM. All rights reserved.",Economics; End-to-end argument; Internet; ISP; Legal Aspects,
Searching the Web,2001,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880240041&doi=10.1145%2f383034.383035&partnerID=40&md5=af639bb1d830d03320ca0e77c4ba06f4,"We offer an overview of current Web search engine design. After introducing a generic search engine architecture, we examine each engine component in turn. We cover crawling, local Web page storage, indexing, and the use of link analysis for boosting search performance. The most common design and implementation techniques for each of these components are presented. For this presentation we draw from the literature and from our own experimental search engine testbed. Emphasis is on introducing the fundamental concepts and the results of several performance analyses we conducted to compare different designs. © 2001, ACM. All rights reserved.",Algorithms; Authorities; Crawling; Design; Hits; Indexing; Information retrieval; Link analysis; Pagerank; Performance; Search engine,
"HTTP Cookies: Standards, Privacy, and Politics",2001,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012128179&doi=10.1145%2f502152.502153&partnerID=40&md5=4f946ee60aacec54f65f611f16d220e4,"How did we get from a world where cookies were something you ate and where “nontechies” were unaware of “Netscape cookies” to a world where cookies are a hot-button privacy issue for many computer users? This article describes how HTTP “cookies” work and how Netscape's original specification evolved into an IETF Proposed Standard. I also offer a personal perspective on how what began as a straightforward technical specification turned into a political flashpoint when it tried to address nontechnical issues such as privacy. © 2001, ACM. All rights reserved.",Cookies; Design; Http; Privacy; Security; Standardization; State Management; World Wide Web,
Fog Computing Platforms for Smart City Applications: A Survey,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143865925&doi=10.1145%2f3488585&partnerID=40&md5=f8a5c07be307a6ec30c7ba85b507fd17,"Emerging IoT applications with stringent requirements on latency and data processing have posed many challenges to cloud-centric platforms for Smart Cities. Recently, Fog Computing has been advocated as a promising approach to support such new applications and handle the increasing volume of IoT data and devices. The Fog Computing paradigm is characterized by a horizontal system-level architecture where devices close to end-users and IoT devices are used for processing, storage, and networking functions. Fog Computing platforms aim to facilitate the development of applications and systems for Smart Cities by providing services and abstractions designed to integrate data from IoT devices and various information systems deployed in the city. Despite the potential of the Fog Computing paradigm, the literature still lacks a broad, comprehensive overview of what has been investigated on the use of such paradigm in platforms for Smart Cities and open issues to be addressed in future research and development. In this paper, a systematic mapping study was performed and we present a comprehensive understanding of the use of the Fog Computing paradigm in Smart Cities platforms, providing an overview of the current state of research on this topic, and identifying important gaps in the existing approaches and promising research directions. © 2022 Association for Computing Machinery.",edge computing; Fog computing; smart cities,Data handling; Digital storage; Edge computing; Fog; Internet of things; Smart city; 'current; Computing paradigm; Computing platform; Edge computing; End-users; New applications; Research and development; Stringent requirement; System-level architectures; Systematic mapping studies; Fog computing
Enabling Short-Term Energy Flexibility Markets Through Blockchain,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151854011&doi=10.1145%2f3542949&partnerID=40&md5=32e9603f07d7cd527a4f7f531f76aa99,"Climate change has put significant pressure on energy markets. Political decisions such as the plan of the German government to shut down coal power plants by 2038 are shifting electricity production towards renewable and distributed energy resources. The share of these resources will continue to grow significantly in the coming years. This trend changes the ways how energy markets work which mandates fundamental changes in the underlying IT infrastructure. In this paper, we propose a blockchain-based solution which enables an economically viable and grid-serving integration of distributed energy resources into the existing energy system. Our blockchain-based approach targets intraday and day-ahead operating reserve markets, on which energy grid operators and operators of distributed energy resources can trade flexibilities within the schedulable energy production and consumption of their resources. By utilizing these flexibilities as an operating reserve, renewable and climate-friendly technologies can contribute to maintaining the grid stability and security of supply while simultaneously creating economically interesting business models for their operators. We propose to define blockchain-based short-term energy markets by utilizing the concept of general-purpose smart contracts and cryptocurrencies. This enables direct and decentralized trading of energy flexibilities without any intermediary or central instance. We demonstrate the feasibility of our approach through an implementation of a prototype of the proposed markets based on the Ethereum blockchain and provide a detailed evaluation of its efficiency and scalability.  © 2022 Association for Computing Machinery.",Blockchain; distributed ledger; energy market; operating reserve; smart contracts,Blockchain; Climate change; Climate models; Coal deposits; Distributed ledger; Energy resources; Power markets; Block-chain; Coal power plants; Distributed Energy Resources; Electricity production; Energy flexibility; Energy markets; German government; Operating reserve; Political decision; Renewable energies; Smart contract
PPRP: Preserving-Privacy Route Planning Scheme in VANETs,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133442117&doi=10.1145%2f3430507&partnerID=40&md5=a0f656cb2fd3e54d6e6717d2ee6d38ca,"Route planning helps a vehicle to share a message with the roadside units (RSUs) on its path in advance, which greatly speeds the authentication between the vehicle and the RSUs when the vehicle enters the RSUs' coverage. In addition, since only a small amount of necessary information needs to be shared between the vehicle and the RSUs, route planning can reduce the storage overhead of the vehicle's on-board unit (OBU) and the RSUs. However, the message sharing requires the assistance of the certification authority (CA), which will lead CA easily to obtain the vehicle's planning route. Although CA knows the vehicle's registration information and helps the vehicle to communicate with RSUs, it is unacceptable that the path of their vehicle is obtained by CA for most drivers. In fact, vehicle's sensitive information such as planning route, starting time, stop place, should be privacy for others including CA. Inspired with the method of oblivious transfer, a preserving-privacy route planning scheme in VANETs is proposed in this article, in which, a vehicle deduces the information of RSUs on its path with the help of CA, while CA knows nothing about which RSUs' information has been deduced by the vehicle. Later, fast authentication or other service is easily achieved between the vehicle and the RSUs (V2R) with the pre-shared information. After V2R authentication, vehicles could easily communicate with adjacent vehicles with the help of RSUs (V2V). Finally, compared with related schemes, performance evaluation illustrates the proposed scheme is better in terms of time consumption. © 2022 Association for Computing Machinery.",oblivious transfer; route planning privacy; VANETs,Authentication; Privacy-preserving techniques; Vehicular ad hoc networks; Certification authorities; Oblivious transfer; On-board units; Planning scheme; Roadside units; Route planning; Route planning privacy; Storage overhead; VANET; Vehicle registration; Vehicles
Deep Learning-Based Network Traffic Prediction for Secure Backbone Networks in Internet of Vehicles,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151538556&doi=10.1145%2f3433548&partnerID=40&md5=f09a367a5ea3365a3c1fe7875da9f637,"Internet of Vehicles (IoV), as a special application of Internet of Things (IoT), has been widely used for Intelligent Transportation System (ITS), which leads to complex and heterogeneous IoV backbone networks. Network traffic prediction techniques are crucial for efficient and secure network management, such as routing algorithm, network planning, and anomaly and intrusion detection. This article studies the problem of end-to-end network traffic prediction in IoV backbone networks, and proposes a deep learning-based method. The constructed system considers the spatio-temporal feature of network traffic, and can capture the long-range dependence of network traffic. Furthermore, a threshold-based update mechanism is put forward to improve the real-time performance of the designed method by using Q-learning. The effectiveness of the proposed method is evaluated by a real network traffic dataset. © 2022 Association for Computing Machinery.",deep learning; Internet of vehicles; network security; traffic prediction,Deep learning; Forecasting; Intelligent systems; Intelligent vehicle highway systems; Internet of things; Intrusion detection; Reinforcement learning; Traffic control; Vehicles; Back-bone network; Deep learning; Intelligent transportation systems; Internet of vehicle; Network traffic; Network traffic predictions; Networks security; Prediction techniques; Special applications; Traffic prediction; Network security
Efficient Cryptographic Hardware for Safety Message Verification in Internet of Connected Vehicles,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133867088&doi=10.1145%2f3431499&partnerID=40&md5=433140a7021414a06bfbd8ba4ab3c11c,"An important security requirement in automotive networks is to authenticate, sign, and verify thousands of short messages per second by each vehicle. This requirement mandates the use of a high speed Elliptic Curve Cryptography (ECC) hardware. The Residue Number Systems (RNS) provide a natural parallelism and carry-free operations that could speed-up long integer arithmetics of cryptographic algorithms. In this article, we propose a high-speed RNS Montgomery modular reduction units with parallel computing to reduce the latency of the field modular operations. We propose a fully RNS-based ECC scalar multiplication co-processor for NIST-P256r1 and Brainpool256r1 standard curves and improved the scalar multiplication speed using NAF and DBC numbering systems. Compared to the literature, our scheme provides faster computation without compromising the security level. The performance of our fully RNS-ECC point multiplication meets the requirements of the automotive industry.  © 2022 Association for Computing Machinery.",Cryptography; elliptic curve; Internet of connected vehicles; montgomery reduction; residue number system; safety; security; smart city,Automotive industry; Geometry; Numbering systems; Public key cryptography; Smart city; Cryptographic hardware; Curve cryptography; Elliptic curve; High Speed; Internet of connected vehicle; Montgomery reduction; Residue number system; Safety messages; Scalar multiplication; Security; Vehicles
Governance of Autonomous Agents on the Web: Challenges and Opportunities,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151794347&doi=10.1145%2f3507910&partnerID=40&md5=465b0fe009f36cbbb0213f5325af1e88,"The study of autonomous agents has a long history in the Multiagent System and the Semantic Web communities, with applications ranging from automating business processes to personal assistants. More recently, the Web of Things (WoT), which is an extension of the Internet of Things (IoT) with metadata expressed in Web standards, and its community provide further motivation for pushing the autonomous agents research agenda forward. Although representing and reasoning about norms, policies, and preferences is crucial to ensuring that autonomous agents act in a manner that satisfies stakeholder requirements, normative concepts, policies, and preferences have yet to be considered as first-class abstractions in Web-based multiagent systems. Towards this end, this article motivates the need for alignment and joint research across the Multiagent Systems, Semantic Web, and WoT communities, introduces a conceptual framework for governance of autonomous agents on the Web, and identifies several research challenges and opportunities.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Autonomous agents; governance; norms; policies; preferences,Internet of things; Multi agent systems; Semantic Web; Agent research; Business Process; Governance; Norm; Personal assistants; Preference; Research agenda; Semantic-Web; Web community; Web standards; Autonomous agents
Optimization-Based Predictive Congestion Control for the Tor Network: Opportunities and Challenges,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151556737&doi=10.1145%2f3520440&partnerID=40&md5=de9001d1220562c880a5f6176ae07ad3,"Based on the principle of onion routing, the Tor network achieves anonymity for its users by relaying user data over a series of intermediate relays. This approach makes congestion control in the network a challenging task. As of this writing, this results in higher latencies due to considerable backlog as well as unfair data rate allocation. In this article, we present a concept study of PredicTor, a novel approach to congestion control that tackles clogged overlay networks. Unlike traditional approaches, it is built upon the idea of distributed model predictive control, a recent advancement from the area of control theory. PredicTor is tailored to minimizing latency in the network and achieving max-min fairness. We contribute a thorough evaluation of its behavior in both toy scenarios to assess the optimizer and complex networks to assess its potential. For this, we conduct large-scale simulation studies and compare PredicTor to existing congestion control mechanisms in Tor. We show that PredicTor is highly effective in reducing latency and realizing fair rate allocations. In addition, we strive to bring the ideas of modern control theory to the networking community, enabling the development of improved, future congestion control. Thus, we demonstrate benefits and issues alike with this novel research direction. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",model predictive control; multi-hop congestion control; Tor network,Complex networks; Model predictive control; Network security; Toys; Concept studies; Data rate allocation; Model-predictive control; Multi-hop congestion control; Multi-hops; Onion routing; Optimisations; Tor networks; Traditional approaches; User data; Control theory
An Intent-driven DaaS Management Framework to Enhance User Quality of Experience,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151501201&doi=10.1145%2f3488586&partnerID=40&md5=a83d7a511317cd40dc645103388fd282,"Desktop as a Service (DaaS) has become widely used by enterprises. In 2020, the use of DaaS increased dramatically due to the demand to work remotely from home during the COVID-19 pandemic. The DaaS market is expected to continue growing rapidly [1]. The quality of experience (QoE) of a DaaS service has been one of the main factors to enhance DaaS user satisfaction. To ensure user QoE, the amount of cloud computation resources for a DaaS service must be appropriately designed. We propose an Intent-driven DaaS Management (IDM) framework to autonomously determine the cloud-resource-amount configurations for a given DaaS QoE requirement. IDM enables autonomous resource design by abstracting the knowledge about the dependency between DaaS workload, resource configuration, and performance from previous DaaS performance log data. To ensure the IDM framework's applicability to actual DaaS services, we analyzed five main challenges in applying the IDM framework to actual DaaS services: identifying the resource-design objective, quantifying DaaS QoE, addressing low log data availability, designing performance-inference models, and addressing low resource variations in the log data. We addressed these challenges through detailed designing of IDM modules. The effectiveness of the IDM framework was assessed from the aspects of DaaS performance-inference precision, DaaS resource design, and time and human-resource cost reduction. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cloud resource design; DaaS; intent-driven management,Cost reduction; Cloud resource design; Desktop as a service; Intent-driven management; Log data; Management frameworks; Performance; Service management; Service markets; Service performance; Users' satisfactions; Quality of service
Towards Semantic Management of On-Device Applications in Industrial IoT,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141396512&doi=10.1145%2f3510820&partnerID=40&md5=70de2ae2458fde136c28d4ba683311a3,"The Internet of Things (IoT) is revolutionizing the industry. Powered by pervasive embedded devices, the Industrial IoT (IIoT) provides a unique solution for retrieving and analyzing data near the source in real-time. Many emerging techniques, such as Tiny Machine Learning (TinyML) and Complex Event Processing (CEP), are actively being developed to support decision making at the edge, shifting the paradigm from centralized processing to distributed computing. However, distributed computing presents management challenges, as IoT devices are diverse and constrained, and their number is growing exponentially. The situation is even more challenging when various on-device applications (so-called artifacts) are deployed across decentralized IoT networks. Questions to be addressed include how to discover an appropriate function, whether that function can be executed on a certain device, and how to orchestrate a cross-platform service. To tackle these challenges, we propose an approach for the scalable management of on-device applications among distributed IoT devices. By leveraging the W3C Web of Things (WoT), the capabilities of each IoT device, or more precisely, its interaction patterns, can be semantically expressed in a Thing Description (TD). In addition, we introduce semantic modeling of on-device applications to supplement an TD with additional information regarding applications on the device. Specifically, we demonstrate two examples of semantic modeling: neural networks (NN) and CEP rules. The ontologies are evaluated by answering a set of competency questions. By hosting the enriched semantic knowledge of the entire IoT system in a Knowledge Graph (KG), we can discover and interoperate edge devices and artifacts across the decentralized network. This can reduce fragmentation and increase the reusability of IoT components. We demonstrate the feasibility of our concept on an industrial workstation consisting of a conveyor belt and several IoT devices. Finally, the requirements for constructing an IoT semantic management system are discussed. © 2022 Association for Computing Machinery.",complex event processing; Industrial IoT; neural network; semantic management; semantic modeling; Tiny machine learning,Complex networks; Decision making; Internet of things; Reusability; Semantic Web; Semantics; Complex event processing; Complex events; Device application; Event Processing; Industrial internet of thing; Machine-learning; Neural-networks; Semantic modelling; Semantics management; Tiny machine learning; Machine learning
Decentralized Dynamic Scheduling of TCPS Flows and a Simulator for Time-sensitive Networking,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148183389&doi=10.1145%2f3498729&partnerID=40&md5=a06616b20f29604ca1e9a8c9eaf9ced4,"Cybersickness and control-loop instabilities are two main concerns in Tactile Cyber-Physical Systems (TCPS). TCPS applications demand stringent bounds on end-to-end latencies to avoid their occurrences. Traditional best-effort networks cannot guarantee packet latencies in the presence of external traffic. However, emerging deterministic networks such as IEEE 802.1 Time-Sensitive Networking (TSN) can isolate time-critical flows from external traffic using IEEE 802.1Qbv Time-Aware Shaper (TAS) to guarantee bounded end-to-end packet latencies. In this work, we develop eDDSCH-TSN, a decentralized dynamic scheduling protocol to configure non-overlapping gate slots in TAS-enabled TSN switches to support TCPS flows. eDDSCH-TSN supports plug-and-play operation of compatible TCPS terminals with guaranteed minimal end-to-end packet latencies. Compared to the state-of-the-art, eDDSCH-TSN provides three orders lower end-to-end packet latencies for TCPS flows in mid-size networks with 10 hops between source and destination terminals. Further, we also present PYTSN, an open-source discrete-event TSN simulator that we use for evaluating eDDSCH-TSN. In particular, we use PYTSN to show the isolation of TCPS flows from external traffic and plug-and-play operation of TCPS terminals.  © 2022 Association for Computing Machinery.",network simulator; Tactile Cyber-Physical Systems; tactile internet; Time-Aware Shaper; Time-sensitive networking,Dynamics; Embedded systems; IEEE Standards; Cybe-physical systems; Cyber-physical systems; End to end; Network simulators; Packet latencies; System flow; Tactile cybe-physical system; Tactile internet; Time-aware shaper; Time-sensitive networking; Cyber Physical System
A Novel Approach to Enhance the End-to-End Quality of Service for Avionic Wireless Sensor Networks,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146645950&doi=10.1145%2f3520441&partnerID=40&md5=43d6b4d0aa89fcc405cff13b581dec29,"Going wireless is one of the key industrial trends, which assists the emergence of new manufacturing and maintenance processes by reducing the complexity and cost of physical equipment. However, the adoption of Wireless Sensor Networks (WSNs) in production environments is limited due to the strict Quality of Service (QoS) requirements of industrial applications. In particular, Wireless Avionics Intra-Communication (WAIC) systems operating in 4.3 GHz band are designed for intra-aircraft use cases with considerable restrictions on the transmission power of sensors, which results in multi-hop topologies, complicating a guaranteed QoS. The Internet Engineering Task Force (IETF) has developed the protocol stack IPv6 over the Time Slotted Channel Hopping (TSCH) mode of IEEE 802.15.4 (6TiSCH) based on the IEEE 802.15.4 Standard for Low-Rate Wireless Networks, which combines the TSCH reliability with ubiquitous IPv6 connectivity and with the robust Routing Protocol for Low-Power and Lossy Networks (RPL). The Scheduling Function (SF) is a core IPv6 over the TSCH mode of IEEE 802.15.4 (6TiSCH) component, but the specification of the SF is an open research topic: numerous scientific articles investigated how QoS for a wide range of applications can be met by developing specialized SFs. However, no full-scale information exchange between the layers of the 6TiSCH stack was considered to optimize the SFs and to improve the network performance. In this work, we propose a novel solution named 6TiSCH-CLX to satisfy demanding QoS requirements using cross-layer communication. It is an extension of the 6TiSCH framework at the network and Medium Access Control (MAC) layers, addressing latency and reliability challenges agnostic of the physical layer. 6TiSCH-CLX is evaluated both analytically and in simulations for several safety-critical avionic intra-communication use cases in WAIC. Preliminary results indicate considerable improvements to latency, while maintaining almost 100% Packet Delivery Ratio (PDR) without retransmissions and they highlight the capability of the cross-layer approach compared to existing solutions. © 2022 Copyright held by the owner/author(s).",6TiSCH; avionics; end-to-end QoS; IEEE 802.15.4; IPv6; MSF; RPL; scheduling function; TSCH; WAIC; WSN,Air traffic control; Avionics; IEEE Standards; Internet protocols; Low power electronics; Medium access control; Mobile telecommunication systems; Network layers; Safety engineering; Titanium compounds; Wireless sensor networks; 6TiSCH; Channel hopping; End-to-end quality of service; Ieee 802.15.4/zigbee; Ipv6; MSF; RPL; Scheduling functions; Slotted channels; Time slotted channel hopping; Wireless avionic intra-communication; Quality of service
Driver Identification Using Optimized Deep Learning Model in Smart Transportation,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151536831&doi=10.1145%2f3412353&partnerID=40&md5=e661bbac8f0f604b472184a11321415c,"The Intelligent Transportation System (ITS) is said to revolutionize the travel experience by making it safe, secure, and comfortable for the people. Although vehicles have been automated up to a certain extent, it still has critical security issues that require thorough study and advanced solutions. The security vulnerabilities of ITS allows the attacker to steal the vehicle. Therefore, the identification of drivers is required in order to develop a safe and secure system so that the vehicles can be protected from theft. There are two ways in which a driver can be identified: 1) face recognition of the driver, and 2) based on driving behavior. Face recognition includes image processing of 2-D images and learning of the features, which require high computational power. Drivers are known to have unique driving styles, whose data can be captured by the sensors. Therefore, the second method identifies drivers based on the analysis of the sensor data and it requires comparatively lesser computational power. In this paper, an optimized deep learning model is trained on the sensor data to correctly identify the drivers. The Long Short-Term Memory (LSTM) deep learning model is optimized for better performance. The novelty of the approach in this work is the inclusion of hyperparameter tuning using a nature-inspired optimization algorithm, which is an important and essential step in discovering the optimal hyperparameters for training the model which in turn increases the accuracy. The CAN-BUS dataset is used for experimentation and evaluation of the training model. Evaluation parameters such as accuracy, precision score, F1 score, and ROC AUC curve are considered to evaluate the performance of the model. © 2022 Association for Computing Machinery.",Auto -theft systems; crow search algorithm; hyperparameter tuning; Intelligent Transportation Systems (ITS); LSTM; security,Behavioral research; Biomimetics; Crime; Face recognition; Intelligent systems; Intelligent vehicle highway systems; Learning systems; Vehicles; Auto -theft system; Computational power; Crow search algorithm; Hyper-parameter; Hyperparameter tuning; Intelligent transportation system; Intelligent transportation systems; Learning models; Search Algorithms; Security; Long short-term memory
Conditional Identity Privacy-preserving Authentication Scheme Based on Cooperation of Multiple Fog Servers under Fog Computing-based IoVs,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144988457&doi=10.1145%2f3538381&partnerID=40&md5=14247bdde76eb9c379c44fab213e6468,"Internet of vehicles (IoVs) is a variant of vehicular ad hoc network, which provides an efficient communication method for vehicles. However, some traffic messages usually include sensitive identity information, which is easy to bring about the leakage of vehicular identities during data communications. Further, if vehicular identities are fully protected, then it can lead to trusted authority cannot reveal the real identities of malicious vehicles, which incurs more security issues in IoVs. Therefore, in this article, we propose an efficient conditional identity privacy-preserving authentication scheme based on cooperation of multiple fog servers under fog computing-based IoVs, where fog servers are used to verify (authenticate) the legitimacy of vehicles without revealing their real identities. Further, an associated vehicular identity updating mechanism is constructed to solve the problem that some compromised fog servers may leak their stored verification information to pool real vehicular identities. Additionally, a malicious vehicular identity tracing mechanism is proposed to support related fog servers that receive signed false messages can trace the real identities of malicious vehicles. Compared with other related schemes, our scheme further improves its security. Experimental results show our scheme is efficient under fog computing-based IoVs.  © 2022 Association for Computing Machinery.",authentication; fog computing; identity privacy-preserving; Internet of vehicles,Authentication; Fog; Fog computing; Privacy-preserving techniques; Vehicle to vehicle communications; Vehicular ad hoc networks; Authentication scheme; Communication method; Data-communication; Efficient communications; Identity information; Identity privacy-preserving; Internet of vehicle; Privacy preserving; Privacy-preserving authentication; Vehicular Adhoc Networks (VANETs); Vehicles
A Reputation-based Framework for Honest Provenance Reporting,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151852420&doi=10.1145%2f3507908&partnerID=40&md5=88b98240c2ba6edde7a9312092c27a33,"Given the distributed, heterogenous, and dynamic nature of service-based IoT systems, capturing circumstances data underlying service provisions becomes increasingly important for understanding process flow and tracing how outputs came about, thus enabling clients to make more informed decisions regarding future interaction partners. Whilst service providers are the main source of such circumstances data, they may often be reluctant to release it, e.g., due to the cost and effort required, or to protect their interests. In response, this article introduces a reputation-based framework, guided by intelligent software agents, to support the sharing of truthful circumstances information by providers. In this framework, assessor agents, acting on behalf of clients, rank and select service providers according to reputation, while provider agents, acting on behalf of service providers, learn from the environment and adjust provider's circumstances provision policies in the direction that increases provider profit with respect to perceived reputation. The novelty of the reputation assessment model adopted by assessor agents lies in affecting provider reputation scores by whether or not they reveal truthful circumstances data underlying their service provisions, in addition to other factors commonly adopted by existing reputation schemes. The effectiveness of the proposed framework is demonstrated through an agent-based simulation including robustness against a number of attacks, with a comparative performance analysis against FIRE as a baseline reputation model.  © 2022 Association for Computing Machinery.",circumstances; honest reporting; provenance; Reputation,Circumstance; Dynamic nature; Honest reporting; Informed decision; Process flows; Provenance; Reputation; Service provider; Service provisions; Service-based; Software agents
EECDN: Energy-efficient Cooperative DNN Edge Inference in Wireless Sensor Networks,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142759822&doi=10.1145%2f3544969&partnerID=40&md5=9b5224a0da9972e9bfd471bc3dd53860,"Multi-access edge computing (MEC) is emerging to improve the quality of experience of mobile devices including internet of things sensors by offloading computing intensive tasks to MEC servers. Existing MEC-enabled cooperative computation offloading works focus on the optimization of total energy consumption but fail to exploit multi-relay diversity and min-max fairness of energy consumption on participated sensors. We explore a typical wireless sensor network with multi-source, multi-relay, and one edge server, where relay nodes can provide both cooperative communication and computation services. We divide the energy efficiency optimization problem into two sub-problems: One is to minimize the weighted average total energy consumption per time slot, and the other is to minimize the maximum weighted energy consumption. For the first sub-problem, we propose an optimal algorithm named as optimal weighted average total energy consumption algorithm (OTCA) based on bipartite matching. For the second sub-problem, greedy algorithm for fairness guarantee (GAF) is proposed with an approximation ratio of (1 + ϵ), where ϵ is a small positive constant. Extensive numerical results show that OTCA outperforms the baseline algorithms by 26.7-77.4% on the average total weighted energy consumption while GAF outperforms benchmark algorithms by 30.7-84.4%. NS-3 simulation experiments comply with numerical results. © 2022 Association for Computing Machinery.",cooperative communication; energy efficiency; Multi-access edge computing; offloading; wireless sensor networks,Approximation algorithms; Computation offloading; Cooperative communication; Energy efficiency; Quality of service; Sensor nodes; Statistical methods; Edge computing; Energy-consumption; Fairness guarantee; Greedy algorithms; Multi-access edge computing; Multiaccess; Offloading; Sub-problems; Total energy; Weighted averages; Energy utilization
Resilient Distributed Constraint Reasoning to Autonomously Configure and Adapt IoT Environments,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142070640&doi=10.1145%2f3507907&partnerID=40&md5=aac219145f1427947977ee458c72a495,"In this article, we investigate multi-agent techniques to install autonomy and adaptation in IoT-based smart environment settings, like smart home scenarios. We particularly make use of the smart environment configuration problem (SECP) framework, and map it to a distributed optimization problem (DCOP). This consists in enabling smart objects to coordinate and self-configure as to meet both user-defined requirements and energy efficiency, by operating a distributed constraint reasoning process over a computation graph. As to cope with the dynamics of the environment and infrastructure (e.g., by adding or removing devices), we also specify the k-resilient distribution of graph-structured computations supporting agent decisions, over dynamic and physical multi-agent systems. We implement a self-organizing distributed repair method, based on a distributed constraint optimization algorithm to adapt the distribution as to ensure the system still performs collective decisions and remains resilient to upcoming changes. We provide a full stack of mechanisms to install resilience in operating stateless DCOP solution methods, which results in a robust approach using a fast DCOP algorithm to repair any stateless DCOP solution methods at runtime. We experimentally evaluate the performances of these techniques when operating stateless DCOP algorithms to solve SECP instances. © 2022 Association for Computing Machinery.",adaptation; DCOP; IoT; resilience; smart home,Automation; Autonomous agents; Constrained optimization; Energy efficiency; Internet of things; Repair; Adaptation; Constraint reasoning; DCOP; Distributed constraints; IoT; Multi-agent techniques; Resilience; Smart environment; Smart homes; Solution methods; Multi agent systems
Providing Reliable Service for Parked-vehicle-assisted Mobile Edge Computing,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139212216&doi=10.1145%2f3514242&partnerID=40&md5=1da18718ad274da09298aafb101a6f2f,"Nowadays, a growing number of computation-intensive applications appear in our daily life. Those applications make the loads of both the core network and the mobile devices, in terms of energy and bandwidth, hugely increase. Offloading computation-intensive tasks to edge cloud is proposed to address this issue. Since edge clouds have limited computation resources compared with the remote cloud, they would get over-loaded because of the heavy computation burden. Parked-vehicle-assisted mobile edge computing becomes one of the promising solutions for this problem. However, several critical issues in parked-vehicle-assisted mobile edge computing would result in low reliable edge service. The open environment would bring about uncertainty, and the data privacy is hard to ensure. In addition, different from edge cloud, each parked vehicle only has limited parking duration and can leave unexpectedly for personal reasons. Moreover, edge cloud and vehicle adopt different execution models of computation and communication. The heterogeneous environment may result in negative effect on cooperativeness. Ignoring those issues can result in substantial performance degradation. To tackle this challenge and explore the benefits of parked-vehicle-assisted offloading, we study the task offloading and resource-allocation problem by fully considering the above issues. First, we propose a resource-management scheme to address the privacy issue. Second, we review the execution model of computation and communication in parked-vehicle-assisted computation offloading. Then, we formulate the problem into a mixed-integer nonlinear programming. The problem is hard to tackle due to its non-convex nature, which means that the time complexity of finding global optimal solution is unaffordable. Finally, we decompose the original problem into two sub-problems with lower complexity, and related algorithms are given to deal with the sub-problems. Simulation results demonstrate the effectiveness of the proposed solution.  © 2022 Association for Computing Machinery.",Mobile edge computing; parked vehicle; reliable service; resource allocation; task offloading,Complex networks; Computation offloading; Computer architecture; Data privacy; Integer programming; Mobile edge computing; Nonlinear programming; Vehicle to vehicle communications; Vehicles; Computation intensives; Daily lives; Edge clouds; Execution modeling; Model of computation; Parked vehicle; Reliable service; Resources allocation; Sub-problems; Task offloading; Resource allocation
CARES: Context-Aware Trust Estimation for Realtime Crowdsensing Services in Vehicular Edge Networks,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128458547&doi=10.1145%2f3514243&partnerID=40&md5=1ade38e490049d10d878d8e93ea82386,"The growing number of smart vehicles makes it possible to envision a crowdsensing service where vehicles can share video data of their surroundings for seeking out traffic conditions and car accidents ahead. However, the service may need to deal with situations like malicious vehicles propagating false information to divert other vehicles to arrive at destinations earlier or lead them to dangerous locations. This article proposes a context-aware trust estimation scheme that can allow roadside units in a vehicular edge network to provide real-time crowdsensing services in a reliable manner by selectively using information from trustworthy sources. Our proposed scheme is novel in that its trust estimation does not require any prior knowledge of vehicles on roads but quickly obtains the accurate trust value of each vehicle by leveraging transfer learning. and its Q-learning-based dynamic adjustment scheme autonomously estimates trust levels of oncoming vehicles with the aim of detecting malicious vehicles and accordingly filtering out untrustworthy input from them. Based on an extensive simulation study, we prove that the proposed scheme outperforms existing ones in terms of malicious vehicle detection accuracy.  © 2022 Association for Computing Machinery.",adversarial attacks; crowdsensing; reinforcement learning; transfer learning; Trust; vehicular edge networks,Accidents; Deep learning; Knowledge management; Transfer learning; Vehicles; Adversarial attack; Context-Aware; Crowdsensing; EDGE Networks; Real- time; Reinforcement learnings; Smart vehicles; Transfer learning; Trust; Vehicular edge network; Reinforcement learning
Pedestrian Trajectory Prediction in Heterogeneous Traffic using Facial Keypoints-based Convolutional Encoder-decoder Network,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151514364&doi=10.1145%2f3410444&partnerID=40&md5=3998ac42ebaf2e91b99b35c2dea6cd0a,"Future pedestrian trajectory prediction offers great prospects for many practical applications such as unmanned vehicles, building evacuation design and robotic path planning. Most existing methods focus on social interaction among pedestrians but ignore the fact that heterogeneous traffic objects (cars, dogs, bicycles, motorcycles, etc.) have significant influence on the future trajectory of a subject pedestrian. Also, the walking direction intention of a pedestrian may be referred by his/her facial keypoints. Considering this, this work proposes to predict a pedestrian's future trajectory by jointly using neighboring heterogeneous traffic information and his/her facial keypoints. To fulfill this, an end-to-end facial keypoints-based convolutional encoder-decoder network (FK-CEN) is designed, in which the heterogeneous traffic and facial keypoints are input. After training, FK-CEN is evaluated on 5 crowded video sequences collected from the public datasets MOT-16 and MOT-17. Experimental results demonstrate that it outperforms state-of-the-art approaches, in terms of prediction errors. © 2022 Association for Computing Machinery.",attention; convolutional long-short-term memory; encoder-decoder; pedestrian intention; Social-interaction,Convolution; Decoding; Forecasting; Motion planning; Network coding; Robot programming; Unmanned vehicles; Attention; Convolutional encoders; Convolutional long-short-term memory; Encoder-decoder; Heterogeneous traffic; Keypoints; Pedestrian intention; Pedestrian trajectories; Social interactions; Trajectory prediction; Trajectories
Edge Computing AI-IoT Integrated Energy-efficient Intelligent Transportation System for Smart Cities,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144902186&doi=10.1145%2f3507906&partnerID=40&md5=f0edef90eb8699b54f7e22b0d8f5e833,"With the advancement of information and communication technologies (ICTs), there has been high-scale utilization of IoT and adoption of AI in the transportation system to improve the utilization of energy, reduce greenhouse gas (GHG) emissions, increase quality of services, and provide many extensive benefits to the commuters and transportation authorities. In this article, we propose a novel edge-based AI-IoT integrated energy-efficient intelligent transport system for smart cities by using a distributed multi-agent system. An urban area is divided into multiple regions, and each region is sub-divided into a finite number of zones. At each zone an optimal number of RSUs are installed along with the edge computing devices. The MAS deployed at each RSU collects a huge volume of data from the various sensors, devices, and infrastructures. The edge computing device uses the collected raw data from the MAS to process, analyze, and predict. The predicted information will be shared with the neighborhood RSUs, vehicles, and cloud by using MAS with the help of IoT. The predicted information can be used by freight vehicles to maintain smooth and steady movement, which results in reduction in GHG emissions and energy consumption, and finally improves the freight vehicles' mileage by reducing traffic congestion in the urban areas. We have exhaustively carried out the simulation results and demonstrated the effectiveness of the proposed system.  © 2022 Association for Computing Machinery.",Artificial intelligence; cloud computing; cyber physical systems; edge computing; Internet of Things; ITS; MAS,Cyber Physical System; Edge computing; Embedded systems; Energy efficiency; Energy utilization; Freight transportation; Gas emissions; Greenhouse gases; Intelligent agents; Intelligent systems; Intelligent vehicle highway systems; Motor transportation; Multi agent systems; Smart city; Traffic congestion; Vehicles; Cloud-computing; Computing devices; Cybe-physical systems; Cyber-physical systems; Edge computing; Energy efficient; Greenhouse gas emissions; ITS; MAS; Urban areas; Internet of things
Web of Digital Twins,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132159083&doi=10.1145%2f3507909&partnerID=40&md5=058c3ca6058140d27118fa97186ec742,"In recent years, digital twins have been pervading different application domains - from manufacturing to healthcare - as an approach for virtualising different kinds of physical entities (things, products, machines). The dominant view developed in the literature so far is about the virtualisation of individual physical assets in a closed-system perspective. In this article, we introduce and explore a broader perspective that we call Web of Digital Twins (WoDT), in which the digital twin paradigm is exploited for the pervasive softwarisation of possibly large-scale interrelated physical realities. A WoDT can be conceived as an open, distributed and dynamic ecosystem of connected digital twins, functioning as an interoperable service-oriented layer for applications running on top, especially smart applications and multiagent systems. The article introduces an abstract model and architecture aimed to capture key aspects of the idea not bound to any specific application domains or implementing technologies and discusses their adoption in engineering real-world systems. To this purpose, two concrete case studies are considered, in the context of healthcare and smart mobility. Finally, the article includes a discussion of a selected set of research directions. © 2022 Association for Computing Machinery.",agents; Digital twins; MAS; web; WoDT,Abstracting; Interoperability; Multi agent systems; Applications domains; Closed systems; Interoperable services; Large-scales; MAS; Physical assets; Physical reality; Virtualizations; Web; Web of digital twin; Health care
Improving Security of Internet of Vehicles Based on Post-quantum Signatures with Systolic Divisions,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151793297&doi=10.1145%2f3410445&partnerID=40&md5=00ead4941bf66f2ef9547a5040e455df,"Internet of Things (IoT) techniques have been employed in many areas, e.g., vehicles, smart home, and medicine. Among the applications of IoTs, the Internet of Vehicles (IoV) is one of the most popular techniques. IoVs are protected by public key cryptographic systems, such as RSA and ECC. However, such systems are vulnerable to quantum computer attacks. Thus, we improve the security of IoV-based post-quantum signatures, which can resist quantum computer attacks. The key operations are divisions in a finite field. Hence, we improve the security of IoV-based post-quantum signatures with division by employing systolic architectures. We propose a systolic architecture for computing division in composite fields. After that, we improve the IoT security-based post-quantum signatures with systolic divisions. We test and verify our design on a Field-Programmable Gate Array (FPGA); the experimental results confirm our estimates. Furthermore, the optimized method proposed can be further applied to various applications like solving system of linear equations and cryptographic applications for IoT security.  © 2022 Association for Computing Machinery.",Internet of Things (IoT); Internet of Vehicles (IoV); post-quantum signature; systolic division,Authentication; Automation; Computer architecture; Cryptography; Field programmable gate arrays (FPGA); Network security; Vehicles; Computer attacks; Internet of thing; Internet of vehicle; Post quantum; Post-quantum signature; Quanta computers; Quantum signature; Smart homes; Systolic architecture; Systolic division; Internet of things
esDNN: Deep Neural Network Based Multivariate Workload Prediction in Cloud Computing Environments,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137472800&doi=10.1145%2f3524114&partnerID=40&md5=7ea57974f22112aff4b5238370683a26,"Cloud computing has been regarded as a successful paradigm for IT industry by providing benefits for both service providers and customers. In spite of the advantages, cloud computing also suffers from distinct challenges, and one of them is the inefficient resource provisioning for dynamic workloads. Accurate workload predictions for cloud computing can support efficient resource provisioning and avoid resource wastage. However, due to the high-dimensional and high-variable features of cloud workloads, it is difficult to predict the workloads effectively and accurately. The current dominant work for cloud workload prediction is based on regression approaches or recurrent neural networks, which fail to capture the long-term variance of workloads. To address the challenges and overcome the limitations of existing works, we proposed an efficient supervised learning-based Deep Neural Network (esDNN) approach for cloud workload prediction. First, we utilize a sliding window to convert the multivariate data into a supervised learning time series that allows deep learning for processing. Then, we apply a revised Gated Recurrent Unit (GRU) to achieve accurate prediction. To show the effectiveness of esDNN, we also conduct comprehensive experiments based on realistic traces derived from Alibaba and Google cloud data centers. The experimental results demonstrate that esDNN can accurately and efficiently predict cloud workloads. Compared with the state-of-the-art baselines, esDNN can reduce the mean square errors significantly, e.g., 15%. rather than the approach using GRU only. We also apply esDNN for machines auto-scaling, which illustrates that esDNN can reduce the number of active hosts efficiently, thus the costs of service providers can be optimized.  © 2022 Association for Computing Machinery.",auto-scaling; Cloud computing; gate recurrent unit; supervised learning; workloads prediction,Data handling; Deep neural networks; Forecasting; Mean square error; Recurrent neural networks; Supervised learning; Auto-scaling; Cloud computing environments; Cloud-computing; Gate recurrent unit; High-dimensional; IT industry; Network-based; Scalings; Service provider; Workload predictions; Cloud computing
DECENT: A Decentralized Configurator for Controlling Elasticity in Dynamic Edge Networks,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137648276&doi=10.1145%2f3530692&partnerID=40&md5=8e860a63adb7aa62c38b6d8dbac2a5f8,"Recent advancements in distributed systems have enabled deploying low-latency and highly resilient edge applications close to the IoT domain at the edge of the network. The broad range of edge application requirements combined with heterogeneous, resource-constrained, and dynamic edge networks make it particularly challenging to configure and deploy them. Besides that, missing elastic capabilities on the edge makes it difficult to operate such applications under dynamic workloads. To this end, this article proposes a lightweight, self-adaptive, and decentralized mechanism (DECENT) for (1) deploying edge applications on edge resources and on premises of Edge-Cloud infrastructure and (2) controlling elasticity requirements. DECENT enables developers to characterize their edge applications by specifying elasticity requirements, which are automatically captured, interpreted, and enforced by our decentralized elasticity interpreters. In response to dynamic workloads, edge applications automatically adapt in compliance with their elasticity requirements. We discuss the architecture, processes of the approach, and the experiment conducted on a real-world testbed to validate its feasibility on low-powered edge devices. Furthermore, we show performance and adaptation aspects through an edge safety application and its evolution in elasticity space (i.e., cost, resource, and quality). © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",edge computing; edge-cloud; Elasticity; Internet of Things (IoT),Edge computing; Elasticity; Application requirements; Configurator; Decentralised; Distributed systems; Edge clouds; Edge computing; EDGE Networks; Highly resilient; Internet of thing; Low latency; Internet of things
The SemIoTic Ecosystem: A Semantic Bridge between IoT Devices and Smart Spaces,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137750702&doi=10.1145%2f3527241&partnerID=40&md5=5851ddf9ab943f0ce224db069ed3fcdd,"Smart space administration and application development is challenging in part due to the semantic gap that exists between the high-level requirements of users and the low-level capabilities of IoT devices. The stakeholders in a smart space are required to deal with communicating with specific IoT devices, capturing data, processing it, and abstracting it out to generate useful inferences. Additionally, this makes reusability of smart space applications difficult, since they are developed for specific sensor deployments. In this article, we present a holistic approach to IoT smart spaces, the SemIoTic ecosystem, to facilitate application development, space management, and service provision to its inhabitants. The ecosystem is based on a centralized repository, where developers can advertise their space-agnostic applications, and a SemIoTic system deployed in each smart space that interacts with those applications to provide them with the required information. SemIoTic applications are developed using a metamodel that defines high-level concepts abstracted from the smart space about the space itself and the people within it. Application requirements can be expressed then in terms of user-friendly high-level concepts, which are automatically translated by SemIoTic into sensor/actuator commands adapted to the underlying device deployment in each space. We present a reference implementation of the ecosystem that has been deployed at the University of California, Irvine and is abstracting data from hundreds of sensors in the space and providing applications to campus members.  © 2022 Association for Computing Machinery.",Internet-of-things; open APIs; privacy; semantic interoperability,Abstracting; Data handling; Ecosystems; Reusability; Semantic Web; Semantics; Space applications; Application development; Capturing data; Open API; Privacy; Semantic gap; Semantic interoperability; Smart space; Smart space applications; Space administration; Specific sensors; Internet of things
On Optimizing Transaction Fees in Bitcoin using AI: Investigation on Miners Inclusion Pattern,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137826317&doi=10.1145%2f3528669&partnerID=40&md5=2ac54960c11a21a0e538ef2817a788c8,"The transaction-rate bottleneck built into popular proof-of-work (PoW)-based cryptocurrencies, like Bitcoin and Ethereum, leads to fee markets where transactions are included according to a first-price auction for block space. Many attempts have been made to adjust and predict the fee volatility, but even well-formed transactions sometimes experience unexpected delays and evictions unless a substantial fee is offered. In this article, we propose a novel transaction inclusion model that describes the mechanisms and patterns governing miners decisions to include individual transactions in the Bitcoin system. Using this model we devise a Machine Learning (ML) approach to predict transaction inclusion. We evaluate our predictions method using historical observations of the Bitcoin network from a five month period that includes more than 30 million transactions and 120 million entries. We find that our Machine Learning (ML) model can predict fee volatility with an accuracy of up to 91%. Our findings enable Bitcoin users to improve their fee expenses and the approval time for their transactions.  © 2022 Copyright held by the owner/author(s).",Bitcoin; blockchain; cryptocurrencies; fee market; first price auction; machine learning; proof-of-work; transaction inclusion,Blockchain; Forecasting; Machine learning; Miners; Block space; Block-chain; Fee market; First price auction; Inclusion models; Machine-learning; Proof of work; Transaction inclusion; Transaction rates; Well-formed transactions; Bitcoin
Secure and Efficient Hybrid Data Deduplication in Edge Computing,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137765455&doi=10.1145%2f3537675&partnerID=40&md5=db291e374cb362e8d875b9b07c11ed06,"As an extension of cloud computing, edge computing introduces additional intermediate devices, called edge nodes near clients, providing computing services on behalf of the central cloud more efficiently. Although edge computing brings several benefits such as low latency and bandwidth savings on the edge side, rapid increase in the amount of data transmitted to the central cloud hinders efficient utilization of the storage system on the central cloud side especially when the data from edge devices are encrypted. To mitigate this issue in a privacy-preserving manner, data deduplication techniques for encrypted data have been extensively studied to enhance both the security and efficiency in the conventional cloud system with two different approaches. A server-side secure deduplication approach protects data privacy but impairs network efficiency by allowing duplicate uploads, while a client-side one improves network efficiency but suffers from potential information leakage due to its vulnerability to the side-channel attack. In this article, we propose a hybrid secure deduplication scheme for edge computing, which guarantees both advantages of the aforementioned two approaches. Specifically, our scheme guarantees data privacy by applying the server-side deduplication technique between the client and the edge nodes and maximizes network efficiency through the client-side deduplication technique between the edge nodes and the cloud. In addition, we devise a novel additively homomorphic encryption for efficient deduplication operations in the resource-limited edge nodes. Based on our experimental results, the proposed scheme reduces the communication costs by approximately 2.5 times for a storage server when the duplicate ratio is 50%, and the response time is reduced by about 2 times when the data size is 16 MB.  © 2022 Association for Computing Machinery.",cloud computing; edge computing; key sharing protocol; Secure data deduplication,Digital storage; Efficiency; Network security; Privacy-preserving techniques; Side channel attack; Cloud-computing; Data de duplications; Deduplication; Edge computing; Edge nodes; Key sharing; Key sharing protocol; Network efficiency; Secure data deduplication; Server sides; Edge computing
Modelling and Analysing Replica- and Fault-aware Management of Horizontally Scalable Applications,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134339165&doi=10.1145%2f3511302&partnerID=40&md5=8805fce64f47acde3ef25e3ba9a76ab1,"Modern enterprise applications integrate multiple interdependent software components, whose management must be suitably coordinated. This must be done by taking into account all inter-component dependencies, the faults potentially affecting them, and the fact that each component can be horizontally scaled, i.e., that multiple instances of each component can be spawned or destroyed, depending on application needs. In this article, we introduce a novel solution for suitably modelling and analysing the replica- and fault-aware management of multi-component applications, based on topology graphs and management protocols. More precisely, we first introduce a compositional model of the management behaviour of the (possibly multiple) instances of the components forming an application, faults included. We then show how this model enables automating various useful analyses, from checking the validity of management plans to automatically determining management plans allowing the instance of an application to reach and maintain a desired target configuration.  © 2022 Association for Computing Machinery.",Application management; fault resilience; finite state machines; horizontal scaling; management planning,Application programs; Application management; Enterprise applications; Fault resilience; Finite states machine; Horizontal scaling; Inter-component dependencies; Management planning; Management plans; Multiple instances; Software-component; Topology
SkillBot: Identifying Risky Content for Children in Alexa Skills,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137876078&doi=10.1145%2f3539609&partnerID=40&md5=5f113d8bbbb0ff01ba033a44f58399d4,"Many households include children who use voice personal assistants (VPA) such as Amazon Alexa. Children benefit from the rich functionalities of VPAs and third-party apps but are also exposed to new risks in the VPA ecosystem. In this article, we first investigate ""risky""child-directed voice apps that contain inappropriate content or ask for personal information through voice interactions. We build SkillBot - a natural language processing-based system to automatically interact with VPA apps and analyze the resulting conversations. We find 28 risky child-directed apps and maintain a growing dataset of 31,966 non-overlapping app behaviors collected from 3,434 Alexa apps. Our findings suggest that although child-directed VPA apps are subject to stricter policy requirements and more intensive vetting, children remain vulnerable to inappropriate content and privacy violations. We then conduct a user study showing that parents are concerned about the identified risky apps. Many parents do not believe that these apps are available and designed for families/kids, although these apps are actually published in Amazon's ""Kids""product category. We also find that parents often neglect basic precautions, such as enabling parental controls on Alexa devices. Finally, we identify a novel risk in the VPA ecosystem: confounding utterances or voice commands shared by multiple apps that may cause a user to interact with a different app than intended. We identify 4,487 confounding utterances, including 581 shared by child-directed and non-child-directed apps. We find that 27% of these confounding utterances prioritize invoking a non-child-directed app over a child-directed app. This indicates that children are at real risk of accidentally invoking non-child-directed apps due to confounding utterances.  © 2022 Association for Computing Machinery.",automated system; Child safety; parents' perceptions; risky voice app,Automation; Natural language processing systems; Automated systems; Child safety; Exposed to; Natural languages; Parent' perception; Personal assistants; Personal information; Risky voice app; Third parties; Voice interaction; Ecosystems
On the Neural Backdoor of Federated Generative Models in Edge Computing,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130375734&doi=10.1145%2f3425662&partnerID=40&md5=0ebbd68bb2e849db9eb4c3214ff13977,"Edge computing, as a relatively recent evolution of cloud computing architecture, is the newest way for enterprises to distribute computational power and lower repetitive referrals to central authorities. In the edge computing environment, Generative Models (GMs) have been found to be valuable and useful in machine learning tasks such as data augmentation and data pre-processing. Federated learning and distributed learning refer to training machine learning models in the edge computing network. However, federated learning and distributed learning also bring additional risks to GMs since all peers in the network have access to the model under training. In this article, we study the vulnerabilities of federated GMs to data-poisoning-based backdoor attacks via gradient uploading. We additionally enhance the attack to reduce the required poisonous data samples and cope with dynamic network environments. Last but not least, the attacks are formally proven to be stealthy and effective toward federated GMs. According to the experiments, neural backdoors can be successfully embedded by including merely poisonous samples in the local training dataset of an attacker.  © 2021 Association for Computing Machinery.",cloud computing; Deep learning; edge computing; federated learning; generative neural networks; neural backdoor,Computer architecture; Data handling; Deep neural networks; Backdoors; Cloud-computing; Deep learning; Distributed learning; Edge computing; Federated learning; Generative model; Generative neural network; Neural backdoor; Neural-networks; Edge computing
IoT-based Cloud Service for Secured Android Markets using PDG-based Deep Learning Classification,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130317059&doi=10.1145%2f3418206&partnerID=40&md5=6dbec3d894c140883a1fc7ff2057d678,"Software piracy is an act of illegal stealing and distributing commercial software either for revenue or identify theft. Pirated applications on Android app stores are harming developers and their users by clone scammers. The scammers usually generate pirated versions of the same applications and publish them in different open-source app stores. There is no centralized system between these app stores to prevent scammers from publishing pirated applications. As most of the app stores are hosted on cloud storage, therefore a cloud-based interaction system can prevent scammers from publishing pirated applications. In this paper, we proposed IoT-based cloud architecture for clone detection using program dependency analysis. First, the newly submitted APK and possible original files are selected from app stores. The APK Extractor and JDEX decompiler extract APK and DEX files for Java source code analysis. The dependency graphs of Java files are generated to extract a set of weighted features. The Stacked-Long Short-Term Memory (S-LSTM) deep learning model is designed to predict possible clones.Experimental results have shown that the proposed approach can achieve an average accuracy of 95.48% among clones from different application stores.  © 2021 Association for Computing Machinery.",Clone detection; cloud services; deep learning; Internet of Things; program dependency graph,Android (operating system); Cloning; Crime; Java programming language; Long short-term memory; Open source software; Web services; Android apps; Android markets; App stores; Centralized systems; Clone detection; Cloud services; Commercial software; Deep learning; Open-source; Program dependency graphs; Internet of things
A Shared Two-way Cybersecurity Model for Enhancing Cloud Service Sharing for Distributed User Applications,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130369452&doi=10.1145%2f3430508&partnerID=40&md5=0e6288d961e606ebbb0368920bfab24b,"Cloud services provide decentralized and pervasive access for resources to reduce the complex infrastructure requirements of the user. In decentralized service access, the implication of security is tedious to match the user requirements. Therefore, cloud services incorporate cybersecurity measures for administering standard resource access to users. In this paper, a shared two-way security model (STSM) is proposed to provide adaptable service security for the end-users. In this security model, a cooperative closed access session for information sharing between the cloud and end-user is designed with the help of cybersecurity features. This closed access provides less complex authentication for users and data that is capable of matching the verifications of the cloud services. A deep belief learning algorithm is used to differentiate the cooperative and non-cooperative secure sessions between the users and the cloud to ensure closed access throughout the data sharing time. The output of the belief network decides the actual session time between the user and the cloud, improving the span of the sharing session. Besides, the proposed model reduces false alarm, communication failures, under controlled complexity.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Belief learning; cloud services; cybersecurity; service authentication; user verification,Authentication; Complex networks; Cybersecurity; Deep learning; Distributed database systems; Learning algorithms; Belief learning; Closed access; Cloud services; Cyber security; Decentralised; End-users; Security modeling; Service authentication; Two ways; User verification; Web services
Analysis of Trending Topics and Text-based Channels of Information Delivery in Cybersecurity,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130401033&doi=10.1145%2f3483332&partnerID=40&md5=1b44789c2a1de06eee93841cbc623fb4,"Computer users are generally faced with difficulties in making correct security decisions. While an increasingly fewer number of people are trying or willing to take formal security training, online sources including news, security blogs, and websites are continuously making security knowledge more accessible. Analysis of cybersecurity texts from this grey literature can provide insights into the trending topics and identify current security issues as well as how cyber attacks evolve over time. These in turn can support researchers and practitioners in predicting and preparing for these attacks. Comparing different sources may facilitate the learning process for normal users by creating the patterns of the security knowledge gained from different sources. Prior studies neither systematically analysed the wide range of digital sources nor provided any standardisation in analysing the trending topics from recent security texts. Moreover, existing topic modelling methods are not capable of identifying the cybersecurity concepts completely and the generated topics considerably overlap. To address this issue, we propose a semi-automated classification method to generate comprehensive security categories to analyse trending topics. We further compare the identified 16 security categories across different sources based on their popularity and impact. We have revealed several surprising findings as follows: (1) The impact reflected from cybersecurity texts strongly correlates with the monetary loss caused by cybercrimes, (2) security blogs have produced the context of cybersecurity most intensively, and (3) websites deliver security information without caring about timeliness much.  © 2021 Association for Computing Machinery.",cybersecurity topics; Empirical study; news; security blogs; trend analysis,Computer crime; Cybersecurity; Network security; Websites; Computer users; Cyber security; Cybersecurity topic; Empirical studies; Information delivery; News; Number of peoples; Security blog; Trend analysis; Trending topics; Blogs
"Mobile Crowd-sensing Applications: Data Redundancies, Challenges, and Solutions",2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130342408&doi=10.1145%2f3431502&partnerID=40&md5=6f92ff84643bd98826b2f07a8a81f211,"Conventional data collection methods that use Wireless Sensor Networks (WSNs) suffer from disadvantages such as deployment location limitation, geographical distance, as well as high construction and deployment costs of WSNs. Recently, various efforts have been promoting mobile crowd-sensing (such as a community with people using mobile devices) as a way to collect data based on existing resources. A Mobile Crowd-Sensing System can be considered as a Cyber-Physical System (CPS), because it allows people with mobile devices to collect and supply data to CPSs' centers. In practical mobile crowd-sensing applications, due to limited budgets for the different expenditure categories in the system, it is necessary to minimize the collection of redundant information to save more resources for the investor. We study the problem of selecting participants in Mobile Crowd-Sensing Systems without redundant information such that the number of users is minimized and the number of records (events) reported by users is maximized, also known as the Participant-Report-Incident Redundant Avoidance (PRIRA) problem. We propose a new approximation algorithm, called the Maximum-Participant-Report Algorithm (MPRA) to solve the PRIRA problem. Through rigorous theoretical analysis and experimentation, we demonstrate that our proposed method performs well within reasonable bounds of computational complexity.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Data redundancy; mobile crowd-sensing; optimization; participatory sensing,Approximation algorithms; Budget control; Data acquisition; Embedded systems; Human computer interaction; Redundancy; Application data; Data challenges; Data collection method; Data solutions; Data-redundancy; Mobile crowd-sensing; Optimisations; Participatory Sensing; Sensing applications; Sensing systems; Wireless sensor networks
Privacy-Preserving Distributed Multi-Task Learning against Inference Attack in Cloud Computing,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130329828&doi=10.1145%2f3426969&partnerID=40&md5=f9e41e8463e4ba9d7e1e8c6e3568f00a,"Because of the powerful computing and storage capability in cloud computing, machine learning as a service (MLaaS) has recently been valued by the organizations for machine learning training over some related representative datasets. When these datasets are collected from different organizations and have different distributions, multi-task learning (MTL) is usually used to improve the generalization performance by scheduling the related training tasks into the virtual machines in MLaaS and transferring the related knowledge between those tasks. However, because of concerns about privacy breaches (e.g., property inference attack and model inverse attack), organizations cannot directly outsource their training data to MLaaS or share their extracted knowledge in plaintext, especially the organizations in sensitive domains. In this article, we propose a novel privacy-preserving mechanism for distributed MTL, namely NOInfer, to allow several task nodes to train the model locally and transfer their shared knowledge privately. Specifically, we construct a single-server architecture to achieve the private MTL, which protects task nodes' local data even if out of nodes colluded. Then, a new protocol for the Alternating Direction Method of Multipliers (ADMM) is designed to perform the privacy-preserving model training, which resists the inference attack through the intermediate results and ensures that the training efficiency is independent of the number of training samples. When releasing the trained model, we also design a differentially private model releasing mechanism to resist the membership inference attack. Furthermore, we analyze the privacy preservation and efficiency of NOInfer in theory. Finally, we evaluate our NOInfer over two testing datasets and evaluation results demonstrate that NOInfer efficiently and effectively achieves the distributed MTL.  © 2021 Association for Computing Machinery.",cloud computing; differential privacy; homomorphic cryptography; Multi-task learning; privacy preservation,Cloud computing; Computation theory; Digital storage; Efficiency; Inverse problems; Knowledge management; Linearization; Privacy-preserving techniques; Cloud-computing; Computing capability; Different distributions; Differential privacies; Homomorphic cryptography; Inference attacks; Multitask learning; Privacy preservation; Privacy preserving; Storage capability; Learning systems
An Efficient Interaction Protocol Inference Scheme for Incompatible Updates in IoT Environments,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130346320&doi=10.1145%2f3430501&partnerID=40&md5=9522f01acc94f7d26bffd8e3ed319bf6,"Incompatible updates of IoT systems and protocols give rise to interoperability problems. Even though various protocol adaptation and unknown protocol inference schemes have been proposed, they either do not work where the updated protocol specifications are not given or suffer from inefficiency issues. In this work, we present an efficient protocol inference scheme for incompatible updates in IoT environments. The scheme refines an active automata learning algorithm, L∗, by incorporating a knowledge base of the legacy protocol behavior into its membership query selection procedure for updated protocol behavior inference. It also infers protocol syntax based on our previous work that computes the most probable message field updates and adapts the legacy protocol message accordingly. We evaluate the proposed scheme with two case studies with the most popular IoT protocols and prove that it infers updated protocols efficiently while improving the L∗algorithm's performance for resolving the incompatibility.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Internet of Things; interoperability; Knowledge-based inference; meaningful interaction; protocol adaptation; protocol heterogeneity,Inference engines; Internet of things; Efficient interaction; Interaction protocols; Knowledge based; Knowledge-based inference; Legacy protocol; Meaningful interaction; Protocol adaptation; Protocol behavior; Protocol heterogeneity; Protocol specifications; Interoperability
Machine Learning and Soil Humidity Sensing: Signal Strength Approach,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130409142&doi=10.1145%2f3418207&partnerID=40&md5=4becd2c0396f8b75a22073888cb0f753,"The Internet-of-Things vision of ubiquitous and pervasive computing gives rise to future smart irrigation systems comprising the physical and digital worlds. A smart irrigation ecosystem combined with Machine Learning can provide solutions that successfully solve the soil humidity sensing task in order to ensure optimal water usage. Existing solutions are based on data received from the power hungry/expensive sensors that are transmitting the sensed data over the wireless channel. Over time, the systems become difficult to maintain, especially in remote areas due to the battery replacement issues with a large number of devices. Therefore, a novel solution must provide an alternative, cost- and energy-effective device that has unique advantage over the existing solutions. This work explores the concept of a novel, low-power, LoRa-based, cost-effective system that achieves humidity sensing using Deep Learning techniques that can be employed to sense soil humidity with high accuracy simply by measuring the signal strength of the given underground beacon device.  © 2021 Association for Computing Machinery.",Deep learning; LoRa; LSTM; RSSI; Soil humidity; SVR,Deep learning; Humidity sensors; Irrigation; Soils; Ubiquitous computing; Deep learning; Humidity sensing; Lora; LSTM; Machine-learning; RSSI; Sensing signals; Signal strengths; Soil humidity; SVR; Cost effectiveness
Task Offloading with Task Classification and Offloading Nodes Selection for MEC-Enabled IoV,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130391928&doi=10.1145%2f3475871&partnerID=40&md5=4e85025d723249d33ed904506f4a7720,"The Mobile Edge Computing (MEC)-based task offloading in the Internet of Vehicles (IoV) scenario, which transfers computational tasks to mobile edge nodes and fixed edge nodes with available computing resources, has attracted interest in recent years. The MEC-based task offloading can achieve low latency and low operational cost under the tasks delay constraints. However, most existing research generally focuses on how to divide and migrate these tasks to the other devices. This research ignores delay constraints and offloading node selection for different tasks. In this article, we design the MEC-enabled IoV architecture, in which all vehicles and MEC servers act as offloading nodes. Mobile offloading nodes (i.e., vehicles) and fixed offloading nodes (i.e., MEC servers) provide low latency offloading services cooperatively through roadside units. Then we propose the task offloading scheme that considers task classification and offloading nodes selection (TO-TCONS). Our goal is to minimize the total execution time of tasks. In TO-TCONS Scheme, we divide the task offloading into the same region offloading mode and cross-region offloading mode, which is based on the delay constraints of tasks and the travel time of the target vehicle. Moreover, we propose the mobile offloading nodes selection strategy to select offloading nodes for each task, which evaluates offloading candidates for each task based on computing resources and transmission rates. Simulation results demonstrate that TO-TCONS Scheme is indeed capable of reducing total latency of tasks execution under the delay constraints in MEC-enabled IoV.  © 2021 Association for Computing Machinery.",Internet of Vehicles; mobile edge computing; offloading nodes selection; task classification; Task offloading,Mobile edge computing; Travel time; Computing resource; Delay constraints; Edge nodes; Internet of vehicle; Low latency; Node selection; Offloading node selection; Selection scheme; Task classification; Task offloading; Vehicles
Deep-Confidentiality: An IoT-Enabled Privacy-Preserving Framework for Unstructured Big Biomedical Data,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129306280&doi=10.1145%2f3421509&partnerID=40&md5=e49b07619dbeb73b1dfcc599c0d42446,"Due to the Internet of Things evolution, the clinical data is exponentially growing and using smart technologies. The generated big biomedical data is confidential, as it contains a patient's personal information and findings. Usually, big biomedical data is stored over the cloud, making it convenient to be accessed and shared. In this view, the data shared for research purposes helps to reveal useful and unexposed aspects. Unfortunately, sharing of such sensitive data also leads to certain privacy threats. Generally, the clinical data is available in textual format (e.g., perception reports). Under the domain of natural language processing, many research studies have been published to mitigate the privacy breaches in textual clinical data. However, there are still limitations and shortcomings in the current studies that are inevitable to be addressed. In this article, a novel framework for textual medical data privacy has been proposed as Deep-Confidentiality. The proposed framework improves Medical Entity Recognition (MER) using deep neural networks and sanitization compared to the current state-of-the-art techniques. Moreover, the new and generic utility metric is also proposed, which overcomes the shortcomings of the existing utility metric. It provides the true representation of sanitized documents as compared to the original documents. To check our proposed framework's effectiveness, it is evaluated on the i2b2-2010 NLP challenge dataset, which is considered one of the complex medical data for MER. The proposed framework improves the MER with 7.8% recall, 7% precision, and 3.8% F1-score compared to the existing deep learning models. It also improved the data utility of sanitized documents up to 13.79%, where the value of the k is 3.  © 2021 Association for Computing Machinery.",big biomedical data; CNN; Deep neural network; LSTM; textual data privacy,Clinical research; Internet of things; Long short-term memory; Natural language processing systems; Privacy-preserving techniques; Sensitive data; 'current; Big biomedical data; Biomedical data; Clinical data; Entity recognition; LSTM; Privacy preserving; Smart technology; Textual data; Textual data privacy; Deep neural networks
DANCE: Distributed Generative Adversarial Networks with Communication Compression,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130317108&doi=10.1145%2f3458929&partnerID=40&md5=7e43e4832bcec710739e6083ee6d27b0,"Generative adversarial networks (GANs) have shown great success in deep representations learning, data generation, and security enhancement. With the development of the Internet of Things, 5th generation wireless systems (5G), and other technologies, the large volume of data collected at the edge of networks provides a new way to improve the capabilities of GANs. Due to privacy, bandwidth, and legal constraints, it is not appropriate to upload all the data to the cloud or servers for processing. Therefore, this article focuses on deploying and training GANs at the edge rather than converging edge data to the central node. To address this problem, we designed a novel distributed learning architecture for GANs, called DANCE. DANCE can adaptively perform communication compression based on the available bandwidth, while supporting both data and model parallelism training of GANs. In addition, inspired by the gossip mechanism and Stackelberg game, a compatible algorithm, AC-GAN is proposed. The theoretical analysis guarantees the convergence of the model and the existence of approximate equilibrium in AC-GAN. Both simulation and prototype system experiments show that AC-GAN can achieve better training effectiveness with less communication overhead than the SOTA algorithms, i.e., FL-GAN and MD-GAN.  © 2021 Association for Computing Machinery.",communication compression; distributed learning; Generative adversarial networks,5G mobile communication systems; Bandwidth; Bandwidth compression; Deep learning; Bandwidth constraint; Communication compression; Data generation; Distributed learning; Large volumes; Learning data; Legal constraint; Privacy constraints; Security enhancements; Wireless systems; Generative adversarial networks
Collusion-free for Cloud Verification toward the View of Game Theory,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130314365&doi=10.1145%2f3423558&partnerID=40&md5=dc531256838b70c7f4e41a24e2d83e62,"At present, clients can outsource lots of complex and abundant computation, e.g., Internet of things (IoT), tasks to clouds by the ""pay as you go""model. Outsourcing computation can save costs for clients and fully utilize the existing cloud infrastructures. However, it is hard for clients to trust the clouds even if blockchain is used as the trusted platform. In this article, we utilize the verification method as SETI@home by only two rational clouds, who hope to maximize their utilities. Utilities are defined as the incomes of clouds when they provide computation results to clients. More specifically, one client outsources two jobs to two clouds and each job contains n tasks, which include k identical sentinels. Two clouds can either honestly compute each task or collude on the identical sentinel tasks by agreeing on random values. If the results of identical sentinels are identical, then client regards the jobs as correctly computed without verification. Obviously, rational clouds have incentives to deviate by collusion and provide identical random results for a higher income. We discuss how to prevent collusion by using deposits, e.g., bit-coins. Furthermore, utilities for each cloud can be automatically assigned by a smart contract. We prove that, given proper parameters, two rational clouds will honestly send correct results to the client without collusion.  © 2021 Association for Computing Machinery.",blockchain; Cloud computing; collusion free; game theory; IoT; sequential equilibrium,Blockchain; Computation theory; Computer games; Internet of things; Trusted computing; Block-chain; Cloud infrastructures; Cloud-computing; Collusion free; Go model; Pay as you go; Random values; Sequential equilibrium; Trusted platforms; Verification method; Game theory
Enhancing Security-Problem-Based Deep Learning in Mobile Edge Computing,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130414853&doi=10.1145%2f3458931&partnerID=40&md5=682316926cd63bc31678c0d4ee5b608e,"The implementation of a variety of complex and energy-intensive mobile applications by resource-limited mobile devices (MDs) is a huge challenge. Fortunately, mobile edge computing (MEC) as a new computing paragon can offer rich resources to perform all or part of the MD's task, which greatly reduces the energy consumption of the MD and improves the quality of service (QoS) for applications. However, offloading tasks to the edge server is vulnerable to attacks such as tampering and snooping, resulting in a deep learning (DL) security feature developed by major cloud service providers. An effective security strategy method to minimize ongoing attacks in the MEC setting is proposed. The algorithm is based on the synthetic principle of a special set of strategies, and it can quickly construct suboptimal solutions even if the number of targets achieves hundreds of millions. In addition, for a given structure and a given number of patrollers, the upper bound of the protection level can be obtained, and the lower bound required for a given protection level can also be inferred. These bounds apply to universal strategies. By comparing with the previous three basic experiments, it can be proved that our algorithm is better than the previous ones in terms of security and running time.  © 2022 Association for Computing Machinery.",deep learning; mobile device; Mobile edge computing; quality of service; security strategies; suboptimal; synthetic theories,Computation theory; Deep learning; Energy utilization; Mobile cloud computing; Mobile computing; Mobile edge computing; Deep learning; Energy; Mobile applications; Problem-based; Protection level; Quality-of-service; Security problems; Security strategies; Suboptimal; Synthetic theories; Quality of service
InFeMo: Flexible Big Data Management Through a Federated Cloud System,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130328255&doi=10.1145%2f3426972&partnerID=40&md5=41a968fdc7c0129d62ba0c8ace27d76b,"This paper introduces and describes a novel architecture scenario based on Cloud Computing and counts on the innovative model of Federated Learning. The proposed model is named Integrated Federated Model, with the acronym InFeMo. InFeMo incorporates all the existing Cloud models with a federated learning scenario, as well as other related technologies that may have integrated use with each other, offering a novel integrated scenario. In addition to this, the proposed model is motivated to deliver a more energy efficient system architecture and environment for the users, which aims to the scope of data management. Also, by applying the InFeMo the user would have less waiting time in every procedure queue. The proposed system was built on the resources made available by Cloud Service Providers (CSPs) and by using the PaaS (Platform as a Service) model, in order to be able to handle user requests better and faster. This research tries to fill a scientific gap in the field of federated Cloud systems. Thus, taking advantage of the existing scenarios of FedAvg and CO-OP, we were keen to end up with a new federated scenario that merges these two algorithms, and aiming for a more efficient model that is able to select, depending on the occasion, if it ""trains""the model locally in client or globally in server.  © 2021 Association for Computing Machinery.",big data; Cloud computing; federated learning system; management; secure,Big data; Computer architecture; Energy efficiency; Learning systems; Platform as a Service (PaaS); Cloud modeling; Cloud systems; Cloud-computing; Federated clouds; Federated learning system; Innovative models; Learning scenarios; Novel architecture; Scenario-based; Secure; Information management
Multi-Tier Stack of Block Chain with Proxy Re-Encryption Method Scheme on the Internet of Things Platform,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130371625&doi=10.1145%2f3421508&partnerID=40&md5=8c40911a715cf9e0fe5ee92ed79966e0,"Block chain provides an innovative solution to information storage, transaction execution, security, and trust building in an open environment. The block chain is technological progress for cyber security and cryptography, with efficiency-related cases varying in smart grids, smart contracts, over the IoT, etc. The movement to exchange data on a server has massively increased with the introduction of the Internet of Things. Hence, in this research, Splitting of proxy re-encryption method (Split-PRE) has been suggested based on the IoT to improve security and privacy in a private block chain. This study proposes a block chain-based proxy re-encryption program to resolve both the trust and scalability problems and to simplify the transactions. After encryption, the system saves the Internet of Things data in a distributed cloud. The framework offers dynamic, smart contracts between the sensor and the device user without the intervention of a trustworthy third party to exchange the captured IoT data. It uses an efficient proxy re-encryption system, which provides the owner and the person existing in the smart contract to see the data. The experimental outcomes show that the proposed approach enhances the efficiency, security, privacy, and feasibility of the system when compared to other existing methods.  © 2021 Association for Computing Machinery.",Internet of Things; privacy; Private block chain; proxy re-encryption (PRE) scheme; security,Cryptography; Digital storage; Efficiency; Internet of things; Trusted computing; Block-chain; Encryption methods; Innovative solutions; Multi-tier; Privacy; Private block chain; Private blocks; Proxy re encryptions; Proxy re-encryption schemes; Security; Smart contract
Adaptive Fuzzy Game-Based Energy-Efficient Localization in 3D Underwater Sensor Networks,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130323237&doi=10.1145%2f3406533&partnerID=40&md5=e753f19e5d60af7bae9a19b32b30cb11,"Numerous applications in 3D underwater sensor networks (UWSNs), such as pollution detection, disaster prevention, animal monitoring, navigation assistance, and submarines tracking, heavily rely on accurate localization techniques. However, due to the limited batteries of sensor nodes and the difficulty for energy harvesting in UWSNs, it is challenging to localize sensor nodes successfully within a short sensor node lifetime in an unspecified underwater environment. Therefore, we propose the Adaptive Energy-Efficient Localization Algorithm (Adaptive EELA) to enable energy-efficient node localization while adapting to the dynamic environment changes. Adaptive EELA takes a fuzzy game-theoretic approach, whereby the Stackelberg game is used to model the interactions among sensor and anchor nodes in UWSNs and employs the adaptive neuro-fuzzy method to set the appropriate utility functions. We prove that a socially optimal Stackelberg-Nash equilibrium is achieved in Adaptive EELA. Through extensive numerical simulations under various environmental scenarios, the evaluation results show that our proposed algorithm accomplishes a significant energy reduction, e.g., 66% lower compared to baselines, while achieving a desired performance level in terms of localization coverage, error, and delay.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adaptive EELA; dynamic environment; energy consumption; localization; localization coverage; Underwater sensor networks,Disaster prevention; Energy efficiency; Energy harvesting; Fuzzy inference; Game theory; Sensor nodes; Adaptive energy-efficient localization algorithm; Adaptive-fuzzy; Dynamic environments; Energy efficient; Energy-consumption; Fuzzy games; Localisation; Localisation coverage; Localization algorithm; Underwater sensor networks; Energy utilization
A Security Cost Modelling Framework for Cyber-Physical Systems,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130349080&doi=10.1145%2f3450752&partnerID=40&md5=236fdd6fea097320022277d9d78098aa,"Cyber-Physical Systems (CPS) are formed through interconnected components capable of computation, communication, sensing and changing the physical world. The development of these systems poses a significant challenge, since they have to be designed in a way to ensure cyber-security without impacting their performance. This article presents the Security Cost Modelling Framework (SCMF) and shows supported by an experimental study how it can be used to measure, normalise, and aggregate the overall performance of a CPS. Unlike previous studies, our approach uses different metrics to measure the overall performance of a CPS and provides a methodology for normalising the measurement results of different units to a common Cost Unit. Moreover, we show how the Security Costs can be extracted from the overall performance measurements, which allows us to quantify the overhead imposed by performing security-related tasks. Furthermore, we describe the architecture of our experimental testbed and demonstrate the applicability of SCMF in an experimental study. Our results show that measuring the overall performance and extracting the security costs using SCMF can serve as basis to redesign interactions to achieve the same overall goal at less costs.  © 2022 Association for Computing Machinery.",aggregation; Cyber-physical systems; interaction comparison; metric types; normalisation; security cost evaluation; security cost modelling,Cyber Physical System; Cybersecurity; Cost evaluations; Cost models; Interaction comparison; Metric type; Modelling framework; Normalisation; Performance; Physical world; Security cost evaluation; Security cost modeling; Embedded systems
Predictive Analytics of Energy Usage by IoT-Based Smart Home Appliances for Green Urban Development,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130359852&doi=10.1145%2f3426970&partnerID=40&md5=4b2200c61aee188670bdb60004caac8e,"Green IoT primarily focuses on increasing IoT sustainability by reducing the large amount of energy required by IoT devices. Whether increasing the efficiency of these devices or conserving energy, predictive analytics is the cornerstone for creating value and insight from large IoT data. This work aims at providing predictive models driven by data collected from various sensors to model the energy usage of appliances in an IoT-based smart home environment. Specifically, we address the prediction problem from two perspectives. Firstly, an overall energy consumption model is developed using both linear and non-linear regression techniques to identify the most relevant features in predicting the energy consumption of appliances. The performances of the proposed models are assessed using a publicly available dataset comprising historical measurements from various humidity and temperature sensors, along with total energy consumption data from appliances in an IoT-based smart home setup. The prediction results comparison show that LSTM regression outperforms other linear and ensemble regression models by showing high variability (R2) with the training (96.2%) and test (96.1%) data for selected features. Secondly, we develop a multi-step time-series model using the auto regressive integrated moving average (ARIMA) technique to effectively forecast future energy consumption based on past energy usage history. Overall, the proposed predictive models will enable consumers to minimize the energy usage of home appliances and the energy providers to better plan and forecast future energy demand to facilitate green urban development.  © 2021 Association for Computing Machinery.",energy usage; Green IoT; predictive analytics; smart home,Automation; Domestic appliances; Energy utilization; Forecasting; Intelligent buildings; Long short-term memory; Predictive analytics; Regression analysis; Sustainable development; Urban growth; Energy; Energy usage; Energy-consumption; Future energies; Green IoT; Large amounts; Model-driven; Predictive models; Smart homes; Urban development; Internet of things
Privacy-preserving Secure Media Streaming for Multi-user Smart Environments,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130371723&doi=10.1145%2f3423047&partnerID=40&md5=1f8d5bc32f466be3b65a17f31dbf5f43,"Over the last years, our lifestyle has been positively upset by the sudden advent of technology. The Internet of Things (IoT), offering universal and ubiquitous connectivity to both people and objects, revealed to be the silver bullet for enabling a vast number of previously unexpected applications. In particular, media streaming providers are growing in business and scope, and we can forecast that soon, video streaming will substitute TV broadcasting activities. With the increasing success of multi-user smart environments, empowered by new-generation smart devices and IoT architectures, multimedia contents (i.e., images and videos) need to be effectively accessed anytime and anywhere. Recent advances in computer vision technologies have made the development of intelligent monitoring systems for video surveillance and ambient-assisted living. Such a scenario permits better integration among technologies, multimedia content, and end-users. However, there are several challenges, and some are still open. More precisely, due to the sensitivity of some multimedia content (e.g., video-surveillance streams), it is paramount to preserve users' privacy. Again, it is necessary to guarantee the integrity of usage rights during any multimedia transmission process, starting from the video encoding phase. In this way, the private content is disclosed only when the stream is decoded on the other endpoint, by the legitimate user.In this article, we present a secure video transmission strategy that can address the challenges mentioned above. The proposed strategy takes advantage of both watermarking and video scrambling techniques to make it possible for the secure and privacy-preserving transmission of multimedia streaming. Through our proposal, multimedia streaming is of low quality and thus unusable. However, it can be fully recovered and enjoyed only by authorized users. Finally, due to its low complexity and energy-efficiency, our proposal is particularly suitable for onboard implementations.  © 2021 Association for Computing Machinery.",energy-efficiency; low-complexity; Multimedia streaming; privacy; protection; smart environments,Data privacy; Image communication systems; Internet of things; Network security; Security systems; Television broadcasting; Video streaming; Lower complexity; Multimedia contents; Multimedia streaming; Multiusers; Privacy; Privacy preserving; Protection; Smart environment; Video surveillance; Video-streaming; Energy efficiency
DADC: A Novel Duty-cycling Scheme for IEEE 802.15.4 Cluster-tree-based IoT Applications,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130312899&doi=10.1145%2f3409487&partnerID=40&md5=678ec9c80b191a4dd323b1fd016d6003,"The IEEE 802.15.4 standard is one of the widely adopted specifications for realizing different applications of the Internet of Things. It defines several physical layer options and Medium Access Control (MAC) sub-layer for devices with low-power operating at low data rates. As devices implementing this standard are primarily battery-powered, minimizing their power consumption is a significant concern. Duty-cycling is one such power conserving mechanism that allows a device to schedule its active and inactive radio periods effectively, thus preventing energy drain due to idle listening. The standard specifies two parameters, beacon order and superframe order, which define the active and inactive period of a device. However, it does not specify a duty-cycling scheme to adapt these parameters for varying network conditions. Existing works in this direction are either based on superframe occupation ratio or buffer/queue length of devices. In this article, the particular limitations of both the approaches mentioned above are presented. Later, a novel duty-cycling mechanism based on MAC parameters is proposed. Also, we analyze the role of synchronization schemes in achieving efficient duty-cycles in synchronized cluster-tree network topologies. A Markov model has also been developed for the MAC protocol to estimate the delay and energy consumption during frame transmission.  © 2021 Association for Computing Machinery.",cluster-tree; duty-cycling; energy conservation; IEEE 802.15.4; IoT; synchronization,Energy utilization; IEEE Standards; Internet of things; Markov processes; Medium access control; Network layers; Synchronization; Battery powered; Cluster tree; Duty-cycling; IEEE 802.15.4 standards; Ieee 802.15.4/zigbee; Low data rates; Low Power; Physical layers; Sub-layers; Tree-based; Energy conservation
Guest Editorial Introduction for the Special Section on Deep Learning Algorithms and Systems for Enhancing Security in Cloud Services,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130336186&doi=10.1145%2f3516806&partnerID=40&md5=472f36ad094aa399d7e10b14a010b4cd,[No abstract available],,
A Mutual Security Authentication Method for RFID-PUF Circuit Based on Deep Learning,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130382313&doi=10.1145%2f3426968&partnerID=40&md5=fa74d59a9f3d4339c5ed1ae50a94ac97,"The Industrial Internet of Things (IIoT) is designed to refine and optimize the process controls, thereby leveraging improvements in economic benefits, such as efficiency and productivity. However, the Radio Frequency Identification (RFID) technology in an IIoT environment has problems such as low security and high cost. To overcome such issues, a mutual authentication scheme that is suitable for RFID systems, wherein techniques in Deep Learning (DL) are incorporated onto the Arbiter Physical Unclonable Function (APUF) for the secured access authentication of the IC circuits on the IoT, is proposed. The design applies the APUF-MPUF mutual authentication structure obtained by DL to generate essential real-time authentication information, thereby taking advantage of the feature that the tag in the PUF circuit structure does not need to store any essential information and resolving the problem of key storage. The proposed scheme also uses a bitwise comparison method, which hides the PUF response information and effectively reduces the resource overhead of the system during the verification process, to verify the correctness of the two strings. Security analysis demonstrates that the proposed scheme has high robustness and security against different conventional attack methods, and the storage and communication costs are 95.7% and 42.0% lower than the existing schemes, respectively.  © 2021 Association for Computing Machinery.",arbiter physical unclonable function (APUF); deep learning (DL); Industrial internet of things (IIoT); radio frequency identification (RFID),Authentication; Cost benefit analysis; Cryptography; Deep learning; Hardware security; Internet of things; Radio waves; Timing circuits; Arbiter physical unclonable function; Authentication methods; Deep learning; Economic benefits; Industrial internet of thing; Mutual authentication; Radio frequency identification; Radio frequency identification technology; Radio-frequency-identification; Security authentication; Radio frequency identification (RFID)
Optimal Energy-Centric Resource Allocation and Offloading Scheme for Green Internet of Things Using Machine Learning,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130373173&doi=10.1145%2f3431500&partnerID=40&md5=5d17eda5a7e91607e8a29676fa2accb5,"Resource allocation and offloading in green Internet of Things (IoT) relies on the multi-level heterogeneous platforms. The energy expenses of the platform determine the reliability of green IoT based services and applications. This manuscript introduces a decisive energy management scheme for optimal resource allocation and offloading along with energy constraints. This scheme handles both the allocation and energy-cost in a balanced manner through deterministic task offloading. In particular, resource allocation solution for non-delay tolerant green IoT applications is focused by confining the failures of discrete tasks through neural learning. The dropout process augmented with the learning process improves the feasible conditions for resource handling and task offloading among the active IoT service providers. Through extensive simulations the performance of the proposed scheme is analyzed and energy consumption, failure rate, processing, and completion time metrics are used for a comparative study. Further, the optimal utilization and on-demand dissipation of such stored resources help to improve the sustainability of green power and communication technologies in the smart city environment.  © 2022 Association for Computing Machinery.",Dropout learning; green IoT; neural network; resource allocation; task offloading,Energy utilization; Failure analysis; Green computing; Internet of things; Machine learning; Sustainable development; Dropout learning; Green internet of thing; Green internets; Heterogeneous platforms; Machine-learning; Multilevels; Neural-networks; Optimal energy; Resources allocation; Task offloading; Resource allocation
Transfer Learning-powered Resource Optimization for Green Computing in 5G-Aided Industrial Internet of Things,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127953809&doi=10.1145%2f3434774&partnerID=40&md5=212d73f75a560a0e49f26a0fb9951de7,"Objective: Green computing meets the needs of a low-carbon society and it is an important aspect of promoting social sustainable development and technological progress. In the investigation, green computing for resource management and allocation issues is only discussed. Therefore, in the context of the 5G communication network, the investigation of the data classification and resource optimization of the Internet of Things are conducted. Method: The virtualization architecture of the heterogeneous wireless network resource based on 5G technology is designed. The related investigation is conducted based on 5G network and Internet of Things technology. Under the traditional method, the transfer learning is introduced to improve the AdaBoost (Adaptive Boosting) algorithm to classify the data. The investigated complete resource reuse method is used to optimize resources. A method that a sub-channel can be reused by a cellular link and any number of D2D links at the same time is proposed to conduct resource optimization investigation. Results: The investigation indicates that the classification accuracy of the algorithm is excellent for the data classification of the Internet of Things and has different advantages in various aspects compared with other algorithms. The designed algorithm can find a larger set of resource reuse and have a significant increase in spectrum utilization efficiency. Conclusion: The investigation can contribute to the boom in the Internet of Things in terms of data classification and resource optimization based on 5G.  © 2021 Association for Computing Machinery.",5G; AdaBoost; data classification; Internet of Things; resource optimization; transfer learning,Adaptive boosting; Classification (of information); Heterogeneous networks; Internet of things; Learning systems; Resource allocation; Communications networks; Data classification; Data resources; Low-carbon societies; Resource management; Resource reuse; Resources allocation; Resources optimization; Technological progress; Transfer learning; 5G mobile communication systems
A Green Stackelberg-game Incentive Mechanism for Multi-service Exchange in Mobile Crowdsensing,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127439833&doi=10.1145%2f3421506&partnerID=40&md5=245e3c5ec4255036239ead30432eb811,"Although mobile crowdsensing (MCS) has become a green paradigm of collecting, analyzing, and exploiting massive amounts of sensory data, existing incentive mechanisms are not effective to stimulate users's active participation and service contribution in multi-service exchange in MCS due to its specific features: a large number of heterogeneous users have asymmetric service requirements, workers have the freedom to choose sensing tasks as well as participation levels, and multiple sensing tasks have heterogeneous values which may be untruthful declared by the corresponding requesters. To address this issue, this article develops a green Stackelberg-game incentive mechanism to achieve selective fairness, truthfulness, and bounded efficiency while reducing the burden on the platform. First, we model the multi-service exchange problem as a Stackelberg multi-service exchange game consisting of multi-leader and multi-follower, in which each requester as a leader first chooses the reward declaration strategy and thus the payment for each sensing task, each worker as a follower then chooses the sensing plan strategy to maximize her own utility. We next introduce the concept of virtual currency to maintain the selective fairness to balance service request and service provision between users, in which a user earns/consumes virtual currency for providing/receiving services, and thus no one can always get services without providing services. Then, we present two novel algorithms to compute the unique Nash equilibrium for the sensing plan determination game and the reward declaration determination game, respectively, which together forms a unique Stackelberg equilibrium for the proposed game. Afterwards, we theoretically prove that the proposed green Stackelberg-game incentive mechanism achieves the desirable properties of selective fairness, truthfulness, bounded efficiency. Finally, extensive evaluation results are provided to support the validity and effectiveness of our mechanism compared with both baseline and theoretical optimal approaches.  © 2021 Association for Computing Machinery.",Incentive mechanism; mobile crowdsensing; multi-service exchange; selective fairness; Stackelberg game,Efficiency; Electronic money; Incentive mechanism; Mobile crowdsensing; Multi-service exchange; Multi-services; Selective fairness; Sensing tasks; Sensory data; Stackelberg Games; Virtual currency; Workers'; Quality of service
A Taxonomy of Multimedia-based Graphical User Authentication for Green Internet of Things,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122865313&doi=10.1145%2f3433544&partnerID=40&md5=05a6985c4935f8a4a80680b2df6949d1,"Authentication receives enormous consideration from the research community and is proven to be an interesting field in today's era. User authentication is the major concern because people have their private data on devices. To strengthen user authentication, passwords have been introduced. In the past, the text-based password was the traditional way of authentication, but this method has particular shortcomings. The graphical password has been introduced as an alternative, which uses a picture or a set of pictures to generate a password. In the future, it is a requirement of such approaches to maintain robustness and consume fewer energy resources to become suitable for the Green Internet of Things (IoT). Similarly, diverse graphical password authentication mechanisms have been used to provide users with better security and usability. In this article, we conduct an extensive survey on the existing approaches of graphical password authentication to highlight the challenges required to be addressed for Green IoT. In comparison to other existing surveys, the objective is to consolidate the graphical password technique and to identify the problem associated with it. Besides, this survey will also identify the vulnerabilities of the graphical password against several potential attacks. We have also examined the strengths and weaknesses of each technique along with the future research directions. This study also evaluates the usability of each approach by considering learnability, memorability, and so forth and also presents a comparative analysis with security.  © 2021 Association for Computing Machinery.",alphanumeric password; Graphical password; information security; privacy and security; user authentication,Energy resources; Internet of things; Surveys; Alphanumeric passwords; Authentication mechanisms; Graphical password; Green internets; Password-authentication; Privacy and security; Private data; Research communities; Security and usabilities; User authentication; Authentication
A Flexible and Privacy-Preserving Collaborative Filtering Scheme in Cloud Computing for VANETs,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124153700&doi=10.1145%2f3425708&partnerID=40&md5=ebb4aef621ae4b360c52b4cb5570493c,"The vehicular ad hoc network (VANET) has become a hot topic in recent years. With the development of VANETs, how to achieve secure and efficient machine learning in VANETs is an urgent problem to be solved. Besides, how to ensure that users obtain the accurate results of machine learning is also a challenge. Based on the homomorphic encryption and secure multiparty computing technology, a flexible and privacy-preserving collaborative filtering scheme is proposed to accomplish the personalized recommendation for users, which is based on users' interests and locations. On the one hand, the data can be updated by users flexibly to ensure the freshness and accuracy of the dataset of interest. On the other hand, the weighted values of user interest can be safely sorted to improve the accuracy of collaborative filtering effectively. Moreover, a novel collaborative filtering algorithm based on the homomorphic encryption technology is designed, which can guarantee that the calculated decryption result by machine learning is the same as the plaintext. Note that the privacy of user data can be preserved during machine learning in this algorithm. Both theoretical and experimental analyses demonstrate that the proposed scheme is secure and efficient for collaborative filtering in cloud computing in VANETs.  © 2021 Association for Computing Machinery.",collaborative filtering; homomorphic encryption; Machine learning; multi-party computation,Cloud computing; Collaborative filtering; Learning algorithms; Privacy-preserving techniques; Vehicular ad hoc networks; Cloud-computing; Filtering schemes; Ho-momorphic encryptions; Homomorphic-encryptions; Hot topics; Multiparty computation; Privacy preserving; Urgent problems; Users' interests; Vehicular Adhoc Networks (VANETs); Machine learning
Special Section on Edge-AI for Connected Living,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137806524&doi=10.1145%2f3514196&partnerID=40&md5=76e33ec7b225d5e54506f6d8fc4ba85e,[No abstract available],,
AI-Enabled Task Offloading for Improving Quality of Computational Experience in Ultra Dense Networks,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131557773&doi=10.1145%2f3491217&partnerID=40&md5=61064b9a88b5973e4d337651f8ea227d,"Multi-access edge computing (MEC) and ultra-dense networking (UDN) are recognized as two promising paradigms for future mobile networks that can be utilized to improve the spectrum efficiency and the quality of computational experience (QoCE). In this paper, we study the task offloading problem in an MEC-enabled UDN architecture with the aim to minimize the task duration while satisfying the energy budget constraints. Due to the dynamics associated with the environment and parameter uncertainty, designing an optimal task offloading algorithm is highly challenging. Consequently, we propose an online task offloading algorithm based on a state-of-the-art deep reinforcement learning (DRL) technique: asynchronous advantage actor-critic (A3C). It is worthy of remark that the proposed method requires neither instantaneous channel state information (CSI) nor prior knowledge of the computational capabilities of the base stations. Simulations show that the our method is able to learn a good offloading policy to obtain a near-optimal task allocation while meeting energy budget constraints of mobile devices in the UDN environment.  © 2022 Association for Computing Machinery.",deep reinforcement learning; Edge-AI; MEC; quality of computational experience; task offloading; UDN,Budget control; Channel state information; computation offloading; Computational efficiency; Constraint satisfaction problems; Deep learning; Spectrum efficiency; Deep reinforcement learning; Edge computing; Edge-AI; Energy budgets; Multi-access edge computing; Multiaccess; Quality of computational experience; Reinforcement learnings; Task offloading; Ultra-dense networking; Reinforcement learning
PoSSUM: An Entity-centric Publish/Subscribe System for Diverse Summarization in Internet of Things,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134656955&doi=10.1145%2f3507911&partnerID=40&md5=f2e8e08994603e0d155ef8fb106b7acd,"Users are interested in entity information provided by multiple sensors in the Internet of Things. The challenges regarding this environment span from data-centric ones due to data integration, heterogeneity, and enrichment, to user-centric ones due to the need for high-level data interpretation and usability for non-expert users, to system-centric ones due to resource constraints. Publish/Subscribe systems (PSSs) are suitable schemes for large-scale applications, but they are limited in dealing with the data and user challenges. In this article, we propose PoSSUM, a novel entity-centric PSS that provides entity summaries for user-friendly subscriptions through data integration, a novel Density-Based VARiance Clustering (DBVARC) for diverse entity summarization that is parameter-free and partly incremental, reasoning rules, and a novel Triple2Rank scoring for top-k filtering based on importance, informativeness, and diversity. We introduce a novel evaluation methodology that creates ground truths and metrics that capture the quality of entity summaries. We compare our approach with a previous dynamic approach and a static diverse entity summarization approach that we adapted to dynamic environments. The evaluation results for two use cases, Healthcare and Smart Cities, show that when users are provided with less information, their data diversity desire could reach up to 80%. Summarization approaches achieve from 80% to 99% message reduction, with PoSSUM having the best-ranking quality for more than half of the entities by a significant margin. PoSSUM has the highest conceptual clustering F-score, ranging from 0.69 to 0.83, and a redundancy-aware F-score up to 0.95, with cases, where it is almost two times better than the other approaches. PoSSUM takes 50% or less clustering processing time and it performs scoring significantly faster for larger windows. It also has comparable end-to-end latency and throughput values, and it occupies a third of the memory compared to the second-best approach.  © 2022 Copyright held by the owner/author(s).",Diverse entity summarization; Internet of Things; Top-k filtering; triple ranking,Data integration; Publishing; Quality control; Clusterings; Data centric; Data enrichments; Data heterogeneity; Diverse entity summarization; F-score; Multiple sensors; Publish/Subscribe system; Top-k filtering; Triple ranking; Internet of things
An Empirical View on Consolidation of the Web,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137649427&doi=10.1145%2f3503158&partnerID=40&md5=4132ef1d5cc9aae0bee5656e684fc4dc,"The majority of Web content is delivered by only a few companies that provide Content Delivery Infrastructuress (CDIss) such as Content Delivery Networkss (CDNss) and cloud hosts. Due to increasing concerns about trends of centralization, empirical studies on the extent and implications of resulting Internet consolidation are necessary. Thus, we present an empirical view on consolidation of the Web by leveraging datasets from two different measurement platforms. We first analyze Web consolidation around CDIs at the level of landing webpages, before narrowing down the analysis to a level of embedded page resources. The datasets cover 1(a) longitudinal measurements of DNS records for 166.5 M Web domains over five years, 1(b) measurements of DNS records for Alexa Top 1 M over a month and (2) measurements of page loads and renders for 4.3 M webpages, which include data on 392.3 M requested resources. We then define CDIs penetration as the ratio of CDI-hosted objects to all measured objects, which we use to quantify consolidation around CDIs. We observe that CDI penetration has close to doubled since 2015, reaching a lower bound of 15% for all .com, .net, and .org Web domains as of January 2020. Overall, we find a set of six CDIss to deliver the majority of content across all datasets, with these six CDIss being responsible for more than 80% of all 221.9 M CDI-delivered resources (56.6% of all resources in total). We find high dependencies of Web content on a small group of CDIss, in particular, for fonts, ads, and trackers, as well as JavaScript resources such as jQuery. We further observe CDIss to play important roles in rolling out IPv6 and TLS 1.3 support. Overall, these observations indicate a potential oligopoly, which brings both benefits but also risks to the future of the Web.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",centralization; consolidation; content delivery; Web,Internet protocols; Websites; Centralisation; Contents deliveries; Empirical studies; Javascript; Low bound; Measurements of; Web; Web content; Web domains; Web-page; Competition
IP Geolocation through Reverse DNS,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119209646&doi=10.1145%2f3457611&partnerID=40&md5=3c1ccefa21ef84259231c25f2f3e77f8,"IP Geolocation databases are widely used in online services to map end-user IP addresses to their geographical location. However, they use proprietary geolocation methods, and in some cases they have poor accuracy. We propose a systematic approach to use reverse DNS hostnames for geolocating IP addresses, with a focus on end-user IP addresses as opposed to router IPs. Our method is designed to be combined with other geolocation data sources. We cast the task as a machine learning problem where, for a given hostname, we first generate a list of potential location candidates, and then we classify each hostname and candidate pair using a binary classifier to determine which location candidates are plausible. Finally, we rank the remaining candidates by confidence (class probability) and break ties by population count. We evaluate our approach against three state-of-the-art academic baselines and two state-of-the-art commercial IP geolocation databases. We show that our work significantly outperforms the academic baselines and is complementary and competitive with commercial databases. To aid reproducibility, we open source our entire approach and make it available to the academic community. © 2021 Association for Computing Machinery.",contextual relevance; geographic personalization; geographic targeting; geotargeting; hostname geolocation; IP geolocation; IPv4; IPv6; local search; reverse DNS,Database systems; Routers; Contextual relevance; Geographic personalization; Geographic targeting; Geographics; Geolocations; Geotargeting; Hostname geolocation; IP geolocation; Ipv4; Ipv6; Local search; Personalizations; Reverse DNS; Location
Automated Security Assessment Framework for Wearable BLE-enabled Health Monitoring Devices,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119201071&doi=10.1145%2f3448649&partnerID=40&md5=ae3beec87b525983d8edb12ea6ef2f56,"The growth of IoT technology, increasing prevalence of embedded devices, and advancements in biomedical technology have led to the emergence of numerous wearable health monitoring devices (WHMDs) in clinical settings and in the community. The majority of these devices are Bluetooth Low Energy (BLE) enabled. Though the advantages offered by BLE-enabled WHMDs in tracking, diagnosing, and intervening with patients are substantial, the risk of cyberattacks on these devices is likely to increase with device complexity and new communication protocols. Furthermore, vendors face risk and financial tradeoffs between speed to market and ensuring device security in all situations. Previous research has explored the security and privacy of such devices by manually testing popular BLE-enabled WHMDs in the market and generally discussed categories of possible attacks, while mostly focused on IP devices. In this work, we propose a new semi-automated framework that can be used to identify and discover both known and unknown vulnerabilities in WHMDs. To demonstrate its implementation, we validate it with a number of commercially available BLE-enabled enabled wearable devices. Our results show that the devices are vulnerable to a number of attacks, including eavesdropping, data manipulation, and denial of service attacks. The proposed framework could therefore be used to evaluate potential devices before adoption into a secure network or, ideally, during the design and implementation of new devices. © 2021 Association for Computing Machinery.",e-health security; medical devices; security assessment,Automation; Commerce; Cybersecurity; Denial-of-service attack; Network security; Wearable technology; Biomedical technologies; Clinical settings; Cyber-attacks; E-health securities; Embedded device; Health monitoring devices; Lower energies; Medical Devices; Security assessment; Security assessment framework; Diagnosis
Cloud-based Network Virtualization in IoT with OpenStack,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119214175&doi=10.1145%2f3460818&partnerID=40&md5=5b3d1b69a548c4edd35c4ca95af64e95,"In Cloud computing deployments, specifically in the Infrastructure-as-a-Service (IaaS) model, networking is one of the core enabling facilities provided for the users. The IaaS approach ensures significant flexibility and manageability, since the networking resources and topologies are entirely under users' control. In this context, considerable efforts have been devoted to promoting the Cloud paradigm as a suitable solution for managing IoT environments. Deep and genuine integration between the two ecosystems, Cloud and IoT, may only be attainable at the IaaS level. In light of extending the IoT domain capabilities' with Cloud-based mechanisms akin to the IaaS Cloud model, network virtualization is a fundamental enabler of infrastructure-oriented IoT deployments. Indeed, an IoT deployment without networking resilience and adaptability makes it unsuitable to meet user-level demands and services' requirements. Such a limitation makes the IoT-based services adopted in very specific and statically defined scenarios, thus leading to limited plurality and diversity of use cases. This article presents a Cloud-based approach for network virtualization in an IoT context using the de-facto standard IaaS middleware, OpenStack, and its networking subsystem, Neutron. OpenStack is being extended to enable the instantiation of virtual/overlay networks between Cloud-based instances (e.g., virtual machines, containers, and bare metal servers) and/or geographically distributed IoT nodes deployed at the network edge. © 2021 Association for Computing Machinery.",Cloud; edge computing; IaaS; IoT; network virtualization; Neutron; OpenStack,Edge computing; Infrastructure as a service (IaaS); Middleware; Platform as a Service (PaaS); Virtual reality; Virtualization; Cloud-based; Cloud-computing; Edge computing; Model networks; Network virtualization; Service levels; Service modeling; Service-cloud model; Suitable solutions; User control; Internet of things
NLUBroker: A QoE-driven Broker System for Natural Language Understanding Services,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137651246&doi=10.1145%2f3497807&partnerID=40&md5=0beb4f10df7947268c72fb1a3bd542c8,"Cloud-based Natural Language Understanding (NLU) services are becoming more popular with the development of artificial intelligence. More applications are integrated with cloud-based NLU services to enhance the way people communicate with machines. However, with NLU services provided by different companies powered by unrevealed AI technology, how to choose the best one is a problem for developers. Existing tools that can provide guidance to developers and make recommendations based on their needs are severely limited. This article comprehensively evaluates multiple state-of-the-art NLU services, and the results indicate that there is no absolute winner for different usage requirements. Motivated by this observation, we provide several insights and propose NLUBroker, a Quality of Experience-driven (QoE-driven) broker system, to select the proper service according to the environment. NLUBroker senses the client and service status and leverages a solution to the multi-armed bandit problem to conduct online learning, aiming to achieve maximum expected QoE. The performance of NLUBroker is evaluated in both simulation and real-world environments, and the evaluation results demonstrate that NLUBroker is an efficient solution for selecting NLU services. It is adaptive to changes in the environment, outperforms three baseline methods we evaluated and improves overall QoE up to 1.5× for the evaluated state-of-the-art NLU services.  © 2022 Association for Computing Machinery.",Quality of experience; service broker,AI Technologies; Cloud-based; Multiarmed bandit problems (MABP); Multiple state; Natural language understanding; Online learning; Provide guidances; Quality of experience; Service broker; State of the art; Quality of service
Energy-Efficient Computation Offloading for UAV-Assisted MEC: A Two-Stage Optimization Scheme,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119210792&doi=10.1145%2f3430503&partnerID=40&md5=c1efa49a40fd5d58d82b6efe87976d4d,"In addition to the stationary mobile edge computing (MEC) servers, a few MEC surrogates that possess a certain mobility and computation capacity, e.g., flying unmanned aerial vehicles (UAVs) and private vehicles, have risen as powerful counterparts for service provision. In this article, we design a two-stage online scheduling scheme, targeting computation offloading in a UAV-assisted MEC system. On our stage-one formulation, an online scheduling framework is proposed for dynamic adjustment of mobile users' CPU frequency and their transmission power, aiming at producing a socially beneficial solution to users. But the major impediment during our investigation lies in that users might not unconditionally follow the scheduling decision released by servers as a result of their individual rationality. In this regard, we formulate each step of online scheduling on stage one into a non-cooperative game with potential competition over the limited radio resource. As a solution, a centralized online scheduling algorithm, called ONCCO, is proposed, which significantly promotes social benefit on the basis of the users' individual rationality. On our stage-two formulation, we are working towards the optimization of UAV computation resource provision, aiming at minimizing the energy consumption of UAVs during such a process, and correspondingly, another algorithm, called WS-UAV, is given as a solution. Finally, extensive experiments via numerical simulation are conducted for an evaluation purpose, by which we show that our proposed algorithms achieve satisfying performance enhancement in terms of energy conservation and sustainable service provision. © 2021 Association for Computing Machinery.",Computation offloading; mobile edge computing; nash equilibrium; non-cooperative game; unmanned aerial vehicles,Antennas; Computer games; Energy efficiency; Energy utilization; Green computing; Mobile edge computing; Scheduling; Scheduling algorithms; Unmanned aerial vehicles (UAV); Computation offloading; Efficient computation; Energy efficient; Individual rationality; Nash equilibria; Noncooperative game; Online scheduling; Optimization scheme; Service provisions; Two stage optimizations; Game theory
Network Temperature: A Novel Statistical Index for Networks Measurement and Management,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128690393&doi=10.1145%2f3511093&partnerID=40&md5=9e47d58e305602eaa6aaa7537978d361,"Being able to monitor each packet path is critical for effective measurement and management of networks. However, such detailed monitoring can be very expensive especially for large-scale networks. To address such problem, inspired by thermodynamics, which uses the statistical characteristics of a large number of molecules’ motion but not each molecule’s trajectory for analysis, we propose the new concept of network temperature together with the notions of network-specific heat and network temperature gradient. Our approach does not only provide a statistical view of the current network state consisting of all the active packet paths at each time instant, but can be used to represent transitions among network states. Our network temperature-based methods have a broad applicability, such as to DDoS detection, dynamic node importance ranking, network stability and robustness evaluation, reliable packets routing, provenance compression assessment, and so on. Numerical and/or the experimental results show that our methods are effective. © 2022 Copyright held by the owner/author(s).",active packet paths; data provenance; network management; network measurement; Network security; network temperature,Information management; Network management; Network security; Packet networks; Specific heat; Temperature; Active packet path; Active packets; Data provenance; Large-scale network; Network measurement; Network state; Network temperature; Networks management; Networks security; Statistical indices; Molecules
A Deep Learning Approach for Voice Disorder Detection for Smart Connected Living Environments,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119206168&doi=10.1145%2f3433993&partnerID=40&md5=4fcfb6f79fd6722edfdbe891f47871fb,"Edge Analytics and Artificial Intelligence are important features of the current smart connected living community. In a society where people, homes, cities, and workplaces are simultaneously connected through various devices, primarily through mobile devices, a considerable amount of data is exchanged, and the processing and storage of these data are laborious and difficult tasks. Edge Analytics allows the collection and analysis of such data on mobile devices, such as smartphones and tablets, without involving any cloud-centred architecture that cannot guarantee real-time responsiveness. Meanwhile, Artificial Intelligence techniques can constitute a valid instrument to process data, limiting the computation time, and optimising decisional processes and predictions in several sectors, such as healthcare. Within this field, in this article, an approach able to evaluate the voice quality condition is proposed. A fully automatic algorithm, based on Deep Learning, classifies a voice as healthy or pathological by analysing spectrogram images extracted by means of the recording of vowel /a/, in compliance with the traditional medical protocol. A light Convolutional Neural Network is embedded in a mobile health application in order to provide an instrument capable of assessing voice disorders in a fast, easy, and portable way. Thus, a straightforward mobile device becomes a screening tool useful for the early diagnosis, monitoring, and treatment of voice disorders. The proposed approach has been tested on a broad set of voice samples, not limited to the most common voice diseases but including all the pathologies present in three different databases achieving F1-scores, over the testing set, equal to 80%, 90%, and 73%. Although the proposed network consists of a reduced number of layers, the results are very competitive compared to those of other ""cutting edge""approaches constructed using more complex neural networks, and compared to the classic deep neural networks, for example, VGG-16 and ResNet-50. © 2021 Association for Computing Machinery.",deep learning; disease detection; spectrogram; Voice analysis; voice classification,Convolutional neural networks; Deep neural networks; Digital storage; Medical imaging; mHealth; Multilayer neural networks; Quality control; Spectrographs; Speech recognition; 'current; Deep learning; Disease detection; Important features; Learning approach; Living environment; Spectrograms; Voice analysis; Voice classification; Voice disorders; Diagnosis
Automated Discovery of Network Cameras in Heterogeneous Web Pages,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119214981&doi=10.1145%2f3450629&partnerID=40&md5=eec30d9ed26707453e389dfaac9948c2,"Reduction in the cost of Network Cameras along with a rise in connectivity enables entities all around the world to deploy vast arrays of camera networks. Network cameras offer real-time visual data that can be used for studying traffic patterns, emergency response, security, and other applications. Although many sources of Network Camera data are available, collecting the data remains difficult due to variations in programming interface and website structures. Previous solutions rely on manually parsing the target website, taking many hours to complete. We create a general and automated solution for aggregating Network Camera data spread across thousands of uniquely structured web pages. We analyze heterogeneous web page structures and identify common characteristics among 73 sample Network Camera websites (each website has multiple web pages). These characteristics are then used to build an automated camera discovery module that crawls and aggregates Network Camera data. Our system successfully extracts 57,364 Network Cameras from 237,257 unique web pages. © 2021 Copyright held by the owner/author(s).",data streaming; multimedia streaming; network cameras; sensor networks; service discovery and interfaces; web cameras; web crawling; Web indexing; web scraping,Automation; Cameras; Media streaming; Sensor networks; Web crawler; Data streaming; Multimedia streaming; Network cameras; Sensors network; Service discovery; Service interfaces; Web camera; Web Crawling; Web indexing; Web scrapings; Websites
Extracting Formats of Service Messages with Varying Payloads,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137621872&doi=10.1145%2f3503159&partnerID=40&md5=966fc0908c53cd6420f074a1268a3baa,"Having precise specifications of service APIs is essential for many Software Engineering activities. Unfortunately, available documentation of services is often inadequate and/or imprecise and, hence, cannot be fully relied upon. Generating service documentation manually is a tedious and error-prone task, especially in light of changes to services. Therefore, there is a need for automated support in generating service documentation. In this work, we present a novel approach to infer the API of a service by analyzing recorded messages sent to and received from this service. Our approach includes a novel, two-level clustering technique to cluster messages, a step that many existing approaches to infer message formats fail to perform precisely in the presence of significant variation of payload information of the available messages. We have evaluated our approach on message traces from four different real-world services. The experimental result shows that our approach is more effective than existing techniques in extracting correct message formats from recorded messages.  © 2022 Association for Computing Machinery.",format extraction; payload variation; positional keyword; Service API,Automated support; Clustering techniques; Engineering activities; Error prone tasks; Format extraction; Message format; Payload variations; Positional keyword; Service API; Service documentation; Software engineering
Neural Network Approximation of Graph Fourier Transform for Sparse Sampling of Networked Dynamics,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119211263&doi=10.1145%2f3461838&partnerID=40&md5=70dfb2b8ca85cf7024de5edf19d4761f,"Infrastructure monitoring is critical for safe operations and sustainability. Like many networked systems, water distribution networks (WDNs) exhibit both graph topological structure and complex embedded flow dynamics. The resulting networked cascade dynamics are difficult to predict without extensive sensor data. However, ubiquitous sensor monitoring in underground situations is expensive, and a key challenge is to infer the contaminant dynamics from partial sparse monitoring data. Existing approaches use multi-objective optimization to find the minimum set of essential monitoring points but lack performance guarantees and a theoretical framework. Here, we first develop a novel Graph Fourier Transform (GFT) operator to compress networked contamination dynamics to identify the essential principal data collection points with inference performance guarantees. As such, the GFT approach provides the theoretical sampling bound. We then achieve under-sampling performance by building auto-encoder (AE) neural networks (NN) to generalize the GFT sampling process and under-sample further from the initial sampling set, allowing a very small set of data points to largely reconstruct the contamination dynamics over real and artificial WDNs. Various sources of the contamination are tested, and we obtain high accuracy reconstruction using around 5%-10% of the network nodes for known contaminant sources, and 50%-75% for unknown source cases, which although larger than that of the schemes for contaminant detection and source identifications, is smaller than the current sampling schemes for contaminant data recovery. This general approach of compression and under-sampled recovery via NN can be applied to a wide range of networked infrastructures to enable efficient data sampling for digital twins. © 2021 Association for Computing Machinery.",graph fourier transform; neural networks; Sampling theory,Computer system recovery; Contamination; Dynamics; Embedded systems; Flow graphs; Monitoring; Multiobjective optimization; Neural networks; Pollution detection; Water pollution; Contaminant sources; Contamination dynamic; Graph Fourier transforms; Infrastructure monitoring; Neural-network approximations; Neural-networks; Performance guarantees; Sampling theory; Sparse sampling; Water distribution networks; Water distribution systems
IBRDM: An Intelligent Framework for Brain Tumor Classification Using Radiomics- And DWT-based Fusion of MRI Sequences,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119200897&doi=10.1145%2f3434775&partnerID=40&md5=847704c2205c4400a51076215435685d,"Brain tumors are one of the critical malignant neurological cancers with the highest number of deaths and injuries worldwide. They are categorized into two major classes, high-grade glioma (HGG) and low-grade glioma (LGG), with HGG being more aggressive and malignant, whereas LGG tumors are less aggressive, but if left untreated, they get converted to HGG. Thus, the classification of brain tumors into the corresponding grade is a crucial task, especially for making decisions related to treatment. Motivated by the importance of such critical threats to humans, we propose a novel framework for brain tumor classification using discrete wavelet transform-based fusion of MRI sequences and Radiomics feature extraction. We utilized the Brain Tumor Segmentation 2018 challenge training dataset for the performance evaluation of our approach, and we extract features from three regions of interest derived using a combination of several tumor regions. We used wrapper method-based feature selection techniques for selecting a significant set of features and utilize various machine learning classifiers, Random Forest, Decision Tree, and Extra Randomized Tree for training the model. For proper validation of our approach, we adopt the five-fold cross-validation technique. We achieved state-of-the-art performance considering several performance metrics, (Acc, Sens, Spec, F1-score, MCC, AUC ) ( 98.60%, 99.05%, 97.33%, 99.05%, 96.42%, 98.19% ), where Acc, Sens, Spec, F1-score, MCC, and AUC represents the accuracy, sensitivity, specificity, F1-score, Matthews correlation coefficient, and area-under-the-curve, respectively. We believe our proposed approach will play a crucial role in the planning of clinical treatment and guidelines before surgery. © 2021 Association for Computing Machinery.",decision tree and random forest; extremely randomized tree; Gliomas,Brain; Discrete wavelet transforms; Feature extraction; Learning systems; Tumors; Brain tumor classifications; Brain tumors; Decision tree and random forest; Extremely randomized tree; F1 scores; High-grade gliomas; Low-grade gliomas; MRI sequences; Random forests; Randomized trees; Decision trees
Data Dissemination for Industry 4.0 Applications in Internet of Vehicles Based on Short-term Traffic Prediction,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119213432&doi=10.1145%2f3430505&partnerID=40&md5=5cfa609b34d9d9f4ef62f099cd268bba,"As a key use case of Industry 4.0 and the Smart City, the Internet of Vehicles (IoV) provides an efficient way for city managers to regulate the traffic flow, improve the commuting performance, reduce the transportation facility cost, alleviate the traffic jam, and so on. In fact, the significant development of Internet of Vehicles has boosted the emergence of a variety of Industry 4.0 applications, e.g., smart logistics, intelligent transforation, and autonomous driving. The prerequisite of deploying these applications is the design of efficient data dissemination schemes by which the interactive information could be effectively exchanged. However, in Internet of Vehicles, an efficient data scheme should adapt to the high node movement and frequent network changing. To achieve the objective, the ability to predict short-term traffic is crucial for making optimal policy in advance. In this article, we propose a novel data dissemination scheme by exploring short-term traffic prediction for Industry 4.0 applications enabled in Internet of Vehicles. First, we present a three-tier network architecture with the aim to simply network management and reduce communication overheads. To capture dynamic network changing, a deep learning network is employed by the controller in this architecture to predict short-term traffic with the availability of enormous traffic data. Based on the traffic prediction, each road segment can be assigned a weight through the built two-dimensional delay model, enabling the controller to make routing decisions in advance. With the global weight information, the controller leverages the ant colony optimization algorithm to find the optimal routing path with minimum delay. Extensive simulations are carried out to demonstrate the accuracy of the traffic prediction model and the superiority of the proposed data dissemination scheme for Industry 4.0 applications. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data dissemination; deep learning; Industry 4.0; internet of vehicles; traffic prediction,Ant colony optimization; Controllers; Cost reduction; Deep learning; Forecasting; Industry 4.0; Network architecture; Traffic congestion; Autonomous driving; Data dissemination; Deep learning; Flow improvers; Internet of vehicle; Performance; Traffic flow; Traffic jams; Traffic prediction; Transportation facilities; Vehicles
QoS aware Mesh-based Multicast Routing Protocols in Edge Ad Hoc Networks: Concepts and Challenges,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119201299&doi=10.1145%2f3428150&partnerID=40&md5=a233b9a38e9194ce5a9c91445b89fe5f,"Multicast communication plays a pivotal role in Edge based Mobile Ad hoc Networks (MANETs). MANETs can provide low-cost self-configuring devices for multimedia data communication that can be used in military battlefield, disaster management, connected living, and public safety networks. A Multicast communication should increase the network performance by decreasing the bandwidth consumption, battery power, and routing overhead. In recent years, a number of multicast routing protocols (MRPs) have been proposed to resolve above listed challenges. Some of them are used for dynamic establishment of reliable route for multimedia data communication. This article provides a detailed survey of the merits and demerits of the recently developed techniques. An ample study of various Quality of Service (QoS) techniques and enhancement is also presented. Later, mesh topology-based MRPs are classified according to enhancement in routing mechanism and QoS modification. This article covers the most recent, robust, and reliable QoS-aware mesh based MRPs, classified on the basis of their operational features, and pros and cons. Finally, a comparative study has been presented on the basis of their performance parameters on the proposed protocols. © 2021 Association for Computing Machinery.",MANETs; mesh based; multicast; quality of service; routing protocol; survey,Convolutional codes; Disaster prevention; Disasters; Mesh generation; MESH networking; Mobile ad hoc networks; Mobile telecommunication systems; Multicasting; Quality of service; Routing protocols; Ad-hoc networks; Classifieds; Edge-based; Mesh based; Mobile ad-hoc networks; Multicast communication; Multicast routing protocol; Quality-of-service; Routing-protocol; Service-aware; Surveys
Malware Classification Based on Multilayer Perception and Word2Vec for IoT Security,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119209184&doi=10.1145%2f3436751&partnerID=40&md5=aaea888d26d11218afeb63f748803be0,"With the construction of smart cities, the number of Internet of Things (IoT) devices is growing rapidly, leading to an explosive growth of malware designed for IoT devices. These malware pose a serious threat to the security of IoT devices. The traditional malware classification methods mainly rely on feature engineering. To improve accuracy, a large number of different types of features will be extracted from malware files in these methods. That brings a high complexity to the classification. To solve these issues, a malware classification method based on Word2Vec and Multilayer Perception (MLP) is proposed in this article. First, for one malware sample, Word2Vec is used to calculate a word vector for all bytes of the binary file and all instructions in the assembly file. Second, we combine these vectors into a 256x256x2-dimensional matrix. Finally, we designed a deep learning network structure based on MLP to train the model. Then the model is used to classify the testing samples. The experimental results prove that the method has a high accuracy of 99.54%. © 2021 Association for Computing Machinery.",IoT; Malware classification; multilayer perception; Word2Vec,Deep learning; Malware; Multilayers; Assembly files; Binary files; Classification methods; Explosive growth; Feature engineerings; High complexity; Malware classifications; Multi-layer perception; Word vectors; Word2vec; Internet of things
"Alexa, Who Am i Speaking To?: Understanding Users' Ability to Identify Third-Party Apps on Amazon Alexa",2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119174171&doi=10.1145%2f3446389&partnerID=40&md5=fe24e00c7caaf35913539debfdf942fd,"Many Internet of Things devices have voice user interfaces. One of the most popular voice user interfaces is Amazon's Alexa, which supports more than 50,000 third-party applications (""skills""). We study how Alexa's integration of these skills may confuse users. Our survey of 237 participants found that users do not understand that skills are often operated by third parties, that they often confuse third-party skills with native Alexa functions, and that they are unaware of the functions that the native Alexa system supports. Surprisingly, users who interact with Alexa more frequently are more likely to conclude that a third-party skill is a native Alexa function. The potential for misunderstanding creates new security and privacy risks: attackers can develop third-party skills that operate without users' knowledge or masquerade as native Alexa functions. To mitigate this threat, we make design recommendations to help users better distinguish native functionality and third-party skills, including audio and visual indicators of native and third-party contexts, as well as a consistent design standard to help users learn what functions are and are not possible on Alexa. © 2021 Association for Computing Machinery.",Internet of Things; network measurement; privacy; security; Smart home,Automation; Data privacy; Intelligent buildings; Network security; Surveys; User interfaces; Network measurement; Privacy; Security; Security and privacy; Security risks; Smart homes; Systems support; Third parties; Third party application (Apps); Voice user interface; Internet of things
Serving at the Edge: An Edge Computing Service Architecture Based on ICN,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119184497&doi=10.1145%2f3464428&partnerID=40&md5=954ee3393e6faf43129880dd0296d9ad,"Different from cloud computing, edge computing moves computing away from the centralized data center and closer to the end-user. Therefore, with the large-scale deployment of edge services, it becomes a new challenge of how to dynamically select the appropriate edge server for computing requesters based on the edge server and network status. In the TCP/IP architecture, edge computing applications rely on centralized proxy servers to select an appropriate edge server, which leads to additional network overhead and increases service response latency. Due to its powerful forwarding plane, Information-Centric Networking (ICN) has the potential to provide more efficient networking support for edge computing than TCP/IP. However, traditional ICN only addresses named data and cannot well support the handle of dynamic content. In this article, we propose an edge computing service architecture based on ICN, which contains the edge computing service session model, service request forwarding strategies, and service dynamic deployment mechanism. The proposed service session model can not only keep the overhead low but also push the results to the computing requester immediately once the computing is completed. However, the service request forwarding strategies can forward computing requests to an appropriate edge server in a distributed manner. Compared with the TCP/IP-based proxy solution, our forwarding strategy can avoid unnecessary network transmissions, thereby reducing the service completion time. Moreover, the service dynamic deployment mechanism decides whether to deploy an edge service on an edge server based on service popularity, so that edge services can be dynamically deployed to hotspot, further reducing the service completion time. © 2021 Association for Computing Machinery.",Edge computing; forwarding strategy; information-centric networking; service architecture,Dynamics; Memory architecture; Network architecture; Quality of service; Transmission control protocol; Architecture-based; Centralised; Computing services; Edge computing; Edge server; Edge services; Forwarding strategies; Information-centric networkings; Service requests; Services Architectures; Edge computing
PANOLA: A Personal Assistant for Supporting Users in Preserving Privacy,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119187931&doi=10.1145%2f3471187&partnerID=40&md5=516056e6633c0b6dd2ea1a1e54edde48,"Privacy is the right of individuals to keep personal information to themselves. When individuals use online systems, they should be given the right to decide what information they would like to share and what to keep private. When a piece of information pertains only to a single individual, preserving privacy is possible by providing the right access options to the user. However, when a piece of information pertains to multiple individuals, such as a picture of a group of friends or a collaboratively edited document, deciding how to share this information and with whom is challenging. The problem becomes more difficult when the individuals who are affected by the information have different, possibly conflicting privacy constraints. Resolving this problem requires a mechanism that takes into account the relevant individuals' concerns to decide on the privacy configuration of information. Because these decisions need to be made frequently (i.e., per each piece of shared content), the mechanism should be automated. This article presents a personal assistant to help end-users with managing the privacy of their content. When some content that belongs to multiple users is about to be shared, the personal assistants of the users employ an auction-based privacy mechanism to regulate the privacy of the content. To do so, each personal assistant learns the preferences of its user over time and produces bids accordingly. Our proposed personal assistant is capable of assisting users with different personas and thus ensures that people benefit from it as they need it. Our evaluations over multiagent simulations with online social network content show that our proposed personal assistant enables privacy-respecting content sharing. © 2021 Association for Computing Machinery.",Autonomous agents; online social networks; privacy; reinforcement learning,E-learning; Information dissemination; Online systems; Privacy-preserving techniques; Reinforcement learning; Social networking (online); End-users; Learn+; Multi-agents simulations; Multiple user; Personal assistants; Personal information; Privacy; Privacy constraints; Privacy mechanisms; Shared contents; Autonomous agents
Machine Learning in Mobile Crowd Sourcing: A Behavior-Based Recruitment Model,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119172131&doi=10.1145%2f3451163&partnerID=40&md5=f7334d74dd13332688311fe6dffcb5f3,"With the advent of mobile crowd sourcing (MCS) systems and its applications, the selection of the right crowd is gaining utmost importance. The increasing variability in the context of MCS tasks makes the selection of not only the capable but also the willing workers crucial for a high task completion rate. Most of the existing MCS selection frameworks rely primarily on reputation-based feedback mechanisms to assess the level of commitment of potential workers. Such frameworks select workers having high reputation scores but without any contextual awareness of the workers, at the time of selection, or the task. This may lead to an unfair selection of workers who will not perform the task. Hence, reputation on its own only gives an approximation of workers' behaviors since it assumes that workers always behave consistently regardless of the situational context. However, following the concept of cross-situational consistency, where people tend to show similar behavior in similar situations and behave differently in disparate ones, this work proposes a novel recruitment system in MCS based on behavioral profiling. The proposed approach uses machine learning to predict the probability of the workers performing a given task, based on their learned behavioral models. Subsequently, a group-based selection mechanism, based on the genetic algorithm, uses these behavioral models in complementation with a reputation-based model to recruit a group of workers that maximizes the quality of recruitment of the tasks. Simulations based on a real-life dataset show that considering human behavior in varying situations improves the quality of recruitment achieved by the tasks and their completion confidence when compared with a benchmark that relies solely on reputation. © 2021 Association for Computing Machinery.",behavioral profiling; Machine learning; mobile crowd sourcing; quality of recruitment; selection management,Behavioral research; Benchmarking; Genetic algorithms; Machine learning; Behavior-based; Behavioral model; Behavioral profiling; Crowd sourcing; Machine-learning; Mobile crowd sourcing; Quality of recruitment; Recruitment models; Selection management; Workers'; Crowdsourcing
Layer-based Composite Reputation Bootstrapping,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119199691&doi=10.1145%2f3448610&partnerID=40&md5=a035f21d478f8f7039d74ebd1c2f3bab,We propose a novel generic reputation bootstrapping framework for composite services. Multiple reputation-related indicators are considered in a layer-based framework to implicitly reflect the reputation of the component services. The importance of an indicator on the future performance of a component service is learned using a modified Random Forest algorithm. We propose a topology-aware Forest Deep Neural Network (fDNN) to find the correlations between the reputation of a composite service and reputation indicators of component services. The trained fDNN model predicts the reputation of a new composite service with the confidence value. Experimental results with real-world dataset prove the efficiency of the proposed approach. © 2021 Association for Computing Machinery.,bootstrapping confidence; composite services; composition topology; Deep Neural Network; Random Forest; Reputation bootstrapping; reputation indicators,Decision trees; Multilayer neural networks; Random forests; Bootstrapping confidence; Composite services; Composition topology; Future performance; Neural network model; Random forest algorithm; Random forests; Reputation bootstrapping; Reputation indicator; Topology aware; Deep neural networks
Incremental Group-Level Popularity Prediction in Online Social Networks,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119196905&doi=10.1145%2f3461839&partnerID=40&md5=ff61088eb2afcfc2a73b05d66b79030e,"Predicting the popularity of web contents in online social networks is essential for many applications. However, existing works are usually under non-incremental settings. In other words, they have to rebuild models from scratch when new data occurs, which are inefficient in big data environments. It leads to an urgent need for incremental prediction, which can update previous results with new data and conduct prediction incrementally. Moreover, the promising direction of group-level popularity prediction has not been well treated, which explores fine-grained information while keeping a low cost. To this end, we identify the problem of incremental group-level popularity prediction, and propose a novel model IGPP to address it. We first predict the group-level popularity incrementally by exploiting the incremental CANDECOMP/PARAFCAC (CP) tensor decomposition algorithm. Then, to reduce the cumulative error by incremental prediction, we propose three strategies to restart the CP decomposition. To the best of our knowledge, this is the first work that identifies and solves the problem of incremental group-level popularity prediction. Extensive experimental results show significant improvements of the IGPP method over other works both in the prediction accuracy and the efficiency. © 2021 Association for Computing Machinery.",Group level; incremental approach; information diffusion; online social networks; popularity prediction; tensor analysis,Forecasting; Tensors; Data environment; Fine grained; Group level; Incremental approach; Information diffusion; Low-costs; Popularity predictions; Tensor analysis; Tensor decomposition; Web content; Social networking (online)
Using Social Media Data to Analyse Issue Engagement during the 2017 German Federal Election,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119171294&doi=10.1145%2f3467020&partnerID=40&md5=771f37d183c351602cf694eab562cd89,"A fundamental tenet of democracy is that political parties present policy alternatives, such that the public can participate in the decision-making process. Parties, however, strategically control public discussion by emphasising topics that they believe will highlight their strengths in voters' minds. Political strategy has been studied for decades, mostly by manually annotating and analysing party statements, press coverage, or TV ads. Here we build on recent work in the areas of computational social science and eDemocracy, which studied these concepts computationally with social media. We operationalize issue engagement and related political science theories to measure and quantify politicians' communication behavior using more than 366k Tweets posted by over 1,000 prominent German politicians in the 2017 election year. To this end, we first identify issues in posted Tweets by utilising a hashtag-based approach well known in the literature. This method allows several prominent issues featuring in the political debate on Twitter that year to be identified. We show that different political parties engage to a larger or lesser extent with these issues. The findings reveal differing social media strategies by parties located at different sides of the political left-right scale, in terms of which issues they engage with, how confrontational they are and how their strategies evolve in the lead-up to the election. Whereas previous work has analysed the general public's use of Twitter or politicians' communication in terms of cross-party polarisation, this is the first study of political science theories, relating to issue engagement, using politicians' social media data. © 2021 Association for Computing Machinery.",Computational social science; eDemocracy; issue engagement; political communication; twitter,Behavioral research; Computation theory; Decision making; Computational social science; Decision-making process; E-democracy; German Federal Elections; Issue engagement; Political communication; Political parties; Political science; Social media; Social media datum; Social networking (online)
PCAM: A Data-driven Probabilistic Cyber-alert Management Framework,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137814702&doi=10.1145%2f3511101&partnerID=40&md5=d45998f50508425d202e5db94167c167,"We propose PCAM, a Probabilistic Cyber-Alert Management framework, that enables chief information security officers to better manage cyber-alerts. Workers in Cyber Security Operation Centers usually work in 8- or 12-hour shifts. Before a shift, PCAM analyzes data about all past alerts and true alerts during the shift time-frame to schedule a given set of analysts in accordance with workplace constraints so that the expected number of ""uncovered""true alerts (i.e., true alerts not shown to an analyst) is minimized. PCAM achieves this by formulating the problem as a bi-level non-linear optimization problem and then shows how to linearize and solve this complex problem. We have tested PCAM extensively. Using statistics derived from 44 days of real-world alert data, we are able to minimize the expected number of true alerts that are not manually examined by a team consisting of junior, senior, and principal analysts. We are also able to identify the optimal mix of junior, senior, and principal analysts needed during both day and night shifts given a budget, outperforming some reasonable baselines. We tested PCAM's proposed schedule (from statistics on 44 days) on a further 6 days of data, using an off-the-shelf false alarm classifier to predict which alerts are real and which ones are false. Moreover, we show experimentally that PCAM is robust to various kinds of errors in the statistics used.  © 2022 Association for Computing Machinery.",Cyber-Alert Management; Optimization; Prediction; Scheduling,Cybersecurity; Nonlinear programming; Scheduling; Alert management; Chief information security officers; Cybe-alert management; Cyber security; Data driven; Management frameworks; Optimisations; Probabilistics; Scheduling; Workers'; Budget control
Virtual Reality Aided High-Quality 3D Reconstruction by Remote Drones,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119199966&doi=10.1145%2f3458930&partnerID=40&md5=f479b5a7eef6cac0250d987eba9bde9d,"Artificial intelligence including deep learning and 3D reconstruction methods is changing the daily life of people. Now, an unmanned aerial vehicle that can move freely in the air and avoid harsh ground conditions has been commonly adopted as a suitable tool for 3D reconstruction. The traditional 3D reconstruction mission based on drones usually consists of two steps: image collection and offline post-processing. But there are two problems: one is the uncertainty of whether all parts of the target object are covered, and another is the tedious post-processing time. Inspired by modern deep learning methods, we build a telexistence drone system with an onboard deep learning computation module and a wireless data transmission module that perform incremental real-time dense reconstruction of urban cities by itself. Two technical contributions are proposed to solve the preceding issues. First, based on the popular depth fusion surface reconstruction framework, we combine it with a visual-inertial odometry estimator that integrates the inertial measurement unit and allows for robust camera tracking as well as high-accuracy online 3D scan. Second, the capability of real-time 3D reconstruction enables a new rendering technique that can visualize the reconstructed geometry of the target as navigation guidance in the HMD. Therefore, it turns the traditional path-planning-based modeling process into an interactive one, leading to a higher level of scan completeness. The experiments in the simulation system and our real prototype demonstrate an improved quality of the 3D model using our artificial intelligence leveraged drone system. © 2021 Association for Computing Machinery.",3D reconstruction; human-robot-interaction; telexistence; unmanned aerial vehicle; virtual reality,3D modeling; Antennas; Deep learning; Drones; Human robot interaction; Image reconstruction; Intelligent robots; Motion planning; Three dimensional computer graphics; 3D reconstruction; Daily lives; Drone system; Ground conditions; High quality; Humans-robot interactions; Post-processing; Reconstruction method; Step images; Telexistence; Virtual reality
Secure Distributed Mobile Volunteer Computing with Android,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119169181&doi=10.1145%2f3428151&partnerID=40&md5=57aaa0cf299ba5ca1e6d8059beaea0d2,"Volunteer Computing provision of seamless connectivity that enables convenient and rapid deployment of greener and cheaper computing infrastructure is extremely promising to complement next-generation distributed computing systems. Undoubtedly, without tactile Internet and secure VC ecosystems, harnessing its full potentials and making it an alternative viable and reliable computing infrastructure is next to impossible. Android-enabled smart devices, applications, and services are inevitable for Volunteer computing. Contrarily, the progressive developments of sophisticated Android malware may reduce its exponential growth. Besides, Android malwares are considered the most potential and persistent cyber threat to mobile VC systems. To secure Android-based mobile volunteer computing, the authors proposed MulDroid, an efficient and self-learning autonomous hybrid (Long-Short-Term Memory, Convolutional Neural Network, Deep Neural Network) multi-vector Android malware threat detection framework. The proposed mechanism is highly scalable with well-coordinated infrastructure and self-optimizing capabilities to proficiently tackle fast-growing dynamic variants of sophisticated malware threats and attacks with 99.01% detection accuracy. For a comprehensive evaluation, the authors employed current state-of-the-art malware datasets (Android Malware Dataset, Androzoo) with standard performance evaluation metrics. Moreover, MulDroid is compared with our constructed contemporary hybrid DL-driven architectures and benchmark algorithms. Our proposed mechanism outperforms in terms of detection accuracy with a trivial tradeoff speed efficiency. Additionally, a 10-fold cross-validation is performed to explicitly show unbiased results. © 2021 Association for Computing Machinery.",android malware; deep learning (DL); tactile internet; Volunteer computing (VC),Android (operating system); Convolutional neural networks; Distributed computer systems; Malware; Mobile security; Android malware; Computing infrastructures; Deep learning; Detection accuracy; Distributed computing systems; Rapid deployments; Seamless connectivity; Tactile internet; Volunteer computing; Deep neural networks
Neural Network Pruning by Recurrent Weights for Finance Market,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137845161&doi=10.1145%2f3433547&partnerID=40&md5=aa6f5ae45cd0122d583fb2c63d0caa63,"Convolutional Neural Networks (CNNs) and deep learning technology are applied in current financial market to rapidly promote the development of finance market and Internet economy. The continuous development of neural networks with more hidden layers improves the performance but increases the computational complexity. Generally, channel pruning methods are useful to compact neural networks. However, typical channel pruning methods would remove layers by mistake due to the static pruning ratio of manual setting, which could destroy the whole structure of neural networks. It is difficult to improve the ratio of compressing neural networks only by pruning channels while maintaining good network structures. Therefore, we propose a novel neural Networks Pruning by Recurrent Weights (NPRW) that can repeatedly evaluate the significance of weights and adaptively adjust them to compress neural networks within acceptable loss of accuracy. The recurrent weights with low sensitivity are compulsorily set to zero by evaluating the magnitude of weights, and pruned network only uses a few significant weights. Then, we add the regularization to the scaling factors on neural networks, in which recurrent weights with high sensitivity can be dynamically updated and weights of low sensitivity stay at zero invariably. By this way, the significance of channels can be quantitatively evaluated by recurrent weights. It has been verified with typical neural networks of LeNet, VGGNet, and ResNet on multiple benchmark datasets involving stock index futures, digital recognition, and image classification. The pruned LeNet-5 achieves the 58.9% reduction amount of parameters with 0.29% loss of total accuracy for Shanghai and Shenzhen 300 stock index futures. As for the CIFAR-10, the pruned VGG-19 reduces more than 50% FLOPs, and the decrease of network accuracy is less than 0.5%. In addition, the pruned ResNet-164 tested on the SVHN reduces more than 58% FLOPs with relative improvement on accuracy by 0.11%. © 2022 Association for Computing Machinery.",Channel pruning; finance market; neural networks; recurrent weights,Classification (of information); Convolution; Convolutional neural networks; Multilayer neural networks; Recurrent neural networks; Channel pruning; Convolutional neural network; Finance market; Learning technology; Low sensitivity; Network pruning; Neural-networks; Pruning methods; Recurrent weight; Stock index futures; Commerce
Multi-criteria Web Services Selection: Balancing the Quality of Design and Quality of Service,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119174262&doi=10.1145%2f3446388&partnerID=40&md5=50283a6184db5fb83083ead2ae73fb29,"Web service composition allows developers to create applications via reusing available services that are interoperable to each other. The process of selecting relevant Web services for a composite service satisfying the developer requirements is commonly acknowledged to be hard and challenging, especially with the exponentially increasing number of available Web services on the Internet. The majority of existing approaches on Web Services Selection are merely based on the Quality of Service (QoS) as a basic criterion to guide the selection process. However, existing approaches tend to ignore the service design quality, which plays a crucial role in discovering, understanding, and reusing service functionalities. Indeed, poorly designed Web service interfaces result in service anti-patterns, which are symptoms of bad design and implementation practices. The existence of anti-pattern instances in Web service interfaces typically complicates their reuse in real-world service-based systems and may lead to several maintenance and evolution problems. To address this issue, we introduce a new approach based on the Multi-Objective and Optimization on the basis of Ratio Analysis method (MOORA) as a multi-criteria decision making (MCDM) method to select Web services based on a combination of their (1) QoS attributes and (2) QoS design. The proposed approach aims to help developers to maintain the soundness and quality of their service composite development processes. We conduct a quantitative and qualitative empirical study to evaluate our approach on a Quality of Web Service dataset. We compare our MOORA-based approach against four commonly used MCDM methods as well as a recent state-of-the-art Web service selection approach. The obtained results show that our approach outperforms state-of-the-art approaches by significantly improving the service selection quality of top-k selected services while providing the best trade-off between both service design quality and desired QoS values. Furthermore, we conducted a qualitative evaluation with developers. The obtained results provide evidence that our approach generates a good trade-off for what developers need regarding both QoS and quality of design. Our selection approach was evaluated as ""relevant""from developers point of view, in improving the service selection task with an average score of 3.93, compared to an average of 2.62 for the traditional QoS-based approach. © 2018 Association for Computing Machinery.",Multi-criteria decision Making; quality of service and design; service-based software systems,Decision making; Economic and social effects; Interoperability; Quality control; Web services; Websites; Multi criteria decision-making; Multicriteria decision-making; Multicriterion decision makings; Quality of design; Quality-of-service; Service-based; Service-based software system; Services designs; Software-systems; Web service selection; Quality of service
Negative Information Measurement at AI Edge: A New Perspective for Mental Health Monitoring,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133916345&doi=10.1145%2f3471902&partnerID=40&md5=17bd71711caac2368ee884ef57f32a2e,"The outbreak of the corona virus disease 2019 (COVID-19) has caused serious harm to people's physical and mental health. Due to the serious situation of the epidemic, a lot of negative energy information increases people's psychological burden. However, effective interventions against mental health problems are not in abundance. To address such challenges, in this article, we propose the concept of negative information to describe information that has a negative impact on people's mental health. To achieve the measurement of negative information, the level of mental health inversely measures the degree of negative information. Specifically, we design a system to measure the negative information used to monitor the mental health state of the user under the impact of negative information. The cognition of mental health is realized based on the intelligent algorithm deployed on the edge cloud, and the needs of users can be responded to in real time in practical applications. Finally, we use real collected dataset to verify the influence of negative information. The experiments show that the system can achieve negative information measurement and provide an effective countermeasure for solving mental health problems during a pandemic situation. © 2022 Association for Computing Machinery.",cognitive computing; edge cloud; mental health; Negative information,Edge computing; Viruses; Cognitive Computing; Edge clouds; Energy information; Health monitoring; Information measurement; Measurements of; Mental health; Negative information; Physical health; Virus disease; Health
Adaptive Management of Volatile Edge Systems at Runtime with Satisfiability,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119179399&doi=10.1145%2f3470658&partnerID=40&md5=730db1eb38fc71525eaef3c0c6db0d46,"Edge computing offers the possibility of deploying applications at the edge of the network. To take advantage of available devices' distributed resources, applications often are structured as microservices, often having stringent requirements of low latency and high availability. However, a decentralized edge system that the application may be intended for is characterized by high volatility, due to devices making up the system being unreliable or leaving the network unexpectedly. This makes application deployment and assurance that it will continue to operate under volatility challenging. We propose an adaptive framework capable of deploying and efficiently maintaining a microservice-based application at runtime, by tackling two intertwined problems: (i) finding a microservice placement across device hosts and (ii) deriving invocation paths that serve it. Our objective is to maintain correct functionality by satisfying given requirements in terms of end-to-end latency and availability, in a volatile edge environment. We evaluate our solution quantitatively by considering performance and failure recovery. © 2021 Association for Computing Machinery.",adaptive systems; distributed systems; edge computing; Resource management,Adaptive Management; Distributed resources; Edge computing; High availability; Low latency; Low-high; Resource management; Runtimes; Satisfiability; Stringent requirement; Edge computing
Proactive Defense for Internet-of-things: Moving Target Defense with Cyberdeception,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119197643&doi=10.1145%2f3467021&partnerID=40&md5=2bbf287aae846ff658134ba79df071e6,"Resource constrained Internet-of-Things (IoT) devices are highly likely to be compromised by attackers, because strong security protections may not be suitable to be deployed. This requires an alternative approach to protect vulnerable components in IoT networks. In this article, we propose an integrated defense technique to achieve intrusion prevention by leveraging cyberdeception (i.e., a decoy system) and moving target defense (i.e., network topology shuffling). We evaluate the effectiveness and efficiency of our proposed technique analytically based on a graphical security model in a software-defined networking (SDN)-based IoT network. We develop four strategies (i.e., fixed/random and adaptive/hybrid) to address ""when""to perform network topology shuffling and three strategies (i.e., genetic algorithm/decoy attack path-based optimization/random) to address ""how""to perform network topology shuffling on a decoy-populated IoT network, and we analyze which strategy can best achieve a system goal, such as prolonging the system lifetime, maximizing deception effectiveness, maximizing service availability, or minimizing defense cost. We demonstrated that a software-defined IoT network running our intrusion prevention technique at the optimal parameter setting prolongs system lifetime, increases attack complexity of compromising critical nodes, and maintains superior service availability compared with a counterpart IoT network without running our intrusion prevention technique. Further, when given a single goal or a multi-objective goal (e.g., maximizing the system lifetime and service availability while minimizing the defense cost) as input, the best combination of ""when""and ""how""strategies is identified for executing our proposed technique under which the specified goal can be best achieved. © 2021 Association for Computing Machinery.",graphical security models; Internet-of-Things; moving target defense; software defined networking,Cybersecurity; Genetic algorithms; Network security; Software defined networking; Topology; Defense costs; Graphical security model; Intrusion prevention; Moving target defense; Network topology; Prevention techniques; Security modeling; Services availability; Software-defined networkings; System lifetimes; Internet of things
Semi-Direct Monocular Visual-Inertial Odometry Using Point and Line Features for IoV,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119185363&doi=10.1145%2f3432248&partnerID=40&md5=875adff7ab45428fca7ffee14fd40f74,"The precise measuring of vehicle location has been a critical task in enhancing the autonomous driving in terms of intelligent decision making and safe transportation. Internet of Vehicles (IoV) is an important infrastructure in support of autonomous driving, allowing real-time road information exchanging and sharing for localizing vehicles. Global positioning System (GPS) is widely used in the traditional IoV system. GPS is unable to meet the key application requirements of autonomous driving due to meter level error and signal deterioration. In this article, we propose a novel solution, named Semi-Direct Monocular Visual-Inertial Odometry using Point and Line Features (SDMPL-VIO) for precise vehicle localization. Our SDMPL-VIO model takes advantage of a low-cost Inertial Measurement Unit (IMU) and monocular camera, using them as the sensor to acquire the surrounding environmental information. Visual-Inertial Odometry (VIO), taking into account both point and line features, is proposed, which is able to deal with both weak texture and dynamic environment. We use a semi-direct method to deal with keyframes and non-keyframes, respectively. Dual sliding window mechanisms can effectively fuse point-line and IMU information. To evaluate our SDMPL-VIO system model, we conduct extensive experiments on both an indoor dataset (i.e., EuRoC) and an outdoor dataset (i.e., KITTI) from the real-world applications, respectively. The experimental results show that the accuracy of SDMPL-VIO proposed by us is better than the mainstream VIO system at present. Especially in the weak texture of the datasets, fast-moving datasets, and other challenging datasets, SDMPL-VIO has a relatively high robustness. © 2021 Association for Computing Machinery.",Internet of vehicles; point-dine feature; semi-direct; visual-inertial odometry,Decision making; Deterioration; Global positioning system; Textures; Autonomous driving; Inertial measurements units; Internet of vehicle; Key-frames; Line features; Odometry; Point features; Point-dine feature; Semi-direct; Visual-inertial odometry; Autonomous vehicles
Data-Driven Prediction and Optimization of Energy Use for Transit Fleets of Electric and ICE Vehicles,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119175281&doi=10.1145%2f3433992&partnerID=40&md5=d4757c658450232f59b10ad542d86f7d,"Due to the high upfront cost of electric vehicles, many public transit agencies can afford only mixed fleets of internal combustion and electric vehicles. Optimizing the operation of such mixed fleets is challenging because it requires accurate trip-level predictions of electricity and fuel use as well as efficient algorithms for assigning vehicles to transit routes. We present a novel framework for the data-driven prediction of trip-level energy use for mixed-vehicle transit fleets and for the optimization of vehicle assignments, which we evaluate using data collected from the bus fleet of CARTA, the public transit agency of Chattanooga, TN. We first introduce a data collection, storage, and processing framework for system-level and high-frequency vehicle-level transit data, including domain-specific data cleansing methods. We train and evaluate machine learning models for energy prediction, demonstrating that deep neural networks attain the highest accuracy. Based on these predictions, we formulate the problem of minimizing energy use through assigning vehicles to fixed-route transit trips. We propose an optimal integer program as well as efficient heuristic and meta-heuristic algorithms, demonstrating the scalability and performance of these algorithms numerically using the transit network of CARTA. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",combinatorial optimization; deep learning; electric vehicle; energy use; environmental impact; genetic algorithm; integer program; machine learning; public transportation,Combinatorial optimization; Data mining; Deep neural networks; Digital storage; Fleet operations; Forecasting; Genetic algorithms; Heuristic algorithms; Integer programming; Urban transportation; Vehicles; Data driven; Deep learning; Energy use; Integer program; Internal combustion; Optimisations; Public transit; Public transportation; Transit agencies; Transit fleets; Environmental impact
GREENHOME: A Household Energy Consumption and CO2 Footprint Metering Environment,2022,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135878668&doi=10.1145%2f3505264&partnerID=40&md5=dc58e33a475f6ab0ad547c315c938f92,"This article presents the GREENHOME environment, a toolkit providing several data analytical tools for metering household energy consumption and CO2 footprint under different perspectives. GREENHOME enables a multi-perspective analysis of household energy consumption and CO2 footprint using and combining several variables through various statistics and data mining algorithms. To test GREENHOME, the article reports on experiments conducted for modelling and forecasting energy consumption and CO2 footprint in the context of the Triple-A European project. © 2022 Association for Computing Machinery.",big data; CO<sub>2</sub> footprint; Energy consumption; internet of things,Carbon dioxide; Data mining; Energy utilization; Internet of things; Analytical tool; CO2 footprint; Data mining algorithm; Energy-consumption; European program; Household energy consumption; Modeling and forecasting; Multi-perspective; Several variables; Big data
Topic-aware Incentive Mechanism for Task Diffusion in Mobile Crowdsourcing through Social Network,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144431016&doi=10.1145%2f3487580&partnerID=40&md5=42c1bb7e33880029123a38bf4e96d9b0,"Crowdsourcing has become an efficient paradigm to utilize human intelligence to perform tasks that are challenging for machines. Many incentive mechanisms for crowdsourcing systems have been proposed. However, most of existing incentive mechanisms assume that there are sufficient participants to perform crowdsourcing tasks. In large-scale crowdsourcing scenarios, this assumption may be not applicable. To address this issue, we diffuse the crowdsourcing tasks in social network to increase the number of participants. To make the task diffusion more applicable to crowdsourcing system, we enhance the classic Independent Cascade model so the influence is strongly connected with both the types and topics of tasks. Based on the tailored task diffusion model, we formulate the Budget Feasible Task Diffusion (BFTD) problem for maximizing the value function of platform with constrained budget. We design a parameter estimation algorithm based on Expectation Maximization algorithm to estimate the parameters in proposed task diffusion model. Benefitting from the submodular property of the objective function, we apply the budget-feasible incentive mechanism, which satisfies desirable properties of computational efficiency, individual rationality, budget-feasible, truthfulness, and guaranteed approximation, to stimulate the task diffusers. The simulation results based on two real-world datasets show that our incentive mechanism can improve the number of active users and the task completion rate by 9.8% and 11%, on average. © 2021 Association for Computing Machinery.",EM algorithm; incentive mechanism; Mobile crowdsourcing; reverse auction; social network,Budget control; Computational efficiency; Diffusion; Electronic commerce; Image segmentation; Maximum principle; Social networking (online); Cascade modeling; Diffusion model; EM algorithms; Human intelligence; Incentive mechanism; Large-scales; Mobile crowdsourcing; Property; Reverse auction; Social network; Crowdsourcing
A COVID-19 Detection Algorithm Using Deep Features and Discrete Social Learning Particle Swarm Optimization for Edge Computing Devices,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129108577&doi=10.1145%2f3453170&partnerID=40&md5=e48e2bb0f5815ef60049944889f0599e,"COVID-19 has been spread around the world and has caused a huge number of deaths. Early detection of this disease is the most efficient way to prevent its rapid spread. Due to the development of internet technology and edge intelligence, developing an early detection system for COVID-19 in the medical environment of the Internet of Things (IoT) can effectively alleviate the spread of the disease. In this paper, a detection algorithm is developed, which can detect COVID-19 effectively by utilizing the features from Chest X-ray (CXR) images. First, a pre-trained model (ResNet18) is adopted for feature extraction. Then, a discrete social learning particle swarm optimization algorithm (DSLPSO) is proposed for feature selection. By filtering redundant and irrelevant features, the dimensionality of the feature vector is reduced. Finally, the images are classified by a Support Vector Machine (SVM) for COVID-19 detection. Experimental results show that the proposed algorithm can achieve competitive performance with fewer features, which is suitable for edge computing devices with lower computation power.  © 2021 Association for Computing Machinery.",COVID-19; DSLPSO; edge computing devices; ResNet18; SVM,Deep learning; Edge computing; Feature Selection; Internet of things; Learning algorithms; Particle swarm optimization (PSO); Signal detection; Support vector machines; Swarm intelligence; Computing devices; Detection algorithm; Discrete social learning particle swarm optimization algorithm; Edge computing; Edge computing device; Particle swarm; Particle swarm optimization algorithm; Resnet18; Social learning; Support vectors machine; COVID-19
Introduction To The Special Section On Edge/Fog Computing For Infectious Disease Intelligence,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137745109&doi=10.1145%2f3494119&partnerID=40&md5=8aff241464562c0af11a026b063f9bd4,[No abstract available],,
A Lossless Data-Hiding based IoT Data Authenticity Model in Edge-AI for Connected Living,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137718895&doi=10.1145%2f3453171&partnerID=40&md5=b2ea2eaa07ca89fdd15e9e3fb1249eef,"Edge computing is an emerging technology for the acquisition of Internet-of-Things (IoT) data and provisioning different services in connected living. Artificial Intelligence (AI) powered edge devices (edge-AI) facilitate intelligent IoT data acquisition and services through data analytics. However, data in edge networks are prone to several security threats such as external and internal attacks and transmission errors. Attackers can inject false data during data acquisition or modify stored data in the edge data storage to hamper data analytics. Therefore, an edge-AI device must verify the authenticity of IoT data before using them in data analytics. This article presents an IoT data authenticity model in edge-AI for a connected living using data hiding techniques. Our proposed data authenticity model securely hides the data source's identification number within IoT data before sending it to edge devices. Edge-AI devices extract hidden information for verifying data authenticity. Existing data hiding approaches for biosignal cannot reconstruct original IoT data after extracting the hidden message from it (i.e., lossy) and are not usable for IoT data authenticity. We propose the first lossless IoT data hiding technique in this article based on error-correcting codes (ECCs). We conduct several experiments to demonstrate the performance of our proposed method. Experimental results establish the lossless property of the proposed approach while maintaining other data hiding properties.  © 2021 Association for Computing Machinery.",connected living; data authenticity; Edge-AI; lossless data hiding; reversible IoT steganography,Authentication; Data acquisition; Data Analytics; Data mining; Digital storage; Internet of things; Technology transfer; Connected living; Data analytics; Data authenticity; Data-hiding; Edge computing; Edge-artificial intelligence; Lossless; Lossless data hiding; Property; Reversible internet-of-thing steganography; Steganography
MEC-Based Jamming-Aided Anti-Eavesdropping with Deep Reinforcement Learning for WBANs,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129549319&doi=10.1145%2f3453186&partnerID=40&md5=45c05f18d9cd56d5f9368c00af049c3c,"Wireless body area network (WBAN) suffers secure challenges, especially the eavesdropping attack, due to constraint resources. In this article, deep reinforcement learning (DRL) and mobile edge computing (MEC) technology are adopted to formulate a DRL-MEC-based jamming-aided anti-eavesdropping (DMEC-JAE) scheme to resist the eavesdropping attack without considering the channel state information. In this scheme, a MEC sensor is chosen to send artificial jamming signals to improve the secrecy rate of the system. Power control technique is utilized to optimize the transmission power of both the source sensor and the MEC sensor to save energy. The remaining energy of the MEC sensor is concerned to ensure routine data transmission and jamming signal transmission. Additionally, the DMEC-JAE scheme integrates with transfer learning for a higher learning rate. The performance bounds of the scheme concerning the secrecy rate, energy consumption, and the utility are evaluated. Simulation results show that the DMEC-JAE scheme can approach the performance bounds with high learning speed, which outperforms the benchmark schemes. © 2021 Association for Computing Machinery.",anti-eavesdropping; mobile edge computing; power control; Wireless body area networks,Benchmarking; Channel state information; Computing power; Deep learning; Energy utilization; Jamming; Mobile edge computing; Power control; Security systems; Wireless local area networks (WLAN); Anti-eavesdropping; Channel-state information; Computing technology; Eavesdropping attacks; Higher learning; Jamming signals; Performance bounds; Power-control; Reinforcement learnings; Wireless body area network; Reinforcement learning
Towards Communication-Efficient and Attack-Resistant Federated Edge Learning for Industrial Internet of Things,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122776284&doi=10.1145%2f3453169&partnerID=40&md5=31a0574cc4a57ad4b96dc3ac0e7d0034,"Federated Edge Learning (FEL) allows edge nodes to train a global deep learning model collaboratively for edge computing in the Industrial Internet of Things (IIoT), which significantly promotes the development of Industrial 4.0. However, FEL faces two critical challenges: communication overhead and data privacy. FEL suffers from expensive communication overhead when training large-scale multi-node models. Furthermore, due to the vulnerability of FEL to gradient leakage and label-flipping attacks, the training process of the global model is easily compromised by adversaries. To address these challenges, we propose a communication-efficient and privacy-enhanced asynchronous FEL framework for edge computing in IIoT. First, we introduce an asynchronous model update scheme to reduce the computation time that edge nodes wait for global model aggregation. Second, we propose an asynchronous local differential privacy mechanism, which improves communication efficiency and mitigates gradient leakage attacks by adding well-designed noise to the gradients of edge nodes. Third, we design a cloud-side malicious node detection mechanism to detect malicious nodes by testing the local model quality. Such a mechanism can avoid malicious nodes participating in training to mitigate label-flipping attacks. Extensive experimental studies on two real-world datasets demonstrate that the proposed framework can not only improve communication efficiency but also mitigate malicious attacks while its accuracy is comparable to traditional FEL frameworks.  © 2021 Association for Computing Machinery.",edge intelligence; Federated edge learning; gradient leakage attack; local differential privacy; poisoning attack,Data privacy; Deep learning; Edge computing; Efficiency; Network security; Communication overheads; Differential privacies; Edge computing; Edge intelligence; Edge nodes; Federated edge learning; Global models; Gradient leakage attack; Local differential privacy; Poisoning attacks; Internet of things
Labeling Privacy Protection SVM Using Privileged Information for COVID-19 Diagnosis,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137648606&doi=10.1145%2f3475868&partnerID=40&md5=1138bf5d6d052e1cbdd7ac28ca853fec,"Edge/fog computing works at the local area network level or devices connected to the sensor or the gateway close to the sensor. These nodes are located in different degrees of proximity to the user, while the data processing and storage are distributed among multiple nodes. In healthcare applications in the Internet of things, when data is transmitted through insecure channels, its privacy and security are the main issues. In recent years, learning from label proportion methods, represented by inverse calibration (InvCal) method, have tried to predict the class label based on class label proportions in certain groups. For privacy protection, the class label of the sample is often sensitive and invisible. As a compromise, only the proportion of class labels in certain groups can be used in these methods. However, due to their weak labeling scheme, their classification performance is often unsatisfactory. In this article, a labeling privacy protection support vector machine using privileged information, called LPP-SVM-PI, is proposed to promote the accuracy of the classifier in infectious disease diagnosis. Based on the framework of the InvCal method, besides using the proportion information of the class label, the idea of learning using privileged information is also introduced to capture the additional information of groups. The slack variables in LPP-SVM-PI are represented as correcting function and projected into the correcting space so that the hidden information of training samples in groups is captured by relaxing the constraints of the classification model. The solution of LPP-SVM-PI can be transformed into a classic quadratic programming problem. The experimental dataset is collected from the Coronavirus disease 2019 (COVID-19) transcription polymerase chain reaction at Hospital Israelita Albert Einstein in Brazil. In the experiment, LPP-SVM-PI is efficiently applied for COVID-19 diagnosis.  © 2021 Association for Computing Machinery.",COVID-19; Fog/edge computing; labeling privacy protection; learning using privileged information; supervised classifier,Classification (of information); Diagnosis; Digital storage; Gateways (computer networks); Information use; Inverse problems; Linear programming; Polymerase chain reaction; Quadratic programming; Sensitive data; Support vector machines; Calibration method; Class labels; Edge computing; Fog/edge computing; Labeling privacy protection; Labelings; Learning using privileged information; Network level; Privacy protection; Supervised classifiers; COVID-19
Forecasting Trend of Coronavirus Disease 2019 using Multi-Task Weighted TSK Fuzzy System,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137614940&doi=10.1145%2f3475870&partnerID=40&md5=7e56adf1cd0999be8c5506a05eeec81b,"Artificial intelligence-(AI) based fog/edge computing has become a promising paradigm for infectious disease. Various AI algorithms are embedded in cooperative fog/edge devices to construct medical Internet of Things environments, infectious disease forecast systems, smart health, and so on. However, these systems are usually done in isolation, which is called single-task learning. They do not consider the correlation and relationship between multiple/different tasks, so some common information in the model parameters or data characteristics is lost. In this study, each data center in fog/edge computing is considered as a task in the multi-task learning framework. In such a learning framework, a multi-task weighted Takagi-Sugeno-Kang (TSK) fuzzy system, called MW-TSKFS, is developed to forecast the trend of Coronavirus disease 2019 (COVID-19). MW-TSKFS provides a multi-task learning strategy for both antecedent and consequent parameters of fuzzy rules. First, a multi-task weighted fuzzy c-means clustering algorithm is developed for antecedent parameter learning, which extracts the public information among all tasks and the private information of each task. By sharing the public cluster centroid and public membership matrix, the differences of commonality and individuality can be further exploited. For consequent parameter learning of MW-TSKFS, a multi-task collaborative learning mechanism is developed based on ϵ-insensitive criterion and L2 norm penalty term, which can enhance the generalization and forecasting ability of the proposed fuzzy system. The experimental results on the real COVID-19 time series show that the forecasting tend model based on multi-task the weighted TSK fuzzy system has a high application value.  © 2021 Association for Computing Machinery.",COVID-19; Fog/edge computing; multi-task weighted fuzzy c-means clustering; multi-tasking learning; TSK fuzzy system,Clustering algorithms; Embedded systems; Fog; Fog computing; Forecasting; Fuzzy clustering; Fuzzy inference; Learning systems; Edge computing; Fog/edge computing; Fuzzy C-Means clustering; Infectious disease; Multi tasking; Multi tasks; Multi-task weighted fuzzy c-mean clustering; Multi-tasking learning; Takagi-Sugeno-Kang fuzzy system; Weighted fuzzy c-means; COVID-19
Synergic Deep Learning for Smart Health Diagnosis of COVID-19 for Connected Living and Smart Cities,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126388299&doi=10.1145%2f3453168&partnerID=40&md5=b13751451c7f5ab78f844b62edc68f02,"COVID-19 pandemic has led to a significant loss of global deaths, economical status, and so on. To prevent and control COVID-19, a range of smart, complex, spatially heterogeneous, control solutions, and strategies have been conducted. Earlier classification of 2019 novel coronavirus disease (COVID-19) is needed to cure and control the disease. It results in a requirement of secondary diagnosis models, since no precise automated toolkits exist. The latest finding attained using radiological imaging techniques highlighted that the images hold noticeable details regarding the COVID-19 virus. The application of recent artificial intelligence (AI) and deep learning (DL) approaches integrated to radiological images finds useful to accurately detect the disease. This article introduces a new synergic deep learning (SDL)-based smart health diagnosis of COVID-19 using Chest X-Ray Images. The SDL makes use of dual deep convolutional neural networks (DCNNs) and involves a mutual learning process from one another. Particularly, the representation of images learned by both DCNNs is provided as the input of a synergic network, which has a fully connected structure and predicts whether the pair of input images come under the identical class. Besides, the proposed SDL model involves a fuzzy bilateral filtering (FBF) model to pre-process the input image. The integration of FBL and SDL resulted in the effective classification of COVID-19. To investigate the classifier outcome of the SDL model, a detailed set of simulations takes place and ensures the effective performance of the FBF-SDL model over the compared methods.  © 2021 Association for Computing Machinery.",classification; COVID-19; deep learning; deep neural network; pre-processing,Classification (of information); Convolutional neural networks; Deep neural networks; Diagnosis; Disease control; Image classification; Learning systems; Smart city; Viruses; Bilateral filtering; Control solutions; Control strategies; Convolutional neural network; Deep learning; Health diagnosis; Heterogeneous control; Input image; Learning models; Pre-processing; COVID-19
Edge Computing to Solve Security Issues for Infectious Disease Intelligence Prevention,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137623378&doi=10.1145%2f3475869&partnerID=40&md5=b3177a76b98b91c570110381599c66e0,"Nowadays, with the rapid development of intelligent technology, it is urgent to effectively prevent infectious diseases and ensure people's privacy. The present work constructs the intelligent prevention system of infectious diseases based on edge computing by using the edge computing algorithm, and further deploys and optimizes the privacy information security defense strategy of users in the system, controls the cost, constructs the optimal conditions of the system security defense, and finally analyzes the performance of the model. The results show that the system delay decreases with the increase of power in the downlink. In the analysis of the security performance of personal privacy information, it is found that six different nodes can maintain the optimal strategy when the cost is minimized in the finite time domain and infinite time domain. In comparison with other classical algorithms in the communication field, when the intelligent prevention system of infectious diseases constructed adopts the best defense strategy, it can effectively reduce the consumption of computing resources of edge network equipment, and the prediction accuracy is obviously better than that of other algorithms, reaching 83%. Hence, the results demonstrate that the model constructed can ensure the safety performance and forecast accuracy, and achieve the best defense strategy at low cost, which provides experimental reference for the prevention and detection of infectious diseases in the later period.  © 2021 Association for Computing Machinery.",data information security; Edge computing; infectious disease prediction; intelligent prediction system; time delay,Cost benefit analysis; Data privacy; Diseases; Edge computing; Network security; Time delay; Time domain analysis; Timing circuits; Data information security; Data informations; Defense strategy; Edge computing; Infectious disease; Infectious disease prediction; Intelligent prediction; Intelligent prediction system; Prediction systems; Time-delays; Forecasting
SANTM: Efficient Self-attention-driven Network for Text Matching,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137624463&doi=10.1145%2f3426971&partnerID=40&md5=efee10dfbc9a20fcef0e9c0543590b0a,"Self-attention mechanisms have recently been embraced for a broad range of text-matching applications. Self-attention model takes only one sentence as an input with no extra information, i.e., one can utilize the final hidden state or pooling. However, text-matching problems can be interpreted either in symmetrical or asymmetrical scopes. For instance, paraphrase detection is an asymmetrical task, while textual entailment classification and question-answer matching are considered asymmetrical tasks. In this article, we leverage attractive properties of self-attention mechanism and proposes an attention-based network that incorporates three key components for inter-sequence attention: global pointwise features, preceding attentive features, and contextual features while updating the rest of the components. Our model follows evaluation on two benchmark datasets cover tasks of textual entailment and question-answer matching. The proposed efficient Self-attention-driven Network for Text Matching outperforms the state of the art on the Stanford Natural Language Inference and WikiQA datasets with much fewer parameters.  © 2021 Association for Computing Machinery.",attention mechanism; deep learning; Text matching,Text processing; Attention mechanisms; Attention model; Deep learning; Hidden state; Matching problems; Matchings; Point wise; Property; Text-matching; Textual entailment; Deep learning
Detecting Malicious Switches for a Secure Software-defined Tactile Internet,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116274756&doi=10.1145%2f3415146&partnerID=40&md5=25c0cb73d4814e0ba6db75f8c4c0ed69,"The rapid development of the Internet of Things has led to demand for high-speed data transformation. Serving this purpose is the Tactile Internet, which facilitates data transfer in extra-low latency. In particular, a Tactile Internet based on software-defined networking (SDN) has been broadly deployed because of the proven benefits of SDN in flexible and programmable network management. However, the vulnerabilities of SDN also threaten the security of the Tactile Internet. Specifically, an SDN controller relies on the network status (provided by the underlying switches) to make network decisions, e.g., calculating a routing path to deliver data in the Tactile Internet. Hence, the attackers can compromise the switches to jeopardize the SDN and further attack Tactile Internet systems. For example, an attacker can compromise switches to launch distributed denial-of-service attacks to overwhelm the SDN controller, which will disrupt all the applications in the Tactile Internet. In pursuit of a more secure Tactile Internet, the problem of abnormal SDN switches in the Tactile Internet is analyzed in this article, including the cause of abnormal switches and their influences on different network layers. Then we propose an approach that leverages the messages sent by all switches to identify abnormal switches, which adopts a linear structure to store historical messages at a relatively low cost. By mapping each flow message to the flow establishment model, our method can effectively identify malicious SDN switches in the Tactile Internet and thus enhance its security. © 2021 Association for Computing Machinery.",malicious switch detection; network security; Software-defined network,Data transfer; Denial-of-service attack; Metadata; Network layers; Software defined networking; Datum transformation; Flexible networks; High-speed data; Internet based; Low latency; Malicious switch detection; Networks security; Secure software; Software-defined networkings; Software-defined networks; Network security
Short-Term Load Forecasting by Using Improved GEP and Abnormal Load Recognition,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115380370&doi=10.1145%2f3447513&partnerID=40&md5=0cf6765d4048696eb0bdc2d6aa4e5e7d,"Load forecasting in short term is very important to economic dispatch and safety assessment of power system. Although existing load forecasting in short-Term algorithms have reached required forecast accuracy, most of the forecasting models are black boxes and cannot be constructed to display mathematical models. At the same time, because of the abnormal load caused by the failure of the load data collection device, time synchronization, and malicious tampering, the accuracy of the existing load forecasting models is greatly reduced. To address these problems, this article proposes a Short-Term Load Forecasting algorithm by using Improved Gene Expression Programming and Abnormal Load Recognition (STLF-IGEP_ALR). First, the Recognition algorithm of Abnormal Load based on Probability Distribution and Cross Validation is proposed. By analyzing the probability distribution of rows and columns in load data, and using the probability distribution of rows and columns for cross-validation, misjudgment of normal load in abnormal load data can be better solved. Second, by designing strategies for adaptive generation of population parameters, individual evolution of populations and dynamic adjustment of genetic operation probability, an Improved Gene Expression Programming based on Evolutionary Parameter Optimization is proposed. Finally, the experimental results on two real load datasets and one open load dataset show that compared with the existing abnormal data detection algorithms, the algorithm proposed in this article have higher advantages in missing detection rate, false detection rate and precision rate, and STLF-IGEP_ALR is superior to other short-Term load forecasting algorithms in terms of the convergence speed, MAE, MAPE, RSME, and R2. © 2021 Association for Computing Machinery.",abnormal load recognition; adaptive evolution; Gene expression programming; power load forecasting; probability distribution,Electric load dispatching; Electric power plant loads; Forecasting; Gene expression; Genetic algorithms; Scheduling; Abnormal load recognition; Abnormal loads; Adaptive evolution; Forecasting algorithm; Gene-expression programming; Load data; Load recognition; Power load forecasting; Probability: distributions; Short term load forecasting; Probability distributions
Introduction to the Special Section on Cognitive Robotics on 5G/6G Networks,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116245339&doi=10.1145%2f3476466&partnerID=40&md5=c2690777e8132e0897c44a89238a4460,[No abstract available],,
Sentence Semantic Matching Based on 3D CNN for Human Robot Language Interaction,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116232836&doi=10.1145%2f3450520&partnerID=40&md5=d4e826121602b23a7de53956e635cb3f,"The development of cognitive robotics brings an attractive scenario where humans and robots cooperate to accomplish specific tasks. To facilitate this scenario, cognitive robots are expected to have the ability to interact with humans with natural language, which depends on natural language understanding (NLU) technologies. As one core task in NLU, sentence semantic matching (SSM) has widely existed in various interaction scenarios. Recently, deep learning-based methods for SSM have become predominant due to their outstanding performance. However, each sentence consists of a sequence of words, and it is usually viewed as one-dimensional (1D) text, leading to the existing available neural models being restricted into 1D sequential networks. A few researches attempt to explore the potential of 2D or 3D neural models in text representation. However, it is hard for their works to capture the complex features in texts, and thus the achieved performance improvement is quite limited. To tackle this challenge, we devise a novel 3D CNN-based SSM (3DSSM) method for human-robot language interaction. Specifically, first, a specific architecture called feature cube network is designed to transform a 1D sentence into a multi-dimensional representation named as semantic feature cube. Then, a 3D CNN module is employed to learn a semantic representation for the semantic feature cube by capturing both the local features embedded in word representations and the sequential information among successive words in a sentence. Given a pair of sentences, their representations are concatenated together to feed into another 3D CNN to capture the interactive features between them to generate the final matching representation. Finally, the semantic matching degree is judged with the sigmoid function by taking the learned matching representation as the input. Extensive experiments on two real-world datasets demonstrate that 3DSSM is able to achieve comparable or even better performance over the state-of-The-Art competing methods. © 2021 Association for Computing Machinery.",3D CNN; human-robot interaction; representation learning; semantic feature cube; Sentence semantic matching,Deep learning; Human robot interaction; Man machine systems; Semantics; 3d CNN; Human robots; Humans-robot interactions; Performance; Representation learning; Robot language; Semantic feature cube; Semantic features; Semantic matching; Sentence semantic matching; Geometry
The Efficient Mining of Skyline Patterns from a Volunteer Computing Network,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116280112&doi=10.1145%2f3423557&partnerID=40&md5=7de552f257fcbe64f6351f10147d4251,"In the ever-growing world, the concepts of High-utility Itemset Mining (HUIM) as well as Frequent Itemset Mining (FIM) are fundamental works in knowledge discovery. Several algorithms have been designed successfully. However, these algorithms only used one factor to estimate an itemset. In the past, skyline pattern mining by considering both aspects of frequency and utility has been extensively discussed. In most cases, however, people tend to focus on purchase quantities of itemsets rather than frequencies. In this article, we propose a new knowledge called skyline quantity-utility pattern (SQUP) to provide better estimations in the decision-making process by considering quantity and utility together. Two algorithms, respectively, called SQU-Miner and SKYQUP are presented to efficiently mine the set of SQUPs. Moreover, the usage of volunteer computing is proposed to show the potential in real supermarket applications. Two new efficient utility-max structures are also mentioned for the reduction of the candidate itemsets, respectively, utilized in SQU-Miner and SKYQUP. These two new utility-max structures are used to store the upper-bound of utility for itemsets under the quantity constraint instead of frequency constraint, and the second proposed utility-max structure moreover applies a recursive updated process to further obtain strict upper-bound of utility. Our in-depth experimental results prove that SKYQUP has stronger performance when a comparison is made to SQU-Miner in terms of memory usage, runtime, and the number of candidates. © 2021 Association for Computing Machinery.",data mining; Skyline quantity-utility patterns (SQUPs); utility-max; utility-quantity list,Behavioral research; Decision making; Distributed computer systems; Miners; Frequent itemset mining; High utility itemset minings; Itemset; One-factor; Pattern mining; Skyline quantity-utility pattern; Upper Bound; Utility-max; Utility-quantity list; Volunteer computing networks; Data mining
Game-Theoretic Strategic Coordination and Navigation of Multiple Wheeled Robots,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116299902&doi=10.1145%2f3450521&partnerID=40&md5=90d6555a803e83d35be7988a22818cdb,"Multiple robots negotiating in a dynamic workspace may lead to collisions. To avoid such issues, multi-robot navigation and coordination becomes necessary but is computationally very challenging, particularly when there are many robots. This article addresses the problem of multi-robot navigation where individual robots require coordination. Although a few such attempts for modeling multi-robot coordination and navigation have been studied, this work proposes a game-Theoretic coordination strategy, also referred to as strategic coordination. We make use of a genetic algorithm tuned fuzzy logic-based motion planner. The proposed strategic coordination strategy has been pitted against a basic potential field-based motion planner, also referred to as the heuristic method, for performance comparison. Results are compared through computer simulation with 8 to 17 robots at different rounds. From the obtained results, it was observed that the proposed coordination scheme's efficacy is strong for a larger number of robots. In addition, the proposed strategic coordination scheme with the genetic-fuzzy-based motion planner was found to outperform other combinations as far as the quality of solutions and time to reach the goal positions. The computational complexity of different methods has also been compared and presented. © 2021 Association for Computing Machinery.",coordination; fuzzy logic controller; game theory; genetic algorithm; Multi-Agent system; potential field method,Computer circuits; Fuzzy logic; Game theory; Genetic algorithms; Heuristic methods; Industrial robots; Multipurpose robots; Navigation; Coordination; Coordination scheme; Coordination strategy; Fuzzy logic controllers; Game-theoretic; Motion planners; Multi-robot coordination; Multi-robot navigation; Potential field methods; Strategic coordination; Multi agent systems
Chinese Emotional Dialogue Response Generation via Reinforcement Learning,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116244433&doi=10.1145%2f3446390&partnerID=40&md5=ec7882acf4719928427579281b4fa49d,"In an open-domain dialogue system, recognition and expression of emotions are the key factors for success. Most of the existing research related to Chinese dialogue systems aims at improving the quality of content but ignores the expression of human emotions. In this article, we propose a Chinese emotional dialogue response generation algorithm based on reinforcement learning that can generate responses not only according to content but also according to emotion. In the proposed method, a multi-emotion classification model is first used to add emotion labels to the corpus of post-response pairs. Then, with the help of reinforcement learning, the reward function is constructed based on two aspects, namely, emotion and content. Among the generated candidates, the system selects the one with long-Term success as the best reply. At the same time, to avoid safe responses and diversify dialogue, a diversity beam search algorithm is applied in the decoding process. The comparative experiments demonstrate that the proposed model achieves satisfactory results according to both automatic and human evaluations. © 2021 Association for Computing Machinery.",Dialogue generation; emotion classification; reinforcement learning; reward function; safe response,Classification (of information); Learning algorithms; Reinforcement learning; Dialogue generations; Dialogue systems; Emotion classification; Human emotion; Key factors; Quality of contents; Reinforcement learnings; Response generation; Reward function; Safe response; Speech processing
A Multi-graph Convolutional Network Framework for Tourist Flow Prediction,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116259368&doi=10.1145%2f3424220&partnerID=40&md5=26c536921c6dbf98742a33cb567f78c2,"With the advancement of Cyber Physic Systems and Social Internet of Things, the tourism industry is facing challenges and opportunities. We can now able to collect, store, and analyze large amounts of travel data. With the help of data science and artificial intelligence, smart tourism enables tourists with great autonomy and convenience for an intelligent trip. It is of great significance to make full use of these massive data to provide better services for smart tourism. However, due to the skewed and imbalanced visiting for point of interest located at different places, it is of great significance to predict the tourist flow of each place, which can help the service providers for designing a better schedule visiting strategy in advance. Against this background, this article proposes a multi-graph convolutional network framework, named AMOUNT, for tourist flow prediction. To capture the diverse relationships among POIs, AMOUNT first constructs three subgraphs, including the geographical graph, interaction graph, and the co-relation graph. Then, a multi-graph convolution network is utilized to predict the future tourist flow. Experimental results on two real-world datasets indicate that the proposed AMOUNT model outperforms all other baseline tourist flow prediction approaches. © 2021 Association for Computing Machinery.",cyber physical systems; deep learning; smart tourism; Social internet of things; tourist flow prediction,Convolution; Cyber Physical System; Deep learning; Embedded systems; Flow graphs; Forecasting; Tourism; Convolutional networks; Cybe-physical systems; Cyber-physical systems; Deep learning; Flow prediction; Network frameworks; Smart tourism; Social internet of thing; Tourism industry; Tourist flow prediction; Internet of things
AI-empowered IoT Security for Smart Cities,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115087337&doi=10.1145%2f3406115&partnerID=40&md5=cfaa60ece2d71da59d5ab6aa66c989a4,"Smart cities fully utilize the new generation of Internet of Things (IoT) technology in the process of urban informatization to optimize the urban management and service. However, in the IoT system, while information exchange and communication, wireless sensor network devices may not be able to resist all forms of attacks, which may lead to security issues such as user data disclosure. Aiming at the information security risks in smart city, the typical technologies in IoT is analyzed from the perspective of IoT perception layer and provides corresponding security solutions for the existing security threats. Regarding the communication security, the emerging wireless technology, long range (LoRa), is discussed, and the performance of wireless communication protocol is analyzed through simulation experiments, to verify that the IoT technology based on LoRa communication technology can improve the security of the system in the construction of smart city. The results show that REBEB, a new backoff algorithm, is similar to the binary exponential backoff algorithm in terms of throughput performance. REBEB focuses more on fairness, which is up to 0.985, and to a certain extent, its security is significantly improved. The fairness of REBEB algorithm is more than 0.4 in different nodes and competing windows, and the fairness of the system is better when the number of nodes is small. To sum up, the IoT system based on LoRa communication can effectively improve the security performance of the system in the construction of smart city and avoid the security threats in the IoT signal transmission. © 2021 Association for Computing Machinery.",artificial intelligence; internet of things security; LoRa communication; REBEB; Smart city,Network security; Risk perception; Security systems; Sensor nodes; Smart city; Information communication; Information exchanges; Informatization; Internet of thing security; Internet of things technologies; Long-range communications; REBEB; Security threats; Urban management; Urban services; Internet of things
Joint QoS-Aware and Cost-efficient Task Scheduling for Fog-cloud Resources in a Volunteer Computing System,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116246286&doi=10.1145%2f3418501&partnerID=40&md5=001586e06cabd587ee6508eb7abdfa3e,"Volunteer computing is an Internet-based distributed computing in which volunteers share their extra available resources to manage large-scale tasks. However, computing devices in a Volunteer Computing System (VCS) are highly dynamic and heterogeneous in terms of their processing power, monetary cost, and data transferring latency. To ensure both of the high Quality of Service (QoS) and low cost for different requests, all of the available computing resources must be used efficiently. Task scheduling is an NP-hard problem that is considered as one of the main critical challenges in a heterogeneous VCS. Due to this, in this article, we design two task scheduling algorithms for VCSs, named Min-CCV and Min-V. The main goal of the proposed algorithms is jointly minimizing the computation, communication, and delay violation cost for the Internet of Things (IoT) requests. Our extensive simulation results show that proposed algorithms are able to allocate tasks to volunteer fog/cloud resources more efficiently than the state-of-The-Art. Specifically, our algorithms improve the deadline satisfaction task rates around 99.5% and decrease the total cost between 15 to 53% in comparison with the genetic-based algorithm. © 2021 Association for Computing Machinery.",cloud computing; cost-efficient; fog computing; quality of service (QoS); task scheduling; Volunteer computing,Computational complexity; Fog; Fog computing; Genetic algorithms; Internet of things; Multitasking; Scheduling algorithms; Cloud-computing; Computing system; Cost-efficient; Joint quality; Quality of service; Quality-of-service; Service costs; Service-aware; Tasks scheduling; Volunteer computing; Quality of service
Fast Search of Lightweight Block Cipher Primitives via Swarm-like Metaheuristics for Cyber Security,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116286304&doi=10.1145%2f3417296&partnerID=40&md5=537fb2cb1a563e5fe2bbb6cb7fc93d5a,"With the construction and improvement of 5G infrastructure, more devices choose to access the Internet to achieve some functions. People are paying more attention to information security in the use of network devices. This makes lightweight block ciphers become a hotspot. A lightweight block cipher with superior performance can ensure the security of information while reducing the consumption of device resources. Traditional optimization tools, such as brute force or random search, are often used to solve the design of Symmetric-Key primitives. The metaheuristic algorithm was first used to solve the design of Symmetric-Key primitives of SKINNY. The genetic algorithm and the simulated annealing algorithm are used to increase the number of active S-boxes in SKINNY, thus improving the security of SKINNY. Based on this, to improve search efficiency and optimize search results, we design a novel metaheuristic algorithm, named particle swarm-like normal optimization algorithm (PSNO) to design the Symmetric-Key primitives of SKINNY. With our algorithm, one or better algorithm components can be obtained more quickly. The results in the experiments show that our search results are better than those of the genetic algorithm and the simulated annealing algorithm. The search efficiency is significantly improved. The algorithm we proposed can be generalized to the design of Symmetric-Key primitives of other lightweight block ciphers with clear evaluation indicators, where the corresponding indicators can be used as the objective functions. © 2021 Association for Computing Machinery.",Lightweight block cipher; metaheuristic algorigthm; particle swarm optimization algorithm; SKINNY,5G mobile communication systems; Cryptography; Efficiency; Genetic algorithms; Particle swarm optimization (PSO); Simulated annealing; Annealing algorithm; Fast search; Lightweight block ciphers; Meta-heuristics algorithms; Metaheuristic; Metaheuristic algorigthm; Particle swarm optimization algorithm; Search efficiency; SKINNY; Symmetric keys; Security of data
AI-enabled IoT-Edge Data Analytics for Connected Living,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114014874&doi=10.1145%2f3421510&partnerID=40&md5=cdc349c291497819773f1d68c44db67d,"As deep learning, virtual reality, and other technologies become mature, real-Time data processing applications running on intelligent terminals are emerging endlessly; meanwhile, edge computing has developed rapidly and has become a popular research direction in the field of distributed computing. Edge computing network is a network computing environment composed of multi-edge computing nodes and data centers. First, the edge computing framework and key technologies are analyzed to improve the performance of real-Time data processing applications. In the system scenario where the collaborative deployment tasks of multi-edge nodes and data centers are considered, the stream processing task deployment process is formally described, and an efficient multi-edge node-computing center collaborative task deployment algorithm is proposed, which solves the problem of copy-free task deployment in the task deployment problem. Furthermore, a heterogeneous edge collaborative storage mechanism with tight coupling of computing and data is proposed, which solves the contradiction between the limited computing and storage capabilities of data and intelligent terminals, thereby improving the performance of data processing applications. Here, a Feasible Solution (FS) algorithm is designed to solve the problem of placing copy-free data processing tasks in the system. The FS algorithm has excellent results once considering the overall coordination. Under light load, the V value is reduced by 73% compared to the Only Data Center-Available (ODC) algorithm and 41% compared to the Hash algorithm. Under heavy load, the V value is reduced by 66% compared to the ODC algorithm and 35% compared to the Hash algorithm. The algorithm has achieved good results after considering the overall coordination and cooperation and can more effectively use the bandwidth of edge nodes to transmit and process data stream, so that more tasks can be deployed in edge computing nodes, thereby saving time for data transmission to the data centers. The end-To-end collaborative real-Time data processing task scheduling mechanism proposed here can effectively avoid the disadvantages of long waiting times and unable to obtain the required data, which significantly improves the success rate of the task and thus ensures the performance of real-Time data processing. © 2021 Association for Computing Machinery.",edge data; end-To-end collaboration; Internet of Things; smart device,Deep learning; Digital storage; Edge computing; Hash functions; Virtual reality; Data processing applications; Datacenter; Edge computing; Edge data; Edge nodes; End to end; End-to-end collaboration; Performance; Real-time data processing; Smart devices; Internet of things
Cognitive Wearable Robotics for Autism Perception Enhancement,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116293517&doi=10.1145%2f3450630&partnerID=40&md5=c359821112458e22bf687eb1d41415f8,"Autism spectrum disorder (ASD) is a serious hazard to the physical and mental health of children, which limits the social activities of patients throughout their lives and places a heavy burden on families and society. The developments of communication techniques and artificial intelligence (AI) have provided new potential methods for the treatment of autism. The existing treatment systems based on AI for children with ASD focus on detecting health status and developing social skills. However, the contradiction between the terminal interaction capability and availability cannot meet the needs for real application scenarios. At the same time, the lack of diverse data cannot provide individualized care for autistic children. To explore this robot-based approach, a novel AI-based first-view-robot architecture is proposed in this article. By providing care from the first-person perspective, the proposed wearable robot overcomes the difficulty of the absence of cognitive ability in the third-view of traditional robotics and improves the social interaction ability of children with ASD. The first-view-robot architecture meets the requirements of dynamic, individualized, and highly immersed interaction services for autistic children. First, the multi-modal and multi-scene data collection processes of standard, static, and dynamic datasets are introduced in detail. Then, to comprehensively evaluate the learning ability of children with ASD through mental states and external performances, a learning assessment model with emotion correction is proposed. Besides, a wearable robot-Assisted environment perception and expression enhancement mechanism for children with ASD is realized by reinforcement learning, which can be adapted to interactive environments with optimal action policies. An interactive testbed for children with ASD treatments is demonstrated and experimental cases for test subjects are presented. Last, three open issues are discussed from data processing, robot designing, and service responding perspectives. © 2021 Association for Computing Machinery.",Autism therapy; emotion perception; multi-modal and multi-scene; reinforcement learning,Data handling; Diseases; Memory architecture; Reinforcement learning; Robotics; Robots; Wearable technology; Autism spectrum disorders; Autism therapies; Autistic children; Children with autisms; Emotion perception; Multi-modal; Multi-modal and multi-scene; Robot architecture; Wearable robotics; Wearable robots; Behavioral research
Merging Grid Maps in Diverse Resolutions by the Context-based Descriptor,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116216543&doi=10.1145%2f3403948&partnerID=40&md5=5e387902befab88db4ac81667914070b,"Building an accurate map is essential for autonomous robot navigation in the environment without GPS. Compared with single-robot, the multiple-robot system has much better performance in terms of accuracy, efficiency and robustness for the simultaneous localization and mapping (SLAM). As a critical component of multiple-robot SLAM, the problem of map merging still remains a challenge. To this end, this article casts it into point set registration problem and proposes an effective map merging method based on the context-based descriptors and correspondence expansion. It first extracts interest points from grid maps by the Harris corner detector. By exploiting neighborhood information of interest points, it automatically calculates the maximum response radius as scale information to compute the context-based descriptor, which includes eigenvalues and normals computed from local structures of each interest point. Then, it effectively establishes origin matches with low precision by applying the nearest neighbor search on the context-based descriptor. Further, it designs a scale-based corresponding expansion strategy to expand each origin match into a set of feature matches, where one similarity transformation between two grid maps can be estimated by the Random Sample Consensus algorithm. Subsequently, a measure function formulated from the trimmed mean square error is utilized to confirm the best similarity transformation and accomplish the coarse map merging. Finally, it utilizes the scaling trimmed iterative closest point algorithm to refine initial similarity transformation so as to achieve accurate merging. As the proposed method considers scale information in the context-based descriptor, it is able to merge grid maps in diverse resolutions. Experimental results on real robot datasets demonstrate its superior performance over other related methods on accuracy and robustness. © 2021 Association for Computing Machinery.",correspondence propagation; grid map merging; Multi-robot systems; similarity transformation,Edge detection; Eigenvalues and eigenfunctions; Industrial robots; Iterative methods; Mean square error; Merging; Nearest neighbor search; Robotics; Autonomous robot navigation; Context-based; Correspondence propagation; Descriptors; Grid map; Grid map merging; Map merging; Performance; Similarity transformation; Simultaneous localization and mapping; Multipurpose robots
Cognitive Robotics on 5G Networks,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116200473&doi=10.1145%2f3414842&partnerID=40&md5=cbda64802dd268989ff6157676454106,"Emotional cognitive ability is a key technical indicator to measure the friendliness of interaction. Therefore, this research aims to explore robots with human emotion cognitively. By discussing the prospects of 5G technology and cognitive robots, the main direction of the study is cognitive robots. For the emotional cognitive robots, the analysis logic similar to humans is difficult to imitate; the information processing levels of robots are divided into three levels in this study: cognitive algorithm, feature extraction, and information collection by comparing human information processing levels. In addition, a multi-scale rectangular direction gradient histogram is used for facial expression recognition, and robust principal component analysis algorithm is used for facial expression recognition. In the pictures where humans intuitively feel smiles in sad emotions, the proportion of emotions obtained by the method in this study are as follows: calmness accounted for 0%, sadness accounted for 15.78%, fear accounted for 0%, happiness accounted for 76.53%, disgust accounted for 7.69%, anger accounted for 0%, and astonishment accounted for 0%. In the recognition of micro-expressions, humans intuitively feel negative emotions such as surprise and fear, and the proportion of emotions obtained by the method adopted in this study are as follows: calmness accounted for 32.34%, sadness accounted for 34.07%, fear accounted for 6.79%, happiness accounted for 0%, disgust accounted for 0%, anger accounted for 13.91%, and astonishment accounted for 15.89%. Therefore, the algorithm explored in this study can realize accuracy in cognition of emotions. From the preceding research results, it can be seen that the research method in this study can intuitively reflect the proportion of human expressions, and the recognition methods based on facial expressions and micro-expressions have good recognition effects, which is in line with human intuitive experience. © 2021 Association for Computing Machinery.",5G; Cognitive robots; facial expression recognition; human emotion recognition; micro-facial expression recognition; multi-scale rectangular direction gradient histogram (R-HOG),Face recognition; Graphic methods; Man machine systems; Principal component analysis; Robotics; Cognitive robotics; Cognitive robots; Facial expression recognition; Gradient histograms; Human emotion recognition; Micro-expressions; Micro-facial expression recognition; Multi-scale rectangular direction gradient histogram (R-HOG); Multi-scales; 5G mobile communication systems
A Comparative Study of AI-Based Intrusion Detection Techniques in Critical Infrastructures,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116257085&doi=10.1145%2f3406093&partnerID=40&md5=fe471bf11c23d57e496c3b3e57436316,"Volunteer computing uses Internet-connected devices (laptops, PCs, smart devices, etc.), in which their owners volunteer them as storage and computing power resources, has become an essential mechanism for resource management in numerous applications. The growth of the volume and variety of data traffic on the Internet leads to concerns on the robustness of cyberphysical systems especially for critical infrastructures. Therefore, the implementation of an efficient Intrusion Detection System for gathering such sensory data has gained vital importance. In this article, we present a comparative study of Artificial Intelligence (AI)-driven intrusion detection systems for wirelessly connected sensors that track crucial applications. Specifically, we present an in-depth analysis of the use of machine learning, deep learning and reinforcement learning solutions to recognise intrusive behavior in the collected traffic. We evaluate the proposed mechanisms by using KDD'99 as real attack dataset in our simulations. Results present the performance metrics for three different IDSs, namely the Adaptively Supervised and Clustered Hybrid IDS (ASCH-IDS), Restricted Boltzmann Machine-based Clustered IDS (RBC-IDS), and Q-learning based IDS (Q-IDS), to detect malicious behaviors. We also present the performance of different reinforcement learning techniques such as State-Action-Reward-State-Action Learning (SARSA) and the Temporal Difference learning (TD). Through simulations, we show that Q-IDS performs with detection rate while SARSA-IDS and TD-IDS perform at the order of . © 2021 Association for Computing Machinery.",deep learning; Intrusion detection; machine learning; reinforcement learning; restricted Boltzmann machine; wireless sensor networks,Computer crime; Critical infrastructures; Deep learning; Digital storage; Embedded systems; Network security; Public works; Reinforcement learning; Wireless sensor networks; Action learning; Comparatives studies; Deep learning; Intrusion Detection Systems; Intrusion-Detection; Q-learning; Restricted boltzmann machine; Smart devices; Temporal difference learning; Volunteer computing; Intrusion detection
Intelligent Traffic Signal Control Based on Reinforcement Learning with State Reduction for Smart Cities,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116299324&doi=10.1145%2f3418682&partnerID=40&md5=b35dd9d490b7377a7b5e5c3bb22aed5d,"Efficient signal control at isolated intersections is vital for relieving congestion, accidents, and environmental pollution caused by increasing numbers of vehicles. However, most of the existing studies not only ignore the constraint of the limited computing resources available at isolated intersections but also the matching degree between the signal timing and the traffic demand, leading to high complexity and reduced learning efficiency. In this article, we propose a traffic signal control method based on reinforcement learning with state reduction. First, a reinforcement learning model is established based on historical traffic flow data, and we propose a dual-objective reward function that can reduce vehicle delay and improve the matching degree between signal time allocation and traffic demand, allowing the agent to learn the optimal signal timing strategy quickly. Second, the state and action spaces of the model are preliminarily reduced by selecting a proper control phase combination; then, the state space is further reduced by eliminating rare or nonexistent states based on the historical traffic flow. Finally, a simplified Q-Table is generated and used to optimize the complexity of the control algorithm. The results of simulation experiments show that our proposed control algorithm effectively improves the capacity of isolated intersections while reducing the time and space costs of the signal control algorithm. © 2021 Association for Computing Machinery.",Q-learning; Traffic signal control,Accidents; Pollution control; Smart city; Street traffic control; Traffic congestion; Traffic signals; Intelligent traffics; Isolated intersection; Matching degree; Q-learning; Signal timing; State reduction; State-space; Traffic demands; Traffic flow; Traffic signal control; Reinforcement learning
A Secure Ticket-Based Authentication Mechanism for Proxy Mobile IPv6 Networks in Volunteer Computing,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116288020&doi=10.1145%2f3407189&partnerID=40&md5=57a2bd490938c2dc6916fb2e4890c7fa,"Technology advances-such as improving processing power, battery life, and communication functionalities-contribute to making mobile devices an attractive research area. In 2008, in order to manage mobility, the Internet Engineering Task Force (IETF) developed Proxy Mobile IPv6, which is a network-based mobility management protocol to support seamless connectivity of mobile devices. This protocol can play a key role in volunteer computing paradigms as a user can seamlessly access computing resources. The procedure of user authentication is not defined in this standard; thus, many studies have been carried out to propose suitable authentication schemes. However, in the current authentication methods, with reduced latency and packet loss, some security and privacy considerations are neglected. In this study, we propose a secure and anonymous ticket-based authentication (SATA) method to protect mobile nodes against existing security and privacy issues. The proposed method reduces the overhead of handover authentication procedures using the ticket-based concept. We evaluated security and privacy strengths of the proposed method using security theorems and BAN logic. © 2021 Association for Computing Machinery.",network protocol; network security; proxy mobile IPv6; security and privacy; User authentication; volunteer computing,Authentication; Computation theory; Cryptography; Internet protocols; Mobile security; Mobile telecommunication systems; Authentication mechanisms; Authentication methods; Mobile IPV6 networks; Networks security; Processing power; Proxy Mobile IPv6; Security and privacy; Technology advances; User authentication; Volunteer computing; Network security
A Novel Real-Time Anti-spam Framework,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116224217&doi=10.1145%2f3423153&partnerID=40&md5=755c582d6e395558f6076c1eb1c20204,"As one of the most pervasive current modes of communication, email needs to be fast and reliable. However, spammers and attackers use it as a primary channel to conduct illegal activities. Although many approaches have been developed and evaluated for spam detection, they do not provide sufficient accuracy. This deficiency results in significant economic losses for organizations. In this article, we first propose a framework for creating novel spam filters using Keras to combine a Convolutional Neural Network (CNN) with Long Short-Term Memory (LSTM) classification models. We then use this framework to introduce a specific solution applicable to realistic scenarios involving dynamic incoming email data in real-Time. This solution takes the form of a real-Time content-based spam classifier. We evaluate its performance concerning accuracy, precision, recall, false-positive, and false-negative rates. Our experimental results show that our approach can significantly outperform existing solutions for real-Time spam detection. © 2021 Association for Computing Machinery.",CNN; content based filters; cybercrime; cybersecurity; deep learning; LSTM; machine learning; natural language processing; neural networks; RNN; Spam filtering; spam intrusion detection; text representation,Computer crime; Convolutional neural networks; Electronic mail; Intrusion detection; Learning algorithms; Long short-term memory; Losses; Natural language processing systems; Content-based filters; Convolutional neural network; Cyber security; Cyber-crimes; Deep learning; Intrusion-Detection; Neural-networks; RNN; Spam filtering; Spam intrusion detection; Text representation; Cybersecurity
An Incentive-based Mechanism for Volunteer Computing Using Blockchain,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116283380&doi=10.1145%2f3419104&partnerID=40&md5=263ea131153e0c06d65039b4016f1fca,"The rise of fast communication media both at the core and at the edge has resulted in unprecedented numbers of sophisticated and intelligent wireless IoT devices. Tactile Internet has enabled the interaction between humans and machines within their environment to achieve revolutionized solutions both on the move and in real-Time. Many applications such as intelligent autonomous self-driving, smart agriculture and industrial solutions, and self-learning multimedia content filtering and sharing have become attainable through cooperative, distributed, and decentralized systems, namely, volunteer computing. This article introduces a blockchain-enabled resource sharing and service composition solution through volunteer computing. Device resource, computing, and intelligence capabilities are advertised in the environment to be made discoverable and available for sharing with the aid of blockchain technology. Incentives in the form of on-demand service availability are given to resource and service providers to ensure fair and balanced cooperative resource usage. Blockchains are formed whenever a service request is initiated with the aid of fog and mobile edge computing (MEC) devices to ensure secure communication and service delivery for the participants. Using both volunteer computing techniques and tactile internet architectures, we devise a fast and reliable service provisioning framework that relies on a reinforcement learning technique. Simulation results show that the proposed solution can achieve high reward distribution, increased number of blockchain formations, reduced delays, and balanced resource usage among participants, under the premise of high IoT device availability. © 2021 Association for Computing Machinery.",5G; 6G; AI; Blockchain; Internet of Things; volunteer computing,5G mobile communication systems; Blockchain; Multimedia systems; Quality of service; Reinforcement learning; 6g; Block-chain; Communication media; Incentive based mechanisms; Industrial solutions; Real- time; Resource usage; Self drivings; Smart agricultures; Volunteer computing; Internet of things
A Novel Approach for Efficient Packet Transmission in Volunteered Computing MANET,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116283751&doi=10.1145%2f3418203&partnerID=40&md5=84187ad3498d01bb799e2ef001791f9a,"A mobile ad hoc network (MANET) is summarized as a combination device that can move, synchronize and converse without any preceding management. Enhancing the lifetime energy is based on the status of the concerned channel. The node is accomplished of control the control messages. Due to unplanned methods of energy conservation, the node lifespan and quality of packet flow is defaced in the existing solution. It results in a network-To-node-energy trade-off, ensuing in a failure of the post-network. This failure results in reduced time-To-live and higher overhead. This paper discusses an effective buffer management mechanism, in addition to proposing a novel performance modeling in Volunteered Computing MANET and tactile internet Next, the best execution the nodes can accomplish under fractional data is completely portrayed for utilities for a general purpose. To associate the space between network efficiency and energy conservation based on the minimal overhead, this article proposes a switch state promoting mutual Optimized MAC protocol for conservation of a node's energy and the optimal use of available nodes before their energy drain. Simulation results are provided as proof of the proposed solution. The simulation results are compared with the existing system with performance measures of delay, throughput, energy consumption, and availability of the node. © 2021 Association for Computing Machinery.",energy consumption; MANET; OMAC protocol; volunteer computing,Distributed computer systems; Economic and social effects; Energy conservation; Medium access control; Mobile ad hoc networks; Packet networks; Combination devices; Control messages; Energy; Energy-consumption; Lifespans; Mobile ad-hoc networks; Node energy; OMAC protocol; Packet transmissions; Volunteer computing; Energy utilization
A Flow-based Multi-Agent Data Exfiltration Detection Architecture for Ultra-low Latency Networks,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116270977&doi=10.1145%2f3419103&partnerID=40&md5=6f3370f6390c3f47ee3ced31deb912fb,"Modern network infrastructures host converged applications that demand rapid elasticity of services, increased security, and ultra-fast reaction times. The Tactile Internet promises to facilitate the delivery of these services while enabling new economies of scale for high fidelity of machine-To-machine and human-To-machine interactions. Unavoidably, critical mission systems served by the Tactile Internet manifest high demands not only for high speed and reliable communications but equally, the ability to rapidly identify and mitigate threats and vulnerabilities. This article proposes a novel Multi-Agent Data Exfiltration Detector Architecture (MADEX), inspired by the mechanisms and features present in the human immune system. MADEX seeks to identify data exfiltration activities performed by evasive and stealthy malware that hides malicious traffic from an infected host in low-latency networks. Our approach uses cross-network traffic information collected by agents to effectively identify unknown illicit connections by an operating system subverted. MADEX does not require prior knowledge of the characteristics or behavior of the malicious code or a dedicated access to a knowledge repository. We tested the performance of MADEX in terms of its capacity to handle real-Time data and the sensitivity of our algorithm's classification when exposed to malicious traffic. Experimental evaluation results show that MADEX achieved 99.97% sensitivity, 98.78% accuracy, and an error rate of 1.21% when compared to its best rivals. We created a second version of MADEX, called MADEX level 2, that further improves its overall performance with a slight increase in computational complexity. We argue for the suitability of MADEX level 1 in non-critical environments, while MADEX level 2 can be used to avoid data exfiltration in critical mission systems. To the best of our knowledge, this is the first article in the literature that addresses the detection of rootkits real-Time in an agnostic way using an artificial immune system approach while it satisfies strict latency requirements. © 2021 Association for Computing Machinery.",Artificial immune systems; flow-based analysis; multi-Agent systems; rootkits; Tactile Internet,Economics; Immune system; Multi agent systems; Network architecture; Network security; Agent data; Artificial Immune System; Data exfiltration; Flow-based analysis; Low-latency networks; Malicious traffic; Mission systems; Multi agent; Performance; Tactile internet; Malware
Screw Slot Quality Inspection System Based on Tactile Network,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116207875&doi=10.1145%2f3423556&partnerID=40&md5=9a3108d12b271510f597f3b02fc96ea8,"The popularity of 5G networks has made smart manufacturing not limited to high-Tech industries such as semiconductors due to its high speed, ultra-high reliability, and low latency. With the advance of system on chip (SoC) design and manufacturing, 5G is also suitable for data transmission in harsh manufacturing environments such as high temperatures, dust, and extreme vibration. The defect of the screw head is caused by the wear and deformation of the die forming the head after mass production. Therefore, the screw quality inspection system based on the tactile network in this article monitors the production quality of the screw; the system will send a warning signal through the router to remind the technician to solve the production problem when the machine produces a defective product. Sensors are embedded into the traditional screw heading machine, and sensing data are transmitted through a gateway to the voluntary computing node for screw slot quality inspection. The anomaly detection data set collected by the screw heading machine has a ratio of anomaly to normal data of 0.006; thus, we propose a time-series deep AutoEncoder architecture for anomaly detection of screw slots. Our experimental results show that the proposed solution outperforms existing works in terms of efficiency and that the specificity and accuracy can reach 97% through the framework proposed in this article. © 2021 Association for Computing Machinery.",Anomaly detection; feature extraction; tactile network,5G mobile communication systems; Anomaly detection; Application specific integrated circuits; Defects; Inspection; Inspection equipment; Programmable logic controllers; System-on-chip; Anomaly detection; Features extraction; High reliability; High Speed; High tech industry; High-low; Quality inspection systems; Smart manufacturing; Tactile network; Ultra-high; Feature extraction
Machine Learning-based Mist Computing Enabled Internet of Battlefield Things,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115796585&doi=10.1145%2f3418204&partnerID=40&md5=139ecc7ba9ff9526d4c8e7203372dbe8,"The rapid advancement in information and communication technology has revolutionized military departments and their operations. This advancement also gave birth to the idea of the Internet of Battlefield Things (IoBT). The IoBT refers to the fusion of the Internet of Things (IoT) with military operations on the battlefield. Various IoBT-based frameworks have been developed for the military. Nonetheless, many of these frameworks fail to maintain a high Quality of Service (QoS) due to the demanding and critical nature of IoBT. This study makes the use of mist computing while leveraging machine learning. Mist computing places computational capabilities on the edge itself (mist nodes), e.g., on end devices, wearables, sensors, and micro-controllers. This way, mist computing not only decreases latency but also saves power consumption and bandwidth as well by eliminating the need to communicate all data acquired, produced, or sensed. A mist-based version of the IoTNetWar framework is also proposed in this study. The mist-based IoTNetWar framework is a four-layer structure that aims at decreasing latency while maintaining QoS. Additionally, to further minimize delays, mist nodes utilize machine learning. Specifically, they use the delay-based K nearest neighbour algorithm for device-To-device communication purposes. The primary research objective of this work is to develop a system that is not only energy, time, and bandwidth-efficient, but it also helps military organizations with time-critical and resources-critical scenarios to monitor troops. By doing so, the system improves the overall decision-making process in a military campaign or battle. The proposed work is evaluated with the help of simulations in the EdgeCloudSim. The obtained results indicate that the proposed framework can achieve decreased network latency of 0.01 s and failure rate of 0.25% on average while maintaining high QoS in comparison to existing solutions. © 2021 Association for Computing Machinery.",battlefield monitoring system; device-To-device communication; fog computing; IoBT; K nearest neighbours; Mist computing; real-Time system,Bandwidth; Decision making; Failure analysis; Fog computing; Interactive computer systems; Internet of things; Machine learning; Nearest neighbor search; Battlefield monitoring system; Device-to-Device communications; High quality; Internet of battlefield thing; K near neighbor; Mist computing; Monitoring system; Nearest-neighbour; Quality-of-service; Real - Time system; Real time systems
Investigating the Spread of Coronavirus Disease via Edge-AI and Air Pollution Correlation,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113745257&doi=10.1145%2f3424222&partnerID=40&md5=ece3bb27df963d983e05434cc112e869,"Coronavirus Disease 19 (COVID-19) is a highly infectious viral disease affecting millions of people worldwide in 2020. Several studies have shown that COVID-19 results in a severe acute respiratory syndrome and may lead to death. In past research, a greater number of respiratory diseases has been caused by exposure to air pollution for long periods of time. This article investigates the spread of COVID-19 as a result of air pollution by applying linear regression in machine learning method based edge computing. The analysis in this investigation have been based on the death rates caused by COVID-19 as well as the region of death rates based on hazardous air pollution using data retrieved from the Copernicus Sentinel-5P satellite. The results obtained in the investigation prove that the mortality rate due to the spread of COVID-19 is 77% higher in areas with polluted air. This investigation also proves that COVID-19 severely affected 68% of the individuals who had been exposed to polluted air. © 2021 Association for Computing Machinery.",air pollution; cloud data; Coronavirus Disease 19 (COVID-19); edge artificial intelligence; linear regression; machine learning,Air pollution; Machine learning; Population statistics; Pulmonary diseases; Cloud data; Coronavirus disease 19; Coronaviruses; Death rates; Edge artificial intelligence; Edge computing; Machine learning methods; Pollution correlations; Severe acute respiratory syndrome; Viral disease; Coronavirus
Big Data Processing on Volunteer Computing,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110650703&doi=10.1145%2f3409801&partnerID=40&md5=22552add16fcac69d0379d3cadb263e8,"In order to calculate the node big data contained in complex networks and realize the efficient calculation of complex networks, based on voluntary computing, taking ICE middleware as the communication medium, the loose coupling distributed framework DCBV based on voluntary computing is proposed. Then, the Master, Worker, and MiddleWare layers in the framework, and the development structure of a DCBV framework are designed. The task allocation and recovery strategy, message passing and communication mode, and fault tolerance processing are discussed. Finally, to calculate and verify parameters such as the average shortest path of the framework and shorten calculation time, an improved accurate shortest path algorithm, the N-SPFA algorithm, is proposed. Under different datasets, the node calculation and performance of the N-SPFA algorithm are explored. The algorithm is compared with four approximate shortest-path algorithms: Combined Link and Attribute (CLA), Lexicographic Breadth First Search (LBFS), Approximate algorithm of shortest path length based on center distance of area division (CDZ), and Hub Vertex of area and Core Expressway (HEA-CE). The results show that when the number of CPU threads is 4, the computation time of the DCBV framework is the shortest (514.63 ms). As the number of CPU cores increases, the overall computation time of the framework decreases gradually. For every 2 additional CPU cores, the number of tasks increases by 1. When the number of Worker nodes is 8 and the number of nodes is 1, the computation time of the framework is the shortest (210,979 ms), and the IO statistics data increase with the increase of Worker nodes. When the datasets are Undirected01 and Undirected02, the computation time of the N-SPFA algorithm is the shortest, which is 4520 ms and 7324 ms, respectively. However, the calculation time in the ca-condmatundirected dataset is 175,292 ms, and the performance is slightly worse. Overall, however, the performance of the N-SPFA and SPFA algorithms is good. Therefore, the two algorithms are combined. For networks with less complexity, the computational scale coefficient of the SPFA algorithm can be set to 0.06, and for general networks, 0.2. When compared with other algorithms in different datasets, the pretreatment time, average query time, and overall query time of N-SPFA algorithm are the shortest, being 49.67 ms, 5.12 ms, and 94,720 ms, respectively. The accuracy (1.0087) and error rate (0.024) are also the best. In conclusion, voluntary computing can be applied to the processing of big data, which has a good reference significance for the distributed analysis of large-scale complex networks. © 2021 Association for Computing Machinery.",accurate shortest paths; big data; large-scale complex networks; loosely coupled distributed frameworks; Volunteer computing,Big data; Fault tolerance; Graph theory; Message passing; Middleware; Accurate short path; Computation time; Distributed framework; Large-scale complex network; Large-scales; Loosely coupled; Loosely coupled distributed framework; Short-path; Voluntary computing; Volunteer computing; Complex networks
Pre-Trained Convolutional Neural Networks for Breast Cancer Detection Using Ultrasound Images,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112603216&doi=10.1145%2f3418355&partnerID=40&md5=8a202d3ebd97e8739575276ba7119e7d,"Volunteer computing based data processing is a new trend in healthcare applications. Researchers are now leveraging volunteer computing power to train deep learning networks consisting of billions of parameters. Breast cancer is the second most common cause of death in women among cancers. The early detection of cancer may diminish the death risk of patients. Since the diagnosis of breast cancer manually takes lengthy time and there is a scarcity of detection systems, development of an automatic diagnosis system is needed for early detection of cancer. Machine learning models are now widely used for cancer detection and prediction research for improving the successive therapy of patients. Considering this need, this study implements pre-Trained convolutional neural network based models for detecting breast cancer using ultrasound images. In particular, we tuned the pre-Trained models for extracting key features from ultrasound images and included a classifier on the top layer. We measured accuracy of seven popular state-of-The-Art pre-Trained models using different optimizers and hyper-parameters through fivefold cross validation. Moreover, we consider Grad-CAM and occlusion mapping techniques to examine how well the models extract key features from the ultrasound images to detect cancers. We observe that after fine tuning, DenseNet201 and ResNet50 show 100% accuracy with Adam and RMSprop optimizers. VGG16 shows 100% accuracy using the Stochastic Gradient Descent optimizer. We also develop a custom convolutional neural network model with a smaller number of layers compared to large layers in the pre-Trained models. The model also shows 100% accuracy using the Adam optimizer in classifying healthy and breast cancer patients. It is our belief that the model will assist healthcare experts with improved and faster patient screening and pave a way to further breast cancer research. © 2021 Association for Computing Machinery.",breast cancer; convolutional neural network; Deep learning; gaze detection,Convolution; Convolutional neural networks; Data handling; Deep learning; Diagnosis; Gradient methods; Medical imaging; Multilayer neural networks; Optimization; Patient treatment; Stochastic systems; Ultrasonic applications; Breast Cancer; Breast cancer detection; Convolutional neural network; Deep learning; Gaze detection; Health care application; Key feature; Optimizers; Ultrasound images; Volunteer computing; Diseases
An Efficient Angle-based Universum Least Squares Twin Support Vector Machine for Classification,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114274044&doi=10.1145%2f3387131&partnerID=40&md5=ef6644567ddcf53ba214235ec68062c2,"Universum-based support vector machine incorporates prior information about the distribution of data in training of the classifier. This leads to better generalization performance but with increased computation cost. Various twin hyperplane-based models are proposed to reduce the computation cost of universum-based algorithms. In this work, we present an efficient angle-based universum least squares twin support vector machine (AULSTSVM) for classification. This is a novel approach of incorporating universum in the formulation of least squares-based twin SVM model. First, the proposed AULSTSVM constructs a universum hyperplane, which is proximal to universum data points. Then, the classifying hyperplane is constructed by minimizing the angle with the universum hyperplane. This gives prior information about data distribution to the classifier. In addition to the quadratic loss, we introduce linear loss in the optimization problem of the proposed AULSTSVM, which leads to lesser computation cost of the model. Numerical experiments are performed on several benchmark synthetic, real-world, and large-scale datasets. The results show that proposed AULSTSVM performs better than existing algorithms w.r.t. generalization performance as well as computation time. Moreover, an application to Alzheimer's disease is presented, where AULSTSVM obtains accuracy of 95% for classification of healthy and Alzheimers subjects. The results imply that the proposed AULSTSVM is a better alternative for classification of large-scale datasets and biomedical applications. © 2021 Association for Computing Machinery.",Alzheimer's disease; Angle measure; magnetic resonance imaging; twin support vector machine,Classification (of information); Geometry; Large dataset; Medical applications; Neurodegenerative diseases; Alzheimer's disease; Biomedical applications; Generalization performance; Large-scale datasets; Least squares twin support vector machines; Numerical experiments; Optimization problems; Prior information; Support vector machines
Robust Facial Image Super-Resolution by Kernel Locality-Constrained Coupled-Layer Regression,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114284125&doi=10.1145%2f3418462&partnerID=40&md5=a55af45ed59d5eb8bc85ebce0bf4471f,"Super-resolution methods for facial image via representation learning scheme have become very effective methods due to their efficiency. The key problem for the super-resolution of facial image is to reveal the latent relationship between the low-resolution (LR) and the corresponding high-resolution (HR) training patch pairs. To simultaneously utilize the contextual information of the target position and the manifold structure of the primitive HR space, in this work, we design a robust context-patch facial image super-resolution scheme via a kernel locality-constrained coupled-layer regression (KLC2LR) scheme to obtain the desired HR version from the acquired LR image. Here, KLC2LR proposes to acquire contextual surrounding patches to represent the target patch and adds an HR layer constraint to compensate the detail information. Additionally, KLC2LR desires to acquire more high-frequency information by searching for nearest neighbors in the HR sample space. We also utilize kernel function to map features in original low-dimensional space into a high-dimensional one to obtain potential nonlinear characteristics. Our compared experiments in the noisy and noiseless cases have verified that our suggested methodology performs better than many existing predominant facial image super-resolution methods. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contextual patches; coupled-layer representation; Face super-resolution; locality-constrained representation,Internet; Contextual information; High-frequency informations; Low-dimensional spaces; Manifold structures; Nearest neighbors; Nonlinear characteristics; Super resolution; Superresolution methods; Optical resolving power
Dynamic Scheduling Algorithm in Cyber Mimic Defense Architecture of Volunteer Computing,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114277044&doi=10.1145%2f3408291&partnerID=40&md5=997f3278738fdabc4c02a404ed6a4c3f,"Volunteer computing uses computers volunteered by the general public to do distributed scientific computing. Volunteer computing is being used in high-energy physics, molecular biology, medicine, astrophysics, climate study, and other areas. These projects have attained unprecedented computing power. However, with the development of information technology, the traditional defense system cannot deal with the unknown security problems of volunteer computing. At the same time, Cyber Mimic Defense (CMD) can defend the unknown attack behavior through its three characteristics: dynamic, heterogeneous, and redundant. As an important part of the CMD, the dynamic scheduling algorithm realizes the dynamic change of the service centralized executor, which can enusre the security and reliability of CMD of volunteer computing. Aiming at the problems of passive scheduling and large scheduling granularity existing in the existing scheduling algorithms, this article first proposes a scheduling algorithm based on time threshold and task threshold and realizes the dynamic randomness of mimic defense from two different dimensions; finally, combining time threshold and random threshold, a dynamic scheduling algorithm based on multi-level queue is proposed. The experiment shows that the dynamic scheduling algorithm based on multi-level queue can take both security and reliability into account, has better dynamic heterogeneous redundancy characteristics, and can effectively prevent the transformation rule of heterogeneous executors from being mastered by attackers. © 2021 Association for Computing Machinery.",Cyber Mimic Defense; Dynamic Scheduling; Multi-level Queue; Task Threshold; Time Threshold; Volunteer computing,Astrophysics; Computer architecture; High energy physics; Molecular biology; Redundancy; Scheduling; Defense architecture; Distributed scientific computing; Dynamic randomness; Dynamic scheduling algorithms; Security and reliabilities; Security problems; Transformation rules; Volunteer computing; Network security
Waiting for Tactile: Robotic and Virtual Experiences in the Fog,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114279860&doi=10.1145%2f3421507&partnerID=40&md5=b07abead3190f83ef9edc6b55a9fc574,"Social robots adopt an emotional touch to interact with users inducing and transmitting humanlike emotions. Natural interaction with humans needs to be in real time and well grounded on the full availability of information on the environment. These robots base their way of communicating on direct interaction (touch, listening, view), supported by a range of sensors on the surrounding environment that provide a radially central and partial knowledge on it. Over the past few years, social robots have been demonstrated to implement different features, going from biometric applications to the fusion of machine learning environmental information collected on the edge. This article aims at describing the experiences performed and still ongoing and characterizes a simulation environment developed for the social robot Pepper that aims to foresee the new scenarios and benefits that tactile connectivity will enable. © 2021 Association for Computing Machinery.",3D simulation; ambient-robot interaction; robotics; Tactile Internet; wireless IoT communication,Robotics; Biometric applications; Direct interactions; Environmental information; Natural interactions; Partial knowledge; Real time; Simulation environment; Surrounding environment; Social robots
Spatio-temporal Bayesian Learning for Mobile Edge Computing Resource Planning in Smart Cities,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114271063&doi=10.1145%2f3448613&partnerID=40&md5=d2575e6e29a90c140f9f46a46b0b203a,"A smart city improves operational efficiency and comfort of living by harnessing techniques such as the Internet of Things (IoT) to collect and process data for decision-making. To better support smart cities, data collected by IoT should be stored and processed appropriately. However, IoT devices are often task-specialized and resource-constrained, and thus, they heavily rely on online resources in terms of computing and storage to accomplish various tasks. Moreover, these cloud-based solutions often centralize the resources and are far away from the end IoTs and cannot respond to users in time due to network congestion when massive numbers of tasks offload through the core network. Therefore, by decentralizing resources spatially close to IoT devices, mobile edge computing (MEC) can reduce latency and improve service quality for a smart city, where service requests can be fulfilled in proximity. As the service demands exhibit spatial-temporal features, deploying MEC servers at optimal locations and allocating MEC resources play an essential role in efficiently meeting service requirements in a smart city. In this regard, it is essential to learn the distribution of resource demands in time and space. In this work, we first propose a spatio-temporal Bayesian hierarchical learning approach to learn and predict the distribution of MEC resource demand over space and time to facilitate MEC deployment and resource management. Second, the proposed model is trained and tested on real-world data, and the results demonstrate that the proposed method can achieve very high accuracy. Third, we demonstrate an application of the proposed method by simulating task offloading. Finally, the simulated results show that resources allocated based upon our models' predictions are exploited more efficiently than the resources are equally divided into all servers in unobserved areas. © 0000 Association for Computing Machinery.",geospatial intelligence; Spatial-temporal analysis,Decision making; Digital storage; Edge computing; Quality of service; Resource allocation; Smart city; Computing resource; Hierarchical learning; Internet of thing (IOT); Network congestions; Operational efficiencies; Resource management; Service requirements; Spatial-temporal features; Internet of things
Time-Efficient Ensemble Learning with Sample Exchange for Edge Computing,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114270093&doi=10.1145%2f3409265&partnerID=40&md5=84cee54d412c0e45c6f132794a9307ce,"In existing ensemble learning algorithms (e.g., random forest), each base learner's model needs the entire dataset for sampling and training. However, this may not be practical in many real-world applications, and it incurs additional computational costs. To achieve better efficiency, we propose a decentralized framework: Multi-Agent Ensemble. The framework leverages edge computing to facilitate ensemble learning techniques by focusing on the balancing of access restrictions (small sub-dataset) and accuracy enhancement. Specifically, network edge nodes (learners) are utilized to model classifications and predictions in our framework. Data is then distributed to multiple base learners who exchange data via an interaction mechanism to achieve improved prediction. The proposed approach relies on a training model rather than conventional centralized learning. Findings from the experimental evaluations using 20 real-world datasets suggest that Multi-Agent Ensemble outperforms other ensemble approaches in terms of accuracy even though the base learners require fewer samples (i.e., significant reduction in computation costs). © 2021 Association for Computing Machinery.",decentralized ensemble learning; Edge computing; ensemble learning; Multi-Agent Ensemble,Decision trees; Edge computing; Learning systems; Multi agent systems; Accuracy enhancement; Computational costs; Ensemble approaches; Ensemble learning algorithm; Experimental evaluation; Interaction mechanisms; Model classification; Real-world datasets; Learning algorithms
A Blockchain-empowered Access Control Framework for Smart Devices in Green Internet of Things,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112676115&doi=10.1145%2f3433542&partnerID=40&md5=0ab91714789d043f704aa9c973a10cb2,"Green Internet of things (GIoT) generally refers to a new generation of Internet of things design concept. It can save energy and reduce emissions, reduce environmental pollution, waste of resources, and harm to human body and environment, in which green smart device (GSD) is a basic unit of GIoT for saving energy. With the access of a large number of heterogeneous bottom-layer GSDs in GIoT, user access and control of GSDs have become more and more complicated. Since there is no unified GSD management system, users need to operate different GIoT applications and access different GIoT cloud platforms when accessing and controlling these heterogeneous GSDs. This fragmented GSD management model not only increases the complexity of user access and control for heterogeneous GSDs, but also reduces the scalability of GSDs applications. To address this issue, this article presents a blockchain-empowered general GSD access control framework, which provides users with a unified GSD management platform. First, based on the World Wide Web Consortium (W3C) decentralized identifiers (DIDs) standard, users and GSD are issued visual identity (VID). Then, we extended the GSD-DIDs protocol to authenticate devices and users. Finally, based on the characteristics of decentralization and non-tampering of blockchain, a unified access control system for GSD was designed, including the registration, granting, and revoking of access rights. We implement and test on the Raspberry Pi device and the FISCO-BCOS alliance chain. The experimental results prove that the framework provides a unified and feasible way for users to achieve decentralized, lightweight, and fine-grained access control of GSDs. The solution reduces the complexity of accessing and controlling GSDs, enhances the scalability of GSD applications, as well as guarantees the credibility and immutability of permission data and identity data during access. © 2021 Association for Computing Machinery.",access control; application fragmentation; blockchain; decentralized identifier; Green Internet of Things,Blockchain; Internet of things; Man machine systems; Scalability; Cloud platforms; Control framework; Environmental pollutions; Management Model; Management platforms; Management systems; Waste of resources; World wide web consortiums; Authentication
Rotating behind Privacy: An Improved Lightweight Authentication Scheme for Cloud-based IoT Environment,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111987577&doi=10.1145%2f3425707&partnerID=40&md5=73c20ff2e0c06e93d6f323c94194ecf2,"The advancements in the internet of things (IoT) require specialized security protocols to provide unbreakable security along with computation and communication efficiencies. Moreover, user privacy and anonymity has emerged as an integral part, along with other security requirements. Unfortunately, many recent authentication schemes to secure IoT-based systems were either proved as vulnerable to different attacks or prey of inefficiencies. Some of these schemes suffer from a faulty design that happened mainly owing to undue emphasis on privacy and anonymity alongside performance efficiency. This article aims to show the design faults by analyzing a very recent hash functions-based authentication scheme for cloud-based IoT systems with misunderstood privacy cum efficiency tradeoff owing to an unadorned design flaw, which is also present in many other such schemes. Precisely, it is proved in this article that the scheme of Wazid et al. cannot provide mutual authentication and key agreement between a user and a sensor node when there exists more than one registered user. We then proposed an improved scheme and proved its security through formal and informal methods. The proposed scheme completes the authentication cycle with a minor increase in computation cost but provides all security goals along with privacy. © 2021 Association for Computing Machinery.",anonymity; cloud security; incorrectness; IoT; key-agreement; Security; traceability,Authentication; Efficiency; Hash functions; Network security; Privacy by design; Sensor nodes; Authentication scheme; Communication efficiency; Different attacks; Internet of thing (IOT); Mutual authentication; Performance efficiency; Security protocols; Security requirements; Internet of things
Intelligent Control and Security of Fog Resources in Healthcare Systems via a Cognitive Fog Model,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114281382&doi=10.1145%2f3382770&partnerID=40&md5=f75a0ef9e30784bc2951b094b8b36b50,"There have been significant advances in the field of Internet of Things (IoT) recently, which have not always considered security or data security concerns: A high degree of security is required when considering the sharing of medical data over networks. In most IoT-based systems, especially those within smart-homes and smart-cities, there is a bridging point (fog computing) between a sensor network and the Internet which often just performs basic functions such as translating between the protocols used in the Internet and sensor networks, as well as small amounts of data processing. The fog nodes can have useful knowledge and potential for constructive security and control over both the sensor network and the data transmitted over the Internet. Smart healthcare services utilise such networks of IoT systems. It is therefore vital that medical data emanating from IoT systems is highly secure, to prevent fraudulent use, whilst maintaining quality of service providing assured, verified and complete data. In this article, we examine the development of a Cognitive Fog (CF) model, for secure, smart healthcare services, that is able to make decisions such as opting-in and opting-out from running processes and invoking new processes when required, and providing security for the operational processes within the fog system. Overall, the proposed ensemble security model performed better in terms of Accuracy Rate, Detection Rate, and a lower False Positive Rate (standard intrusion detection measurements) than three base classifiers (K-NN, DBSCAN, and DT) using a standard security dataset (NSL-KDD). © 2021 Association for Computing Machinery.",cognitive fog; Fog computing; fog security; medical data security,Automation; Classification (of information); Cognitive systems; Data Sharing; Fog; Fog computing; Health care; Intelligent buildings; Intrusion detection; Network security; Quality of service; Sensor networks; Sensor nodes; Base classifiers; Detection rates; False positive rates; Health-care system; Healthcare services; Internet of Things (IOT); Operational process; Running process; Internet of things
Multimodal Brain Tumor Segmentation Based on an Intelligent UNET-LSTM Algorithm in Smart Hospitals,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114280178&doi=10.1145%2f3450519&partnerID=40&md5=3c519ccf3b0730e9e91580e7b5024057,"Smart hospitals are important components of smart cities. An intelligent medical system for brain tumor segmentation is required to construct smart hospitals. To achieve intelligent brain tumor segmentation, morphological variety and serious category imbalance must be managed effectively. Conventional deep neural networks have difficulty in predicting high-accuracy segmentation images due to these issues. To solve these problems, we propose using multimodal brain tumor images combined with the UNET and LSTM models to construct a new network structure with a mixed loss function to solve sample imbalance and describe an intelligent segmentation process to identify brain tumors. To verify the practicability of this algorithm, we used the open source Brain Tumor Segmentation Challenge dataset to train and verify the proposed network. We obtained DSCs of 0.91, 0.82, and 0.80; sensitivities of 0.93, 0.85, and 0.82; and specificities of 0.99, 0.99, and 0.98 in three tumor regions, including the whole tumor (WT), tumor core (TC), and enhanced tumor (ET). We also compared the results of the proposed network with those of other brain tumor segmentation methods, and the results showed that the proposed algorithm could segment different tumor lesions more accurately, highlighting its potential application value in the clinical diagnosis of brain tumors. © 2021 Association for Computing Machinery.",Brain tumor segmentation; smart hospitals; UNET-LSTM algorithm,Brain; Deep neural networks; Hospitals; Image segmentation; Long short-term memory; Tumors; Brain tumor segmentation; Clinical diagnosis; Intelligent segmentation; Loss functions; Medical systems; Network structures; Segmentation images; Smart hospital; Diagnosis
Machine Learning Empowered IoT for Intelligent Vehicle Location in Smart Cities,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114279400&doi=10.1145%2f3448612&partnerID=40&md5=c0510e3ca1ffaebf4ab7a999202bb0db,"Intelligent Transportation System (ITS) can boost the development of smart cities, and artificial intelligence and edge computing are key technologies that support the implementation of ITS. Vehicle localization is critical for ITS since the safety driving and location-aware serves highly depend on the accurate location information. In this article, we construct a vehicle localization system architecture composed of multiple Internet of Things (IoT) with arbitrary array configuration and a large amount of vehicles in smart cities. In order to deal with the coexisting of circular and non-circular signals transmitted by vehicles, we proposed several vehicle number estimation methods for non-circular signals. Based on the machine learning technique, we extend the vehicle number estimation method into mixed signals in more complex scenario of smart cities. Then the DOA estimation method for non-circular signals based on IoT is proposed, and then the performance of this method is analyzed as well. Simulation outcomes verify the excellent performance of the proposed vehicle number estimation methods and the DOA estimation method in smart cities, and the vehicle positions can be achieved with high estimation accuracy. © 2021 Association for Computing Machinery.",IoT; machine learning; Smart cities; vehicle location,Direction of arrival; Intelligent systems; Internet of things; Location; Machine learning; Smart city; Vehicles; Accurate location; DOA estimation method; Estimation methods; Intelligent transportation systems; Internet of Things (IOT); Machine learning techniques; Vehicle localization; Vehicle position; Intelligent vehicle highway systems
A Novel Image Inpainting Framework Using Regression,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114280239&doi=10.1145%2f3402177&partnerID=40&md5=e9fc60fe7d68a298890367a33aaf771e,"In this article, a blockwise regression-based image inpainting framework is proposed. The core idea is to fill the unknown region in two stages: Extrapolate the edges to the unknown region and then fill the unknown pixels values in each sub-region demarcated by the extended edges. Canny edge detection and linear edge extension are used to respectively identify and extend edges to the unknown region followed by regression within each sub-region to predict the unknown pixel values. Two different regression models based on K-nearest neighbours and support vectors machine are used to predict the unknown pixel values. The proposed framework has the advantage of inpainting without requiring prior training on any image dataset. The extensive experiments on different images with contrasting distortions demonstrate the robustness of the proposed framework and a detailed comparative analysis shows that the proposed technique outperforms existing state-of-the-art image inpainting methods. Finally, the proposed techniques are applied to MRI images suffering from susceptibility artifacts to illustrate the practical usage of the proposed work. © 2021 Association for Computing Machinery.",edge detection and extension; Image inpainting; k-nearest neighbours; regression; support vector machine,Edge detection; Nearest neighbor search; Pixels; Support vector regression; Canny edge detection; Comparative analysis; Image Inpainting; K-nearest neighbours; Regression model; State of the art; Support vectors machine; Susceptibility artifacts; Magnetic resonance imaging
Deep Attentive Multimodal Network Representation Learning for Social Media Images,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114273423&doi=10.1145%2f3417295&partnerID=40&md5=ec2f0481efc662c911b16c828cd8ceca,"The analysis for social networks, such as the socially connected Internet of Things, has shown a deep influence of intelligent information processing technology on industrial systems for Smart Cities. The goal of social media representation learning is to learn dense, low-dimensional, and continuous representations for multimodal data within social networks, facilitating many real-world applications. Since social media images are usually accompanied by rich metadata (e.g., textual descriptions, tags, groups, and submitted users), simply modeling the image is not effective to learn the comprehensive information from social media images. In this work, we treat the image and its textual description as multimodal content, and transform other metainformation into the links between contents (such as two images marked by the same tag or submitted by the same user). Based on the multimodal content and social links, we propose a Deep Attentive Multimodal Graph Embedding model named DAMGE for more effective social image representation learning. We introduce both small- and large-scale datasets to conduct extensive experiments, of which the results confirm the superiority of the proposal on the tasks of social image classification and link prediction. © 2021 Association for Computing Machinery.",attention network; graph convolutional network; multimodal; representation learning; Social image,Classification (of information); Deep learning; Large dataset; Metadata; Social networking (online); Social sciences computing; Comprehensive information; Graph embeddings; Industrial systems; Intelligent information processing; Large-scale datasets; Multi-modal data; Multimodal network; Textual description; Economic and social effects
Design and Implementation of BCI-based Intelligent Upper Limb Rehabilitation Robot System,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114274475&doi=10.1145%2f3392115&partnerID=40&md5=989bef3e80ffe1540aa000ca33425349,"The present study aimed to use the proposed system to measure and analyze brain waves of users to allow intelligent upper limb rehabilitation and to optimize the system using a genetic algorithm. The study used EPOC Neuroheadset for Emotiv with EEG electrodes attached as a non-invasive method for measuring brain waves. The brain waves were measured according to the EEG 10-20 standard electrode layout, which allows measurement of signals from each spot where electrodes are attached based on EEG characteristics. The measured data were added in a database. In the intelligent neuro-fuzzy model, wave transform was used for extracting brain wave characteristics according to user intentions and to eliminate noise from the signals in an effort to increase reliability. Moreover, to construct the option rules of the neuro-fuzzy system, FCM technique and optimal cluster evaluation method were used. Furthermore, the asymmetric Gaussian membership function was used to improve performance, whereas SD and WF divided into left and right sides were used to express the chromosomes. Optimal EEG electrode locations were found, and comparative analysis was performed on the differences based on membership function, number of clusters, and number of learning generations, learning algorithm, and wavelet settings. The performance evaluation results showed that the optimal EEG electrode locations were F7, F8, FC5, and FC6, whereas the accuracy of learning and test data of user-intention recognition was found to be 94.2% and 92.3%, respectively, which suggests that the proposed system can be used to recognize user intention for specific behavior. The system proposed in the present study can allow continued rehabilitation exercise in everyday living according to user intentions, which is expected to help improve the user's willingness to participate in rehabilitation and his or her quality of life. © 2021 Association for Computing Machinery.",BCI (brain-computer interface); genetic algorithm; neural network; neuro-fuzzy system; rehabilitation robot system; upper limb rehabilitation,Biomedical signal processing; Brain computer interface; Chromosomes; Electrodes; Function evaluation; Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Genetic algorithms; Intelligent robots; Machine design; Membership functions; Neuromuscular rehabilitation; Noninvasive medical procedures; Asymmetric Gaussian membership functions; Cluster evaluations; Comparative analysis; Design and implementations; Improve performance; Noninvasive methods; Rehabilitation exercise; Upper-limb rehabilitation; Learning algorithms
ISDNet: AI-enabled Instance Segmentation of Aerial Scenes for Smart Cities,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114277110&doi=10.1145%2f3418205&partnerID=40&md5=3925953d40db50a9e4b89a829824c8ee,"Aerial scenes captured by UAVs have immense potential in IoT applications related to urban surveillance, road and building segmentation, land cover classification, and so on, which are necessary for the evolution of smart cities. The advancements in deep learning have greatly enhanced visual understanding, but the domain of aerial vision remains largely unexplored. Aerial images pose many unique challenges for performing proper scene parsing such as high-resolution data, small-scaled objects, a large number of objects in the camera view, dense clustering of objects, background clutter, and so on, which greatly hinder the performance of the existing deep learning methods. In this work, we propose ISDNet (Instance Segmentation and Detection Network), a novel network to perform instance segmentation and object detection on visual data captured by UAVs. This work enables aerial image analytics for various needs in a smart city. In particular, we use dilated convolutions to generate improved spatial context, leading to better discrimination between foreground and background features. The proposed network efficiently reuses the segment-mask features by propagating them from early stages using residual connections. Furthermore, ISDNet makes use of effective anchors to accommodate varying object scales and sizes. The proposed method obtains state-of-the-art results in the aerial context. © 2021 Association for Computing Machinery.",aerial scenes; deep learning; instance segmentation; object detection; Smart cities; UAVs,Aircraft detection; Deep learning; Learning systems; Object detection; Smart city; Background clutter; Background features; Detection networks; High resolution data; Land cover classification; Learning methods; State of the art; Urban surveillance; Antennas
Random Graph-based Multiple Instance Learning for Structured IoT Smart City Applications,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114278609&doi=10.1145%2f3448611&partnerID=40&md5=f6e2e9892d87bb061b50c6d3089e23c3,"Because of the complex activities involved in IoT networks of a smart city, an important question arises: What are the core activities of the networks as a whole and its basic information flow structure? Identifying and discovering core activities and information flow is a crucial step that can facilitate the analysis. This is the question we are addressing - that is, to identify the core services as a common core substructure despite the probabilistic nature and the diversity of its activities. If this common substructure can be discovered, a systemic analysis and planning can then be performed and key policies related to the community can be developed. Here, a local IoT network can be represented as an attributed graph. From an ensemble of attributed graphs, identifying the common subgraph pattern is then critical in understanding the complexity. We introduce this as the common random subgraph (CRSG) modeling problem, aiming at identifying a subgraph pattern that is the structural ""core""that conveys the probabilistically distributed graph characteristics. Given an ensemble of network samples represented as attributed graphs, the method generates a CRSG model that encompasses both structural and statistical characteristics from the related samples while excluding unrelated networks. In generating a CRSG model, our method using a multiple instance learning algorithm transforms an attributed graph (composed of structural elements as edges and their two endpoints) into a ""bag""of instances in a vector space. Common structural components across positively labeled graphs are then identified as the common instance patterns among instances across different bags. The structure of the CRSG arises through the combining of common patterns. The probability distribution of the CRSG can then be estimated based on the connections and distributions from the common elements. Experimental results demonstrate that CRSG models are highly expressive in describing typical network characteristics. © 2021 Association for Computing Machinery.",core activity; graph embedding; multiple instance learning; partial entropy; random graphs; Smart city; subgraph pattern recognition,Complex networks; Graph algorithms; Graphic methods; Internet of things; Learning algorithms; Probability distributions; Smart city; Vector spaces; Attributed graphs; Information flows; Multiple instance learning; Network characteristics; Statistical characteristics; Structural component; Structural elements; Systemic analysis; Learning systems
Power Side-Channel Analysis of RNS GLV ECC Using Machine and Deep Learning Algorithms,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114279145&doi=10.1145%2f3423555&partnerID=40&md5=ebc02cefe698c7adfb871490d3b08573,"Many Internet of Things applications in smart cities use elliptic-curve cryptosystems due to their efficiency compared to other well-known public-key cryptosystems such as RSA. One of the important components of an elliptic-curve-based cryptosystem is the elliptic-curve point multiplication which has been shown to be vulnerable to various types of side-channel attacks. Recently, substantial progress has been made in applying deep learning to side-channel attacks. Conceptually, the idea is to monitor a core while it is running encryption for information leakage of a certain kind, for example, power consumption. The knowledge of the underlying encryption algorithm can be used to train a model to recognise the key used for encryption. The model is then applied to traces gathered from the crypto core in order to recover the encryption key. In this article, we propose an RNS GLV elliptic curve cryptography core which is immune to machine learning and deep learning based side-channel attacks. The experimental analysis confirms the proposed crypto core does not leak any information about the private key and therefore it is suitable for hardware implementations. © 2021 Association for Computing Machinery.",convolutional neural networks; deep learning; Elliptic curve cryptography; Gallant-Lambert-Vanstone (GLV) point multiplication; machine learning; side-channel attacks,Deep learning; Geometry; Learning algorithms; Learning systems; Public key cryptography; Elliptic curve cryptography; Elliptic Curve cryptosystems; Elliptic curve point multiplications; Encryption algorithms; Experimental analysis; Hardware implementations; Public key cryptosystems; Side-channel analysis; Side channel attack
FinPrivacy: A Privacy-preserving Mechanism for Fingerprint Identification,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114276797&doi=10.1145%2f3387130&partnerID=40&md5=23ba4a23dae984d4b49ad6551c560df4,"Fingerprint provides an extremely convenient way of identification for a wide range of real-life applications owing to its universality, uniqueness, collectability, and invariance. However, digitized fingerprints may reveal the privacy of individuals. Differential privacy is a promising privacy-preserving solution that is enforced by injecting random noise into preserved objects, such that an adversary with arbitrary background knowledge cannot infer private input from the noisy results. This study proposes FinPrivacy, a privacy-preserving mechanism for fingerprint identification. This mechanism utilizes the low-rank matrix approximation to reduce the dimensionality of fingerprint and the exponential mechanism to carefully determine the value of the optimal rank. Thereafter, FinPrivacy injects Laplace noise to the singular values of the approximated singular matrix, thereby trading off between privacy and utility. Analytic proofs and results of the comparative experiments demonstrate that FinPrivacy can simultaneously enforce δ-differential privacy and maintain an efficient fingerprint recognition. © 2021 Association for Computing Machinery.",differential privacy; fingerprint; low-rank matrix approximation; sensitivity,Privacy by design; Back-ground knowledge; Comparative experiments; Differential privacies; Fingerprint identification; Fingerprint Recognition; Low-rank matrix approximations; Privacy preserving solutions; Real-life applications; Palmprint recognition
Predictive Analytics for Smart Parking: A Deep Learning Approach in Forecasting of IoT Data,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114274135&doi=10.1145%2f3412842&partnerID=40&md5=f831ceb4cc276c640932f1804cf14730,"Nowadays, a sustainable and smart city focuses on energy efficiency and the reduction of polluting emissions through smart mobility projects and initiatives to ""sensitize""infrastructure. Smart parking is one of the building blocks of intelligent mobility, innovative mobility that aims to be flexible, integrated, and sustainable and consequently integrated into a Smart City. By using the Internet of Things (IoT) sensors located in the parking areas or the underground car parks in combination with a mobile application, which indicates to citizens the free places in the different areas of the city and guides them toward the chosen parking, it is possible to reduce air pollution and fluidifying noise traffic. In this article, we present and discuss an innovative Deep Learning-based ensemble technique in forecasting the parking space occupancy to reduce the search time for parking and to optimize the flow of cars in particularly congested areas, with an overall positive impact on traffic in urban centres. A genetic algorithm has also been used to optimize predictors parameters. The main goal is to design an intelligent IoT-based service that can predict, in the next few hours, the parking spaces occupancy of a street. The proposed approach has been assessed on a real IoT dataset composed by over than 15M of collected sensor records. Obtained results demonstrate that our method outperforms both single predictors and the widely used strategy of the mean providing inherently robust predictions. © 2021 Association for Computing Machinery.",artificial intelligence; Deep learning; internet of things; predictive analytics; smart city,Deep learning; Energy efficiency; Forecasting; Genetic algorithms; Predictive analytics; Smart city; Ensemble techniques; Intelligent mobility; Internet of thing (IOT); Learning approach; Mobile applications; Polluting emission; Robust predictions; Underground car parks; Internet of things
Sustainable Security for the Internet of Things Using Artificial Intelligence Architectures,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114284172&doi=10.1145%2f3448614&partnerID=40&md5=a18d4cf8e900a43da553254f6583608e,"In this digital age, human dependency on technology in various fields has been increasing tremendously. Torrential amounts of different electronic products are being manufactured daily for everyday use. With this advancement in the world of Internet technology, cybersecurity of software and hardware systems are now prerequisites for major business' operations. Every technology on the market has multiple vulnerabilities that are exploited by hackers and cyber-criminals daily to manipulate data sometimes for malicious purposes. In any system, the Intrusion Detection System (IDS) is a fundamental component for ensuring the security of devices from digital attacks. Recognition of new developing digital threats is getting harder for existing IDS. Furthermore, advanced frameworks are required for IDS to function both efficiently and effectively. The commonly observed cyber-attacks in the business domain include minor attacks used for stealing private data. This article presents a deep learning methodology for detecting cyber-attacks on the Internet of Things using a Long Short Term Networks classifier. Our extensive experimental testing show an Accuracy of 99.09%, F1-score of 99.46%, and Recall of 99.51%, respectively. A detailed metric representing our results in tabular form was used to compare how our model was better than other state-of-the-art models in detecting cyber-attacks with proficiency. © 2021 Association for Computing Machinery.",Cybersecurity; DDoS; deep learning; IDS; IoT; network traffic,Computer crime; Crime; Deep learning; Digital devices; Internet of things; Intrusion detection; Personal computing; Business domain; Electronic product; Experimental testing; Fundamental component; Internet technology; Intrusion Detection Systems; Software and hardwares; State of the art; Network security
Privacy-preserving Time-series Medical Images Analysis Using a Hybrid Deep Learning Framework,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114276840&doi=10.1145%2f3383779&partnerID=40&md5=11339ed98d06041f32eb88238a31b067,"Time-series medical images are an important type of medical data that contain rich temporal and spatial information. As a state-of-the-art, computer-aided diagnosis (CAD) algorithms are usually used on these image sequences to improve analysis accuracy. However, such CAD algorithms are often required to upload medical images to honest-but-curious servers, which introduces severe privacy concerns. To preserve privacy, the existing CAD algorithms support analysis on each encrypted image but not on the whole encrypted image sequences, which leads to the loss of important temporal information among frames. To meet this challenge, a convolutional-LSTM network, named HE-CLSTM, is proposed for analyzing time-series medical images encrypted by a fully homomorphic encryption mechanism. Specifically, several convolutional blocks are constructed to extract discriminative spatial features, and LSTM-based sequence analysis layers (HE-LSTM) are leveraged to encode temporal information from the encrypted image sequences. Moreover, a weighted unit and a sequence voting layer are designed to incorporate both spatial and temporal features with different weights to improve performance while reducing the missed diagnosis rate. The experimental results on two challenging benchmarks (a Cervigram dataset and the BreaKHis public dataset) provide strong evidence that our framework can encode visual representations and sequential dynamics from encrypted medical image sequences; our method achieved AUCs above 0.94 both on the Cervigram and BreaKHis datasets, constituting a significant margin of statistical improvement compared with several competing methods. © 2021 Association for Computing Machinery.",CNN-LSTM; deep learning; Time-series medical images analysis,Computer aided analysis; Computer aided diagnosis; Convolution; Convolutional neural networks; Cryptography; Deep learning; Encoding (symbols); Image analysis; Image enhancement; Long short-term memory; Privacy by design; Time series; Time series analysis; Computer Aided Diagnosis(CAD); Fully homomorphic encryption; Improve performance; Learning frameworks; Temporal and spatial; Temporal information; Time-series medical image; Visual representations; Medical image processing
CROWD: Crow search and deep learning based feature extractor for classification of parkinson's disease,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112260105&doi=10.1145%2f3418500&partnerID=40&md5=e8a700c6afa88b0745096e23ec0708c2,"Edge Artificial Intelligence (AI) is the latest trend for next-generation computing for data analytics, particularly in predictive edge analytics for high-risk diseases like Parkinson's Disease (PD). Deep learning learning techniques facilitate edge AI applications for enhanced, real-time handling of data. Dopamine is the cause of Parkinson's that happens due to the interference of brain cells that produce the substance to regulate the communication of brain cells. The brain cells responsible for generating the dopamine perform adaptation, control, and movement with fluency. Parkinson's motor symptoms appear on the loss of 60% to 80% of cells, due to the non-production of appropriate dopamine. Recent research found a close connection between the speech impairment and PD. Many researchers have developed a classification algorithm to identify the PD from speech signals. In this article, Adaptive Crow Search Algorithm (ACSA) and Deep Learning (DL)-based optimal feature selection method are introduced. The proposed model is the combination of CROW Search and Deep learning (CROWD) stack sparse autoencoder neural network. Parkinson's dataset is taken for the experiment from the Irvine dataset repository at the University of California (UCI). In the first phase, dataset cleaning is performed to handle the missing values in the dataset. After that, the proposed ACSA algorithm is employed to find the scrunched feature vector. Furthermore, stack spare autoencoder with seven hidden layers is employed to generate the compressed feature vector. The performance of the proposed CROWD autoencoder model is compared with three feature selection approaches for six supervised classification techniques. The experiment result demonstrates that the performance of the proposed CROWD autoencoder feature selection model has outperformed the benchmarked feature selection techniques: (i) Maximum Relevance (mRMR) (ii) Recursive Feature Elimination (RFE), and (iii) Correlation-based Feature Selection (CFS), to classify Parkinson's disease. This research has significance in the healthcare sector for the enhancement of classification accuracy up to 0.96%. © 2021 Association for Computing Machinery.",artificial Intelligence; Deep learning; feature extraction; Parkinson disease,Amines; Cells; Classification (of information); Cytology; Data Analytics; Feature extraction; Learning systems; Neurodegenerative diseases; Neurophysiology; Predictive analytics; Classification accuracy; Classification algorithm; Compressed feature vectors; Correlation based feature selections (CFS); Optimal feature selections; Recursive feature elimination; Supervised classification; University of California; Deep learning
The Security of Medical Data on Internet Based on Differential Privacy Technology,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114283540&doi=10.1145%2f3382769&partnerID=40&md5=91df1452273510d394f071f5d4a434b9,"The study aims at discussing the security of medical data in the Internet era. By using k-anonymity (K-A) and differential privacy (DP), an algorithm model combining K-A and DP was proposed, which was simulated through the experiments. In the Magic and EIA datasets, the algorithm constructed was compared with K-A and the L-diversity model to verify the performance of the model. The model constructed based on DP had the lowest privacy-leakage risks, which increased with the number of identifiers in the Magic and EIA datasets, and the information disclosure was the least. In addition, in its usability analysis, it was found that its value was the most obviously improved and its operation efficiency was the highest. The K-A-DP algorithm can effectively reduce the risk of privacy leakage and information loss, and has achieved excellent results. Despite the deficiencies in the process of the experiment, the study still provides a reference for solving the problem of medical data security. © 2021 Association for Computing Machinery.",DP; information loss; k-anonymity; medical data; privacy leakage risk,Internet; Algorithm model; Differential privacies; Information disclosure; Information loss; L diversities; Operation efficiencies; Privacy leakages; Usability analysis; Privacy by design
Secure DNA Motif-Finding Method Based on Sampling Candidate Pruning,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099538729&doi=10.1145%2f3382078&partnerID=40&md5=20931fa1a61cbcdd1ee7426f3804a1fe,"With the continuous exploration of genetic research, gradually exposed privacy issues become the bottleneck that limits its development. DNA motif finding is an important study to understand the regulation of gene expression; however, the existing methods generally ignore the potential sensitive information that may be exposed in the process. In this work, we utilize the -differential privacy model to provide provable privacy guarantees which is independent of attackers' background knowledge. Our method makes use of sample databases to prune the generated candidate motifs to lower the magnitude of added noise. Furthermore, to improve the utility of mining results, a strategy of threshold modification is designed to reduce the propagation and random sampling errors in the mining process. Extensive experiments on actual DNA databases confirm that our approach can privately find DNA motifs with high utility and efficiency. © 2021 Association for Computing Machinery.",data privacy; differential privacy; DNA computing; Motif finding,Gene expression; Privacy by design; Random errors; Back-ground knowledge; Candidate pruning; Continuous exploration; Differential privacies; Genetic research; Random sampling; Sample Database; Sensitive informations; DNA
EPRT: An Efficient Privacy-Preserving Medical Service Recommendation and Trust Discovery Scheme for eHealth System,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101671578&doi=10.1145%2f3397678&partnerID=40&md5=a8f80146014574453300f6e421da5e8a,"As one of the essential applications of health information technology, the eHealth system plays a significant role in enabling various internet medicine service scenes, most of which primarily rely on service recommendation or an evaluation mechanism. To avoid privacy leakage, some privacy-preserving mechanisms must be adopted to protect raters' privacy and make evaluation trust reliable. To tackle this challenge, this article proposes an efficient service recommendation and evaluation scheme, called EPRT, which is based on a similarity calculation and trust discovery method. This scheme uses homomorphic encryption technology to encrypt the sensitive data and combines the threshold mechanism and double-trap mechanism to realize the secure computing on the encrypted data, so as to ensure that the plaintexts of the final calculation results (e.g., recommendation value and evaluation truth) are only obtained by the authorized subject. In addition, a detailed security analysis shows that the proposed EPRT scheme can achieve the expected security. In addition, performance comparison results are carried out, demonstrating its effectiveness and accuracy. © 2021 Association for Computing Machinery.",eHealth system; Privacy preserving; service recommendation; truth discovery,Cryptography; Privacy by design; Calculation results; Health information technologies; Ho-momorphic encryptions; Performance comparison; Privacy preserving; Recommendation values; Service recommendations; Similarity calculation; eHealth
A Hybrid Feature Selection Algorithm Based on a Discrete Artificial Bee Colony for Parkinson's Diagnosis,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099891928&doi=10.1145%2f3397161&partnerID=40&md5=65c62ca6d08976793379bda2dac11949,"Parkinson's disease is a neurodegenerative disease that affects millions of people around the world and cannot be cured fundamentally. Automatic identification of early Parkinson's disease on feature data sets is one of the most challenging medical tasks today. Many features in these datasets are useless or suffering from problems like noise, which affect the learning process and increase the computational burden. To ensure the optimal classification performance, this article proposes a hybrid feature selection algorithm based on an improved discrete artificial bee colony algorithm to improve the efficiency of feature selection. The algorithm combines the advantages of filters and wrappers to eliminate most of the uncorrelated or noisy features and determine the optimal subset of features. In the filter, three different variable ranking methods are employed to pre-rank the candidate features, then the population of artificial bee colony is initialized based on the significance degree of the re-rank features. In the wrapper part, the artificial bee colony algorithm evaluates individuals (feature subsets) based on the classification accuracy of the classifier to achieve the optimal feature subset. In addition, for the first time, we introduce a strategy that can automatically select the best classifier in the search framework more quickly. By comparing with several publicly available datasets, the proposed method achieves better performance than other state-of-the-art algorithms and can extract fewer effective features. © 2021 Association for Computing Machinery.",artificial bee colony algorithm; feature extraction; machine learning; Parkinson's disease,Automation; Diagnosis; Feature extraction; Neurodegenerative diseases; Optimization; Artificial bee colonies; Artificial bee colony algorithms; Classification accuracy; Computational burden; Hybrid feature selections; Optimal classification; Parkinson's disease; State-of-the-art algorithms; Classification (of information)
DuroNet: A Dual-robust Enhanced Spatial-temporal Learning Network for Urban Crime Prediction,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101711105&doi=10.1145%2f3432249&partnerID=40&md5=d4a1d70c7115290ceca589d60ebac5c2,"Urban crime is an ongoing problem in metropolitan development and attracts general concern from the international community. As an effective means of defending urban safety, crime prediction plays a crucial role in patrol force allocation and public safety. However, urban crime data is a macro result of crime patterns overlapped by various irrelevant factors that cause inhomogeneous noises-local outliers and irregular waves. These noises might obstruct the learning process of crime prediction models and result in a deviation of performance. To tackle the problem, we propose a novel paradigm of <underline>Du</underline>al-<underline>ro</underline>bust Enhanced Spatial-temporal Learning <underline>Net</underline>work (DuroNet), an encoder-decoder architecture that possesses an adaptive robustness for reducing the effect of outliers and waves. The robustness is mainly reflected on two aspects. One is a locality enhanced module that employs local temporal context information to smooth the deviation of outliers and dynamic spatial information to assist in understanding normal points. The other is a self-attention-based pattern representation module to weaken the effect of irregular waves by learning attentive weights. Finally, extensive experiments are conducted on two real-world crime datasets before and after adding Gaussian noises. The results demonstrate the superior performance of our DuroNet over the state-of-the-art methods. © 2021 ACM.",Crime prediction; noise data; representation learning; robust neural network; spatial-temporal correlation,Crime; Forecasting; Gaussian noise (electronic); Predictive analytics; Statistics; Encoder-decoder architecture; Force allocation; International community; Local temporal contexts; Pattern representation; Prediction model; Spatial temporals; State-of-the-art methods; Learning systems
Privacy-preserving Federated Deep Learning for Wearable IoT-based Biomedical Monitoring,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101696545&doi=10.1145%2f3428152&partnerID=40&md5=a78ca3284d5787bf4defd3f07dbc1080,"IoT devices generate massive amounts of biomedical data with increased digitalization and development of the state-of-the-art automated clinical data collection systems. When combined with advanced machine learning algorithms, the big data could be useful to improve the health systems for decision-making, diagnosis, and treatment. Mental healthcare is also attracting attention, since most medical problems can be associated with mental states. Affective computing is among the emerging biomedical informatics fields for automatically monitoring a person's mental state in ambulatory environments by using physiological and physical signals. However, although affective computing applications are promising to improve our daily lives, before analyzing physiological signals, privacy issues and concerns need to be dealt with. Federated learning is a promising candidate for developing high-performance models while preserving the privacy of individuals. It is a privacy protection solution that stores model parameters instead of the data itself and abides by the data protection laws such as EU General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA). We applied federated learning to heart activity data collected with smart bands for stress-level monitoring in different events. We achieved encouraging results for using federated learning in IoT-based wearable biomedical monitoring systems by preserving the privacy of the data. © 2021 ACM.",affective computing; data protection; deep learning; federated learning; PPG; Privacy-preserving; smartwatch; stress detection,Behavioral research; Consumer protection; Decision making; Deep learning; Diagnosis; Learning algorithms; Learning systems; Medical problems; Physiology; Privacy by design; Wearable technology; Affective Computing; Ambulatory environments; Biomedical informatics; Biomedical monitoring; Biomedical monitoring systems; Data protection laws; General data protection regulations; Physiological signals; Internet of things
A User-Centric Mechanism for Sequentially Releasing Graph Datasets under Blowfish Privacy,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101720065&doi=10.1145%2f3431501&partnerID=40&md5=7f4ed374f95c585ac26af392dff49fab,"In this article, we present a privacy-preserving technique for user-centric multi-release graphs. Our technique consists of sequentially releasing anonymized versions of these graphs under Blowfish Privacy. To do so, we introduce a graph model that is augmented with a time dimension and sampled at discrete time steps. We show that the direct application of state-of-the-art privacy-preserving Differential Private techniques is weak against background knowledge attacker models. We present different scenarios where randomizing separate releases independently is vulnerable to correlation attacks. Our method is inspired by Differential Privacy (DP) and its extension Blowfish Privacy (BP). To validate it, we show its effectiveness as well as its utility by experimental simulations. © 2021 ACM.",Blowfish Privacy; Call detail records; differential privacy; graph anonymization; multi-release datasets,Internet; Attacker models; Back-ground knowledge; Correlation attack; Differential privacies; Experimental simulations; Privacy preserving; State of the art; Time dimension; Privacy by design
Integrity Verification Mechanism of Sensor Data Based on Bilinear Map Accumulator,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101708984&doi=10.1145%2f3380749&partnerID=40&md5=a454f3e91e274a6950ddd01479a3e671,"With the explosive growth in the number of IoT devices, ensuring the integrity of the massive data generated by these devices has become an important issue. Due to the limitation of hardware, most past data integrity verification schemes randomly select partial data blocks and then perform integrity validation on those blocks instead of examining the entire dataset. This will result in that unsampled data blocks cannot be detected even if they are tampered with. To solve this problem, we propose a new and effective integrity auditing mechanism of sensor data based on a bilinear map accumulator. Using the proposed approach will examine all the data blocks in the dataset, not just some of the data blocks, thus, eliminating the possibility of any cloud manipulation. Compared with other schemes, our proposed solution has been proved to be highly secure for all necessary security requirements, including tag forgery, data deletion, replacement, replay, and data leakage attacks. The solution reduces the computational and storage costs of cloud storage providers and verifiers, and also supports dynamic operations for data owners to insert, delete, and update data by using a tag index table (TIT). Compared with existing schemes based on RSA accumulator, our scheme has the advantages of fast verification and witness generation and no need to map data blocks to prime numbers. The new solution supports all the characteristics of a data integrity verification scheme. © 2021 ACM.",accumulator; data integrity verification; Sensor data storage,Security of data; Storage as a service (STaaS); Auditing mechanism; Cloud storages; Data integrity; Dynamic operations; Explosive growth; Integrity verifications; Security requirements; Storage costs; Digital storage
Multilateral Personal Portfolio Authentication System Based on Hyperledger Fabric,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101709276&doi=10.1145%2f3423554&partnerID=40&md5=1b4aa09e4a1db828df1fd89272f6d8ed,"Korean education-related evaluation agencies utilize a centralized system that directly manages learner data. This leaves the intellectual property of the organizations and the personal information of the students vulnerable to leakage should the central server be attacked. In this study, the researchers propose a multilateral personal portfolio authentication system that guarantees the reliability, integrity, and transparency of the data such as learner's schooling history. The system uses the features of blockchain in a distributed network wherein a learner submits schooling data to a peer in the network. The data is then verified through an agreement among the peers and recorded in a chronologically encrypted ledger. The proposed system is implemented based on Hyperledger Fabric and the analysis thereof conducted by evaluating its processing speed, capacity, and security. The analysis indicates that the system is able to successfully intercept the transactions of unvalidated users, thereby preventing the recording of incorrect data by an unauthorized user. Furthermore, it provides a record of previously made changes to a learner's profile, thus improving the integrity and reliability of the data. This system provides a platform to share learner information safely and promptly among schools, certification authorities, and higher learning institutions. © 2021 ACM.",authentication; Blockchain; e-business model; hyperledger fabric; personal portfolio,Authentication; Authentication systems; Centralized systems; Certification authorities; Distributed networks; Higher learning institutions; Learner information; Personal information; Personal portfolio; Learning systems
Phishing Scams Detection in Ethereum Transaction Network,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101685410&doi=10.1145%2f3398071&partnerID=40&md5=ee5f77fd770c286e857baa0205eb8981,"Blockchain has attracted an increasing amount of researches, and there are lots of refreshing implementations in different fields. Cryptocurrency as its representative implementation, suffers the economic loss due to phishing scams. In our work, accounts and transactions are treated as nodes and edges, thus detection of phishing accounts can be modeled as a node classification problem. Correspondingly, we propose a detecting method based on Graph Convolutional Network and autoencoder to precisely distinguish phishing accounts. Experiments on different large-scale real-world datasets from Ethereum show that our proposed model consistently performs promising results compared with related methods. © 2020 ACM.",Cryptocurrency; graph convolutional network; graph embedding; node classification; phishing detection,Computer crime; Convolutional neural networks; Large dataset; Losses; Auto encoders; Convolutional networks; Detecting methods; Economic loss; Phishing; Real-world datasets; Ethereum
Enabling User-centered Privacy Controls for Mobile Applications: COVID-19 Perspective,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101696121&doi=10.1145%2f3434777&partnerID=40&md5=b7b6efd2d6647f50a102f6c7b6e3caf4,"Mobile apps have transformed many aspects of clinical practice and are becoming a commonplace in healthcare settings. The recent COVID-19 pandemic has provided the opportunity for such apps to play an important role in reducing the spread of the virus. Several types of COVID-19 apps have enabled healthcare professionals and governments to communicate with the public regarding the pandemic spread, coronavirus awareness, and self-quarantine measures. While these apps provide immense benefits for the containment of the spread, privacy and security of these digital tracing apps are at the center of public debate. To address this gap, we conducted an online survey of a midwestern region in the United State to assess people's attitudes toward such apps and to examine their privacy and security concerns and preferences. Survey results from 1,550 participants indicate that privacy/security protections and trust play a vital role in people's adoption of such apps. Furthermore, results reflect users' preferences wanting to have control over their personal information and transparency on how their data is handled. In addition, personal data protection priorities selected by the participants were surprising and yet revealing of the disconnect between technologists and users. In this article, we present our detailed survey results as well as design guidelines for app developers to develop innovative human-centered technologies that are not only functional but also respectful of social norms and protections of civil liberties. Our study examines users' preferences for COVID-19 apps and integrates important factors of trust, willingness, and preferences in the context of app development. Through our research findings, we suggest mechanisms for designing inclusive apps' privacy and security measures that can be put into practice for healthcare-related apps, so that timely adoption is made possible. © 2021 ACM.",COVID-19; human-centered; Mobile apps; privacy 8 security; trust,Electronic assessment; Health care; Privacy by design; Viruses; Civil liberties; Clinical practices; Health care professionals; Mobile applications; Online surveys; Personal information; Privacy and security; Privacy control; Surveys
Data Privacy Based on IoT Device Behavior Control Using Blockchain,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101664692&doi=10.1145%2f3434776&partnerID=40&md5=e7e63f5f611fd33372f6f04a22bf65ea,"The Internet of Things (IoT) is expected to improve the individuals' quality of life. However, ensuring security and privacy in the IoT context is a non-trivial task due to the low capability of these connected devices. Generally, the IoT device management is based on a centralized entity that validates communication and connection rights. Therefore, this centralized entity can be considered as a single point of failure. Yet, in the case of distributed approaches, it is difficult to delegate the right validation to IoT devices themselves in untrustworthy IoT environments. Fortunately, the blockchain may provide decentralization of overcoming the trust problem while designing a privacy-preserving system. To this end, we propose a novel privacy-preserving IoT device management framework based on the blockchain technology. In the proposed system, the IoT devices are controlled by several smart contracts that validate the connection rights according to the privacy permission settings predefined by the data owners and the stored record array of detected misbehavior of each IoT device. In fact, smart contracts can immediately detect the devices that have vulnerabilities and have been hacked or pose a threat to the IoT network. Therefore, the data owner's privacy is preserved by enforcing the control over the own devices. For validation purposes, we deploy the proposed solution on a private Ethereum blockchain and give the performance evaluation. © 2021 ACM.",Behavior control; blockchain; Internet of Things; smart contract,Blockchain; Privacy by design; Behavior control; Device management; Distributed approaches; Internet of thing (IOT); Non-trivial tasks; Privacy preserving; Quality of life; Security and privacy; Internet of things
Privacy Care: A Tangible Interaction Framework for Privacy Management,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101663516&doi=10.1145%2f3430506&partnerID=40&md5=3877e082a6b49d53760ec90709b6b6a1,"The emergence of ubiquitous computing (UbiComp) environments has increased the risk of undesired access to individuals' physical space or their information, anytime and anywhere, raising potentially serious privacy concerns. Individuals lack awareness and control of the vulnerabilities in everyday contexts and need support and care in regulating disclosures to their physical and digital selves. Existing GUI-based solutions, however, often feel physically interruptive, socially disruptive, time-consuming and cumbersome. To address such challenges, we investigate the user interaction experience and discuss the need for more tangible and embodied interactions for effective and seamless natural privacy management in everyday UbiComp settings. We propose the Privacy Care interaction framework, which is rooted in the literature of privacy management and tangible computing. Keeping users at the center, Awareness and Control are established as the core parts of our framework. This is supported with three interrelated interaction tenets: Direct, Ready-to-Hand, and Contextual. Direct refers to intuitiveness through metaphor usage. Ready-to-Hand supports granularity, non-intrusiveness, and ad hoc management, through periphery-to-center style attention transitions. Contextual supports customization through modularity and configurability. Together, they aim to provide experience of an embodied privacy care with varied interactions that are calming and yet actively empowering. The framework provides designers of such care with a basis to refer to, to generate effective tangible tools for privacy management in everyday settings. Through five semi-structured focus groups, we explore the privacy challenges faced by a sample set of 15 older adults (aged 60+) across their cyber-physical-social spaces. The results show conformity to our framework, demonstrating the relevance of the facets of the framework to the design of privacy management tools in everyday UbiComp contexts. © 2021 ACM.",privacy care framework; privacy management; tangibility for privacy; Ubiquitous computing,Ubiquitous computing; User experience; Contextual support; Embodied interaction; Interaction framework; Privacy concerns; Privacy management; Tangible computing; Tangible interaction; User interaction; Privacy by design
DAN-SNR: A Deep Attentive Network for Social-aware Next Point-of-interest Recommendation,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101715711&doi=10.1145%2f3430504&partnerID=40&md5=ebe6b0f0855c836708da0ea3bc514c34,"Next (or successive) point-of-interest (POI) recommendation, which aims to predict where users are likely to go next, has recently emerged as a new research focus of POI recommendation. Most of the previous studies on next POI recommendation attempted to incorporate the spatiotemporal information and sequential patterns of user check-ins into recommendation models to predict the target user's next move. However, few of the next POI recommendation approaches utilized the social influence of each user's friends. In this study, we discuss a new topic of next POI recommendation and present a deep attentive network for social-aware next POI recommendation called DAN-SNR. In particular, the DAN-SNR makes use of the self-attention mechanism instead of the architecture of recurrent neural networks to model sequential influence and social influence in a unified manner. Moreover, we design and implement two parallel channels to capture short-term user preference and long-term user preference as well as social influence, respectively. By leveraging multi-head self-attention, the DAN-SNR can model long-range dependencies between any two historical check-ins efficiently and weigh their contributions to the next destination adaptively. We also carried out a comprehensive evaluation using large-scale real-world datasets collected from two popular location-based social networks, namely, Gowalla and Brightkite. Experimental results indicate that the DAN-SNR outperforms seven competitive baseline approaches regarding recommendation performance and is highly efficient among six neural-network-based methods, four of which utilize the attention mechanism. © 2020 ACM.",embedding; location-based service; Next point-of-interest recommendation; self-attention; social influence,Economic and social effects; Large dataset; Signal to noise ratio; Attention mechanisms; Comprehensive evaluation; Design and implements; Location-based social networks; Long-range dependencies; Recommendation performance; Sequential patterns; Spatiotemporal information; Recurrent neural networks
PROLISEAN: A New Security Protocol for Programmable Matter,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101677731&doi=10.1145%2f3432250&partnerID=40&md5=225da53217efd5b2579b2af4983d1a11,"The vision for programmable matter is to create a material that can be reprogrammed to have different shapes and to change its physical properties on demand. They are autonomous systems composed of a huge number of independent connected elements called particles. The connections to one another form the overall shape of the system. These particles are capable of interacting with each other and take decisions based on their environment. Beyond sensing, processing, and communication capabilities, programmable matter includes actuation and motion capabilities. It could be deployed in different domains and will constitute an intelligent component of the IoT. A lot of applications can derive from this technology, such as medical or industrial applications. However, just like any other technology, security is a huge concern. Given its distributed architecture and its processing limitations, programmable matter cannot handle the traditional security protocols and encryption algorithms. This article proposes a new security protocol optimized and dedicated for IoT programmable matter. This protocol is based on lightweight cryptography and uses the same encryption protocol as a hashing function while keeping the distributed architecture in mind. The analysis and simulation results show the efficiency of the proposed method and that a supercomputer will need about 5.93 × 1025 years to decrypt the message. © 2021 ACM.",amoebots; distributed computing; IOT; lightweight cryptography; Modular robots; programmable matter; security algorithms; security protocol,Cryptography; Internet protocols; Network architecture; Network security; Supercomputers; Analysis and simulation; Communication capabilities; Distributed architecture; Encryption algorithms; Encryption protocols; Intelligent components; Light-weight cryptography; Programmable matter; Internet of things
Intelligent Mediator-based Enhanced Smart Contract for Privacy Protection,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101704286&doi=10.1145%2f3404892&partnerID=40&md5=9cb22d16d2779f8ba4a0a282b2a12668,"With the development of ICT technology, users are actively participated in content creation and sharing. Service providers have provided more diverse and extensive information services. With the recent evolution to a customized service, the demand for the use of personal information is increasing. However, it affects the efficiency and convenience of service provision, as social problems and security threats such as personal information leakage, human trafficking, and misuse increase. In this article, we proposed an intelligent mediator-based enhanced smart contract for privacy protection. The proposed approach performs the tasks, which are mediation transactions, authorization, and transaction record management, with blockchain-based personal information management. Then it is possible to prevent misuse of personal information and to support rational decision-making on the transparency of information and the use of personal information by autonomously performing personal information management. © 2021 ACM.",Blockchain; Information Management; Information Sharing; Personal Information; Platform,Data privacy; Information management; Information services; Customized services; Human trafficking; Personal information; Personal information management; Privacy protection; Rational decision making; Service provisions; Transaction records; Decision making
RTChain: A Reputation System with Transaction and Consensus Incentives for E-commerce Blockchain,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101717222&doi=10.1145%2f3430502&partnerID=40&md5=13b1a847d0f3fc631e762494f4fca694,"Blockchain technology, whose most successful application is Bitcoin, enables non-repudiation and non-tamperable online transactions without the participation of a trusted central party. As a global ledger, the blockchain achieves the consistency of replica stored on each node through a consensus mechanism. A well-designed consensus mechanism, on one hand, needs to be efficient to meet the high frequency of online transactions. For example, the existing electronic payment systems can handle over 50,000 transactions per second (TPS), while Bitcoin can only handle an average of about 3TPS. On the other hand, it needs to have good security and high fault tolerance; that is, in the case when some nodes are captured by adversaries, the network can still operate normally. In this article, we establish a reputation system, called RTChain, to be integrated into the e-commerce blockchain to achieve a distributed consensus and transaction incentives. The proposed scheme has the following advantages. First, an incentive mechanism is used to influence the consensus behavior of nodes and the transaction behavior of users, which in turn influence the reputation scores of both nodes and users. That is, when a node correctly processes a transaction, it will receive the corresponding reputation value as a reward, and the reputation value will be reduced as punishment not only when the node is dishonest and violates the consensus agreement but also the transaction is not completed as required. Just like electronic transactions in the real world, the higher the reputation of the user, the more likely it is to be selected as the transaction partner. A user with a low reputation will be gradually eliminated in our system because it is difficult to complete the transaction. Second, RTChain uses a verifiable random function to generate the leader in each round, which guarantees fairness for all participants and, unlike PoW, does not consume a large amount of computing resources. Then our consensus mechanism selects the nodes with high reputation scores to reduce the number of nodes participating in the consensus, thus improving the consensus efficiency, so that RTChain's throughput can reach 4,000TPS. Third, we built a reputation chain to implement the distributed storage and management of reputation. Finally, our consensus mechanism is secure against existing attacks, such as flash attacks, selfish mining attacks, eclipse attacks, and double spending attacks, and allows nodes that participate in the consensus to fail, as long as the reputation of the failure node does not exceed one-third of the total reputation. We build a prototype of RTChain, and the experimental results show that RTChain is promising and deployable for e-commerce blockchains. © 2020 ACM.",Blockchain; consensus mechanism; e-commerce; PBFT; reputation system,Binary alloys; Bitcoin; Fault tolerance; Polonium alloys; Tungsten alloys; Distributed consensus; Distributed storage; Electronic payment systems; Electronic transaction; High fault tolerances; Incentive mechanism; Online transaction; Verifiable random function; Blockchain
Blockchain in eCommerce: A Special Issue of the ACM Transactions on Internet of Things,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101717099&doi=10.1145%2f3445788&partnerID=40&md5=a54d3d8f358c20206785b12ed373975c,"As blockchain technology is becoming a driving force in the global economy, it is also gaining critical acclaim in the e-commerce industry. Both the blockchain and e-commerce are inseparable as they involve transactions. Blockchain protect transactions and e-commerce activities rely on them. Blockchain technology enables a decentralized marketplace to support important business activities like secure payments, managing the supply chain and reducing the fraud to mention few. In this special issue editorial we are introducing 11 research articles in this hot area of research that were selected by our reviewers from over than 250 submissions. As blockchain technology is becoming a driving force in the global economy, it is also gaining critical acclaim in the e-commerce industry. Both the blockchain and e-commerce are inseparable as they involve transactions. Blockchain protect transactions and e-commerce activities rely on them. Blockchain technology enables a decentralized marketplace to support important business activities like secure payments, managing the supply chain and reducing the fraud to mention few. In this special issue editorial we are introducing 11 research articles in this hot area of research that were selected by our reviewers from over than 250 submissions. © 2021 ACM.",B2B and B2C applications; Blockchain; business protection; distributed marketplace; eCommerce services; supply chain,Crime; Electronic commerce; Internet of things; Supply chains; Business activities; Driving forces; Global economies; Hot areas; Blockchain
Fine-Grained Network Analysis for Modern Software Ecosystems,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101661873&doi=10.1145%2f3418209&partnerID=40&md5=d904ebaef4ef0a4fc45e7ba6f92ac155,"Modern software development is increasingly dependent on components, libraries, and frameworks coming from third-party vendors or open-source suppliers and made available through a number of platforms (or forges). This way of writing software puts an emphasis on reuse and on composition, commoditizing the services that modern applications require. On the other hand, bugs and vulnerabilities in a single library living in one such ecosystem can affect, directly or by transitivity, a huge number of other libraries and applications. Currently, only product-level information on library dependencies is used to contain this kind of danger, but this knowledge often reveals itself too imprecise to lead to effective (and possibly automated) handling policies. We will discuss how fine-grained function-level dependencies can greatly improve reliability and reduce the impact of vulnerabilities on the whole software ecosystem. © 2020 ACM.",network analysis; security breaches; Software reuse,Application programs; Computer software reusability; Libraries; Open source software; Open systems; Program debugging; Software design; Software reliability; Fine grained; Function levels; Modern applications; Open sources; Software ecosystems; Third party vendors; Ecosystems
Incentive-Driven Computation Offloading in Blockchain-Enabled E-Commerce,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101710460&doi=10.1145%2f3397160&partnerID=40&md5=34d7a168418c547b1b5f094ea8706de9,"Blockchain is regarded as one of the most promising technologies to upgrade e-commerce. This article analyzes the challenges that current e-commerce is facing and introduces a new scenario of e-commerce enabled by blockchain. A framework is proposed for mining tasks in this scenario offloaded onto edge servers based on mobile edge computing. Then, the offloading issue is modeled as a multi-constrained optimization problem, and evolutionary algorithms are utilized and re-designed as solvers. The experimental results validate the efficiency of the framework and algorithms and also show that the lower bound of computation resources exists to obtain the maximum overall revenue. © 2020 ACM.",Blockchain; computation offloading; e-commerce; edge computing,Blockchain; Constrained optimization; Electronic commerce; Evolutionary algorithms; Computation offloading; Computation resources; Constrained optimi-zation problems; Edge server; Framework and algorithms; Lower bounds; Mining tasks; Computational efficiency
Joint Encryption and Compression-Based Watermarking Technique for Security of Digital Documents,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101681938&doi=10.1145%2f3414474&partnerID=40&md5=9efecd9e277953842c6b3a95268b0667,"Recently, due to the increase in popularity of the Internet, the problem of digital data security over the Internet is increasing at a phenomenal rate. Watermarking is used for various notable applications to secure digital data from unauthorized individuals. To achieve this, in this article, we propose a joint encryption then-compression based watermarking technique for digital document security. This technique offers a tool for confidentiality, copyright protection, and strong compression performance of the system. The proposed method involves three major steps as follows: (1) embedding of multiple watermarks through non-sub-sampled contourlet transform, redundant discrete wavelet transform, and singular value decomposition; (2) encryption and compression via SHA-256 and Lempel Ziv Welch (LZW), respectively; and (3) extraction/recovery of multiple watermarks from the possibly distorted cover image. The performance estimations are carried out on various images at different attacks, and the efficiency of the system is determined in terms of peak signal-to-noise ratio (PSNR) and normalized correlation (NC), structural similarity index measure (SSIM), number of changing pixel rate (NPCR), unified averaged changed intensity (UACI), and compression ratio (CR). Furthermore, the comparative analysis of the proposed system with similar schemes indicates its superiority to them. © 2021 ACM.",compression; encryption; LWZ; NSCT; SVD; Watermarking,Copyrights; Discrete wavelet transforms; Security of data; Signal to noise ratio; Singular value decomposition; Watermarking; Wavelet decomposition; Compression performance; Non-sub-sampled contourlet transforms; Normalized correlation; Peak signal to noise ratio; Performance estimation; Redundant discrete wavelet transform; Structural similarity index measures (SSIM); Watermarking algorithms; Cryptography
Efficient Distributed Decryption Scheme for IoT Gateway-based Applications,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101703369&doi=10.1145%2f3414475&partnerID=40&md5=915b7c2efce429e0bc35f34ec636f8a6,"With the evolvement of the Internet of things (IoT), privacy and security have become the primary indicators for users to deploy IoT applications. In the gateway-based IoT architecture, gateways aggregate data collected by perception-layer devices and upload message packets to platforms, while platforms automatically push different categories of data to different applications. However, security in processes of data transmission via gateways, storage in platforms, access by applications is the major challenge for user privacy protection. To tackle this challenge, this article presents a secure IoT scheme based on a fine-grained multi-receive signcryption scheme to realize end-to-end secure transmission and data access control. To enhance the security of online application decryption keys, we design a distributed threshold decryption scheme based on secret-sharing. Moreover, from the provable security perspective, we demonstrate that the scheme can achieve the expected IND-CCA security and EUF-CMA security. After the performance analysis, evaluation results show that the computational performance is efficient and linearly subject to the number of messages and the number of receivers. © 2021 ACM.",data sharing; distributed decryption; IoT gateway; multi-receiver signcryption,Access control; Cryptography; Digital storage; Gateways (computer networks); Privacy by design; Transmissions; Computational performance; Data access control; Internet of thing (IOT); On-line applications; Performance analysis; Privacy and security; Signcryption schemes; Threshold decryption; Internet of things
Account Guarantee Scheme: Making Anonymous Accounts Supervised in Blockchain,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101689816&doi=10.1145%2f3406092&partnerID=40&md5=bb09bbdc055647a26b73f0702857fcee,"In blockchain networks, reaching effective supervision while maintaining anonymity to the public has been an ongoing challenge. In existing solutions, certification authorities need to record all pairs of identities and pseudonyms, which is demanding and costly. This article proposed an account guarantee scheme to realize feasible supervision for existing anonymous blockchain networks with lower storage costs. Users are able to guarantee anonymous accounts with account guarantee key pairs generated from certificated polynomial functions, which inherently maintains one-to-n mapping certifications. Single or limited account guarantee key pairs do not leak privacy. Victims are able to request TCs to screen a cheater or disclose a cheater with enough fraud transactions by themselves. Detailed security and privacy analysis showed that the account guarantee scheme preserves user privacy and realizes feasible supervision. Experimental results demonstrated that the account guarantee scheme is efficient and practical. © 2021 ACM.",anonymity; Blockchain; polynomial function; supervision,Internet; Certification authorities; Key pairs; Polynomial functions; Security and privacy analysis; Storage costs; User privacy; Blockchain
A Supply-chain System Framework Based on Internet of Things Using Blockchain Technology,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101675628&doi=10.1145%2f3409798&partnerID=40&md5=5a0937e481046246cd9f0f8e3a271aaa,"Numerous supply-chain combines with internet of things (IoT) applications have been proposed, and many methods and algorithms enhance the convenience of supply chains. However, new businesses still find it challenging to enter a supply chain, because unauthorised IoT devices of different companies illegally access resources. As security is paramount in a supply chain, IoT management has become very difficult. Public resources allocation and waste management also pose a problem. To solve the above problems, we proposed a new IoT management framework that embraces blockchain technology to help companies to form a supply chain effectively. This framework consists of an access control system, a backup peer mechanism and an internal data isolation and transmission approach. The access control system has a registrar module and an inspection module. The registrar module is mainly responsible for information registration with a registration policy, which has to be followed by all the companies in the supply chain. Besides, it provides a revocation and updating function. The inspection module focuses on judging misbehaviour and monitors the actions of the subjects; when any misoperation occurs, the system will correspondingly penalise violators. So that all related actions and information are verified and stored into blockchain, the IoT access control and safety of IoT admission are enhanced. Furthermore, in a blockchain system, if one single peer in the network breaks down, then the whole system may stop, because consensus cannot be reached. The data of the broken peer may be lost if it does not commit yet. The backup peer mechanism allows the primary peer and the backup peer to connect to an inspecting server for acquiring real-time data. The internal data isolation and transmission modules transmit and stores private data without creating a new subchannel. The proposed method is taken full account of the stability of the network and the fault tolerance to guarantee the robust of the system. To obtain unbiases results, experiments are conducted in two different blockchain environment. The results show our proposed method are promising IoT blockchain system for the supply chain. © 2021 ACM.",Blockchain; internet of things; supply chain,Access control; Blockchain; Control systems; Fault tolerance; Inspection; Supply chain management; Supply chains; Waste management; Internet of Things (IOT); Management frameworks; Misbehaviour; Private data; Public resources; Real-time data; Supply chain systems; Transmission modules; Internet of things
"Introduction to the Special Section on Human-centered Security, Privacy, and Trust in the Internet of Things",2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101690739&doi=10.1145%2f3445790&partnerID=40&md5=b73c7937285899f1d923624af672aa73,[No abstract available],,
Beyond Frequency: Utility Mining with Varied Item-specific Minimum Utility,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101687601&doi=10.1145%2f3425498&partnerID=40&md5=7bd06f23f54965234ea5c1d2dd9a3dd7,"Consumer behavior plays a very important role in economics and targeted marketing. However, understanding economic consumer behavior is quite challenging, such as finding credible and reliable information on product profitability. Different from frequent pattern mining, utility-oriented mining integrates utility theory and data mining. Utility mining is a useful tool for understanding economic consumer behavior. Traditional algorithms for mining high-utility patterns (HUPs) applies a single/uniform minimum utility threshold (minutil) to obtain the set of HUPs, but in some real-life circumstances, some specific products may bring lower utilities compared with others, but their profit may offer some vital information. If minutil is set high, the patterns with low minutil are missed; if minutil is set low, the number of patterns becomes unmanageable. In this article, an efficient one-phase utility-oriented pattern mining algorithm, called HIMU, is proposed for mining HUPs with varied item-specific minimum utility. A novel tree structure called a multiple item utility set-enumeration tree (MIU-tree) and the global sorted and the conditional downward closure properties are introduced in HIMU. In addition, we extended the compact utility-list structure to keep the necessary information, and thus this one-phase HIMU model greatly reduces the computational costs and memory requirements. Moreover, two pruning strategies are then extended to enhance the performance. We conducted extensive experiments in several synthetic and real-world datasets; the results indicate that the designed one-phase HIMU algorithm can address the ""rare item problem""and has better performance than the state-of-the-art algorithms in terms of runtime, memory usage, and scalability. Furthermore, the enhanced algorithms outperform the non-optimized HIMU approach. © 2021 ACM.",Economic behavior; pruning strategies; rare item problem; set-enumeration tree; utility mining,Computation theory; Consumer behavior; Forestry; Profitability; Trees (mathematics); Computational costs; Downward closure properties; Frequent pattern mining; Memory requirements; Pattern mining algorithms; Real-world datasets; Set enumeration tree; State-of-the-art algorithms; Data mining
Achieving Secure Search over Encrypted Data for e-Commerce,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101673617&doi=10.1145%2f3408309&partnerID=40&md5=7c3f5abb18fc70db483006cf2bf82720,"The advances of Internet technology has resulted in the rapid and pervasive development of e-commerce, which has not only changed the production and operation mode of many enterprises, but also affected the economic development mode of the whole society. This trend has incurred a strong need to store and process large amounts of sensitive data. The traditional data storage and search solutions cannot meet such requirements. To tackle this problem, in this article, we proposed Consortium Blockchain-based Distributed Secure Search (CBDSS) Scheme over encrypted data in e-Commerce environment. By integrating the blockchain and searchable encryption model, sensitive data can be effectively protected. The consortium blockchain can ensure that only authorized nodes can join the system. To fairly assign nodes for the search tasks, we developed an endorsement strategy in which two agent roles are set up to divide and match the search tasks with the virtual resources according to the load capacity of each node. The security analysis and experiments are conducted to evaluate the performance of our proposed scheme. The evaluation results have proved the reliability and security of our scheme over existing methods. © 2020 ACM.",Blockchain; searchable encryption; smart contract; Top-k search,Blockchain; Cryptography; Data privacy; Digital storage; Electronic commerce; Development modes; Encrypted data; Evaluation results; Internet technology; Searchable encryptions; Security analysis; Sensitive datas; Virtual resource; Economic and social effects
The Cloud-edge-based Dynamic Reconfiguration to Service Workflow for Mobile Ecommerce Environments,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101720429&doi=10.1145%2f3391198&partnerID=40&md5=46ad73a30b7b1df59c90ccf5a693d615,"The emergence of mobile service composition meets the current needs for real-time eCommerce. However, the requirements for eCommerce, such as safety and timeliness, are becoming increasingly strict. Thus, the cloud-edge hybrid computing model has been introduced to accelerate information processing, especially in a mobile scenario. However, the mobile environment is characterized by limited resource storage and users who frequently move, and these characteristics strongly affect the reliability of service composition running in this environment. Consequently, applications are likely to fail if inappropriate services are invoked. To ensure that the composite service can operate normally, traditional dynamic reconfiguration methods tend to focus on cloud services scheduling. Unfortunately, most of these approaches cannot support timely responses to dynamic changes. In this article, the cloud-edge based dynamic reconfiguration to service workflow for mobile eCommerce environments is proposed. First, the service quality concept is extended. Specifically, the value and cost attributes of a service are considered. The value attribute is used to assess the stability of the service for some time to come, and the cost attribute is the cost of a service invocation. Second, a long short-term memory (LSTM) neural network is used to predict the stability of services, which is related to the calculation of the value attribute. Then, in view of the limited available equipment resources, a method for calculating the cost of calling a service is introduced. Third, candidate services are selected by considering both service stability and the cost of service invocation, thus yielding a dynamic reconfiguration scheme that is more suitable for the cloud-edge environment. Finally, a series of comparative experiments were carried out, and the experimental results prove that the method proposed in this article offers higher stability, less energy consumption, and more accurate service prediction. © 2021 ACM.",Cloud-edge environment; dynamic reconfiguration; eCommerce and its dependability; mobile service composition; quantitative evaluation; service quality prediction,Dynamic models; Electronic commerce; Energy utilization; Long short-term memory; Quality of service; Stability; Comparative experiments; Composite services; Dynamic re-configuration; Dynamic reconfiguration scheme; Mobile environments; Mobile service composition; Service compositions; Service invocation; Storage as a service (STaaS)
Social-Chain: Decentralized Trust Evaluation Based on Blockchain in Pervasive Social Networking,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099520217&doi=10.1145%2f3419102&partnerID=40&md5=5247882b6e6b8ce72134d94ca996e667,"Pervasive Social Networking (PSN) supports online and instant social activities with the support of heterogeneous networks. Since reciprocal activities among both familiar/unfamiliar strangers and acquaintances are quite common in PSN, it is essential to offer trust information to PSN users. Past work normally evaluates trust based on a centralized party, which is not feasible due to the dynamic changes of PSN topology and its specific characteristics. The literature still lacks a decentralized trust evaluation scheme in PSN. In this article, we propose a novel blockchain-based decentralized system for trust evaluation in PSN, called Social-Chain. Considering mobile devices normally lack computing resources to process cryptographic puzzle calculation, we design a lightweight consensus mechanism based on Proof-of-Trust (PoT), which remarkably improves system effectivity compared with other blockchain systems. Serious security analysis and experimental results further illustrate the security and efficiency of Social-Chain for being feasibly applied into PSN. © 2021 ACM.",blockchain; consensus mechanism; pervasive social networking; Trust evaluation,Heterogeneous networks; Network security; Social networking (online); Computing resource; Decentralized system; Dynamic changes; Mechanism-based; Pervasive social networkings; Security analysis; Social activities; Trust evaluation; Blockchain
Concurrent Practical Byzantine Fault Tolerance for Integration of Blockchain and Supply Chain,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092140319&doi=10.1145%2f3395331&partnerID=40&md5=aef3b3f1472fe5fbaef3af35faab3c10,"Currently, the integration of the supply chain and blockchain is promising, as blockchain successfully eliminates the bullwhip effect in the supply chain. Generally, concurrent Practical Byzantine Fault Tolerance (PBFT) consensus method, named C-PBFT, is powerful to deal with the consensus inefficiencies, caused by the fast node expansion in the supply chain. However, due to the tremendous complicated transactions in the supply chain, it remains challenging to select the credible primary peers in the concurrent clusters. To address this challenge, the peers in the supply chain are classified into several clusters by analyzing the historic transactions in the ledger. Then, the primary peer for each cluster is identified by reputation assessment. Finally, the performance of C-PBFT is evaluated by conducting experiments in Fabric. © 2021 ACM.",Blockchain; C-PBFT; reputation assessment; supply chain,Blockchain; Fault tolerance; Bullwhip effects; Byzantine fault tolerance; Consensus methods; Reputation assessments; Supply chains
Introduction to the Special Section on Data Science for Cyber-Physical Systems,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114290486&doi=10.1145%2f3464766&partnerID=40&md5=22813bb0631c21d7b95ba135f1e0e818,"The theme section of the special issue of the ACM Transactions on Internet Technology journal is titled as ‘Data Science for Cyber-Physical Systems (CPS)’. This special section aims to attract high-quality research and survey articles that promote research and reflect the most recent advances in addressing data science methodologies and applications for CPS. This edition of the special section is focused primarily on data science for CPS. This issue is intended to provide a highly recognized international forum to present recent advances in ACM Transactions on Internet Technology. CPS, as a multidimensional complex system that connects the physical world and the cyber world, has a strong demand for processing large amounts of heterogeneous data. These tasks also include natural language inference (NLI) tasks based on text from different sources.",,Cyber Physical System; Embedded systems; Cyber-physical systems (CPS); Heterogeneous data; High-quality research; Internet technology; Natural languages; Physical world; Science methodologies; Special sections; Data Science
A Hybrid Siamese Neural Network for Natural Language Inference in Cyber-Physical Systems,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114284006&doi=10.1145%2f3418208&partnerID=40&md5=94e966aff336cc12e75ab6702be4232d,"Cyber-Physical Systems (CPS), as a multi-dimensional complex system that connects the physical world and the cyber world, has a strong demand for processing large amounts of heterogeneous data. These tasks also include Natural Language Inference (NLI) tasks based on text from different sources. However, the current research on natural language processing in CPS does not involve exploration in this field. Therefore, this study proposes a Siamese Network structure that combines Stacked Residual Long Short-Term Memory (bidirectional) with the Attention mechanism and Capsule Network for the NLI module in CPS, which is used to infer the relationship between text/language data from different sources. This model is mainly used to implement NLI tasks and conduct a detailed evaluation in three main NLI benchmarks as the basic semantic understanding module in CPS. Comparative experiments prove that the proposed method achieves competitive performance, has a certain generalization ability, and can balance the performance and the number of trained parameters. © 2021 Association for Computing Machinery.",Cyber-physical systems; Natural language inference; Siamese neural networks,Cyber Physical System; Embedded systems; Neural networks; Semantics; Attention mechanisms; Comparative experiments; Competitive performance; Cyber-physical systems (CPS); Generalization ability; NAtural language processing; Network structures; Semantic understanding; Natural language processing systems
Improving Vaccine Safety Using Blockchain,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114272952&doi=10.1145%2f3388446&partnerID=40&md5=b20829621903f7c70eb8a7bd7119964a,"In recent years, vaccine incidents occurred around the world, which endangers people's lives. In the technical respect, these incidents are partially due to the fact that existing vaccine management systems are distributively managed by different entities in the vaccine supply chain. This architecture makes it relatively easy to modify or even delete the vaccine circulation data maliciously, which makes tracing problematic vaccine hard and identifying the responsibility for a vaccine accident hard. To solve these issues, this article presents a blockchain-based solution to protect the whole process of vaccine circulation. We first propose a model to supervise the vaccine circulation process by incorporating existing regulatory practices. Then, we propose a blockchain-based tracing system to implement this model. The proposed system takes the blockchain as a global, unique, and verifiable database to store all the circulation data. Through data insertions and queries on the global and unique database, the proposed system achieves the protection of vaccine circulation. We also implement a proof-of-concept prototype of the proposed system. Experimental results confirm that the proposed system is beneficial. © 2021 Association for Computing Machinery.",blockchain; smart contract; supply chain; Vaccine safety,Blockchain; Query languages; Supply chains; Circulation datum; Data insertion; Management systems; Proof of concept; Vaccine safety; Vaccine supplies; Whole process; Vaccines
Unsupervised Derivation of Keyword Summary for Short Texts,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114272791&doi=10.1145%2f3397162&partnerID=40&md5=0c5f67caa69b68f670fd31ddc9297037,"Automatically summarizing a group of short texts that mainly share one topic is a fundamental task in many applications, e.g., summarizing the main symptoms for a disease based on a group of medical texts that are usually short, i.e., tens of words. Conventional unsupervised short text summarization techniques tend to find the most representative short text document. However, they may cause privacy issues, e.g., personal information in the medical texts may be exposed. Moreover, compared with the complete short text where some unimportant words may exist, a summary consisting of only a few keywords is more preferable by the user due to its clear and concise form. Due to the above reasons, in this article, we aim to solve the problem of unsupervised derivation of keyword summary for short texts. Existing keyword extraction methods such as Latent Dirichlet Allocation cannot be applied to solve this problem, since (1) the ordering relations among the extracted keywords are ignored, which causes troubles for people to capture the main idea of the event, and (2) short texts contain limited context, which makes it hard to find the optimal words for semantic coverage. Hence, we propose a simple but yet effective method named Frequent Closed Wordsets Ranking (FCWRank) to derive the keyword summary from a short text cluster. FCWRank is an unsupervised method that builds on the idea of frequent closed itemset mining in transaction database. FCWRank first mines all frequent closed wordsets from a cluster of short texts and then selects the most important wordset based on an importance model where the similarity between closed wordsets and the relation between the closed wordset and the short text document are considered simultaneously. To make the keywords within the wordset more understandable, FCWRank further unfolds the semantics behind them by sorting them. Experiments on real-world short text collections show that FCWRank outperforms the state-of-the-art baselines in terms of Recall-Oriented Understudy for Gisting Evaluation-Longest common subsequence F1, precision and recall scores. © 2021 Association for Computing Machinery.",Keyword summary; short texts; unsupervised,Data privacy; Semantics; Statistics; Frequent closed itemset mining; Latent Dirichlet allocation; Longest common subsequences; Personal information; Precision and recall; Short-text documents; Transaction database; Unsupervised method; Text processing
Operating Systems for Resource-adaptive Intelligent Software: Challenges and Opportunities,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114280626&doi=10.1145%2f3425866&partnerID=40&md5=67eb6c09c9d94851ee319c29f83ab3e1,"The past decades witnessed the fast and wide deployment of Internet. The Internet has bred the ubiquitous computing environment that is spanning the cloud, edge, mobile devices, and IoT. Software running over such a ubiquitous computing environment environment is eating the world. A recently emerging trend of Internet-based software systems is ""resource adaptive,""i.e., software systems should be robust and intelligent enough to the changes of heterogeneous resources, both physical and logical, provided by their running environment. To keep pace of such a trend, we argue that some considerations should be taken into account for the future operating system design and implementation. From the structural perspective, rather than the ""monolithic OS""that manages the aggregated resources on the single machine, the OS should be dynamically composed over the distributed resources and flexibly adapt to the resource and environment changes. Meanwhile, the OS should leverage advanced machine/deep learning techniques to derive configurations and policies and automatically learn to tune itself and schedule resources. This article envisions our recent thinking of the new OS abstraction, namely, ServiceOS, for future resource-adaptive intelligent software systems. The idea of ServiceOS is inspired by the delivery model of ""Software-as-a-Service""that is supported by the Service-Oriented Architecture (SOA). The key principle of ServiceOS is based on resource disaggregation, resource provisioning as a service, and learning-based resource scheduling and allocation. The major goal of this article is not providing an immediately deployable OS. Instead, we aim to summarize the challenges and potentially promising opportunities and try to provide some practical implications for researchers and practitioners. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",machine learning; Operating systems; resource disaggregation; service-oriented,Computer software; Information services; Service oriented architecture (SOA); Software as a service (SaaS); Ubiquitous computing; Distributed resources; Heterogeneous resources; Intelligent software; Intelligent software systems; Operating system design; Resource and environment; Resource scheduling and allocations; Ubiquitous computing environment; Learning systems
δRisk: Toward Context-aware Multi-objective Privacy Management in Connected Environments,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114273376&doi=10.1145%2f3418499&partnerID=40&md5=cd28526d0402476b767bffde359fd855,"In today's highly connected cyber-physical environments, users are becoming more and more concerned about their privacy and ask for more involvement in the control of their data. However, achieving effective involvement of users requires improving their privacy decision-making. This can be achieved by: (i) raising their awareness regarding the direct and indirect privacy risks they accept to take when sharing data with consumers; (ii) helping them in optimizing their privacy protection decisions to meet their privacy requirements while maximizing data utility. In this article, we address the second goal by proposing a user-centric multi-objective approach for context-aware privacy management in connected environments, denoted δ-Risk. Our approach features a new privacy risk quantification model to dynamically calculate and select the best protection strategies for the user based on her preferences and contexts. Computed strategies are optimal in that they seek to closely satisfy user requirements and preferences while maximizing data utility and minimizing the cost of protection. We implemented our proposed approach and evaluated its performance and effectiveness in various scenarios. The results show that δ-Risk delivers scalability and low-complexity in time and space. Besides, it handles privacy reasoning in real-time, making it able to support the user in various contexts, including ephemeral ones. It also provides the user with at least one best strategy per context. © 2021 Association for Computing Machinery.",context-aware computing; Internet of Things; privacy by design; privacy risk quantification; semantic reasoning; User-centric privacy,Consumer protection; Data Sharing; Decision making; Environmental management; Cyber physicals; Data utilities; Multi objective; Privacy management; Privacy protection; Privacy requirements; Protection strategy; User requirements; Privacy by design
Adaptive multi-task dual-structured learning with its application on alzheimer's disease study,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114276134&doi=10.1145%2f3398728&partnerID=40&md5=81ec93625af383e3cf20a873474b67b4,"Multi-task learning has been widely applied to Alzheimer's Disease (AD) studies due to its capability of simultaneously rating the disease severity (classification) and predicting corresponding clinical scores (regression). In this article, we propose a novel technique of Adaptive Multi-task Dual-Structured Learning, named AMDSL, by mutually exploring the dual manifold structure for the label and regression score of the disease data under joint classification and regression tasks, while learning an adaptive shared similarity measure and corresponding feature mapping among these two tasks. We encode both the reconstructed label representation and regression score adaptive to the ideal similarity measure on disease data to achieve the ideal performance on these two joint tasks. The alternating algorithm is proposed to optimize the above objective. We theoretically prove the convergence of the optimization algorithm. The superiority of AMDSL is experimentally validated under joint classification and regression as per various evaluation metrics against the most authoritative Alzheimer's disease data. © 2021 Association for Computing Machinery.",Alzheimer's predictive model; Multi-task learning,Neurodegenerative diseases; Alzheimer's disease; Disease severity; Evaluation metrics; Ideal performance; Manifold structures; Optimization algorithms; Similarity measure; Structured learning; Multi-task learning
Energy and SLA-driven MapReduce Job Scheduling Framework for Cloud-based Cyber-Physical Systems,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114273664&doi=10.1145%2f3409772&partnerID=40&md5=2dcdc85e953b2477b80d320304acb3c6,"Energy consumption minimization of cloud data centers (DCs) has attracted much attention from the research community in the recent years; particularly due to the increasing dependence of emerging Cyber-Physical Systems on them. An effective way to improve the energy efficiency of DCs is by using efficient job scheduling strategies. However, the most challenging issue in selection of efficient job scheduling strategy is to ensure service-level agreement (SLA) bindings of the scheduled tasks. Hence, an energy-aware and SLA-driven job scheduling framework based on MapReduce is presented in this article. The primary aim of the proposed framework is to explore task-to-slot/container mapping problem as a special case of energy-aware scheduling in deadline-constrained scenario. Thus, this problem can be viewed as a complex multi-objective problem comprised of different constraints. To address this problem efficiently, it is segregated into three major subproblems (SPs), namely, deadline segregation, map and reduce phase energy-aware scheduling. These SPs are individually formulated using Integer Linear Programming. To solve these SPs effectively, heuristics based on Greedy strategy along with classical Hungarian algorithm for serial and serial-parallel systems are used. Moreover, the proposed scheme also explores the potential of splitting Map/Reduce phase(s) into multiple stages to achieve higher energy reductions. This is achieved by leveraging the concepts of classical Greedy approach and priority queues. The proposed scheme has been validated using real-time data traces acquired from OpenCloud. Moreover, the performance of the proposed scheme is compared with the existing schemes using different evaluation metrics, namely, number of stages, total energy consumption, total makespan, and SLA violated. The results obtained prove the efficacy of the proposed scheme in comparison to the other schemes under different workload scenarios. © 2021 Association for Computing Machinery.",and MapReduce; Cyber-physical systems; energy optimization; greedy approach; Hungarian algorithm; job scheduling,Cyber Physical System; Embedded systems; Energy efficiency; Energy utilization; Green computing; Integer programming; Power management; Scheduling; Energy Consumption Minimization; Energy-aware scheduling; Hungarian algorithm; Integer Linear Programming; Multi-objective problem; Research communities; Service Level Agreements; Total energy consumption; Job shop scheduling
Blockchain-enabled Tensor-based Conditional Deep Convolutional GAN for Cyber-physical-Social Systems,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114272611&doi=10.1145%2f3404890&partnerID=40&md5=2b99e72c616597b6743974df810294e1,"Deep learning techniques have shown significant success in cyber-physical-social systems (CPSS). As an instance of deep learning models, generative adversarial nets (GAN) model enables powerful and flexible image augmentation, image generation, and classification, thus can be applied to real-world CPSS settings. GAN model training needs a large collection of cyber-physical-social data originating from various CPSS devices. Numerous prevailing GAN models depend on a tacit assumption that several cyber-physical-social data providers present a reliable source to collect training data, which is seldom the case in real CPSS. The existing GAN models also fail to consider multi-dimensional latent structure. In our work, we put forward a novel blockchain-enabled tensor-based conditional deep convolutional GAN (TCDC-GAN) model for cyber-physical-social systems. The blockchain is employed to develop a decentralized and reliable cyber-physical-social data-sharing platform between numerous cyber-physical-social data providers, such that the training data and the model are documented on a ledger that is distributed. Furthermore, a tensor-based generator and a tensor-based discriminator are well designed by employing the tensor model. The results of extensive simulation experiments show the efficacy of the proposed TCDC-GAN model. Compared with the state-of-the-art models, our model gains superior estimation performance. © 2021 Association for Computing Machinery.",blockchain; Cyber-physical-social systems; deep learning; generative adversarial network; tensor,Blockchain; Convolution; Cyber Physical System; Data Sharing; Deep learning; Tensors; Estimation performance; Extensive simulations; Image generations; Latent structures; Learning models; Learning techniques; Multi dimensional; State of the art; Learning systems
A Novel Cloud-Assisted Secure Deep Feature Classification Framework for Cancer Histopathology Images,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114270675&doi=10.1145%2f3424221&partnerID=40&md5=57c3a7491c6ab23e6632c957b9798faf,"The advancements in the Internet of Things (IoT) and cloud services have enabled the availability of smart e-healthcare services in a distant and distributed environment. However, this has also raised major privacy and efficiency concerns that need to be addressed. While sharing clinical data across the cloud that often consists of sensitive patient-related information, privacy is a major challenge. Adequate protection of patients' privacy helps to increase public trust in medical research. Additionally, DL-based models are complex, and in a cloud-based approach, efficient data processing in such models is complicated. To address these challenges, we propose an efficient and secure cancer diagnostic framework for histopathological image classification by utilizing both differential privacy and secure multi-party computation. For efficient computation, instead of performing the whole operation on the cloud, we decouple the layers into two modules: one for feature extraction using the VGGNet module at the user side and the remaining layers for private prediction over the cloud. The efficacy of the framework is validated on two datasets composed of histopathological images of the canine mammary tumor and human breast cancer. The application of differential privacy preserving to the proposed model makes the model secure and capable of preserving the privacy of sensitive data from any adversary, without significantly compromising the model accuracy. Extensive experiments show that the proposed model efficiently achieves the trade-off between privacy and model performance. © 2021 Association for Computing Machinery.",and cloud computing; canine mammary tumor; Deep learning; differential privacy; histopathology image classification; multiparty computation; transfer learning,Diagnosis; Diseases; Economic and social effects; Image classification; Internet of things; Privacy by design; Differential privacies; Distributed environments; E-healthcare services; Efficient computation; Feature classification; Histopathological images; Internet of thing (IOT); Secure multi-party computation; Medical imaging
Federated Learning in a Medical Context: A Systematic Literature Review,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114272453&doi=10.1145%2f3412357&partnerID=40&md5=edde98bf42b5945cc4d3be443fa46dd0,"Data privacy is a very important issue. Especially in fields like medicine, it is paramount to abide by the existing privacy regulations to preserve patients' anonymity. However, data is required for research and training machine learning models that could help gain insight into complex correlations or personalised treatments that may otherwise stay undiscovered. Those models generally scale with the amount of data available, but the current situation often prohibits building large databases across sites. So it would be beneficial to be able to combine similar or related data from different sites all over the world while still preserving data privacy. Federated learning has been proposed as a solution for this, because it relies on the sharing of machine learning models, instead of the raw data itself. That means private data never leaves the site or device it was collected on. Federated learning is an emerging research area, and many domains have been identified for the application of those methods. This systematic literature review provides an extensive look at the concept of and research into federated learning and its applicability for confidential healthcare datasets. © 2021 Copyright held by the owner/author(s).",Federated learning,Machine learning; Complex correlation; Current situation; Large database; Machine learning models; Privacy regulation; Private data; Systematic literature review; Training machines; Data privacy
An Effective Hyperparameter Optimization Algorithm for DNN to Predict Passengers at a Metro Station,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114271923&doi=10.1145%2f3410156&partnerID=40&md5=e45d39221cb749e33e3b7e5ec41ea5f8,"As one of the public transportation systems, metro is certainly an indispensable part in urban areas of a metropolis today. Several successful results have shown that deep learning technologies might provide an effective way to predict the number of passengers at a metro station. However, most information systems based on deep learning technologies are usually designed and tuned manually by using domain knowledge and trial-and-error; thus, how to find out a set of suitable hyperparameters for a deep neural network (DNN) has become a critical research issue. To deal with the problem of hyperparameter setting for a DNN in solving the prediction of passengers at a metro station, a novel metaheuristic algorithm called search economics for hyperparameter optimization is presented to improve the accuracy rate of such a prediction system. The basic idea of the proposed algorithm is to divide the solution space into a set of subspaces first and then assign a different number of search agents to each subspace based on the ""potential of each subspace.""The potential is estimated based on the objective values of the searched solutions, the objective values of the probe solutions, and the computation time invested in each subspace. The proposed method is compared with Bayesian, random forest, support vector regression, DNN, and DNN with different hyperparameter search algorithms, namely, grid search, simulated annealing, and particle swarm optimization. The simulation results using the data provided by the government of Taipei city, Taiwan, indicate that the proposed method outperforms all the other forecasting methods compared in this article in terms of the mean absolute percentage error. © 2021 Association for Computing Machinery.",and hyperparameter optimization; Deep learning,Decision trees; Deep neural networks; Forecasting; Particle swarm optimization (PSO); Simulated annealing; Subway stations; Support vector regression; Urban transportation; Vectors; Critical researches; Forecasting methods; Hyper-parameter optimizations; Learning technology; Mean absolute percentage error; Meta heuristic algorithm; Prediction systems; Public transportation systems; Deep learning
Big Data Analysis of Internet of Things System,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114277759&doi=10.1145%2f3389250&partnerID=40&md5=4e6db6b55db02d6d8908f401f8dc887b,"The study aims at exploring the Internet of things (IoT) system from the perspective of data and further improving the performance of the IoT system. The IoT data energy collection and information transmission system model is constructed by combining IoT and wireless relay cooperative transmission technology. Moreover, the energy efficiency, outage probability (OP), and accuracy of the model are evaluated by simulation experiments. The results show that, in the energy efficiency analysis, with the increase of power split factor ρ, the information transmission ability of the system increases. Whereas, the energy collection ability decreases, so the energy efficiency is reduced. Thus, choosing a more suitable power split factor for the energy efficiency of IoT is important. By analyzing OP and bit error rate (BER), as the values of m (Nakagami, the fading index of the fading distribution) and multi-hop paths increase, the OP and BER are reduced while the system performance is increased. Therefore, this article uses wireless relay cooperative transmission technology to integrate big data analysis into the IoT system. Finally, by adding multi-hop path and other methods to reduce the OP and BER of system, the system performance is improved. It provides experimental basis for the development of IoT systems. © 2021 Association for Computing Machinery.",big data analysis; energy efficiency; Internet of things; relay cooperation,Big data; Bit error rate; Data communication systems; Data handling; Electric power transmission; Energy efficiency; Information analysis; Transmissions; Cooperative transmission; Energy collection; Energy efficiency analysis; Fading distribution; Information transmission; Information transmission systems; Internet of thing (IOT); Outage probability; Internet of things
Blockchain-based Data Sharing System for Sensing-as-a-Service in Smart Cities,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111763024&doi=10.1145%2f3397202&partnerID=40&md5=be3899f5a4cde6501a479dd6c296ecd7,"The sensing-as-a-service (SaaS) model has been explored to address the challenge of intractability of managing a large number of sensors faced by future smart cities. However, how to effectively share sensor data without compromising confidentiality, privacy protection, and fair trading without third parties is one of critical issues that must be solved in the SaaS in smart cities. While blockchain shows promise in solving these issues, the existing blockchain-based data sharing (BBDS) systems are difficult to apply to SaaS in smart cities because of many unresolved issues such as requiring a customized blockchain, huge storage, communication and computation costs, and dependence on a third party to achieve fair trading. We propose a BBDS system model with its security requirements before we present a concrete construction by combining -protocol, Paillier encryption scheme, and any secure symmetrical encryption and signature schemes. To demonstrate the utility of our proposed BBDS system, we present a security analysis and compare our system with other solutions. We implement the prototype in Remix to analyze the gas cost, and we conduct experiments to evaluate the communication and computation costs of the BBDS system using symmetric encryption (advanced encryption standard (AES)) and a signature scheme (elliptic curve digital signature algorithm (ECDSA)). © 2021 Association for Computing Machinery.",data sharing; Fairness; pseudonymity; Sensor-as-a-Service; smart cities,Authentication; Blockchain; Commerce; Data Sharing; Digital storage; Privacy by design; Smart city; Software as a service (SaaS); Advanced Encryption Standard; Data sharing systems; Elliptic curve digital signature algorithm; Encryption schemes; Privacy protection; Security analysis; Security requirements; Symmetric encryption; Cryptography
Product Quality Monitoring in Hydraulic Presses Using a Minimal Sample of Sensor and Actuator Data,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114273431&doi=10.1145%2f3436238&partnerID=40&md5=161dcd629c89a30d127df083cbe9ef65,"Machine learning and artificial intelligence provide methods and algorithms to take advantage of sensor and actuator data in automated production systems. Product quality monitoring is one of the promising applications available for data-driven modeling, particularly in cases where the quality parameters cannot be measured with reasonable effort. This is the case for defects such as cracks in workpieces of hydraulic metal powder presses. However, the variety of shapes produced at a powder press requires training of individual models based on a minimal sample size of unlabeled data to adapt to changing settings. Therefore, this article proposes an unsupervised product quality monitoring approach based on dynamic time warping and non-linear regression to detect anomalies in unlabeled sensor and actuator data. A preprocessing step that isolates only the relevant intervals of the process is further introduced, facilitating efficient product quality monitoring. The evaluation on an industrial dataset with 37 samples, generated in test runs, shows a true-positive rate for detected product quality defects of 100% while preserving an acceptable accuracy. Moreover, the approach achieves the output within less than 10 seconds, assuring that the result is available before the next workpiece is processed. In this way, efficient product quality management is possible, reducing time- and cost-intensive quality inspections. © 2021 Copyright held by the owner/author(s).",cyber-physical system; hydraulic metal powder press; minimal sample size; Product quality monitoring; unsupervised machine learning,Defects; Machine learning; Powder metals; Presses (machine tools); Quality control; Quality management; Statistical tests; Automated production systems; Non-linear regression; Pre-processing step; Product quality monitoring; Quality inspection; Quality parameters; Sensor and actuators; True positive rates; Hydraulic actuators
Epilepsy Diagnosis Using Multi-view & Multi-medoid Entropy-based Clustering with Privacy Protection,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112426284&doi=10.1145%2f3404893&partnerID=40&md5=177d89a8fd208d12036fef51bd4d2fc0,"Using unsupervised learning methods for clinical diagnosis is very meaningful. In this study, we propose an unsupervised multi-view & multi-medoid variant-entropy-based fuzzy clustering (M2VEFC) method for epilepsy EEG signals detecting. Comparing with existing related studies, M2VEFC has four main merits and contributions: (1) Features in original EEG data are represented from different perspectives that can provide more pattern information for epilepsy signals detecting. (2) During multi-view modeling, multi-medoids are used to capture the structure of clusters in each view. Furthermore, we assume that the medoids in a cluster observed from different views should keep invariant, which is taken as one of the collaborative learning mechanisms in this study. (3) A variant entropy is designed as another collaborative learning mechanism in which view weight learning is controlled by a user-free parameter. The parameter is derived from the distribution of samples in each view such that the learned weights have more discrimination. (4) M2VEFC does not need original data as its input - it only needs a similarity matrix and feature statistical information. Therefore, the original data are not exposed to users and hence the privacy is protected. We use several different kinds of feature extraction techniques to extract several groups of features as multi-view data from original EEG data to test the proposed method M2VEFC. Experimental results indicate M2VEFC achieves a promising performance that is better than benchmarking models. © 2021 Association for Computing Machinery.",entropy; Epilepsy EEG; multi-view learning; multiple medoids; privacy protection,Benchmarking; Diagnosis; Entropy; Feature extraction; Neurology; Privacy by design; Signal detection; Unsupervised learning; Benchmarking models; Clinical diagnosis; Collaborative learning; Feature extraction techniques; Multi-view modeling; Pattern information; Statistical information; Unsupervised learning method; Learning systems
A Novel Memory-hard Password Hashing Scheme for Blockchain-based Cyber-physical Systems,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114277923&doi=10.1145%2f3408310&partnerID=40&md5=434deafac394b41603b0a16856ce2632,"There has been an increasing interest of integrating blockchain into cyber-physical systems (CPS). The design of password hashing schemes (PHSs) is in the core of blockchain security. However, no existing PHS seems to meet both the requirements of sufficient security and small code size for blockchain-based CPSs. In this article, a novel memory-hard PHS based on the classic PBKDF2 is proposed. Evaluation results show that the proposed scheme is promising for blockchain-based CPS, as it manages to provide enhanced security in comparison to PBKDF2 with limited increase in code size. © 2021 Association for Computing Machinery.",blockchain; cyber-physical system; hashing function; Password,Authentication; Cyber Physical System; Embedded systems; Code size; Cyber-physical systems (CPS); Evaluation results; Blockchain
A Blockchain-based Iterative Double Auction Protocol Using Multiparty State Channels,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114274655&doi=10.1145%2f3389249&partnerID=40&md5=aa61c970dc4114bef9cf6dc2edba5c6e,"Although the iterative double auction has been widely used in many different applications, one of the major problems in its current implementations is that they rely on a trusted third party to handle the auction process. This imposes the risk of single point of failures, monopoly, and bribery. In this article, we aim to tackle this problem by proposing a novel decentralized and trustless framework for iterative double auction based on blockchain. Our design adopts the smart contract and state channel technologies to enable a double auction process among parties that do not need to trust each other, while minimizing the blockchain transactions. In specific, we propose an extension to the original concept of state channels that can support multiparty computation. Then, we provide a formal development of the proposed framework and prove the security of our design against adversaries. Finally, we develop a proof-of-concept implementation of our framework using Elixir and Solidity, on which we conduct various experiments to demonstrate its feasibility and practicality. © 2021 Association for Computing Machinery.",blockchain; Iterative double auction; state channel; trustless,Internet; Auction process; Double auction; Double auction protocol; Formal development; Multi-party state; Multiparty computation; Proof of concept; Trusted third parties; Blockchain
DataOps for Cyber-Physical Systems Governance: The Airport Passenger Flow Case,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114276568&doi=10.1145%2f3432247&partnerID=40&md5=28c688af3cafbdbe3200e7a426b84158,"Recent advancements in information technology have ushered a new wave of systems integrating Internet technology with sensing, wireless communication, and computational resources over existing infrastructures. As a result, myriad complex, non-traditional Cyber-Physical Systems (CPS) have emerged, characterized by interaction among people, physical facilities, and embedded sensors and computers, all generating vast amounts of complex data. Such a case is encountered within a contemporary airport hall setting: passengers roaming, information systems governing various functions, and data being generated and processed by cameras, phones, sensors, and other Internet of Things technology. This setting has considerable potential of contributing to goals entertained by the CPS operators, such as airlines, airport operators/owners, technicians, users, and more. We model the airport setting as an instance of such a complex, data-intensive CPS where multiple actors and data sources interact, and generalize a methodology to support it and other similar systems. Furthermore, this article instantiates the methodology and pipeline for predictive analytics for passenger flow, as a characteristic manifestation of such systems requiring a tailored approach. Our methodology also draws from DataOps principles, using multi-modal and real-life data to predict the underlying distribution of the passenger flow on a flight-level basis (improving existing day-level predictions), anticipating when and how the passengers enter the airport and move through the check-in and baggage drop-off process. This allows to plan airport resources more efficiently while improving customer experience by avoiding passenger clumping at check-in and security. We demonstrate results obtained over a case from a major international airport in the Netherlands, improving up to 60% upon predictions of daily passenger flow currently in place. © 2021 Association for Computing Machinery.",airport management; big data; cyber-physical systems; Data-intensive systems; DataOps; systems governance,Airport passenger transportation; Airport security; Cyber Physical System; Embedded systems; Forecasting; Predictive analytics; Computational resources; Customer experience; Cyber-physical systems (CPS); International airport; Internet of things technologies; Internet technology; Underlying distribution; Wireless communications; Airports
A Polishing Robot Force Control System Based on Time Series Data in Industrial Internet of Things,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113604009&doi=10.1145%2f3419469&partnerID=40&md5=a7448c69bebaebd8e8f2781b90dc6191,"Installing a six-dimensional force/torque sensor on an industrial arm for force feedback is a common robotic force control strategy. However, because of the high price of force/torque sensors and the closedness of an industrial robot control system, this method is not convenient for industrial mass production applications. Various types of data generated by industrial robots during the polishing process can be saved, transmitted, and applied, benefiting from the growth of the industrial internet of things (IIoT). Therefore, we propose a constant force control system that combines an industrial robot control system and industrial robot offline programming software for a polishing robot based on IIoT time series data. The system mainly consists of four parts, which can achieve constant force polishing of industrial robots in mass production. (1) Data collection module. Install a six-dimensional force/torque sensor at a manipulator and collect the robot data (current series data, etc.) and sensor data (force/torque series data). (2) Data analysis module. Establish a relationship model based on variant long short-term memory which we propose between current time series data of the polishing manipulator and data of the force sensor. (3) Data prediction module. A large number of sensorless polishing robots of the same type can utilize that model to predict force time series. (4) Trajectory optimization module. The polishing trajectories can be adjusted according to the prediction sequences. The experiments verified that the relational model we proposed has an accurate prediction, small error, and a manipulator taking advantage of this method has a better polishing effect. © 2021 Association for Computing Machinery.",Constant force control; industrial internet of things (IIoT); polishing robot system; recurrent neural network; trajectory optimization,Control systems; Data acquisition; Force control; Forecasting; Industrial manipulators; Industrial robots; Manipulators; Polishing; Robot programming; Time series; Accurate prediction; Constant Force Polishing; Data collection modules; Industrial robot controls; Off line programming; Robotic force control; Six-dimensional force; Trajectory optimization; Industrial internet of things (IIoT)
A Multi-Threshold Ant Colony System-based Sanitization Model in Shared Medical Environments,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113709995&doi=10.1145%2f3408296&partnerID=40&md5=706ae15ea5203c703eb2104fb3a90511,"During the past several years, revealing some useful knowledge or protecting individual's private information in an identifiable health dataset (i.e., within an Electronic Health Record) has become a tradeoff issue. Especially in this era of a global pandemic, security and privacy are often overlooked in lieu of usability. Privacy preserving data mining (PPDM) is definitely going to be have an important role to resolve this problem. Nevertheless, the scenario of mining information in an identifiable health dataset holds high complexity compared to traditional PPDM problems. Leaking individual private information in an identifiable health dataset has becomes a serious legal issue. In this article, the proposed Ant Colony System to Data Mining algorithm takes the multi-threshold constraint to secure and sanitize patents' records in different lengths, which is applicable in a real medical situation. The experimental results show the proposed algorithm not only has the ability to hide all sensitive information but also to keep useful knowledge for mining usage in the sanitized database. © 2021 Association for Computing Machinery.",ant colony system; evolutionary algorithm; Privacy-preserving data mining; sensitive itemsets,Ant colony optimization; Health; Medical computing; Privacy by design; Ant colony systems; Data mining algorithm; Electronic health record; Privacy-preserving data mining; Private information; Sanitized database; Security and privacy; Sensitive informations; Data mining
Blockchain-Based Power Energy Trading Management,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114279169&doi=10.1145%2f3409771&partnerID=40&md5=45ff2f8302ddd8b153b9545df49cf2c2,"Distributed peer-to-peer power energy markets are emerging quickly. Due to central governance and lack of effective information aggregation mechanisms, energy trading cannot be efficiently scheduled and tracked. We devise a new distributed energy transaction system over the energy Industrial Internet of Things based on predictive analytics, blockchain, and smart contract technologies. We propose a solution for scheduling distributed energy sources based on the Minimum Cut Maximum Flow theory. Blockchain is used to record transactions and reach consensus. Payment clearing for the actual power consumption is executed via smart contracts. Experimental results on real data show that our solution is practical and achieves a lower total cost for power energy consumption. © 2021 Association for Computing Machinery.",blockchain; Distributed energy market; minimum cut maximum flow; predictive analytics; smart contract,Blockchain; Energy utilization; Fintech; Industrial internet of things (IIoT); Predictive analytics; Distributed energies; Distributed energy sources; Energy trading; Information aggregation mechanisms; Maximum flows; Peer to peer; Power energy; Transaction systems; Power markets
Proof-of-Prestige: A Useful Work Reward System for Unverifiable Tasks,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114272935&doi=10.1145%2f3419483&partnerID=40&md5=2c15ad36bb4277f1a237f8f8d6ea037e,"As cryptographic tokens and altcoins are increasingly being built to serve as utility tokens, the notion of useful work consensus protocols is becoming ever more important. With useful work consensus protocols, users get rewards after they have carried out some specific tasks useful for the network. While in some cases the proof of some utility or service can be provided, the majority of tasks are impossible to verify reliably. To deal with such cases, we design ""Proof-of-Prestige""(PoP) - a reward system that can run directly on Proof-of-Stake (PoS) blockchains or as a smart contract on top of Proof-of-Work (PoW) blockchains. PoP introduces ""prestige,""which is a volatile resource that, in contrast to coins, regenerates over time. Prestige can be gained by performing useful work, spent when benefiting from services, and directly translates to users minting power. Our scheme allows us to reliably reward decentralized workers while keeping the system free for the end-users. PoP is resistant against Sybil and collusion attacks and can be used with a vast range of unverifiable tasks. We build a simulator to assess the cryptoeconomic behavior of the system and deploy a full prototype of a content dissemination platform rewarding its participants. We implement the blockchain component on both Ethereum (PoW) and Cosmos (PoS), provide a mobile application, and connect it with our scheme with a negligible memory footprint. Finally, we adapt a fair exchange protocol allowing us to atomically exchange files for rewards also in scenarios where not all the parties have Internet connectivity. Our evaluation shows that even for large Ethereum traces, PoP introduces sub-millisecond computational overhead for miners in Cosmos and less than 0.013$ smart contract invocation cost for users in Ethereum. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Blockchain; network security; security and privacy,Blockchain; Electronic document exchange; Ethereum; Collusion attack; Computational overheads; Consensus protocols; Content dissemination; Fair-exchange protocols; Internet connectivity; Memory footprint; Mobile applications; Internet protocols
Introduction to the Special Issue on Decentralized Blockchain Applications and Infrastructures for Next Generation Cyber-Physical Systems,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114330633&doi=10.1145%2f3464768&partnerID=40&md5=0ef5df7214b810206998a6efe4d9d838,[No abstract available],,
Leveraging data augmentation for service QoS prediction in cyber-physical systems,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107760725&doi=10.1145%2f3425795&partnerID=40&md5=52f3703190f432eb04dd603bfa7421d9,"With the fast-developing domain of cyber-physical systems (CPS), constructing the CPS with high-quality services becomes an imperative task. As one of the effective solutions for information overload in CPS construction, quality of service (QoS)-aware service recommendation has drawn much attention in academia and industry. However, the lack of most QoS values limits the recommendation performance and it is time-consuming for users to get the QoS values by invoking all the services. Therefore, a powerful prediction model is required to predict the unobserved QoS values. Considering the fact that most existing QoS prediction models are unable to effectively address the data-sparsity problem, a novel two-stage framework called AgQ is proposed for QoS prediction. Specifically, a data augmentation strategy is designed in the first stage to enlarge the training set by drawing additional virtual instances. In the second stage, a prediction model is applied that considers both virtual and factual instances during the training procedure. We conduct extensive experiments on the WSDream dataset to demonstrate the effectiveness of the our QoS prediction framework and verify that the data augmentation strategy can indeed alleviate the data-sparsity problem. In terms of mean absolute error, taking the Multilayer Perceptron model as an example, the maximum improvement achieves 5% under 5% sparsity. © 2021 Association for Computing Machinery.",Cyber-physical systems; Data augmentation; Neural network; QoS prediction,Cyber Physical System; Embedded systems; Forecasting; Multilayer neural networks; Predictive analytics; Service industry; Cyber-physical systems (CPS); Data sparsity problems; High quality service; Information overloads; Mean absolute error; Recommendation performance; Service recommendations; Training procedures; Quality of service
Large-Scale Least Squares Twin SVMs,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107645455&doi=10.1145%2f3398379&partnerID=40&md5=d924da950f2e9f35f1e8f4a0763d4e97,"In the last decade, twin support vector machine (TWSVM) classifiers have achieved considerable emphasis on pattern classification tasks. However, the TWSVM formulation still suffers from the following two shortcomings: (1) TWSVM deals with the inverse matrix calculation in the Wolfe-dual problems, which is intractable for large-scale datasets with numerous features and samples, and (2) TWSVM minimizes the empirical risk instead of the structural risk in its formulation. With the advent of huge amounts of data today, these disadvantages render TWSVM an ineffective choice for pattern classification tasks. In this article, we propose an efficient large-scale least squares twin support vector machine (LS-LSTSVM) for pattern classification that rectifies all the aforementioned shortcomings. The proposed LS-LSTSVM introduces different Lagrangian functions to eliminate the need for calculating inverse matrices. The proposed LS-LSTSVM also does not employ kernel-generated surfaces for the non-linear case, and thus uses the kernel trick directly. This ensures that the proposed LS-LSTSVM model is superior to the original TWSVM and LSTSVM. Lastly, the structural risk is minimized in LS-LSTSVM. This exhibits the essence of statistical learning theory, and consequently, classification accuracy on datasets can be improved due to this change. The proposed LS-LSTSVM is solved using the sequential minimal optimization (SMO) technique, making it more suitable for large-scale problems. We further proved the convergence of the proposed LS-LSTSVM. Exhaustive experiments on several real-world benchmarks and NDC-based large-scale datasets demonstrate that the proposed LS-LSTSVM is feasible for large datasets and, in most cases, performed better than existing algorithms. © 2021 Association for Computing Machinery.",large scale SVMs; least squares twin SVM; Machine learning; support vector machines (SVMs),Classification (of information); Inverse problems; Matrix algebra; Optimization; Support vector machines; Classification accuracy; Lagrangian functions; Large-scale datasets; Large-scale problem; Least squares twin support vector machines; Sequential minimal optimization; Statistical learning theory; Twin support vector machines; Large dataset
A Novel Multiobjective GDWCN-PSO Algorithm and Its Application to Medical Data Security,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104972060&doi=10.1145%2f3397679&partnerID=40&md5=978ca806087e888aad27ec85347e8994,"Nature-inspired optimization is one of the most prevalent research domains with a confounding history that fascinates the research communities. Particle Swarm Optimization is one of the well-known optimizers that belongs to the family of nature-inspired algorithms. It often suffers from premature convergence leading to a local optimum. To address this, several methods were presented using different network topologies of the particles, but either lacked accuracy or were slow. To solve these problems, an improved version of the Directed Weighted Complex Network Particle Swarm Optimization using the Genetic Algorithm (GDWCN-PSO) is presented. This method uses the concept of the Genetic Algorithm after each update to enhance convergence and diversity. Since most of the real-world applications and complex optimization problems involve more than one objective function so to suit this problem, a multiobjective version of GDWCN-PSO is also proposed and validated on standard benchmarks. To demonstrate its applicability in real-world applications, GDWCN-PSO is applied to solve the optimal key-based medical image encryption. It is one of the most challenging problems in health IoTs for protecting sensitive and confidential patient data as well as addressing the major concern of integrity and security of data in today's advanced digital world. © 2021 Association for Computing Machinery.",directed weighted complex network; IoTs; key generation; medical image encryption and data security; multiobjective optimization; Particle swarm optimization,Benchmarking; Biomimetics; Complex networks; Cryptography; Data privacy; Genetic algorithms; Hospital data processing; Medical imaging; Complex optimization problems; Medical image encryptions; Nature inspired algorithms; Network topology; Objective functions; Pre-mature convergences; Research communities; Weighted complex networks; Particle swarm optimization (PSO)
A Simulation-driven Methodology for IoT Data Mining Based on Edge Computing,2021,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107913812&doi=10.1145%2f3402444&partnerID=40&md5=b1a3f7c5d0aba4bf54446b2b4eb8b8ff,"With the ever-increasing diffusion of smart devices and Internet of Things (IoT) applications, a completely new set of challenges have been added to the Data Mining domain. Edge Mining and Cloud Mining refer to Data Mining tasks aimed at IoT scenarios and performed according to, respectively, Cloud or Edge computing principles. Given the orthogonality and interdependence among the Data Mining task goals (e.g., accuracy, support, precision), the requirements of IoT applications (mainly bandwidth, energy saving, responsiveness, privacy preserving, and security) and the features of Edge/Cloud deployments (de-centralization, reliability, and ease of management), we propose EdgeMiningSim, a simulation-driven methodology inspired by software engineering principles for enabling IoT Data Mining. Such a methodology drives the domain experts in disclosing actionable knowledge, namely descriptive or predictive models for taking effective actions in the constrained and dynamic IoT scenario. A Smart Monitoring application is instantiated as a case study, aiming to exemplify the EdgeMiningSim approach and to show its benefits in effectively facing all those multifaceted aspects that simultaneously impact on IoT Data Mining. © 2021 Association for Computing Machinery.",cloud computing; Data mining; edge computing; Internet of Things,Application programs; Digital storage; Edge computing; Energy conservation; Internet of things; Knowledge acquisition; Precision engineering; Predictive analytics; Privacy by design; Software reliability; Data mining tasks; Domain experts; Internet of Things (IOT); IOT applications; Predictive models; Privacy preserving; Smart monitoring; Software engineering principles; Data mining
Serendipity-based Points-of-Interest Navigation,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095968722&doi=10.1145%2f3391197&partnerID=40&md5=95d0a847c26d8fa8f8041e86957d7856,"Traditional venue and tour recommendation systems do not necessarily provide a diverse set of recommendations and leave little room for serendipity. In this article, we design MPG, a Mobile Personal Guide that recommends: (i) a set of diverse yet surprisingly interesting venues that are aligned to user preferences and (ii) a set of routes, constructed from the recommended venues. We also introduce EPUI, an Experimental Platform for Urban Informatics. Our comparison with the state-of-the-art schemes indicates that MPG is capable of providing high-quality venues and route recommendations while incorporating seamlessly both the notion of diversity and that of serendipity.  © 2020 ACM.",diversity; POIs recommendation; relevance; serendipity,Experimental platform; High quality; Points of interest; State-of-the-art scheme; Urban Informatics; Internet
An efficient service function chaining placement algorithm in mobile edge computing,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095966146&doi=10.1145%2f3388241&partnerID=40&md5=5c93734d07f815a7eaf5d1f405dc53df,"Mobile Edge Computing (MEC) is a promising network architecture that pushes network control and mobile computing to the network edge. Recent studies propose to deploy MEC applications in the Network Function Virtualization (NFV) environment. The mobile network service in NFV is deployed as a Service Function Chaining (SFC). In the dynamic and resource-limited mobile network, SFC placement aiming at optimizing resource utilization is a challenging problem. In this article, we solve the SFC placement problem in the MEC-NFV environment. We formulate the SFC placement problem as a weighted graph matching problem, including two sub-problems: a graph matching problem and an SFC mapping problem. To efficiently solve the graph matching problem, we propose a Linear Programming-(LP) based approach to calculate the similarity between VNFs and physical nodes. Based on the similarity, we design a Hungarian-based algorithm to solve the SFC mapping problem. Evaluation results show that our proposed LP-based solutions outperform the heuristic algorithms in terms of execution time and resource utilization.  © 2020 ACM.",linear programming; Mobile edge computing; network function virtualization; placement; service function chaining,Cellular radio systems; Edge computing; Heuristic algorithms; Linear programming; Mapping; Mobile telecommunication systems; Pattern matching; Wireless networks; Evaluation results; Graph matching problems; Mapping problem; Network services; Placement algorithm; Placement problems; Resource utilizations; Service functions; Network function virtualization
Improving collaborative filtering with social influence over heterogeneous information networks,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095964763&doi=10.1145%2f3397505&partnerID=40&md5=51f6c497fa6b1aec82445c4c69fc08c9,"The advent of social networks and activity networks affords us an opportunity of utilizing explicit social information and activity information to improve the quality of recommendation in the presence of data sparsity. In this article, we present a social-influence-based collaborative filtering (SICF) framework over heterogeneous information networks with three unique features. First, we integrate different types of entities, links, attributes, and activities from rating networks, social networks, and activity networks into a unified social-influence-based collaborative filtering model through the intra-network and inter-network social influence. Second, we propose three social-influence propagation models to capture three kinds of information propagation within heterogeneous information networks: user-based influence propagation on user rating networks, item-based influence propagation on user-rating activity networks, and term-based influence propagation on user-review activity networks, respectively. We compute three kinds of social-influence-based user similarity scores based on three social-influence propagation models, respectively. Third, a unified social-influence-based CF prediction model is proposed to infer rating tastes by incorporating three kinds of social-influence-based similarity measures with different weighting factors. We design a weight-learning algorithm, SICF, to refine the prediction result by quantifying the contribution of each kind of information propagation to make a good balance between prediction accuracy and data sparsity. Extensive evaluation on real datasets demonstrates that SICF outperforms existing representative collaborative filtering methods.  © 2020 ACM.",Collaborative filtering; data sparsity; heterogeneous information networks; social influence,Economic and social effects; Forecasting; Information dissemination; Information services; Learning algorithms; Predictive analytics; Activity informations; Collaborative filtering methods; Heterogeneous information; Information propagation; Prediction accuracy; Quality of recommendations; Similarity measure; Social information; Collaborative filtering
Routing in large-scale dynamic networks: A Bloom filter-based dual-layer scheme,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095969029&doi=10.1145%2f3407192&partnerID=40&md5=82a1a5e9df919a0d4defbef77a7f0845,"The increasing volume of network-connected devices comprising Internet of Things and the variety of heterogeneous network architectures across these devices pose significant challenges to effective deployment and routing. In this article, we consider the adoption of probabilistic data structures to develop a novel Bloom Filter-based dual-layer inter-domain routing scheme. Our designed scheme implements internal and external routing layers in network gateways constructed upon the counting bloom filter and the original bloom filter. We first compare several representative structures in both theory and experimentation. We then propose our novel Bloom Filter-based dual-layer inter-domain routing scheme. In the design of the routing scheme, we consider issues related to the overall space cost and routing loop prevention, as well as present corresponding solutions. We also detail the principal structures and algorithms. Further, we conduct a theoretical analysis of the space efficiency of our proposed scheme compared to traditional routing with respect to the size of data packets and the size of routing tables, as well as in routing loop avoidance. Finally, via extensive performance evaluation, our experimental results demonstrate the effectiveness and efficiency of our proposed scheme.  © 2020 ACM.",Bloom filter; inter-domain routing; Internet-of-Things; mobile networks; mobile objects,Data structures; Efficiency; Heterogeneous networks; Network architecture; Network layers; Routing protocols; Corresponding solutions; Counting bloom filters; Effectiveness and efficiencies; Interdomain Routing; Large-scale dynamics; Principal structures; Probabilistic data; Space efficiencies; Network routing
Enabling reference verifiability for the world wide web with webchain,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095964941&doi=10.1145%2f3392097&partnerID=40&md5=5a0907dbdfb6e966b2efc6faca0b1170,"As online sources are becoming more prevalent in journalism and scientific literature, the ephemeral nature of the World Wide Web is becoming an increasingly serious issue for their verifiability, replicability, and reproducibility. The architecture of Webchain, a new system enabling source and reference verifiability on the Web, is combining distributed ledger technologies with secure timestamping to ensure the history of creation, ownership, and referential integrity of online resources. We present the architecture and system extensions, conduct a security analysis, and evaluate the Webchain system based on a comprehensive prototype implementation. The results confirm the feasibility and robustness of our approach.  © 2020 ACM.",blockchain; networked journalism; references; World Wide Web,Computer architecture; Online resources; Online sources; Prototype implementations; Referential integrity; Reproducibilities; Scientific literature; Security analysis; System extension; World Wide Web
Evolving influence maximization in evolving networks,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095979083&doi=10.1145%2f3409370&partnerID=40&md5=d4247a8f59f88ed12403a2ef3070396d,"Influence Maximization (IM) aims to maximize the number of people that become aware of a product by finding the ""best""set of ""seed""users to initiate the product advertisement. Unlike most prior arts on the static networks containing fixed number of users, we study the evolving IM in more realistic evolving networks with temporally growing topology. The task of evolving IM, however, is far more challenging over static cases in the sense that the seed selection should consider its impact on future users who will join network during influence diffusion and the probabilities that users influence one another also evolve over time. We address the challenges brought by network evolution through EIM, a newly proposed bandit-based framework that alternates between seed nodes selection and knowledge (i.e., nodes' growing speed and evolving activation probabilities) learning during network evolution. Remarkably, the EIM framework involves three novel components to handle the uncertainties brought by evolution: (1) A fully adaptive particle learning of nodes' growing speed for accurately estimating future influenced size, with real growing behaviors delineated by a set of weighted particles. (2) A bandit-based refining method with growing arms to cope with the evolving activation probabilities via growing edges from previous influence diffusion feedbacks. (3) Evo-IMM, an evolving seed selection algorithm, which leverages the Influence Maximization via Martingale (IMM) framework, with the objective to maximize the influence spread to highly attractive users during evolution. Theoretically, the EIM framework returns a regret bound that provably maintains its sublinearity with respect to the growing network size. Empirically, the effectiveness of the EIM framework is also validated with three notable million-scale evolving network datasets possessing complete social relationships and nodes' joining time. The results confirm the superiority of the EIM framework in terms of an up to 50% larger influenced size over four static baselines.  © 2020 ACM.",evolving influence maximization; Evolving social network; multi-arm bandit,Chemical activation; Activation probabilities; Evolving networks; Growing behaviors; Influence maximizations; Network evolution; Number of peoples; Product advertisements; Social relationships; Uncertainty analysis
A reinforcement learning approach to optimize discount and reputation tradeoffs in E-commerce systems,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095968987&doi=10.1145%2f3400024&partnerID=40&md5=89053cb66accc80f5662aeefd40d9440,"Feedback-based reputation systems are widely deployed in E-commerce systems. Evidence shows that earning a reputable label (for sellers of such systems) may take a substantial amount of time, and this implies a reduction of profit. We propose to enhance sellers' reputation via price discounts. However, the challenges are as follows: (1) The demands from buyers depend on both the discount and reputation, and (2) the demands are unknown to the seller. To address these challenges, we first formulate a profit maximization problem via a semi-Markov decision process to explore the optimal tradeoffs in selecting price discounts. We prove the monotonicity of the optimal profit and optimal discount. Based on the monotonicity, we design a Q-learning with forward projection (QLFP) algorithm, which infers the optimal discount from historical transaction data. We prove that the QLFP algorithm convergences to the optimal policy. We conduct trace-driven simulations using a dataset from eBay to evaluate the QLFP algorithm. Evaluation results show that QLFP improves the profit by as high as 50% over both Q-learning and Speedy Q-learning. The QLFP algorithm also improves both the reputation and profit by as high as two times over the scheme of not providing any price discount.  © 2020 ACM.",discount; reinforcement learning; Reputation systems,Electronic commerce; Markov processes; Mobile telecommunication systems; Profitability; Algorithm convergence; E-commerce systems; Evaluation results; Profit maximization; Reinforcement learning approach; Reputation systems; Semi-Markov decision process; Trace driven simulation; Reinforcement learning
Overexposure-aware influence maximization,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095970031&doi=10.1145%2f3408315&partnerID=40&md5=eb578d38e8f7366ee69ae40ab040faef,"Viral marketing campaigns are often negatively affected by overexposure. Overexposure occurs when users become less likely to favor a promoted product after receiving information about the product from too large a fraction of their friends. Yet, existing influence diffusion models do not take overexposure into account, effectively overestimating the number of users who favor the product and diffuse information about it. In this work, we propose the first influence diffusion model that captures overexposure. In our model, Latency Aware Independent Cascade Model with Overexposure (LAICO), the activation probability of a node representing a user is multiplied (discounted) by an overexposure score, which is calculated based on the ratio between the estimated and the maximum possible number of attempts performed to activate the node. We also study the influence maximization problem under LAICO. Since the spread function in LAICO is non-submodular, algorithms for submodular maximization are not appropriate to address the problem. Therefore, we develop an approximation algorithm that exploits monotone submodular upper and lower bound functions of spread, and a heuristic that aims to maximize a proxy function of spread iteratively. Our experiments show the effectiveness and efficiency of our algorithms.  © 2020 ACM.",influence diffusion; Influence maximization; social networks,Diffusion; Iterative methods; Marketing; Activation probabilities; Diffusion model; Effectiveness and efficiencies; Influence maximizations; Proxy functions; Spread functions; Upper and lower bounds; Viral marketing; Approximation algorithms
Design and implementation of a compressed certificate status protocol,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095971527&doi=10.1145%2f3392096&partnerID=40&md5=ebae404e855158b2aa92ee23e9fcd9a7,"Trust in Secure Sockets Layer-based communications is traditionally provided by Certificate (or Certification) Authorities (CAs) in the form of signed certificates. Checking the validity of a certificate involves three steps: (i) checking its expiration date, (ii) verifying its signature, and (iii) ensuring that it is not revoked. Currently, such certificate revocation checks (i.e., step (iii) above) are done either via Certificate Revocation Lists (CRLs), or Online Certificate Status Protocol (OCSP) servers. Unfortunately, despite the existence of these revocation checks, sophisticated cyber-attackers can still trick web browsers to trust a revoked certificate, believing that it is still valid. Although frequently updated, nonced, and timestamped certificates can reduce the frequency and impact of such cyber-attacks, they add a huge burden to the CAs and OCSP servers. Indeed, CAs and/or OCSP servers need to timestamp and sign on a regular basis all the responses, for every certificate they have issued, resulting in a very high overhead. To mitigate this and provide a solution to the described cyber-attacks, we present CCSP : a new approach to provide timely information regarding the status of certificates, which capitalizes on a newly introduced notion called Signed Collections. In this article, we present in detail the notion of Signed Collections and the complete design, implementation, and evaluation of our approach. Performance evaluation shows that CCSP (i) reduces space requirements by more than an order of magnitude, (ii) lowers the number of signatures required by six orders of magnitude compared to OCSP-based methods, and (iii) adds only a few milliseconds of overhead in the overall user latency.  © 2020 ACM.",Certificate Revocation; CRL; HTTPS; OCSP; OCSP Stapling; PKI; TLS; WebPKI,Crime; Network security; Web browsers; Certificate revocation; Certificate revocation list; Design and implementations; Online certificate status protocol; Orders of magnitude; Revocation checks; Secure sockets layers; Space requirements; Computer crime
Descriptions from the customers: Comparative analysis of review-based product description generation methods,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095970862&doi=10.1145%2f3418202&partnerID=40&md5=371277b7b126b5779062ed1b9944830d,"Product descriptions play an important role in the e-commerce ecosystem. Yet, on leading e-commerce websites product descriptions are often lacking or missing. In this work, we suggest to overcome these issues by generating product descriptions from user reviews. We identify the set of candidates using a supervised approach that extracts review sentences in their original form, diversifies them, and selects the top candidates. We present extensive analyses of the generated descriptions, including a comparison to the original descriptions and examination of review coverage. We also perform an A/B test that demonstrates the impact of presenting our descriptions on user traffic.  © 2020 ACM.",Deep multi-task leaning; electronic commerce; language generation; user-generated content,Internet; Comparative analysis; E-commerce ecosystems; E-commerce websites; Generation method; Product descriptions; User reviews; User traffics; Electronic commerce
Trustworthy and transparent third-party authority,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095966259&doi=10.1145%2f3386262&partnerID=40&md5=57f92ed1befbd4becbd85209a1b64aba,"Recent advances in cryptographic approaches, such as Functional Encryption and Attribute-based Encryption and their variants, have shown significant promise for enabling public clouds to provide secure computation and storage services for users' sensitive data. A crucial component of these approaches is a third-party authority (TPA) that must be trusted to set up public parameters, provide private key service, and so on. Components of deployed cryptographic mechanisms such as the certificate authorities (CAs), which are the TPAs of the underlying PKI for the SSL/TLS protocol, have faced several types of attacks (e.g., stealthy targeted and censorship attacks), and certificate mis-issuance problems. Such practical challenges indicate that the successful deployment of newer emerging cryptographic schemes will also significantly depend on the trustworthiness of the TPAs. Furthermore, recently proposed decentralized TPA approaches that lower the threshold on the conditions required for an entity to become an authority can make the trust issue much worse. To address this issue, we propose an authority transparency framework to ensure the trustworthiness of TPAs of recent and emerging advanced cryptographic schemes. The framework includes a formal model and a secure logging-based approach to implement the framework. Further, to address the issues related to privacy, we also present a privacy-preserving authority transparency approach. We present security analysis and performance evaluation to show that authority transparency achieves the security and performance goals.  © 2020 ACM.",access control; audit; secure computation; Transparency; trusted authority,Digital storage; Network security; Privacy by design; Storage as a service (STaaS); Transparency; Attribute-based encryptions; Certificate authority; Cryptographic schemes; Functional encryptions; Privacy preserving; Secure computation; Security analysis; Security and performance; Cryptography
Personalized review recommendation based on users' aspect sentiment,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090842094&doi=10.1145%2f3414841&partnerID=40&md5=f504980a977cd7d928094c6f3436012f,"Product reviews play an important role in guiding users' purchase decision-making in e-commerce platforms. However, it is challenging for users to find helpful reviews that meet their preferences and experiences among an overwhelming amount of reviews. Some works have been done to recommend helpful reviews to users, either from personalized or non-personalized views. While some existing models recommend similar users' reviews for a target user, they either neglect the target user's aspect preferences or the user-product interactions for measuring user similarity. Moreover, those models predict review helpfulness at the review-level (a review is taken as a whole); few of them consider the aspect-level. To address the above issues, we propose an aspect sentiment similarity-based personalized review recommendation model (A2SPR), which quantifies review helpfulness and recommends reviews that are customized for each individual. We analyze users' aspect preferences from reviews and improve user similarity with users' fine-grained sentiment and product relevance. Furthermore, we redefine the review helpfulness score at the aspect level, which indicates the review's reference value for users' purchase decisions. Finally, we recommend the top k helpful reviews for individuals based on the review helpfulness score. To validate the performance of the proposed model, eight baselines are developed and compared. Experimental results show that our model performs better than those baselines in both the coverage and precision.  © 2020 ACM.",aspect level; personalized review recommendation; Product relevance; sentiment analysis,Internet; Fine grained; Personalized views; Product interaction; Product reviews; Purchase decision; Reference values; Review recommendations; Decision making
An intelligent edge-centric queries allocation scheme based on ensemble models,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095963516&doi=10.1145%2f3417297&partnerID=40&md5=04fc4a07ec9bad83f57ab2d627dccd97,"The combination of Internet of Things (IoT) and Edge Computing (EC) can assist in the delivery of novel applications that will facilitate end-users' activities. Data collected by numerous devices present in the IoT infrastructure can be hosted into a set of EC nodes becoming the subject of processing tasks for the provision of analytics. Analytics are derived as the result of various queries defined by end-users or applications. Such queries can be executed in the available EC nodes to limit the latency in the provision of responses. In this article, we propose a meta-ensemble learning scheme that supports the decision making for the allocation of queries to the appropriate EC nodes. Our learning model decides over queries' and nodes' characteristics. We provide the description of a matching process between queries and nodes after concluding the contextual information for each envisioned characteristic adopted in our meta-ensemble scheme. We rely on widely known ensemble models, combine them, and offer an additional processing layer to increase the performance. The aim is to result a subset of EC nodes that will host each incoming query. Apart from the description of the proposed model, we report on its evaluation and the corresponding results. Through a large set of experiments and a numerical analysis, we aim at revealing the pros and cons of the proposed scheme.  © 2020 Owner/Author.",decision making; edge computing; ensemble classification models; Internet of Things; query allocation,Decision making; Contextual information; Ensemble learning; Internet of Things (IOT); Learning models; Matching process; Novel applications; OR applications; Processing layer; Internet of things
SMig-RL: An evolutionary migration framework for cloud services based on deep reinforcement learning,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095962574&doi=10.1145%2f3414840&partnerID=40&md5=1b5ee9b0d14b611e915d7a1261c4ce2d,"Service migration is an often-used approach in cloud computing to minimize the access cost by moving the service close to most users. Although it is effective in a certain sense, the service migration in existing research still suffers from some deficiencies in its evolutionary abilities in scalability, sensitivity, and adaptability to effectively react to the dynamically changing environments. This article proposes an evolutionary framework based on deep reinforcement learning for virtual service migration in large-scale mobile cloud centers. To enhance the spatio-temporal sensitivity of the algorithm, we design a scalable reward function for virtual service migration, redefine the input state, and add a Recurrent Neural Network (RNN) to the learning framework. Additionally, in order to enhance the adaptability of the algorithm, we also decompose the action space and exploit the network cost to adjust the number of virtual machine (VMs). The experimental results show that, compared with the existing results, the migration strategy generated by the algorithm can not only significantly reduce the total service cost and achieve the load balancing at the same time, but also address the burst situations with low cost in dynamic environments.  © 2020 ACM.",Cloud computing; deep reinforcement learning; dynamic service migration; mobile access; Q-learning; RNN,Balancing; Evolutionary algorithms; Reinforcement learning; Web services; Changing environment; Dynamic environments; Evolutionary framework; Learning frameworks; Migration strategy; Recurrent neural network (RNN); Service migration; Spatio-temporal sensitivity; Recurrent neural networks
Ursa: Robust performance for nakamoto consensus with self-adaptive throughput,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095963415&doi=10.1145%2f3412341&partnerID=40&md5=d939dfea277012603132b492e6fc4b40,"With the increasing number of users in blockchain-based cryptocurrencies, the public has raised the demand for transaction throughput, and many protocols are designed to improve the throughput following the Nakamoto consensus. Although astonishing progress has been made in the on-chain throughput improvement, high throughput makes the blockchains suffer from the increasing blockchain size, hard forks, and possible attacks. In this work, we propose a quantitative model to describe and analyze the Nakamoto consensus. We then design a robust scheme named Ursa to reduce storage requirements and to reduce the forks by automatically adjusting block size according to users' needs.  © 2020 ACM.",consistency; decentralization; Nakamoto consensus; throughput,Internet; Block sizes; High throughput; Quantitative modeling; Robust performance; Storage requirements; Throughput improvement; Transaction throughput; Blockchain
Optimally Self-Healing IoT Choreographies,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092373553&doi=10.1145%2f3386361&partnerID=40&md5=027ee10132cbdad2b86e7f1aa68ccd84,"In the industrial Internet of Things domain, applications are moving from the Cloud into the Edge, closer to the devices producing and consuming data. This means that applications move from the scalable and homogeneous Cloud environment into a potentially constrained heterogeneous Edge network. Making Edge applications reliable enough to fulfill Industry 4.0 use cases remains an open research challenge. Maintaining operation of an Edge system requires advanced management techniques to mitigate the failure of devices. This article tackles this challenge with a twofold approach: (1) a policy-enabled failure detector that enables adaptable failure detection and (2) an allocation component for the efficient selection of failure mitigation actions. The parameters and performance of the failure detection approach are evaluated, and the performance of an energy-efficient allocation technique is measured. Finally, a vision for a complete system and an example use case are presented.  © 2020 ACM.",failure detection; IOT; optimization,Energy efficiency; Self-healing materials; Cloud environments; Complete system; Energy efficient; Failure detection; Failure Detectors; Failure mitigation; Management techniques; Research challenges; Industrial internet of things (IIoT)
Cloud-based Enabling Mechanisms for Container Deployment and Migration at the Network Edge,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092380756&doi=10.1145%2f3380955&partnerID=40&md5=26067d33b165fe04e3cc4eb805fb6bd4,"In recent years, a new trend of advanced applications with huge demands in terms of Quality of Service (QoS) is gaining ground. Even though Cloud computing provides mature management facilities with ubiquitous capabilities, novel requirements and workloads, foisted by new services, start to expose its weaknesses. In this context, a new Information and Communication Technologies (ICT) trend aims at pushing computation from the Cloud to be much close as possible to data sources, raising in the evolution of new paradigms namely Fog and Mist computing. Specifically, the Fog computing paradigm exploits powerful nodes such as servers, routers, and cloudlets that are coupled with the end devices or their access networks accordingly; they are ""relatively""close by the data sources. Whereas Mist computing, which is a lightweight form of Fog computing, pushes the resources even closer. Precisely, Mist computing uses particular nodes that could reside within the same network (e.g., Local Area Network (LAN)) as the end-devices. Considering the advancement that the hardware is knowing nowadays, Fog and Mist nodes are seen suitable to provide resources such as processing, storage, and networking in the proximity of data sources; thereby, the requirements of the new services could be met. Together with the Cloud, the Fog and Mist paradigms introduce a stacked architecture for data processing where a data pre-processing could be performed at the Mist level, then offloaded vertically to the upper layers (i.e., Fog nodes or the Cloud). In these circumstances, it is fundamental to build a management system able to provision efficiently the Fog/Mist-based applications. For this purpose, the Operating System (OS)-level virtualization using containerization technologies, considering its light footprint, fits as a suitable solution to provide Fog/Mist services. The industrial-grade Cloud middlewares, such as OpenStack, which is a reference architecture for Infrastructure-as-a-Service solutions, are still far away from incorporating this new trend. This article proposes an OpenStack-based middleware platform through which containers can be deployed/managed at the Fog/Mist levels.  © 2020 ACM.",Cloud computing; containers; edge computing; fog computing; IoT; OpenStack,Computer architecture; Containers; Data handling; Digital storage; Fog; Infrastructure as a service (IaaS); Local area networks; Middleware; Network architecture; Platform as a Service (PaaS); Quality of service; Storage as a service (STaaS); Ubiquitous computing; Advanced applications; Computing paradigm; Data preprocessing; Local area networks (LAN); Management systems; Middleware platforms; New information and communication technologies; Reference architecture; Fog computing
Cracking Channel Hopping Sequences and Graph Routes in Industrial TSCH Networks,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092374940&doi=10.1145%2f3372881&partnerID=40&md5=8f0a9d3bccbbc165d6fbce600da90ad3,"Industrial networks typically connect hundreds or thousands of sensors and actuators in industrial facilities, such as manufacturing plants, steel mills, and oil refineries. Although the typical industrial Internet of Things (IoT) applications operate at low data rates, they pose unique challenges because of their critical demands for reliable and real-time communication in harsh industrial environments. IEEE 802.15.4-based wireless sensor-actuator networks (WSANs) technology is appealing for use to construct industrial networks because it does not require wired infrastructure and can be manufactured inexpensively. Battery-powered wireless modules easily and inexpensively retrofit existing sensors and actuators in industrial facilities without running cables for communication and power. To address the stringent real-time and reliability requirements, WSANs made a set of unique design choices such as employing the Time-Synchronized Channel Hopping (TSCH) technology. These designs distinguish WSANs from traditional wireless sensor networks (WSNs) that require only best effort services. The function-based channel hopping used in TSCH simplifies the network operations at the cost of security. Our study shows that an attacker can reverse engineer the channel hopping sequences and graph routes by silently observing the transmission activities and put the network in danger of selective jamming attacks. The cracked knowledge on the channel hopping sequences and graph routes is an important prerequisite for launching selective jamming attacks to TSCH networks. To our knowledge, this article represents the first systematic study that investigates the security vulnerability of TSCH channel hopping and graph routing under realistic settings. In this article, we demonstrate the cracking process, present two case studies using publicly accessible implementations (developed for Orchestra and WirelessHART), and provide a set of insights.  © 2020 ACM.",graph routing; IEEE 802.15.4e; industrial wireless sensor-actuator networks; selective jamming attack; Time-synchronized channel hopping,Actuators; IEEE Standards; Industrial internet of things (IIoT); Jamming; Best effort services; Industrial environments; Industrial facilities; Real-time communication; Reliability requirements; Security vulnerabilities; Sensors and actuators; Wireless sensor network (WSNs); Wireless sensor networks
Fog in the Clouds: UAVs to Provide Edge Computing to IoT Devices,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092400415&doi=10.1145%2f3382756&partnerID=40&md5=563ba4febce0c5bc88352b70297bba59,"Internet of Things (IoT) has emerged as a huge paradigm shift by connecting a versatile and massive collection of smart objects to the Internet, coming to play an important role in our daily lives. Data produced by IoT devices can generate a number of computational tasks that cannot be executed locally on the IoT devices. The most common solution is offloading these tasks to external devices with higher computational and storage capabilities, usually provided by centralized servers in remote clouds or on the edge by using the fog computing paradigm. Nevertheless, in some IoT scenarios there are remote or challenging areas where it is difficult to connect an IoT network to a fog platform with appropriate links, especially if IoT devices produce a lot of data that require processing in real-time. To this purpose, in this article, we propose to use unmanned aerial vehicles (UAVs) as fog nodes. Although this idea is not new, this is the first work that considers power consumption of the computing element installed on board UAVs, which is crucial, since it may influence flight mission duration. A System Controller (SC) is in charge of deciding the number of active CPUs at runtime by maximizing an objective function weighing power consumption, job loss probability, and processing latency. Reinforcement Learning (RL) is used to support SC in its decisions. A numerical analysis is carried out in a use case to show how to use the model introduced in the article to decide the computation power of the computing element in terms of number of available CPUs and CPU clock speed, and evaluate the achieved performance gain of the proposed framework.  © 2020 ACM.",energy efficiency; fog computing; Internet of Things; performance evaluation; Reinforcement Learning,Antennas; Data handling; Digital storage; Edge computing; Electric power utilization; Employment; Fog; Fog computing; Green computing; Program processors; Reinforcement learning; Centralized server; Computational task; Computing element; Computing paradigm; Internet of Things (IOT); Objective functions; Storage capability; System controllers; Internet of things
Introduction to the Special Issue on Evolution of IoT Networking Architectures,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092401106&doi=10.1145%2f3406087&partnerID=40&md5=0b3bdc7193cfe1e83a22e5cbdafe45a3,[No abstract available],,
Parameter Self-Adaptation for Industrial Wireless Sensor-Actuator Networks,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092340544&doi=10.1145%2f3388240&partnerID=40&md5=5f7d9f4523dd90dfc632607d735a534c,"Wireless sensor-actuator network (WSAN) technology is gaining rapid adoption by industrial Internet of Things applications in recent years. A WSAN typically connects sensors, actuators, and controllers in industrial facilities, such as steel mills, oil refineries, chemical plants, and infrastructures implementing complex monitoring and control processes. IEEE 802.15.4-based WSANs operate at low power and can be manufactured inexpensively, which makes them ideal where battery lifetime and costs are important. Recent studies have shown that the selection of network parameters has a significant effect on network performance. However, the current practice of parameter selection is largely based on experience and rules of thumb involving a coarse-grained analysis of expected network load and dynamics or measurements during a few field trials, resulting in non-optimal decisions in many cases. In this work, we develop P-SAFE (Parameter Selection and Adaptation FramEwork), which optimally selects the network parameters based on the application quality-of-service demands and adapts the parameter configuration at runtime to consistently satisfy the dynamic requirements. We implement P-SAFE and evaluate it on three physical testbeds. Experimental results show that our solution can significantly better meet the application quality-of-service demand compared to the state of the art.  © 2020 ACM.",Industrial wireless sensor-actuator networks; parameter selection,Actuators; Chemical plants; IEEE Standards; Industrial internet of things (IIoT); Quality of service; Adaptation framework; Application quality; Complex monitoring; Current practices; Industrial facilities; Industrial wireless sensors; Network parameters; Parameter selection; Wireless sensor networks
Low-cost Security for Next-generation IoT Networks,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092357149&doi=10.1145%2f3406280&partnerID=40&md5=11e240361fccc82da953877b41ec4560,"In recent years, the ubiquitous nature of Internet-of-Things (IoT) applications as well as the pervasive character of next-generation communication protocols, such as the 5G technology, have become widely evident. In this work, we identify the need for low-cost security in current and next-generation IoT networks and address this demand through the implementation, testing, and validation of an intrinsic low-cost and low-overhead hardware-based security primitive within an inherent network component. In particular, an intrinsic Physical Unclonable Function (PUF) is implemented in the peripheral network module of a tri-band commercial off-the-shelf router. Subsequently, we demonstrate the robustness of this PUF to ambient temperature variations and to limited natural aging, and examine in detail its potential for securing the next generation of IoT networks and other applications. Finally, the security of the proposed PUF-based schemes is briefly assessed and discussed.  © 2020 ACM.",60 GHz; Internet of Things (IoT); peripheral; physical unclonable function (PUF); static random access memory (SRAM),5G mobile communication systems; Costs; Hardware security; Network security; Next generation networks; Temperature; Internet of Things (IOT); IOT networks; Low costs; Low overhead; Natural aging; Physical unclonable functions (PUF); Temperature variation; Tri-bands; Internet of things
IoT Architecture for Urban Data-Centric Services and Applications,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092415636&doi=10.1145%2f3396850&partnerID=40&md5=bf71b430b605029758c864ab1d2b68dc,"In this work, we describe an urban Internet of Things (IoT) architecture, grounded in big data patterns and focused on the needs of cities and their key stakeholders. First, the architecture of the dedicated platform USE4IoT (Urban Service Environment for the Internet of Things), which gathers and processes urban big data and extends the Lambda architecture, is proposed. We describe how the platform was used to make IoT an enabling technology for intelligent transport planning. Moreover, key data processing components vital to provide high-quality IoT data streams in a near-real-time manner are defined. Furthermore, tests showing how the IoT platform described in this study provides a low-latency analytical environment for smart cities are included.  © 2020 ACM.",big data; data processing; Data stream; public transport,Architecture; Big data; Data streams; Data patterns; Enabling technologies; Intelligent transport; Internet of Things (IOT); Iot architectures; Near-real time; Services and applications; Urban services; Internet of things
To transmit or not to transmit: Controlling communications in the mobile iot domain,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092383030&doi=10.1145%2f3369389&partnerID=40&md5=0fcd3bca1c2a5877ce873cac0ce5928a,"The Mobile IoT domain has been significantly expanded with the proliferation of drones and unmanned robotic devices. In this new landscape, the communication between the resource-constrained device and the fixed infrastructure is similarly expanded to include new messages of varying importance, control, and monitoring. To efficiently and effectively control the exchange of such messages subject to the stochastic nature of the underlying wireless network, we design a time-optimized, dynamic, and distributed decision-making mechanism based on the principles of the Optimal Stopping and Change Detection theories. The findings from our experimentation platform are promising and solidly supportive to a vast spectrum of real-time and latency-sensitive applications with quality-of-service requirements in mobile IoT environments.  © 2020 ACM.",change-point detection; mobile IoT; optimal stopping theory; Real-time decision-making; unmanned vehicles,Decision making; Quality of service; Stochastic systems; Change detection; Distributed decision making; Experimentation platforms; Optimal stopping; Resourceconstrained devices; Robotic devices; Sensitive application; Stochastic nature; Internet of things
Efficient Latency Control in Fog Deployments via Hardware-Accelerated Popularity Estimation,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092392303&doi=10.1145%2f3366020&partnerID=40&md5=ffd78caa5606ea99a4c8b37ba25bb33c,"Introduced as an extension of the Cloud at the network edge for computing and storage purposes, the Fog is increasingly considered a key enabler for Internet-of-Things applications whose latency requirements are not compatible with a Cloud-only approach. Unlike Cloud platforms, which can elastically accommodate large numbers of requests, Fog deployments are usually dimensioned for an average traffic load and, thus, unable to handle sudden bursts of requests without violating latency guarantees. In this article, we address the problem of efficiently controlling Fog admission to guarantee application response time. We propose request-aware admission control (AC) strategies maximizing the number of Fog-handled requests by means of dynamic popularity estimation. In particular, the LRU-AC, an AC strategy based on online learning of the request popularity distribution via a Least Recently Used (LRU) filter, is introduced. We contribute an analytical model for assessing LRU-AC performance and quantifying the incurred reduction of Cloud offload cost, w.r.t. both an ideal oracle-based and a request-oblivious AC strategy. Further, we propose a feasible implementation design of LRU-AC on FPGA hardware using Aging Bloom Filters (ABF) to mimic the function of the LRU-AC, while providing a compact memory representation. The use of ABFs for LRU-AC is theoretically validated and verified through simulation. The current implementation shows a throughput of 16.7 Mpps and a processing latency of less than 3μ s while multiplying the Fog acceptance-rate by 10 in the evaluated scenario.  © 2020 ACM.",Edge Computing; Fog; Information-Centric Networking; NetFPGA,Internet; Acceptance rate; Cloud platforms; Hardware-accelerated; Implementation design; Latency control; Least recently used; Online learning; Popularity distribution; Fog
Optimal Receiver Placement for K-barrier Coverage in Passive Bistatic Radar Sensor Networks,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092379749&doi=10.1145%2f3377402&partnerID=40&md5=9851248a85282667d18843b1bea0441f,"The improvement of coverage quality in the construction of multiple-barrier coverage is a critical problem in a wireless sensor network. In this article, we investigate the K-barrier coverage construction problem in passive bistatic radar sensor networks. In contrast to traditional bistatic radar networks, the transmitters in a passive bistatic radar network are predeployed and noncooperative. To construct K barriers, we need to deploy receivers that couple with predeployed transmitters to build continuous barriers. In this work, we focus on the minimum number of receivers problem of constructing K-barrier coverage, where the minimum number of receivers is based on the predeployed transmitters. To handle this problem, we first investigate the optimal placement of receivers between adjacent transmitters for a sub-barrier formation and then determine the optimal placement of receivers for the one-barrier construction. For multiple-barrier coverage construction, we introduce a weighted transmitter graph (WTG) to describe the relation among different transmitters, where the weight in the graph is the minimum number of receivers needed for these two transmitters for a sub-barrier formation. Based on WTG, the minimum receivers problem changes to a problem of how to find K-disjoint paths with the minimum total weight in the graph. For large-scale networks, we also propose two efficient heuristic algorithms to solve the corresponding problem. Finally, we conduct extensive experiments to validate the correctness and the efficiency of the proposed algorithms.  © 2020 ACM.",K-barrier coverage; minimum receiver problem; Passive bistatic radar network,Heuristic algorithms; Optimization; Radar; Radar equipment; Transmitters; Barrier coverages; Barrier formation; Bistatic radar networks; Construction problem; Continuous barrier; Large-scale network; Minimum total weights; Optimal placements; Wireless sensor networks
Understanding Ethereum via Graph Analysis,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085484592&doi=10.1145%2f3381036&partnerID=40&md5=ab47ebca9ba61cdd485b6052e09544af,"Ethereum, a blockchain, supports its own cryptocurrency named Ether and smart contracts. Although more than 8M smart contracts have been deployed on Ethereum, little is known about the characteristics of its users, smart contracts, and the relationships among them. We conduct the first systematic study on Ethereum by leveraging graph analysis to characterize three major activities on Ethereum, namely money transfer, smart contract creation, and smart contract invocation. We collect all transaction data, construct three graphs from the data to characterize major activities via graph analysis, and discover new insights. Moreover, we address three security issues based on graphs. © 2020 ACM.",Blockchain; contract creation graph; contract invocation graph; Ethereum; graph analysis; money flow graph,Internet; Graph analysis; Money transfers; Security issues; Systematic study; Transaction data; Ethereum
A Multilingual Evaluation for Online Hate Speech Detection,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085511143&doi=10.1145%2f3377323&partnerID=40&md5=9fd844a649a6dc7caeaf40ccb17bf6e1,"The increasing popularity of social media platforms such as Twitter and Facebook has led to a rise in the presence of hate and aggressive speech on these platforms. Despite the number of approaches recently proposed in the Natural Language Processing research area for detecting these forms of abusive language, the issue of identifying hate speech at scale is still an unsolved problem. In this article, we propose a robust neural architecture that is shown to perform in a satisfactory way across different languages; namely, English, Italian, and German. We address an extensive analysis of the obtained experimental results over the three languages to gain a better understanding of the contribution of the different components employed in the system, both from the architecture point of view (i.e., Long Short Term Memory, Gated Recurrent Unit, and bidirectional Long Short Term Memory) and from the feature selection point of view (i.e., ngrams, social network-specific features, emotion lexica, emojis, word embeddings). To address such in-depth analysis, we use three freely available datasets for hate speech detection on social media in English, Italian, and German. © 2020 ACM.",Hate speech detection; multilingual data; social media; text classification,Brain; Long short-term memory; Memory architecture; Natural language processing systems; Network architecture; Social networking (online); Facebook; In-depth analysis; NAtural language processing; Neural architectures; Social media; Social media platforms; Speech detection; Unsolved problems; Speech recognition
Detecting Misogyny and Xenophobia in Spanish Tweets Using Language Technologies,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085516040&doi=10.1145%2f3369869&partnerID=40&md5=ef52518cee2a8184c3cfd9f821e19306,"Today, misogyny and xenophobia are some of the most important social problems. With the increase in the use of social media, this feeling of hatred toward women and immigrants can be more easily expressed, and therefore it can have harmful effects on social media users. For this reason, it is important to develop systems capable of detecting hateful comments automatically. In this article, we analyze the hate speech in Spanish tweets against women and immigrants conducting classification experiments using different approaches. Moreover, we create appropriate language resources for hate speech detection in Spanish. © 2020 ACM.",classifier ensemble; hate speech classification; lexicon; machine learning; Misogyny detection; social media; text mining; xenophobia detection,Internet; Harmful effects; Language resources; Language technology; Social media; Social problems; Speech detection; Social networking (online)
Introduction to the Special Section on Computational Modeling and Understanding of Emotions in Conflictual Social Interactions,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085522950&doi=10.1145%2f3392334&partnerID=40&md5=acffdc5e17d308bdfc96e8fa267f1a9f,[No abstract available],,
A Cost-Efficient Container Orchestration Strategy in Kubernetes-Based Cloud Computing Infrastructures with Heterogeneous Resources,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085479823&doi=10.1145%2f3378447&partnerID=40&md5=d836b106fff608206434bde56f55841f,"Containers, as a lightweight application virtualization technology, have recently gained immense popularity in mainstream cluster management systems like Google Borg and Kubernetes. Prevalently adopted by these systems for task deployments of diverse workloads such as big data, web services, and IoT, they support agile application deployment, environmental consistency, OS distribution portability, application-centric management, and resource isolation. Although most of these systems are mature with advanced features, their optimization strategies are still tailored to the assumption of a static cluster. Elastic compute resources would enable heterogeneous resource management strategies in response to the dynamic business volume for various types of workloads. Hence, we propose a heterogeneous task allocation strategy for cost-efficient container orchestration through resource utilization optimization and elastic instance pricing with three main features. The first one is to support heterogeneous job configurations to optimize the initial placement of containers into existing resources by task packing. The second one is cluster size adjustment to meet the changing workload through autoscaling algorithms. The third one is a rescheduling mechanism to shut down underutilized VM instances for cost saving and reallocate the relevant jobs without losing task progress. We evaluate our approach in terms of cost and performance on the Australian National Cloud Infrastructure (Nectar). Our experiments demonstrate that the proposed strategy could reduce the overall cost by 23% to 32% for different types of cloud workload patterns when compared to the default Kubernetes framework. © 2020 ACM.",Cluster management; container orchestration; cost efficiency; resource heterogeneity,Cloud computing; Containers; Costs; Web services; Application deployment; Cloud computing infrastructures; Cloud infrastructures; Cluster management system; Heterogeneous resources; Lightweight application; Optimization strategy; Resource utilizations; Information management
Emo2Vec: Learning Emotional Embeddings via Multi-Emotion Category,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085480577&doi=10.1145%2f3372152&partnerID=40&md5=de5d9356ab1657fd956457a91a45cac5,"Sentiment analysis or opinion mining for subject information extraction from the text has become more and more dependent on natural language processing, especially for business and healthcare, since the online products and service reviews affect the consuming behaviors. Word embeddings that can map the words to low-dimensional vector representations have been widely used in natural language processing tasks. But the word embeddings based on context such as Word2Vec and GloVe fail to capture the sentiment information. Most of existing sentiment analysis methods incorporate emotional polarity (positive and negative) to improve the sentiment embeddings for the emotion classification. This article takes advantage of an emotional psychology model to learn the emotional embeddings in Chinese first. In order to combine the semantic space and an emotional space, we present two different purifying models from local (LPM) and global (GPM) perspectives based on Plutchik's wheel of emotions to add the emotional information into word vectors. The two models aim to improve the word vectors so that not only the semantically similar words but also the sentimentally similar words can be closer than before. The Plutchik's wheel of emotions model can give eight-dimensional vector for one word in emotional space that can capture more sentiment information than the binary polarity labels. The obvious advantage of the local purifying model is that it can be fit for any pretrained word embeddings. For the global purifying model, we can get the final emotional embeddings at once. These models have been extended to handle English texts. The experimental results on Chinese and English datasets show that our purifying model can improve the conventional word embeddings and some proposed sentiment embeddings for sentiment classification and multi-emotion classification. © 2020 ACM.",emotional embeddings; multi-emotion classification; Plutchik's wheel of emotions; purified word vectors; sentiment classification; Word Embeddings,Consumer behavior; Embeddings; Semantics; Sentiment analysis; Text mining; Vector spaces; Vectors; Wheels; Dimensional vectors; Emotion classification; Emotional information; Emotions modeling; Low dimensional; NAtural language processing; Psychology Model; Sentiment classification; Classification (of information)
An Emotional Analysis of False Information in Social Media and News Articles,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085517035&doi=10.1145%2f3381750&partnerID=40&md5=532be3e67579d8209e0fd9e272e9be86,"Fake news is risky, since it has been created to manipulate readers' opinions and beliefs. In this work, we compared the language of false news to the real one of real news from an emotional perspective, considering a set of false information types (propaganda, hoax, clickbait, and satire) from social media and online news article sources. Our experiments showed that false information has different emotional patterns in each of its types, and emotions play a key role in deceiving the reader. Based on that, we proposed an LSTM neural network model that is emotionally infused to detect false news. © 2020 ACM.",emotional analysis; Fake news; false information; suspicious news,Long short-term memory; Emotional analysis; Emotional patterns; Information types; Neural network model; News articles; Online news; Social media; Social networking (online)
MANDOLA: A Big-Data Processing and Visualization Platform for Monitoring and Detecting Online Hate Speech,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085531234&doi=10.1145%2f3371276&partnerID=40&md5=c5ec28d7dea78ba6ff13a6aa3f1857ff,"In recent years, the increasing propagation of hate speech in online social networks and the need for effective counter-measures have drawn significant investment from social network companies and researchers. This has resulted in the development of many web platforms and mobile applications for reporting and monitoring online hate speech incidents. In this article, we present MANDOLA, a big-data processing system that monitors, detects, visualizes, and reports the spread and penetration of online hate-related speech using big-data approaches. MANDOLA consists of six individual components that intercommunicate to consume, process, store, and visualize statistical information regarding hate speech spread online. We also present a novel ensemble-based classification algorithm for hate speech detection that can significantly improve the performance of MANDOLA's ability to detect hate speech. To present the functionality and usability of our system, we present a use case scenario of real-life event annotation and data correlation. As shown from the performance of the individual modules, as well as the usability and functionality of the whole system, MANDOLA is a powerful system for reporting and monitoring online hate speech. © 2020 ACM.",big-data processing platform; deep learning; Hate speech; online social networks; system approach,Audio signal processing; Big data; Data handling; Data visualization; Monitoring; Social networking (online); Speech; Classification algorithm; Data processing systems; Individual components; Mobile applications; Monitoring on line; On-line social networks; Statistical information; Visualization platforms; Speech recognition
Sub-Population Specific Models of Couples' Conflict,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085493341&doi=10.1145%2f3372045&partnerID=40&md5=4eb1eaeabe75dac34c0c05f4c4f5a6ca,"Interpersonal conflict between couples is a significant source of stress with long-lasting effects on partners' physical and psychological health. Motivated by findings in psychological science, we study how couples with distinct relationship functioning characteristics experience conflict in real life. We propose sub-population specific machine learning models using hierarchical and adaptive learning frameworks to automatically detect interpersonal conflict through the ambulatory monitoring of couples' physiological signals, audio samples, and linguistic indices. Results indicate that the proposed models outperform a general model learned for the entire population and separate models independently trained on each sub-population, providing a foundation toward personalized health applications. © 2020 ACM.",ambulatory monitoring; feedforward neural network; Interpersonal conflict; multi-task learning; sub-population specific models,Internet; Adaptive learning; Ambulatory monitoring; General model; Machine learning models; Personalized healths; Physiological signals; Psychological health; Sub-populations; Physiological models
Exploiting Proxy Sensing for Efficient Monitoring of Large-Scale Sensor Networks,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085526285&doi=10.1145%2f3376919&partnerID=40&md5=edaaa4f86130fa017fa9e88dec795e0b,"Large networks of IoT devices, each consisting of one or more sensors, are being increasingly deployed for comprehensive real-time monitoring of cyber-physical systems. Such networks form an essential component of the emerging edge computing paradigm and are expected to increase in complexity and size. The physical phenomenon sensed by different sensors (within the same or different IoT devices in close proximity) often have relationships that makes them correlated. This is a form of proxy sensing that can be exploited for achieving better energy efficiency and higher robustness in monitoring. In this article, we explore how a set of sensors can optimize its data collection rates efficiently in a semi-distributed manner and yet provide the advantages of autonomy, relative isolation, and distributed control that is essential in a large-scale network. © 2020 ACM.",adaptive sampling; Edge computing; heterogeneous sensing; proxy sensing; rate adaptation,Distributed parameter control systems; Embedded systems; Energy efficiency; Real time systems; Sensor networks; Computing paradigm; Distributed control; Efficient monitoring; Large scale sensor network; Large-scale network; Physical phenomena; Real time monitoring; Semi-distributed; Internet of things
An Experimental Analysis of Security Vulnerabilities in Industrial IoT Devices,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085393294&doi=10.1145%2f3379542&partnerID=40&md5=4ab2dc33b68dd5da69167f198e89a437,"The revolutionary development of the Internet of Things has triggered a huge demand for Internet of Things devices. They are extensively applied to various fields of social activities, and concerning manufacturing, they are a key enabling concept for the Industry 4.0 ecosystem. Industrial Internet of Things (IIoT) devices share common vulnerabilities with standard IoT devices, which are increasingly exposed to the attackers. As such, connected industrial devices may become sources of cyber, as well as physical, threats for people and assets in industrial environments. In this work, we examine the attack surfaces of a networked embedded system, composed of devices representative of those typically used in the IIoT field. We carry on an analysis of the current state of the security of IIoT technologies. The analysis guides the identification of a set of attack vectors for the examined networked embedded system. We set up the corresponding concrete attack scenarios to gain control of the system actuators and perform some hazardous operations. In particular, we propose a couple of variations of Mirai attack specifically tailored for attacking industrial environments. Finally, we discuss some possible. © 2020 ACM.",Industrial IoT security; mirai attack; stealthy attack,Embedded systems; Attack scenarios; Experimental analysis; Hazardous operations; Industrial devices; Industrial environments; Networked embedded systems; Security vulnerabilities; Social activities; Industrial internet of things (IIoT)
Cloud Deployment Tradeoffs for the Analysis of Spatially Distributed Internet of Things Systems,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085472524&doi=10.1145%2f3381452&partnerID=40&md5=df12209acd59d939d429e7c3502e769f,"Internet-enabled devices operating in the physical world are increasingly integrated in modern distributed systems. We focus on systems where the dynamics of spatial distribution is crucial; in such cases, devices may need to carry out complex computations (e.g., analyses) to check satisfaction of spatial requirements. The requirements are partly global - as the overall system should achieve certain goals - and partly individual, as each entity may have different goals. Assurance may be achieved by keeping a model of the system at runtime, monitoring events that lead to changes in the spatial environment, and performing requirements analysis. However, computationally intensive runtime spatial analysis cannot be supported by resource-constrained devices and may be offloaded to the cloud. In such a scenario, multiple challenges arise regarding resource allocation, cost, performance, among other dimensions. In particular, when the workload is unknown at the system's design time, it may be difficult to guarantee application-service-level agreements, e.g., on response times. To address and reason on these challenges, we first instantiate complex computations as microservices and integrate them to an IoT-cloud architecture. Then, we propose alternative cloud deployments for such an architecture - based on virtual machines, containers, and the recent Functions-as-a-Service paradigm. Finally, we assess the feasibility and tradeoffs of the different deployments in terms of scalability, performance, cost, resource utilization, and more. We adopt a workload scenario from a known dataset of taxis roaming in Beijing, and we derive other workloads to represent unexpected request peaks and troughs. The approach may be replicated in the design process of similar classes of spatially distributed IoT systems. © 2020 ACM.",,Commerce; Computer architecture; Spatial distribution; Application services; Cloud architectures; Complex computation; Requirements analysis; Resource utilizations; Resourceconstrained devices; Spatial environments; Spatial requirements; Internet of things
Beyond Artificial Reality: Finding and Monitoring Live Events from Social Sensors,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081636413&doi=10.1145%2f3374214&partnerID=40&md5=8551b00dd14479619a9e0ff352f4f7d7,"With billions of active social media accounts and millions of live video cameras, live new big data offer many opportunities for smart applications. However, the main consumers of the new big data have been humans. We envision the research on live knowledge, to automatically acquire real-time, validated, and actionable information. Live knowledge presents two significant and diverging technical challenges: big noise and concept drift. We describe the EBKA (evidence-based knowledge acquisition) approach, illustrated by the LITMUS landslide information system. LITMUS achieves both high accuracy and wide coverage, demonstrating the feasibility and promise of EBKA approach to achieve live knowledge. © 2020 Association for Computing Machinery. All rights reserved.",Artificial reality; concept drift; evidencebased knowledge acquisition; live knowledge; real-time event detection; true novelty,Big data; Knowledge acquisition; Virtual reality; Concept drifts; Evidence-based; live knowledge; Real time; true novelty; Video cameras
Mobile Pervasive Augmented Reality Systems MPARS: The Role of User Preferences in the Perceived Quality of Experience in Outdoor Applications,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081577974&doi=10.1145%2f3375458&partnerID=40&md5=cdb1caf0f14d948ea91104840a0e2aae,"After briefly introducing aspects concerning Mobile Augmented Reality Systems, this article delves into the evolution of these systems as pervasive technology. The work debates also on acceptance of this technology in the context of outdoor applications. The need to develop context-aware, close-to-real-time feedback mechanisms that take into consideration a continuous measurement of Quality of Experience is also discussed. For this purpose, the work goes over how to integrate user preferences into context-aware feedback systems, proposing a theoretical model for measuring Quality of Experience. The model is derived from an analysis of previous technology adoption models and incorporates the knowledge of user preferences. This knowledge has been gathered via a public questionnaire. © 2020 Association for Computing Machinery.",,Augmented reality; Augmented reality systems; Continuous measurements; Mobile augmented reality; Pervasive technologies; Quality of experience (QoE); Real-time feedback; Technology adoption models; Theoretical modeling; Quality of service
A Smartphone-based Network Architecture for Post-disaster Operations Using WiFi Tethering,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081557055&doi=10.1145%2f3372145&partnerID=40&md5=bc6e7e1395c9e82151c7f72043fc50b3,"Electronic communication is crucial for monitoring the rescue-relief operations and providing assistance to the affected people during and after disasters. Given the ubiquity of smartphones, we envision that smartphones with lost connection (due to damage) to the communications infrastructure are nevertheless integrated seamlessly into the network as far as possible. To achieve this, we propose to build ad hoc subnetworks of disconnected smartphones using theWiFi tethering technology and ultimately connect them to either the emergency communication equipment deployed in the disaster area or to other smartphones that have still the network connectivity. The proposed architecture for such integration and a defined software-based control through the emergency control center (ECC) enables battery aware collection of critical data through smartphone sensors. The developed solution supports mobility of all smartphones, including those that have lost direct cellular connectivity as well as those that have not and are willing to act as gateways. We demonstrate how the proposed scheme can be tied to the standardized wireless emergency alert service and how it can effectively handle mobility tolerant device discovery and data transfer. © 2020 Association for Computing Machinery. All rights reserved.",ad-hoc network; Disaster communications; grid-based quorum; WiFi tethering,Ad hoc networks; Data transfer; Disasters; Smartphones; Wi-Fi; Wireless local area networks (WLAN); Cellular connectivity; Communications infrastructure; Disaster communications; Electronic communications; Emergency communication; Grid-based; Network connectivity; Proposed architectures; Network architecture
Five challenges in cloud-enabled intelligence and control,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081544929&doi=10.1145%2f3366021&partnerID=40&md5=c8c0cfaa8e6ecb95ef0fc133e1471f23,"The proliferation of connected embedded devices, or the Internet of Things (IoT), together with recent advances in machine intelligence, will change the profile of future cloud services and introduce a variety of new research problems, both in cloud applications and infrastructure layers. These problems are centered around empowering individually resource-limited devices to exhibit intelligent behavior, both in sensing and control, thanks to a judicious utilization of cloud resources. Cloud services will enable learning from data, perform inference, and execute control, all with assurances on outcomes. This article discusses such emerging services and outlines five resulting new research directions towards enabling and optimizing intelligent, cloud-assisted sensing and control in the age of the Internet of Things. © 2020 Association for Computing Machinery. All rights reserved.",deep learning; edge intelligence; intelligent control; Internet of things,Deep learning; Distributed database systems; Intelligent control; Web services; Cloud applications; Edge intelligence; Embedded device; Intelligent behavior; Internet of thing (IOT); Machine intelligence; Research problems; Resource-limited devices; Internet of things
Privacy-preserving network path validation,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081604321&doi=10.1145%2f3372046&partnerID=40&md5=d3bdab6e0d34294057e09b39f7434f90,"The end-users communicating over a network path currently have no control over the path. For a better quality of service, the source node often opts for a superior (or premium) network path to send packets to the destination node. However, the current Internet architecture provides no assurance that the packets indeed follow the designated path. Network path validation schemes address this issue and enable each node present on a network path to validate whether each packet has followed the specific path so far. In this work, we introduce two notions of privacy—path privacy and index privacy—in the context of network path validation. We show that, in case a network path validation scheme does not satisfy these two properties, the scheme is vulnerable to certain practical attacks (that affect the privacy, reliability, neutrality and quality of service offered by the underlying network). To the best of our knowledge, ours is the first work that addresses privacy issues related to network path validation. We design PrivNPV, a privacy-preserving network path validation protocol, that satisfies both path privacy and index privacy.We discuss several attacks related to network path validation and how PrivNPV defends against these attacks. Finally, we discuss the practicality of PrivNPV based on relevant parameters. © 2020 Association for Computing Machinery. All rights reserved.",index privacy; Network path validation; path privacy; source authentication,Internet; Destination nodes; Internet architecture; Network paths; Privacy issue; Privacy preserving; Source authentication; Underlying networks; Validation protocols; Quality of service
Internet technology outlook: From communication to storage and cognitive computing,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079484913&doi=10.1145%2f3378661&partnerID=40&md5=5ffc3622987315e54b59905bc083c0cd,[No abstract available],Cognitive computing; Communication; Internet technology; Storage,
SSL-SVD: Semi-supervised learning-based sparse trust recommendation,2020,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079477353&doi=10.1145%2f3369390&partnerID=40&md5=b48c656fb119c0a67e414eca2ef6e89f,"Recommendation systems have been widely used in large e-commerce websites, but cold start and data sparsity seriously affect the accuracy of recommendation. To solve these problems, we propose SSL-SVD, which works to mine the sparse trust between users and improve the performance of the recommendation system. Specifically, we mine sparse trust relationships by decomposing trust impact into fine-grained factors and employing the Transductive Support Vector Machine algorithm to combine these factors. Then, we incorporate both social trust and sparse trust information into the SVD++ model, which can effectively utilize the explicit and implicit influence of trust for rating prediction in the recommendation system. Experiments show that our SSL-SVD increases the trust density degree of each dataset by more than 65% and improves the recommendation accuracy by up to 4.3%. © 2020 Association for Computing Machinery.",Recommendation system; Sparse trust; SSL-SVD; SVD++; Transductive Support Vector Machine,Semi-supervised learning; Singular value decomposition; Support vector machines; Cold start; Data sparsity; E-commerce websites; Fine grained; Recommendation accuracy; Sparse trust; Transductive support vector machine; Trust relationship; Recommender systems
Betweenness centrality based software defined routing: Observation from practical internet datasets,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075612847&doi=10.1145%2f3355605&partnerID=40&md5=9d3a625404f1b5d28573330b3bb50f8f,"Software-defined networking (SDN) enables routing control to program in the logically centralized controllers. It is expected to improve the routing efficiency even in highly dynamic situations. In this article, we make an in-depth observation of practical Internet datasets and investigate the relationship between betweenness centrality and network throughput. Furthermore, we propose a new routing observation factor, differential ratio of betweenness centrality (DRBC), to denote the varying amplitude of betweenness centrality to node degree. We reveal an interesting phenomenon that DRBC is proportional to the routing efficiency when the maximum betweenness centrality varies in a small range. Based on this, a DRBC-based routing scheme is proposed to improve routing efficiency. The experimental results verify that DRBC-based routing can improve the network throughput and accelerate the routing optimization. © 2019 Copyright held by the owner/author(s).",Betweenness centrality; Internet datasets; Routing adjustment; Software defined networking,Software defined networking; Betweenness centrality; Centralized controllers; Differential ratio; Network throughput; Routing adjustment; Routing efficiency; Routing optimization; Software defined networking (SDN); Efficiency
A pure visual approach for automatically extracting and aligning structuredweb data,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075625230&doi=10.1145%2f3365376&partnerID=40&md5=c4464211886a61f4401c3d07dcfa5da3,"Database-driven websites and the amount of data stored in their databases are growing enormously. Web databases retrieve relevant information in response to users' queries; the retrieved information is encoded in dynamically generatedweb pages as structured data records. Identifying and extracting retrieved data records is a fundamental task for many applications, such as competitive intelligence and comparison shopping. This task is challenging due to the complex underlying structure of such web pages and the existence of irrelevant information. Numerous approaches have been introduced to address this problem, but most of them are HTML-dependent solutions that may no longer be functional with the continuous development of HTML. Although a few vision-based techniques have been introduced, various issues exist that inhibit their performance. To overcome this, we propose a novel visual approach, i.e., programming-language-independent, for automatically extracting structured web data. The proposed approach makes full use of the natural human tendency of visual object perception and the Gestalt laws of grouping. The extraction system consists of two tasks: (1) data record extraction, where we apply three of the Gestalt laws (i.e., laws of continuity, proximity, and similarity), which are used to group the adjacently aligned visually similar data records on a web page; and (2) data item extraction and alignment, where we employ the Gestalt law of similarity, which is utilized to group the visually identical data items. Our experiments upon large-scale test sets show that the proposed system is highly effective and outperforms the two state-of-art vision-based approaches, ViDE and rExtractor. The experiments produce an average F1 score of 86.02%, which is approximately 55% and 36% better than that of ViDE and rExtractor for data record extraction, respectively; and an average F1 score of 86.19%, which is approximately 39% better than that of ViDE for data item extraction. © 2019 Association for Computing Machinery.",Block tree; Data item; Data record; Extended subtree; Gestalt laws of grouping; Information extraction,Competition; Competitive intelligence; Data recording; Data visualization; Database systems; HTML; Information retrieval; Trees (mathematics); Visual languages; Websites; Block trees; Continuous development; Data items; Database-driven Web sites; Gestalt law; Structured web datum; Sub trees; Vision-based approaches; Data mining
Trust Prediction via Matrix Factorisation,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075611505&doi=10.1145%2f3323163&partnerID=40&md5=e2f6f02870c0288485fdc3cd77097481,"In this article, we propose the PTP-MF (Pairwise Trust Prediction through Matrix Factorisation) algorithm, an approach to predicting the intensity of trust and distrust relations in Online Social Networks (OSNs). Our algorithm maps each OSN user i onto two low-dimensional vectors, namely, the trustor profile (describing her/his inclination to trust others) and the trustee profile (modelling how others perceive i as trustworthy) and it computes the trust a user i places in a user j as the dot product of trustor profile of i and the trustee profile of j. The PTP-MF algorithm incorporates also biases in trustor and trustee behaviour to make more accurate predictions. Experiments on four real-life datasets indicate that the PTP-MF algorithm significantly outperforms other methods in accuracy and it showcases a high scalability. © 2019 Association for Computing Machinery.",Online social networks; Trust prediction,Factorization; Forecasting; Accurate prediction; High scalabilities; Low dimensional; Matrix factorisation; On-line social networks; Online social networks (OSNs); Real life datasets; Trust predictions; Social networking (online)
Introduction to the special section on trust and AI,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075623547&doi=10.1145%2f3365675&partnerID=40&md5=a5ef1a10aaaef93fa4a2d95851831b27,[No abstract available],,
CAN-TM: Chain augmented naïve bayes-based trust model for reliable cloud service selection,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075621240&doi=10.1145%2f3341732&partnerID=40&md5=c8e48cb34c1fa6e2b7a18cc6f5758531,"The increasing proliferation of Cloud Services (CSs) has made the reliable CS selection problem a major challenge. To tackle this problem, this article introduces a new trust model called Chain Augmented Naïve Bayesbased Trust Model (CAN-TM). This model leverages the correlation that may exist among QoS attributes to solve many issues in reliable CS selection challenge, such as predicting missing assessments and improving accuracy of trust computing. This is achieved by combining both the n-gram Markov model and the Naïve Bayes model. Experiments are conducted to validate that our proposed CAN-TM outperforms state-of-the-art approaches. © 2019 Association for Computing Machinery.",Reliable cloud service selection; Robustness; Trust management systems,Bayesian networks; Distributed database systems; Markov processes; Robustness (control systems); Cloud service selections; Cloud services; QoS attributes; Selection problems; State-of-the-art approach; Trust computing; Trust management systems; Trust modeling; Trusted computing
Introduction to the special section on advances in internet-based collaborative technologies,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077746878&doi=10.1145%2f3361071&partnerID=40&md5=19fad8e180e66c72958560a823dd72c9,"Individuals, organizations, and government agencies are increasingly relying on Internet-enabled collaboration among distributed teams of humans, computer applications, and autonomous entities such as robots to develop products and deliver services. Technology trends in areas such as networking, data analytics, and distributed systems have significantly shifted the landscape of Internet-based collaborative tools and services. This particular special issue contains articles describing novel and innovative Internet-based collaborative technologies that leverage emerging technologies and enable seamless collaboration. © 2019 Copyright held by the owner/author(s).",Collaboration technology,Internet; Autonomous entities; Collaboration technology; Collaborative technologies; Collaborative tools; Distributed systems; Emerging technologies; Government agencies; Technology trends; Data Analytics
An incentive mechanism for crowdsourcing systems with network effects,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075604877&doi=10.1145%2f3347514&partnerID=40&md5=4858f9c027911cb2af4d3968fdc632cf,"In a crowdsourcing system, it is important for the crowdsourcer to engineer extrinsic rewards to incentivize the participants. With mobile social networking, a user enjoys an intrinsic benefit when she aligns her behavior with the behavior of others. Referred to as network effects, such an intrinsic benefit becomes more significant as more users join and contribute to the crowdsourcing system. But should a crowdsourcer design her extrinsic rewards differently when such network effects are taken into consideration? In this article, we incorporate network effects as a contributing factor to intrinsic rewards, and study its influence on the design of extrinsic rewards. We show that the number of participating users and their contributions to the crowdsourcing system evolve to a steady equilibrium, thanks to subtle interactions between intrinsic rewards due to network effects and extrinsic rewards offered by the crowdsourcer. Taken network effects into consideration, we design progressively more sophisticated extrinsic reward mechanisms, and propose new and optimal strategies for a crowdsourcer to obtain a higher utility. Through simulations and examples, we demonstrate that with our new strategies, a crowdsourcer is able to attract more participants with higher contributed efforts; and the participants gain higher utilities from both intrinsic and extrinsic rewards. © 2019 Association for Computing Machinery.",Crowdsourcing; Incentive mechanism; Intrinsic rewards; Network effects,Internet; Contributing factor; Extrinsic rewards; Incentive mechanism; Intrinsic and extrinsic rewards; Intrinsic rewards; Network effects; Optimal strategies; Subtle interaction; Crowdsourcing
Pay as your service needs: An application-driven pricing approach for the internet economics,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075602548&doi=10.1145%2f3361148&partnerID=40&md5=d0d153467103799851f625440a110586,"Various differentiated pricing schemes have been proposed for the Internet market. Aiming at replacing the traditional single-class pricing for better welfare, yet, researchers have shown that existing schemes can bring only marginal profit gain for the ISPs. In this article, we point out that a proper form of differentiated pricing for the Internet should not only consider congestion, but more importantly, it should provide application specific treatment to data delivery. Formally, we propose an ""application-driven pricing"" approach, where an ISP offers a number of service classes in terms of a guaranteed quality of service and announces a unit usage price for each class, and content providers are free to choose which class to use depending on the requirement of their applications. Unlike previous studies, we point out that the revenue gain of multi-class pricing under our scheme can be significant. This is because we capture important aspects of application heterogeneity and take the quality of service and price as control knobs. We identify key factors that impact the revenue gain and reveal fundamental understandings on when and why an application-driven multi-class pricing can significantly increase the revenue of ISPs. © 2019 Association for Computing Machinery.",Application driven pricing; Quality of service; Revenue maximization,Costs; Internet service providers; Quality of service; Telecommunication services; Application specific; Content providers; Guaranteed quality; Internet economics; Internet markets; Number of services; Pricing scheme; Revenue maximization; Economics
"Social network de-anonymization: More adversarial knowledge, more users re-identified?",2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074862508&doi=10.1145%2f3310363&partnerID=40&md5=66354a7268829c90874960190617c89b,"Previous works on social network de-anonymization focus on designing accurate and efficient deanonymization methods. We attempt to investigate the intrinsic relationship between the attacker's knowledge and the expected de-anonymization gain. A common intuition is that more knowledge results in more successful de-anonymization. However, our analysis shows this is not necessarily true if the attacker uses the full background knowledge for de-anonymization. Our findings leave intriguing implications for the attacker to make better use of the background knowledge for de-anonymization and for the data owners to better measure the privacy risk when releasing their data to third parties. © 2019 Association for Computing Machinery.",Adversarial knowledge; Background knowledge; De-anonymization; Quantification; Social network,Internet; Social networking (online); Adversarial knowledge; Anonymization; Back-ground knowledge; Deanonymization; Privacy risks; Quantification; Third parties; Risk assessment
CloseUp-A community-driven live online search engine,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074869202&doi=10.1145%2f3301442&partnerID=40&md5=7721931bd0ac2efa2b6872c1d9ce8671,"Search engines are still the most common way of finding information on the Web. However, they are largely unable to provide satisfactory answers to time- and location-specific queries. Such queries can best and often only be answered by humans that are currently on-site. Although online platforms for community question answering are very popular, very few exceptions consider the notion of users' current physical locations. In this article, we present CloseUp, our prototype for the seamless integration of community-driven live search into a Google-like search experience. Our efforts focus on overcoming the defining differences between traditional Web search and community question answering, namely the formulation of search requests (keyword-based queries vs. well-formed questions) and the expected response times (milliseconds vs. minutes/hours). To this end, the system features a deep learning pipeline to analyze submitted queries and translate relevant queries into questions. Searching users can submit suggested questions to a community of mobile users. CloseUp provides a stand-alone mobile application for submitting, browsing, and replying to questions. Replies from mobile users are presented as live results in the search interface. Using a field study, we evaluated the feasibility and practicability of our approach. © 2019 Association for Computing Machinery.",Collaborative service; Community question answering; Crowdsourcing; Live online search; Query transformation; Social computing,Crowdsourcing; Deep learning; Mobile telecommunication systems; Online searching; Collaborative services; Community question answering; Mobile applications; Physical locations; Query transformations; Seamless integration; Search interfaces; Social computing; Search engines
Source-aware crisis-relevant tweet identification and key information summarization,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074846900&doi=10.1145%2f3300229&partnerID=40&md5=f6837e5ab0351768bc486c9fb630a971,"Twitter is an important source of information that people frequently contribute to and rely on for emerging topics, public opinions, and event awareness. Crisis-relevant tweets can potentially avail a magnitude of applications such as helping authorities and governments become aware of situations and thus offer better responses. Onemajor challenge toward crisis-awareness in Twitter is to identify those tweets that are relevant to unseen crises. In this article, we propose an automatic labeling approach to distinguishing crisis-relevant tweets while differentiating source types (e.g., government or personal accounts) simultaneously. We first analyze and identify tweet-specific linguistic, sentimental, and emotional features based on statistical topic modeling. Then, we design a novel correlative convolutional neural network which uses a shared hidden layer to learn effective representations of the multi-faceted features. The model can discover salient information while being robust to the variations and noises in tweets and sources. To obtain a bird's-eye view of a crisis event, we further develop an approach to automatically summarize key information of identified tweets. Empirical evaluation on a real Twitter dataset demonstrates the feasibility of discerning relevant tweets for an unseen crisis. The applicability of our proposed approach is further demonstrated with a crisis aider system. © 2019 Association for Computing Machinery.",Convolutional neural network; Information summarization; Social media,Convolution; Social networking (online); Automatic labeling; Bird's eye view; Convolutional neural network; Emerging topics; Empirical evaluations; Information summarization; Public opinions; Social media; Multilayer neural networks
Threat management in data-centric IoT-based collaborative systems,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074810879&doi=10.1145%2f3323232&partnerID=40&md5=4918ab07bdf7ef50215dfb0467cbe13f,"In this article, we propose a threat management system (TMS) for Data-centric Internet-of-Things-based Collaborative Systems (DIoTCSs). In particular, we focus on tampering attacks that target shared databases and can affect the execution of the DIoTCS services. The novelty of the proposed system is to isolate the damage caused by tampering attacks into data partitions. We formulate the partitioning problem as a cost-driven optimization problem, prove its NP-hardness, and propose two polynomial-time heuristics. We evaluate a TMS experimentally and demonstrate that intelligent partitioning of the database improves the overall availability of the DIoTCS. © 2019 Association for Computing Machinery.",Cost-driven optimization; Database systems; Internet-of-Things; Threat management,Database systems; Information management; Optimization; Polynomial approximation; Collaborative systems; Data partition; Optimization problems; Partitioning problem; Polynomial time heuristics; Tampering attacks; Threat management; Threat-management systems; Internet of things
Constructing novel block layouts for webpage analysis,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074835144&doi=10.1145%2f3326457&partnerID=40&md5=785d5e9934f901fbd354bc824995dd8e,"Webpage segmentation is the basic building block for a wide range of webpage analysis methods. The rapid development of Web technologies results in more dynamic and complex webpages, which bring new challenges to this area. To improve the performance of webpage segmentation, we propose a two-stage segmentation method that can combine visual, logic, and semantic features of the contents on a webpage. Specifically, we devise a new model to measure the similarities of the elements on webpages based on both visual layout and logic organization in the first stage, and we propose a novel block regrouping method using semantic statistics and visual positions in the second stage. This two-stage method can effectively conduct webpage segmentation on complicated and dynamic webpages. The performance and accuracy of the method are verified by comparing with two existing webpage segmentation methods. The experiment results show that the proposed method significantly outperforms the existing state of the art in terms of higher precision, recall, and accuracy. © 2019 Association for Computing Machinery.",Clustering; Semantic regourping; Similarity model; Webpage analysis; Webpage segmentation,Computer circuits; Semantics; Tunneling (excavation); Basic building block; Clustering; Segmentation methods; Semantic features; Similarity models; Two-stage methods; Two-stage segmentations; Web-page analysis; Websites
Mitigating tail response time of n-tier applications: The impact of asynchronous invocations,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074827588&doi=10.1145%2f3340462&partnerID=40&md5=e6c78785608f62758a26d9291041ed68,"Consistent low response time is essential for e-commerce due to intense competitive pressure. However, practitioners of web applications have often encountered the long-tail response time problem in cloud data centers as the system utilization reaches moderate levels (e.g., 50%). Our fine-grained measurements of an open source n-tier benchmark application (RUBBoS) show such long response times are often caused by Cross-tier Queue Overflow (CTQO). Our experiments reveal the CTQO is primarily created by the synchronous nature of RPC-style call/response inter-tier communications, which create strong inter-tier dependencies due to the request processing chain of classic n-tier applications composed of synchronous RPC/thread-based servers. We remove gradually the dependencies in n-tier applications by replacing the classic synchronous servers (e.g., Apache, Tomcat, and MySQL) with their corresponding event-driven asynchronous version (e.g., Nginx, XTomcat, and XMySQL) one-by-one. Our measurements with two application scenarios (virtual machine co-location and background monitoring interference) show that replacing a subset of asynchronous servers will shift the CTQO, without significant improvements in long-tail response time. Only when all the servers become asynchronous the CTQO is resolved. In synchronous n-tier applications, long-tail response times resulting from CTQO arise at utilization as low as 43%. On the other hand, the completely asynchronous n-tier system can disrupt CTQO and remove the long tail latency at utilization as high as 83%. © 2019 Association for Computing Machinery. © 2019 Association for Computing Machinery.",Asynchronous; Cloud computing; n-tier systems; Performance; Scalability,Benchmarking; Cloud computing; Distributed computer systems; Scalability; Application scenario; Asynchronous; Asynchronous invocation; Asynchronous version; Benchmark applications; Competitive pressure; N-tier systems; Performance; Response time (computer systems)
Policy adaptation in hierarchical attribute-based access control systems,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074842387&doi=10.1145%2f3323233&partnerID=40&md5=550d5306b6acb1c27d39d0da273f5cbc,"In Attribute-Based Access Control (ABAC), access to resources is given based on the attributes of subjects, objects, and environment. There is an imminent need for the development of efficient algorithms that enable migration to ABAC. However, existing policy mining approaches do not consider possible adaptation to the policy of a similar organization. In this article, we address the problem of automatically determining an optimal assignment of attribute values to subjects for enabling the desired accesses to be granted while minimizing the number of ABAC rules used by each subject or other appropriate metrics. We show the problem to be NP-Complete and propose a heuristic solution. © 2019 Association for Computing Machinery.",ABAC policy; Attribute value hierarchy; Policy adaptation,Hierarchical systems; Access to resources; Appropriate metrics; Attribute based access control; Attribute values; Heuristic solutions; Hierarchical attributes; Minimizing the number of; Policy adaptation; Access control
An anonymous delegatable attribute-based credential scheme for a collaborative E-health environment,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074839032&doi=10.1145%2f3338854&partnerID=40&md5=e243034b8aace502045e047e0fbcc1d9,"We propose an efficient anonymous, attribute-based credential scheme capable of provisioning multi-level credential delegations. It is integrated with a mechanism to revoke the anonymity of credentials for resolving access disputes andmaking users accountable for their actions. The proposed scheme has a lower end-user computational complexity in comparison to existing credential schemes with delegatability and has a comparable level of performancewith the credential standards of U-Prove and Idemix. Furthermore,we demonstrate how the proposed scheme can be applied to a collaborative e-health environment to provide its users with the necessary anonymous access with delegation capabilities. © 2019 Association for Computing Machinery.",Anonymous credentials; Delegatability; Electronic health records,Internet; Anonymous credential; Attribute-based; Delegatability; Electronic health record; End users; Multilevels; U-Prove; eHealth
Universal social network bus: Toward the federation of heterogeneous online social network services,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074839174&doi=10.1145%2f3323333&partnerID=40&md5=1301d29fdc0ae4cb7e071b1819d8d771,"Online Social Network Services (OSNSs) are changing the fabric of our society, impacting almost every aspect of it. Over the past few decades, an aggressive market rivalry has led to the emergence of multiple competing, ""closed"" OSNSs. As a result, users are trapped in the walled gardens of their OSNS, encountering restrictions about what they can do with their personal data, the people they can interact with, and the information they get access to. As an alternative to the platform lock-in, ""open"" OSNSs promote the adoption of open, standardized APIs. However, users still massively adopt closed OSNSs to benefit from the services' advanced functionalities and/or follow their ""friends,"" although the users' virtual social sphere is ultimately limited by the OSNSs they join. Our work aims at overcoming such a limitation by enabling users to meet and interact beyond the boundary of their OSNSs, including reaching out to ""friends"" of distinct closed OSNSs. We specifically introduce Universal Social Network Bus (USNB), which revisits the ""service bus"" paradigm that enables interoperability across computing systems to address the requirements of ""social interoperability."" USNB features synthetic profiles and personae for interaction across the boundaries of closed and open and profile- and non-profile-based OSNSs through a reference social interaction service. We ran a 1-dayworkshop with a panel of users who experimented with the USNB prototype to assess the potential benefits of social interoperability for social network users. Results show the positive evaluation of users for USNB, especially as an enabler of applications for civic participation. This further opens up new perspectives for future work, among which includes enforcing security and privacy guarantees. © 2019 Copyright held by the owner/author(s).",Decentralization; Federation; Interoperability; Middleware,Buses; Interoperability; Middleware; Social sciences computing; Computing system; Decentralization; Evaluation of users; Federation; On-line social networks; Potential benefits; Security and privacy; Social interactions; Social networking (online)
Multi-objective optimisation of online distributed software update for DevOps in clouds,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071759378&doi=10.1145%2f3338851&partnerID=40&md5=b8a3b449cffbef4c33dfa12ff139ee47,"This article studies synchronous online distributed software update, also known as rolling upgrade in DevOps, which in clouds upgrades software versions in virtual machine instances even when various failures may occur. The goal is to minimise completion time, availability degradation, and monetary cost for entire rolling upgrade by selecting proper parameters. For this goal,we propose a stochastic model and a novel optimisation method. We validate our approach to minimise the objectives through both experiments in Amazon Web Service (AWS) and simulations. © 2019 Association for Computing Machinery.",Multi-objective optimisation; Rolling upgrade; Software operation; Software system reliability; Stochastic modelling,Software reliability; Stochastic models; Stochastic systems; Web services; Amazon web services; Completion time; Distributed software; Monetary costs; Optimisation method; Software systems; Software versions; Multiobjective optimization
It is an equal failing to trust everybody and to trust nobody: Stock price prediction using trust filters and enhanced user sentiment on twitter,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072537517&doi=10.1145%2f3338855&partnerID=40&md5=19a0768e13e08c8d327c0535fb7f6bdd,"Social media are providing a huge amount of information, in scales never possible before. Sentiment analysis is a powerful tool that uses social media information to predict various target domains (e.g., the stock market). However, social media information may or may not come from trustworthy users. To utilize this information, a very first critical problem to solve is to filter credible and trustworthy information from contaminated data, advertisements, or scams. We investigate different aspects of a social media user to score his/her trustworthiness and credibility. Furthermore, we provide suggestions on how to improve trustworthiness on social media by analyzing the contribution of each trust score. We apply trust scores to filter the tweets related to the stock market as an example target domain. While social media sentiment analysis has been on the rise over the past decade, our trust filters enhance conventional sentiment analysis methods and provide more accurate prediction of the target domain, here, the stock market. We argue that while it is a failing to ignore the information social media provide, effectively trusting nobody, it is an equal failing to trust everybody on social media too: Our filters seek to identify whom to trust. © 2019 Association for Computing Machinery.",Credibility; Return on investment (ROI); Sentiment analysis; Social media; Stock market; Trust; Twitter,Commerce; Financial markets; Forecasting; Investments; Sentiment analysis; Credibility; Re-turn-on; Social media; Trust; Twitter; Social networking (online)
The influence of trust score on cooperative behavior,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072533572&doi=10.1145%2f3329250&partnerID=40&md5=4bc22e3a0e9899e09423db2f813ff395,"The assessment of trust between users is essential for collaboration. General reputation and ID mechanisms may support users’ trust assessment. However, these mechanisms lack sensitivity to pairwise interactions and specific experience such as betrayal over time. Moreover, they place an interpretation burden that does not scale to dynamic, large-scale systems. While several pairwise trust mechanisms have been proposed, no empirical research examines trust score influence on participant behavior. We study the influence of showing a partner trust score and/or ID on participants’ behavior in a small-group collaborative laboratory experiment based on the trust game. We show that trust score availability has the same effect as an ID to improve cooperation as measured by sending behavior and receiver response. Excellent models based on the trust score predict sender behavior and document participant sensitivity to the provision of partner information. Models based on the trust score for recipient behavior have some predictive ability regarding trustworthiness, but suggest the need for more complex functions relating experience to participant response. We conclude that the parameters of a trust score, including pairwise interactions and betrayal, influence the different roles of participants in the trust game differently, but complement traditional ID and have the advantage of scalability. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cooperation; Reputation; Trust; Trust game,Large scale systems; Co-operative behaviors; Cooperation; Laboratory experiments; Pairwise interaction; Predictive abilities; Reputation; Trust; Trust games; Behavioral research
Understanding the Influences of Past Experience on Trust in Human-agent Teamwork,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072515644&doi=10.1145%2f3324300&partnerID=40&md5=f6dc38de0e629cb49d667918e5e4fe35,"People use the knowledge acquired from past experiences in assessing the trustworthiness of a trustee. In a time where the agents are being increasingly accepted as partners in collaborative efforts and activities, it is critical to understand all aspects of human trust development in agent partners. For human-agent virtual ad hoc teams to be effective, humans must be able to trust their agent counterparts. To earn the humans’ trust, agents need to quickly develop an understanding of the expectation of human team members and adapt accordingly. This study empirically investigates the impact of past experience on human trust in and reliance on agent teammates. To do so, we developed a team coordination game, the Game of Trust (GoT), in which two players repeatedly cooperate to complete team tasks without prior assignment of subtasks. The effects of past experience on human trust are evaluated by performing an extensive set of controlled experiments with participants recruited from Amazon Mechanical Turk, a crowdsourcing marketplace. We collect both teamwork performance data as well as surveys to gauge participants’ trust in their agent teammates. The results show that positive (negative) past experience increases (decreases) human trust in agent teammates; lack of past experience leads to higher trust levels compared to positive past experience; positive (negative) past experience facilitates (hinders) reliance on agent teammates; the relationship between trust in and reliance on agent teammates is not always correlated. These findings provide clear and significant evidence of the influence of key factors on human trust in virtual agent teammates and enhance our understanding of the changes in human trust in peer-level agent teammates with respect to past experience. © 2019 Association for Computing Machinery.",Human-agent teamwork; Past experience; Reliance; Trust,Surveys; Virtual reality; Amazon mechanical turks; Controlled experiment; Human-agent teamwork; Past experience; Performance data; Reliance; Team coordination; Trust; Human resource management
Adaptive resource allocation for computation offloading: A control-theoretic approach,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065712060&doi=10.1145%2f3284553&partnerID=40&md5=ae6c524a1ad5bc38925638edaea869ef,"Although mobile devices today have powerful hardware and networking capabilities, they fall short when it comes to executing compute-intensive applications. Computation offloading (i.e., delegating resource-consuming tasks to servers located at the edge of the network) contributes toward moving to a mobile cloud computing paradigm. In this work, a two-level resource allocation and admission control mechanism for a cluster of edge servers offers an alternative choice to mobile users for executing their tasks. At the lower level, the behavior of edge servers is modeled by a set of linear systems, and linear controllers are designed to meet the system's constraints and quality of service metrics, whereas at the upper level, an optimizer tackles the problems of load balancing and application placement toward the maximization of the number the offloaded requests. The evaluation illustrates the effectiveness of the proposed offloading mechanism regarding the performance indicators, such as application average response time, and the optimal utilization of the computational resources of edge servers. © 2019 Association for Computing Machinery.",Edge computing; Feedback control; Linear modeling,Edge computing; Feedback control; Linear control systems; Linear systems; Mobile cloud computing; Mobile telecommunication systems; Quality of service; Adaptive resource allocations; Application placements; Computation offloading; Computational resources; Control-theoretic approach; Linear modeling; Performance indicators; Quality of service metrics; Resource allocation
Deep reinforcement scheduling for mobile crowdsensing in fog computing,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065720541&doi=10.1145%2f3234463&partnerID=40&md5=f8da84687d1cbc0c7109f61e926dda47,"Mobile crowdsensing becomes a promising technology for the emerging Internet of Things (IoT) applications in smart environments. Fog computing is enabling a new breed of IoT services, which is also a new opportunity for mobile crowdsensing. Thus, in this article, we introduce a framework enabling mobile crowdsensing in fog environments with a hierarchical scheduling strategy. We first introduce the crowdsensing framework that has a hierarchical structure to organize different resources. Since different positions and performance of fog nodes influence the quality of service (QoS) of IoT applications, we formulate a scheduling problem in the hierarchical fog structure and solve it by using a deep reinforcement learning-based strategy. From extensive simulation results, our solution outperforms other scheduling solutions for mobile crowdsensing in the given fog computing environment. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep reinforcement learning; Fog computing; Mobile crowdsensing,Deep learning; Fog; Internet of things; Machine learning; Quality of service; Reinforcement learning; Scheduling; Computing environments; Extensive simulations; Hierarchical scheduling; Hierarchical structures; Internet of Things (IOT); Mobile crowdsensing; Scheduling problem; Smart environment; Fog computing
"Guest editors' introduction to the special issue on fog, edge, and cloud integration for smart environments",2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065737984&doi=10.1145%2f3319404&partnerID=40&md5=7895a1e97e82659e840066b89e401c55,[No abstract available],,
DM2-ECOP: An Efficient Computation Offloading Policy for Multi-user Multi-cloudlet Mobile Edge Computing Environment,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065738092&doi=10.1145%2f3241666&partnerID=40&md5=b118843f4a0f0361fbfbbf97367c3e77,"Mobile Edge Computing is a promising paradigm that can provide cloud computing capabilities at the edge of the network to support low latency mobile services. The fundamental concept relies on bringing cloud computation closer to users by deploying cloudlets or edge servers, which are small clusters of servers that are mainly located on existing wireless Access Points (APs), set-top boxes, or Base Stations (BSs). In this article, we focus on computation offloading over a heterogeneous cloudlet environment. We consider several users with different energy-and latency-constrained tasks that can be offloaded over cloudlets with differentiated system and network resources capacities. We investigate offloading policies that decide which tasks should be offloaded and select the assigned cloudlet, accordingly with network and system resources. The objective is to minimize an offloading cost function, which we defined as a combination of tasks' execution time and mobiles' energy consumption. We formulate this problem as a Mixed-Binary Programming. Since the centralized optimal solution is NP-hard, we propose a distributed linear relaxation-based heuristic approach that relies on the Lagrangian decomposition method. To solve the subproblems, we also propose a greedy heuristic algorithm that computes the best cloudlet selection and bandwidth allocation following tasks' offloading costs. Numerical results show that our offloading policy achieves a good solution quickly. We also discuss the performances of our approach for large-scale scenarios and compare it to state-of-the-art approaches from the literature. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cloudlet; Ing; Lagrangian decomposition,Cost functions; Edge computing; Energy utilization; Heuristic algorithms; Lagrange multipliers; Set-top boxes; Cloudlet; Computation offloading; Computing environments; Efficient computation; Fundamental concepts; Lagrangian decomposition; State-of-the-art approach; Wireless access points; Heuristic methods
Contextaide: End-to-End Architecture for Mobile Crowd-sensing Applications,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065744645&doi=10.1145%2f3301444&partnerID=40&md5=2957544efeef006dff35a47884ef5535,"Mobile crowd-sensing (MCS) enables development of context-aware applications by mining relevant information from a large set of devices selected in an ad hoc manner. For example, MCS has been used for real-time monitoring such as Vehicle ad hoc Networks-based traffic updates as well as offline data mining and tagging for future use in applications with location-based services. However, MCS could be potentially used for much more demanding applications such as real-time perpetrator tracking by online mining of images from nearby mobile users. A recent example is tracking the miscreant responsible for the Boston bombing. We present a new design approach for tracking using MCS for such complex processing in real time. Since MCS applications assume an unreliable underlying computational platform, most typically sample size for recruited devices is guided by concerns such as fault tolerance and reliability of information. As the real-time requirements get stricter coupled with increasing complexity of data-mining approaches, the communication and computation overheads can impose a very tight constraint on the sample size of devices needed for realizing real-time operation. This results in trade-off in acquiring context-relevant data and resource usage incurred while the real-time operation requirements get updated dynamically. Such effects have not been properly studied and optimized to enable real-time MCS applications such as perpetrator tracking. In this article, we propose ContextAiDe architecture, a combination of API, middleware, and optimization engine. The key innovation in ContextAiDe is context-optimized recruitment for execution of computation- and communication-heavy MCS applications in edge environment. ContextAiDe uses a notion of two types of contexts, exact (hard constraints), which have to be satisfied, and preferred (soft constraints), which may be satisfied to a certain degree. By adjusting the preferred contexts, ContextAiDe can optimize the operational overheads to enable real-time operation. ContextAiDe provides an API to specify contexts requirements and the code of MCS app, offload execution environment, a middleware that enables context-optimized and a fault-tolerant distributed execution. ContextAiDe evaluation using a real-time perpetrator tracking application shows reduced energy consumption of 37.8%, decrease in data transfer of 24.8%, and 43% less time compared to existing strategy. In spite of a small increase in the minimum distance from the perpetrator, iterations of optimization tracks the perpetrator successfully. Proactively learning the context and using stochastic optimization strategy minimizes the performance degradation caused due to uncertainty (<20%) in usage-dependent contexts. © 2019 Association for Computing Machinery.",Context aware; Edge computing; Middleware; Mobile crowd-sensing,Complex networks; Data transfer; Economic and social effects; Edge computing; Energy utilization; Fault tolerance; Location based services; Middleware; Network architecture; Optimization; Real time systems; Telecommunication services; Vehicular ad hoc networks; Computational platforms; Context aware applications; Context-Aware; Mobile crowd-sensing; Performance degradation; Reliability of information; Stochastic optimization strategies; Vehicle ad-hoc networks; Data mining
A dynamic service migration mechanism in edge cognitive computing,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065718754&doi=10.1145%2f3239565&partnerID=40&md5=4e2055589c06d1620622144d22ee4664,"Driven by the vision of edge computing and the success of rich cognitive services based on artificial intelligence, a new computing paradigm, edge cognitive computing (ECC), is a promising approach that applies cognitive computing at the edge of the network. ECC has the potential to provide the cognition of users and network environmental information, and further to provide elastic cognitive computing services to achieve a higher energy efficiency and a higher Quality of Experience (QoE) compared to edge computing. This article first introduces our architecture of the ECC and then describes its design issues in detail. Moreover, we propose an ECC-based dynamic service migration mechanism to provide insight into how cognitive computing is combined with edge computing. In order to evaluate the proposed mechanism, a practical platform for dynamic service migration is built up, where the services are migrated based on the behavioral cognition of a mobile user. The experimental results show that the proposed ECC architecture has ultra-low latency and a high user experience, while providing better service to the user, saving computing resources, and achieving a high energy efficiency. © 2019 Association for Computing Machinery.",,Dynamics; Edge computing; Energy efficiency; Memory architecture; Network architecture; Quality of service; Cognitive Computing; Computing paradigm; Computing resource; Dynamic services; Environmental information; High energy efficiency; Practical platforms; Quality of experience (QoE); Green computing
"Enabling Workload Engineering in Edge, Fog, and Cloud Computing through OpenStack-based Middleware",2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065748278&doi=10.1145%2f3309705&partnerID=40&md5=79693368bb5eba3fd77eec8868e6e5bf,"To enable and support smart environments, a recent ICT trend promotes pushing computation from the remote Cloud as close to data sources as possible, resulting in the emergence of the Fog and Edge computing paradigms. Together with Cloud computing, they represent a stacked architecture, in which raw datasets are first pre-processed locally at the Edge and then vertically offloaded to the Fog and/or the Cloud. However, as hardware is becoming increasingly powerful, Edge devices are seen as candidates for offering data processing capabilities, able to pool and share computing resources to achieve better performance at a lower network latency-a pattern that can be also applied to Fog nodes. In these circumstances, it is important to enable efficient, intelligent, and balanced allocation of resources, as well as their further orchestration, in an elastic and transparent manner. To address such a requirement, this article proposes an OpenStack-based middleware platform through which resource containers at the Edge, Fog, and Cloud levels can be discovered, combined, and provisioned to end users and applications, thereby facilitating and orchestrating offloading processes. As demonstrated through a proof of concept on an intelligent surveillance system, by converging the Edge, Fog, and Cloud, the proposed architecture has the potential to enable faster data processing, as compared to processing at the Edge, Fog, or Cloud levels separately. This also allows architects to combine different offloading patterns in a flexible and fine-grained manner, thus providing new workload engineering patterns. Measurements demonstrated the effectiveness of such patterns, even outperforming edge clusters. © 2019 Association for Computing Machinery.",Big data; Cloud computing; Edge; Fog; IoT; Offloading; Smart environment; Stack4Things,Big data; Cloud computing; Data handling; Fog; Middleware; Network architecture; Platform as a Service (PaaS); Edge; Engineering patterns; Intelligent surveillance systems; Offloading; Processing capability; Proposed architectures; Smart environment; Stack4Things; Computer architecture
Fog computing for the Internet of Things: A survey,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063939776&doi=10.1145%2f3301443&partnerID=40&md5=49ad63c8f0f1685fd1d7365914fc86f5,"Research in the Internet of Things (IoT) conceives a world where everyday objects are connected to the Internet and exchange, store, process, and collect data from the surrounding environment. IoT devices are becoming essential for supporting the delivery of data to enable electronic services, but they are not sufficient in most cases to host application services directly due to their intrinsic resource constraints. Fog Computing (FC) can be a suitable paradigm to overcome these limitations, as it can coexist and cooperate with centralized Cloud systems and extends the latter toward the network edge. In this way, it is possible to distribute resources and services of computing, storage, and networking along the Cloud-to-Things continuum. Assuch, FC brings all the benefits of Cloud Computing (CC) closer to end (user) devices. This article presents a survey on the employment of FC to support IoT devices and services. The principles and literature characterizing FC are described, highlighting six IoT application domains that may benefit from the use of this paradigm. The extension of Cloud systems towards the network edge also creates new challenges and can have an impact on existing approaches employed in Cloud-based deployments. Research directions being adopted by the community are highlighted, with an indication of which of these are likely to have the greatest impact. An overview of existing FC software and hardware platforms for the IoT is also provided, along with the standardisation efforts in this area initiated by the OpenFog Consortium (OFC). © 2019 Association for Computing Machinery.",Cloud computing; Fog computing; Internetofthings; Topological proximity,Cloud computing; Digital storage; Fog; Fog computing; Surveys; Application services; Electronic services; Internet of thing (IOT); IOT applications; Resource Constraint; Software and hardwares; Surrounding environment; Topological proximity; Internet of things
"Cloud, Fog, or Mist in IoT? That is the qestion",2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063952111&doi=10.1145%2f3309709&partnerID=40&md5=f661a68be86a5ce8caeece19c59ca972,"Internet of Things (IoT) has been commercially explored as Platforms as a Services (PaaS). The standard solution for this kind of service is to combine the Cloud computing infrastructure with IoT software, services, and protocols also known as CoT (Cloud of Things). However, the use of CoT in latency-sensitive applications has been shown to be unfeasible due to the inherent latency of cloud computing services. One proposal to solve this problem is the use of the computational resources available at the edge of the network, which is called Fog computing. Fog computing solves the problem of latency but adds complexity to the use of these resources due to the dynamism and heterogeneity of the IoT. An even more accentuated form of fog computing is Mist computing, where the use of the computational resources is limited to the close neighborhood of the client device. The decision of what computing infrastructure (Fog, Mist, or Cloud computing) is the best to provide computational resources is not always simple, especially in cases where latency requirements should be met by CoT. This work proposes an algorithm for selecting the best physical infrastructure to use the computational resource (Fog, Mist, or Cloud computing) based on cost, bandwidth, and latency criteria defined by the client device, resource availability, and topology of the network. The article also introduces the concept of feasible Fog that limits the growth of device search time in the neighborhood of the client device. Simulation results suggest the algorithm's choice adequately attends the client's device requirements and that the proposed method can be used in IoT environment located on the edge of the network. © 2019 Association for Computing Machinery.",Edge computing; Fog computing; Internet of things; Mist computing,Edge computing; Fog; Internet of things; Platform as a Service (PaaS); Cloud computing infrastructures; Cloud computing services; Computational resources; Computing infrastructures; Internet of Things (IOT); Resource availability; Sensitive application; Standard solutions; Fog computing
Practical privacy-preserving high-order Bi-lanczos in integrated edge-fog-cloud architecture for cyber-physical-social systems,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063965845&doi=10.1145%2f3230641&partnerID=40&md5=e3aa83ae8d9d28ce8c1c73326cbd6e19,"Smart environments, also referred to as cyber-physical-social systems (CPSSs), are expected to significantly benefit from the integration of edge, fog, and cloud for intelligence service flexibility, efficiency, and cost saving. High-order Bi-Lanczos method has emergedasapowerful tool serving asmulti-dimensional data processing, such as prevailing feature extraction, classification, and clustering of high-order data, in CPSSs. However, integrated edge-fog-cloud architecture is open and users have very limited control; how to carry out big data processing without compromising the security and privacy is a challenging issue in edge-fog-cloud-assisted smart applications. In this work, we propose a novel and practical privacy-preserving high-order Bi-Lanczos scheme in integrated edge-fog-cloud architectural paradigm for smart environments. More precisely, we first propose a privacy-preserving big data processing model using the synergy of edge, fog, and cloud. The proposed model enables edge, fog, and cloud to cooperatively complete big data processing without compromising users' privacy for large-scale tensor data in CPSSs. Subsequently, making use of the model, we present a privacy-preserving high-order Bi-Lanczos scheme. Finally, we theoretically and empirically analyze the security and efficiency of the proposed privacy-preserving high-order Bi-Lanczos scheme based on an intelligent surveillance system case study. And the results demonstrate that the proposed scheme provides a privacy-preserving and efficient way of computations in integrated edge-fog-cloud paradigm for smart environments. © 2019 Association for Computing Machinery.",Cyber-physical-social systems; Edge-fog-cloud computing; High-order Bi-Lanczos; Privacy preservation; Smart environments; Tensor,Big data; Computer architecture; Cyber Physical System; Efficiency; Fog; Fog computing; Tensors; Data processing models; Intelligence services; Intelligent surveillance systems; Lanczos; Privacy preservation; Security and privacy; Smart environment; Social systems; Data privacy
A unified model for the mobile-edge-cloud continuum,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063949797&doi=10.1145%2f3226644&partnerID=40&md5=37f1332f2beca0fd666d900cc95cdccb,"Technologies such as mobile, edge, and cloud computing have the potential to form a computing continuum for new, disruptive applications. At runtime, applications can choose to execute parts of their logic on different infrastructures that constitute the continuum, with the goal of minimizing latency and battery consumption and maximizing availability. In this article, we propose A3-E, a unified model for managing the life cycle of continuum applications. In particular, A3-E exploits the Functions-as-a-Service model to bring computation to the continuum in the form of microservices. Furthermore, A3-E selects where to execute a certain function based on the specific context and user requirements. The article also presents a prototype framework that implements the concepts behind A3-E. Results show that A3-E is capable of dynamically deploying microservices and routing the application's requests, reducing latency by up to 90% when using edge instead of cloud resources, and battery consumption by 74% when computation has been offloaded. © 2019 Association for Computing Machinery.",Computing continuum; Edge computing; Fog computing; Functions-as-a-Service; Mobile computing; Ops automation; Real-time systems,Computation theory; Edge computing; Electric batteries; Interactive computer systems; Life cycle; Mobile computing; Real time systems; Battery consumption; Computing continuum; Edge clouds; Runtimes; Service Model; Unified Modeling; User requirements; Fog computing
A fog-based application for human activity recognition using personal smart devices,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063963445&doi=10.1145%2f3266142&partnerID=40&md5=ec11c37f004a60bae856c9c3eb59bb60,"The diffusion of heterogeneous smart devices capable of capturing and analysing data about users, and/or the environment, has encouraged the growth of novel sensing methodologies. One of the most attractive scenarios in which such devices, such as smartphones, tablet computers, or activity trackers, can be exploited to infer relevant information is human activity recognition (HAR). Even though some simple HAR techniques can be directly implemented on mobile devices, in some cases, such as when complex activities need to be analysed timely, users' smart devices can operate as part of a more complex architecture. In this article, we propose a multi-device HAR framework that exploits the fog computing paradigm to move heavy computation from the sensing layer to intermediate devices and then to the cloud. As compared to traditional cloud-based solutions, this choice allows to overcome processing and storage limitations of wearable devices while also reducing the overall bandwidth consumption. Experimental analysis aims to evaluate the performance of the entire platform in terms of accuracy of the recognition process while also highlighting the benefits it might bring in smart environments. © 2019 Association for Computing Machinery.",Fog computing; Human activity recognition; Mobile crowdsensing,Digital storage; Fog; Pattern recognition; Bandwidth consumption; Complex architectures; Computing paradigm; Experimental analysis; Human activity recognition; Mobile crowdsensing; Recognition process; Storage limitation; Fog computing
Oops: Optimizing operation-mode selection for IoT edge devices,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063947773&doi=10.1145%2f3230642&partnerID=40&md5=8ac39aa8b763c1d27e77faf263cb84f9,"The massive increase of IoT devices and their collected data raises the question of how to analyze all that data. Edge computing provides a suitable compromise, but the question remains: How much processing should be done locally vs. offloaded to other devices? The diverse application requirements and limited resources at the edge extend the challenges. We propose Oops, an optimization framework to adapt the resource management at runtime distributedly. It orchestrates the IoT devices and adapts their operation mode with respect to their constraints and the gateway's limited shared resources. Oops reduces runtime overhead significantly while increasing user utility compared to state-of-the-art. © 2019 Association for Computing Machinery.",Constrained devices; Edge computing; Internet of Things; IoT; Resource management; Runtime optimization,Constrained optimization; Edge computing; Natural resources management; Resource allocation; Constrained devices; Diverse applications; Optimization framework; Resource management; Runtime optimization; Runtime overheads; Shared resources; State of the art; Internet of things
Fog-based secure communications for low-power IoT devices,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063964715&doi=10.1145%2f3284554&partnerID=40&md5=0e8a667878a7cb497882dab351d20826,"Designing secure, scalable, and resilient IoT networks is a challenging task because of resource-constrained devices and no guarantees of reliable network connectivity. Fog computing improves the resiliency of IoT, but its security model assumes that fog nodes are fully trusted. We relax this latter constraint by proposing a solution that guarantees confidentiality of messages exchanged through semi-honest fog nodes thanks to a lightweight proxy re-encryption scheme. We demonstrate the feasibility of the solution by applying it to IoT networks of low-power devices through experiments on microcontrollers and ARM-based architectures. © 2019 Association for Computing Machinery.",Authorization; Confidentiality; Encryption; Fog computing; Internet-of-things; Proxy re-encryption; Publish/subscribe; Secure communications; Security,Cryptography; Fog; Fog computing; Low power electronics; Network security; Secure communication; Authorization; Confidentiality; Proxy re encryptions; Publish/subscribe; Security; Internet of things
BPMS-RA: A novel reference architecture for business process management systems,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062336422&doi=10.1145%2f3232677&partnerID=40&md5=892501d9b1eb8dc6a0ca6d558f4516ff,"A growing number of business process management systems is under development both in academia and in practice. These systems typically are based on modern system engineering principles, such as service-oriented architecture. At the same time, the advent of big data analytics has changed the scope of these systems, including functionality such as data mining. However, existing reference architectures for business process management systems date back 20 years and, consequently, are not up-to-date with these modern developments. To fill the gap, this article proposes an up-to-date reference architecture, called BPMS-RA, for modern business process management systems. BPMS-RA is based on analysis of recent literature and of existing commercial implementations. This reference architecture aims to provide a guideline template for the development of modern-day business process management systems by specifying functions and interfaces that need to be provided by these systems as well as a set of quality criteria that they need to meet. © 2019 Association for Computing Machinery.",Business process management systems; Reference architecture; Workflow,Architecture; Data Analytics; Data mining; Enterprise resource management; Information services; Business process management systems; Commercial implementation; Engineering principles; Modern development; Quality criteria; Reference architecture; Workflow; Service oriented architecture (SOA)
Guest editors' introduction to the special issue on knowledge-driven business process management,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062345859&doi=10.1145%2f3296981&partnerID=40&md5=49788d9c2b94cde8b40976b8fd3735fa,[No abstract available],,
Measuring performance in knowledge-intensive processes,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062332410&doi=10.1145%2f3289180&partnerID=40&md5=3903b6b2aea333e0d6092a94b3ca5a16,"Knowledge-intensive Processes (KIPs) are processes whose execution is heavily dependent on knowledge workers performing various interconnected knowledge-intensive decision-making tasks. Among other characteristics, KIPs are usually non-repeatable, collaboration-oriented, unpredictable, and, in many cases, driven by implicit knowledge, derived from the capabilities and previous experiences of participants. Despite the growing body of research focused on understanding KIPs and on proposing systems to support these KIPs, the research question on how to define performance measures thereon remains open. In this article, we address this issue with a proposal to enable the performance management of KIPs. Our approach comprises an ontology that allows us to define process performance indicators (PPIs) in the context of KIPs, and a methodology that builds on the ontology and the concepts of lead and lag indicators to provide process participants with actionable guidelines that help them conduct the KIP in a way that fulfills a set of performance goals. Both the ontology and the methodology have been applied to a case study of a real organization in Brazil to manage the performance of an Incident Troubleshooting Process within an ICT (Information and Communications Technology) Outsourcing Company. © 2019 Association for Computing Machinery.",Knowledge-intensive processes; Performance measure; Process performance indicators,Benchmarking; Decision making; Outsourcing; Implicit knowledge; Information and communications technology; Knowledge intensive process; Measuring performance; Outsourcing companies; Performance management; Performance measure; Process performance indicators; Ontology
A dynamic data-throttling approach to minimize workflow imbalance,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065496736&doi=10.1145%2f3278720&partnerID=40&md5=f165fbdf8ed77e617844b822c537a767,"Scientific workflows enable scientists to undertake analysis on large datasets and perform complex scientific simulations. These workflows are often mapped onto distributed and parallel computational infrastructures to speed up their executions. Prior to its execution, a workflow structure may suffer transformations to accommodate the computing infrastructures, normally involving task clustering and partitioning. However, these transformations may cause workflow imbalance because of the difference between execution task times (runtime imbalance) or because of unconsidered data dependencies that lead to data locality issues (data imbalance). In this article, to mitigate these imbalances, we enhance the workflow lifecycle process in use by introducing a workflow imbalance phase that quantifies workflow imbalance after the transformations. Our technique is based on structural analysis of Petri nets, obtained by model transformation of a data-intensive workflow, and Linear Programming techniques. Our analysis can be used to assist workflow practitioners in finding more efficient ways of transforming and scheduling their workflows. Moreover, based on our analysis, we also propose a technique to mitigate workflow imbalance by data throttling. Our approach is based on autonomic computing principles that determine how data transmission must be throttled throughout workflow jobs. Our autonomic data-throttling approach mainly monitors the execution of the workflow and recompute data-throttling values when certain watchpoints are reached and time derivation is observed. We validate our approach by a formal proof and by simulations along with the Montage workflow. Our findings show that a dynamic data-throttling approach is feasible, does not introduce a significant overhead, and minimizes the usage of input buffers and network bandwidth. © 2019 Association for Computing Machinery.",Linear programming; Optimization; Petri nets; Scientific workflows,Large dataset; Life cycle; Linear programming; Linear transformations; Mathematical transformations; Optimization; Petri nets; Autonomic Computing; Computational infrastructure; Computing infrastructures; Linear programming techniques; Model transformation; Scientific simulations; Scientific workflows; Workflow structures; Metadata
Guest editors' introduction for special issue on service management for the internet of things,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061233570&doi=10.1145%2f3293539&partnerID=40&md5=b3fdaa03d7bfdf3097d9c0dc6df69bc5,[No abstract available],,
Incentive-based crowdsourcing of hotspot services,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061277228&doi=10.1145%2f3229047&partnerID=40&md5=0f3ac5a46b8c35d500fd3f07d3e0c1d2,"We present a new spatio-temporal incentive-based approach to achieve a geographically balanced coverage of crowdsourced services. The proposed approach is based on a new spatio-temporal incentive model that considers multiple parameters including location entropy, time of day, and spatio-temporal density to encourage the participation of crowdsourced service providers. We present a greedy network ?ow algorithm that o?ers incentives to redistribute crowdsourced service providers to improve the crowdsourced coverage balance within an area. A novel participation probability model is also introduced to estimate the expected number of crowdsourced service providers movement based on spatio-temporal features. Experimental results validate the efciency and e?ectiveness of the proposed approach. © 2019 ACM.",Coverage distribution; Crowdsourced service; IoT; IoT services; Mobile crowdsourcing; Network flow; Sensor cloud; Spatio-temporal crowdsourcing; Spatio-temporal incentive model; Spatiotemporal data; Task assignment; WiFi hotspot coverage,Crowdsourcing; Internet of things; Internet service providers; Wi-Fi; Coverage distribution; Crowdsourced service; Incentive models; Iot services; Mobile crowdsourcing; Network flows; Spatio temporal; Spatio-temporal data; Task assignment; Wi-Fi hotspot; Location based services
A hybrid approach for improving the design quality of web service interfaces,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061278790&doi=10.1145%2f3226593&partnerID=40&md5=2c3fdf127d1f9c286dc702b09bf1698f,"A key success of a Web service is to appropriately design its interface to make it easy to consume and understand. In the context of service-oriented computing (SOC), the service's interface is the main source of interaction with the consumers to reuse the service functionality in real-world applications. The SOC paradigm provides a collection of principles and guidelines to properly design services to provide best practice of third-party reuse. However, recent studies showed that service designers tend to pay little care to the design of their service interfaces, which often lead to several side effects known as antipatterns. One of the most common Web service interface antipatterns is to expose a large number of semantically unrelated operations, implementing different abstractions, in one single interface. Such bad design practices may have a significant impact on the service reusability, understandability, as well as the development and run-time characteristics. To address this problem, in this article, we propose a hybrid approach to improve the design quality of Web service interfaces and fix antipatterns as a combination of both deterministic and heuristic-based approaches. The first step consists of a deterministic approach using a graph partitioning-based technique to split the operations of a large service interface into more cohesive interfaces, each one representing a distinct abstraction. Then, the produced interfaces will be checked using a heuristic-based approach based on the non-dominated sorting genetic algorithm (NSGA-II) to correct potential antipatterns while reducing the interface design deviation to avoid taking the service away from its original design. To evaluate our approach, we conduct an empirical study on a benchmark of 26 real-world Web services provided by Amazon and Yahoo. Our experiments consist of a quantitative evaluation based on design quality metrics, as well as a qualitative evaluation with developers to assess its usefulness in practice. The results show that our approach significantly outperforms existing approaches and provides more meaningful results from a developer's perspective. © 2018 ACM",Antipattern; Search-based software engineering; Service-oriented computing; Web service design; Web services,Abstracting; Distributed computer systems; Genetic algorithms; Petroleum reservoir evaluation; Quality control; Reusability; Software engineering; Websites; Anti-patterns; Non dominated sorting genetic algorithm (NSGA II); Qualitative evaluations; Quantitative evaluation; Search-based software engineering; Service design; Service functionalities; Service oriented computing; Web services
Achieving business process improvement via ubiquitous decision-aware business processes,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061204767&doi=10.1145%2f3298986&partnerID=40&md5=90e6c5ac5d1d9314539329964a2bf0cd,"Business process improvement is an endless challenge for many organizations. As long as there is a process, it must be improved. Nowadays, improvement initiatives are driven by professionals. This is no longer practical because people cannot perceive the enormous data of current business environments. Here, we introduce ubiquitous decision-aware business processes. They pervade the physical space, analyze the ever-changing environments, and make decisions accordingly. We explain how they can be built and used for improvement. Our approach can be a valuable improvement option to alleviate the workload of participants by helping focus on the crucial rather than the menial tasks. © 2019 Association for Computing Machinery.",Business process improvement; Context; DMN; Ubiquitous decision-aware business process; Ubiquitous decisions; UBPMN,Internet; Business Process; Business process improvement; Context; Ubiquitous decisions; UBPMN; Process engineering
Integrating multi-level tag recommendation with external knowledge bases for automatic question answering,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065499501&doi=10.1145%2f3319528&partnerID=40&md5=1e3e3dd7b6d201ac3deab99ccab5909f,"We focus on using natural language unstructured textual Knowledge Bases (KBs) to answer questions from community-based Question-and-Answer (Q&A) websites. We propose a novel framework that integrates multi-level tag recommendation with external KBs to retrieve the most relevant KB articles to answer user posted questions. Different from many existing efforts that primarily rely on the Q&A sites' own historical data (e.g., user answers), retrieving answers from authoritative external KBs (e.g., online programming documentation repositories) has the potential to provide rich information to help users better understand the problem, acquire the knowledge, and hence avoid asking similar questions in future. The proposed multilevel tag recommendation best leverages the rich tag information by first categorizing them into different semantic levels based on their usage frequencies. A post-tag co-clustering model, augmented by a two-step tag recommender, is used to predict tags at different levels for a given user posted question. A KB article retrieval component leverages the recommended multi-level tags to select the appropriate KBs and search/rank the matching articles thereof. We conduct extensive experiments using real-world data from a Q&A site and multiple external KBs to demonstrate the effectiveness of the proposed question-answering framework. © 2019 Association for Computing Machinery.",Co-clustering; Question answering; Tag recommendation,Semantics; Automatic question answering; Co-clustering; External knowledge; Knowledge basis (KBs); Natural languages; On-line programming; Question Answering; Tag recommendations; Natural language processing systems
On the profitability of bundling sale strategy for online service markets with network effects,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065482652&doi=10.1145%2f3277667&partnerID=40&md5=25defe99e603ee652260ab862bea483c,"In recent years, we have witnessed a growing trend for online service companies to offer “bundling sales” to increase revenue. Bundling sale means that a company groups a set of products/services and charges this bundle at a fixed price, which is usually less than the total price of individual items in the bundle. In this work, our aim is to understand the underlying dynamics of bundling, particularly what is the optimal bundling sale strategy and under what situations it will be more attractive than the separate sales. We focus on online service markets that exhibit network effects. We formulate mathematical models to capture the interactions between buyers and sellers, analyze the market equilibrium and its stability, and provide an optimization framework to determine the optimal sale strategy for a service provider. We analyze the impact of various factors on the profitability of bundling, including the network effects, operating costs, and variance and correlation of customers' valuations toward these services. We show that bundling is more profitable when the variance of customers' valuations and the operational cost of the services are small. In addition, a positive network effect and a negative correlation among customers' valuation on services increase the profitability of bundling, whereas the heterogeneity of services and the asymmetry of operating costs reduce its advantage. © 2019 Association for Computing Machinery.",And Phrases: Bundling sale; Company's profit; Online service market,Commerce; Operating costs; Profitability; Buyers and sellers; Market equilibria; Negative correlation; Network effects; On-line service; Optimization framework; Service provider; Underlying dynamics; Sales
Reasoning about property preservation in adaptive case management,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061196307&doi=10.1145%2f3177778&partnerID=40&md5=50f36b5096f47e3a4ea17d2df8ef38ce,"Adaptive Case Management (ACM) has emerged as a key BPM technology for supporting the unstructured business process. A key problem in ACM is that case schemas need to be changed to best fit the case at hand. Such changes are ad hoc, and may result in schemas that do not reflect the intended logic or properties. This article presents a formal approach for reasoning about which properties of a case schema are preserved after a modification, and describes change operations that are guaranteed to preserve certain properties. The approach supports reasoning about rollbacks. The Case Management model used here is a variant of the Guard-Stage-Milestone model for declarative business artifacts. A real-life example illustrates applicability. © 2019 Association for Computing Machinery.",Business artifacts; Process changes,Best fit; Business Artifacts; Business Process; Case management; Change operations; Formal approach; Process change; Property preservation; Internet
Local concurrency detection in business process event logs,2019,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061201957&doi=10.1145%2f3289181&partnerID=40&md5=f4bf7bb40e6505da8dca4a38e2e8efee,"Process mining techniques aim at analyzing records generated during the execution of a business process in order to provide insights on the actual performance of the process. Detecting concurrency relations between events is a fundamental primitive underpinning a range of process mining techniques. Existing approaches to this problem identify concurrency relations at the level of event types under a global interpretation. If two event types are declared to be concurrent, every occurrence of one event type is deemed to be concurrent to one occurrence of the other. In practice, this interpretation is too coarse-grained and leads to over-generalization. This article proposes a finer-grained approach, whereby two event types may be deemed to be in a concurrency relation relative to one state of the process, but not relative to other states. In other words, the detected concurrency relation holds locally, relative to a set of states. Experimental results both with artificial and real-life logs show that the proposed local concurrency detection approach improves the accuracy of existing concurrency detection techniques. © 2019 Association for Computing Machinery.",Concurrency oracle; Event structure; Process mining,Artificial life; Business Process; Coarse-grained; Concurrency oracle; Detection approach; Event logs; Event structures; Event Types; Process mining; Data mining
Special issue on the economics of security and privacy: Guest editors' introduction,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061246247&doi=10.1145%2f3216902&partnerID=40&md5=a759011537c5be8a7831e2a9f5b69d60,[No abstract available],Economics; Game theory; Incentives; Privacy; Security,
Should credit card issuers reissue cards in response to a data breach?: Uncertainty and transparency in metrics for data security policymaking,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061267035&doi=10.1145%2f3122983&partnerID=40&md5=a7b0efe3257660f3b5e25a27ae9625d3,"When card data is exposed in a data breach but has not yet been used to attempt fraud, the overall social costs of that breach depend on whether the financial institutions that issued those cards immediately cancel them and issue new cards or instead wait until fraud is attempted. This article empirically investigates the social costs and benefits of those options. We use a parameterized model and Monte Carlo simulation to compare the cost of reissuing cards to the total expected cost of fraud if cards are not reissued. The ranges and distributions in our model are informed by publicly available information, from which we extrapolate estimates of the number of credit card records historically exposed in data breaches, the probability that a card exposed in a breach will be used for fraud, and the associated expected cost of existing-account credit card fraud. We find that automatically reissuing cards may have lower social costs than the costs of waiting until fraud is attempted, although the range of results is considerably broad. © 2018 ACM",Data breach; Economics of information security; Estimation; Identity theft; Monte Carlo,Crime; Estimation; Intelligent systems; Monte Carlo methods; Probability distributions; Security of data; Credit card frauds; Data breach; Economics of information; Expected costs; Financial institution; Identity theft; Parameterized model; Total expected costs; Cost benefit analysis
Revisiting the risks of bitcoin currency exchange closure,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061231244&doi=10.1145%2f3155808&partnerID=40&md5=5aae8ebf9e8434813e6ab5548170ac14,"Bitcoin has enjoyed wider adoption than any previous cryptocurrency; yet its success has also attracted the attention of fraudsters who have taken advantage of operational insecurity and transaction irreversibility. We study the risk that investors face from the closure of Bitcoin exchanges, which convert between Bitcoins and hard currency. We examine the track record of 80 Bitcoin exchanges established between 2010 and 2015. We find that nearly half (38) have since closed, with customer account balances sometimes wiped out. Fraudsters are sometimes to blame, but not always. Twenty-five exchanges suffered security breaches, 15 of which subsequently closed. We present logistic regressions using longitudinal data on Bitcoin exchanges aggregated quarterly. We find that experiencing a breach is correlated with a 13 times greater odds that an exchange will close in that same quarter. We find that higher-volume exchanges are less likely to close (each doubling in trade volume corresponds to a 12% decrease in the odds of closure). We also find that exchanges that derive most of their business from trading less popular (fiat) currencies, which are offered by at most one competitor, are less likely to close. © 2018 ACM 1533-5399/2018/09-ART50 $15.00",Bitcoin; Currency exchanges; Cybercrime; Security economics,Internet; Account balance; Currency exchange; Cybercrime; Logistic regressions; Longitudinal data; Security breaches; Security Economics; Volume exchange; Bitcoin
Fine-grained control over tracking to support the ad-based web economy,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061263037&doi=10.1145%2f3158372&partnerID=40&md5=06d9f6b2e6ae747d7c90ecc952e16371,"The intrusiveness of Web tracking and the increasing invasiveness of digital advertising have raised serious concerns regarding user privacy and Web usability, leading a substantial chunk of the populace to adopt ad-blocking technologies in recent years. The problem with these technologies, however, is that they are extremely limited and radical in their approach, and they completely disregard the underlying economic model of the Web, in which users get content free in return for allowing advertisers to show them ads. Nowadays, with around 200 million people regularly using such tools, said economic model is in danger. In this article, we investigate an Internet technology that targets users who are not, in general, against advertising, accept the trade-off that comes with the “free” content, but-for privacy concerns-they wish to exert fine-grained control over tracking. Our working assumption is that some categories of web pages (e.g., related to health or religion) are more privacy-sensitive to users than others (e.g., about education or science). Capitalizing on this, we propose a technology that allows users to specify the categories of web pages that are privacy-sensitive to them and block the trackers present on such web pages only. As tracking is prevented by blocking network connections of third-party domains, we avoid not only tracking but also third-party ads. Since users continue receiving ads on those web pages that belong to non-sensitive categories, our approach may provide a better point of operation within the trade-off between user privacy and the Web economy. To test the appropriateness and feasibility of our solution, we implemented it as a Web-browser plug-in, which is currently available for Google Chrome and Mozilla Firefox. Experimental results from the collected data of 746 users during one year show that only 16.25% of ads are blocked by our tool, which seems to indicate that the economic impact of the ad-blocking exerted by privacy-sensitive users could be significantly reduced. © 2018 ACM",Ad-blocking; Internet economy; User privacy,Economic and social effects; Economics; Electronic document exchange; Marketing; Websites; Ad-blocking; Fine-grained control; Internet economy; Internet technology; Network connection; Point of operation; User privacy; Web browser plug-in; Web browsers
A provenance-aware multi-dimensional reputation system for online rating systems,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061293295&doi=10.1145%2f3183323&partnerID=40&md5=6c648c5f2e681573e4c1f0d34e602170,"Online rating systems are widely accepted as means for quality assessment on the web and users increasingly rely on these systems when deciding to purchase an item online. This makes such rating systems frequent targets of attempted manipulation by posting unfair rating scores. Therefore, providing useful, realistic rating scores as well as detecting unfair behavior are both of very high importance. Existing solutions are mostly majority based, also employing temporal analysis and clustering techniques. However, they are still vulnerable to unfair ratings. They also ignore distances between options, the provenance of information, and different dimensions of cast rating scores while computing aggregate rating scores and trustworthiness of users. In this article, we propose a robust iterative algorithm which leverages information in the profile of users and provenance of information, and which takes into account the distance between options to provide both more robust and informative rating scores for items and trustworthiness of users. We also prove convergence of iterative ranking algorithms under very general assumptions, which are satisfied by the algorithm proposed in this article. We have implemented and tested our rating method using both simulated data as well as four real-world datasets from various applications of reputation systems. The experimental results demonstrate that our model provides realistic rating scores even in the presence of a massive amount of unfair ratings and outperforms the well-known ranking algorithms. © 2018 ACM",Iterative algorithm; Online rating; Rating provenance; Reputation system,Online systems; Clustering techniques; Iterative algorithm; Online rating systems; Online ratings; Quality assessment; Real-world datasets; Reputation systems; Robust iterative algorithms; Iterative methods
A game theoretic model for the formation of navigable small-world networks-the tradeoff between distance and reciprocity,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061267110&doi=10.1145%2f3183325&partnerID=40&md5=4f2ed8e5fd370385325dcd84f7f2ce8a,"Kleinberg proposed a family of small-world networks to explain the navigability of large-scale real-world social networks. However, the underlying mechanism that drives real networks to be navigable is not yet well understood. In this article, we present a game theoretic model for the formation of navigable small-world networks. We model the network formation as a game called the Distance-Reciprocity Balanced (DRB) game in which people seek for both high reciprocity and long-distance relationships. We show that the game has only two Nash equilibria: One is the navigable small-world network, and the other is the random network in which each node connects with each other node with equal probability, and any other network state can reach the navigable small world via a sequence of best-response moves of nodes. We further show that the navigable small-world equilibrium is very stable-(a) no collusion of any size would benefit from deviating from it; and (b) after an arbitrary deviations of a large random set of nodes, the network would return to the navigable small world as soon as every node takes one best-response step. In contrast, for the random network, a small group collusion or random perturbations is guaranteed to bring the network out of the random-network equilibrium and move to the navigable network as soon as every node takes one best-response step. Moreover, we show that navigable small-world equilibrium has much better social welfare than the random network, and we provide the price-of-anarchy and price-of-stability results of the game. Our empirical evaluation further demonstrates that the system always converges to the navigable network even when limited or no information about other players' strategies is available, and the DRB game simulated on real-world networks leads to navigability characteristic that is very close to that of the real networks, even though the real-world networks have non-uniform population distributions different from Kleinberg's small-world model. Our theoretical and empirical analyses provide important new insight on the connection between distance, reciprocity, and navigability in social networks. © 2018 ACM",Game theory; Navigability; Reciprocity; Small-world network,Game theory; Empirical analysis; Empirical evaluations; Game-theoretic model; Navigability; Price of Stability; Random perturbations; Real-world networks; Reciprocity; Small-world networks
IoT data prefetching in indoor navigation SOAs,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050819643&doi=10.1145%2f3177777&partnerID=40&md5=5d7aea3f5053ae0c0a80625a0987effa,"Internet-based Indoor Navigation Service-Oriented Architectures (IIN-SOA) organize signals collected by IoTbased devices to enable a wide range of novel applications indoors, where people spend 80-90% of their time. In this article, we study the problem of prefetching (or hoarding) the most important IoT data from an IINSOA to a mobile device, without knowing its user's destination during navigation. Our proposed Grap (Graph Prefetching) framework structurally analyzes building topologies to identify important areas that become virtual targets to an online heuristic search algorithm we developed. We tested Grap with datasets from a real IIN-SOA and found it to be impressively accurate. © 2018 Association for Computing Machinery.",Indoor navigation; Internet-of-things; Mobile prefetching,Heuristic algorithms; Indoor positioning systems; Information services; Navigation; Service oriented architecture (SOA); Topology; Data pre-fetching; Heuristic search algorithms; In-door navigations; Internet based; Novel applications; Prefetching; Virtual target; Internet of things
Latency-aware application module management for fog computing environments,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055956983&doi=10.1145%2f3186592&partnerID=40&md5=e416f3c258d5d82f8fb6ba3bbcda19f9,"The fog computing paradigm has drawn significant research interest as it focuses on bringing cloud-based services closer to Internet of Things (IoT) users in an efficient and timelymanner. Most of the physical devices in the fog computing environment, commonly named fog nodes, are geographically distributed, resource constrained, and heterogeneous. To fully leverage the capabilities of the fog nodes, large-scale applications that are decomposed into interdependent Application Modules can be deployed in an orderly way over the nodes based on their latency sensitivity. In this article,we propose a latency-aware ApplicationModule management policy for the fog environment that meets the diverse service delivery latency and amount of data signals to be processed in per unit of time for different applications. The policy aims to ensure applications' Quality of Service (QoS) in satisfying service delivery deadlines and to optimize resource usage in the fog environment. We model and evaluate our proposed policy in an iFogSim-simulated fog environment. Results of the simulation studies demonstrate significant improvement in performance over alternative latency-aware strategies. © 2018 Association for Computing Machinery.",Application management; Application placement; Application QoS; Fog computing; Internet of things; Latency awareness; Resource optimization,Environmental management; Fog; Internet of things; Quality of service; Application management; Application placements; Application QoS; Latency awareness; Resource optimization; Fog computing
Making constrained things reachable: A secure IP-agnostic NAT traversal approach for IoT,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055190055&doi=10.1145%2f3230640&partnerID=40&md5=2d051a31b41c0ce3934115472229fdb4,"The widespread adoption of the Internet of Things (IoT) has created a demand for ubiquitous connectivity of IoT devices into the Internet.While end-to-end connectivity for IoT requires in practice IPv6, a vast majority of nodes in Internet are only IPv4-capable. To address this issue, the use of Network Address Translation (NAT) at the IoT network boundary becomes necessary. However, the constrained nature of the IoT devices hinders the integration of traditional NAT traversal architectures through IoT networks. In this article, we introduce a novel transition mechanism that transparently enables IoT devices behind NATs to connect across different network-layer infrastructures. Our mechanism adopts the IoT standards to provide a global connectivity solution in a transparent, secure, and elegant way. Additionally, we revisit the NAT solutions for IoT and describe and evaluate our current implementation. © 2018 ACM.",Constrained networks; IoT; M2M; NAT traversal,Computer system firewalls; Internet protocols; Network layers; End-to-end connectivity; Global connectivity; Internet of thing (IOT); Iot devices; IOT networks; NAT traversal; Network address translations; Transition mechanism; Internet of things
Fine-grained emotion role detection based on retweet information,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058332821&doi=10.1145%2f3191820&partnerID=40&md5=86be28a697a26a513a8eb6aecdba67a2,"User behaviors in online social networks convey not only literal information but also one's emotional attitudes towards the information. To compute this attitude, we define the concept of emotion role as the concentrated reflection of a user's online emotional characteristics. Emotion role detection aims to better understand the structure and sentiments of online social networks and support further analysis, e.g., revealing public opinions, providing personalized recommendations, and detecting influential users. In this article, we first introduce the definition of a fine-grained emotion role, which consists of two dimensions: emotion orientation (i.e., positive, negative, and neutral) and emotion influence (i.e., leader and follower). We then propose a Multi-dimensional Emotion Role Mining model (MERM) to determine a user's emotion role in online social networks. Specifically, we tend to identify emotion roles by combining a set of features that reflect a user's online emotional status, including degree of emotional characteristics, accumulated emotion preference, structural factor, temporal factor, and emotion change factor. Experiment results on a real-life micro-blog reposting dataset show that the classification accuracy of the proposed model can achieve up to 90.1%. © 2018 ACM.",Emotion role; Online social networks; Repost information,Behavioral research; Classification (of information); Classification accuracy; Emotion role; Including degrees; Multi dimensional; On-line social networks; Personalized recommendation; Repost information; Structural factor; Social networking (online)
An autonomic cognitive pattern for smart IoT-based system manageability: Application to comorbidity management,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058345957&doi=10.1145%2f3166070&partnerID=40&md5=f83bacbbd9ac84132a832ccd5158990c,"The adoption of the Internet of Things (IoT) drastically witnesses an increase in different domains and contributes to the fast digitalization of the universe. Henceforth, next generation of IoT-based systems are set to become more complex to design and manage. Collecting real-time IoT-generated data unleashes a new wave of opportunities for business to take more precise and accurate decisions at the right time. However, a set of challenges, including the design complexity of IoT-based systems and the management of the ensuing heterogeneous big data as well as the system scalability, need to be addressed for the development of flexible smart IoT-based systems. Consequently, we proposed a set of design patterns that diminish the system design complexity through selecting the appropriate combination of patterns based on the system requirements. These patterns identify four maturity levels for the design and development of smart IoT-based systems. In this article, we are mainly dealing with the system design complexity to manage the context changeability at runtime. Thus, we delineate the autonomic cognitive management pattern, which is at the most mature level. Based on the autonomic computing, this pattern identifies a combination of management processes able to continuously detect and manage the context changes. These processes are coordinated based on cognitive mechanisms that allow the system perceiving and understanding the meaning of the received data to make business decisions, as well as dynamically discovering new processes that meet the requirements evolution at runtime. We demonstrated the use of the proposed pattern with a use case from the healthcare domain; more precisely, the patient comorbidity management based on wearables. © 2018 Association for Computing Machinery.",Autonomic computing; Cognitive computing; Design patterns; Healthcare; IoT-based system; Maturity level,Big data; Epidemiology; Health care; Information management; Systems analysis; Autonomic Computing; Cognitive Computing; Design Patterns; IoT-based system; Maturity levels; Internet of things
Grouping peers based on complementary degree and social relationship using genetic algorithm,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058263916&doi=10.1145%2f3193180&partnerID=40&md5=cd33eeb288a0faa0695a9ae73209808f,"The aim of this article is to propose a new innovative grouping approach using the genetic algorithm (GA) to enhance the interaction and collaboration among peers by considering the complementary degree of students' learning status and their social relationships. In order to validate our approach, experiments were designed with a group of students and the outcomes were tested with an e-Learning system. The auto-grouping mechanism is developed using GA for better learning results, which is justified based on the performance of students tested on the e-Learning system. The outcomes clearly indicate that the proposed approach can generate a high degree of heterogeneous grouping and encourage students to learn better. The technical contribution of this article can be implemented in any massive open online course platforms with thousands of students, with regard to identifying peers for collaborative works. © 2018 ACM.",Automatic grouping; Cooperative learning; Genetic algorithm; MOOCs,E-learning; Learning algorithms; Learning systems; Social aspects; Students; Automatic grouping; Collaborative Work; Cooperative learning; Massive open online course; MOOCs; Social relationships; Technical contribution; Genetic algorithms
Context-driven and real-time provisioning of data-centric IoT services in the cloud,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058338332&doi=10.1145%2f3151006&partnerID=40&md5=d693263fb5a1c8c26050bb1ee0688e85,"The convergence of Internet of Things (IoT) and the Cloud has significantly facilitated the provision and management of services in large-scale applications, such as smart cities.With a huge number of IoT services accessible through clouds, it is very important to model and expose cloud-based IoT services in an efficient manner, promising easy and real-time delivery of cloud-based, data-centric IoT services. The existing work in this area has adopted a uniform and flat view to IoT services and their data, making it difficult to achieve the above goal. In this article, we propose a software framework, Context-driven And Real-time IoT (CARIoT) for real-time provisioning of cloud-based IoT services and their data, driven by their contextual properties. The main idea behind the proposed framework is to structure the description of data-centric IoT services and their real-time and historical data in a hierarchical form in accordance with the end-user application's context model. CARIoT features design choices and software services to realize this service provisioning model and the supporting data structures for hierarchical IoT data access. Using this approach, end-user applications can access IoT services and subscribe to their real-time and historical data in an efficient manner at different contextual levels, e.g., from a municipal district to a street in smart city use cases. We leverage a popular cloud-based data storage platform, called Firebase, to implement the CARIoT framework and evaluate its efficiency. The evaluation results show that CARIoT's hierarchical structure imposes no additional overhead with less data notification delay as compared to existing flat structures. © 2018 Association for Computing Machinery.",Cloud computing; Data-centric services; Internet of things,Cloud computing; Computer programming; Digital storage; Contextual properties; Data centric; End-user applications; Hierarchical structures; Internet of Things (IOT); Large-scale applications; Service provisioning; Software frameworks; Internet of things
Duplicate detection in programming question answering communities,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066906638&doi=10.1145%2f3169795&partnerID=40&md5=fd68a3e3adb8a2e0bcf26a3a34f99df1,"Community-based Question Answering (CQA) websites are attracting increasing numbers of users and contributors in recent years. However, duplicate questions frequently occur in CQA websites and are currently manually identified by the moderators. Automatic duplicate detection, on one hand, alleviates this laborious effort for moderators before taking close actions, and, on the other hand, helps question issuers quickly find answers. A number of studies have looked into related problems, but very limited works target Duplicate Detection in Programming CQA (PCQA), a branch of CQA that is dedicated to programmers. Existing works framed the task as a supervised learning problem on the question pairs and relied on only textual features. Moreover, the issue of selecting candidate duplicates from large volumes of historical questions is often un-addressed. To tackle these issues, we model duplicate detection as a two-stage ""ranking-classification"" problem over question pairs. In the first stage, we rank the historical questions according to their similarities to the newly issued question and select the top ranked ones as candidates to reduce the search space. In the second stage, we develop novel features that capture both textual similarity and latent semantics on question pairs, leveraging techniques in deep learning and information retrieval literature. Experiments on real-world questions about multiple programming languages demonstrate that our method works very well; in some cases, up to 25% improvement compared to the state-of-the-art benchmarks. © 2018 ACM.",Association rules; Classification; Community-based question answering; Latent semantics; Question quality,Deep learning; Moderators; Semantics; Websites; Candidate duplicates; Community-based question answering; Duplicate detection; Latent semantics; Question answering communities; State of the art; Supervised learning problems; Textual similarities; Problem oriented languages
Providing geo-elasticity in geographically distributed clouds,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081706883&doi=10.1145%2f3169794&partnerID=40&md5=9690ebbf75a60f3ce7328ed203789f23,"Geographically distributed cloud platforms are well suited for serving a geographically diverse user base. However, traditional cloud provisioning mechanisms that make local scaling decisions are not adequate for delivering the best possible performance for modern web applications that observe both temporal and spatial workload fluctuations. We propose GeoScale, a system that provides geo-elasticity by combining model-driven proactive and agile reactive provisioning approaches. GeoScale can dynamically provision server capacity at any location based on workload dynamics. We conduct a detailed evaluation of GeoScale on Amazon's geo-distributed cloud and show up to 40% improvement in the 95th percentile response time when compared to traditional elasticity techniques. © 2018 ACM.",Dynamic resource management; Geo-distributed clouds,Internet; Combining model; Distributed clouds; Local scaling; Location based; Server capacity; Temporal and spatial; WEB application; Elasticity
Cross-browser differences detection based on an empirical metric for web page visual similarity,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072765661&doi=10.1145%2f3140544&partnerID=40&md5=1cad3b22292eeaa6a78dd5d56d0ed6b2,"This article aims to develop a method to detect visual differences introduced into web pages when they are rendered in different browsers. To achieve this goal, we propose an empirical visual similarity metric by mimicking human mechanisms of perception. The Gestalt laws of grouping are translated into a computer compatible rule set. A block tree is then parsed by the rules for similarity calculation. During the translation of the Gestalt laws, experiments are performed to obtain metrics for proximity, color similarity, and image similarity. After a validation experiment, the empirical metric is employed to detect cross-browser differences. Experiments and case studies on the world's most popular web pages provide positive results for this methodology. © 2018 ACM.",Block tree; Cross-browser differences detection; Extended subtree; Gestalt laws of grouping; Web page visual similarity,Internet; Block trees; Color similarity; Experiments and case studies; Gestalt law; Image similarity; Similarity calculation; Visual differences; Visual similarity; Websites
Enhanced audit strategies for collaborative and accountable data sharing in social networks,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047109797&doi=10.1145%2f3134439&partnerID=40&md5=a82298b1e21dc4e9250482797c30a21b,"Data sharing and access control management is one of the issues still hindering the development of decentralized online social networks (DOSNs), which are now gaining more research attention with the recent developments in P2P computing, such as the secure public ledger-based protocols (Blockchains) for monetary systems. In a previous work, we proposed an initial audit-based model for access control in DOSNs. In this article, we focus on enhancing the audit strategies and the privacy issues emerging from records kept for audit purposes. We propose enhanced audit and collaboration strategies, for which experimental results, on a real online social network graph with simulated sharing behavior, show an improvement in the detection rate of bad behavior of more than 50% compared to the basic model.We also provide an analysis of the related privacy issues and discuss possible privacy-preserving alternatives. © 2018 ACM.",Accountability; Apriori access control; Decentralized social networks,Access control; Data privacy; Distributed computer systems; Information management; Network security; Online systems; Peer to peer networks; Research and development management; Social sciences computing; Access control managements; Accountability; Apriori; Detection rates; Monetary system; On-line social networks; P2P computing; Privacy preserving; Social networking (online)
Pre serving privacy as social responsibility in online social networks,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047149969&doi=10.1145%2f3158373&partnerID=40&md5=d60347ace841de4ed2de7b8ee72e9136,"Online social networks provide an environment for their users to share content with others, where the user who shares a content item is put in charge, generally ignoring others that might be affected by it. However, a content that is shared by one user can very well violate the privacy of other users. To remedy this, ideally, all users who are related to a content should get a say in how the content should be shared. Recent approaches advocate the use of agreement technologies to enable stakeholders of a post to discuss the privacy configurations of a post. This allows related individuals to express concerns so that various privacy violations are avoided up front. Existing techniques try to establish an agreement on a single post. However, most of the time, agreement should be established over multiple posts such that the user can tolerate slight breaches of privacy in return of a right to share posts themselves in future interactions. As a result, users can help each other preserve their privacy, viewing this as their social responsibility. This article develops a reciprocitybased negotiation for reaching privacy agreements among users and introduces a negotiation architecture that combines semantic privacy rules with utility functions. We evaluate our approach over multiagent simulations with software agents that mimic users based on a user study. © 2018 ACM.",Agreement; Negotiation; Privacy,Contracts; Data privacy; Economic and social effects; Semantics; Social aspects; Software agents; Agreement technologies; Multi agent simulation; Negotiation; Negotiation architectures; On-line social networks; Privacy agreements; Privacy violation; Social responsibilities; Social networking (online)
Rotten apples or bad harvest? What we are measuring when we are measuring abuse,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049177933&doi=10.1145%2f3122985&partnerID=40&md5=b96c17cfc5416863b678d7b37debb021,"Internet security and technology policy research regularly uses technical indicators of abuse to identify culprits and to tailor mitigation strategies. As a major obstacle, current inferences from abuse data that aim to characterize providers with poor security practices often use a naive normalization of abuse (abuse counts divided by network size) and do not take into account other inherent or structural properties of providers. Even the size estimates are subject to measurement errors relating to attribution, aggregation, and various sources of heterogeneity. More precise indicators are costly to measure at Internet scale. We address these issues for the case of hosting providers with a statistical model of the abuse data generation process, using phishing sites in hosting networks as a case study. We decompose error sources and then estimate key parameters of the model, controlling for heterogeneity in size and business model. We find that 84% of the variation in abuse counts across 45,358 hosting providers can be explained with structural factors alone. Informed by the fitted model, we systematically select and enrich a subset of 105 homogeneous “statistical twins” with additional explanatory variables, unreasonable to collect for all hosting providers. We find that abuse is positively associated with the popularity of websites hosted and with the prevalence of popular content management systems. Moreover, hosting providers who charge higher prices (after controlling for level differences between countries) witness less abuse. These structural factors together explain a further 77% of the remaining variation. This calls into question premature inferences from raw abuse indicators about the security efforts of actors, and suggests the adoption of similar analysis frameworks in all domains where network measurement aims at informing technology policy. © 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Abuse concentrations; Hosting providers; Measurement errors; Statistical modeling; Web security,Measurement errors; Security systems; Content management system; Explanatory variables; Hosting providers; Mitigation strategy; Network measurement; Statistical modeling; Technical indicator; WEB security; Network security
PrivacyCheck: Automatic summarization of privacy policies using data mining,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053447710&doi=10.1145%2f3127519&partnerID=40&md5=0e8dcc54a8acda78df498c27587a72aa,"Prior research shows that only a tiny percentage of users actually read the online privacy policies they implicitly agree to while using a website. Prior research also suggests that users ignore privacy policies because these policies are lengthy and, on average, require 2 years of college education to comprehend. We propose a novel technique that tackles this problem by automatically extracting summaries of online privacy policies. We use data mining models to analyze the text of privacy policies and answer 10 basic questions concerning the privacy and security of user data, what information is gathered from them, and how this information is used. In order to train the data mining models, we thoroughly study privacy policies of 400 companies (considering 10% of all listings on NYSE, Nasdaq, and AMEX stock markets) across industries. Our free Chrome browser extension, PrivacyCheck, utilizes the data mining models to summarize any HTML page that contains a privacy policy. PrivacyCheck stands out from currently available counterparts because it is readily applicable on any online privacy policy. Cross-validation results show that PrivacyCheck summaries are accurate 40% to 73% of the time. Over 400 independent Chrome users are currently using PrivacyCheck. © 2018 ACM.",Classification; Data mining; Privacy policy,Classification (of information); Data privacy; Electronic trading; Automatic summarization; College education; Cross validation; Data mining models; Novel techniques; Online privacy; Privacy and security; Privacy policies; Data mining
Measuring third-party tracker power across web and mobile,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053475791&doi=10.1145%2f3176246&partnerID=40&md5=981d062650b13a35b7054c7df7017e74,"Third-party networks collect vast amounts of data about users via websites and mobile applications. Consolidations among tracker companies can significantly increase their individual tracking capabilities, prompting scrutiny by competition regulators. Traditional measures of market share, based on revenue or sales, fail to represent the tracking capability of a tracker, especially if it spans both web and mobile. This article proposes a new approach to measure the concentration of tracking capability, based on the reach of a tracker on popular websites and apps. Our results reveal that tracker prominence and parent-subsidiary relationships have significant impact on accurately measuring concentration. © 2018 ACM.",Antitrust; Competition; Economics; Privacy; Tracking,Competition; Data privacy; Economics; Surface discharges; Antitrust; Market share; Mobile applications; New approaches; Third parties; Tracking capability; Websites
Automatic resolution of normative conflicts in supportive technology based on user values,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047158280&doi=10.1145%2f3158371&partnerID=40&md5=effe8de72a19d011f1ff5abb6efa4c7e,"Social commitments (SCs) provide a flexible, norm-based, governance structure for sharing and receiving data. However, users of data sharing applications can subscribe to multiple SCs, possibly producing opposing sharing and receiving requirements. We propose resolving such conflicts automatically through a conflict resolution model based on relevant user values such as privacy and safety. The model predicts a user's preferred resolution by choosing the commitment that best supports the user's values. We show through an empirical user study (n = 396) that values, as well as recency and norm type, significantly improve a system's ability to predict user preference in location sharing conflicts. © 2018 ACM.",Conflict resolution; Location sharing; Normative frameworks; Social commitments; Social media; User values,Conflict Resolution; Location sharing; Normative frameworks; Social commitment; Social media; User values; Internet
Special issue: Computational ethics and accountability,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047124929&doi=10.1145%2f3195835&partnerID=40&md5=9a40c6c9fa8ad188eb4a3c1980533e5e,[No abstract available],,
On the assessment of systematic risk in networked systems,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053461403&doi=10.1145%2f3166069&partnerID=40&md5=ea0455903104bcc05269138c4c35a252,"In a networked system, the risk of security compromises depends not only on each node's security but also on the topological structure formed by the connected individuals, businesses, and computer systems. Research in network security has been exploring this phenomenon for a long time, with a variety of modeling frameworks predicting how many nodes we should expect to lose, on average, for a given network topology, after certain types of incidents. Meanwhile, the pricing of insurance contracts for risks related to information technology (better known as cyber-insurance) requires determining additional information, for example, the maximum number of nodes we should expect to lose within a 99.5% confidence interval. Previous modeling research in network security has not addressed these types of questions, while research on cyber-insurance pricing for networked systems has not taken into account the network's topology. Our goal is to bridge that gap, by providing a mathematical basis for the assessment of systematic risk in networked systems. We define a loss-number distribution to be a probability distribution on the total number of compromised nodes within a network following the occurrence of a given incident, and we provide a number of modeling results that aim to be useful for cyber-insurers in this context. We prove NP-hardness for the general case of computing the loss-number distribution for an arbitrary network topology but obtain simplified computable formulas for the special cases of star topologies, ER-random topologies, and uniform topologies. We also provide a simulation algorithm that approximates the loss-number distribution for an arbitrary network topology and that appears to converge efficiently for many common classes of topologies. Scale-free network topologies have a degree distribution that follows a power law and are commonly found in real-world networks. We provide an example of a scale-free network in which a cyber-insurance pricing mechanism that relies naively on incidence reporting data will fail to accurately predict the true risk level of the entire system. We offer an alternative mechanism that yields an accurate forecast by taking into account the network topology, thus highlighting the lack/importance of topological data in security incident reporting. Our results constitute important steps toward the understanding of systematic risk and help to contribute to the emergence of a viable cyber-insurance market. © 2018 ACM.",Cyber-insurance; Economics of security; Networks; Risk mitigation; Scale-free networks; Security; Topology,Complex networks; Costs; Economics; Forecasting; Insurance; Networks (circuits); Probability distributions; Risk assessment; Topology; Cyber insurances; Degree distributions; Number distribution; Risk mitigation; Scale-free network topology; Security; Simulation algorithms; Topological structure; Network security
EasIE: Easy-to-use information extraction for constructing CSR databases from the Web,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047129776&doi=10.1145%2f3155807&partnerID=40&md5=1a5b39ebe730aec1fb6bbb3f374b2053,"Public awareness of and concerns about companies' social and environmental impacts have seen a marked increase over recent decades. In parallel, the quantity of relevant information has increased, as states pass laws requiring certain forms of reporting, researchers investigate companies' performance, and companies themselves seek to gain a competitive advantage by being seen to operate fairly and transparently. However, this information is typically dispersed and non-standardized, making it complicated to collect and analyze. To address this challenge, the WikiRate platform aims to collect this information and store it in a standardized format within a centralized public repository, making it much more amenable to analysis. In the context of WikiRate, this article introduces easIE, an easy-to-use information extraction (IE) framework that leverages generalWeb IE principles for building datasets with environmental, social, and governance information from the Web. To demonstrate the flexibility and value of easIE, we built a large-scale corporate social responsibility database comprising 654, 491 metrics related to 49, 009 companies spending less than 16 hours for data engineering, collection, and indexing. Finally, a data collection exercise involving 12 subjects was performed to showcase the ease of use of the developed framework. © 2018 ACM.","Corporate social responsibility (CSR); Environmental, social, and governance (ESG); Information extraction; Web wrapper","Artificial intelligence; Competition; Data acquisition; Environmental impact; Information retrieval; Social aspects; Competitive advantage; Corporate social responsibilities (CSR); Data engineering; Environmental, social, and governance (ESG); Public awareness; Public repositories; Social and environmental impact; Web wrappers; Information use"
Accountable protocols in abductive logic programming,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047163533&doi=10.1145%2f3107936&partnerID=40&md5=b6024519e5d18901a64d6ee43c846600,"Finding the entity responsible for an unpleasant situation is often difficult, especially in artificial agent societies. SCIFF is a formalization of agent societies, including a language to describe rules and protocols, and an abductive proof procedure for compliance checking. However, how to identify the entity responsible for a violation is not always clear. In this work, a definition of accountability for artificial societies is formalized in SCIFF. Two tools are provided for the designer of interaction protocols: a guideline, in terms of syntactic features that ensure accountability of the protocol, and an algorithm (implemented in a software tool) to investigate if, for a given protocol, nonaccountability issues could arise. © 2018 ACM.",Abductive logic programming; Accountability; SCIFF,Compliance control; Computer circuits; Abductive logic programming; Accountability; Artificial agents; Artificial societies; Compliance checking; Inter-action protocols; SCIFF; Syntactic features; Logic programming
Measuring moral acceptability in E-deliberation: A practical application of ethics by participation,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047159167&doi=10.1145%2f3183324&partnerID=40&md5=add2f34758ac22bc69624685f1eb0924,"Current developments in governance and policy setting are challenging traditional top-down models of decision-making. Whereas, on the one hand, citizens are increasingly demanding and expected to participate directly on governance questions, social networking platforms are, on the other hand, increasingly providing podia for the spread of unfounded, extremist and/or harmful ideas. Participatory deliberation is a form of democratic policy making in which deliberation is central to decision-making using both consensus decision-making and majority rule. However, by definition, it will lead to socially accepted results rather than ensuring the moral acceptability of the result. In fact, participation per se offers no guidance regarding the ethics of the decisions taken, nor does it provide means to evaluate alternatives in terms of their moral ""quality."" This article proposes an open participatory model, Massive Open Online Deliberation (MOOD), that can be used to solve some of the current policy authority deficits. MOOD taps on individual understanding and opinions by harnessing open, participatory, crowd-sourced, and wiki-like methodologies, effectively producing collective judgements regarding complex political and social issues in real time. MOOD offers the opportunity for people to develop and draft collective judgements on complex issues and crises in real time. MOOD is based on the concept of Ethics by Participation, a formalized and guided process of moral deliberation that extends deliberative democracy platforms to identify morally acceptable outcomes and enhance critical thinking and reflection among participants. © 2018 ACM.",Ethics by participation; Online deliberation; Participatory systems,Behavioral research; Online systems; Philosophical aspects; Public policy; Critical thinking; Ethics by participation; Majority rule; Online deliberation; Participatory modeling; Policy setting; Social issues; Top down models; Decision making
SHARE: A stackelberg honey-based adversarial reasoning engine,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055875929&doi=10.1145%2f3137571&partnerID=40&md5=942f3a9ff9d03665c3cffd866dd7e5c6,"A ""noisy-rich"" (NR) cyber-attacker (Lippmann et al. 2012) is one who tries all available vulnerabilities until he or she successfully compromises the targeted network. We develop an adversarial foundation, based on Stackelberg games, for how NR-attackers will explore an enterprise network and how they will attack it, based on the concept of a system vulnerability dependency graph. We develop a mechanism by which the network can be modified by the defender to induce deception by placing honey nodes and apparent vulnerabilities into the network to minimize the expected impact of the NR-attacker's attacks (according to multiple measures of impact). We also consider the case where the adversary learns from blocked attacks using reinforcement learning. We run detailed experiments with real network data (but with simulated attack data) and show that Stackelberg Honey-based Adversarial Reasoning Engine performs very well, even when the adversary deviates from the initial assumptions made about his or her behavior. We also develop a method for the attacker to use reinforcement learning when his or her activities are stopped by the defender. We propose two stopping policies for the defender: Stop Upon Detection allows the attacker to learn about the defender's strategy and (according to our experiments) leads to significant damage in the long run, whereas Stop After Delay allows the defender to introduce greater uncertainty into the attacker, leading to better defendability in the long run. © 2018 ACM.",Adversarial models; Computer security; Enterprise systems,Damage detection; Engines; Food products; Network security; Adversarial reasoning; Cyber attackers; Dependency graphs; Enterprise networks; Real network datum; Simulated attacks; Stackelberg Games; System vulnerability; Reinforcement learning
Toward efficient short-video sharing in the youtube social network,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080360222&doi=10.1145%2f3137569&partnerID=40&md5=72df9c7cff231705d04ab75b1c896c9c,"The past few years have seen an explosion in the popularity of online short-video sharing in YouTube. As the number of users continue to grow, the bandwidth required to maintain acceptable quality of service (QoS) has greatly increased. Peer-to-peer (P2P) architectures have shown promise in reducing the bandwidth costs; however, the previous works build one P2P overlay for each video, which provides limited availability of video providers and produces high overlay maintenance overhead. To handle these problems, in this work, we novelly leverage the existing social network in YouTube, where a user subscribes to another user's channel to track all his/her uploaded videos. The subscribers of a channel tend to watch the channel's videos and common-interest nodes tend to watch the same videos. Also, the popularity of videos in one channel varies greatly. We study real trace data to confirm these properties. Based on these properties, we propose SocialTube, which builds the subscribers of one channel into a P2P overlay and also clusters common-interest nodes in a higher level. It also incorporates a prefetching algorithm that prefetches higher-popularity videos. To enhance the system performance, we further propose the demand/supply-based cache management scheme and reputation-based neighbor management scheme. Extensive trace-driven simulation results and PlanetLab real-world experimental results verify the effectiveness of SocialTube at reducing server load and overlay maintenance overhead and at improving QoS for users. © 2018 ACM.",P2P networks; Social networks; Video on demand; Youtube,Bandwidth; Cost reduction; Quality of service; Watches; Cache management schemes; Common interests; Maintenance overhead; Management scheme; Peer-to-peer architectures; Prefetching algorithm; Trace driven simulation; Video sharing; Social networking (online)
Introduction to the special issue on emerging software technologies for internet-based systems: Internetware and DevOps,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047079884&doi=10.1145%2f3173572&partnerID=40&md5=78aa478eba30b2d427a0f1a4d90668ae,[No abstract available],,
Genetic algorithms for solving problems of access control design and reconfiguration in computer networks,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085117303&doi=10.1145%2f3093898&partnerID=40&md5=410eedbce1f6b9b416c71653a51fdc5f,"To create solutions for providing the required access control in computer networks it is not sufficient to have only tools and protocols in the network that are needed for it. It is necessary to create corresponding configuration, or scheme, of such tools, which will allow us to satisfy the existing security requirements. At the same time, the problems of creating an access control scheme, as a rule, are NP-complete and require heuristic models for their solving. In this article, we propose a unified approach to creation of control access schemes, based on usage of genetic algorithms. The approach is applied not only to original schemes configuration but to reconfiguration as well. Successful testing of the suggested approach on RBAC, VLAN, and VPN schemes allows us to suppose that it may be applied to other types of access control schemes as well. Experimental testing of suggested genetic algorithms, performed on a specially designed test bed, showed their sufficiently high efficiency. © 2018 ACM.",Genetic algorithm; Role-based access control; Virtual local area network; Virtual private network,Computer networks; Genetic algorithms; Well testing; Access control schemes; Control access; Experimental testing; Heuristic model; High-efficiency; NP Complete; Security requirements; Unified approach; Access control
Guest editors' introduction,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084752117&doi=10.1145%2f3177884&partnerID=40&md5=b34f08adbef2bd688f73aad5209df882,[No abstract available],,
Decision networks for security risk assessment of critical infrastructures,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065236931&doi=10.1145%2f3137570&partnerID=40&md5=cd2d93db52a2425d00828a170b1edc5d,"We exploit Decision Networks (DN) for the analysis of attack/defense scenarios in critical infrastructures. DN extend Bayesian Networks (BN) with decision and value nodes. DN inherit from BN the possibility to naturally address uncertainty at every level, making possible the modeling of situations that are not limited to Boolean combinations of events. By means of decision nodes, DN can include the interaction level of attacks and countermeasures. Inference algorithms can be directly exploited for implementing a probabilistic analysis of both the risk and the importance of the attacks. Thanks to value nodes, a sound decision theoretic analysis has the goal of selecting the optimal set of countermeasures to activate. © 2018 ACM.",BGP; Critical infrastructures; Decision Networks; Impact; Importance measures; Return on investment; Risk; SCADA,Bayesian networks; Critical infrastructures; Inference engines; Public works; Uncertainty analysis; Boolean combinations; Decision network; Inference algorithm; Interaction levels; Optimal sets; Probabilistic analysis; Security risk assessments; Sound decision; Risk assessment
Interest-aware content discovery in peer-to-peer social networks,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048016136&doi=10.1145%2f3176247&partnerID=40&md5=9efa28894613bac61c7e995c21a15594,"With the increasing popularity and rapid development of Online Social Networks (OSNs), OSNs not only bring fundamental changes to information and communication technologies, but also make an extensive and profound impact on all aspects of our social life. Efficient content discovery is a fundamental challenge for large-scale distributed OSNs. However, the similarity between social networks and online social networks leads us to believe that the existing social theories are useful for improving the performance of social content discovery in online social networks. In this article, we propose an interest-aware social-like peer-to-peer (IASLP) model for social content discovery in OSNs by mimicking ten different social theories and strategies. In the IASLP network, network nodes with similar interests can meet, help each other, and co-operate autonomously to identify useful contents. The presented model has been evaluated and simulated in a dynamic environment with an evolving network. The experimental results show that the recall of IASLP is 20% higher than the existing method SESD while the overhead is 10% lower. The IASLP can generate higher flexibility and adaptability and achieve better performance than the existing methods. © 2018 ACM.",Content discovery; Online social networks; Self-organization,Peer to peer networks; Social sciences computing; Content discoveries; Dynamic environments; Fundamental changes; Information and Communication Technologies; On-line social networks; Online social networks (OSNs); Self organizations; Similar Interests; Social networking (online)
Are we there yet? ipv6 in Australia and China,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042105611&doi=10.1145%2f3158374&partnerID=40&md5=80678ea7bf8480528e4f6ffad8705bb8,"IP (Internet Protocol) version 6 (IPv6) was standardised in 1998 to address the expected runout of IP version 4 (IPv4) addresses. However, the transition from IPv4 to IPv6 has been very slow in many countries. We investigate the state of IPv6 deployment in Australian and Chinese organisations based on a survey of organisations’ IT staff. Compared to earlier studies, IPv6 deployment has advanced markedly, but it is still years away for a significant portion of organisations. We provide insights into the deployment problems, arguments for deploying IPv6, and how to speed up the transition, which are relevant for many countries.",IPv6 deployment Australia and China,Societies and institutions; Australia; Deployment problems; IP (internet protocol); IPv6 deployments; IT staff; Run outs; Speed up; Internet protocols
Architectural principles for cloud software,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042490583&doi=10.1145%2f3104028&partnerID=40&md5=0977e25bfaeec439adb4158e75a7fdeb,"A cloud is a distributed Internet-based software system providing resources as tiered services. Through service-orientation and virtualization for resource provisioning, cloud applications can be deployed and managed dynamically. We discuss the building blocks of an architectural style for cloud-based software systems. We capture style-defining architectural principles and patterns for control-theoretic, model-based architectures for cloud software. While service orientation is agreed on in the form of service-oriented architecture and microservices, challenges resulting from multi-tiered, distributed and heterogeneous cloud architectures cause uncertainty that has not been sufficiently addressed. We define principles and patterns needed for effective development and operation of adaptive cloud-native systems. © 2018 ACM.",Adaptive system; Architectural style; Cloud computing; Cloud-native; Control theory; Devops; Microservice; Model-based controller; Software architecture; Uncertainty,Adaptive control systems; Adaptive systems; Architecture; Cloud computing; Computation theory; Computer software; Control theory; Distributed computer systems; Information services; Service oriented architecture (SOA); Software architecture; Architectural style; Cloud natives; Devops; Microservice; Model-based controller; Uncertainty; Computer architecture
A learning-based framework for improving querying on web interfaces of curated knowledge bases,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041706992&doi=10.1145%2f3155806&partnerID=40&md5=b1da878edf146d78cb2a31000fc2b9bb,"Knowledge Bases (KBs) are widely used as one of the fundamental components in SemanticWeb applications as they provide facts and relationships that can be automatically understood by machines. Curated knowledge bases usually use Resource Description Framework (RDF) as the data representation model. To query the RDF-presented knowledge in curated KBs, Web interfaces are built via SPARQL Endpoints. Currently, querying SPARQL Endpoints has problems like network instability and latency, which affect the query efficiency. To address these issues, we propose a client-side caching framework, SPARQL Endpoint Caching Framework (SECF), aiming at accelerating the overall querying speed over SPARQL Endpoints. SECF identifies the potential issued queries by leveraging the querying patterns learned from clients' historical queries and prefecthes/caches these queries. In particular,we develop a distance function based on graph edit distance to measure the similarity of SPARQL queries. We propose a feature modelling method to transform SPARQL queries to vector representation that are fed into machine-learning algorithms. A time-aware smoothingbased method, Modified Simple Exponential Smoothing (MSES), is developed for cache replacement. Extensive experiments performed on real-world queries showcase the effectiveness of our approach, which outperforms the state-of-the-art work in terms of the overall querying speed. © 2018 ACM.",Caching; Knowledge base query-answering; Query suggestion; SPARQL,Knowledge based systems; Learning systems; Pattern matching; Caching; Data representation models; Query answering; Query suggestion; Resource description framework; Simple exponential smoothing; SPARQL; Vector representations; Learning algorithms
Fuzzy clustering of crowdsourced test reports for apps,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042481897&doi=10.1145%2f3106164&partnerID=40&md5=be04d035175511d1e3cd073e48800ee7,"DevOps is a new approach to drive a seamless Application (App) cycle from development to delivery. As a critical part to promote the successful implementation of DevOps, testing can significantly improve team productivity and reliably deliver user experience. However, it is difficult to use traditional testing to cover diverse mobile phones, network environments, operating systems, and so on. Hence, many large companies crowdsource their App testing tasks to workers from open platforms. In crowdsourced testing, test reports submitted by workers May be highly redundant, and their quality May vary sharply. Meanwhile, multi-bug test reports May be submitted, and their root causes are hard to diagnose. Hence, it is a time-consuming and tedious task for developers to manually inspect these test reports. To help developers address the above challenges, we issue the new problem of Fuzzy Clustering Test Reports (FULTER). Aiming to resolve FULTER, a series of barriers need to be overcome. In this study, we propose a new framework named Test Report Fuzzy Clustering Framework (TERFUR) by aggregating redundant and multi-bug test reports into clusters to reduce the number of inspected test reports. First, we construct a filter to remove invalid test reports to break through the invalid barrier. Then, a preprocessor is built to enhance the descriptions of short test reports to break through the uneven barrier. Last, a two-phase merging algorithm is proposed to partition redundant and multi-bug test reports into clusters that can break through the multi-bug barrier. Experimental results over 1,728 test reports from five industrial Apps show that TERFUR can cluster test reports by up to 78.15% in terms of AverageP, 78.41% in terms of AverageR, and 75.82% in terms of AverageF1 and outperform comparative methods by up to 31.69%, 33.06%, and 24.55%, respectively. In addition, the effectiveness of TERFUR is validated in prioritizing test reports for manual inspection. © 2018 ACM.",Crowdsourced testing; Duplicate detection; Fuzzy clustering; Test report; Unsupervised method,Cellular telephone systems; Fuzzy clustering; Inspection; Comparative methods; Duplicate detection; Manual inspection; Merging algorithms; Network environments; Team productivities; Test reports; Unsupervised method; Testing
Fine-grained access control via policy-carrying data,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041733339&doi=10.1145%2f3133324&partnerID=40&md5=769ac7c4c1e14cf3d77c40b7a65956ce,"We address the problem of associating access policies with datasets and how to monitor compliance via policy-carrying data. Our contributions are a formal model in first-order logic inspired by normative multiagent systems to regulate data access, and a computational model for the validation of specific use cases and the verification of policies against criteria. Existing work on access policy identifies roles as a key enabler, with which we concur, but much of the rest focusses on authentication and authorization technology. Our proposal aims to address the normative principles put forward in Berners-Lee's bill of rights for the internet, through human-readable but machine-processable access control policies. © 2018 ACM.",Action language; Answer set programming; Data sharing; Deontic logic; Privacy policy,Authentication; Computation theory; Computer circuits; Computer programming; Formal logic; Logic programming; Multi agent systems; Action language; Answer set programming; Data Sharing; Deontic Logic; Privacy policies; Access control
Quantifying privacy leakage in multi-agent planning,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041702754&doi=10.1145%2f3133326&partnerID=40&md5=01e2f5add57326ac2ae6467c6feb1842,"Multi-agent planning using MA-STRIPS-related models is often motivated by the preservation of private information. Such a motivation is not only natural for multi-agent systems but also is one of the main reasons multi-agent planning problems cannot be solved with a centralized approach. Although the motivation is common in the literature, the formal treatment of privacy is often missing. In this article, we expand on a privacy measure based on information leakage introduced in previous work, where the leaked information is measured in terms of transition systems represented by the public part of the problem with regard to the information obtained during the planning process. Moreover, we present a general approach to computing privacy leakage of search-based multi-agent planners by utilizing search-tree reconstruction and classification of leaked superfluous information about the applicability of actions. Finally, we present an analysis of the privacy leakage of two well-known algorithms-multi-agent forward search (MAFS) and Secure-MAFS- both in general and on a particular example. The results of the analysis show that Secure-MAFS leaks less information than MAFS. © 2018 ACM.",Deterministic domain-independent planning; Multi-agent planning; privacy; Security,Classification (of information); Data privacy; Motivation; Centralized approaches; Deterministic domains; Information leakage; Multi-agent planning; Planning process; Private information; Security; Transition system; Multi agent systems
Collaborative location recommendation by integrating multi-dimensional contextual information,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041707486&doi=10.1145%2f3134438&partnerID=40&md5=b85d7fc5ded96e8abd823210d2898ea9,"Point-of-Interest (POI) recommendation is a new type of recommendation task that comes along with the prevalence of location-based social networks and services in recent years. Compared with traditional recommendation tasks, POI recommendation focuses more on making personalized and context-aware recommendations to improve user experience. Traditionally, the most commonly used contextual information includes geographical and social context information. However, the increasing availability of check-in data makes it possible to design more effective location recommendation applications by modeling and integrating comprehensive types of contextual information, especially the temporal information. In this article, we propose a collaborative filtering method based on Tensor Factorization, a generalization of the Matrix Factorization approach, tomodel themulti-dimensional contextual information. Tensor Factorization naturally extendsMatrix Factorization by increasing the dimensionality of concerns, within which the three-dimensional model is the one most popularly used. Our method exploits a high-order tensor to fuse heterogeneous contextual information about users' check-ins instead of the traditional two-dimensional user-location matrix. The factorization of this tensor leads to a more compact model of the data that is naturally suitable for integrating contextual information to make POI recommendations. Based on the model, we further improve the recommendation accuracy by utilizing the internal relations within users and locations to regularize the latent factors. Experimental results on a large real-world dataset demonstrate the effectiveness of our approach. © 2018 ACM.",Location-based social networks; Recommendation; Regularization; Tensor factorization,Collaborative filtering; Data integration; Factorization; Location; Location based services; Matrix algebra; Tensors; Collaborative filtering methods; Collaborative locations; Context-aware recommendations; Location-based social networks; Recommendation; Regularization; Tensor factorization; Three-dimensional model; Information filtering
Utility-Based decision making for migrating cloud-Based applications,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042517089&doi=10.1145%2f3140545&partnerID=40&md5=e340bc5d8f1ba602ed5baf840d2fce8a,"Nowadays, cloud providers offer a broad catalog of services for migrating and distributing applications in the cloud. However, the existence of a wide spectrum of cloud services has become a challenge for deciding where to host applications, as these vary in performance and cost. This work addresses such a challenge, and provides a utility-based decision support model and method that evaluates and ranks during design time potential application distributions spanned among heterogeneous cloud services. The utility model is evaluated using the MediaWiki (Wikipedia) application, and shows an improved efficiency for selecting cloud services in comparison to other decision making approaches. © 2018 ACM.",Cloud application topologies; Cloud services selection; Decision making; Utility theory,Decision support systems; Decision theory; Distributed database systems; Web services; Cloud applications; Cloud providers; Cloud services; Cloud-based applications; Decision support models; Utility model; Utility theory; Wide spectrum; Decision making
Privacy-preserving publishing of multilevel utility-controlled graph datasets,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042931049&doi=10.1145%2f3125622&partnerID=40&md5=bf4a16c978c6c65b578945df6a4d82bf,"Conventional private data publication schemes are targeted at publication of sensitive datasets either after the k-anonymization process or through differential privacy constraints. Typically these schemes are designed with the objective of retaining as much utility as possible for the aggregate queries while ensuring the privacy of the individual records. Such an approach, though suitable for publishing aggregate information as public datasets, is inapplicable when users have different levels of access to the same data. We argue that existing schemes either result in increased disclosure of private information or lead to reduced utility when some users have more access privileges than the others. In this article, we present an anonymization framework for publishing large datasets with the goals of providing different levels of utility to the users based on their access privilege levels. We design and implement our proposed multilevel utility-controlled anonymization schemes in the context of large association graphs considering three levels of user utility, namely, (1) users having access to only the graph structure, (2) users having access to the graph structure and aggregate query results, and (3) users having access to the graph structure, aggregate query results, and individual associations. Our experiments on real large association graphs show that the proposed techniques are effective and scalable 4 and yield the required level of privacy and utility for each user privacy and access privilege level. © 2018 ACM.",Association datasets; Bipartite graphs; Data anonymization; Data privacy; Multilevel privacy,Graphic methods; Aggregate queries; Association graph; Bipartite graphs; Data anonymization; Design and implements; Differential privacies; Privacy preserving; Private information; Data privacy
i-Jacob: An internetware-oriented approach to optimizing computation-intensive mobile web browsing,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047012333&doi=10.1145%2f3093899&partnerID=40&md5=9cb0f20b3147c42a783d9ce92aa01376,"Web browsing is always a key requirement of Internet users. Current mobile Web apps can contain computation-intensive JavaScript logics and thus affect browsing performance. Learning from our over-decade research and development experiences of the Internetware paradigm, we present the novel and generic i-Jacob approach to improving the performance of mobile Web browsing with effective JavaScript-code offloading. Our approach proposes a programming abstraction to make mobile Web situational and adaptive to contexts, by specifying the computation-intensive and “offloadable” code, and develops a platform-independent lightweight runtime spanning the mobile devices and the cloud. We demonstrate the efficiency of i-Jacob with some typical computation-intensive tasks over various combinations of hardware, operating systems, browsers, and network connections. The improvements can reach up to 49× speed-up in response time and 90% saving in energy. © 2018 ACM.",Energy; JavaScript offloading; Mobile Web; Performance,High level languages; Computation intensives; Computation-intensive task; Energy; Javascript; Mobile web; Performance; Programming abstractions; Research and development; Computation theory
Multi-objective optimization of deployment topologies for distributed applications,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041432521&doi=10.1145%2f3106158&partnerID=40&md5=8d927d1fb9b1465e91e0cf0c559337cc,"Modern applications are typically implemented as distributed systems comprising several components. Deciding where to deploy which component is a difficult task that today is usually assisted by logical topology recommendations. Choosing inefficient topologies allocates the wrong amount of resources, leads to unnecessary operation costs, or results in poor performance. Testing different topologies to find good solutions takes a lot of time and might delay productive operations. Therefore, this work introduces a software-based deployment topology optimization approach for distributed applications. We use an enhanced performance model generator that extracts models from operational monitoring data of running applications. The extracted model is used to simulate performance metrics (e.g., resource utilization, response times, throughput) and runtime costs of distributed applications. Subsequently, we introduce a deployment topology optimizer, which selects an optimized topology for a specified workload and considers on-premise, cloud, and hybrid topologies. The following three optimization goals are presented in this work: (i) minimum response time for an optimized user experience, (ii) approximate resource utilization around certain peaks, and (iii) minimum cost for running the application. To evaluate the approach, we use the SPECjEnterpriseNEXT industry benchmark as distributed application in an on-premise and in a cloud/on-premise hybrid environment. The evaluation demonstrates the accuracy of the simulation compared to the actual deployment by deploying an optimized topology and comparing measurements with simulation results.",Deployment topology optimzation; Distributed enterprise applications; Memory simulation; Performance model; Performance model generation,Application programs; Benchmarking; Costs; Multiobjective optimization; Deployment topology optimzation; Distributed applications; Distributed enterprise application; Operational monitoring; Performance metrics; Performance Model; Resource utilizations; Running applications; Topology
An online algorithm for task offloading in heterogeneous mobile clouds,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041445921&doi=10.1145%2f3122981&partnerID=40&md5=bebf74df398e9a93d19ab2844becd69a,"Mobile cloud computing is emerging as a promising approach to enrich user experiences at the mobile device end. Computation offloading in a heterogeneous mobile cloud environment has recently drawn increasing attention in research. The computation offloading decision making and tasks scheduling among heterogeneous shared resources in mobile clouds are becoming challenging problems in terms of providing global optimal task response time and energy efficiency. In this article, we address these two problems together in a heterogeneous mobile cloud environment as an optimization problem. Different from conventional distributed computing system scheduling problems, our joint offloading and scheduling optimization problem considers unique contexts of mobile clouds such as wireless network connections and mobile device mobility, which makes the problem more complex. We propose a context-aware mixed integer programming model to provide off-line optimal solutions for making the offloading decisions and scheduling the offloaded tasks among the shared computing resources in heterogeneous mobile clouds. The objective is to minimize the global task completion time (i.e., makespan). To solve the problem in real time, we further propose a deterministic online algorithm-the Online Code Offloading and Scheduling (OCOS) algorithm-based on the rent/buy problem and prove the algorithm is 2-competitive. Performance evaluation results show that the OCOS algorithm can generate schedules that have around two times shorter makespan than conventional independent task scheduling algorithms. Also, it can save around 30% more on makespan of task execution schedules than conventional offloading strategies, and scales well as the number of users grows.",Code offloading; Mixed integer programming; Mobile cloud computing; Online scheduling,Cloud computing; Decision making; Distributed computer systems; Energy efficiency; Integer programming; Mobile devices; Optimization; Problem solving; Scheduling; Scheduling algorithms; Code offloading; Computation offloading; Deterministic online algorithms; Distributed computing systems; Mixed integer programming; Mixed integer programming model; Online scheduling; Scheduling optimization; Mobile cloud computing
Crowdservice: Optimizing mobile crowdsourcing and service composition,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041436805&doi=10.1145%2f3108935&partnerID=40&md5=e99ca90f8b1655616c0f02df2beb6dad,"Some user needs can only be met by leveraging the capabilities of others to undertake particular tasks that require intelligence and labor. Crowdsourcing such capabilities is one way to achieve this. But providing a service that leverages crowd intelligence and labor is a challenge, since various factors need to be considered to enable reliable service provisioning. For example, the selection of an optimal set of workers from those who bid to perform a task needs to be made based on their reliability, expected reward, and distance to the target locations. Moreover, for an application involving multiple services, the overall cost and time constraints must be optimally allocated to each involved service. In this article, we develop a framework, named CrowdService, that supplies crowd intelligence and labor as publicly accessible crowd services via mobile crowdsourcing. The article extends our earlier work by providing an approach for constraints synthesis and worker selection. It employs a genetic algorithm to dynamically synthesize and update near-optimal cost and time constraints for each crowd service involved in a composite service and selects a near-optimal set of workers for each crowd service to be executed. We implement the proposed framework on Android platforms and evaluate its effectiveness, scalability, and usability in both experimental and user studies.",Collaboration; Mobile crowdsourcing; Reliability; Service composition,Genetic algorithms; Quality of service; Reliability; Android platforms; Collaboration; Composite services; Mobile crowdsourcing; Multiple services; Publicly accessible; Service compositions; Service provisioning; Crowdsourcing
Improving verification accuracy of CPS by modeling and calibrating interaction uncertainty,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041445999&doi=10.1145%2f3093894&partnerID=40&md5=8457b83e8c7061b881ac060d210d46c3,"Cyber-Physical Systems (CPS) intrinsically combine hardware and physical systems with software and network, which are together creating complex and correlated interactions. CPS applications often experience uncertainty in interacting with environment through unreliable sensors. They can be faulty and exhibit runtime errors if developers have not considered environmental interaction uncertainty adequately. Existing work in verifying CPS applications ignores interaction uncertainty and thus may overlook uncertainty-related faults. To improve verification accuracy, in this article we propose a novel approach to verifying CPS applications with explicit modeling of uncertainty arisen in the interaction between them and the environment. Our approach builds an Interactive State Machine network for a CPS application and models interaction uncertainty by error ranges and distributions. Then it encodes both the application and uncertainty models to Satisfiability Modulo Theories (SMT) formula to leverage SMT solvers searching for counterexamples that represent application failures. The precision of uncertainty model can affect the verification results. However, it may be difficult to model interaction uncertainty precisely enough at the beginning, because of the uncontrollable noise of sensors and insufficient data sample size. To further improve the accuracy of the verification results, we propose an approach to identifying and calibrating imprecise uncertainty models. We exploit the inconsistency between the counterexamples' estimate and actual occurrence probabilities to identify possible imprecision in uncertainty models, and the calibration of imprecise models is to minimize the inconsistency, which is reduced to a Search-Based Software Engineering problem. We experimentally evaluated our verification and calibration approaches with real-world CPS applications, and the experimental results confirmed their effectiveness and efficiency.",Calibration; Cyber-physical systems; Verification,Calibration; Cyber Physical System; Embedded systems; Probability; Software engineering; Verification; Cyber-physical systems (CPS); Effectiveness and efficiencies; Environmental interactions; Occurrence probability; Satisfiability modulo Theories; Search-based software engineering; Verification and calibrations; Verification results; Uncertainty analysis
Exploiting content spatial distribution to improve detection of intrusions,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041451635&doi=10.1145%2f3143422&partnerID=40&md5=50e27d7962fe25d3d8d05b8ae2e5ce70,"We present PCkAD, a novel semisupervised anomaly-based IDS (Intrusion Detection System) technique, detecting application-level content-based attacks. Its peculiarity is to learn legitimate payloads by splitting packets into chunks and determining the within-packet distribution of n-grams. This strategy is resistant to evasion techniques as blending. We prove that finding the right legitimate content is NP-hard in the presence of chunks. Moreover, it improves the false-positive rate for a given detection rate with respect to the case where the spatial information is not considered. Comparison with well-known IDSs using n-grams highlights that PCkAD achieves state-of-the-art performances.",Anomaly detection; Intrusion detection systems; N-grams; Semisupervised learning,Blending; Computer crime; Mercury (metal); Anomaly detection; False positive rates; Intrusion Detection Systems; N-grams; Packet distribution; Semi- supervised learning; Spatial informations; State-of-the-art performance; Intrusion detection
Adaptive speculation for efficient internetware application execution in clouds,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041436763&doi=10.1145%2f3093896&partnerID=40&md5=2f9caf95a586d5db3921d63de6af13fe,"Modern Cloud computing systems are massive in scale, featuring environments that can execute highly dynamic Internetware applications with huge numbers of interacting tasks. This has led to a substantial challenge-the straggler problem, whereby a small subset of slow tasks significantly impede parallel job completion. This problem results in longer service responses, degraded system performance, and late timing failures that can easily threaten Quality of Service (QoS) compliance. Speculative execution (or speculation) is the prominent method deployed in Clouds to tolerate stragglers by creating task replicas at runtime. The method detects stragglers by specifying a predefined threshold to calculate the difference between individual tasks and the average task progression within a job. However, such a static threshold debilitates speculation effectiveness as it fails to capture the intrinsic diversity of timing constraints in Internetware applications, as well as dynamic environmental factors, such as resource utilization. By considering such characteristics, different levels of strictness for replica creation can be imposed to adaptively achieve specified levels of QoS for different applications. In this article, we present an algorithm to improve the execution efficiency of Internetware applications by dynamically calculating the straggler threshold, considering key parameters including job QoS timing constraints, task execution progress, and optimal system resource utilization. We implement this dynamic straggler threshold into the YARN architecture to evaluate it's effectiveness against existing state-of-the-art solutions. Results demonstrate that the proposed approach is capable of reducing parallel job response time by up to 20% compared to the static threshold, as well as a higher speculation success rate, achieving up to 66.67% against 16.67% in comparison to the static method.",Adaptive speculation; Execution efficiency; QoS; Replicas; Stragglers,Distributed computer systems; Efficiency; Adaptive speculation; Application execution; Environmental factors; Replicas; Resource utilizations; Speculative execution; Stragglers; System resource utilization; Quality of service
CloudMF: Model-driven management of multi-cloud applications,2018,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041437398&doi=10.1145%2f3125621&partnerID=40&md5=5f750525e61dc00c590d3092c15a9918,"While the number of cloud solutions is continuously increasing, the development and operation of largescale and distributed cloud applications are still challenging. A major challenge is the lack of interoperability between the existing cloud solutions, which increases the complexity of maintaining and evolving complex applications potentially deployed across multiple cloud infrastructures and platforms. In this article, we show how the Cloud Modelling Framework leverages model-driven engineering and supports the DevOps ideas to tame this complexity by providing: (i) a domain-specific language for specifying the provisioning and deployment of multi-cloud applications, and (ii) a models@run-time environment for their continuous provisioning, deployment, and adaptation.",Cloud computing; DevOps; Model-driven engineering; Models@run-time; Multi-cloud,Cloud computing; Computer programming languages; Modeling languages; Problem oriented languages; Cloud infrastructures; Development and operations; DevOps; Domain specific languages; Model-driven Engineering; Multi-clouds; Runtime environments; Runtimes; Embedded systems
Seamless virtual network for international business continuity in presence of intentional blocks,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040121263&doi=10.1145%2f3133325&partnerID=40&md5=0eb04ebb6b4422852e90d40fd52c623d,"In developing countries, links are poor among domestic communities or internet service providers. Besides, international internet channels are suddenly blocked by such as Golden Shield (GS) in China. Offshore business communications are involved in these. To avoid such involvement, a seamless virtual network is proposed as an international business communication bridging solution. This uses Round Trip Time (RTT) based multiple thresholds for differential switch to Virtual Private Network (VPN) bypass. The characteristics are (1) using multiple threshold integrated differential calculus of RTT increase, a sign of the block is recognized as the steep staircase increase of RTT, (2) followed by the immediate automatic switch to VPN having RTT below 200ms. (3) Asymmetrically, only the absolute threshold value and continuation time are used to determine when to switch back. This method is analytically and statistically evaluated as being successful (below 3% errors), using around 200 cases of data on GS blocks. Furthermore, it has been validated by the real seamless usage in more than 20 offshore companies for three years. Besides response time in offshore applications, our method can also alleviate problems such as voice echoes and video jitters which irritate business users. These effects were validated analytically and by questionnaires to scores of business customers.",Differentially switched vpn bypass; Seamless communication,Calculations; Developing countries; Surveys; Virtual private networks; Business communications; Differential switch; Differentially switched vpn bypass; International business; International business communication; Offshore applications; Seamless communication; Virtual private networks (VPN); Differentiation (calculus)
Hadoop-based intelligent care system (HICS): Analytical approach for big data in IoT,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040083277&doi=10.1145%2f3108936&partnerID=40&md5=e00b579c95fde2f1bfebef710cbecb6f,"The Internet of Things (IoT) is increasingly becoming a worldwide network of interconnected things that are uniquely addressable, via standard communication protocols. The use of IoT for continuous monitoring of public health is being rapidly adopted by various countries while generating a massive volume of heterogeneous, multisource, dynamic, and sparse high-velocity data. Handling such an enormous amount of high-speed medical data while integrating, collecting, processing, analyzing, and extracting knowledge constitutes a challenging task. On the other hand, most of the existing IoT devices do not cooperate with one another by using the same medium of communication. For this reason, it is a challenging task to develop healthcare applications for IoT that fulfill all user needs through real-Time monitoring of health parameters. Therefore, to address such issues, this article proposed a Hadoop-based intelligent care system (HICS) that demonstrates IoT-based collaborative contextual Big Data sharing among all of the devices in a healthcare system. In particular, the proposed system involves a network architecture with enhanced processing features for data collection generated by millions of connected devices. In the proposed system, various sensors, such as wearable devices, are attached to the human body and measure health parameters and transmit them to a primary mobile device (PMD). The collected data are then forwarded to intelligent building (IB) using the Internet where the data are thoroughly analyzed to identify abnormal and serious health conditions. Intelligent building consists of (1) a Big Data collection unit (used for data collection, filtration, and load balancing); (2) a Hadoop processing unit (HPU) (composed of Hadoop distributed file system (HDFS) and MapReduce); and (3) an analysis and decision unit. The HPU, analysis, and decision unit are equipped with a medical expert system, which reads the sensor data and performs actions in the case of an emergency situation. To demonstrate the feasibility and efficiency of the proposed system, we use publicly available medical sensory datasets and real-Time sensor traffic while identifying the serious health conditions of patients by using thresholds, statistical methods, and machine-learning techniques. The results show that the proposed system is very efficient and able to process high-speed WBAN sensory data in real time. © 2017 ACM.",Big data; Healthcare; Intelligent buil; Iot; PMD,Data acquisition; Data handling; Expert systems; File organization; Health; Health care; Intelligent buildings; Internet of things; Learning systems; Mobile devices; Network architecture; Patient monitoring; Polarization mode dispersion; Wearable sensors; Continuous monitoring; Hadoop distributed file system (HDFS); Health care application; Intelligent buil; Internet of thing (IOT); Machine learning techniques; Medical expert system; Real time monitoring; Big data
Real-Time traffic event detection from social media,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040055534&doi=10.1145%2f3122982&partnerID=40&md5=cbf7c14416c33cd5174582a44c1a3332,"Smart communities are composed of groups, organizations, and individuals who share information and make use of that shared information for better decision making. Shared information can come from many sources, particularly, but not exclusively, from sensors and social media. Social media has become an important source of near-instantaneous user-generated information that can be shared and analyzed to support better decision making. One domain where social media data can add value is transportation and traffic management. This article looks at the exploitation of Twitter data in the traffic reporting domain. A key challenge is how to identify relevant information from a huge amount of user-generated data and then analyze the relevant data for automatic geocoded incident detection. The article proposes an instant traffic alert and warning system based on a novel latent Dirichlet allocation (LDA) approach (""tweet-LDA""). The system is evaluated and shown to perform better than related approaches.",Incremental learning; Latent dirichlet allocation (lda); Text mining; Traffic alert system; Tweet mining,Data mining; Decision making; Statistics; Alert systems; Incremental learning; Latent dirichlet allocations; Text mining; Tweet minings; Social networking (online)
Crowd-sourced data collection for urban monitoring via mobile sensors,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033239962&doi=10.1145%2f3093895&partnerID=40&md5=e198eb97736f9bdb3eb72ed1c6853c3a,"A considerable amount of research has addressed Internet of Things and connected communities. It is possible to exploit the sensing capabilities of connected communities, by leveraging the continuously growing use of cloud computing solutions and mobile devices. The pervasiveness of mobile sensors also enables the Mobile Crowd Sensing (MCS) paradigm, which aims at using mobile-embedded sensors to extend monitoring of multiple (environmental) phenomena in expansive urban areas. In this article, we discuss our approach with a cloud-based platform to pave the way for applying crowd sensing in urban scenarios.We have implemented a complete solution for environmental monitoring of several pollutants, like noise, air, electromagnetic fields, and so on in an urban area based on this paradigm. Through extensive experimentation, specifically on noise pollution, we show how the proposed infrastructure exhibits the ability to collect data from connected communities, and enables a seamless support of services needed for improving citizens' quality of life and eventually helps city decision makers in urban planning. © 2017 ACM.",Mobile crowed sensing; Smart cities; Social sensing,Decision making; Electromagnetic fields; Noise pollution; Pollution; Smart city; Urban planning; Cloud based platforms; Complete solutions; Decision makers; Embedded sensors; Environmental Monitoring; Mobile crowed sensing; Social sensing; Urban monitoring; Data acquisition
Behind the myths of citizen participation: Identifying sustainability factors of hyper-local information systems,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040097486&doi=10.1145%2f3093892&partnerID=40&md5=b9354519c24ff038df9e9f307a93bb56,"Various information systems have emerged to facilitate citizen participation in the life of their communities. However, there is a lack of robust understanding of what enables the sustainability of such systems. This work introduces a framework to identify and analyze various factors that influence the sustainability of ""hyperlocal"" information systems. Using longitudinal observations of participation from 35 online neighborhood discussion forums over six years, we analyze the relationship between sustainability and online-offline community characteristics. Our results not only show patterns consistent with previous observations but reveal the dubious influences of member heterogeneity and network structure. Design insights are discussed. © 2017 ACM.",Collective factors; Content; Local online forums; Membership; Sustainability; User attraction; User retention,Information systems; Collective factors; Content; Membership; Online forums; User attraction; User retention; Sustainable development
A multi-dimensional smart community discovery scheme for IoT-enriched smart homes,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033237502&doi=10.1145%2f3062178&partnerID=40&md5=3bb093aea09d3020e1fe4afda7f0d81c,"The proliferation of the Internet into every household has provided more opportunities for residents to become closer to each other than before. However, solid structural barrier is raised and social relationships within such neighborhoods are weak compared to those in traditional towns. Accordingly, activating communities and ultimately enhancing a sense of community through constructive participation and communal sharing of labor among residents has currently emerged as a challenging issue in a contemporary housing complex. In an effort to activate those communities, a notion of smart community is presented in which multiple smart homes are equipped with Internet of Things and interconnected with each other. Beyond the unadorned smart community composed by physical proximity, it is essential to discover a human-centric community that achieves communal benefits and enables residents to maximize individual economic gain by leveraging collective intelligence. In this article, we present a multi-dimensional smart community discovery scheme that enables householders to find human-centric community considering multi-dimensional factors in terms of physical, social, and economical aspects. We conduct experiments with 30 real households by applying a community-based energy saving scenario. Experiment results show that the proposed scheme performs better when compared to the physical proximity-based one in energy consumption and user satisfaction. © 2017 ACM.",Human-centric community formation; Internet of things; Smart community,Automation; Energy conservation; Energy utilization; Intelligent buildings; Collective intelligences; Economical aspects; Human-centric; Multi dimensional; Physical proximity; Sense of community; Smart community; Social relationships; Internet of things
Adaptive message routing and replication in mobile opportunistic networks for connected communities,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033239136&doi=10.1145%2f3122984&partnerID=40&md5=5a4b71997d69b89763361a08ffba9466,"Mobile opportunistic networking is a promising technology that can supplement existing cellular and WiFi networks to provide desirable services for smart and connected communities. Message routing is the most compelling challenge in mobile opportunistic networks due to the lack of contemporaneous end-To-end paths and the resource constraints at mobile devices. To improve the probability of successful message delivery, most existing routing schemes use the past contact history to predict future contacts for message forwarding, and exploit message replication and redundancy for multicopy routing. However, most existing predictionbased routing schemes simply use the average pairwise contact probability as the routing metric and neglect the benefits of exploring fine-grained contact information such as pairwise repeated contact patterns to improve the accuracy of predicting future contacts. Moreover, there is no efficient mechanism that can adaptively control message replication in a decentralized manner to achieve both high probability of successful message delivery and low message overhead. To address these problems, we present FGAR, a routing protocol designed for mobile opportunistic networks by leveraging fine-grained contact characterization and adaptive message replication. In FGAR, contact history is characterized in a fine-grained manner with timing information using a sliding window mechanism, and future contacts are predicted based on the fine-grained contact information, thereby improving the accuracy of contact prediction. We further design an efficient message replication scheme in which message replication is controlled in a fully decentralized manner by taking into account the expected message delivery probability, the replication history, and the quality of the encountered device. A replica is generated only when it is necessary to fulfill the expected message delivery probability. We evaluate our scheme through trace-driven simulations, and the simulation results show that FGAR outperforms existing schemes. In comparison with PRoPHET, FGAR can achieve more than 20% improvement on average on successful message delivery, whereas the message overhead has been reduced by a factor up to 15. © 2017 ACM.",Connected communities; Fine-grained contact characterization; Message replication; Repeated contact pattern; Routing,Forecasting; Mobile telecommunication systems; Probability; Quality control; Routing protocols; Wi-Fi; Contact pattern; Fine grained; Message replication; Mobile opportunistic networks; Opportunistic networking; Prediction-based routing; Routing; Sliding window mechanism; Network routing
On the need of trustworthy sensing and crowdsourcing for urban accessibility in smart city,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033219936&doi=10.1145%2f3133327&partnerID=40&md5=cd4c58211fef40e646753e2124bdd65a,"Mobility in urban environments is an undisputed key factor that can affect citizens' well-being and quality of life. This is particularly relevant for those people with disabilities or with reduced mobility who have to face the presence of barriers in urban areas. In this scenario, the availability of information about such architectural elements (together with facilities) can greatly support citizens' mobility by enhancing their independence and their abilities in conducting daily outdoor activities. With this in mind, we have designed and developed mobile Pervasive Accessibility Social Sensing (mPASS), a system that provides users with personalized paths, computed on the basis of their own preferences and needs, with a customizable and accessible interface. The system collects data from crowdsourcing and crowdsensing to map urban and architectural accessibility by providing reliable information coming from different data sources with different levels of trustworthiness. In this context, reliability can be ensured by properly managing crowdsourced and crowdsensed data, combined when possible with authoritative datasets, provided by disability rights organizations and local authorities. To demonstrate this claim, in this article we present our trustworthiness model and discuss results we have obtained by simulations. © 2017 ACM.",Credibility; Crowdsensing; Crowdsourcing; Trustworthiness; Urban Accessibility,Crowdsourcing; Smart city; Architectural element; Credibility; Crowdsensing; Outdoor activities; People with disabilities; Trustworthiness; Urban accessibilities; Urban environments; Transportation
ODIN: Obfuscation-based privacy-preserving consensus algorithm for decentralized information fusion in smart device networks,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033221864&doi=10.1145%2f3137573&partnerID=40&md5=bd64ad57ec40e2344677d4e19dad7166,"The large spread of sensors and smart devices in urban infrastructures are motivating research in the area of the Internet of Things (IoT) to develop new services and improve citizens' quality of life. Sensors and smart devices generate large amounts of measurement data from sensing the environment, which is used to enable services such as control of power consumption or traffic density. To deal with such a large amount of information and provide accurate measurements, service providers can adopt information fusion, which given the decentralized nature of urban deployments can be performed by means of consensus algorithms. These algorithms allow distributed agents to (iteratively) compute linear functions on the exchanged data, and take decisions based on the outcome, without the need for the support of a central entity. However, the use of consensus algorithms raises several security concerns, especially when private or security critical information is involved in the computation. In this article we propose ODIN, a novel algorithm allowing information fusion over encrypted data. ODIN is a privacy-preserving extension of the popular consensus gossip algorithm, which prevents distributed agents from having direct access to the data while they iteratively reach consensus; agents cannot access even the final consensus value but can only retrieve partial information (e.g., a binary decision). ODIN uses efficient additive obfuscation and proxy re-encryption during the update steps and garbled circuits to make final decisions on the obfuscated consensus.We discuss the security of our proposal and show its practicability and efficiency on real-world resource-constrained devices, developing a prototype implementation for Raspberry Pi devices. © 2017 ACM.",Consensus algorithms; Information fusion; Internet of Things; Privacypreserving applications; Proxy re-encryption; Secure multiparty computation,Cryptography; Data privacy; Information fusion; Internet of things; Software agents; Consensus algorithms; Decentralized informations; Internet of thing (IOT); Privacy preserving; Prototype implementations; Proxy re encryptions; Resourceconstrained devices; Secure multi-party computation; Iterative methods
Quantify resilience enhancement of UTS through exploiting connected community and internet of everything emerging technologies,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033232664&doi=10.1145%2f3137572&partnerID=40&md5=f2ae6d3d9b4bcea26863c1184d451505,"This work aims at investigating and quantifying the Urban Transport System (UTS) resilience enhancement enabled by the adoption of emerging technology such as Internet of Everything (IoE) and the new trend of the Connected Community (CC). A conceptual extension of Functional Resonance Analysis Method (FRAM) and its formalization have been proposed and used to model UTS complexity. The scope is to identify the system functions and their interdependencies with a particular focus on those that have a relation and impact on people and communities. Network analysis techniques have been applied to the FRAM model to identify and estimate the most critical community-related functions. The notion of Variability Rate (VR) has been defined as the amount of output variability generated by an upstream function that can be tolerated/absorbed by a downstream function, without significantly increasing of its subsequent output variability. A fuzzybased quantification of the VR based on expert judgment has been developed when quantitative data are not available. Our approach has been applied to a critical scenario as flash flooding considering two cases: when UTS has CC and IoE implemented or not. However, the method can be applied in different scenarios and critical infrastructures. The results show a remarkable VR enhancement if CC and IoE are deployed. © 2017 ACM.",Functional Resonance Analysis Method; Fuzzy Logic; Resilience,Transportation; Urban transportation; Analysis techniques; Emerging technologies; Functional resonance; Functional resonance analysis methods (FRAM); Output variability; Related functions; Resilience; Urban transport systems; Fuzzy logic
Extending the outreach: From smart cities to connected communities,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033219356&doi=10.1145%2f3140543&partnerID=40&md5=4bdb9692847ea5c78bfb6fa27f7ac846,"Connected Communities (CCs) are socio-Technical systems that rely on an information and communication technology (ICT) infrastructure to integrate people and organizations (companies, schools, hospitals, universities, local and national government agencies) willing to share information and perform joint decisionmaking to create sustainable and equitable work and living environments. We discuss a research agenda considering CCs from three distinct but complementary points of view: CC metaphors, models, and services. © 2017 ACM.",Connected communities; Rural-urban divide; Service models,Smart city; Societies and institutions; Information and Communication Technologies; Living environment; National governments; Research agenda; Service Model; Sociotechnical systems; Cesium compounds
Exploiting contextual information in attacking set-generalized transactions,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029782120&doi=10.1145%2f3106165&partnerID=40&md5=22642b236cec226964e37e1001101dcc,"Transactions are records that contain a set of items about individuals. For example, items browsed by a customer when shopping online form a transaction. Today, many activities are carried out on the Internet, resulting in a large amount of transaction data being collected. Such data are often shared and analyzed to improve business and services, but they also contain private information about individuals that must be protected. Techniques have been proposed to sanitize transaction data before their release, and set-based generalization is one such method. In this article, we study how well set-based generalization can protect transactions. We propose methods to attack set-generalized transactions by exploiting contextual information that is available within the released data. Our results show that set-based generalization may not provide adequate protection for transactions, and up to 70% of the items added into the transactions during generalization to obfuscate original data can be detected by our methods with a precision over 80%. © 2017 ACM.",De-anonymization; Privacy; Semantic relationship; Transaction data,Data privacy; Internet; Anonymization; Contextual information; Large amounts; Private information; Semantic relationships; Transaction data; Well-set; Semantics
Taming the costs of trustworthy provenance through policy reduction,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029504490&doi=10.1145%2f3062180&partnerID=40&md5=735e9ac31907788a6d22a3a6d9a30cec,"Provenance is an increasingly important tool for understanding and even actively preventing system intrusion, but the excessive storage burden imposed by automatic provenance collection threatens to undermine its value in practice. This situation is made worse by the fact that the majority of this metadata is unlikely to be of interest to an administrator, instead describing system noise or other background activities that are not germane to the forensic investigation. To date, storing data provenance in perpetuity was a necessary concession in even the most advanced provenance tracking systems in order to ensure the completeness of the provenance record for future analyses. In this work, we overcome this obstacle by proposing a policybased approach to provenance filtering, leveraging the confinement properties provided by Mandatory Access Control (MAC) systems in order to identify and isolate subdomains of system activity for which to collect provenance. We introduce the notion of minimal completeness for provenance graphs, and design and implement a system that provides this property by exclusively collecting provenance for the trusted computing base of a target application. In evaluation, we discover that, while the efficacy of our approach is domain dependent, storage costs can be reduced by as much as 89% in critical scenarios such as provenance tracking in cloud computing data centers. To the best of our knowledge, this is the first policy-based provenance monitor to appear in the literature. © 2017 ACM.",Integrity; Mandatory policy; Provenance; TCB,Access control; Digital storage; Confinement properties; Design and implements; Forensic investigation; Integrity; Mandatory access control; Policy-based approaches; Provenance; Trusted computing base; Cost reduction
Facilitating adoption of internet technologies and services with externalities via cost subsidization,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028615283&doi=10.1145%2f3091109&partnerID=40&md5=c83f2dc2d75271b7ab3808eaf397b658,"This article models the temporal adoption dynamics of an abstracted Internet technology or service, where the instantaneous net value of the service perceived by each (current or potential) user/customer incorporates three key features: (i) user service affinity heterogeneity, (ii) a network externality, and (iii) a subscription cost. Internet technologies and services with network externalities face a ""chicken-and-egg"" adoption problem in that the service requires an established customer base to attract new customers. In this article, we study cost subsidization as a means to ""reach the knee,"" at which point the externality drives rapid service adoption, and thereby change the equilibrium service fractional adoption level from an initial near-zero level to a final near-one level (full adoption). We present three simple subsidy models and evaluate them under two natural performance metrics: (i) the duration required for the subsidized service to reach a given target adoption level and (ii) the aggregate cost of the subsidy born by the service provide. First, we present a ""two-target adoption subsidy"" that subsidizes the cost to keep the fraction of users with positive net utility at a (constant) target level until the actual adoption target is reached. Second, we study a special case of the above where the target ensures all users have positive net utility, corresponding to a ""quickest adoption"" subsidy (QAS). Third, we introduce an approximation of QAS that only requires the service provider adjust the subsidy level a prescribed number of times. Fourth, we study equilibria and their stability under uniformly and normally distributed user service affinities, highlighting the unstable equilibrium in each case as the natural target adoption level for the provider. Finally, we provide a fictional case study to illustrate the application of the results in a (hopefully) realistic scenario, along with a brief discussion of the limitations of the model and analysis.",diffusion adoption; Network externality; social networks; subsidization,Costs; Internet service providers; Normal distribution; Social networking (online); Internet technology; Model and analysis; Network externality; Performance metrics; Realistic scenario; Service provider; subsidization; Unstable equilibriums; Web services
"A canonical form for PROV documents and its application to equality, signature, and validation",2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028614126&doi=10.1145%2f3032990&partnerID=40&md5=38f9bdee8baa61f4c0ae851275784e6b,"We present a canonical form for prov that is a normalized way of representing prov documents as mathematical expressions. As opposed to the normal form specified by the prov-constraints recommendation, the canonical form we present is defined for all prov documents, irrespective of their validity, and it can be serialized in a unique way. The article makes the case for a canonical form for prov and its potential uses, namely comparison of prov documents in different formats, validation, and signature of prov documents. A signature of a prov document allows the integrity and the author of provenance to be ascertained; since the signature is based on the canonical form, these checks are not tied to a particular encoding, but can be performed on any representation of prov.",canonical form; digital signature; equivalence; Provenance,Electronic document identification systems; Canonical form; equivalence; ITS applications; Mathematical expressions; Normal form; Provenance; Internet
Bandwidth measurements within the cloud: Characterizing regular behaviors and correlating downtimes,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028603196&doi=10.1145%2f3093893&partnerID=40&md5=01c189ad1a5e753a05c14f992577b61b,"The search for availability, reliability, and quality of service has led cloud infrastructure customers to disseminate their services, contents, and data over multiple cloud data centers, often involving several Cloud service providers (CSPs). The consequence of this is that a large amount of data must be transmitted across the public Cloud. However, little is known about the bandwidth dynamics involved. To address this, we have conducted a measurement campaign for bandwidth between 18 data centers of four major CSPs. This extensive campaign allowed us to characterize the resulting time series of bandwidth as the addition of a stationary component and some infrequent excursions (typically downtimes). While the former provides a description of the bandwidth users can expect in the Cloud, the latter is closely related to the robustness of the Cloud (i.e., the occurrence of downtimes is correlated). Both components have been studied further by applying factor analysis, specifically analysis of variance, as a mechanism to formally compare data centers' behaviors and extract generalities. The results show that the stationary process is closely related to the data center locations and CSPs involved in transfers that, fortunately, make the Cloud more predictable and allow the set of reported measurements to be extrapolated. On the other hand, although correlation in the Cloud is low, that is, only 10% of the measured pair of paths showed some correlation, we found evidence that such correlation depends on the particular relationships between pairs of data centers with little connection to more general factors. Positively, this implies that data centers either in the same area or within the same CSP do not show qualitatively more correlation than other data centers, which eases the deployment of robust infrastructures. On the downside, this metric is scarcely generalizable and, consequently, calls for exhaustive monitoring.",ANOVA; inter-cloud; Public cloud; TCP bandwidth; traffic correlation,Analysis of variance (ANOVA); Maintenance; Quality of service; Bandwidth measurements; Cloud infrastructures; Cloud service providers; Data center locations; Inter clouds; Measurement campaign; Public clouds; Stationary components; Bandwidth
Guest editorial: The provenance of online data,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028611027&doi=10.1145%2f3108938&partnerID=40&md5=e3475a0805203492d3b759058e563dac,[No abstract available],,
Managing provenance of implicit data flows in scientific experiments,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028624188&doi=10.1145%2f3053372&partnerID=40&md5=44d1faf1d7cb95da32313c475e6f2b01,"Scientific experiments modeled as scientific workflows may create, change, or access data products not explicitly referenced in the workflow specification, leading to implicit data flows. The lack of knowledge about implicit data flows makes the experiments hard to understand and reproduce. In this article, we present Prov- Monitor, an approach that identifies the creation, change, or access to data products even within implicit data flows. ProvMonitor links this information with the workflow activity that generated it, allowing for scientists to compare data products within and throughout trials of the same workflow, identifying side effects on data evolution caused by implicit data flows.We evaluated ProvMonitor and observed that it could answer queries for scenarios that demand specific knowledge related to implicit provenance.",Implicit data flows; implicit provenance; scientific experiments; workflows,Query processing; Data evolution; Data flow; implicit provenance; Scientific experiments; Scientific workflows; Specific knowledge; Work-flows; Workflow specification; Data transfer
PROV2R: Practical provenance analysis of unstructured processes,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028584109&doi=10.1145%2f3062176&partnerID=40&md5=0fc1cf29fb95229a2fd7a278f2acb2f3,"Information produced by Internet applications is inherently a result of processes that are executed locally. Think of a web server that makes use of a CGI script, or a content management system where a post was first edited using a word processor. Given the impact of these processes to the content published online, a consumer of that information may want to understand what those impacts were. For example, understanding from where text was copied and pasted to make a post, or if the CGI script was updated with the latest security patches, may all influence the confidence on the published content. Capturing and exposing this information provenance is thus important to ascertaining trust to online content. Furthermore, providers of internet applications may wish to have access to the same information for debugging or audit purposes. For processes following a rigid structure (such as databases or workflows), disclosed provenance systems have been developed that efficiently and accurately capture the provenance of the produced data. However, accurately capturing provenance from unstructured processes, for example, user-interactive computing used to produce web content, remains a problem to be tackled. In this article,we address the problem of capturing and exposing provenance from unstructured processes. Our approach, called PROV2R (PROVenance Record and Replay) is composed of two parts: (a) the decoupling of provenance analysis from its capture; and (b) the capture of high-fidelity provenance from unmodified programs. We use techniques originating in the security and reverse engineering communities, namely, record and replay and taint tracking. Taint tracking fundamentally addresses the data provenance problem but is impractical to apply at runtime due to extremely high overhead. With a number of case studies, we demonstrate that PROV2R enables the use of taint analysis for high-fidelity provenance capture, while keeping the runtime overhead at manageable levels. In addition, we show how captured information can be represented using the W3C PROV provenance model for exposure on the Web.",Data provenance; introspection; PANDA; taint analysis; W3C PROV,Reverse engineering; Data provenance; introspection; PANDA; taint analysis; W3C PROV; Program debugging
Using Argumentation to improve classification in natural language problems,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025096478&doi=10.1145%2f3017679&partnerID=40&md5=04a3106cce1886af766949ddfb736762,"Argumentation has proven successful in a number of domains, including Multi-Agent Systems and decision support in medicine and engineering. We propose its application to a domain yet largely unexplored by argumentation research: computational linguistics. We have developed a novel classification methodology that incorporates reasoning through argumentation with supervised learning. We train classifiers and then argue about the validity of their output. To do so, we identify arguments that formalise prototypical knowledge of a problem and use them to correct misclassifications. We illustrate our methodology on two tasks. On the one hand, we address cross-domain sentiment polarity classification, where we train classifiers on one corpus, for example, Tweets, to identify positive/negative polarity and classify instances from another corpus, for example, sentences from movie reviews. On the other hand, we address a form of argumentation mining that we call Relation-based Argumentation Mining, where we classify pairs of sentences based on whether the first sentence attacks or supports the second or whether it does neither. Whenever we find that one sentence attacks/supports the other, we consider both to be argumentative, irrespective of their stand-alone argumentativeness. For both tasks, we improve classification performance when using our methodology, compared to using standard classifiers only. © 2017 ACM.",Abstract argumentation; Argumentation; Argumentation mining; Computational linguistics; Quantitative argumentation; Sentiment analysis,Computational linguistics; Linguistics; Multi agent systems; Natural language processing systems; Abstract argumentation; Argumentation; Classification methodologies; Classification performance; Decision support in medicine; Polarity classification; Quantitative argumentation; Sentiment analysis; Classification (of information)
Using argumentative structure to interpret debates in online deliberative democracy and erulemaking,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024482229&doi=10.1145%2f3032989&partnerID=40&md5=bd17ddfcb039cb3e88dc0f6805cb42b6,"Governments around the world are increasingly utilising online platforms and social media to engage with, and ascertain the opinions of, their citizens. Whilst policy makers could potentially benefit from such enormous feedback from society, they first face the challenge of making sense out of the large volumes of data produced. In this article, we show how the analysis of argumentative and dialogical structures allows for the principled identification of those issues that are central, controversial, or popular in an online corpus of debates. Although areas such as controversy mining work towards identifying issues that are a source of disagreement, by looking at the deeper argumentative structure, we show that a much richer understanding can be obtained. We provide results from using a pipeline of argument-mining techniques on the debate corpus, showing that the accuracy obtained is sufficient to automatically identify those issues that are key to the discussion, attracting proportionately more support than others, and those that are divisive, attracting proportionately more conflicting viewpoints. 2017 Copyright is held by the owner/author(s).",Analytics; Argument; Argumentation; Corpus; Dialogue; Engagement; Sensemaking,Analytics; Argument; Argumentation; Corpus; Dialogue; Engagement; Sensemaking; Internet
Experimental assessment of aggregation principles in argumentation-enabled collective intelligence,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041115011&doi=10.1145%2f3053371&partnerID=40&md5=1bf3cf4b71cd07ec1159910fcb32f843,"On the Web, there is always a need to aggregate opinions from the crowd (as in posts, social networks, forums, etc.). Different mechanisms have been implemented to capture these opinions such as Like in Facebook, Favorite in Twitter, thumbs-up/-down, flagging, and so on. However, in more contested domains (e.g., Wikipedia, political discussion, and climate change discussion), these mechanisms are not sufficient, since they only deal with each issue independently without considering the relationships between different claims. We can view a set of conflicting arguments as a graph in which the nodes represent arguments and the arcs between these nodes represent the defeat relation. A group of people can then collectively evaluate such graphs. To do this, the group must use a rule to aggregate their individual opinions about the entire argument graph. Here we present the first experimental evaluation of different principles commonly employed by aggregation rules presented in the literature. We use randomized controlled experiments to investigate which principles people consider better at aggregating opinions under different conditions. Our analysis reveals a number of factors, not captured by traditional formal models, that play an important role in determining the efficacy of aggregation. These results help bring formal models of argumentation closer to real-world application. © 2016 ACM.",Argumentation; Experiment; Voting,Aggregates; Experiments; Social networking (online); Argumentation; Collective intelligences; Controlled experiment; Different mechanisms; Experimental assessment; Experimental evaluation; Number of factors; Voting; Climate change
Towards inferring communication patterns in online social networks,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024499000&doi=10.1145%2f3093897&partnerID=40&md5=46aa848d4841f4805f1df7b0ed51921d,"The separation between the public and private spheres on online social networks is known to be, at best, blurred. On the one hand, previous studies have shown how it is possible to infer private attributes from publicly available data. On the other hand, no distinction exists between public and private data when we consider the ability of the online social network (OSN) provider to access them. Even when OSN users go to great lengths to protect their privacy, such as by using encryption or communication obfuscation, correlations between data may render these solutions useless. In this article, we study the relationship between private communication patterns and publicly available OSN data. Such a relationship informs both privacy-invasive inferences as well as OSN communication modelling, the latter being key toward developing effective obfuscation tools. We propose an inference model based on Bayesian analysis and evaluate, using a real social network dataset, how archetypal social graph features can lead to inferences about private communication. Our results indicate that both friendship graph and public traffic data may not be informative enough to enable these inferences, with time analysis having a non-negligible impact on their precision. © 2017 ACM.",Communication; Inference; Online social networks; Privacy,Communication; Cryptography; Data privacy; Online systems; Websites; Bayesian Analysis; Communication pattern; Friendship graphs; Inference; Inference models; On-line social networks; Online social networks (OSN); Private communication; Social networking (online)
Stance and sentiment in Tweets,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048042536&doi=10.1145%2f3003433&partnerID=40&md5=47f66f0d56db96a5df2f5b7faf2beb3e,"We can often detect from a person's utterances whether he or she is in favor of or against a given target entity-one's stance toward the target. However, a person may express the same stance toward a target by using negative or positive language. Here for the first time we present a dataset of tweet-target pairs annotated for both stance and sentiment. The targets may or may not be referred to in the tweets, and they may or may not be the target of opinion in the tweets. Partitions of this dataset were used as training and test sets in a SemEval-2016 shared task competition. We propose a simple stance detection system that outperforms submissions from all 19 teams that participated in the shared task. Additionally, access to both stance and sentiment annotations allows us to explore several research questions. We show that although knowing the sentiment expressed by a tweet is beneficial for stance classification, it alone is not sufficient. Finally, we use additional unlabeled data through distant supervision techniques and word embeddings to further improve stance classification. © 2016 ACM.",Opinion; Polarity; Sentiment; Stance; Text classification; Tweets,Classification (of information); Statistical tests; Opinion; Polarity; Sentiment; Stance; Text classification; Tweets; Text processing
A universal model for discourse-level argumentation analysis,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058037852&doi=10.1145%2f2957757&partnerID=40&md5=19af18bd5c3e8d987e92a68b44438a5b,"The argumentative structure of texts is increasingly exploited for analysis tasks, for example, for stance classification or the assessment of argumentation quality. Most existing approaches, however, model only the local structure of single arguments. This article considers the question of how to capture the global discourse-level structure of a text for argumentation-related analyses. In particular, we propose to model the global structure as a flow of ""task-related rhetorical moves,"" such as discourse functions or aspectbased sentiment. By comparing the flow of a text to a set of common flow patterns, we map the text into the feature space of global structures, thus capturing its discourse-level argumentation. We show how to identify different types of flow patterns, and we provide evidence that they generalize well across different domains of texts. In our evaluation for two analysis tasks, the classification of review sentiment and the scoring of essay organization, the features derived from flow patterns prove both effective and more robust than strong baselines. We conclude with a discussion of the universality of modeling flow for discourse-level argumentation analysis. © 2016 ACM.",Argumentation; Discourse-level structure; Sentiment flow,Flow patterns; Argumentation; Different domains; Global structure; Level structure; Local structure; Sentiment flow; Structure of text; Universal model; Quality control
Mitigating data sparsity using similarity reinforcement-enhanced collaborative filtering,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027054015&doi=10.1145%2f3062179&partnerID=40&md5=c24c9d27fc7de39aeb9f4be668f5abcf,"The data sparsity problem has attracted significant attention in collaborative filtering-based recommender systems. To alleviate data sparsity, several previous efforts employed hybrid approaches that incorporate auxiliary data sources into recommendation techniques, like content, context, or social relationships. However, due to privacy and security concerns, it is generally difficult to collect such auxiliary information. In this article, we focus on the pure collaborative filtering methods without relying on any auxiliary data source. We propose an improved memory-based collaborative filtering approach enhanced by a novel similarity reinforcement mechanism. It can discover potential similarity relationships between users or items by making better use of known but limited user-item interactions, thus to extract plentiful historical rating information from similar neighbors to make more reliable and accurate rating predictions. This approach integrates user similarity reinforcement and item similarity reinforcement into a comprehensive framework and lets them enhance each other. Comprehensive experiments conducted on several public datasets demonstrate that, in the face of data sparsity, our approach achieves a significant improvement in prediction accuracy when compared with the state-of-the-art memory-based and model-based collaborative filtering algorithms. © 2017 ACM.",Data sparsity; Personalization; Rating prediction; Recommender system; Similarity reinforcement,Forecasting; Rating; Recommender systems; Reinforcement; Salinity measurement; Auxiliary information; Collaborative filtering algorithms; Collaborative filtering methods; Data sparsity; Data sparsity problems; Personalizations; Recommendation techniques; Reinforcement mechanisms; Collaborative filtering
An argumentation approach for resolving privacy disputes in online social networks,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022343263&doi=10.1145%2f3003434&partnerID=40&md5=0a7aec1fed8b6cf218becf118bbaf74f,"Preserving users' privacy is important for Web systems. In systems where transactions are managed by a single user, such as e-commerce systems, preserving privacy of the transactions is merely the capability of access control. However, in online social networks, where each transaction is managed by and has effect on others, preserving privacy is difficult. Inmany cases, the users' privacy constraints are distributed, expressed in a high-level manner, and would depend on information that only becomes available over interactions with others. Hence, when a content is being shared by a user, others who might be affected by the content should discuss and agree on how the content will be shared online so that none of their privacy constraints are violated. To enable this, we model users of the social networks as agents that represent their users' privacy constraints as semantic rules. Agents argue with each other on propositions that enable their privacy rules by generating facts and assumptions from their ontology. Moreover, agents can seek help from others by requesting new information to enrich their ontology. Using assumption-based argumentation, agents decide whether a content should be shared or not. We evaluate the applicability of our approach on real-life privacy scenarios in comparison with user surveys. © 2017 ACM.",Agreement; Argumentation; Negotiation; Privacy,Access control; Contracts; Data privacy; Ontology; Semantics; Argumentation; E-commerce systems; Negotiation; On-line social networks; Privacy constraints; Privacy rule; Semantic rules; Single users; Social networking (online)
Argumentation in social media,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023197018&doi=10.1145%2f3056539&partnerID=40&md5=a5893d217e0858e58338348dceaf0f44,[No abstract available],,
Introduction to the special issue on advances in social computing,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020476440&doi=10.1145%2f3080258&partnerID=40&md5=ae444efb49fb6c17d3793b586c64e628,[No abstract available],,
Statistical learning of domain-specific quality-of-service features from user reviews,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019620816&doi=10.1145%2f3053381&partnerID=40&md5=7f3d78faf408e1396654fb392d333f00,"With the fast increase of online services of all kinds, users start to care more about the Quality of Service (QoS) that a service provider can offer besides the functionalities of the services. As a result, QoS-based service selection and recommendation have received significant attention since the mid-2000s. However, existing approaches primarily consider a small number of standard QoS parameters, most of which relate to the response time, fee, availability of services, and so on. As online services start to diversify significantly over different domains, these small set of QoS parameters will not be able to capture the different quality aspects that users truly care about over different domains. Most existing approaches for QoS data collection depend on the information from service providers, which are sensitive to the trustworthiness of the providers. Some service monitoring mechanisms collect QoS data through actual service invocations but may be affected by actual hardware/software configurations. In either case, domain-specific QoS data that capture what users truly care about have not been successfully collected or analyzed by existing works in service computing. To address this demanding issue, we develop a statistical learning approach to extract domain-specific QoS features from user-provided service reviews. In particular, we aim to classify user reviews based on their sentiment orientations into either a positive or negative category. Meanwhile, statistical feature selection is performed to identify statistically nontrivial terms from review text, which can serve as candidate QoS features. We also develop a topic models-based approach that automatically groups relevant terms and returns the term groups to users, where each term group corresponds to one high-level quality aspect of services. We have conducted extensive experiments on three real-world datasets to demonstrates the effectiveness of our approach. © 2017 ACM.",Quality of service; Statistical learning,Data acquisition; Human computer interaction; Telecommunication services; Different domains; Hardware/software; Real-world datasets; Service invocation; Service monitoring; Service selection; Statistical features; Statistical learning; Quality of service
"Debating technology for dialogical argument: Sensemaking, engagement, and analytics",2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053320463&doi=10.1145%2f3007210&partnerID=40&md5=f08e230456bb95b81c5176f21d017fb5,"Debating technologies, a newly emerging strand of research into computational technologies to support human debating, offer a powerful way of providing naturalistic, dialogue-based interaction with complex information spaces. The full potential of debating technologies for dialogical argument can, however, only be realized once key technical and engineering challenges are overcome, namely data structure, data availability, and interoperability between components. Our aim in this article is to show that the Argument Web, a vision for integrated, reusable, semantically rich resources connecting views, opinions, arguments, and debates online, offers a solution to these challenges. Through the use of a running example taken from the domain of citizen dialogue, we demonstrate for the first time that different Argument Web components focusing on sensemaking, engagement, and analytics can work in concert as a suite of debating technologies for rich, complex, dialogical argument. © 2017 Association for Computing Machinery. All rights reserved.",Analytics; Argument; Argumentation; Debating technology; Dialogue; Engagement; Sensemaking,Analytics; Argument; Argumentation; Dialogue; Engagement; Sensemaking; Internet
Anonymous or not? Understanding the factors affecting personal mobile data disclosure,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016463166&doi=10.1145%2f3017431&partnerID=40&md5=99c3d9a39d46c0e173a2b010c1393644,"The wide adoption of mobile devices and social media platforms have dramatically increased the collection and sharing of personal information. More and more frequently, users are called to make decisions concerning the disclosure of their personal information. In this study, we investigate the factors affecting users' choices toward the disclosure of their personal data, including not only their demographic and self-reported individual characteristics, but also their social interactions and their mobility patterns inferred frommonths of mobile phone data activity. We report the findings of a field study conducted with a community of 63 subjects provided with (i) a smart-phone and (ii) a Personal Data Store (PDS) enabling them to control the disclosure of their data. We monitor the sharing behavior of our participants through the PDS and evaluate the contribution of different factors affecting their disclosing choices of location and social interaction data. Our analysis shows that social interaction inferred by mobile phones is an important factor revealing willingness to share, regardless of the data type. In addition, we provide further insights on the individual traits relevant to the prediction of sharing behavior © 2017 ACM.",Human factors; Living labs; Mobile sensing; Personal mobile data; Privacy; Social computing,Cellular telephones; Data privacy; Human engineering; Mobile phones; Smartphones; Telephone sets; Individual characteristics; Living lab; Mobile data; Mobile sensing; Personal information; Social computing; Social media platforms; Willingness to share; Social sciences
Measurement theory-based trust management framework for online social communities,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016493774&doi=10.1145%2f3015771&partnerID=40&md5=14932e24b136064c04753106511c9add,"We propose a trust management framework based on measurement theory to infer indirect trust in online social communities using trust's transitivity property. Inspired by the similarities between human trust and measurement, we propose a new trust metric, composed of impression and confidence, which captures both trust level and its certainty. Furthermore, based on error propagation theory, we propose a method to compute indirect confidence according to different trust transitivity and aggregation operators. We perform experiments on two real data sets, Epinions.com and Twitter, to validate our framework. Also, we show that inferring indirect trust can connect more pairs of users. © 2017 ACM.",Measurement theory; Online social communities; Trust inference operators; Trust management,Mathematical operators; Social networking (online); Aggregation operator; Error propagation; Indirect trusts; Online social communities; Trust inferences; Trust management; Trust management frameworks; Trust transitivities; Measurement theory
Formation and reciprocation of dyadic trust,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016456631&doi=10.1145%2f2996184&partnerID=40&md5=8e29e9ad42a201152771841f9ffd4157,"This paper reports a detailed empirical study of interpersonal trust in a multi-relational online social network. This study addresses two main aspects of interpersonal trust: formation and reciprocation. Computational models developed, using multi-relational networks, for these processes provide interesting insights about online social interactions. Our findings for trust formation (initiation) indicate a strong role of lower familiarity interactions before trust(high familiarity relationship) is formed. Similarly, trust reciprocation is not automatic, but strongly depends on enough lower familiarity interactions. This study is the first quantification of the ""scaffolding role"" played by lower familiarity interactions, in formation of high familiarity relationships. Extension of prior works by Roy et al. [2013] and Singhal et al. [2013]. © 2017 ACM.",Online trust formation; Online virtual social networks; Relationship prediction; Social interaction,Scaffolds; Social sciences; Computational model; Empirical studies; Multi-relational networks; On-line social networks; Online trust; Social interactions; Virtual social networks; Social networking (online)
SPINEL: An opportunistic proxy for connecting sensors to the internet of things,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016514018&doi=10.1145%2f3041025&partnerID=40&md5=aafadac50077f1f3d5c66f856c78d871,"Nowadays, various static wireless sensor networks (WSN) are deployed in the environment for many purposes: traffic control, pollution monitoring, and so on. The willingness to open these legacy WSNs to the users is emerging, by integrating them to the Internet network as part of the future Internet of Things (IoT), for example, in the context of smart cities and open data policies. While legacy sensors cannot be directly connected to the Internet in general, emerging standards such as 6LoWPAN are aimed at solving this issue but require us to update or replace the existing devices. As a solution to connect legacy sensors to the IoT, we propose to take advantage of the multi-modal connectivity as well as the mobility of smartphones to use phones as opportunistic proxies, that is, mobile proxies that opportunistically discover closeby static sensors and act as intermediaries with the IoT, with the additional benefit of bringing fresh information about the environment to the smartphones' owners. However, this requires us to monitor the smartphone's mobility and further infer when to discover and register the sensors to guarantee the efficiency and reliability of opportunistic proxies. To that end, we introduce and evaluate an approach based on mobility analysis that uses a novel path prediction technique to predict when and where the user is not moving, and thereby serves to anticipate the registration of sensors within communication range. We show that this technique enables the deployment of low-cost resource-efficient mobile proxies to connect legacy WSNs with the IoT. © 2017 ACM.",Internet of things; Middleware; Opportunistic networking; Opportunistic proxy; Wireless sensor network,Middleware; Pollution control; Smart city; Smartphones; Wireless ad hoc networks; Wireless sensor networks; Communication range; Efficiency and reliability; Mobility analysis; Opportunistic networking; Opportunistic proxy; Path prediction; Pollution monitoring; Resource-efficient; Internet of things
Eliciting structured knowledge from situated crowd markets,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016496304&doi=10.1145%2f3007900&partnerID=40&md5=b9559d21bbf29d54af53dd106212605b,"We present a crowdsourcing methodology to elicit highly structured knowledge for arbitrary questions. The method elicits potential answers (""options""), criteria against which those options should be evaluated, and a ranking of the top ""options."" Our study shows that situated crowdsourcing markets can reliably elicit/moderate knowledge to generate a ranking of options based on different criteria that correlate with established online platforms. Our evaluation also shows that local crowds can generate knowledge that is missing from online platforms and on how a local crowd perceives a certain issue. Finally, we discuss the benefits and challenges of eliciting structured knowledge from local crowds. © 2017 ACM.",Accuracy; Criteria; Crowdsourcing; Local crowds; Options; Performance; Quality; Questions; Situated; Structured knowledge,Commerce; Image quality; Accuracy; Criteria; Local crowds; Options; Performance; Questions; Situated; Structured knowledge; Crowdsourcing
Dual structure constrained multimodal feature coding for social event detection from flickr data,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016474830&doi=10.1145%2f3015463&partnerID=40&md5=7bb1a9e50d19771dcd3e04f458f7aeb0,"In this work, a three-stage social event detection (SED) framework is proposed to discover events from Flickr-like data. First, multiple bipartite graphs are constructed for the heterogeneous feature modalities to achieve fused features. Furthermore, considering the geometrical structures of dictionary and data, a dual structure constrained multimodal feature coding model is designed to learn discriminative feature codes by incorporating corresponding regularization terms into the objective. Finally, clustering models utilizing density or label knowledge and data recovery residual models are devised to discover real-world events. The proposed SED approach achieves the highest performance on the MediaEval 2014 SED dataset. © 2017 ACM.",Event detection; Feature coding; Multimedia content analysis; Multimodal fusion; Social multimedia analytics,Codes (symbols); Event detection; Feature coding; Multi-modal fusion; Multimedia content analysis; Social multimedia analytics; Feature extraction
Detecting influencers in multiple online genres,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016506926&doi=10.1145%2f3014164&partnerID=40&md5=24ad7ccb1664bce9c7be4808d58a0767,"Social media has become very popular and mainstream, leading to an abundance of content. This wealth of content contains many interactions and conversations that can be analyzed for a variety of information. One such type of information is analyzing the roles people take in a conversation. Detecting influencers, one such role, can be useful for political campaigning, successful advertisement strategies, and detecting terrorist leaders. We explore influence in discussion forums, weblogs, and micro-blogs through the development of learned language analysis components to recognize known indicators of influence. Our components are author traits, agreement, claims, argumentation, persuasion, credibility, and certain dialog patterns. Each of these components ismotivated by social science through Robert Cialdini's ""Weapons of Influence"" [Cialdini 2007]. We classify influencers across five online genres and analyze which features are most indicative of influencers in each genre. First, we describe a rich suite of features that were generated using each of the system components. Then, we describe our experiments and results, including using domain adaptation to exploit the data from multiple online genres. © 2017 ACM.",Computational social science; Influence; Natural language processing; Psychology; Social media,Behavioral research; Social networking (online); Social sciences; Computational social science; Influence; NAtural language processing; Psychology; Social media; Natural language processing systems
Can we predict a riot? Disruptive event detection using twitter,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016502012&doi=10.1145%2f2996183&partnerID=40&md5=9d6aa9a287917bf51fb97cdab922b9ba,"In recent years, there has been increased interest in real-world event detection using publicly accessible data made available through Internet technology such as Twitter, Facebook, and YouTube. In these highly interactive systems, the general public are able to post real-time reactions to ""real world"" events, thereby acting as social sensors of terrestrial activity. Automatically detecting and categorizing events, particularly small-scale incidents, using streamed data is a non-trivial task but would be of high value to public safety organisations such as local police, who need to respond accordingly. To address this challenge, we present an end-to-end integrated event detection framework that comprises five main components: data collection, pre-processing, classification, online clustering, and summarization. The integration between classification and clustering enables events to be detected, as well as related smaller-scale ""disruptive events,"" smaller incidents that threaten social safety and security or could disrupt social order. We present an evaluation of the effectiveness of detecting events using a variety of features derived from Twitter posts, namely temporal, spatial, and textual content. We evaluate our framework on a large-scale, real-world dataset from Twitter. Furthermore, we apply our event detection system to a large corpus of tweets posted during the August 2011 riots in England.We use ground-truth data based on intelligence gathered by the London Metropolitan Police Service, which provides a record of actual terrestrial events and incidents during the riots, and show that our system can perform as well as terrestrial sources, and even better in some cases. © 2017 ACM.",Classification; Clustering; Evaluation; Event detection; Feature selection; Socialmedia,Classification (of information); Feature extraction; Law enforcement; Real time systems; Social networking (online); Classification and clustering; Clustering; Evaluation; Event detection; Internet technology; Publicly accessible; Safety and securities; Socialmedia; Uncertainty analysis
Cyber-physical social networks,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016520175&doi=10.1145%2f2996186&partnerID=40&md5=3889742202d6c68da83347e245c8f859,"In the offline world, getting to know new people is heavily influenced by people's physical context, that is, their current geolocation. People meet in classes, bars, clubs, public transport, and so on. In contrast, first-generation online social networks such as Facebook or Google+ do not consider users' context and thus mainly reflect real-world relationships (e.g., family, friends, colleagues). Location-based social networks, or second-generation social networks, such as Foursquare or Facebook Places, take the physical location of users into account to find new friends. However, with the increasing number and wide range of popular platforms and services on the Web, people spend a considerable time moving through the online worlds. In this article, we introduce cyber-physical social networks (CPSN) as the third generation of online social networks. Beside their physical locations, CPSN consider also users' virtual locations for connecting to new friends. In a nutshell, we regard a web page as a place where people can meet and interact. The intuition is that a web page is a good indicator for a user's current interest, likings, or information needs. Moreover, we link virtual and physical locations, allowing for users to socialize across the online and offline world. Our main contributions focus on the two fundamental tasks of creating meaningful virtual locations as well as creating meaningful links between virtual and physical locations, where ""meaningful"" depends on the application scenario. To this end, we present OneSpace, our prototypical implementation of a cyber-physical social network. OneSpace provides a live and social recommendation service for touristic venues (e.g., hotels, restaurants, attractions). It allows mobile users close to a venue and web users browsing online content about the venue to connect and interact in an ad hoc manner. Connecting users based on their shared virtual and physical locations gives way to a plethora of novel use cases for social computing, as we will illustrate. We evaluate our proposed methods for constructing and linking locations and present the results of a first user study investigating the potential impact of cyber-physical social networks. © 2017 ACM.",Ad-hoc socializing; Cyber-physical social networks; Data linking; Location-based services; Network creation; Social computing,Location; Location based services; Mobile telecommunication systems; Social sciences computing; Telecommunication services; Web crawler; Websites; Ad-hoc socializing; Cyber physicals; Datalinking; Network creation; Social computing; Social networking (online)
Progressive random indexing: Dimensionality reduction preserving local network dependencies,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016457464&doi=10.1145%2f2996185&partnerID=40&md5=b55266860a7f130da988fe72b5a85f19,"The vector space model is undoubtedly among the most popular data representation models used in the processing of large networks. Unfortunately, the vector space model suffers from the so-called curse of dimensionality, a phenomenon where data become extremely sparse due to an exponential growth of the data space volume caused by a large number of dimensions. Thus, dimensionality reduction techniques are necessary to make large networks represented in the vector space model available for analysis and processing. Most dimensionality reduction techniques tend to focus on principal components present in the data, effectively disregarding local relationships that may exist between objects. This behavior is a significant drawback of current dimensionality reduction techniques, because these local relationships are crucial for maintaining high accuracy in many network analysis tasks, such as link prediction or community detection. To rectify the aforementioned drawback, we propose Progressive Random Indexing, a new dimensionality reduction technique. Built upon Reflective Random Indexing, our method significantly reduces the dimensionality of the vector space model while retaining all important local relationships between objects. The key element of the Progressive Random Indexing technique is the use of the gain value at each reflection step, which determines how much information about local relationships should be included in the space of reduced dimensionality. Our experiments indicate that when applied to large real-world networks (Facebook social network, MovieLens movie recommendations), Progressive Random Indexing outperforms state-of-the-art methods in link prediction tasks. Michał Ciesielczyk, Andrzej Szwabe, and Paweł Misiorek have been supported by theNational Science Centre © 2017 ACM.",Data mining; Link prediction; Recommender systems; Reflective random indexing; Social networks,Data mining; Data reduction; Forecasting; Indexing (of information); Recommender systems; Social networking (online); Vectors; Curse of dimensionality; Data representation models; Dimensionality reduction; Dimensionality reduction techniques; Link prediction; Movie recommendations; Random indexing; State-of-the-art methods; Vector spaces
Implementing and evaluating a laughing virtual character,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017143240&doi=10.1145%2f2998571&partnerID=40&md5=9cabb7c5079c528c606e0c1af57de645,"Laughter is a social signal capable of facilitating interaction in groups of people: it communicates interest, helps to improve creativity, and facilitates sociability. This article focuses on: endowing virtual characters with computational models of laughter synthesis, based on an expressivity-copying paradigm; evaluating how the physically co-presence of the laughing character impacts on the user's perception of an audio stimulus and mood. We adopt music as a means to stimulate laughter. Results show that the character presence influences the user's perception of music and mood. Expressivity-copying has an influence on the user's perception of music, but does not have any significant impact on mood. © 2017 ACM.",Evaluation; Hci; Laughter; System; Virtual character,Human computer interaction; Internet; Co-presence; Computational model; Evaluation; Laughter; Social signals; System; User's perceptions; Virtual character; Behavioral research
Using centrality measures to predict helpfulness-based reputation in trust networks,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017187030&doi=10.1145%2f2981545&partnerID=40&md5=bea53cd0f920bb7fc693acc19f8a51b7,"In collaborativeWeb-based platforms, user reputation scores are generally computed according to two orthogonal perspectives: (a) helpfulness-based reputation (HBR) scores and (b) centrality-based reputation (CBR) scores. InHBR approaches, the most reputable users are those who post the most helpful reviews according to the opinion of the members of their community. In CBR approaches, a ""who-Trusts-whom"" network-known as a trust network-is available and the most reputable users occupy the most central position in the trust network, according to some definition of centrality. The identification of users featuring large HBR scores is one of the most important research issue in the field of Social Networks, and it is a critical success factor of many Web-based platforms like e-marketplaces, product review Web sites, and question-And-Answering systems. Unfortunately, user reviews/ratings are often sparse, and this makes the calculation of HBR scores inaccurate. In contrast, CBR scores are relatively easy to calculate provided that the topology of the trust network is known. In this article, we investigate if CBR scores are effective to predict HBR ones, and, to perform our study, we used real-life datasets extracted from CIAO and Epinions (two product review Web sites) andWikipedia and applied five popular centrality measures-Degree Centrality, Closeness Centrality, Betweenness Centrality, PageRank and Eigenvector Centrality-to calculate CBR scores. Our analysis provides a positive answer to our research question: CBR scores allow for predicting HBR ones and Eigenvector Centrality was found to be the most important predictor. Our findings prove that we can leverage trust relationships to spot those users producing the most helpful reviews for the whole community. © 2017 ACM 1533-5399/2017/02-ART8 15.00.",,Eigenvalues and eigenfunctions; Web crawler; Websites; Betweenness centrality; Centrality measures; Closeness centralities; Critical success factor; Eigenvector centralities; Real life datasets; Research questions; Web based platform; Forecasting
"Show me you care: Trait empathy, linguistic style, and mimicry on Facebook",2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017091459&doi=10.1145%2f2996188&partnerID=40&md5=37b63327542f8240b35fac3661f2a717,"Linguistic mimicry, the adoption of another's language patterns, is a subconscious behavior with pro-social benefits. However, some professions advocate its conscious use in empathic communication. This involves mutual mimicry; effective communicators mimic their interlocutors, who also mimic them back. Since mimicry has often been studied in face-to-face contexts, we ask whether individuals with empathic dispositions have unique communication styles and/or elicit mimicry in mediated communication on Facebook. Participants completed Davis's Interpersonal Reactivity Index and provided access to Facebook activity. We confirm that dispositional empathy is correlated to the use of particular stylistic features. In addition, we identify four empathy profiles and find correlations to writing style. When a linguistic feature is used, this often ""triggers"" use by friends. However, the presence of particular features, rather than participant disposition, best predicts mimicry. This suggests that machine-human communications could be enhanced based on recently used features, without extensive user profiling. © 2017 ACM.",Affect; Empathic response; Empathy; Interpersonal relations; Linguistic alignment; Linguistic mimicry; Linguistic style; Social media,Social networking (online); Affect; Empathic response; Empathy; Interpersonal relations; Linguistic styles; Social media; Linguistics
Privacy-enhanced television audience measurements,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017095310&doi=10.1145%2f3009969&partnerID=40&md5=e44e2aa3fa9543098a3c5209ac307778,"Internet-enabled television systems (SmartTVs) are a development that introduces these devices into the interconnected environment of the Internet of Things. We propose a privacy-preserving application for computing Television Audience Measurement (TAM) ratings. SmartTVs communicate over the Internet to calculate aggregate measurements. Contemporary cryptographic building blocks are utilized to ensure the privacy of the participating individuals and the validity of the computed TAM ratings. Additionally, user compensation capabilities are introduced to bring some of the company profits back to the data owners. A prototype implementation is developed on an Android-based SmartTV platform and experimental results illustrate the feasibility of the approach. © 2017 ACM 1533-5399/2017/02-ART10 15.00.",,Data privacy; Building blockes; Compensation capability; Enhanced television; Privacy preserving; Prototype implementations; Smart-TV; Television systems
Affect and interaction in agent-based systems and social media: Guest editors' introduction,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017178668&doi=10.1145%2f3018980&partnerID=40&md5=33bc09740bde0dc3ae4a4714652ebcbb,"Leading research on computational models of affect-related phenomena in interactions occurring either in social media or agent-based systems are reviewed in the special section of ACM Transactions on Internet Technology, March 2017. The article by Mancini and colleagues addresses the complex relation that links empathetic behavior with the manifestation of laughter in interpersonal relations, exploring the valence of laughter for the establishment of a positive mood in the user. The article by Dastani and Pankov proposes a semi-formal model of the so-called CAD triad that specifies the conditions that motivate each emotion type and the goals they trigger when established. Espinosa and colleagues describe an extension of Jason, the BDI agent architecture, called GenIA3 (General-purpose Intelligent Affective Agent Architecture). The proposed architecture assumes a cognitive model of emotions, but is not committed to a specific theory, so different emotion models can be implemented through it. Otterbacher and colleagues propose a statistical study of the relationship between users' empathy profile (obtained by questionnaires concerning users' cognitive and emotional empathy) and the specific linguistic features that are used when engaging friends (behavioral empathy). De Meo and researchers focus on how to fruitfully combine information coming from different perspectives on user reputation in online social networks.",Affective computing; Agent interaction; Socio-affective behavior,Architecture; Computation theory; Computer aided design; Linguistics; Network architecture; Surveys; Affective agent architectures; Affective behaviors; Affective Computing; Agent interaction; Computational model; Interpersonal relations; On-line social networks; Proposed architectures; Social networking (online)
Improving the efficiency of an online marketplace by incorporating forgiveness mechanism,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011396223&doi=10.1145%2f2996189&partnerID=40&md5=65d5f69dad77594eeb642d9642c7605c,"Reputation plays a key role in online marketplace communities improving trust among community members. Reputation works as a decision-making tool for understanding the behavior of the business partners. Success of any online business depends on the trust the business agents share with each other. However, untrustworthy agents have anno place in online marketplaces and are forced to leave the market even if they will potentially cooperate. In this study, we propose an exploration strategy based on a forgiveness mechanism for untrustworthy agents to recover their reputation. Furthermore, a number of experiments based on the NetLogo simulation are performed to validate the applicability of the proposed mechanism. The results show that the online marketplaces incorporating a forgiveness mechanism can be used with the existing reputation systems and improve the efficiency of online marketplaces. © 2017 ACM.",Forgiveness mechanism; NetLogo; Online marketplaces; Reputation,Decision making; Efficiency; Online systems; Business partners; Decision making tool; Exploration strategies; NetLogo; On-line marketplaces; Online business; Reputation; Reputation systems; Electronic commerce
Toward formal modeling of affective agents in a bdi architecture,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009387535&doi=10.1145%2f3001584&partnerID=40&md5=fbb3ac1c0aa68f20838a90e232b2731d,"Affective characteristics are crucial factors that influence human behavior, and often, the prevalence of either emotions or reason varies on each individual. We aim to facilitate the development of agents' reasoning considering their affective characteristics. We first identify core processes in an affective BDI agent, and we integrate them into an affective agent architecture (GenIA3). These tasks include the extension of the BDI agent reasoning cycle to be compliant with the architecture, the extension of the agent language (Jason) to support affect-based reasoning, and the adjustment of the equilibrium between the agent's affective and rational sides. © 2017 ACM.",Affective characteristics; Agents; Emotions; Formalization; Jason,Agents; Behavioral research; Affective agent architectures; Affective characteristics; Agent languages; BDI architecture; Emotions; Formalization; Human behaviors; Jason; Architecture
Brains or beauty: How to engender trust in user-Agent interactions,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009415793&doi=10.1145%2f2998572&partnerID=40&md5=c656910a324de76ac0bb98f56ac5f12a,"Software-based agents are becoming increasingly ubiquitous and automated. However, current technology and algorithms are still fallible, which considerably affects users' trust and interaction with such agents. In this article, we investigate two factors that can engender user trust in agents: Reliability and attractiveness of agents. We show that agent reliability is not more important than agent attractiveness. Subjective user ratings of agent trust and perceived accuracy suggest that attractiveness may be even more important than reliability. © 2017 ACM.",Attractiveness; Intelligent agents; Intelligent personal assistants; Reliability; Software agents; Trust,Intelligent agents; Reliability; Software reliability; Attractiveness; Current technology; Personal assistants; Trust; User agents; User rating; Software agents
Processing affect in social media: A comparison of methods to distinguish emotions in tweets,2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009360782&doi=10.1145%2f2996187&partnerID=40&md5=a373a5336197620be9718a2ce2779cd5,"Emotion analysis in social media is challenging. While most studies focus on positive and negative sentiments, the differentiation between emotions is more difficult. We investigate the problem as a collection of binary classification tasks on the basis of four opposing emotion pairs provided by Plutchik. We processed the content of messages by three alternative methods: structural and lexical features, latent factors, and natural language processing. The final prediction is suggested by classifiers deriving from the state of the art in machine learning. Results are convincing in the possibility to distinguish the emotions pairs in social media. © 2017 ACM.",Latent factors; Lexical approach; Probabilistic methods,Learning algorithms; Learning systems; Natural language processing systems; Binary classification; Comparison of methods; Content of messages; Latent factor; Lexical approach; NAtural language processing; Negative sentiments; Probabilistic methods; Social networking (online)
"Other-condemning moral emotions: Anger, contempt and disgust",2017,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009354328&doi=10.1145%2f2998570&partnerID=40&md5=9a40e5882bd9bfe83baab4c40acddae4,"This article studies and analyzes three other-condemning moral emotions: Anger, contempt, and disgust. We utilize existing psychological theories-Appraisal theories of emotion and the CAD triad hypothesis-And incorporate them into a unified framework. A semiformal specification of the elicitation conditions and prototypical coping strategies for the other-condemning emotions are proposed. The appraisal conditions are specified in terms of cognitive and social concepts such as goals, beliefs, actions, control and accountability, while coping strategies are classified as belief-, goal-And intention-Affecting strategies, and specified in terms of action specifications. Our conceptual analysis and semiformal specification of the three other-condemning moral emotions are illustrated by means of an example of trolling in the domain of social media. © 2017 ACM.",Cognitive models; Moral emotions; Ontology of emotions,Computer aided design; Action specifications; Cognitive model; Conceptual analysis; Coping strategies; Moral emotions; Psychological theory; Semi-formal specification; Unified framework; Specifications
Authentication protocol for an iot-enabled LTE network,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008157558&doi=10.1145%2f2981547&partnerID=40&md5=21b2f1f0482e136972cb84e9aa743b4d,"The Evolved Packet System-based Authentication and Key Agreement (EPS-AKA) protocol of the longterm evolution (LTE) network does not support Internet of Things (IoT) objects and has several security limitations, including transmission of the object's (user/device) identity and key set identifier in plaintext over the network, synchronization, large overhead, limited identity privacy, and security attack vulnerabilities. In this article, we propose a new secure and efficient AKA protocol for the LTE network that supports secure and efficient communications among various IoT devices as well as among the users. Analysis shows that our protocol is secure, efficient, and privacy preserved, and reduces bandwidth consumption during authentication. © 2016 ACM 1533-5399/2016/12-ART23 $15.00.",IoT; Key-ID theft; LTE; Man-in-The-middle attack; Object-ID theft,Authentication; Crime; Internet of things; Internet protocols; Network protocols; Wireless telecommunication systems; Authentication and key agreements; Authentication protocols; Bandwidth consumption; Efficient communications; Id thefts; Internet of Things (IOT); Longterm-evolution (LTE); Man in the middle attacks; Network security
Internet of things (IoT): Smart and secure service delivery,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008193408&doi=10.1145%2f3013520&partnerID=40&md5=fc97d4d217ae4feb0b8b46118b1c716e,"The Internet of Things (IoT) is the latest Internet evolution that incorporates a diverse range of things such as sensors, actuators, and services deployed by different organizations and individuals to support a variety of applications. The information captured by IoT present an unprecedented opportunity to solve large-scale problems in those application domains to deliver services; example applications include precision agriculture, environment monitoring, smart health, smart manufacturing, and smart cities. Like all other Internet based services in the past, IoT-based services are also being developed and deployed without security consideration. By nature, IoT devices and services are vulnerable tomalicious cyber threats as they cannot be given the same protection that is received by enterprise services within an enterprise perimeter.While IoT services will play an important role in our daily life resulting in improved productivity and quality of life, the trend has also ""encouraged"" cyber-exploitation and evolution and diversification of malicious cyber threats. Hence, there is a need for coordinated efforts from the research community to address resulting concerns, such as those presented in this special section. Several potential research topics are also identified in this special section. © 2016 ACM 1533-5399/2016/12-ART23 $15.00.",Internet of things; Security and privacy,Internet; Environment monitoring; Internet of thing (IOT); Internet of Things (IOT); Internet-based services; nocv1; Precision Agriculture; Secure service delivery; Security and privacy; Security considerations; Internet of things
Characterization of wireless multidevice users,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008239850&doi=10.1145%2f2955096&partnerID=40&md5=de289757f5401c3f7018b9b96db29bf0,"The number of wireless-enabled devices owned by a user has had huge growth over the past few years. Over one third of adults in the United States currently own three wireless devices: A smartphone, laptop, and tablet. This article provides a study of the network usage behavior of today's multidevice users. Using data collected from a large university campus, we provide a detailed multidevice user (MDU) measurement study of more than 30,000 users. The major objective of this work is to study how the presence of multiple wireless devices affects the network usage behavior of users. Specifically, we characterize the usage pattern of the different device types in terms of total and intermittent usage, how the usage of different devices overlap over time, and uncarried device usage statistics.We also study user preferences of accessing sensitive content and device-specific factors that govern the choice of WiFi encryption type. The study reveals several interesting findings about MDUs. We see how the use of tablets and laptops are interchangeable and how the overall multidevice usage is additive instead of being shared among the devices.We also observe how current DHCP configurations are oblivious to multiple devices, which results in inefficient utilization of available IP address space. All findings about multidevice usage patterns have the potential to be utilized by different entities, such as app developers, network providers, security researchers, and analytics and advertisement systems, to providemore intelligent and informed services to users who have at least two devices among a smartphone, tablet, and laptop. © 2016 ACM 1533-5399/2016/12-ART23 $15.00.",Campus Network; Laptop; Multidevice Users; Network Utilization; Smartphone; Tablet,Cryptography; Laptop computers; Signal encoding; Smartphones; Campus network; Laptop; Multi-devices; Net work utilization; Tablet; User interfaces
Intelligent intrusion detection in low-power IoTs,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008163210&doi=10.1145%2f2990499&partnerID=40&md5=493e1822c1ed45bd567d7b27b2609999,"Security and privacy of data are one of the prime concerns in today's Internet of Things (IoT). Conventional security techniques like signature-based detection of malware and regular updates of a signature database are not feasible solutions as they cannot secure such systems effectively, having limited resources. Programming languages permitting immediate memory accesses through pointers often result in applications having memory-related errors, which may lead to unpredictable failures and security vulnerabilities. Furthermore, energy efficient IoT devices running on batteries cannot afford the implementation of cryptography algorithms as such techniques have significant impact on the system power consumption. Therefore, in order to operate IoT in a secure manner, the system must be able to detect and prevent any kind of intrusions before the network (i.e., sensor nodes and base station) is destabilised by the attackers. In this article, we have presented an intrusion detection and prevention mechanism by implementing an intelligent security architecture using random neural networks (RNNs). The application's source code is also instrumented at compile time in order to detect out-of-bound memory accesses. It is based on creating tags, to be coupled with each memory allocation and then placing additional tag checking instructions for each access made to the memory. To validate the feasibility of the proposed security solution, it is implemented for an existing IoT system and its functionality is practically demonstrated by successfully detecting the presence of any suspicious sensor node within the system operating range and anomalous activity in the base station with an accuracy of 97.23%. Overall, the proposed security solution has presented a minimal performance overhead. © 2016 ACM 1533-5399/2016/12-ART23 $15.00.",Buffer Overflows; Code Instrumentation; Data Integrity; Illegal Memory Accesses; IoT Security; Neural Networks,Base stations; Data privacy; Energy efficiency; Internet of things; Intrusion detection; Memory architecture; Mercury (metal); Mobile telecommunication systems; Neural networks; Sensor nodes; Buffer overflows; Code instrumentation; Data integrity; IoT Security; Memory access; Network security
Crowdsourced mobile data transfer with delay bound,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008433284&doi=10.1145%2f2939376&partnerID=40&md5=95eb21e2e154131ec1f7f49998d593e2,"In this article, we design a crowdsourcing system, CrowdMAC, where mobile devices form a local community or marketplace to share network access and transfer data for each other. CrowdMAC enables (i) mobile clients to select and exploit multiple mobile hotspots in its vicinity for data transfer and (ii) mobile hotspots to open their cellular connectivity to admit/serve delay-bounded requests from mobile users for a fee. The evaluations of CrowdMAC indicate that (i) mobile clients can tune preferred trade-offs between cost and delay through a control knob, (ii) mobile hotspots comply with all delay bounds, and (iii) the system ensures stable and efficient transfer. © 2016 ACM.",Cellular networks; Crowdsourcing; Internet access sharing; Network optimization; P2P wireless networks,Commerce; Crowdsourcing; Data transfer; Economic and social effects; Mobile telecommunication systems; Wireless networks; Cellular connectivity; Cellular network; Internet access; Local community; Mobile hotspots; Network access; Network optimization; P2p wireless networks; Peer to peer networks
Advanced security testbed framework for wearable IoT devices,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008225276&doi=10.1145%2f2981546&partnerID=40&md5=9a6b581ed8d8f5952fb962b7bbace838,"Analyzing the security of Wearable Internet-of-Things (WIoT) devices is considered a complex task due to their heterogeneous nature. In addition, there is currently no mechanism that performs security testing for WIoT devices in different contexts. In this article, we propose an innovative security testbed framework targeted at wearable devices, where a set of security tests are conducted, and a dynamic analysis is performed by realistically simulating environmental conditions in which WIoT devices operate. The architectural design of the proposed testbed and a proof-of-concept, demonstrating a preliminary analysis and the detection of context-based attacks executed by smartwatch devices, are presented. © 2016 ACM 1533-5399/2016/12-ART23 $15.00.",Internet of things (IoT); Privacy; Security; Testbed framework; Wearable devices,Data privacy; Testbeds; Wearable technology; Environmental conditions; Internet of Things (IOT); Preliminary analysis; Proof of concept; Security; Security testing; Security tests; Wearable devices; Internet of things
Secure data-centric access control for smart grid services based on publish/subscribe systems,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008157537&doi=10.1145%2f3007190&partnerID=40&md5=226b02ffc1bd3919a0c4423db4cf20d9,"The communication systems in existing smart gridsmainly take the request/reply interaction model, in which data access is under the direct control of data producers. This tightly controlled interaction model is not scalable to support complex interactions among smart grid services. On the contrary, the publish/subscribe system features a loose coupling communication infrastructure and allows indirect, anonymous and multicast interactions among smart grid services. The publish/subscribe system can thus support scalable and flexible collaboration among smart grid services. However, the access is not under the direct control of data producers, it might not be easy to implement an access control scheme for a publish/subscribe system. In this article, we propose a Data-Centric Access Control Framework (DCACF) to support secure access control in a publish/subscribe model. This framework helps to build scalable smart grid services, while keeping features of service interactions and data confidentiality at the same time. The data published in our DCACF is encrypted with a fully homomorphic encryption scheme, which allows in-grid homomorphic aggregation of the encrypted data. The encrypted data is accompanied by bloom-filter encoded control policies and access credentials to enable indirect access control. We have analyzed the correctness and security of our DCACF and evaluated its performance in a distributed environment. © 2016 ACM 1533-5399/2016/12-ART23 $15.00.",Access Control; Full Homomorphic Encryption; Internet Of Things; Publish/Subscribe System; Service Collaboration,Cryptography; Electric power transmission networks; Grid computing; Internet of things; Message passing; Publishing; Smart power grids; Access control schemes; Communication infrastructure; Data confidentiality; Distributed environments; Fully homomorphic encryption schemes; Ho-momorphic encryptions; Publish/Subscribe system; Service Collaboration; Access control
Password-Authenticated group key exchange: A cross-layer design,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008145250&doi=10.1145%2f2955095&partnerID=40&md5=58bd37c2518fe1a87a4ff4816cfebdf4,"Two-party password-Authenticated key exchange (2PAKE) protocols provide a natural mechanism for secret key establishment in distributed applications, and they have been extensively studied in past decades. However, only a few efforts have been made so far to design password-Authenticated group key exchange (GPAKE) protocols. In a 2PAKE or GPAKE protocol, it is assumed that short passwords are preshared among users. This assumption, however, would be impractical in certain applications.Motivated by this observation, this article presents a GPAKE protocol without the password sharing assumption. To obtain the passwords, wireless devices, such as smart phones, tablets, and laptops, are used to extract short secrets at the physical layer. Using the extracted secrets, users in our protocol can establish a group key at higher layers with light computation consumptions. Thus, our GPAKE protocol is a cross-layer design. Additionally, our protocol is a compiler, that is, our protocol can transform any provably secure 2PAKE protocol into a GPAKE protocol with only one more round of communications. Besides, the proposed protocol is proved secure in the standard model. © 2016 ACM 1533-5399/2016/12-ART23 $15.00.",Group Key Exchange; Higher Layers; Password; Security; The Physical Layer,Network layers; Smartphones; Group key exchange; Higher Layers; Password; Physical layers; Security; Authentication
Dynamic and efficient private keyword search over inverted index-based encrypted data,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054898922&doi=10.1145%2f2940328&partnerID=40&md5=af8e1c276a925e0800b3bfa8ee83485a,"Querying over encrypted data is gaining increasing popularity in cloud-based data hosting services. Security and efficiency are recognized as two important and yet conflicting requirements for querying over encrypted data. In this article, we propose an efficient private keyword search (EPKS) scheme that supports binary search and extend it to dynamic settings (called DEPKS) for inverted index-based encrypted data. First, we describe our approaches of constructing a searchable symmetric encryption (SSE) scheme that supports binary search. Second, we present a novel framework for EPKS and provide its formal security definitions in terms of plaintext privacy and predicate privacy by modifying Shen et al.'s security notions [Shen et al. 2009]. Third, built on the proposed framework, we design an EPKS scheme whose complexity is logarithmic in the number of keywords. The scheme is based on the groups of prime order and enjoys strong notions of security, namely statistical plaintext privacy and statistical predicate privacy. Fourth, we extend the EPKS scheme to support dynamic keyword and document updates. The extended scheme not only maintains the properties of logarithmic-time search efficiency and plaintext privacy and predicate privacy but also has fewer rounds of communications for updates compared to existing dynamic search encryption schemes. We experimentally evaluate the proposed EPKS and DEPKS schemes and show that they are significantly more efficient in terms of both keyword search complexity and communication complexity than existing randomized SSE schemes. © 2016 ACM.",Binary search; Dynamic updates; Plaintext privacy; Predicate privacy; Searchable symmetric encryption,Efficiency; Indexing (of information); Search engines; Binary search; Communication complexity; Dynamic update; Encryption schemes; Inverted indices; Plaintext; Search efficiency; Symmetric encryption; Cryptography
RSYBL: A framework for specifying and controlling cloud services elasticity,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009379596&doi=10.1145%2f2925990&partnerID=40&md5=09a12c75cac804b2ff04b1c93c5cb715,"Cloud applications can benefit from the on-demand capacity of cloud infrastructures, which offer computing and data resources with diverse capabilities, pricing, and quality models. However, state-of-the-art tools mainly enable the user to specify ""if-then-else"" policies concerning resource usage and size, resulting in a cumbersome specification process that lacks expressiveness for enabling the control of complex multilevel elasticity requirements. In this article, first we propose SYBL, a novel language for specifying elasticity requirements at multiple levels of abstraction. Second, we design and develop the rSYBL framework for controlling cloud services at multiple levels of abstractions. To enforce user-specified requirements, we develop a multilevel elasticity controlmechanism enhanced with conflict resolution. rSYBL supports different cloud providers and is highly extensible, allowing service providers or developers to define their own connectors to the desired infrastructures or tools. We validate it through experiments with two distinct services, evaluating rSYBL over two distinct cloud infrastructures, and showing the importance of multilevel elasticity control. © 2016 ACM.",Cloud computing; Control; Elasticity; Elasticity requirements,Abstracting; Cloud computing; Control engineering; Distributed database systems; Human computer interaction; Platform as a Service (PaaS); Web services; Cloud applications; Cloud infrastructures; Cloud providers; Conflict Resolution; Multiple levels; Service provider; Specification process; State of the art; Elasticity
Irony detection in twitter: The role of affective content,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979055719&doi=10.1145%2f2930663&partnerID=40&md5=c3e302fbf1c18665bf25873ce7d960db,"Irony has been proven to be pervasive in social media, posing a challenge to sentiment analysis systems. It is a creative linguistic phenomenon where affect-related aspects play a key role. In this work, we address the problem of detecting irony in tweets, casting it as a classification problem. We propose a novel model that explores the use of affective features based on a wide range of lexical resources available for English, reflecting different facets of affect. Classification experiments over different corpora show that affective information helps in distinguishing among ironic and nonironic tweets. Our model outperforms the state of the art in almost all cases.",Affective resources; Figurative language processing; Irony detection,Computational linguistics; Social networking (online); Affective resources; Language processing; Lexical resources; Linguistic phenomena; Sentiment analysis; Social media; State of the art; Classification (of information)
Mining and quality assessment of mashup model patterns with the crowd: A feasibility study,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978081289&doi=10.1145%2f2903138&partnerID=40&md5=fcf4f95b9a7c3a3feaa8350e1d8913c6,"Pattern mining, that is, the automated discovery of patterns from data, is a mathematically complex and computationally demanding problem that is generally not manageable by humans. In this article, we focus on small datasets and study whether it is possible to mine patterns with the help of the crowd by means of a set of controlled experiments on a common crowdsourcing platform. We specifically concentrate on mining model patterns from a dataset of real mashup models taken from Yahoo! Pipes and cover the entire pattern mining process, including pattern identification and quality assessment. The results of our experiments show that a sensible design of crowdsourcing tasks indeed may enable the crowd to identify patterns from small datasets (40 models). The results, however, also show that the design of tasks for the assessment of the quality of patterns to decide which patterns to retain for further processing and use is much harder (our experiments fail to elicit assessments from the crowd that are similar to those by an expert). The problem is relevant in general to model-driven development (e.g., UML, business processes, scientific workflows), in that reusable model patterns encode valuable modeling and domain knowledge, such as best practices, organizational conventions, or technical choices, that modelers can benefit from when designing their own models. © 2016 ACM.",Algorithms; Experimentation; Human factors,Algorithms; Crowdsourcing; Human engineering; Automated discovery; Controlled experiment; Crowdsourcing platforms; Experimentation; Feasibility studies; Model driven development; Pattern identification; Scientific workflows; Data mining
Model-based collaborative personalized recommendation on signed social rating networks,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978958329&doi=10.1145%2f2934681&partnerID=40&md5=b534b2c98c2fcdeb21eb936860f80c06,"Recommendation on signed social rating networks is studied through an innovative approach. Bayesian probabilistic modeling is used to postulate a realistic generative process, wherein user and item interactions are explained by latent factors, whose relevance varies within the underlying network organization into user communities and item groups. Approximate posterior inference captures distrust propagation and drives Gibbs sampling to allow rating and (dis)trust prediction for recommendation along with the unsupervised exploratory analysis of network organization. Comparative experiments reveal the superiority of our approach in rating and link prediction on Epinionsand Ciao, besides community quality and recommendation sensitivity to network organization. © 2016 ACM.",Link prediction; Mixed-membership block modeling; Rating prediction,Forecasting; Rating; Bayesian probabilistic models; Block modeling; Comparative experiments; Distrust propagation; Innovative approaches; Link prediction; Network organization; Personalized recommendation; Knowledge based systems
MARSA: A marketplace for realtime human sensing data,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993933036&doi=10.1145%2f2883611&partnerID=40&md5=d1c2b56493693eb852524ea2dda2f49f,"This article introduces a dynamic cloud-based marketplace of near-realtime human sensing data (MARSA) for different stakeholders to sell and buy near-realtime data. MARSA is designed for environments where information technology (IT) infrastructures are not well developed but the need to gather and sell nearrealtime data is great. To this end, we present techniques for selecting data types and managing data contracts based on different cost models, quality of data, and data rights.We design our MARSA platform by leveraging different data transferring solutions to enable an open and scalable communication mechanism between sellers (data providers) and buyers (data consumers). To evaluate MARSA, we carry out several experiments with the near-realtime transportation data provided by people in Ho Chi Minh City, Vietnam, and simulated scenarios in multicloud environments. © 2016 ACM.",Cost model; Data contract; Internet of Things; Platform,Graphical user interfaces; Internet of things; Sales; Cost modeling; Data contracts; Data transferring; Information technology infrastructure; Near real-time datum; Platform; Real-time transportation; Scalable communication; Commerce
Patterns in the chaos-A study of performance variation and predictability in public IaaS clouds,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971432685&doi=10.1145%2f2885497&partnerID=40&md5=1061b3fc4e59d8f1268d0179d13e2413,"Benchmarking the performance of public cloud providers is a common research topic. Previous work has already extensively evaluated the performance of different cloud platforms for different use cases, and under different constraints and experiment setups. In this article, we present a principled, large-scale literature review to collect and codify existing research regarding the predictability of performance in public Infrastructure-as-a-Service (IaaS) clouds.We formulate 15 hypotheses relating to the nature of performance variations in IaaS systems, to the factors of influence of performance variations, and how to compare different instance types. In a second step, we conduct extensive real-life experimentation on four cloud providers to empirically validate those hypotheses. We show that there are substantial differences between providers. Hardware heterogeneity is today less prevalent than reported in earlier research, while multitenancy has a dramatic impact on performance and predictability, but only for some cloud providers. We were unable to discover a clear impact of the time of the day or the day of the week on cloud performance. © 2016 ACM.",Benchmarking; Infrastructure-as-a-service; Public cloud,Benchmarking; Electrostatic actuators; Cloud performance; Cloud platforms; Experiment set-up; Literature reviews; Multi tenancies; Performance variations; Public clouds; Research topics; Infrastructure as a service (IaaS)
Unsupervised extraction of popular product attributes from E-commerce web sites by considering customer reviews,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969922887&doi=10.1145%2f2857054&partnerID=40&md5=d268bceaf534ce379993b5700d8496de,"We develop an unsupervised learning framework for extracting popular product attributes from product description pages originated from different E-commerce Web sites. Unlike existing information extraction methods that do not consider the popularity of product attributes, our proposed framework is able to not only detect popular product features from a collection of customer reviews but also map these popular features to the related product attributes. One novelty of our framework is that it can bridge the vocabulary gap between the text in product description pages and the text in customer reviews. Technically, we develop a discriminative graphical model based on hidden Conditional Random Fields. As an unsupervised model, our framework can be easily applied to a variety of new domains and Web sites without the need of labeling training samples. Extensive experiments have been conducted to demonstrate the effectiveness and robustness of our framework. © 2016 ACM.",Conditional random fields; Customer reviews; Information extraction; Product attribute,Commerce; Electronic commerce; Information analysis; Information retrieval; Random processes; Websites; Conditional random field; Customer review; Hidden conditional random fields; Information extraction methods; Product attributes; Product descriptions; Related products; Unsupervised extraction; Sales
Negotiating premium peering prices: A quantitative model with applications,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971223299&doi=10.1145%2f2883610&partnerID=40&md5=ea2354ff727a7e0cb8d481d50116005f,"We have developed a novel methodology for deriving bandwidth prices for premium direct peering between Access ISPs (A-ISPs) and Content and Service Providers (CSPs) that want to deliver content and services in premium quality. Our methodology establishes a direct link between service profitability, for example, from advertising, user and subscriber loyalty, interconnection costs, and finally bandwidth price for peering. Unlike existing work in both the networking and economics literature, our resulting computational model, built around Nash bargaining, can be used for deriving quantitative results comparable to actual market prices. We analyze the U.S. market and derive prices for video, that compare favorably with existing prices for transit and paid peering. We also observe that the fair prices returned by the model for high-profit/lowvolume services such as search, are orders of magnitude higher than current bandwidth prices. This implies that resolving existing (fierce) interconnection tussles may require per service, instead of wholesale, peering between A-ISPs and CSPs. Our model can be used for deriving initial benchmark prices for such negotiations. ©2016 ACM.",Interconnection economics; Premium service delivery; Tussle analysis,Bandwidth; Commerce; Internet service providers; Profitability; Computational model; Interconnection costs; Novel methodology; Orders of magnitude; Quantitative modeling; Quantitative result; Service delivery; Tussle analysis; Costs
SoIoT: Toward a user-centric ioT-based service framework,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971433438&doi=10.1145%2f2835492&partnerID=40&md5=8a49e76d9d76d9e1a4877ab2f284717f,"An emerging issue in urban computing environments is the seamless selection, composition, and delivery of user-centric services that run over what is known as the Internet of Things (IoT). This challenge is about enabling services actuated by IoT devices to be delivered spontaneously from the perspective of users. To accomplish this goal, we propose the Service-Oriented Internet of Things (SoIoT), a user-centric IoT-based service framework, which integrates services that utilize IoT resources in an urban computing environment. This framework provides a task-oriented computing approach that enables the composition of IoT-based services in a spontaneous manner to accomplish a user task. Tasks can also be recommended to users based on the available IoT resources in an environment and on the contextual knowledge that is represented and managed in social, spatial, and temporal aspects. These tasks are then bound to a set of service instances and performed in a distributed manner. This final composition ensures the Quality of Service (QoS) requirements of the tasks and is assigned to multiple client devices for the efficient utilization of IoT resources.We prove the practicality of our approach by showing a real-case service scenario implemented in our IoT-based test-bed as well as experimental results. © 2016 ACM 1533-5399/2016/04-ART8 $15.00.",Internet of things; Service composition; Task-oriented computing; Urban computing; User centricity,Internet; Quality of service; Contextual knowledge; Internet of thing (IOT); Qualityof-service requirement (QoS); Service compositions; Task-oriented; Urban computing; User centricities; User-centric service; Internet of things
Multiobjective optimization for brokering of multicloud service composition,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973897358&doi=10.1145%2f2870634&partnerID=40&md5=1b590a3f8d4cade62fcbe4e0db570fa5,"The choice of cloud providers whose offers best fit the requirements of a particular application is a complex issue due to the heterogeneity of the services in terms of resources, costs, technology, and service levels that providers ensure. This article investigates the effectiveness of multiobjective genetic algorithms to resolve a multicloud brokering problem. Experimental results provide clear evidence about how such a solution improves the choice made manually by users returning in real time optimal alternatives. It also investigates how the optimality depends on different genetic algorithms and parameters, problem type, and time constraints. © 2016 ACM.",Cloud brokering; Multiobjective constrained problems,Genetic algorithms; Optimization; Cloud providers; Constrained problem; Multi-objective genetic algorithm; Optimality; Real time; Service compositions; Service levels; Time constraints; Multiobjective optimization
A scalable framework for provisioning large-scale IoT deployments,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963811933&doi=10.1145%2f2850416&partnerID=40&md5=7e526d2490be1d6045487e765d91e592,"Internet of Things (IoT) devices are usually considered external application dependencies that only provide data or process and execute simple instructions. The recent emergence of IoT devices with embedded execution environments allows practitioners to deploy and execute custom application logic directly on the device. This approach fundamentally changes the overall process of designing, developing, deploying, and managing IoT systems. However, these devices exhibit significant differences in available execution environments, processing, and storage capabilities. To accommodate this diversity, a structured approach is needed to uniformly and transparently deploy application components onto a large number of heterogeneous devices. This is especially important in the context of large-scale IoT systems, such as in the smart city domain. In this article, we present LEONORE, an infrastructure toolset that provides elastic provisioning of application components on resource-constrained and heterogeneous edge devices in large-scale IoT deployments. LEONORE supports push-based as well as pull-based deployments. To improve scalability and reduce generated network traffic between cloud and edge infrastructure, we present a distributed provisioning approach that deploys LEONORE local nodes within the deployment infrastructure close to the actual edge devices. We show that our solution is able to elastically provision large numbers of devices using a testbed based on a real-world industry scenario. © 2016 ACM.",Framework; Gateway; IoT; Large-scale; Provisioning; Resource-constrained,Digital storage; Gateways (computer networks); Reconfigurable hardware; Application components; Execution environments; Framework; Heterogeneous devices; Internet of Things (IOT); Large-scale; Provisioning; Resource-constrained; Internet of things
Argumentation mining: State of the art and emerging trends,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964734735&doi=10.1145%2f2850417&partnerID=40&md5=a211472e985c4284669404f5d77e2d2a,"Argumentation mining aims at automatically extracting structured arguments from unstructured textual documents. It has recently become a hot topic also due to its potential in processing information originating from the Web, and in particular from social media, in innovative ways. Recent advances in machine learning methods promise to enable breakthrough applications to social and economic sciences, policy making, and information technology: something that only a few years ago was unthinkable. In this survey article, we introduce argumentation models and methods, review existing systems and applications, and discuss challenges and perspectives of this exciting new research area. © 2016 ACM.",Argumentation mining; Artificial intelligence; Computational linguistics; Knowledge representation; Machine learning; Social media,Artificial intelligence; Computational linguistics; Knowledge representation; Social networking (online); Argumentation model; Economic science; Emerging trends; Existing systems; Machine learning methods; Social media; State of the art; Textual documents; Learning systems
Things of interest recommendation by leveraging heterogeneous relations in the internet of things,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964374770&doi=10.1145%2f2837024&partnerID=40&md5=e33c438ca579b0ccec71065797e7d147,"The emerging Internet of Things (IoT) bridges the gap between the physical and the digital worlds, which enables a deeper understanding of user preferences and behaviors. The rich interactions and relations between users and things call for effective and efficient recommendation approaches to better meet users' interests and needs. In this article, we focus on the problem of things recommendation in IoT, which is important for many applications such as e-Commerce and health care. We discuss the new properties of recommending things of interest in IoT, and propose a unified probabilistic factor based framework by fusing relations across heterogeneous entities of IoT, for example, user-thing relations, user-user relations, and thing-thing relations, to make more accurate recommendations. Specifically, we develop a hypergraph to model things' spatiotemporal correlations, on top of which implicit things correlations can be generated. We have built an IoT testbed to validate our approach and the experimental results demonstrate its feasibility and effectiveness. © 2016 ACM.",Data mining; Hypergraph; Internet of things; Latent relationships; Recommendation,Behavioral research; Data mining; Internet; Digital world; Hypergraph; Internet of Things (IOT); Latent relationships; Probabilistic factors; Recommendation; Spatiotemporal correlation; Users' interests; Internet of things
Quality-based online data reconciliation,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964703556&doi=10.1145%2f2806888&partnerID=40&md5=10c2a954b159d07b7e042c8e8c5a79a8,"One of the main challenges in data matching and data cleaning, in highly integrated systems, is duplicates detection. While the literature abounds of approaches detecting duplicates corresponding to the same realworld entity, most of these approaches tend to eliminate duplicates (wrong information) from the sources, hence leading to what is called data repair. In this article, we propose a framework that automatically detects duplicates at query time and effectively identifies the consistent version of the data, while keeping inconsistent data in the sources. Our framework uses matching dependencies (MDS) to detect duplicates through the concept of data reconciliation rules (DRR) and conditional function dependencies (CFDs) to assess the quality of different attribute values. We also build a duplicate reconciliation index (DRI), based on clusters of duplicates detected by a set of DRRs to speed up the online data reconciliation process. Our experiments of a real-world data collection show the efficiency and effectiveness of our framework. © 2016 ACM.",Data quality rules; Data reconciliation; Duplicates; Source quality,Attribute values; Data quality; Data reconciliation; Duplicates; Inconsistent data; Integrated systems; Matching dependencies; Real-world entities; Internet
Constructing maintainable semantic relation network from ambiguous concepts in web content,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964397342&doi=10.1145%2f2814568&partnerID=40&md5=7ee41888a07b218e4c4e3151f9e8199a,"The semantic network is a form of knowledge that represents various relationships between concepts with ambiguity. The knowledge can be employed to identify semantically related objects. It helps, for example, a recommender system to generate effective recommendations to the users.We propose to study a new semantic network, namely, the Concept Relation Network (CRN), which is efficiently constructed and maintained using existing web search engines. CRN tackles the uncertainty and dynamics of web content, and thus is optimized for many important web applications, such as social networks and search engines. It is a large semantic network for the collection, analysis, and interpretation of web content, and serves as a cornerstone for applications such as web search engines, recommendation systems, and social networks that can benefit from a large-scale knowledge base. In this article, we present two applications for CRN: (1) search engine and web analytic and (2) semantic information retrieval. Experimental results show that CRN effectively enhances these applications by considering the heterogenous and polysemous nature of web content. © 2016 ACM.",Concept network; Query suggestion; Search engine; Semantic network; Web analytic; Web search,Information retrieval; Knowledge based systems; Recommender systems; Search engines; Semantic Web; Semantics; Social networking (online); Websites; Concept networks; Query suggestion; Semantic network; Web analytic; Web searches; World Wide Web
SCARE: Reputation Estimation for uncertain web services,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964380527&doi=10.1145%2f2792979&partnerID=40&md5=a958bcfc02bad17f259a9f763a9d6ff3,"In this article, we propose a Statistical Cloud-Assisted Reputation Estimation (sCARE) approach for service-oriented environments in uncertain situations. sCARE uses the ratings from cooperating service consumers to uniformly describe the randomness and fuzziness of the different submitted ratings and their associated relationships in quantitative terms. We also define discriminant functions to model the honesty (or lack thereof) of the service raters. Experiment results show that our proposed model performs in a fairly accurate manner for a number of real-world scenarios. A comparison study with similar existing works is also provided to asses sCARE's performance. © 2016 ACM.",Service oriented architecture; Trust; Uncertainty,Information services; Service oriented architecture (SOA); Comparison study; Discriminant functions; Real-world scenario; Reputation estimation; Service consumers; Service-oriented environment; Trust; Uncertainty; Web services
Using an epidemiological approach to maximize data survival in the internet of things,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964328662&doi=10.1145%2f2812810&partnerID=40&md5=ffbb25b59eb72e7464ed785950a58f91,"The Internet of Things (IoT) has gained worldwide attention in recent years. It transforms the everyday objects that surround us into proactive actors of the Internet, generating and consuming information. An important issue related to the appearance of such a large-scale self-coordinating IoT is the reliability and the collaboration between the objects in the presence of environmental hazards. High failure rates lead to significant loss of data. Therefore, data survivability is a main challenge of the IoT. In this article, we have developed a compartmental e-Epidemic SIR (Susceptible-Infectious-Recovered) model to save the data in the network and let it survive after attacks. Furthermore, our model takes into account the dynamic topology of the network where natural death (crashing nodes) and birth are defined and analyzed. Theoretical methods and simulations are employed to solve and simulate the system of equations developed and to analyze the model. © 2016 ACM.",Data survivability and availability; Epidemic models; Internet of things; Security; Wireless sensor networks,Internet; Topology; Wireless sensor networks; Data survivabilities; Dynamic topologies; Environmental hazards; Epidemic models; Internet of thing (IOT); Security; System of equations; Theoretical methods; Internet of things
Towards anomalous diffusion sources detection in a large network,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964322456&doi=10.1145%2f2806889&partnerID=40&md5=bd3f5e12d0941ba8c2f14e6d33271da4,"Witnessing the wide spread of malicious information in large networks, we develop an efficient method to detect anomalous diffusion sources and thus protect networks from security and privacy attacks. To date, most existing work on diffusion sources detection are based on the assumption that network snapshots that reflect information diffusion can be obtained continuously. However, obtaining snapshots of an entire network needs to deploy detectors on all network nodes and thus is very expensive. Alternatively, in this article, we study the diffusion sources locating problem by learning from information diffusion data collected from only a small subset of network nodes. Specifically, we present a new regression learning model that can detect anomalous diffusion sources by jointly solving five challenges, that is, unknown number of source nodes, few activated detectors, unknown initial propagation time, uncertain propagation path and uncertain propagation time delay. We theoretically analyze the strength of the model and derive performance bounds. We empirically test and compare the model using both synthetic and real-world networks to demonstrate its performance. © 2016 ACM.",,Time delay; Anomalous diffusion; Diffusion sources; Information diffusion; Number of sources; Performance bounds; Propagation paths; Real-world networks; Security and privacy; Diffusion
Supervised anomaly detection in uncertain pseudoperiodic data streams,2016,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964661086&doi=10.1145%2f2806890&partnerID=40&md5=df2e1550f3014cb7c2341745013389e4,"Uncertain data streams have been widely generated in many Web applications. The uncertainty in data streams makes anomaly detection from sensor data streams far more challenging. In this article, we present a novel framework that supports anomaly detection in uncertain data streams. The proposed framework adopts the wavelet soft-thresholding method to remove the noises or errors in data streams. Based on the refined data streams, we develop effective period pattern recognition and feature extraction techniques to improve the computational efficiency. We use classification methods for anomaly detection in the corrected data stream. We also empirically show that the proposed approach shows a high accuracy of anomaly detection on several real datasets. © 2016 ACM.",Anomaly detection; Classification; Segmentation; Uncertain data stream,Classification (of information); Computational efficiency; Data communication systems; Feature extraction; Image segmentation; Pattern recognition; Signal detection; Anomaly detection; Classification methods; High-accuracy; Pattern recognition and feature extraction; Real data sets; Uncertain data streams; Wavelet soft thresholding; WEB application; Data mining
VORTEX: Visual Phishing DetectiOns aRe Through EXplanations,2024,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193486512&doi=10.1145%2f3654665&partnerID=40&md5=cf1d2520b805327076597f7d64cef42d,"Phishing attacks reached a record high in 2022, as reported by the Anti-Phishing Work Group, following an upward trend accelerated during the pandemic. Attackers employ increasingly sophisticated tools in their attempts to deceive unaware users into divulging confidential information. Recently, the research community has turned to the utilization of screenshots of legitimate and malicious websites to identify the brands that attackers aim to impersonate. In the field of Computer Vision, convolutional neural networks (CNNs) have been employed to analyze the visual rendering of websites, addressing the problem of phishing detection. However, along with the development of these new models, arose the need to understand their inner workings and the rationale behind each prediction. Answering the question, “How is this website attempting to steal the identity of a well-known brand?” becomes crucial when protecting end-users from such threats. In cybersecurity, the application of explainable AI (XAI) is an emerging approach that aims to answer such questions. In this article, we propose VORTEX, a phishing website detection solution equipped with the capability to explain how a screenshot attempts to impersonate a specific brand. We conduct an extensive analysis of XAI methods for the phishing detection problem and demonstrate that VORTEX provides meaningful explanations regarding the detection results. Additionally, we evaluate the robustness of our model against Adversarial Example attacks. We adapt these attacks to the VORTEX architecture and evaluate their efficacy across multiple models and datasets. Our results show that VORTEX achieves superior accuracy compared to previous models, and learns semantically meaningful patterns to provide actionable explanations about phishing websites. Finally, VORTEX demonstrates an acceptable level of robustness against adversarial example attacks. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Computer Vision; explainable AI; Phishing,Computer crime; Computer vision; Cybersecurity; Vortex flow; Anti-phishing work groups; Confidential information; Explainable AI; Phishing; Phishing attacks; Phishing detections; Phishing websites; Research communities; Screenshots; Upward trend; Websites
Efficient Vertical Federated Unlearning via Fast Retraining,2024,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193904201&doi=10.1145%2f3657290&partnerID=40&md5=f6e614b5acba6ea48d20771a1d13849c,"Vertical federated learning (VFL) revolutionizes privacy-preserved collaboration for small businesses that have distinct but complementary feature sets. However, as the scope of VFL expands, the constant entering and leaving of participants and the subsequent exercise of the “right to be forgotten” pose a great challenge in practice. The question of how to efficiently erase one’s contribution from the shared model remains largely unexplored in the context of VFL. In this article, we introduce a vertical federated unlearning framework, which integrates model checkpointing techniques with a hybrid, first-order optimization technique. The core concept is to reduce backpropagation time and improve convergence/generalization by combining the advantages of the existing optimizers. We provide in-depth theoretical analysis and time complexity to illustrate the effectiveness of the proposed design. We conduct extensive experiments on six public datasets and demonstrate that our method could achieve up to 6.3× speedup compared to the baseline, with negligible influence on the original learning task. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",fast retraining; federated unlearning; Vertical federated learning,Checkpointing techniques; Complementary features; Fast retraining; Features sets; Federated unlearning; First order; Right to be forgotten; Shared model; Small business; Vertical federated learning
EdgeCI: Distributed Workload Assignment and Model Partitioning for CNN Inference on Edge Clusters,2024,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194158159&doi=10.1145%2f3656041&partnerID=40&md5=3c40bf3c9894cb7f26b246b66f4673c6,"Deep learning technology has grown significantly in new application scenarios such as smart cities and driver-less vehicles, but its deployment needs to consume a lot of resources. It is usually difficult to execute inference task solely on resource-constrained Intelligent Internet-of-Things (IoT) devices to meet strictly service delay requirements. CNN-based inference task is usually offloaded to the edge server or cloud. However, it may lead to unstable performance and privacy leaks. To address the above challenges, this article aims to design a low latency distributed inference framework, EdgeCI, which assigns inference tasks to locally idle, connected, and resource-constrained IoT device cluster networks. EdgeCI exploits two key optimization knobs, including: (1) Auction-based Workload Assignment Scheme (AWAS), which achieves the workload balance by assigning each workload partition to the more matching IoT device; (2) Fused-Layer parallelization strategy based on non-recursive Dynamic Programming (DPFL), which is aimed at further minimizing the inference time. We have implemented EdgeCI based on PyTorch and evaluated its performance with VGG-16 and ResNet-34 image recognition models. The experimental results prove that our proposed AWAS and DPFL outperform the typical state-of-the-art solutions. When they are well combined, EdgeCI can improve inference speed by 34.72% to 43.52%. EdgeCI outperforms the state-of-the art approaches on our edge cluster. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",auction algorithm; Edge computing; fused-layer parallelization; high-speed networks; inference acceleration; workload assignment,Combinatorial optimization; Deep learning; Dynamic programming; Edge computing; HIgh speed networks; Image recognition; Inference engines; Auction algorithms; Distributed workloads; Edge computing; Fused-layer parallelization; High-speed Networks; Inference acceleration; Parallelizations; Performance; Workload assignment; Workload model; Internet of things
Market Manipulation of Cryptocurrencies: Evidence from Social Media and Transaction Data,2024,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193472776&doi=10.1145%2f3643812&partnerID=40&md5=480be9fde3b187d2556b80ed860a8a2b,"The cryptocurrency market cap has experienced a great increase in recent years. However, large price fluctuations demonstrate the need for governance structures and identify whether there are market manipulations. In this article, we conduct three analyses—social media data analysis, blockchain data analysis, and price bubble analysis—to investigate whether market manipulation exists on Bitcoin, Ethereum, and Dogecoin platforms. Social media data analysis aims to find the reasons for price fluctuations. Blockchain data analysis is used to find detailed behavior of the manipulators. Price bubble analysis is used to investigate the relation between price fluctuation and manipulators’ behavior. By using the three analyses, we show that market manipulation exists on Bitcoin, Ethereum, and Dogecoin. However, market manipulation of Bitcoin is limited, and for most of Bitcoin’s price fluctuations, we found other explanations. The price for Ethereum is the most sensitive to technical updates. Technical companies/teams usually hype some new concepts (e.g., ICO, DeFi), which causes a price spike. The price of Dogecoin has a high correlation with Elon Musk’s X (formerly known as Twitter) activity, showing that influential individuals have the ability to manipulate its prices. In addition, the poor monetary liquidity of Dogecoin allows some users to manipulate its price. © 2024 Copyright held by the owner/author(s).",Blockchain; cryptocurrencies; empirical study; market manipulation,Bitcoin; Data handling; Ethereum; Information analysis; Manipulators; Social networking (online); Block-chain; Empirical studies; Governance structures; Influential individuals; Market manipulation; Price fluctuation; Price spike; Social media datum; Social transactions; Transaction data; Blockchain
SDN-enabled Quantized LQR for smart traffic light controller to optimize congestion,2024,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185889718&doi=10.1145%2f3641104&partnerID=40&md5=e82ff4d52886dd3d2c90ffb2ff4eb8cb,"Existing intersection management systems, in urban cities, lack in meeting the current requirements of selfconfiguration, lightweight computing, and software-defined control, which are necessarily required for congested road-lane networks. To satisfy these requirements, this work proposes effective, scalable, multi-input and multi-output, and congestion prevention-enabled intersection management system utilizing a softwaredefined control interface that not only regularly monitors the traffic to prevent congestion for minimizing queue length and waiting time but also offers a computationally efficient solution in real-Time. For effective intersection management, a modified linear-quadratic regulator, i.e., Quantized Linear Quadratic Regulator (QLQR), is designed along with Software-defined Networking (SDN)-enabled control interface to maximize throughput and vehicles speed and minimize queue length and waiting time at the intersection. Experimental results prove that the proposed SDN-QLQR improves the comparative performance in the interval of 24.94%-49.07%, 35.78%-68.86%, 36.67%-59.08%, and 29.94%-57.87% for various performance metrics, i.e., average queue length, average waiting time, throughput, and average speed, respectively. © 2024 Association for Computing Machinery. All rights reserved.",Intelligent transportation system (ITS); Smart city; Traffic light controller (TLC); Urban traffic network,Controllers; Intelligent systems; Intelligent vehicle highway systems; Queueing theory; Traffic congestion; Urban transportation; Intelligent transportation system; Intelligent transportation systems; Intersection managements; Linear quadratic; Management systems; Quadratic regulators; Software-defined networkings; Traffic light controller; Urban traffic networks; Smart city
Open set dandelion network for IoT intrusion detection,2024,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185892387&doi=10.1145%2f3639822&partnerID=40&md5=7e13272b884b078cf190ddd408f3e71f,"As Internet of Things devices become widely used in the real-world, it is crucial to protect them from malicious intrusions. However, the data scarcity of IoT limits the applicability of traditional intrusion detection methods, which are highly data-dependent. To address this, in this article, we propose the Open-Set Dandelion Network (OSDN) based on unsupervised heterogeneous domain adaptation in an open-set manner. The OSDN model performs intrusion knowledge transfer from the knowledge-rich source network intrusion domain to facilitate more accurate intrusion detection for the data-scarce target IoT intrusion domain. Under the open-set setting, it can also detect newly-emerged target domain intrusions that are not observed in the source domain. To achieve this, the OSDN model forms the source domain into a dandelion-like feature space in which each intrusion category is compactly grouped and different intrusion categories are separated, i.e., simultaneously emphasising inter-category separability and intra-category compactness. The dandelion-based targetmembership mechanism then forms the target dandelion. Then, the dandelion angular separation mechanism achieves better inter-category separability, and the dandelion embedding alignment mechanism further aligns both dandelions in a finer manner. To promote intra-category compactness, the discriminating sampled dandelion mechanism is used. Assisted by the intrusion classifier trained using both known and generated unknown intrusion knowledge, a semantic dandelion correction mechanism emphasises easily-confused categories and guides better inter-category separability. Holistically, these mechanisms form the OSDN model that effectively performs intrusion knowledge transfer to benefit IoT intrusion detection. Comprehensive experiments on several intrusion datasets verify the effectiveness of the OSDN model, outperforming three state-of-The-Art baseline methods by 16.9%. The contribution of each OSDN constituting component, the stability and the efficiency of the OSDN model are also verified. © 2024 Association for Computing Machinery. All rights reserved.",Dandelion network; Domain adaptation; Internet of things; Intrusion detection; Open-set domain adaptation,Intrusion detection; Knowledge management; Semantics; Dandelion network; Data scarcity; Domain adaptation; Intrusion detection method; Intrusion-Detection; Knowledge transfer; Network models; Open-set domain adaptation; Real-world; Internet of things
DxHash: A Memory-saving Consistent Hashing Algorithm,2024,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186182108&doi=10.1145%2f3631708&partnerID=40&md5=f7a99bee24378335027e60604e5eec8f,"Consistent Hashing (CH) algorithms are widely adopted in networks and distributed systems for their ability to achieve load balancing and minimize disruptions. However, the rise of the Internet of Things (IoT) has introduced new challenges for existing CH algorithms, characterized by high memory usage and update overhead. This article presents DxHash, a novel CH algorithm based on repeated pseudo-random number generation. DxHash offers significant benefits, including a remarkably low memory footprint, high lookup throughput, and minimal update overhead. Additionally, we introduce a weighted variant of DxHash, enabling adaptable weight adjustments to handle heterogeneous load distribution. Through extensive evaluation, we demonstrate that DxHash outperforms AnchorHash, a state-of-the-art CH algorithm, in terms of the reduction of up to 98.4% in memory footprint and comparable performance in lookup and update. © 2024 Association for Computing Machinery. All rights reserved.",consistent hashing; DxHash; load balance; memory saving,Random number generation; Consistent hashing; Consistent Hashing algorithms; Dxhash; In networks; Load-balance; Load-Balancing; Lookups; Memory footprint; Memory savings; Networks and distributed systems; Internet of things
A Novel Cross-Domain Recommendation with Evolution Learning,2024,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186245833&doi=10.1145%2f3639567&partnerID=40&md5=588a642443d54cc67d1f1b179b3532f7,"In this “info-plosion” era, recommendation systems (or recommenders) play a significant role in finding interesting items in the surge of online digital activities and e-commerce. Several techniques have been widely applied for recommendation systems, but the cold-start and sparsity problems remain a major challenge. The cold-start problem occurs when generating recommendations for new users and items without sufficient information. Sparsity refers to the problem of having a large amount of users and items but with few transactions or interactions. In this article, a novel cross-domain recommendation model, Cross-Domain Evolution Learning Recommendation (abbreviated as CD-ELR), is developed to communicate the information from different domains in order to tackle the cold-start and sparsity issues by integrating matrix factorization and recurrent neural network. We introduce an evolutionary concept to describe the preference variation of users over time. Furthermore, several optimization methods are developed for combining the domain features for precision recommendation. Experimental results show that CD-ELR outperforms existing state-of-the-art recommendation baselines. Finally, we conduct experiments on several real-world datasets to demonstrate the practicability of the proposed CD-ELR. © 2024 Association for Computing Machinery. All rights reserved.",Cross-domain recommendation; deep learning; matrix factorization; recommendation system; recurrent neural network,Electronic commerce; Learning systems; Matrix algebra; Matrix factorization; Online systems; Recurrent neural networks; Cold start problems; Cross-domain; Cross-domain recommendations; Deep learning; Digital activities; Domain evolution; E- commerces; Large amounts; Matrix factorizations; Sparsity problems; Recommender systems
ADTO: A trust active detecting-based task offloading scheme in edge computing for internet of things,2024,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185880476&doi=10.1145%2f3640013&partnerID=40&md5=9b3f22c386c1d35b831d7acaa0b22bd3,"In edge computing, Internet of Things devices with weak computing power offload tasks to nearby edge servers for execution, so the task completion time can be reduced and delay-sensitive tasks can be facilitated. However, if the task is offloaded to malicious edge servers, then the system will suffer losses. Therefore, it is significant to identify the trusted edge servers and offload tasks to trusted edge servers, which can improve the performance of edge computing. However, it is still challenging. In this article, a trust Active Detecting-based Task Offloading (ADTO) scheme is proposed to maximize revenue in edge computing. The main innovation points of our work are as follows: (a) The ADTO scheme innovatively proposes a method to actively get trust by trust detection. This method offloads microtasks to edge servers whose trust needs to be identified, and then quickly identifies the trust of edge servers according to the completion of tasks by edge servers. Based on the identification of the trust, tasks can be offloaded to trusted edge servers, to improve the success rate of tasks. (b) Although the trust of edge servers can be identified by our detection, it needs to pay a price. Therefore, to maximize system revenue, searching the most suitable number of trusted edge servers for various conditions is transformed into an optimization problem. Finally, theoretical and experimental analysis shows the effectiveness of the proposed strategy, which can effectively identify the trusted and untrusted edge servers. The task offloading strategy based on trust detection proposed in this article greatly improves the success rate of tasks, compared with the strategy without trust detection, the task success rate is increased by 40.27%, and there is a significant increase in revenue, which fully demonstrates the effectiveness of the strategy. © 2024 Association for Computing Machinery. All rights reserved.",Active detection; Edge computing; Internet of Things; QoS; Task offloading; Trust computing,Computation offloading; Computing power; Delay-sensitive applications; Trusted computing; Active detection; Computing power; Delay sensitive; Edge computing; Edge server; Microtasks; Performance; Task completion time; Task offloading; Trust computing; Internet of things
EtherShield: Time-interval Analysis for Detection of Malicious Behavior on Ethereum,2024,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186230440&doi=10.1145%2f3633514&partnerID=40&md5=5a7c5dc15489cf6138478ea300354708,"Advances in blockchain technology have attracted significant attention across the world. The practical blockchain applications emerging in various domains, ranging from finance, healthcare, and entertainment, have quickly become attractive targets for adversaries. The novelty of the technology coupled with the high degree of anonymity it provides made malicious activities even less visible in the blockchain environment. This made their robust detection challenging. This article presents EtherShield, a novel approach for identifying malicious activity on the Ethereum blockchain. By combining temporal transaction information and contract code characteristics, EtherShield can detect various types of threats and provide insight into the behavior of contracts. The time-interval-based analysis used by EtherShield enables expedited detection, achieving comparable accuracy to other approaches with significantly less data. Our validation analysis, which involved over 15,000 Ethereum accounts, demonstrated that EtherShield can significantly expedite the detection of malicious activity while maintaining high accuracy levels (86.52% accuracy with 1 hour of transaction history data and 91.33% accuracy with 1 year of transaction history data). © 2024 Association for Computing Machinery. All rights reserved.","Blockchain, security","Ethereum; Block-chain; Blockchain, security; History data; Malicious activities; Malicious behavior; Robust detection; Time interval; Time-interval analysis; Transaction history; Transaction information; Blockchain"
Positional Encoding-based Resident Identification in Multi-resident Smart Homes,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186224055&doi=10.1145%2f3631353&partnerID=40&md5=9aad53b52c27da9a2876afde75fc47d0,"We propose a novel resident identification framework to identify residents in a multi-occupant smart environment. The proposed framework employs a feature extraction model based on the concepts of positional encoding. The feature extraction model considers the locations of homes as a graph. We design a novel algorithm to build such graphs from layout maps of smart environments. The Node2Vec algorithm is used to transform the graph into high-dimensional node embeddings. A Long Short-Term Memory model is introduced to predict the identities of residents using temporal sequences of sensor events with the node embeddings. Extensive experiments show that our proposed scheme effectively identifies residents in a multi-occupant environment. Evaluation results on two real-world datasets demonstrate that our proposed approach achieves 94.5% and 87.9% accuracy, respectively. © 2023 Association for Computing Machinery. All rights reserved.",LSTM; Node Embeddings; Node2Vec; Positional Encoding; Resident Identification; Smart Homes,Automation; Embeddings; Encoding (symbols); Extraction; Intelligent buildings; Long short-term memory; Signal encoding; Embeddings; Encodings; Features extraction; LSTM; Node embedding; Node2vec; Positional encoding; Resident identification; Smart environment; Smart homes; Feature extraction
Digital Twin of Intelligent Small Surface Defect Detection with Cyber-manufacturing Systems,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179134163&doi=10.1145%2f3571734&partnerID=40&md5=fa92eb55178915423a7fce393cc0daae,"With the remarkable technological development in cyber-physical systems, industry 4.0 has evolved by use of a significant concept named digital twin (DT). However, it is still difficult to construct a relationship between twin simulation and a real scenario considering dynamic variations, especially when dealing with small surface defect detection tasks with high performance and computation resource requirements. In this article, we aim to construct cyber-manufacturing systems to achieve a DT solution for small surface defect detection task. Focusing on DT-based solution, the proposed system consists of an Edge-Cloud architecture and a surface defect detection algorithm. Considering dynamic characteristics and real-time response requirement, Edge-Cloud architecture is built to achieve smart manufacturing by efficiently collecting, processing, analyzing, and storing data produced by factory. A deep learning-based algorithm is then constructed to detect surface defeats based on multi-modal data, i.e., imaging and depth data. Experiments show the proposed algorithm could achieve high accuracy and recall in small defeat detection task, thus constructing DT in cyber-manufacturing. © 2023 Association for Computing Machinery.",3D point cloud; cyber manufacturing; Defect detection; digital twin,Cloud computing architecture; Deep learning; Embedded systems; Modal analysis; Surface defects; 3D point cloud; Cloud architectures; Cybe manufacturing; Cybe-physical systems; Cyber-physical systems; Defect detection; Detection tasks; Edge clouds; Surface defect detections; Technological development; Memory architecture
Malicious Account Identification in Social Network Platforms,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179134418&doi=10.1145%2f3625097&partnerID=40&md5=748cee8a18b9062f0ddc45e081458a75,"Today, people of all ages are increasingly using Web platforms for social interaction. Consequently, many tasks are being transferred over social networks, like advertisements, political communications, and so on, yielding vast volumes of data disseminated over the network. However, this raises several concerns regarding the truthfulness of such data and the accounts generating them. Malicious users often manipulate data to gain profit. For example, malicious users often create fake accounts and fake followers to increase their popularity and attract more sponsors, followers, and so on, potentially producing several negative implications that impact the whole society. To deal with these issues, it is necessary to increase the capability to properly identify fake accounts and followers. By exploiting automatically extracted data correlations characterizing meaningful patterns of malicious accounts, in this article we propose a new feature engineering strategy to augment the social network account dataset with additional features, aiming to enhance the capability of existing machine learning strategies to discriminate fake accounts. Experimental results produced through several machine learning models on account datasets of both the Twitter and the Instagram platforms highlight the effectiveness of the proposed approach toward the automatic discrimination of fake accounts. The choice of Twitter is mainly due to its strict privacy laws, and because its the only social network platform making data of their accounts publicly available.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Data management; data profiling; fake accounts; social networks,Fake detection; Machine learning; Social networking (online); User profile; Data correlations; Data profiling; Fake account; Feature engineerings; Machine-learning; Network likes; Network platforms; Political communication; Social interactions; Social network; Information management
Collaborative Hotspot Data Collection with Drones and 5G Edge Computing in Smart City,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179132339&doi=10.1145%2f3617373&partnerID=40&md5=61657d03336b86742cd720596e2e760b,"The construction and governance of smart cities require the collaboration of different systems and different regions. How to realize the monitoring of abnormal hot spots through the collaboration of subsystems with limited resources is related to the stability and efficiency of the city. This work constructs a hot data processing framework for drones and 5G edge computing infrastructure, as well as an Ensemble Multi-Objective Cooperative Learning method to process three different types of hot data. The data collection phase combines set operations with the 0-1 multi-knapsack model, and the cooperative learning phase realizes the degree of cooperation control while retaining the ability of independent optimization of the subsystem. Finally, the advantages of the framework are verified by hot data coverage and collaborative processing efficiency, resource use cost, and balance.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",5G edge computing; cooperative learning; ensemble multi-objective optimization; heat matrix; Smart city,5G mobile communication systems; Data acquisition; Data handling; Drones; Edge computing; Efficiency; Learning systems; Multiobjective optimization; 5g edge computing; Computing infrastructures; Cooperative learning; Data collection; Edge computing; Ensemble multi-objective optimization; Heat matrix; Hotspots; matrix; Multi-objectives optimization; Smart city
"Special Section on Advances in Cyber-Manufacturing: Architectures, Challenges, and Future Research Directions",2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179728199&doi=10.1145%2f3627990&partnerID=40&md5=e1e780b3c9a246876a12f1ce51dbff42,[No abstract available],,
Exploring the Potential of Cyber Manufacturing System in the Digital Age,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179133656&doi=10.1145%2f3596602&partnerID=40&md5=6acba49019200cbc111518556afd4f9d,"Cyber-manufacturing Systems (CMS) have been growing in popularity, transitioning from conventional manufacturing to an innovative paradigm that emphasizes innovation, automation, better customer service, and intelligent systems. A new manufacturing model can improve efficiency and productivity, and provide better customer service and response times. In addition, it may revolutionize the way products are produced, from design to completion. Therefore, it is likely that this new manufacturing model will become increasingly popular. By building new technologies on top of existing CMS, these systems will ensure that data exchange and integration between decentralized systems are reliable and secure. Recently published case studies from industry and the literature support this claim; some challenges remain to be overcome. In general, the use of CMS can revolutionize the manufacturing industry. This study comprehensively analyzes these systems and their potential applications and implications. An overview of the field is then given and various aspects of CMS are also explored with more details. A taxonomy of the most common and current approaches to CMS is presented, including networked cyber-manufacturing systems, distributed cyber-manufacturing systems, cloud-based cyber-manufacturing systems, and cyber-physical systems (CPS). Furthermore, our survey identifies several popular open-source software and datasets and discusses how these resources can reduce barriers to CMS research. In addition, we identify several important issues and research opportunities associated with CMS, including better integration between hardware and software, improved security and privacy protocols, communication protocols, and improved data management systems. In summary, this paper presents a comprehensive overview of current technology and valuable insights are provided for the potential impact of CMS on society and industry.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",automated production processes; cyber manufacturing systems; cybersecurity; Industrial automation,Automation; Cyber Physical System; Cybersecurity; Electronic data interchange; Embedded systems; Industrial research; Information management; Intelligent systems; Open source software; Product design; Automated production process; Automated productions; Conventional manufacturing; Customer-service; Cybe manufacturing system; Cyber security; Digital age; Industrial automation; Manufacturing models; Production process; Open systems
Unpaired Self-supervised Learning for Industrial Cyber-Manufacturing Spectrum Blind Deconvolution,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179122736&doi=10.1145%2f3590963&partnerID=40&md5=9e0ea1b739ba5a1d5b8838112614834c,"Cyber-Manufacturing combines industrial big data with intelligent analysis to find and understand the intangible problems in decision-making, which requires a systematic method to deal with rich signal data. With the development of spectral detection and photoelectric imaging technology, spectral blind deconvolution has achieved remarkable results. However, spectral processing is limited by one-dimensional signal, and there is no available structural information with few training samples. Moreover, in the majority of practical applications, it is entirely feasible to gather unpaired spectrum dataset for training. This training method of unpaired learning is practical and valuable. Therefore, a two-stage deconvolution scheme combining self supervised learning and feature extraction is proposed in this paper, which generates two complementary paired sets through self supervised learning to extract the final deconvolution network. In addition, a new deconvolution network is designed for feature extraction. The spectrum is pre-trained through spectral feature extraction and noise estimation network to improve the training efficiency and meet the assumed noise characteristics. Experimental results show that this method is effective in dealing with different types of synthetic noise. © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Cyber-Manufacturing; spectral blind deconvolution; two stage training network; unpaired learning,Convolution; Decision making; Extraction; Blind deconvolution; Cybe-manufacturing; Deconvolutions; Features extraction; Intelligent analysis; Spectra's; Spectral blind deconvolution; Training network; Two stage training network; Unpaired learning; Feature extraction
Providing Realtime Support for Containerized Edge Services,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179131667&doi=10.1145%2f3617123&partnerID=40&md5=2b81e5954592f94c38d80b5f4766c3a8,"Containers have emerged as a popular technology for edge computing platforms. Although there are varieties of container orchestration frameworks, e.g., Kubernetes to provide high-reliable services for cloud infrastructure, providing real-time support at the containerized edge systems (CESs) remains a challenge. In this paper, we propose EdgeMan, a holistic edge service management framework for CESs, which consists of (1) a model-assisted event-driven lightweight online scheduling algorithm to provide request-level execution plans; (2) a bottleneck-metric-aware progressive resource allocation mechanism to improve resource efficiency. We then build a testbed that installed three containerized services with different latency sensitivities for concrete evaluation. Additionally, we adopt real-world data traces from Alibaba and Twitter for large-scale emulations. Extensive experiments demonstrate that the deadline miss ratio of time-sensitive services run with EdgeMan is reduced by 85.9% on average compared with that of existing methods in both industry and academia. © 2023 held by the owner/author(s). Publication rights licensed to ACM.",cloud-native; containerized edge service; Realtime support,Real time systems; Scheduling algorithms; Cloud infrastructures; Cloud-native; Computing platform; Containerized edge service; Edge computing; Edge services; High reliable; Real- time; Realtime support; Service management; Containers
Offering Two-way Privacy for Evolved Purchase Inquiries,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174747564&doi=10.1145%2f3599968&partnerID=40&md5=2ce82b51dd824f476dc8579d3eb815b2,"Dynamic and flexible business relationships are expected to become more important in the future to accommodate specialized change requests or small-batch production. Today, buyers and sellers must disclose sensitive information on products upfront before the actual manufacturing. However, without a trust relation, this situation is precarious for the involved companies as they fear for their competitiveness. Related work overlooks this issue so far: existing approaches protect the information of a single party only, hindering dynamic and on-demand business relationships. To account for the corresponding research gap of inadequately privacy-protected information and to deal with companies without an established trust relation, we pursue the direction of innovative privacy-preserving purchase inquiries that seamlessly integrate into today's established supplier management and procurement processes. Utilizing well-established building blocks from private computing, such as private set intersection and homomorphic encryption, we propose two designs with slightly different privacy and performance implications to securely realize purchase inquiries over the Internet. In particular, we allow buyers to consider more potential sellers without sharing sensitive information and relieve sellers of the burden of repeatedly preparing elaborate yet discarded offers. We demonstrate our approaches' scalability using two real-world use cases from the domain of production technology. Overall, we present deployable designs that offer two-way privacy for purchase inquiries and, in turn, fill a gap that currently hinders establishing dynamic and flexible business relationships. In the future, we expect significantly increasing research activity in this overlooked area to address the needs of an evolving production landscape. © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Bootstrapping procurement; homomorphic encryption; Internet of Production; private set intersection; secure industrial collaboration,Privacy-preserving techniques; Bootstrapping procurement; Business relationships; Ho-momorphic encryptions; Homomorphic-encryptions; Industrial collaboration; Internet of production; Private set intersection; Secure industrial collaboration; Set intersection; Two ways; Sales
UNION: Fault-tolerant Cooperative Computing in Opportunistic Mobile Edge Cloud,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179132055&doi=10.1145%2f3617994&partnerID=40&md5=2fbabfcdca28319c640cae049e16131f,"Opportunistic Mobile Edge Cloud in which opportunistically connected mobile devices run in a cooperative way to augment the capability of a single device has become a timely and essential topic due to its widespread prospect under resource-constrained scenarios (e.g., disaster rescue). Because of the mobility of devices and the uncertainty of environments, it is inevitable that failures occur among the mobile nodes. Being different from existing studies that mainly focus on either data offloading or computing offloading among mobile devices in an ideal environment, we concentrate on how to guarantee the reliability of the task execution with the consideration of both data offloading and computing offloading under opportunistically connected mobile edge cloud. To this end, an optimization of mobile task offloading when considering reliability is formulated. Then, we propose a probabilistic model for task offloading and a reliability model for task execution, which estimates the probability of successful execution for a specific opportunistic path and describes the dynamic reliability of the task execution. Based on these models, a heuristic algorithm UNION (Fault-Tolerant Cooperative Computing) is proposed to solve this NP-hard problem. Theoretical analysis shows that the complexity of UNION is (|ĝ.|2+||) with guaranteeing the reliability of 0.99. Also, extensive experiments on real-world traces validate the superiority of the proposed algorithm UNION over existing typical strategies. © 2023 held by the owner/author(s). Publication rights licensed to ACM.",cooperative computing; fault tolerance; heuristic algorithm; Opportunistic Mobile Edge Cloud; task offloading,Computational complexity; Fault tolerance; Reliability analysis; Cooperative computing; Disaster rescue; Edge clouds; Fault-tolerant; Heuristics algorithm; Mobile nodes; Opportunistic mobile edge cloud; Task executions; Task offloading; Uncertainty; Heuristic algorithms
Polarized Communities Search via Co-guided Random Walk in Attributed Signed Networks,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179131362&doi=10.1145%2f3613449&partnerID=40&md5=2c34697a57d32dc23934cd32a9996261,"Polarized communities search aims at locating query-dependent communities, in which mostly nodes within each community form intensive positive connections, while mostly nodes across two communities are connected by negative links. Current approaches towards polarized communities search typically model the network topology, while the key factor of node, i.e., the attributes, are largely ignored. Existing studies have shown that community formation is strongly influenced by node attributes and the formation of communities are determined by both network topology and node attributes simultaneously. However, it is nontrivial to incorporate node attributes for polarized communities search. Firstly, it is hard to handle the heterogeneous information from node attributes. Secondly, it is difficult to model the complex relations between network topology and node attributes in identifying polarized communities. To address the above challenges, we propose a novel method Co-guided Random Walk in Attributed signed networks (CoRWA) for polarized communities search by equipping with reasonable attribute setting. For the first challenge, we devise an attribute-based signed network to model the auxiliary relation between nodes and a weight assignment mechanism is designed to measure the reliability of the edges in the signed network. As to the second challenge, a co-guided random walk scheme in two signed networks is designed to explicitly model the relations between topology-based signed network and attribute-based signed network so as to enhance the search result of each other. Finally, we can identify polarized communities by a well-designed Rayleigh quotient in the signed network. Extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed CoRWA. Further analysis reveals the significance of node attributes for polarized communities search. © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Attributed signed network; Co-guided random walk; Polarized communities search; Reliability,Random processes; Attribute-based; Attributed signed network; Co-guided random walk; Community IS; Network node; Network topology; Node attribute; Polarized community search; Random Walk; Signed networks; Network topology
Tolerance Analysis of Cyber-Manufacturing Systems to Cascading Failures,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152709784&doi=10.1145%2f3579847&partnerID=40&md5=b8419bf5036ced08438fd4956253547f,"In practical cyber-manufacturing systems (CMS), the node component is the forwarder of information and the provider of services. This dual role makes the whole system have the typical physical-services interaction characteristic, making CMS more vulnerable to cascading failures than general manufacturing systems. In this work, in order to reasonably characterize the cascading process of CMS, we first develop an interdependent network model for CMS from a physical-service networking perspective. On this basis, a realistic cascading failure model for CMS is designed with full consideration of the routing-oriented load distribution characteristics of the physical network and selective load distribution characteristics of the service network. Through extensive experiments, the soundness of the proposed model has been verified and some meaningful findings have been obtained: (1) attacks on the physical network are more likely to trigger cascading failures and may cause more damage; (2) interdependency failures are the main cause of performance degradation in the service network during cascading failures; and (3) isolation failures are the main cause of performance degradation in the physical network during cascading failures. The obtained results can certainly help users to design a more reliable CMS against cascading failures. © 2023 held by the owner/author(s). Publication rights licensed to ACM.",cascading failures; Cyber-manufacturing; load distribution; network model,Damage tolerance; Electric power plant loads; Failure (mechanical); Cascading failures; Cybe-manufacturing; Distribution characteristics; Dual role; Load distributions; Network models; Performance degradation; Physical network; Services network; Tolerance analysis; Fits and tolerances
Securing Low-Power Blockchain-enabled IoT Devices against Energy Depletion Attack,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170576956&doi=10.1145%2f3511903&partnerID=40&md5=97f51fc78b0f376066dcf86dcc247ef9,"Blockchain-enabled Internet of Things (IoT) envisions a world with rapid development and implementations to change our everyday lives based on smart devices. These devices are attached to the internet that can communicate with each other without human interference. A well-known wireless network in blockchain-enabled IoT frameworks is the Low Power and Lossy Network (LLN) that uses a novel protocol known as Routing protocol for low power and lossy networks (RPL) to provide effective and energy-efficient routing. LLNs that run on RPL are inherently prone to multiple Denial of Service (DoS) attacks due to the low cost, shared medium, and resource-constrained nature of blockchain-enabled IoT devices. A Spam DODAG Information Solicitation (DIS) attack is one of the novel attacks that drains the energy source of legitimate nodes and ends up causing the legitimate nodes to suffer from DoS. To address this problem, a mitigation scheme named DIS Spam Attack Mitigation (DISAM) is proposed. The proposed scheme effectively mitigates the effects of the Spam DIS attack on the network's performance. The experimental results show that DISAM detects and mitigates the attack quickly and efficiently.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesInternet of Things; battery drainage attack; block chain; Denial of Service; DIS attack,Denial-of-service attack; Electric batteries; Energy efficiency; Internet of things; Low power electronics; Network security; Power management (telecommunication); Additional key word and phrasesinternet of thing; Battery drainage; Battery drainage attack; Block-chain; Denial of Service; DODAG information solicitation attack; Key words; Lossy networks; Low Power; Low Power Networks; Blockchain
S-BDS: An Effective Blockchain-based Data Storage Scheme in Zero-Trust IoT,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170569519&doi=10.1145%2f3511902&partnerID=40&md5=7e3fa1a1f5ea52d263684f9f04f56b83,"With the development of the Internet of Things (IoT), a large-scale, heterogeneous, and dynamic distributed network has been formed among IoT devices. There is an extreme need to establish a trust mechanism between devices, and blockchain can provide a zero-Trust security framework for IoT. However, the efficiency of the blockchain is far from meeting the application requirements of the IoT, which has become the biggest resistance to the application of the blockchain in the IoT. Therefore, this paper combines sharding to build an effective Blockchain-based IoT data storage scheme (S-BDS). Sharding can solve the problem of blockchain capacity and scalability. While the blockchain provides data immutability and traceability for the IoT, it also brings huge demands for data credibility verification. The communication delay in the IoT system seriously affects the security of the system, while the Merkle proof of traditional blockchain occupies a lot of communication resources. This paper constructs Insertable Vector Commitment (IVC) in the bilinear group and replaces the Merkle tree with IVC to store IoT data in the blockchain. The construct has small-sized proof. It also has the ability to record the number of updates, which can prevent replay-Attacks. Experiments show that each block processes 1,000 transactions, the proof size of a single data piece is 30% of the original scheme, and proofs from different shards can be aggregated. IVC can effectively reduce communication congestion and improve the stability and security of the IoT system.  © 2023 Association for Computing Machinery.",Blockchain; cryptographic commitment; date storage; Internet of Things; zero-Trust,Digital storage; Internet of things; Trees (mathematics); Block-chain; Cryptographic commitment; CryptoGraphics; Data storage; Date storage; Dynamic distributed networks; Large-scale dynamics; Storage schemes; Trust mechanism; Zero-trust; Blockchain
Privacy-Aware Traffic Flow Prediction Based on Multi-Party Sensor Data with Zero Trust in Smart City,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170546865&doi=10.1145%2f3511904&partnerID=40&md5=d8e130add920a20ceeee26654e739c85,"With the continuous increment of city volume and size, a number of traffic-related urban units (e.g., vehicles, roads, buildings, etc.) are emerging rapidly, which plays a heavy burden on the scientific traffic control of smart cities. In this situation, it is becoming a necessity to utilize the sensor data from massive cameras deployed at city crossings for accurate traffic flow prediction. However, the traffic sensor data are often distributed and stored by different organizations or parties with zero trust, which impedes the multi-party sensor data sharing significantly due to privacy concerns. Therefore, it requires challenging efforts to balance the trade-off between data sharing and data privacy to enable cross-organization traffic data fusion and prediction. In light of this challenge, we put forward an accurate LSH (locality-sensitive hashing)-based traffic flow prediction approach with the ability to protect privacy. Finally, through a series of experiments deployed on a real-world traffic dataset, we demonstrate the feasibility of our proposal in terms of prediction accuracy and efficiency while guaranteeing sensor data privacy.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTraffic flow prediction; locality-sensitive hashing; multi-party sensors; privacy; smart city; zero trust,Data fusion; Economic and social effects; Forecasting; Privacy-preserving techniques; Additional key word and phrasestraffic flow prediction; Data Sharing; Flow prediction; Key words; Locality sensitive hashing; Multi-party sensor; Privacy; Sensors data; Traffic flow prediction; Zero trust; Smart city
BACKM-EHA: A Novel Blockchain-enabled Security Solution for IoMT-based E-healthcare Applications,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170563265&doi=10.1145%2f3511898&partnerID=40&md5=1189e77d4cc4c7ffe08dcf73081ddbca,"E-health is the use of information and communication technology (ICT) for the healthcare-related services. It uses various types of digital technologies and telecommunications, such as computers, sensing devices, Internet, and mobile devices to deliver medical services. Internet of Medical Things (IoMT) is a communication environment optimized for low-power devices (for example, health sensors and actuators) and operation on, in, or around the human body (i.e., a patient). It can be used in various applications that are related to healthcare, such as ""body automation,""""healthcare,""""medical monitoring,""""body interaction,""and ""medical implants (i.e., pacemaker).""Most of the communications happen in IoMT-based e-healthcare system are wireless in nature. This may cause severe threats to the security of the system. Various information security-related attacks, i.e., replay, man-in-The-middle attack (MiTM), impersonation, privileged insider, unauthorised session key computation, credentials leakage, stolen verifier, malware injection are possible in IoMT-based e-healthcare system. These threats and attacks can create serious problems in the social life of an individual, as this may reveal their confidential healthcare information to other unauthorised parties. Therefore, it is essential to propose an access control and key management scheme to secure the communication of a IoMT-based e-healthcare system. Moreover, the security of such kind of scheme can also be enhanced through the deployment of a blockchain mechanism. Therefore, in this article, we propose a blockchain-enabled access control and key management protocol for IoMT-based e-healthcare system that is named as ""BACKM-EHA""in short. The security analysis of proposed BACKM-EHA is also provided through the standard, i.e., ""Real-Or-Random model.""The various conducted security analyses prove the security of BACKM-EHA against the different types of potential attacks. The performance of BACKM-EHA is better than the other existing schemes, as it requires less communication cost, computation cost, and provides more ""security and functionality features."" © 2023 Association for Computing Machinery.",access control; Additional Key Words and PhrasesE-healthcare; blockchain; internet of medical things (IoMT); key agreement; security; simulation,Blockchain; Digital devices; Information use; Internet of things; Malware; Medical information systems; mHealth; Mobile security; Mobile telecommunication systems; Network security; Security systems; Access control managements; Additional key word and phrasese-healthcare; Block-chain; E-healthcare; Healthcare systems; Internet of medical thing; Key agreement; Key words; Security; Simulation; Access control
Designing and Developing a Weed Detection Model for California Thistle,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170532989&doi=10.1145%2f3544491&partnerID=40&md5=7f0f474f1b87b0715dbea09a86747857,"With a great percentage of farms in New Zealand as pastures, they are mainly important in contributing to the milk and meat industries. Pasture quality is highly affected by weeds. Weeds grow fast and invade pastures by seed pollination. They consume the nutrients, water, and other minerals, and once they are bitter, cattle do not eat them. Therefore, dairy farmers have to allocate a significant portion of their budget and time to monitor and clean weeds. Unfortunately, most weed management tasks are manual with no consistent technology. Thus, the motivation behind this article was to design an object detection model for weed monitoring and control in pastures. The model was designed and tested on California thistle, a dominant and widespread weed on New Zealand pastures. Our study is one of the major model designs for identifying weeds in an in-pasture environment, one of the most complicated environments for any object detection model. A synthetic methodology was used to create three types of datasets: plant-based, leaf-based, and mixed. The trained model based on the leaf-based dataset is one of the major contributions of our work and has not been conducted by any other weed detection models. After models had been trained, tuning experimentation was undertaken to improve the model's performance. This involved studying the model's hyperparameters in various ranges and then recording their values at the optimum points. The improved model showed a 93% mAP accuracy in the detection of training images and over 95% accuracy for testing images. The experimentation showed that the leaf-based model was slightly better than other models. The model can automate highly any weed management system. The use of this model will save farmers time and money and help them reduce the errors of manual work.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesImage processing; IoT; MaskRCNN; object detection; weed detection,Agricultural robots; Budget control; Farms; Image enhancement; Internet of things; Object recognition; Weed control; Additional key word and phrasesimage processing; California; Detection models; IoT; Key words; MaskRCNN; New zealand; Objects detection; Weed detection; Weed management; Object detection
DisguisedNets: Secure Image Outsourcing for Confidential Model Training in Clouds,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170571139&doi=10.1145%2f3609506&partnerID=40&md5=50cdac3c8df90a16014411443e073cbe,"Large training data and expensive model tweaking are standard features of deep learning with images. As a result, data owners often utilize cloud resources to develop large-scale complex models, which also raises privacy concerns. Existing cryptographic solutions for training deep neural networks (DNNs) are too expensive, cannot effectively utilize cloud GPU resources, and also put a significant burden on client-side pre-processing. This article presents an image disguising approach: DisguisedNets, which allows users to securely outsource images to the cloud and enables confidential, efficient GPU-based model training. DisguisedNets uses a novel combination of image blocktization, block-level random permutation, and block-level secure transformations: random multidimensional projection (RMT) or AES pixel-level encryption (AES) to transform training data. Users can use existing DNN training methods and GPU resources without any modification to training models with disguised images. We have analyzed and evaluated the methods under a multi-level threat model and compared them with another similar method-InstaHide. We also show that the image disguising approach, including both DisguisedNets and InstaHide, can effectively protect models from model-Targeted attacks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesOutsourced deep learning; confidential computing; image disguising,Cryptography; Graphics processing unit; Additional key word and phrasesoutsourced deep learning; Complex model; Confidential computing; Expensive models; Image disguising; Key words; Large-scales; Model training; Privacy concerns; Training data; Deep neural networks
I-DarkVec: Incremental Embeddings for Darknet Traffic Analysis,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166323066&doi=10.1145%2f3595378&partnerID=40&md5=6e2ed5940aff6b1a8cfb39a63e56a59f,"Darknets are probes listening to traffic reaching IP addresses that host no services. Traffic reaching a darknet results from the actions of internet scanners, botnets, and possibly misconfigured hosts. Such peculiar nature of the darknet traffic makes darknets a valuable instrument to discover malicious online activities, e.g., identifying coordinated actions performed by bots or scanners. However, the massive amount of packets and sources that darknets observe makes it hard to extract meaningful insights, calling for scalable tools to automatically identify and group sources that share similar behaviour.We here present i-DarkVec, a methodology to learn meaningful representations of Darknet traffic. i-DarkVec leverages Natural Language Processing techniques (e.g., Word2Vec) to capture the co-occurrence patterns that emerge when scanners or bots launch coordinated actions. As in NLP problems, the embeddings learned with i-DarkVec enable several new machine learning tasks on the darknet traffic, such as identifying clusters of senders engaged in similar activities.We extensively test i-DarkVec and explore its design space in a case study using real darknets. We show that with a proper definition of services, the learned embeddings can be used to (i) solve the classification problem to associate unknown sources' IP addresses to the correct classes of coordinated actors and (ii) automatically identify clusters of previously unknown sources performing similar attacks and scans, easing the security analyst's job. i-DarkVec leverages a novel incremental embedding learning approach that is scalable and robust to traffic changes, making it applicable to dynamic and large-scale scenarios.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesWord2Vec; darknet; Network Measurements,Embeddings; Internet protocols; Natural language processing systems; Additional key word and phrasesword2vec; Botnets; Coordinated actions; Darknets; Embeddings; Key words; Learn+; Network measurement; Online activities; Traffic analysis; Botnet
Three-Tier Storage Framework Based on TBchain and IPFS for Protecting IoT Security and Privacy,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168805477&doi=10.1145%2f3549910&partnerID=40&md5=cebde0216140f8ef72b1808a13d2743b,"Recently, most of the Internet of things (IoT) infrastructures are highly centralized with single points of failure, which results in serious security and privacy issues of IoT data. Fortunately, blockchain technique can provide a decentralized and secure IoT framework to deal with security issues based on the characteristics of decentralization, non-Tampering, openness, transparency, and traceability. However, the blockchain consensus protocol guarantees the safety and reliability of data, but it also brings problems such as scalability limitations and poor storage extensibility, resulting in the inability to directly integrate blockchain and the IoT in existing conditions. In this article, a private three-Tier local blockchain, Three-Tier architecture Blockchain (TBchain), is proposed to solve the problem by splitting part of the transactions in the public blockchain and locking them in a higher-level blockchain TBchain. Additionally, the private blockchain TBchain is connected to the public blockchain to build a hierarchical blockchain network to provide privacy protection for the IoT data stored on the blockchain. Finally, we implement an IoT framework based on TBchain and the InterPlanetary File System (IPFS) to realize the decentralized IoT, which guarantees the user's access control right to personal data. Experimental results show that the IoT framework based on TBchain and IPFS realizes the user's access control right to personal data by verifying in advance to ensure the confidentiality and security of shared data, and improves the security and privacy of IoT data and transactions. Moreover, we prove that the scalability and storage extensibility of the blockchain is positively correlated with the number of data blocks in TBchain.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesIoT; blockchain; privacy; scalability; security; storage extensibility,Access control; Blockchain; Client server computer systems; Cloud storage; Data privacy; Internet of things; Additional key word and phrasesiot; Block-chain; Decentralised; Filesystem; Key words; Privacy; Security; Security and privacy; Storage extensibility; User access control; Scalability
A Blockchain-Based Access Control Scheme for Zero Trust Cross-Organizational Data Sharing,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170571502&doi=10.1145%2f3511899&partnerID=40&md5=32439bf7c7f6f2bfdd92b77d0ae040fb,"Multi-organization data sharing is becoming increasingly prevalent due to the interconnectivity of systems and the need for collaboration across organizations (e.g., exchange of data in a supply chain involving multiple upstream and downstream vendors). There are, however, data security concerns due to lack of trust between organizations that may be located in jurisdictions with varying security and privacy legislation and culture (also referred to as a zero trust environment). Hence, in such a zero trust setting, one should introduce strengthened, yet efficient, access control mechanisms to facilitate cross-organizational data access and exchange requests. Contemporary access control schemes generally focus on protecting a single objective rather than multiple parties, due to higher security costs. In this article, we propose a blockchain-based access control scheme, designed to facilitate lightweight data sharing among different organizations. Specifically, our approach utilizes the consortium blockchain to establish a trustworthy environment, in which a Role-Based Access Control (RBAC) model is then deployed using our proposed multi-signature protocol and smart contract methods. Evaluation of our proposed approach is performed on the HyperLedger Fabric consortium blockchain platform using both Caliper and BFT-SMaRT benchmarks, and the findings demonstrate the utility of our approach.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",access control; Additional Key Words and PhrasesConsortium blockchain; multi-organizational data sharing; multi-signature; zero trust,Access control; Data Sharing; Distributed ledger; Supply chains; Access control schemes; Additional key word and phrasesconsortium blockchain; Block-chain; Cross-organizational; Data Sharing; Key words; Multi-organizational data sharing; Multi-signature; Organisational; Zero trust; Blockchain
A Highly Compatible Verification Framework with Minimal Upgrades to Secure an Existing Edge Network,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162826111&doi=10.1145%2f3511901&partnerID=40&md5=f31535945379817894342d0d0a043884,"Edge networks are providing services for an increasing number of companies, and they can be used for communication between edge devices and edge gateways. However, the performance of edge devices varies greatly, and it is not easy to upgrade low-performance edge devices. Therefore, cyber attackers can use the vulnerability of edge devices to implement advanced persistent threat attacks. This article proposes a network verification framework for edge networks that can minimize the upgrades needed to strengthen edge network security. First, the communication parties use the data transmitted by the given edge network. Our method uses our proposed PacketVerifier to attach verification information to the packet after it is sent and to verify and restore the packet before it reaches the receiver. Second, due to the performance requirements of edge networks, we design a new data processing structure, namely, a sliding window double ring, to improve the performance of strict sequential protocols in parallel validation. Finally, experimental simulations show that our parallel processing algorithm has good performance in terms of network bandwidth compared with two existing packet processing algorithms. Furthermore, the proposed packet with verification information is compatible with the existing network topology, which helps PacketVerifier establish trustworthy transmission in a zero-Trust environment.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesZero trust; compatibility; internet of things; parallel verification; verification framework,Cybersecurity; Network security; Network topology; Additional key word and phraseszero trust; Compatibility; Cyber attackers; EDGE Networks; Key words; Networks security; Parallel verification; Performance; Processing algorithms; Verification framework; Internet of things
V-Gas: Generating High Gas Consumption Inputs to Avoid Out-of-Gas Vulnerability,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170517018&doi=10.1145%2f3511900&partnerID=40&md5=8327421eab9a44f1ac15567b0f542123,"Out-of-gas errors occur when smart contract programs are provided with inputs that cause excessive gas consumption and which will be easily exploited to perform Denial-of-Service attacks. Various approaches have been proposed to estimate the gas limit of a function in smart contracts to avoid such error. However, underestimation often occurs when the contract is complex In this work, we propose V-Gas, which automatically generates inputs that maximize the gas cost and reduce underestimation. V-Gas is designed based on static analysis and feedback-directed mutational fuzz testing. First, V-Gas builds the gas weighted control flow graph of functions in smart contracts. Then, V-Gas develops gas consumption guided selection and mutation strategies to generate the input that maximize the gas consumption.For evaluation, we implement V-Gas based on js-evm, a widely used Ethereum virtual machine written in Javascript, and conduct experiments on 736 real-world transactions recorded on Ethereum. A total of 44.02% of the transactions would have out-of-gas errors based on the estimation results given by solc, meaning that the recorded real gas consumption for those transactions is larger than the gas limit estimated by solc. In comparison, V-Gas could reduce the underestimation ratio to 13.86%. To evaluate the performance of feedback-directed engine in V-Gas, we implemented other directed fuzzing engines and compared their performance with that of V-Gas. The results showed that V-Gas generates the same or higher gas estimation value on 97.8% of the transactions with less time, usually within 5 minutes. Furthermore, V-Gas has exposed 25 previously unknown out-of-gas vulnerabilities in widely used smart contracts, 6 of which have been assigned unique CVE identifiers in the U.S. National Vulnerability Database.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSmart contracts; Ethereum; fuzzing; gas estimation,Engines; Errors; Ethereum; Feedback; Flow graphs; Function evaluation; Smart contract; Static analysis; Additional key word and phrasessmart contract; Contract projects; Denialof- service attacks; Feedback directed; Fuzzing; Gas consumption; Gas cost; Gas estimation; Key words; Performance; Gases
Blockchain-based Zero Trust Cybersecurity in the Internet of Things,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170513838&doi=10.1145%2f3594535&partnerID=40&md5=5c533c0f3587cb50cd770fd874df830b,[No abstract available],,
An End-To-end Trust Management Framework for Crowdsourced IoT Services,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170556889&doi=10.1145%2f3600232&partnerID=40&md5=14083b0fde793c001a57c6bcb1597b0a,"We propose a novel end-To-end trust management framework for crowdsourced Internet of Things (IoT) services. The framework targets three main aspects: Trust assessment, trust information credibility and accuracy, and trust information storage. We harness the usage patterns of IoT consumers to offer a trust assessment that adapts to IoT consumers' uses. Additionally, our framework ascertains the credibility and accuracy of trust-related information before trust assessment. This is achieved by validating the data collected by IoT consumers and providers. In addition, our framework ensures the contextual fairness between IoT services and trust information. Moreover, we propose a blockchain-based trust information storage approach. Our proposed storage solution preserves the integrity and availability of trust information.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTrust information; crowdsourcing; Internet of Things; IoT services; trust,Crowdsourcing; Digital storage; Web services; Additional key word and phrasestrust information; End to end; Information accuracy; Information credibilities; Internet of thing service; Key words; Trust; Trust assessments; Trust management frameworks; Usage patterns; Internet of things
Securing Scalable Real-time Multiparty Communications with Hybrid Information-centric Networking,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161697985&doi=10.1145%2f3593585&partnerID=40&md5=8ac9fca58fe67bd2565b590d83700ff3,"In this article, we consider security aspects of online meeting applications based on protocols such as WebRTC that leverage the Information-centric Networking (ICN) architecture to make the system fundamentally more scalable. If the scalability properties provided by ICN have been proved in recent literature, the security challenges and implications for real-time applications have not been reviewed. We show that this class of applications can benefit from strong security and scalability jointly without any major tradeoff and with significant performance improvements over traditional WebRTC systems. To achieve this goal, some modifications to the current ICN architecture must be implemented in the way integrity and authentication are verified. Extensive performance analysis of the architecture based on the open source implementation of Hybrid-ICN proves that real-time applications can greatly benefit from this novel network architecture in terms of strong security and scalable communications.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Information-centric networking; multiparty communications; real-time multiparty communications; secure transport,Network architecture; Network security; Information-centric networkings; Multi-party communication; Networking architecture; Real- time; Real-time application; Real-time multiparty communication; Secure transport; Security aspects; Strong securities; Scalability
Personalized Individual Semantics Learning to Support a Large-Scale Linguistic Consensus Process,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148630584&doi=10.1145%2f3533432&partnerID=40&md5=d73903cac02526ac69ac80c5128f070c,"When making decisions, individuals often express their preferences linguistically. The computing with words methodology is a key basis for supporting linguistic decision making, and the words in that methodology may mean different things to different individuals. Thus, in this article, we propose a continual personalized individual semantics learning model to support a consensus-reaching process in large-scale linguistic group decision making. Specifically, we first derive personalized numerical scales from the data of linguistic preference relations. We then perform a clustering ensemble method to divide large-scale group and conduct consensus management. Finally, we present a case study of intelligent route optimization in shared mobility to illustrate the usability of our proposed model. We also demonstrate its effectiveness and feasibility through a comparative analysis.  © 2023 Association for Computing Machinery.",Computing with words; consensus process; Internet of Things; large-scale linguistic group decision making; personalized individual semantics,Decision making; Internet of things; Computing with words; Consensus process; Decisions makings; Group Decision Making; Large-scale linguistic group decision making; Large-scales; Learning models; Making decision; Personalized individual semantic; Semantic learning; Semantics
A Highly Stable Fusion Positioning System of Smartphone under NLoS Acoustic Indoor Environment,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161637157&doi=10.1145%2f3589765&partnerID=40&md5=251b758ed7892a851339f2f8f20b405e,"Fusion positioning technology requires stable and effective positioning data, but this is often challenging to achieve in complex Non-Line-of-Sight (NLoS) environments. This paper proposes a fusion positioning method that can achieve stable and no hop points by adjusting parameters and predicting trends, even with a one-sided lack of fusion data. The method combines acoustic signal and Inertial Measurement Unit (IMU) data, exploiting their respective advantages. The fusion is achieved using the Kalman filter and Bayesian parameter estimation is performed for tuning IMU parameters and predicting motion trends. The proposed method overcomes the problem of fusion failure caused by long-term unilateral data loss in traditional fusion positioning. The positioning trajectory and error distribution analysis show that the proposed method performs optimally in severe NLoS experiments.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bayesian; IMU; Kalman filter; NLoS,Ubiquitous computing; Bayesian; Highly stables; Indoor environment; Inertial measurements units; Non-line-of-sight environments; Nonline of sight; Positioning data; Positioning system; Positioning technologies; Smart phones; Kalman filters
Dynamic Personalized POI Sequence Recommendation with Fine-Grained Contexts,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161702970&doi=10.1145%2f3583687&partnerID=40&md5=58e9d03ad5f3e9afd29ff61b2d739e00,"The Point Of Interest (POI) sequence recommendation is the key task in itinerary and travel route planning. Existing works usually consider the temporal and spatial factors in travel planning. However, the external environment, such as the weather, is usually overlooked. In fact, the weather is an important factor because it can affect a user's check-in behaviors. Furthermore, most of the existing research is based on a static environment for POI sequence recommendation. While the external environment (e.g., the weather) may change during travel, it is difficult for existing works to adjust the POI sequence in time. What's more, people usually prefer the attractive routes when traveling. To address these issues, we first conduct comprehensive data analysis on two real-world check-in datasets to study the effects of weather and time, as well as the features of the POI sequence. Based on this, we propose a model of Dynamic Personalized POI Sequence Recommendation with fine-grained contexts (DPSR for short). It extracts user interest and POI popularity with fine-grained contexts and captures the attractiveness of the POI sequence. Next, we apply the Monte Carlo Tree Search model (MCTS for short) to simulate the process of recommending POI sequence in the dynamic environment, i.e., the weather and time change after visiting a POI. What's more, we consider different speeds to reflect the fact that people may take different transportation to transfer between POIs. To validate the efficacy of DPSR, we conduct extensive experiments. The results show that our model can improve the accuracy of the recommendation significantly. Furthermore, it can better meet user preferences and enhance experiences.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Context-aware; dynamic; fine-grained; personalized POI sequence recommendation,Check-in; Context-Aware; External environments; Fine grained; Personalized point of interest sequence recommendation; Spatial factors; Static environment; Temporal and spatial; Travel planning; Travel route planning; Behavioral research
IRGA: An Intelligent Implicit Real-time Gait Authentication System in Heterogeneous Complex Scenarios,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161714205&doi=10.1145%2f3594538&partnerID=40&md5=86a501cf45a18f7d95ee15e7520c61f5,"Gait authentication as a technique that can continuously provide identity recognition on mobile devices for security has been investigated by academics in the community for decades. However, most of the existing work achieves insufficient generalization to complex real-world environments due to the complexity of the noisy real-world gait data. To address this limitation, we propose an intelligent Implicit Real-time Gait Authentication (IRGA) system based on Deep Neural Networks (DNNs) for enhancing the adaptability of gait authentication in practice. In the proposed system, the gait data (whether with complex interference signals) will first be processed sequentially by the imperceptible collection module and data preprocessing module for improving data quality. In order to illustrate and verify the suitability of our proposal, we provide analysis of the impact of individual gait changes on data feature distribution. Finally, a fusion neural network composed of a Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) is designed to perform feature extraction and user authentication. We evaluate the proposed IRGA system in heterogeneous complex scenarios and present start-of-the-art comparisons on three datasets. Extensive experiments demonstrate that the IRGA system achieves improved performance simultaneously in several different metrics.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Continuous authentication; fusion neural network; gait analysis; heterogeneous complex scenarios,Complex networks; Deep neural networks; Long short-term memory; Authentication systems; Continuous authentications; Fusion neural network; Generalisation; Heterogeneous complex scenario; Identity recognition; Neural-networks; Real world environments; Real- time; Real-world; Authentication
Concept Drift in Software Defect Prediction: A Method for Detecting and Handling the Drift,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161711823&doi=10.1145%2f3589342&partnerID=40&md5=5c61c7408716df06bc0dcec9a5e140a1,"Software Defect Prediction (SDP) is crucial towards software quality assurance in software engineering. SDP analyzes the software metrics data for timely prediction of defect prone software modules. Prediction process is automated by constructing defect prediction classification models using machine learning techniques. These models are trained using metrics data from historical projects of similar types. Based on the learned experience, models are used to predict defect prone modules in currently tested software. These models perform well if the concept is stationary in a dynamic software development environment. But their performance degrades unexpectedly in the presence of change in concept (Concept Drift). Therefore, concept drift (CD) detection is an important activity for improving the overall accuracy of the prediction model. Previous studies on SDP have shown that CD may occur in software defect data and the used defect prediction model may require to be updated to deal with CD. This phenomenon of handling the CD is known as CD adaptation. It is observed that still efforts need to be done in this direction in the SDP domain. In this article, we have proposed a pair of paired learners (PoPL) approach for handling CD in SDP. We combined the drift detection capabilities of two independent paired learners and used the paired learner (PL) with the best performance in recent time for next prediction. We experimented on various publicly available software defect datasets garnered from public data repositories. Experimentation results showed that our proposed approach performed better than the existing similar works and the base PL model based on various performance measures.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Concept drift; paired learning; software defect prediction; software quality assurance,Computer software selection and evaluation; Forecasting; Learning systems; Quality assurance; Software design; Software testing; Concept drifts; Defect prediction; Paired learning; Performance; Prediction process; Software defect prediction; Software defects; Software metrics; Software modules; Software quality assurance; Defects
Taming Internet of Things Application Development with the IoTvar Middleware,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161650900&doi=10.1145%2f3586010&partnerID=40&md5=4bfbb16efebfa5159dfc29e970fb01d4,"In the last years, Internet of Things (IoT) platforms have been designed to provide IoT applications with various services such as device discovery, context management, and data filtering. The lack of standardization has led each IoT platform to propose its own abstractions, APIs, and data models. As a consequence, programming interactions between an IoT consuming application and an IoT platform is time-consuming, is error prone, and depends on the developers' level of knowledge about the IoT platform. To address these issues, this article introduces IoTvar, a middleware library deployed on the IoT consumer application that manages all its interactions with IoT platforms. IoTvar relies on declaring variables automatically mapped to sensors whose values are transparently updated with sensor observations through proxies on the client side. This article presents the IoTvar architecture and shows how it has been integrated into the FIWARE, OM2M, and muDEBS platforms. We also report the results of experiments performed to evaluate IoTvar, showing that it reduces the effort required to declare and manage IoT variables and has no considerable impact on CPU, memory, and energy consumption.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Internet of Things; IoT platforms; Middleware; software abstractions,Abstracting; Application programming interfaces (API); Application programs; Energy utilization; Internet of things; Application development; Consequence programming; Context data; Context management; Data filtering; Device discovery; Error prones; Internet of thing platform; Middleware library; Software abstractions; Middleware
Using Deep Learning Models to Detect Fake News about COVID-19,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161712647&doi=10.1145%2f3533431&partnerID=40&md5=974b15da7b385fa3b3736abe887b50cf,"The proliferation of mobile networked devices has made it easier and faster than ever for people to obtain and share information. However, this occasionally results in the propagation of erroneous information, which may be difficult to distinguish from the truth. The widespread diffusion of such information can result in irrational and poor decision making on potentially important issues. In 2020, this coincided with the global outbreak of Coronavirus Disease (COVID-19), a highly contagious and deadly virus. The proliferation of misinformation about COVID-19 on social media has already been identified as an ""infodemic""by the World Health Organization (WHO), posing significant challenges for global governments seeking to manage the pandemic. This has driven an urgent need for methods to automatically detect and identify such misinformation. The research uses multiple deep learning model frameworks to detect misinformation in Chinese and English, and compare them based on different text feature selections. The model learns the textual characteristics of each type of true and misinformation for subsequent true/false prediction. The long and short-term memory (LSTM) model, the gated recurrent unit (GRU) model, and the bidirectional long and short-term memory (BiLSTM) model were selected for fake news detection. BiLSTM produces the best detection result, with detection accuracy reaching 94% for short-sentence English texts, and 99% for long-sentence English texts, while the accuracy for Chinese texts was 82%.  © 2023 Association for Computing Machinery.",BiLSTM; COVID-19; deep learning; GRU; LSTM; Misinformation detection,Behavioral research; Brain; Decision making; Fake detection; Learning systems; Long short-term memory; Viruses; Bidirectional long and short-term memory; Decisions makings; Deep learning; Gated recurrent unit; Learning models; Long and short term memory; Memory modeling; Misinformation detection; Networked devices; COVID-19
Guest Editors' Introduction for Special Issue on Applications of Computational Linguistics in Multimedia IoT Services,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161661599&doi=10.1145%2f3591355&partnerID=40&md5=ca5c4e2440d0b3bbb13aad0240ec1f23,"The Internet of Multimedia Things (IoMT) [4] is the combination of interfaces, protocols, and associated multimedia-related information, which enables advanced services and applications based on the human-to-device and device-to-device interactions in physical and virtual environments. The rapid growth in multimedia-on-demand traffic that refers to audio, video, and images has drastically shifted on the vision of the Internet of Things (IoT) [1, 5] from scalar to IoMT, which is an integral part of multimedia services such as real-time content delivery, online games, and video conferencing on the global Internet [3, 4]. Complementarily, Computational Linguistics (CL) [2] is an interdisciplinary research field concerned with the processing of languages by computers. Since machine translation began to emerge in the early 1970s, CL has grown and developed exponentially. Nevertheless, the combination of IoT-basedmultimedia with CL services has received less attention so far and has emerged as a newresearch paradigm for future computing applications. The future of smart IoMT devices with NLP is more important in real-time systems such as speech understanding, emotion recognition, and home automation. There are several issues and technical challenges that need attention from the research community. The rapid growth of multimedia IoT services (data abstraction, data sharing, data mining) has led the way to incorporating CL techniques to meet its requirements. This special issue presents multimedia IoT services in real-time systems and highlights the open research challenges to get advantageous use of CL.  © 2023 Copyright held by the owner/author(s).",,Data mining; Emotion Recognition; Interactive computer systems; Internet of things; Multimedia services; Real time systems; Speech recognition; Video conferencing; Audio images; Audio videos; Integral part; Interface protocol; Multimedia internet; Multimedia on demand; Rapid growth; Real - Time system; Real time content; Services and applications; Computational linguistics
Stance-level Sarcasm Detection with BERT and Stance-centered Graph Attention Networks,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159406993&doi=10.1145%2f3533430&partnerID=40&md5=42c7b109476e01b2029957d75371797b,"Computational Linguistics (CL) associated with the Internet of Multimedia Things (IoMT)-enabled multimedia computing applications brings several research challenges, such as real-time speech understanding, deep fake video detection, emotion recognition, home automation, and so on. Due to the emergence of machine translation, CL solutions have increased tremendously for different natural language processing (NLP) applications. Nowadays, NLP-enabled IoMT is essential for its success. Sarcasm detection, a recently emerging artificial intelligence (AI) and NLP task, aims at discovering sarcastic, ironic, and metaphoric information implied in texts that are generated in the IoMT. It has drawn much attention from the AI and IoMT research community. The advance of sarcasm detection and NLP techniques will provide a cost-effective, intelligent way to work together with machine devices and high-level human-to-device interactions. However, existing sarcasm detection approaches neglect the hidden stance behind texts, thus insufficient to exploit the full potential of the task. Indeed, the stance, i.e., whether the author of a text is in favor of, against, or neutral toward the proposition or target talked in the text, largely determines the text's actual sarcasm orientation. To fill the gap, in this research, we propose a new task: stance-level sarcasm detection (SLSD), where the goal is to uncover the author's latent stance and based on it to identify the sarcasm polarity expressed in the text. We then propose an integral framework, which consists of Bidirectional Encoder Representations from Transformers (BERT) and a novel stance-centered graph attention networks (SCGAT). Specifically, BERT is used to capture the sentence representation, and SCGAT is designed to capture the stance information on specific target. Extensive experiments are conducted on a Chinese sarcasm sentiment dataset we created and the SemEval-2018 Task 3 English sarcasm dataset. The experimental results prove the effectiveness of the SCGAT framework over state-of-the-art baselines by a large margin.  © 2023 Association for Computing Machinery.",artificial intelligence; graph attention networks; Sarcasm detection; stance extraction,Artificial intelligence; Computational linguistics; Emotion Recognition; Fake detection; Speech recognition; Computing applications; Graph attention network; Language processing; Multimedia computing; Natural languages; Real- time; Research challenges; Sarcasm detection; Speech understanding; Stance extraction; Cost effectiveness
MWPoW+: A Strong Consensus Protocol for Intra-Shard Consensus in Blockchain Sharding,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161666500&doi=10.1145%2f3584020&partnerID=40&md5=b126b6986f4d49e5e4f2ec2fdbb58cab,"Blockchain sharding splits a blockchain into several shards where consensus is reached at the shard level rather than over the entire blockchain. It improves transaction throughput and reduces the computational resources required of individual nodes. But a derivation of trustworthy consensus within a shard becomes an issue as the longest chain based mechanisms used in conventional blockchains can no longer be used. Instead, a vote-based consensus mechanism must be employed. However, existing vote-based Byzantine fault tolerance consensus protocols do not offer sufficient security guarantees for sharded blockchains. First, when used to support consensus where only one block is allowed at a time (binary consensus), these protocols are susceptible to progress-hindering attacks (i.e., unable to reach a consensus). Second, when used to support a stronger type of consensus where multiple concurrent blocks are allowed (strong consensus), their tolerance of adversary nodes is low. This article proposes a new consensus protocol to address all these issues. We call the new protocol MWPoW+, as its basic framework is based on the existing Multiple Winners Proof of Work (MWPoW) protocol but includes new mechanisms to address the issues mentioned previously. MWPoW+ is a vote-based protocol for strong consensus, asynchronous in consensus derivation but synchronous in communication. We prove that it can tolerate up to f < n/2 adversary nodes in a n-node system as if using a binary consensus protocol and does not suffer from progress-hindering attacks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",asynchronous consensus protocol; BFT; blockchain; blockchain sharding; Byzantine fault protocol; distributed ledger; MWPoW; PBFT; Strong consensus,Distributed ledger; Fault tolerance; Asynchronoi consensus protocol; Asynchronous consensus; BFT; Block-chain; Blockchain sharding; Byzantine fault; Byzantine fault protocol; Consensus protocols; Multiple winner proof of work; PBFT; Proof of work; Strong consensus; Blockchain
"VoiceTalk: Multimedia-IoT Applications for Mixing Mandarin, Taiwanese, and English",2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161700244&doi=10.1145%2f3543854&partnerID=40&md5=6e677bde9d7465c337866e288611c72f,"The voice-based Internet of Multimedia Things (IoMT) is the combination of IoT interfaces and protocols with associated voice-related information, which enables advanced applications based on human-to-device interactions. An example is Automatic Speech Recognition (ASR) for live captioning and voice translation. Three major issues of ASR for IoMT are IoT development cost, speech recognition accuracy, and execution time complexity. For the first issue, most non-voice IoT applications are upgraded with the ASR feature through hard coding, which are error prone. For the second issue, recognition accuracy must be improved for ASR. For the third issue, many multimedia IoT services are real-time applications and, therefore, the ASR delay must be short.This article elaborates on the above issues based on an IoT platform called VoiceTalk. We built the largest Taiwanese spoken corpus to train VoiceTalk ASR (VT-ASR) and show how the VT-ASR mechanism can be transparently integrated with existing IoT applications. We consider two performance measures for VoiceTalk: speech recognition accuracy and VT-ASR delay. For the acoustic tests of PAL-Labs, VT-ASR's accuracy is 96.47%, while Google's accuracy is 94.28%. We are the first to develop an analytic model to investigate the probability that the VT-ASR delay for the first speaker is complete before the second speaker starts talking. From the measurements and analytic modeling, we show that the VT-ASR delay is short enough to result in a very good user experience. Our solution has won several important government and commercial TV contracts in Taiwan. VT-ASR has demonstrated better Taiwanese Mandarin speech recognition accuracy than famous commercial products (including Google and Iflytek) in Formosa Speech Recognition Challenge 2018 (FSR-2018) and was the best among all participating ASR systems for Taiwanese recognition accuracy in FSR-2020.  © 2023 Association for Computing Machinery.",Automatic Speech Recognition (ASR); Computational Linguistics (CL); IoTtalk; multimedia IoT,Analytical models; Internet of things; Multimedia services; Advanced applications; Analytic modeling; Automatic speech recognition; Computational linguistic; Development costs; Google+; Iottalk; Multimedium IoT; Recognition accuracy; Speech recognition
Real-time Pricing-based Resource Allocation in Open Market Environments,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152894638&doi=10.1145%2f3465237&partnerID=40&md5=83299c18c5430bf9879943a0b12ebba2,"Open market environments consist of a set of participants (vendors and consumers) that dynamically leave or join the market. As a result, the arising dynamism leads to uncertainties in supply and demand of the resources in these open markets. In specific, in such uncertain markets, vendors attempt to maximise their revenue by dynamically changing their selling prices according to the market demand. In this regard, an optimal resource allocation approach becomes immensely needed to optimise the selling prices based on the supply and demand of the resources in the open market. Therefore, optimal selling prices should maximise the revenue of vendors while protecting the utility of buyers. In this context, we propose a real-time pricing approach for resource allocation in open market environments. The proposed approach introduces a priority-based fairness mechanism to allocate the available resources in a reverse-auction paradigm. Finally, we compare the proposed approach with two state-of-the-art resource allocation approaches. The experimental results show that the proposed approach outperforms the other two resource allocation approaches in its ability to maximise the vendors' revenue.  © 2023 Association for Computing Machinery.",Open market environments; real-time pricing; reinforcement learning; resource allocation,Commerce; Costs; Reinforcement learning; Sales; Allocation approach; Market environment; Open market; Open market environment; Real time pricing; Reinforcement learnings; Resources allocation; Selling prices; Uncertain markets; Uncertainty; Resource allocation
A Multi-type Classifier Ensemble for Detecting Fake Reviews Through Textual-based Feature Extraction,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152629858&doi=10.1145%2f3568676&partnerID=40&md5=7b02087d239177709560df530614e246,"The financial impact of online reviews has prompted some fraudulent sellers to generate fake consumer reviews for either promoting their products or discrediting competing products. In this study, we propose a novel ensemble model - the Multi-type Classifier Ensemble (MtCE) - combined with a textual-based featuring method, which is relatively independent of the system, to detect fake online consumer reviews. Unlike other ensemble models that utilise only the same type of single classifier, our proposed ensemble utilises several customised machine learning classifiers (including deep learning models) as its base classifiers. The results of our experiments show that the MtCE can adequately detect fake reviews, and that it outperforms other single and ensemble methods in terms of accuracy and other measurements for all the relevant public datasets used in this study. Moreover, if set correctly, the parameters of MtCE, such as base-classifier types, the total number of base classifiers, bootstrap, and the method to vote on output (e.g., majority or priority), can further improve the performance of the proposed ensemble.  © 2023 Association for Computing Machinery.",deep learning; Fake review detection; machine learning; novel ensemble model; online commerce security,Deep learning; E-learning; Electronic commerce; Fake detection; Learning systems; Base classifiers; Classifiers ensemble; Deep learning; Ensemble models; Fake review detections; Features extraction; Machine-learning; Novel ensemble model; Online commerce; Online commerce security; Feature extraction
Finding the Source in Networks: An Approach Based on Structural Entropy,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152895328&doi=10.1145%2f3568309&partnerID=40&md5=e646da2ffd2ffa7e22b48830e76eb920,"The popularity of intelligent devices provides straightforward access to the Internet and online social networks. However, the quick and easy data updates from networks also benefit the risk spreading, such as rumor, malware, or computer viruses. To this end, this article studies the problem of source detection, which is to infer the source node out of an aftermath of a cascade, that is, the observed infected graph GN of the network at some time. Prior arts have adopted various statistical quantities such as degree, distance, or infection size to reflect the structural centrality of the source. In this article, we propose a new metric that we call the infected tree entropy (ITE), to utilize richer underlying structural features for source detection. Our idea of ITE is inspired by the conception of structural entropy [21], which demonstrated that the minimization of average bits to encode the network structures with different partitions is the principle for detecting the natural or true structures in real-world networks. Accordingly, our proposed ITE based estimator for the source tries to minimize the coding of network partitions brought by the infected tree rooted at all the potential sources, thus minimizing the structural deviation between the cascades from the potential sources and the actual infection process included in GN. On polynomially growing geometric trees, with increasing tree heterogeneity, the ITE estimator remarkably yields more reliable detection under only moderate infection sizes, and returns an asymptotically complete detection. In contrast, for regular expanding trees, we still observe guaranteed detection probability of ITE estimator even with an infinite infection size, thanks to the degree regularity property. We also algorithmically realize the ITE based detection that enjoys linear time complexity via a message-passing scheme, and further extend it to general graphs. Extensive experiments on synthetic and real datasets confirm the superiority of ITE to the baselines. For example, ITE returns an accuracy of 85%, ranking the source among the top 10%, far exceeding 55% of the classic algorithm on scale-free networks.  © 2023 Copyright held by the owner/author(s).",graph theory; inference algorithms; Source detection; structural entropy,Computer viruses; Inference engines; Message passing; Social networking (online); Trees (mathematics); Data update; Entropy estimator; Entropy-based; In networks; Infected trees; Inference algorithm; Intelligent devices; Potential sources; Source detection; Structural entropy; Entropy
Uncertainty-Aware Personal Assistant for Making Personalized Privacy Decisions,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152616569&doi=10.1145%2f3561820&partnerID=40&md5=0b7822b48b91d0c0c4e71f3526c61ad1,"Many software systems, such as online social networks, enable users to share information about themselves. Although the action of sharing is simple, it requires an elaborate thought process on privacy: what to share, with whom to share, and for what purposes. Thinking about these for each piece of content to be shared is tedious. Recent approaches to tackle this problem build personal assistants that can help users by learning what is private over time and recommending privacy labels such as private or public to individual content that a user considers sharing. However, privacy is inherently ambiguous and highly personal. Existing approaches to recommend privacy decisions do not address these aspects of privacy sufficiently. Ideally, a personal assistant should be able to adjust its recommendation based on a given user, considering that user's privacy understanding. Moreover, the personal assistant should be able to assess when its recommendation would be uncertain and let the user make the decision on her own. Accordingly, this article proposes a personal assistant that uses evidential deep learning to classify content based on its privacy label. An important characteristic of the personal assistant is that it can model its uncertainty in its decisions explicitly, determine that it does not know the answer, and delegate from making a recommendation when its uncertainty is high. By factoring in the user's own understanding of privacy, such as risk factors or own labels, the personal assistant can personalize its recommendations per user. We evaluate our proposed personal assistant using a well-known dataset. Our results show that our personal assistant can accurately identify uncertain cases, personalize them to its user's needs, and thus helps users preserve their privacy well.  © 2023 Association for Computing Machinery.",online social networks; Privacy; uncertainty,Deep learning; Online systems; Social sciences computing; Content-based; Personal assistants; Privacy; Risk factors; Simple++; Software-systems; Thought process; Uncertainty; User need; User privacy; Social networking (online)
SAM: Multi-turn Response Selection Based on Semantic Awareness Matching,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152925748&doi=10.1145%2f3545570&partnerID=40&md5=ffd1e78471204911f7389f022d97a19f,"Multi-turn response selection is a key issue in retrieval-based chatbots and has attracted considerable attention in the NLP (Natural Language processing) field. So far, researchers have developed many solutions that can select appropriate responses for multi-turn conversations. However, these works are still suffering from the semantic mismatch problem when responses and context share similar words with different meanings. In this article, we propose a novel chatbot model based on Semantic Awareness Matching, called SAM. SAM can capture both similarity and semantic features in the context by a two-layer matching network. Appropriate responses are selected according to the matching probability made through the aggregation of the two feature types. In the evaluation, we pick 4 widely used datasets and compare SAM's performance to that of 12 other models. Experiment results show that SAM achieves substantial improvements, with up to 1.5% R10@1 on Ubuntu Dialogue Corpus V2, 0.5% R10@1 on Douban Conversation Corpus, and 1.3% R10@1 on E-commerce Corpus.  © 2023 Association for Computing Machinery.",BERT; chatbot; multi-turn response selection; Natural language processing,Natural language processing systems; Network layers; BERT; Chatbots; Language processing; Matchings; Multi-turn; Multi-turn response selection; Natural language processing; Natural languages; Response selection; Selection based; Semantics
Introduction to the Special Section on Cyber Security in Internet of Vehicles,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151814873&doi=10.1145%2f3584746&partnerID=40&md5=0ac41524d126d64036be93b3ad002a9f,[No abstract available],,
Emerging Trends of ICT in Airborne Disease Prevention,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151822328&doi=10.1145%2f3564783&partnerID=40&md5=1c78d9167e55ed6868bc5d5d97c8e9fb,"Information and Communication Technologies (ICT) are becoming indispensable nowadays for the healthcare industry. The utilization of ICT in healthcare services has accelerated even faster after the commencement of the COVID-19 outbreak. This study aims to perform a scientometric analysis of scholarly literature on airborne diseases in the discipline of science and technology. It explores the recent advancement of internet technologies in healthcare to control the prevalence of deadly airborne illnesses by applying analytical approaches. It presents publication trends, citation structure, influential sources, co-citation, and co-occurrence network analysis using the CiteSpace tool. It identifies the important research topics, current research hotspots, most active research areas, and leading technologies in this scientific knowledge domain. It inferred significant results from analyses that will benefit researchers and the academic fraternity across the globe to understand the evolving paths and recent scientific progress of ICT in airborne disease management.  © 2022 Association for Computing Machinery.",Additional Key Words and PhrasesAirborne diseases; COVID-19; information and communication technology; internet technologies; scientometric analysis,Health care; Additional key word and phrasesairborne disease; Airborne disease; Disease prevention; Emerging trends; Healthcare industry; Healthcare services; Information and Communication Technologies; Internet technology; Key words; Scientometric analysis; COVID-19
Introduction to the Special Section on Recent Advances in Networks and Distributed Systems,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151909908&doi=10.1145%2f3584743&partnerID=40&md5=7955bc9144845a9ba924637c60957a14,[No abstract available],,
Identity-Based Public Auditing for Cloud Storage of Internet-of-Vehicles Data,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151544673&doi=10.1145%2f3433543&partnerID=40&md5=1df0a81347dccf66dd4bdd5fe7dd9fef,"The Internet of Vehicles (IoV), with the help of cloud computing, can provide rich and powerful application services for vehicles and drivers by sharing and analysing various IoV data. However, how to ensure the integrity of IoV data with multiple sources and diversity outsourced in the cloud is still an open challenge. To address this concern, this paper first presents an identity-based public auditing scheme for cloud storage of IoV data, which can fully achieve the essential function and security requirements, such as classified auditing, multi-source auditing and privacy protection. Particularly, we design a new authenticated data structure, called data mapping table, to track the distribution of each type of IoV data to ensure fine and rapid audits. Moreover, our scheme can reduce the overheads for both the key management and the generation of block tags. We formally prove the security of the presented scheme and evaluate its performance by comprehensive comparisons with the state-of-the-art schemes designed for traditional scenarios. The theoretical analyses and experimental results demonstrate that our scheme can securely and efficiently realize public auditing for IoV data, and outperforms the previous ones in both the computation and communication overheads in most cases. © 2023 Association for Computing Machinery.",cloud storage; data integrity; data mapping table; internet of vehicles; Public auditing,Mapping; Vehicles; Application services; Cloud storages; Cloud-computing; Data integrity; Data mapping table; Data mappings; Identity-based; Internet of vehicle; Multiple source; Public auditing; Cloud storage
Introduction to the Special Issue on Multiagent Systems and Services in the Internet of Things,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151908229&doi=10.1145%2f3584744&partnerID=40&md5=587e5bc8eb9d9e095cacd06e238336ed,[No abstract available],,
Introduction to the Special Section on Edge Computing AI-IoT Integrated Energy Efficient Intelligent Transportation System for Smart Cities,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151818294&doi=10.1145%2f3584745&partnerID=40&md5=d4f89e2e614bce4c4dbcfb7635bfd006,[No abstract available],,
Token-Based Authorization and Authentication for Secure Internet of Vehicles Communication,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151857485&doi=10.1145%2f3491202&partnerID=40&md5=5f30e09ca627d9390f6a7abbf6ff09da,"The Internet of Vehicles (IoV) communication platform provides seamless information exchange facilities in a dynamic mobile city environment. Heterogeneous communication is a common medium for information exchange through autonomous resources distributed and accessed using infrastructure units. Cyber-security is a primary concern in accessing autonomous information from the distributed resources due to anonymity and different types of targeted adversaries. This article proposes token-based authorization and authentication (TAA) for securing IoV communications. The proposed method relies on blockchain technology and random forest learning for authorization and key management for authentication, respectively. In this process, frequent change in tokens and key update features are restricted in a view to maximize the seamlessness in information exchange. Authentication is preceded by knowledge of the data classification without errors to prevent additional overhead. Blockchain-based authorization helps to update specific fields of the tokens to retain the communication ratio by reducing vehicle-to-vehicle losses. The performance of the proposed method is assessed using appropriate simulations for these metrics by varying vehicle density, error rate, and classification sets.  © 2023 Association for Computing Machinery.",Authentication and authorization; blockchain; cyber-security; Internet of Vehicles; random forest,Authorization; Blockchain; Cybersecurity; Information dissemination; Vehicle to vehicle communications; Vehicles; Authentication and authorization; Block-chain; Communication platforms; Cyber security; Distributed resources; Heterogeneous communication; Information exchanges; Internet of vehicle; Random forests; Vehicle communications; Authentication
Breaking CaptchaStar Using the BASECASS Methodology,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152945506&doi=10.1145%2f3546867&partnerID=40&md5=58adab006aa04b2a81373af080b70143,"In this article, we present fundamental design flaws of CaptchaStar. We also present a full analysis using the BASECASS methodology that employs machine learning techniques. By means of this methodology, we find an attack that bypasses CaptchaStar with almost 100% accuracy.  © 2023 Copyright held by the owner/author(s).",BASECASS; CAPTCHA; machine learning; statistical analysis,Electronic mail filters; BASECASS; Breakings; CAPTCHAs; Fundamental design; Machine learning techniques; Machine-learning; Machine learning
Joint Architecture Design and Workload Partitioning for DNN Inference on Industrial IoT Clusters,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152632802&doi=10.1145%2f3551638&partnerID=40&md5=64650a5ad874514888d322837c802467,"The advent of Deep Neural Networks (DNNs) has empowered numerous computer-vision applications. Due to the high computational intensity of DNN models, as well as the resource constrained nature of Industrial Internet-of-Things (IIoT) devices, it is generally very challenging to deploy and execute DNNs efficiently in the industrial scenarios. Substantial research has focused on model compression or edge-cloud offloading, which trades off accuracy for efficiency or depends on high-quality infrastructure support, respectively. In this article, we present EdgeDI, a framework for executing DNN inference in a partitioned, distributed manner on a cluster of IIoT devices. To improve the inference performance, EdgeDI exploits two key optimization knobs, including: (1) Model compression based on deep architecture design, which transforms the target DNN model into a compact one that reduces the resource requirements for IIoT devices without sacrificing accuracy; (2) Distributed inference based on adaptive workload partitioning, which achieves high parallelism by adaptively balancing the workload distribution among IIoT devices under heterogeneous resource conditions. We have implemented EdgeDI based on PyTorch, and evaluated its performance with the NEU-CLS defect classification task and two typical DNN models (i.e., VGG and ResNet) on a cluster of heterogeneous Raspberry Pi devices. The results indicate that the proposed two optimization approaches significantly outperform the existing solutions in their specific domains. When they are well combined, EdgeDI can provide scalable DNN inference speedups that are very close to or even much higher than the theoretical speedup bounds, while still maintaining the desired accuracy.  © 2023 Association for Computing Machinery.",deep learning; distributed inference; DNN architecture; edge computing; Industrial Internet-of-Things (IIoT),Computation offloading; Internet of things; Network architecture; Neural network models; Architecture designs; Deep learning; Deep neural network architecture; Distributed inference; Edge computing; Industrial internet-of-thing; Model compression; Network inference; Neural network architecture; Neural network model; Deep neural networks
Facilitating Serverless Match-based Online Games with Novel Blockchain Technologies,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152629523&doi=10.1145%2f3565884&partnerID=40&md5=61a377606164506f50a7db55cb947bad,"Applying peer-to-peer (P2P) architecture to online video games has already attracted both academic and industrial interests, since it removes the need for expensive server maintenance. However, there are two major issues preventing the use of a P2P architecture, namely how to provide an effective distributed data storage solution, and how to tackle potential cheating behaviors. Inspired by emerging blockchain techniques, we propose a novel consensus model called Proof-of-Play (PoP) to provide a decentralized data storage system that incorporates an anti-cheating mechanism for P2P games, by rewarding players that interact with the game as intended, along with consideration of security measures to address the Nothing-at-stake Problem and the Long-range Attack. To validate our design, we utilize a game-theory model to show that under certain assumptions, the integrity of the PoP system would not be undermined due to the best interests of any user. Then, as a proof-of-concept, we developed a P2P game (Infinity Battle) to demonstrate how a game can be integrated with PoP in practice. Finally, experiments were conducted to study PoP in comparison with Proof-of-Work (PoW) to show its advantages in various aspects.  © 2023 Association for Computing Machinery.",blockchain; consensus model; Peer-to-peer game,Digital storage; Game theory; Human computer interaction; Peer to peer networks; Block-chain; Consensus models; Distributed data storages; On-line games; Online video; P2P architecture; Peer to peer (P2P); Peer-to-peer games; Storage solutions; Video-games; Blockchain
Movie Account Recommendation on Instagram,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152630466&doi=10.1145%2f3579852&partnerID=40&md5=f8fe0f54d1225ce62b08bd322221638f,"With the increasing popularity of social networks, many businesses have started implementing their branding or targeted advertising strategies to reach potential customers through social media platforms. It is desirable and essential to help businesses to reach mass audiences and assist users to find favorite business accounts on social media platforms. In the movie industry, movie companies often create business accounts (movie accounts) to promote their movies and capture the attention of followers on Instagram. Instagram contains rich information about movies and user feedback, while IMDb, one of the most popular online databases, contains well-organized information related to movies. The features extracted from the data collected from Instagram and IMDb can complement each other. Therefore, in this study, we propose a framework for recommending movie accounts to users on Instagram by using the data collected from Instagram and IMDb platforms. The experiment results show that our proposed framework outperforms the comparing methods in terms of precision, recall, F1-score, and Normalized Discounted Cumulative Gain (NDCG), and mitigates the effect of cold start problems. The proposed framework can help movie companies or businesses reach potential audiences and implement effective targeted advertising strategies.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention mechanism; deep learning model; Movie account recommendation; targeted advertising strategy,Deep learning; Marketing; Social networking (online); Advertising strategy; Attention mechanisms; Deep learning model; Learning models; Movie account recommendation; Movie industry; Potential customers; Social media platforms; Targeted advertising; Targeted advertizing strategy; Motion pictures
Attacking DoH and ECH: Does Server Name Encryption Protect Users' Privacy?,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152619146&doi=10.1145%2f3570726&partnerID=40&md5=c57b90a6d7de166257dc8f75070614e2,"Privacy on the Internet has become a priority, and several efforts have been devoted to limit the leakage of personal information. Domain names, both in the TLS Client Hello and DNS traffic, are among the last pieces of information still visible to an observer in the network. The Encrypted Client Hello extension for TLS, DNS over HTTPS or over QUIC protocols aim to further increase network confidentiality by encrypting the domain names of the visited servers.In this article, we check whether an attacker able to passively observe the traffic of users could still recover the domain name of websites they visit even if names are encrypted. By relying on large-scale network traces, we show that simplistic features and off-the-shelf machine learning models are sufficient to achieve surprisingly high precision and recall when recovering encrypted domain names. We consider three attack scenarios, i.e., recovering the per-flow name, rebuilding the set of visited websites by a user, and checking which users visit a given target website. We next evaluate the efficacy of padding-based mitigation, finding that all three attacks are still effective, despite resources wasted with padding. We conclude that current proposals for domain encryption may produce a false sense of privacy, and more robust techniques should be envisioned to offer protection to end users.  © 2023 Copyright held by the owner/author(s).",eavesdropping; encryption; network traffic; Privacy,Data privacy; HTTP; Internet protocols; Network security; Recovery; Websites; DNS traffics; Domain names; Eavesdropping; Large-scale network; Network confidentialities; Network traffic; Off-the-shelf machine; Personal information; Privacy; User privacy; Cryptography
Conco-ERNIE: Complex User Intent Detect Model for Smart Healthcare Cognitive Bot,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152622102&doi=10.1145%2f3574135&partnerID=40&md5=d2b8fa217d427b21303753a82eb5475a,"The outbreak of Covid-19 has exposed the lack of medical resources, especially the lack of medical personnel. This results in time and space restrictions for medical services, and patients cannot obtain health information all the time and everywhere. Based on the medical knowledge graph, healthcare bots alleviate this burden effectively by providing patients with diagnosis guidance, pre-diagnosis, and post-diagnosis consultation services in the way of human-machine dialogue. However, the medical utterance is more complicated in language structure, and there are complex intention phenomena in semantics. It is a challenge to detect the single intent, multi-intent, and implicit intent of a patient's utterance. To this end, we create a high-quality annotated Chinese Medical query (utterance) dataset, CMedQ (about 16.8k queries in medical domain which includes single, multiple, and implicit intents). It is hard to detect intent on such a complex dataset through traditional text classification models. Thus, we propose a novel detect model Conco-ERNIE, using concept co-occurrence patterns to enhance the representation of pre-trained model ERNIE. These patterns are mined using Apriori algorithm and will be embedded via Node2Vec. Their features will be aggregated with semantic features into Conco-ERNIE by using an attention module, which can catch user explicit intents and also predict user implicit intents. Experiments on CMedQ demonstrates that Conco-ERNIE achieves outstanding performance over baseline. Based on Conco-ERNIE, we develop an intelligent healthcare bot, MedicalBot. To provide knowledge support for MedicalBot, we also build a Chinese medical graph, CMedKG (about 45k entities and 283k relationships).  © 2023 Association for Computing Machinery.",cognitive service; healthcare bot; Intent detection; medical knowledge graph,Classification (of information); Health care; Knowledge graph; Semantics; Text processing; Cognitive service; Consultation services; Health informations; Healthcare bot; Intent detection; Knowledge graphs; Medical knowledge; Medical knowledge graph; Medical personnel; Medical services; Diagnosis
White Box: On the Prediction of Collaborative Filtering Recommendation Systems' Performance,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152617792&doi=10.1145%2f3554979&partnerID=40&md5=52003eaf74fd0605e2b8a24219c8a1d6,"Collaborative Filtering (CF) recommendation algorithms are a popular solution to the information overload problem, aiding users in the item selection process. Relevant research has long focused on refining and improving these models to produce better (more effective) recommendations, and has converged on a methodology to predict their effectiveness on target datasets by evaluating them on random samples of the latter. However, predicting the efficiency of the solutions - especially with regard to their time- and resource-hungry training phase, whose requirements dwarf those of the prediction/recommendation phase - has received little to no attention in the literature. This article addresses this gap for a number of representative and highly popular CF models, including algorithms based on matrix factorization, k-nearest neighbors, co-clustering, and slope one schemes. To this end, we first study the computational complexity of the training phase of said CF models and derive time and space complexity equations. Then, using characteristics of the input and the aforementioned equations, we contribute a methodology for predicting the processing time and memory usage of their training phase. Our contributions further include an adaptive sampling strategy, to address the tradeoff between resource usage costs and prediction accuracy, and a framework that quantifies both the efficiency and effectiveness of CF. Finally, a systematic experimental evaluation demonstrates that our method outperforms state-of-the-art regression schemes by a considerable margin, with an overhead that is a small fraction of the overall requirements of CF training.  © 2023 Association for Computing Machinery.",effectiveness evaluation; efficiency evaluation; Recommendation systems; sampling-based time and memory prediction,Clustering algorithms; Collaborative filtering; Efficiency; Matrix factorization; Nearest neighbor search; Recommender systems; Collaborative filtering recommendations; Effectiveness evaluation; Efficiency evaluation; Filtering models; Recommendation algorithms; Sampling-based; Sampling-based time and memory prediction; Systems performance; Training phasis; White box; Forecasting
The Tip of the Buyer: Extracting Product Tips from Reviews,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152951029&doi=10.1145%2f3547140&partnerID=40&md5=1e79c6dd179b21a2950709ad404391a8,"Product reviews play a key role in e-commerce platforms. Studies show that many users read product reviews before a purchase and trust them to the same extent as personal recommendations. However, in many cases, the number of reviews per product is large and extracting useful information becomes a challenging task. Several websites have recently added an option to post tips - short, concise, practical, and self-contained pieces of advice about the products. These tips are complementary to the reviews and usually add a new non-trivial insight about the product, beyond its title, attributes, and description. Yet, most if not all major e-commerce platforms lack the notion of a tip as a first-class citizen and customers typically express their advice through other means, such as reviews. In this work, we propose an extractive method for tip generation from product reviews. We focus on five popular e-commerce domains whose reviews tend to contain useful non-trivial tips that are beneficial for potential customers. We formally define the task of tip extraction in e-commerce by providing the list of tip types, tip timing (before and/or after the purchase), and connection to the surrounding context sentences. To extract the tips, we propose a supervised approach and leverage a publicly available dataset, annotated by human editors, containing 14,000 product reviews. To demonstrate the potential of our approach, we compare different tip generation methods and evaluate them both manually and over the labeled set. Our approach demonstrates particularly high performance for popular products in the Baby, Home Improvement, and Sports & Outdoors domains, with precision of over 95% for the top 3 tips per product. In addition, we evaluate the performance of our methods on previously unseen domains. Finally, we discuss the practical usage of our approach in real-world applications. Concretely, we explain how tips generated from user reviews can be integrated in various use cases within e-commerce platforms and benefit both buyers and sellers.  © 2023 Association for Computing Machinery.",deep learning; E-commerce; machine learning; product reviews; tips generation,Deep learning; Learning systems; Sales; Commerce platforms; Deep learning; E- commerces; E-commerce domains; Machine-learning; Non-trivial; Performance; Personal recommendations; Product reviews; Tip generation; Electronic commerce
Elastically Augmenting the Control-path Throughput in SDN to Deal with Internet DDoS Attacks,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152615145&doi=10.1145%2f3559759&partnerID=40&md5=83c8e7fa966bcbbf1a8eff80b7d5db11,"Distributed denial of service (DDoS) attacks have been prevalent on the Internet for decades. Albeit various defenses, they keep growing in size, frequency, and duration. The new network paradigm, Software-defined networking (SDN), is also vulnerable to DDoS attacks. SDN uses logically centralized control, bringing the advantages in maintaining a global network view and simplifying programmability. When attacks happen, the control path between the switches and their associated controllers may become congested due to their limited capacity. However, the data plane visibility of SDN provides new opportunities to defend against DDoS attacks in the cloud computing environment. To this end, we conduct measurements to evaluate the throughput of the software control agents on some of the hardware switches when they are under attacks. Then, we design a new mechanism, called Scotch, to enable the network to scale up its capability and handle the DDoS attack traffic. In our design, the congestion works as an indicator to trigger the mitigation mechanism. Scotch elastically scales up the control plane capacity by using an Open vSwitch-based overlay. Scotch takes advantage of both the high control plane capacity of a large number of vSwitches and the high data plane capacity of commodity physical switches to increase the SDN network scalability and resiliency under abnormal (e.g., DDoS attacks) traffic surges. We have implemented a prototype and experimentally evaluated Scotch. Our experiments in the small-scale lab environment and large-scale GENI testbed demonstrate that Scotch can elastically scale up the control channel bandwidth upon attacks.  © 2023 Association for Computing Machinery.",DDoS attacks; overlay network; SDN,Denial-of-service attack; Network security; Software agents; Attack traffic; Control path; Control planes; Data planes; Data-plane; Denialof- service attacks; Distributed denial of service; Distributed denial of service attack; Scale-up; Software-defined networkings; Overlay networks
Reaching for the Sky: Maximizing Deep Learning Inference Throughput on Edge Devices with AI Multi-Tenancy,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152930317&doi=10.1145%2f3546192&partnerID=40&md5=4a6a9c1c2ea4079f7b07ed086d5e6582,"The wide adoption of smart devices and Internet-of-Things (IoT) sensors has led to massive growth in data generation at the edge of the Internet over the past decade. Intelligent real-time analysis of such a high volume of data, particularly leveraging highly accurate deep learning (DL) models, often requires the data to be processed as close to the data sources (or at the edge of the Internet) to minimize the network and processing latency. The advent of specialized, low-cost, and power-efficient edge devices has greatly facilitated DL inference tasks at the edge. However, limited research has been done to improve the inference throughput (e.g., number of inferences per second) by exploiting various system techniques. This study investigates system techniques, such as batched inferencing, AI multi-tenancy, and cluster of AI accelerators, which can significantly enhance the overall inference throughput on edge devices with DL models for image classification tasks. In particular, AI multi-tenancy enables collective utilization of edge devices' system resources (CPU, GPU) and AI accelerators (e.g., Edge Tensor Processing Units; EdgeTPUs). The evaluation results show that batched inferencing results in more than 2.4× throughput improvement on devices equipped with high-performance GPUs like Jetson Xavier NX. Moreover, with multi-tenancy approaches, e.g., concurrent model executions (CME) and dynamic model placements (DMP), the DL inference throughput on edge devices (with GPUs) and EdgeTPU can be further improved by up to 3× and 10×, respectively. Furthermore, we present a detailed analysis of hardware and software factors that change the DL inference throughput on edge devices and EdgeTPUs, thereby shedding light on areas that could be further improved to achieve high-performance DL inference at the edge.  © 2023 Association for Computing Machinery.",AI multi-tenancy; and characterization; concurrent model executions; deep learning at the edge; dynamic model placements; Edge computing; performance evaluation,Deep learning; Edge computing; Graphics processing unit; Image enhancement; Internet of things; Learning systems; AI multi-tenancy; And characterization; Concurrent model execution; Concurrent modeling; Deep learning at the edge; Dynamic model placement; Dynamics models; Edge computing; Model executions; Multi tenancies; Performances evaluation; Dynamic models
Tuneman: Customizing Networks to Guarantee Application Bandwidth and Latency,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152626986&doi=10.1145%2f3575657&partnerID=40&md5=a146f57cf99261fee54b2d9d712269a9,"We examine how to provide applications with dedicated bandwidth and guaranteed latency in a programmable mission-critical network. Unlike other SDN approaches such as B4 or SWAN, our system Tuneman optimizes both routes and packet schedules at each node to provide flows with sub-second bandwidth changes. Tuneman uses node-level optimization to compute node schedules in a slotted switch and does dynamic routing using a search procedure with Quality of Service- (QoS) based weights. This allows Tuneman to provide an efficient solution for mission-critical networks that have stringent QoS requirements. We evaluate Tuneman on a telesurgery network using a switch prototype built using FPGAs and also via simulations on India's Tata Network. For mission-critical networks with multiple QoS levels, Tuneman has comparable or better utilization than SWAN while providing delay bounds guarantees.  © 2023 Association for Computing Machinery.",FPGAs; mission-critical networks; network programmability; QoS; scheduling; segment routing,Bandwidth; Network routing; Quality of service; Dynamic routing; Mission critical networks; Network programmability; Optimisations; Programmability; Quality of service-; Quality-of-service; Routings; Segment routing; Slotted switches; Field programmable gate arrays (FPGA)
L2DART: A Trust Management System Integrating Blockchain and Off-Chain Computation,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152618041&doi=10.1145%2f3561386&partnerID=40&md5=7aad443d81b080165185b342f59363bd,"The blockchain technology has been gaining an increasing popularity for the last years, and smart contracts are being used for a growing number of applications in several scenarios. The execution of smart contracts on public blockchains can be invoked by any user with a transaction, although in many scenarios there would be the need for restricting the right of executing smart contracts only to a restricted set of users. To help deal with this issue, this article proposes a system based on a popular access control framework called RT, Role-based Trust Management, to regulate smart contracts execution rights. The proposed system, called Layer 2 DecentrAlized Role-based Trust management (L2DART), implements the RT framework on a public blockchain, and it is designed as a layer-2 technology that involves both on-chain and off-chain functionalities to reduce the blockchain costs while keeping blockchain auditability, i.e., immutability and transparency. The on-chain costs of L2DART have been evaluated on Ethereum and compared with a previous solution implementing on-chain all the functionalities. The results show that the on-chain costs of L2DART are relatively low, making the system deployable in real-world scenarios.  © 2023 Copyright held by the owner/author(s).",access control; Blockchain; layer-2; off-chain computation; smart contract; Trust Management,Access control; Blockchain; Block-chain; Contract execution; Control framework; Decentralised; Layer 2; Management IS; Off-chain computation; Role-based trust managements; Trust management; Trust management systems; Smart contract
SDN-enabled Resource Provisioning Framework for Geo-Distributed Streaming Analytics,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152629943&doi=10.1145%2f3571158&partnerID=40&md5=13d7ec81dcc423abf3a2d457501ace5c,"Geographically distributed (geo-distributed) datacenters for stream data processing typically comprise multiple edges and core datacenters connected through Wide-Area Network (WAN) with a master node responsible for allocating tasks to worker nodes. Since WAN links significantly impact the performance of distributed task execution, the existing task assignment approach is unsuitable for distributed stream data processing with low latency and high throughput demand. In this paper, we propose SAFA, a resource provisioning framework using the Software-Defined Networking (SDN) concept with an SDN controller responsible for monitoring the WAN, selecting an appropriate subset of worker nodes, and assigning tasks to the designated worker nodes. We implemented the data plane of the framework in P4 and the control plane components in Python. We tested the performance of the proposed system on Apache Spark, Apache Storm, and Apache Flink using the Yahoo! streaming benchmark on a set of custom topologies. The results of the experiments validate that the proposed approach is viable for distributed stream processing and confirm that it can improve at least 1.64× the processing time of incoming events of the current stream processing systems.  © 2023 Copyright held by the owner/author(s).",Cluster manager; geo-distributed stream analytics; Software-Defined Networking (SDN); stream processing,Benchmarking; Data handling; Wide area networks; Cluster managers; Datacenter; Geographically distributed stream analytic; Provisioning framework; Software-defined networking; Software-defined networkings; Stream data processing; Stream processing; Wide-area networks; Worker nodes; Software defined networking
A Low-code Development Framework for Cloud-native Edge Systems,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152965329&doi=10.1145%2f3563215&partnerID=40&md5=776e10fcdcddc5e784d1aab7960ac272,"Customizing and deploying an edge system are time-consuming and complex tasks because of hardware heterogeneity, third-party software compatibility, diverse performance requirements, and so on. In this article, we present TinyEdge, a holistic framework for the low-code development of edge systems. The key idea of TinyEdge is to use a top-down approach for designing edge systems. Developers select and configure TinyEdge modules to specify their interaction logic without dealing with the specific hardware or software. Taking the configuration as input, TinyEdge automatically generates the deployment package and estimates the performance with sufficient profiling. TinyEdge provides a unified development toolkit to specify module dependencies, functionalities, interactions, and configurations. We implement TinyEdge and evaluate its performance using real-world edge systems. Results show that: (1) TinyEdge achieves rapid customization of edge systems, reducing 44.15% of development time and 67.79% of lines of code on average compared with the state-of-the-art edge computing platforms; (2) TinyEdge builds compact modules and optimizes the latent circular dependency detection and message routing efficiency; (3) TinyEdge performance estimation has low absolute errors in various settings.  © 2023 Association for Computing Machinery.",cloud-native; Edge computing; low-code development,Codes (symbols); Computation theory; Real time systems; Cloud-native; Code development; Complex task; Development frameworks; Edge computing; Low-code development; Performance; Software compatibility; Third party software; Time-consuming tasks; Edge computing
Breaking Bad: Quantifying the Addiction of Web Elements to JavaScript,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152614756&doi=10.1145%2f3579846&partnerID=40&md5=51ce82c42c88abbb4626e03c478ee491,"While JavaScript established itself as a cornerstone of the modern web, it also constitutes a major tracking and security vector, thus raising critical privacy and security concerns. In this context, some browser extensions propose to systematically block scripts reported by crowdsourced trackers lists. However, this solution heavily depends on the quality of these built-in lists, which may be deprecated or incomplete, thus exposing the visitor to unknown trackers. In this article, we explore a different strategy by investigating the benefits of disabling JavaScript in the browser. More specifically, by adopting such a strict policy, we aim to quantify the JavaScript addiction of web elements composing a web page through the observation of web breakages. As there is no standard mechanism for detecting such breakages, we introduce a framework to inspect several page features when blocking JavaScript, that we deploy to analyze 6,384 pages, including landing and internal web pages. We discover that 43% of web pages are not strictly dependent on JavaScript and that more than 67% of pages are likely to be usable as long as the visitor only requires the content from the main section of the page, for which the user most likely reached the page, while reducing the number of tracking requests by 85% on average. Finally, we discuss the viability of currently browsing the web without JavaScript and detail multiple incentives for websites to be kept usable without JavaScript.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",JavaScript; page breakage; Web privacy,High level languages; Blockings; Breakings; Javascript; Most likely; Page breakage; Privacy and security; Web privacy; Web-page; Websites
The Doge of Wall Street: Analysis and Detection of Pump and Dump Cryptocurrency Manipulations,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152633093&doi=10.1145%2f3561300&partnerID=40&md5=ff7e875a5b14cf63a71c46100f71c13e,"Cryptocurrencies are increasingly popular. Even people who are not experts have started to invest in these assets, and nowadays, cryptocurrency exchanges process transactions for over 100 billion US dollars per month. Despite this, many cryptocurrencies have low liquidity and are highly prone to market manipulation. This paper performs an in-depth analysis of two market manipulations organized by communities over the Internet: The pump and dump and the crowd pump. The pump and dump scheme is a fraud as old as the stock market. Now, it has new vitality in the loosely regulated market of cryptocurrencies. Groups of highly coordinated people systematically arrange this scam, usually on Telegram and Discord. We monitored these groups for more than 3 years, detecting around 900 individual events. We report on three case studies related to pump and dump groups. We leverage our unique dataset of the verified pump and dumps to build a machine learning model able to detect a pump and dump in 25 seconds from the moment it starts, achieving the results of 94.5% of F1-score. Then, we move on to the crowd pump, a new phenomenon that hit the news in the first months of 2021, when a Reddit community inflated the price of the GameStop stocks (GME) by over 1,900% on Wall Street, the world's largest stock exchange. Later, other Reddit communities replicated the operation on the cryptocurrency markets. The targets were DogeCoin (DOGE) and Ripple (XRP). We reconstruct how these operations developed and discuss differences and analogies with the standard pump and dump. We believe this study helps understand a widespread phenomenon affecting cryptocurrency markets. The detection algorithms we develop effectively detect these events in real-time and helps investors stay out of the market when these frauds are in action.  © 2023 Association for Computing Machinery.",cryptocurrencies; fraud detection; Pump and dump,Crime; Electronic money; Financial markets; Investments; Case-studies; Exchange process; F1 scores; Fraud detection; In-depth analysis; Machine learning models; Market manipulation; Pump and dump; US dollar; Wall streets; Pumps
Knowledge Graph Construction with a Façade: A Unified Method to Access Heterogeneous Data Sources on the Web,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152626687&doi=10.1145%2f3555312&partnerID=40&md5=b8195c7562b79438ee75a202af7c7b17,"Data integration is the dominant use case for RDF Knowledge Graphs. However, Web resources come in formats with weak semantics (for example, CSV and JSON), or formats specific to a given application (for example, BibTex, HTML, and Markdown). To solve this problem, Knowledge Graph Construction (KGC) is gaining momentum due to its focus on supporting users in transforming data into RDF. However, using existing KGC frameworks result in complex data processing pipelines, which mix structural and semantic mappings, whose development and maintenance constitute a significant bottleneck for KG engineers. Such frameworks force users to rely on different tools, sometimes based on heterogeneous languages, for inspecting sources, designing mappings, and generating triples, thus making the process unnecessarily complicated. We argue that it is possible and desirable to equip KG engineers with the ability of interacting with Web data formats by relying on their expertise in RDF and the well-established SPARQL query language [2]. In this article, we study a unified method for data access to heterogeneous data sources with Facade-X, a meta-model implemented in a new data integration system called SPARQL Anything. We demonstrate that our approach is theoretically sound, since it allows a single meta-model, based on RDF, to represent data from (a) any file format expressible in BNF syntax, as well as (b) any relational database. We compare our method to state-of-the-art approaches in terms of usability (cognitive complexity of the mappings) and general performance. Finally, we discuss the benefits and challenges of this novel approach by engaging with the reference user community.  © 2023 Copyright held by the owner/author(s).",meta-model; RDF; re-engineering; SPARQL,Data integration; Knowledge graph; Metadata; Query languages; Query processing; Resource Description Framework (RDF); Semantics; Graph construction; Heterogeneous data sources; Knowledge graphs; Meta model; Metamodeling; RDF; Re-engineering; SPARQL; Unified method; Web resources; Mapping
Federated Route Leak Detection in Inter-domain Routing with Privacy Guarantee,2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152623335&doi=10.1145%2f3561051&partnerID=40&md5=1efc2cf7b4bbc9304ae51925fb7700f4,"In the inter-domain network, route leaks can disrupt the Internet traffic and cause large outages. The accurate detection of route leaks requires the sharing of AS business relationship information. However, the business relationship information between ASes is confidential. ASes are usually unwilling to reveal this information to the other ASes, especially their competitors. In this paper, we propose a method named FL-RLD to detect route leaks while maintaining the privacy of business relationships between ASes by using a blockchain-based federated learning framework, where ASes can collaboratively train a global detection model without directly disclosing their specific business relationships. To mitigate the lack of ground-truth validation data in route leaks, FL-RLD provides a self-validation scheme by labeling AS triples with local routing policies. We evaluate FL-RLD under a variety of datasets including imbalanced and balanced datasets, and examine different deployment strategies of FL-RLD under different topologies. According to the results, FL-RLD performs better in detecting route leaks than the single AS detection, whether the datasets are balanced or imbalanced. Additionally, the results indicate that selecting ASes with the most peers to first deploy FL-RLD brings more significant benefits in detecting route leaks than selecting ASes with the most providers and customers.  © 2023 Association for Computing Machinery.",BGP security; federated learning; route leak detection,Border Gateway Protocol; BGP security; Block-chain; Business relationships; Federated learning; Inter-domain; Interdomain Routing; Internet traffic; Leaks detections; Learning frameworks; Route leak detection; Leak detection
Tripartite Transmitting Methodology for Intermittently Connected Mobile Network (ICMN),2023,ACM Transactions on Internet Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151525237&doi=10.1145%2f3433545&partnerID=40&md5=449a599dabede5e32ab4e1b4d8230b1b,"Mobile network is a collection of devices with dynamic behavior where devices keep moving, which may lead to the network track to be connected or disconnected. This type of network is called Intermittently Connected Mobile Network (ICMN). The ICMN network is designed by splitting the region into 'n' regions, ensuring it is a disconnected network. This network holds the same topological structure with mobile devices in it. This type of network routing is a challenging task. Though research keeps deriving techniques to achieve efficient routing in ICMN such as Epidemic, Flooding, Spray, copy case, Probabilistic, and Wait, these derived techniques for routing in ICMN are wise with higher packet delivery ratio, minimum latency, lesser overhead, and so on. A new routing schedule has been enacted comprising three optimization techniques such as Privacy-Preserving Ant Routing Protocol (PPARP), Privacy-Preserving Routing Protocol (PPRP), and Privacy-Preserving Bee Routing Protocol (PPBRP). In this paper, the enacted technique gives an optimal result following various network characteristics. Algorithms embedded with productive routing provide maximum security. Results are pointed out by analysis taken from spreading false devices into the network and its effectiveness at worst case. This paper also aids with the comparative results of enacted algorithms for secure routing in ICMN. © 2022 Association for Computing Machinery.",authentication sequence; ICMN; PPARP; PPBRP; PPRP,Mobile telecommunication systems; Privacy-preserving techniques; Wireless networks; Authentication sequence; Intermittently connected mobile networks; Privacy preserving; Privacy-preserving ant routing protocol; Privacy-preserving bee routing protocol; Privacy-preserving routing protocol; Routing-protocol; Routing protocols
