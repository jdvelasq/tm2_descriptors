Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
A novel continuous and structural VAR modeling approach and its application to reactor noise analysis,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952948724&doi=10.1145%2f2710025&partnerID=40&md5=deee32ff7c5caeceaabac1a398205ef9,"A vector autoregressive model in discrete time domain (DVAR) is often used to analyze continuous time, multivariate, linear Markov systems through their observed time series data sampled at discrete timesteps. Based on previous studies, the DVAR model is supposed to be a noncanonical representation of the system, that is, it does not correspond to a unique system bijectively. However, in this article, we characterize the relations of the DVAR model with its corresponding Structural Vector AR (SVAR) and Continuous Time Vector AR (CTVAR) models through a finite difference method across continuous and discrete time domain. We further clarify that the DVARmodel of a continuous time,multivariate, linearMarkov system is canonical under a highly generic condition. Our analysis shows that we can uniquely reproduce its SVAR and CTVAR models from the DVAR model. Based on these results, we propose a novel Continuous and Structural Vector Autoregressive (CSVAR) modeling approach to derive the SVAR and the CTVAR models from their DVAR model empirically derived from the observed time series of continuous time linear Markov systems. We demonstrate its superior performance through some numerical experiments on both artificial and real-world data. © 2015 ACM.",Canonicality; Continuous time linearMarkov system; CTVAR model; Nuclear reactor noise analysis; SVAR model; VAR model,Finite difference method; Nuclear reactors; Reactor cores; Structural analysis; Time domain analysis; Time series; Transients; Value engineering; Vectors; Canonicality; Continuous-time; Reactor noise analysis; SVAR models; VAR models; Continuous time systems
Learning perceptual causality from video,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952901542&doi=10.1145%2f2809782&partnerID=40&md5=85957a0867f384a09b6067652ce840ce,"Perceptual causality is the perception of causal relationships from observation. Humans, even as infants, form such models from observation of the world around them [Saxe and Carey 2006]. For a deeper understanding, the computer must make similar models through the analogous form of observation: video. In this article, we provide a framework for the unsupervised learning of this perceptual causal structure from video. Ourmethod takes action and object status detections as input and uses heuristics suggested by cognitive science research to produce the causal links perceived between them. We greedily modify an initial distribution featuring independence between potential causes and effects by adding dependencies that maximize information gain. We compile the learned causal relationships into a Causal And-Or Graph, a probabilistic and-or representation of causality that adds a prior to causality. Validated against human perception, experiments show that our method correctly learns causal relations, attributing status changes of objects to causing actions amid irrelevant actions. Our method outperforms Hellinger's Χ2-statistic by considering hierarchical action selection, and outperforms the treatment effect by discounting coincidental relationships. © 2015 ACM.",Causal induction; Information projection; Perceptual causality,Causal induction; Causal relationships; Cognitive science; Human perception; Information projections; Perceptual causality; Status detections; Treatment effects
Semiparametric inference of the complier average causal effect with nonignorable missing outcomes,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952907546&doi=10.1145%2f2668135&partnerID=40&md5=9707e0f2a9111af2531f0ca374441de5,"Noncompliance and missing data often occur in randomized trials, which complicate the inference of causal effects. When both noncompliance and missing data are present, previous papers proposed moment and maximum likelihood estimators for binary and normally distributed continuous outcomes under the latent ignorable missing data mechanism. However, the latent ignorable missing data mechanism may be violated in practice, because the missing data mechanism may depend directly on the missing outcome itself. Under noncompliance and an outcome-dependent nonignorable missing data mechanism, previous studies showed the identifiability of complier average causal effect for discrete outcomes. In this article, we study the semiparametric identifiability and estimation of complier average causal effect in randomized clinical trials with both all-or-none noncompliance and outcome-dependent nonignorable missing continuous outcomes, and propose a two-stepmaximum likelihood estimator in order to eliminate the infinite dimensional nuisance parameter. Our method does not need to specify a parametric form for the missing data mechanism. We also evaluate the finite sample property of our method via extensive simulation studies and sensitivity analysis, with an application to a double-blinded psychiatric clinical trial. © 2015 ACM.",Causal inference; Instrumental variable; Missing not at random; Noncompliance; Outcome-dependent missing; Principal stratification,Maximum likelihood; Medical applications; Normal distribution; Sampling; Sensitivity analysis; Causal inferences; Instrumental variables; Missing not at random; Noncompliance; Outcome-dependent missing; Maximum likelihood estimation
A causal approach to the study of TCP performance,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952937244&doi=10.1145%2f2770878&partnerID=40&md5=22e43bc667566b9d5607ddfda1107971,"Communication networks are complex systems whose operation relies on a large number of components that work together to provide services to end users. As the quality of these services depends on different parameters, understanding how each of them impacts the final performance of a service is a challenging but important problem. However, intervening on individual factors to evaluate the impact of the different parameters is often impractical due to the high cost of intervention in a network. It is, therefore, desirable to adopt a formal approach to understand the role of the different parameters and to predict how a change in any of these parameters will impact performance. The approach of causality pioneered by J. Pearl provides a powerful framework to investigate these questions. Most of the existing theory is non-parametric and does not make any assumption on the nature of the system under study. However, most of the implementations of causal model inference algorithms and most of the examples of usage of a causal model to predict intervention rely on assumptions such linearity, normality, or discrete data. In this article, we present a methodology to overcome the challenges of working with real-world data and extend the application of causality to complex systems in the area of telecommunication networks, for which assumptions of normality, linearity and discrete data do no hold. Specifically, we study the performance of TCP, which is the prevalent protocol for reliable end-to-end transfer in the Internet. Analytical models of the performance of TCP exist, but they take into account the state of network only and disregard the impact of the application at the sender and the receiver, which often influences TCP performance. To address this point, we take as application the file transfer protocol (FTP), which uses TCP for reliable transfer. Studying a well-understood protocol such as TCP allows us to validate our approach and compare its results to previous studies. We first present and evaluate our methodology using TCP traffic obtained via network emulation, which allows us to experimentally validate the prediction of an intervention. We then apply the methodology to real-world TCP traffic sent over the Internet. Throughout the article, we compare the causal approach for studying TCP performance to other approaches such as analytical modeling or simulation and and show how they can complement each other. © 2015 ACM.",TCP; Telecommunication networks,Analytical models; Complex networks; Forecasting; Human computer interaction; Inference engines; Internet; Internet protocols; Telecommunication networks; File transfer protocols; Formal approach; Impact performance; Individual factors; Network emulation; Number of components; Reliable transfer; TCP; Transmission control protocol
Event extraction using structured learning and rich domain knowledge: Application across domains and data sources,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952946870&doi=10.1145%2f2801131&partnerID=40&md5=67ccc21d3b964b668a46bd1d4715fe3c,"We consider the task of record extraction from text documents, where the goal is to automatically populate the fields of target relations, such as scientific seminars or corporate acquisition events. There are various inferences involved in the record-extraction process, including mention detection, unification, and field assignments. We use structured learning to find the appropriate field-value assignments. Unlike previous works, the proposed approach generates feature-rich models that enable the modeling of domain semantics and structural coherence at all levels and across fields. Given labeled examples, such an approach can, for instance, learn likely event durations and the fact that start times should come before end times. While the inference space is large, effective learning is achieved using a perceptron-style method and simple, greedy beam decoding. A main focus of this article is on practical aspects involved in implementing the proposed framework for real-world applications. We argue and demonstrate that this approach is favorable in conditions of data shift, a real-world setting in which models learned using a limited set of labeled examples are applied to examples drawn from a different data distribution. Much of the framework's robustness is attributed to the modeling of domain knowledge. We describe design and implementation details for the case study of seminar event extraction from email announcements, and discuss design adaptations across different domains and text genres. © 2015 ACM.",Beam search; Domain knowledge; Information extraction; Perceptron; Structured learning; Template filling,Information analysis; Information retrieval; Neural networks; Semantics; Beam search; Design adaptations; Design and implementations; Domain knowledge; Effective learning; Extraction process; Structural coherence; Structured learning; Data mining
Gaussian processes for independence tests with non-iid data in causal inference,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952932167&doi=10.1145%2f2806892&partnerID=40&md5=0de4891605312ea9e4f6c9b7c1502d7b,"In applied fields, practitioners hoping to apply causal structure learning or causal orientation algorithms face an important question: which independence test is appropriate for my data? In the case of real-valued iid data, linear dependencies, and Gaussian error terms, partial correlation is sufficient. But once any of these assumptions is modified, the situation becomes more complex. Kernel-based tests of independence have gained popularity to deal with nonlinear dependencies in recent years, but testing for conditional independence remains a challenging problem. We highlight the important issue of non-iid observations: when data are observed in space, time, or on a network, ""nearby"" observations are likely to be similar. This fact biases estimates of dependence between variables. Inspired by the success of Gaussian process regression for handling non-iid observations in a wide variety of areas and by the usefulness of the Hilbert- Schmidt Independence Criterion (HSIC), a kernel-based independence test, we propose a simple framework to address all of these issues: first, use Gaussian process regression to control for certain variables and to obtain residuals. Second, use HSIC to test for independence. We illustrate this on two classic datasets, one spatial, the other temporal, that are usually treated as iid. We show how properly accounting for spatial and temporal variation can lead to more reasonable causal graphs. We also show how highly structured data, like images and text, can be used in a causal inference framework using a novel structured input/output Gaussian process formulation. We demonstrate this idea on a dataset of translated sentences, trying to predict the source language. © 2015 ACM.",Causal inference; causal structure learning; Gaussian process; Reproducing kernel Hilbert space,Algorithms; Complex networks; Gaussian noise (electronic); Testing; Causal inferences; Causal structure learning; Conditional independences; Gaussian process regression; Gaussian Processes; Hilbert-schmidt independence criterions; Reproducing Kernel Hilbert spaces; Spatial and temporal variation; Gaussian distribution
On estimation of functional causal models: General results and application to the post-nonlinear causal model,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952761829&doi=10.1145%2f2700476&partnerID=40&md5=3a55eac3f848e35eccd15b2b5830bc90,"Compared to constraint-based causal discovery, causal discovery based on functional causal models is able to identify the whole causal model under appropriate assumptions [Shimizu et al. 2006; Hoyer et al. 2009; Zhang and Hyvärinen 2009b]. Functional causal models represent the effect as a function of the direct causes together with an independent noise term. Examples include the linear non-Gaussian acyclic model (LiNGAM), nonlinear additive noisemodel, and post-nonlinear (PNL) model. Currently, there are two ways to estimate the parameters in the models: dependence minimization and maximum likelihood. In this article, we show that for any acyclic functional causal model, minimizing the mutual information between the hypothetical cause and the noise term is equivalent to maximizing the data likelihood with a flexible model for the distribution of the noise term. We then focus on estimation of the PNL causal model and propose to estimate it with the warped Gaussian process with the noise modeled by the mixture of Gaussians. As a Bayesian nonparametric approach, it outperforms the previous one based on mutual information minimization with nonlinear functions represented by multilayer perceptrons; we also show that unlike the ordinary regression, estimation results of the PNL causal model are sensitive to the assumption on the noise distribution. Experimental results on both synthetic and real data support our theoretical claims. © 2015 ACM.",Causal discovery; Functional causal model; Maximum likelihood; Post-nonlinear causal model; Statistical independence,Gaussian noise (electronic); Maximum likelihood; Maximum likelihood estimation; Nonlinear analysis; Causal discovery; Causal model; Mixture of Gaussians; Nonlinear functions; Nonparametric approaches; Postnonlinear model; Statistical independence; Synthetic and real data; Additive noise
Causal discovery on discrete data with extensions to mixture model,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952762595&doi=10.1145%2f2700477&partnerID=40&md5=54b66908ebbd1cdfa548651e6943f18f,"In this article, we deal with the causal discovery problem on discrete data. First, we present a causal discovery method for traditional additive noise models that identifies the causal direction by analyzing the supports of the conditional distributions. Then, we present a causal mixture model to address the problem that the function transforming cause to effect varies across the observations. We propose a novel method called Support Analysis (SA) for causal discovery with the mixture model. Experiments using synthetic and real data are presented to demonstrate the performance of our proposed algorithm. © 2015 ACM.",Causal discovery; Discrete; Mixture,Mixtures; Causal discovery; Conditional distribution; Discrete; Discrete data; Mixture model; Support analysis; Synthetic and real data; TO effect; Additive noise
Bounds on direct and indirect effects of treatment on a continuous endpoint,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952897885&doi=10.1145%2f2668134&partnerID=40&md5=e034346dccd403c5d48ba1df4423c26f,"Direct effect of a treatment variable on an endpoint variable and indirect effect through a mediate variable are important concepts for understanding a causal mechanism. However, the randomized assignment of treatment is not sufficient for identifying the direct and indirect effects, and extra assumptions and conditions are required, such as the sequential ignorability assumption without unobserved confounders or the sequential potential ignorability assumption. But these assumptions may not be credible in many applications. In this article, we consider the bounds on controlled direct effect, natural direct effect, and natural indirect effect without these extra assumptions. Cai et al. [2008] presented the bounds for the case of a binary endpoint, and we extend their results to the general case for an arbitrary endpoint. © 2015 ACM.",Bound; Causal inference; Direct and indirect effects; Mediation analysis,Bound; Causal inferences; Ignorability; Indirect effects; Mediation analysis
From observational studies to causal rule mining,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948407567&doi=10.1145%2f2746410&partnerID=40&md5=1c7a39cd279943194787cfa29e53ddc4,"Randomised controlled trials (RCTs) are the most effective approach to causal discovery, but in many circumstances it is impossible to conduct RCTs. Therefore, observational studies based on passively observed data are widely accepted as an alternative to RCTs. However, in observational studies, prior knowledge is required to generate the hypotheses about the cause-effect relationships to be tested, and hence they can only be applied to problems with available domain knowledge and a handful of variables. In practice, many datasets are of high dimensionality, which leaves observational studies out of the opportunities for causal discovery from such a wealth of data sources. In another direction, many efficient data mining methods have been developed to identify associations among variables in large datasets. The problem is that causal relationships imply associations, but the reverse is not always true. However, we can see the synergy between the two paradigms here. Specifically, association rule mining can be used to deal with the high-dimensionality problem, whereas observational studies can be utilised to eliminate noncausal associations. In this article, we propose the concept of causal rules (CRs) and develop an algorithm for mining CRs in large datasets. We use the idea of retrospective cohort studies to detect CRs based on the results of association rule mining. Experiments with both synthetic and real-world datasets have demonstrated the effectiveness and efficiency of CR mining. In comparison with the commonly used causal discovery methods, the proposed approach generally is faster and has better or competitive performance in finding correct or sensible causes. It is also capable of finding a cause consisting of multiple variables - a feature that other causal discovery methods do not possess. 2015 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Association rule; Causal discovery; Cohort study; Odds ratio,Association rules; Large dataset; Causal discovery; Causal relationships; Cause-effect relationships; Cohort studies; Competitive performance; Effective approaches; Effectiveness and efficiencies; Odds ratios; Data mining
Sharp bounds on survivor average causal effects when the outcome is binary and truncated by death,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948393429&doi=10.1145%2f2700498&partnerID=40&md5=62483c10d920e4b7707c9cd0c3376a6e,"In randomized trials with follow-up, outcomes may be undefined for individuals who die before the follow-up is complete. In such settings, Frangakis and Rubin [2002] proposed the ""principal stratum effect"" or ""Survivor Average Causal Effect"" (SACE), which is a fair treatment comparison in the subpopulation that would have survived under either treatment arm. Many of the existing results for estimating the SACE are difficult to carry out in practice. In this article, when the outcome is binary, we apply the symbolic Balke-Pearl linear programming method to derive simple formulas for the sharp bounds on the SACE under the monotonicity assumption commonly used by many researchers. © 2015 ACM.",Linear programming; Randomized trials; Survivor average causal effect; Truncation,Follow up; Monotonicity; Randomized trial; Sharp bounds; Survivor average causal effect; Truncation; Linear programming
Gestures à go go: Authoring synthetic human-like stroke gestures using the kinematic theory of rapid movements,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948398776&doi=10.1145%2f2799648&partnerID=40&md5=ffaaa4e57cc48daa841b020a9a326a8d,"Training a high-quality gesture recognizer requires providing a large number of examples to enable good performance on unseen, future data. However, recruiting participants, data collection, and labeling, etc., necessary for achieving this goal are usually time consuming and expensive. Thus, it is important to investigate how to empower developers to quickly collect gesture samples for improving UI usage and user experience. In response to this need, we introduce Gestures à Go Go (G3), a web service plus an accompanying web application for bootstrapping stroke gesture samples based on the kinematic theory of rapid human movements. The user only has to provide a gesture example once, and G3 will create a model of that gesture. Then, by introducing local and global perturbations to the model parameters, G3 generates from tens to thousands of synthetic human-like samples. Through a comprehensive evaluation, we show that synthesized gestures perform equally similar to gestures generated by human users. Ultimately, this work informs our understanding of designing better user interfaces that are driven by gestures. © 2015 ACM.",Bootstrapping; Gesture recognition; Gesture synthesis; Kinematics; Marks; Multistrokes; Multitouch; Rapid prototyping; Strokes; Symbols; Unistrokes; User interfaces,Codes (symbols); Kinematics; Rapid prototyping; User experience; User interfaces; Web services; Bootstrapping; Marks; Multi-touch; Multistrokes; Strokes; Unistrokes; Gesture recognition
Multi-keyword multi-click advertisement option contracts for sponsored search,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946226782&doi=10.1145%2f2743027&partnerID=40&md5=349c721d903d72c18b0d36147ec0b3e9,"In sponsored search, advertisement (abbreviated ad) slots are usually sold by a search engine to an advertiser through an auction mechanism in which advertisers bid on keywords. In theory, auction mechanisms have many desirable economic properties. However, keyword auctions have a number of limitations including: the uncertainty in payment prices for advertisers; the volatility in the search engine's revenue; and the weak loyalty between advertiser and search engine. In this article, we propose a special ad option that alleviates these problems. In our proposal, an advertiser can purchase an option from a search engine in advance by paying an upfront fee, known as the option price. The advertiser then has the right, but no obligation, to purchase among the prespecified set of keywords at the fixed cost-per-clicks (CPCs) for a specified number of clicks in a specified period of time. The proposed option is closely related to a special exotic option in finance that contains multiple underlying assets (multi-keyword) and is also multi-exercisable (multi-click). This novel structure hasmany benefits: advertisers can have reduced uncertainty in advertising; the search engine can improve the advertisers' loyalty aswell as obtain a stable and increased expected revenue over time. Since the proposed ad option can be implemented in conjunction with the existing keyword auctions, the option price and corresponding fixed CPCs must be set such that there is no arbitrage between the two markets. Option pricing methods are discussed and our experimental results validate the development. Compared to keyword auctions, a search engine can have an increased expected revenue by selling an ad option. Copyright © 2015 ACM.",Exotic option; Pricing model; Revenue analysis; Sponsored search,Commerce; Sales; Search engines; Auction mechanisms; Exotic option; Expected revenue; Novel structures; Option contracts; Option pricing; Pricing models; Sponsored searches; Costs
A hybrid background subtraction method with background and foreground candidates detection,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946211569&doi=10.1145%2f2746409&partnerID=40&md5=ea9be1953dceb3fd5de8ac6745bd8662,"Background subtraction for motion detection is often used in video surveillance systems. However, difficulties in bootstrapping restrict its development. This article proposes a novel hybrid background subtraction technique to solve this problem. For performance improvement of background subtraction, the proposed technique not only quickly initializes the background model but also eliminates unnecessary regions containing only background pixels in the object detection process. Furthermore, an embodiment based on the proposed technique is also presented. Experimental results verify that the proposed technique allows for reduced execution time as well as improvement of performance as evaluated by Recall, Precision, F1, and Similarity metrics when used with state-of-the-art background subtraction methods. Copyright © 2015 ACM.",Background candidates; Background subtraction; Foreground candidates; Motion detection; Video surveillance,Object detection; Security systems; Background candidates; Background subtraction; Foreground candidates; Motion detection; Video surveillance; Motion analysis
Nonnegative multiresolution representation-based texture image classification,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946232826&doi=10.1145%2f2738050&partnerID=40&md5=a97f9195e8e5f5e16f7f3d6cd73a69dc,"Effective representation of image texture is important for an image-classification task. Statistical modelling in wavelet domains has been widely used to image texture representation. However, due to the intraclass complexity and interclass diversity of textures, it is hard to use a predefined probability distribution function to fit adaptively all wavelet subband coefficients of different textures. In this article, we propose a novel modelling approach, Heterogeneous and Incrementally Generated Histogram (HIGH), to indirectly model the wavelet coefficients by use of four local features in wavelet subbands. By concatenating all the HIGHs in allwavelet subbands of a texture, we can construct a nonnegative multiresolution vector (NMV) to represent a texture image. Considering the NMV's high dimensionality and nonnegativity, we further propose a Hessian regularized discriminative nonnegative matrix factorization to compute a low-dimensional basis of the linear subspace of NMVs. Finally, we present a texture classification approach by projecting NMVs on the lowdimensional basis. Experimental results show that our proposed texture classification method outperforms seven representative approaches. Copyright ï¿½ 2015 ACM.",Hessian regularization; Histogram; Manifold regularization; Nonnegative matrix factorization; Texture classification,Distribution functions; Factorization; Graphic methods; Image texture; Matrix algebra; Probability distributions; Hessian regularization; Histogram; Manifold regularizations; Nonnegative matrix factorization; Texture classification; Image classification
Metastrategies in large-scale bargaining settings,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947057086&doi=10.1145%2f2774224&partnerID=40&md5=b0f1928670a589ffacbe3662ab0c41df,"This article presents novel methods for representing and analyzing a special class of multiagent bargaining settings that feature multiple players, large action spaces, and a relationship among players' goals, tasks, and resources. We show how to reduce these interactions to a set of bilateral normal-form games in which the strategy space is significantly smaller than the original settingswhile still preserving much of their structural relationship. The method is demonstrated using the Colored Trails (CT) framework, which encompasses a broad family of games and has been used in many past studies. We define a set of heuristics (metastrategies) in multiplayer CT games that make varying assumptions about players' strategies, such as boundedly rational play and social preferences. We show how these CT settings can be decomposed into canonical bilateral games such as the Prisoners' Dilemma, Stag Hunt, and Ultimatum games in a way that significantly facilitates their analysis. We demonstrate the feasibility of this approach in separate CT settings involving one-shot and repeated bargaining scenarios, which are subsequently analyzed using evolutionary game-theoretic techniques. We provide a set of necessary conditions for CT games for allowing this decomposition. Our results have significance for multiagent systems researchers in mapping large multiplayer CT task settings to smaller, well-known bilateral normal-form games while preserving some of the structure of the original setting. © 2015 ACM.",Decision making; Multiagent systems; Negotiation,Decision making; Game theory; Evolutionary games; Negotiation; Normal form games; Prisoners' Dilemma; Social preference; Strategy space; Structural relationship; Ultimatum game; Multi agent systems
On learning prediction models for tourists paths,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946204460&doi=10.1145%2f2766459&partnerID=40&md5=3939a61bacdcc4c1f47ec1b82a80396c,"In this article, we tackle the problem of predicting the ""next"" geographical position of a tourist, given her history (i.e., the prediction is done accordingly to the tourist's current trail) by means of supervised learning techniques, namely Gradient Boosted Regression Trees and Ranking SVM. The learning is done on the basis of an object space represented by a 68-dimension feature vector specifically designed for tourism-related data. Furthermore, we propose a thorough comparison of several methods that are considered state-of-theart in recommender and trail prediction systems for tourism, as well as a popularity baseline. Experiments show that the methods we propose consistently outperform the baselines and provide strong evidence of the performance and robustness of our solutions. Copyright ï¿½ 2015 ACM.",Geographical PoI prediction; Learning to rank,Supervised learning; Vector spaces; Boosted regression trees; Feature vectors; Geographical positions; Learning to rank; Object space; Prediction model; Prediction systems; Ranking SVM; Forecasting
On-device Mobile Landmark Recognition using binarized descriptor with multifeature fusion,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947075063&doi=10.1145%2f2795234&partnerID=40&md5=f198480ddf6d8a11797e41ea10142819,"Along with the exponential growth of high-performance mobile devices, on-device Mobile Landmark Recognition (MLR) has recently attracted increasing research attention. However, the latency and accuracy of automatic recognition remain as bottlenecks against its real-world usage. In this article, we introduce a novel framework that combines interactive image segmentation with multifeature fusion to achieve improved MLR with high accuracy. First, we propose an effective vector binarization method to reduce the memory usage of image descriptors extracted on-device, which maintains comparable recognition accuracy to the original descriptors. Second, we design a location-aware fusion algorithm that can fuse multiple visual features into a compact yet discriminative image descriptor to improve on-device efficiency. Third, a user-friendly interaction scheme is developed that enables interactive foreground/background segmentation to largely improve recognition accuracy. Experimental results demonstrate the effectiveness of the proposed algorithms for on-device MLR applications. © 2015 ACM.",Binarization; Feature fusion; Mobile landmark recognition; On-device; User interaction,Image enhancement; Image segmentation; Binarizations; Feature fusion; Mobile landmark recognition; On-device; User interaction; Image fusion
Local structure-based sparse representation for face recognition,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947060427&doi=10.1145%2f2733383&partnerID=40&md5=d2f0ac5272f60aaf88a11afc74cec455,"This article presents a simple yet effective face recognition method, called local structure-based sparse representation classification (LS-SRC). Motivated by the ""divide-and-conquer"" strategy, we first divide the face into local blocks and classify each local block, then integrate all the classification results to make the final decision. To classify each local block, we further divide each block into several overlapped local patches and assume that these local patches lie in a linear subspace. This subspace assumption reflects the local structure relationship of the overlapped patches, making sparse representation-based classification (SRC) feasible even when encountering the single-sample-per-person (SSPP) problem. To lighten the computing burden of LS-SRC, we further propose the local structure-based collaborative representation classification (LS-CRC). Moreover, the performance of LS-SRC and LS-CRC can be further improved by using the confusionmatrix of the classifier. Experimental results on four public face databases show that our methods not only generalize well to SSPP problem but also have strong robustness to occlusion; little pose variation; and the variations of expression, illumination, and time. © 2015 ACM.",Bayesian inference; Collaborative representation; Confusion matrix; Face recognition; Local structure; Single-sample-per-person problem; Sparse representation,Bayesian networks; Inference engines; Knowledge representation; Bayesian inference; Collaborative representations; Confusion matrices; Local structure; Single sample; Sparse representation; Face recognition
Analysis of the impact of a tag recommendation system in a real-world Folksonomy,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942782344&doi=10.1145%2f2743026&partnerID=40&md5=28aa27e54269109a81df0d325cacf81f,"Collaborative tagging systems have emerged as a successful solution for annotating contributed resources to online sharing platforms, facilitating searching, browsing, and organizing their contents. To aid users in the annotation process, several tag recommendation methods have been proposed. It has been repeatedly hypothesized that these methods should contribute to improving annotation quality and reducing the cost of the annotation process. It has been also hypothesized that these methods should contribute to the consolidation of the vocabulary of collaborative tagging systems. However, to date, no empirical and quantitative result supports these hypotheses. In this work, we deeply analyze the impact of a tag recommendation system in the folksonomy of Freesound, a real-world and large-scale online sound sharing platform. Our results suggest that tag recommendation effectively increases vocabulary sharing among users of the platform. In addition, tag recommendation is shown to contribute to the convergence of the vocabulary as well as to a partial increase in the quality of annotations. However, according to our analysis, the cost of the annotation process does not seem to be effectively reduced. Our work is relevant to increase our understanding about the nature of tag recommendation systems and points to future directions for the further development of those systems and their analysis. © 2015 ACM.",Experimentation; Measurement; Verification,Cost benefit analysis; Cost reduction; Measurement; Online systems; Thesauri; Verification; Collaborative tagging; Experimentation; Folksonomies; Quantitative result; Real-world; Sharing platforms; Tag recommendations; Recommender systems
Where2Stand: A human position recommendation system for souvenir photography,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946044838&doi=10.1145%2f2770879&partnerID=40&md5=10254740ff6c35466c61cb2476365cd8,"People often take photographs at tourist sites and these pictures usually have two main elements: a person in the foreground and scenery in the background. This type of ""souvenir photo"" is one of the most common photos clicked by tourists. Although algorithms that aid a user-photographer in taking a well-composed picture of a scene exist [Ni et al. 2013], few studies have addressed the issue of properly positioning human subjects in photographs. In photography, the common guidelines of composing portrait images exist. However, these rules usually do not consider the background scene. Therefore, in this article, we investigate human-scenery positional relationships and construct a photographic assistance system to optimize the position of human subjects in a given background scene, thereby assisting the user in capturing high-quality souvenir photos. We collect thousands of well-composed portrait photographs to learn human-scenery aesthetic composition rules. In addition, we define a set of negative rules to exclude undesirable compositions. Recommendation results are achieved by combining the first learned positive rule with our proposed negative rules. We implement the proposed system on an Android platform in a smartphone. The system demonstrates its efficacy by producing well-composed souvenir photos. © 2015 ACM 2157-6904/2015/09-ART9 $15.00.",Human position recommendation; Human-scenery positional relationship; Photographic composition; Souvenir photography,Android platforms; Assistance system; Background scenes; Composition rule; Human position recommendation; Human subjects; Photographic composition; Positional relationship; Photography
Spatiotemporal sequential influence modeling for location recommendations: A gravity-based approach,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947059695&doi=10.1145%2f2786761&partnerID=40&md5=8bbcc79fe93935a947301d41c70f69a8,"Recommending to users personalized locations is an important feature of Location-Based Social Networks (LBSNs), which benefits userswho wish to explore new places and businesses to discover potential customers. In LBSNs, social and geographical influences have been intensively used in location recommendations. However, human movement also exhibits spatiotemporal sequential patterns, but only a few current studies consider the spatiotemporal sequential influence of locations on users' check-in behaviors. In this article, we propose a new gravity model for location recommendations, called LORE, to exploit the spatiotemporal sequential influence on location recommendations. First, LORE extracts sequential patterns from historical check-in location sequences of all users as a Location-Location Transition Graph (L2TG), and utilizes the L2TG to predict the probability of a user visiting a new location through the developed additive Markov chain that considers the effect of all visited locations in the check-in history of the user on the new location. Furthermore, LORE applies our contrived gravity model to weigh the effect of each visited location on the new location derived from the personalized attractive force (i.e., the weight) between the visited location and the new location. The gravity model effectively integrates the spatiotemporal, social, and popularity influences by estimating a power-law distribution based on (i) the spatial distance and temporal difference between two consecutive check-in locations of the same user, (ii) the check-in frequency of social friends, and (iii) the popularity of locations from all users. Finally, we conduct a comprehensive performance evaluation for LORE using three large-scale real-world datasets collected from Foursquare, Gowalla, and Brightkite. Experimental results show that LORE achieves significantly superior location recommendations compared to other state-of-the-art location recommendation techniques. © 2015 ACM.",Additive Markov chain; Gravity model; Location recommendation; Popularity influence; Social influence; Spatiotemporal sequential influence,Additives; Economic and social effects; Large dataset; Markov chains; Comprehensive performance evaluation; Gravity model; Location-based social networks; Popularity influence; Power law distribution; Recommendation techniques; Social influence; Spatiotemporal sequential influence; Location
Learning to rank from noisy data,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947065485&doi=10.1145%2f2576230&partnerID=40&md5=06b3ae5cb71f1d16817bcce79478c662,"Learning to rank, which learns the ranking function from training data, has become an emerging research area in information retrieval and machine learning. Most existing work on learning to rank assumes that the training data is clean, which is not always true, however. The ambiguity of query intent, the lack of domain knowledge, and the vague definition of relevance levels all make it difficult for common annotators to give reliable relevance labels to some documents. As a result, the relevance labels in the training data of learning to rank usually contain noise. If we ignore this fact, the performance of learning-to-rank algorithms will be damaged. In this article, we propose considering the labeling noise in the process of learning to rank and using a two-step approach to extend existing algorithms to handle noisy training data. In the first step, we estimate the degree of labeling noise for a training document. To this end, we assume that the majority of the relevance labels in the training data are reliable and we use a graphical model to describe the generative process of a training query, the feature vectors of its associated documents, and the relevance labels of these documents. The parameters in the graphical model are learned by means of maximum likelihood estimation. Then the conditional probability of the relevance label given the feature vector of a document is computed. If the probability is large, we regard the degree of labeling noise for this document as small; otherwise, we regard the degree as large. In the second step, we extend existing learning-to-rank algorithms by incorporating the estimated degree of labeling noise into their loss functions. Specifically, we give larger weights to those training documents with smaller degrees of labeling noise and smaller weights to those with larger degrees of labeling noise. As examples, we demonstrate the extensions for McRank, RankSVM, RankBoost, and RankNet. Empirical results on benchmark datasets show that the proposed approach can effectively distinguish noisy documents from clean ones, and the extended learning-to-rank algorithms can achieve better performances than baselines. © 2015 ACM.",Noisy data; Robust learning,Graphic methods; Learning algorithms; Maximum likelihood estimation; Benchmark datasets; Conditional probabilities; Generative process; Noisy data; Process of learning; Ranking functions; Robust learning; Training documents; Learning to rank
On optimizing airline ticket purchase timing,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947077164&doi=10.1145%2f2733384&partnerID=40&md5=eaa5d3f4269b20dcf5bd079e0697ab07,"Proper timing of the purchase of airline tickets is difficult even when historical ticket prices and some domain knowledge are available. To address this problem, we introduce an algorithm that optimizes purchase timing on behalf of customers and provides performance estimates of its computed action policy. Given a desired flight route and travel date, the algorithm uses machine-learning methods on recent ticket price quotes from many competing airlines to predict the future expected minimum price of all available flights. The main novelty of our algorithm lies in using a systematic feature-selection technique, which captures time dependencies in the data by using time-delayed features, and reduces the number of features by imposing a class hierarchy among the rawfeatures and pruning the features based on in-situ performance. Our algorithm achieves much closer to the optimal purchase policy than other existing decision theoretic approaches for this domain, and meets or exceeds the performance of existing feature-selection methods from the literature. Applications of our feature-selection process to other domains are also discussed. © 2015 ACM.",Airline ticket prices; Data mining; E-commerce; Feature selection; Price prediction,Air transportation; Costs; Data mining; Electronic commerce; Machine learning; Purchasing; Sales; Timing circuits; Airline tickets; Class hierarchies; Decision theoretic approach; Feature selection methods; Machine learning methods; Price prediction; Selection techniques; Systematic features; Feature extraction
Estimating a ranked list of human genetic diseases by associating phenotype-gene with gene-disease bipartite graphs,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937045147&doi=10.1145%2f2700487&partnerID=40&md5=1e004cdb94ef7341ce80e4897f73f5fc,"With vast amounts of medical knowledge available on the Internet, it is becoming increasingly practical to help doctors in clinical diagnostics by suggesting plausible diseases predicted by applying data and text mining technologies. Recently, Genome-Wide Association Studies (GWAS) have proved useful as a method for exploring phenotypic associations with diseases. However, since genetic diseases are difficult to diagnose because of their low prevalence, large number, and broad diversity of symptoms, genetic disease patients are often misdiagnosed or experience long diagnostic delays. In this article, we propose a method for ranking genetic diseases for a set of clinical phenotypes. In this regard, we associate a phenotype-gene bipartite graph (PGBG) with a gene-disease bipartite graph (GDBG) by producing a phenotype-disease bipartite graph (PDBG), and we estimate the candidate weights of diseases. In our approach, all paths from a phenotype to a disease are explored by considering causative genes to assign a weight based on path frequency, and the phenotype is linked to the disease in a new PDBG. We introduce the Bidirectionally induced Importance Weight (BIW) prediction method to PDBG for approximating the weights of the edges of diseases with phenotypes by considering link information from both sides of the bipartite graph. The performance of our system is compared to that of other known related systems by estimating Normalized Discounted Cumulative Gain (NDCG), Mean Average Precision (MAP), and Kendall's tau metrics. Further experiments are conducted with well-known TF·IDF, BM25, and Jenson-Shannon divergence as baselines. The result shows that our proposed method outperforms the known related tool Phenomizer in terms of NDCG@10, NDCG@20, MAP@10, and MAP@20; however, it performs worse than Phenomizer in terms of Kendall's tau-b metric at the top-10 ranks. It also turns out that our proposed method has overall better performance than the baseline methods. © 2015 ACM.",Bipartite; BM25; Genotype; NDCG; Phenotype,Data mining; Genes; Graph theory; Medical computing; Bipartite; BM25; Genotype; NDCG; Phenotype; Diagnosis
Online planning for large markov decision processes with hierarchical decomposition,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937904889&doi=10.1145%2f2717316&partnerID=40&md5=074052aa414947716805a31c28d1c070,"Markov decision processes (MDPs) provide a rich framework for planning under uncertainty. However, exactly solving a large MDP is usually intractable due to the ""curse of dimensionality"" - the state space grows exponentially with the number of state variables. Online algorithms tackle this problem by avoiding computing a policy for the entire state space. On the other hand, since online algorithm has to find a near-optimal action online in almost real time, the computation time is often very limited. In the context of reinforcement learning, MAXQ is a value function decomposition method that exploits the underlying structure of the original MDP and decomposes it into a combination of smaller subproblems arranged over a task hierarchy. In this article, we present MAXQ-OP - a novel online planning algorithm for large MDPs that utilizes MAXQ hierarchical decomposition in online settings. Compared to traditional online planning algorithms, MAXQ-OP is able to reach much more deeper states in the search tree with relatively less computation time by exploiting MAXQ hierarchical decomposition online. We empirically evaluate our algorithm in the standard Taxi domain - a common benchmark for MDPs - to show the effectiveness of our approach. We have also conducted a long-term case study in a highly complex simulated soccer domain and developed a team named WrightEagle that has won five world champions and five runners-up in the recent 10 years of RoboCup Soccer Simulation 2D annual competitions. The results in the RoboCup domain confirm the scalability of MAXQ-OP to very large domains. © 2015 ACM.",MAXQ-OP; MDP; Online planning; RoboCup,Markov processes; Reinforcement learning; Taxicabs; Curse of dimensionality; Hierarchical decompositions; Markov Decision Processes; MAXQ-OP; On-line algorithms; On-line planning; Planning under uncertainty; Value function decomposition; Football
Peacock: Learning long-tail topic features for industrial applications,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937819726&doi=10.1145%2f2700497&partnerID=40&md5=58b3fc845ff0f710dc1360284208599c,"Latent Dirichlet allocation (LDA) is a popular topic modeling technique in academia but less so in industry, especially in large-scale applications involving search engine and online advertising systems. A main underlying reason is that the topic models used have been too small in scale to be useful; for example, some of the largest LDA models reported in literature have up to 103 topics, which difficultly cover the long-tail semantic word sets. In this article, we show that the number of topics is a key factor that can significantly boost the utility of topic-modeling systems. In particular, we show that a ""big"" LDA model with at least 105 topics inferred from 109 search queries can achieve a significant improvement on industrial search engine and online advertising systems, both of which serve hundreds of millions of users.We develop a novel distributed system called Peacock to learn big LDA models from big data. The main features of Peacock include hierarchical distributed architecture, real-time prediction, and topic de-duplication. We empirically demonstrate that the Peacock system is capable of providing significant benefits via highly scalable LDA topic models for several industrial applications. © 2015 ACM.",Big data; Big topic models; Latent Dirichlet allocation; Long-tail topic features; Online advertising systems; Search engine,Big data; Distributed database systems; Marketing; Online systems; Semantics; Statistics; Distributed architecture; Large-scale applications; Latent Dirichlet allocation; Latent dirichlet allocations; Long tail; Online advertising; Real-time prediction; Topic model; Search engines
Soter: Smart bracelets for children's safety,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937034428&doi=10.1145%2f2700483&partnerID=40&md5=7f1e1021d107366c8c8fdbbe059044d7,"In recent years, crimes against children and cases of missing children have increased at a high rate. Therefore, there is an urgent need for safety support systems to prevent crimes against children or for antiloss, especially when parents are not with their children, such as to and from school. However, existing children's tracking systems are not smart enough to provide the safety supports, as they simply locate the children's positions without offering any notification to parents that their children may be in danger. In addition, there is limited research on children's tracking and their antiloss. In this article, based on location histories, we introduce novel notions of children's life patterns that capture their general lifestyles and regularities, and develop an intelligent data mining framework to learn the safe regions and safe routes of children on the cloud side. When the children may be in danger, their parents will receive automatic notifications from the cloud. Wealso propose an effective energy-efficient positioning scheme that leverages the location tracking accuracy of the children while keeping energy overhead low by using a hybrid global positioning system and a global system for mobile communications. To the best of our knowledge, this is the first attempt in applying data mining techniques to applications designed for children's safety. Our proposed techniques have been incorporated into Soter, a children's safeguard system that is used to provide cloud service for smart bracelets produced by Qihoo. The case studies on real smart bracelet users of Qihoo demonstrate the effectiveness of our proposed methods and Soter for children's safety. © 2015 ACM.",Children's safety; Energy-efficient positioning; Safe region; Safe route; Smart bracelet; Smart notification,Crime; Data mining; Global system for mobile communications; Mobile telecommunication systems; Tracking (position); Energy efficient; Safe region; Safe route; Smart bracelet; Smart notification; Energy efficiency
Automated generation of counterterrorism policies using multiexpert input,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937039549&doi=10.1145%2f2716328&partnerID=40&md5=66825a6b28ce5743054cf61cabacf366,"The use of game theory to model conflict has been studied by several researchers, spearheaded by Schelling. Most of these efforts assume a single payoff matrix that captures players' utilities under different assumptions about what the players will do. Our experience in counterterrorism applications is that experts disagree on these payoffs. We leverage Shapley's notion of vector equilibria, which formulates games where there are multiple payoff matrices, but note that they are very hard to compute in practice. To effectively enumerate large numbers of equilibria with payoffs provided by multiple experts, we propose a novel combination of vector payoffs and well-supported e -approximate equilibria. We develop bounds related to computation of these equilibria for some special cases and give a quasipolynomial time approximation scheme (QPTAS) for the general case when the number of players is small (which is true in many real-world applications). Leveraging this QPTAS, we give efficient algorithms to find such equilibria and experimental results showing that they work well on simulated data. We then built a policy recommendation engine based on vector equilibria, called PREVE. We use PREVE to model the terrorist group Lashkar-e-Taiba (LeT), responsible for the 2008 Mumbai attacks, as a five-player game. Specifically, we apply it to three payoff matrices provided by experts in India-Pakistan relations, analyze the equilibria generated by PREVE, and suggest counterterrorism policies that may reduce attacks by LeT. We briefly discuss these results and identify their strengths and weaknesses from a policy point of view. © 2015 ACM.",Behavioral modeling; Counterterrorism; Game theory; Lashkar-e-Taiba; Vector equilibria,Computation theory; Matrix algebra; Terrorism; Vectors; Approximation scheme; Automated generation; Behavioral model; Counter terrorism; Lashkar-e-Taiba; Policy recommendations; Quasi-polynomial time; Terrorist groups; Game theory
Hazy image restoration by bi-histogram modification,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937034418&doi=10.1145%2f2710024&partnerID=40&md5=68ca686722411b642aa59e86480ec670,"Visibility restoration techniques are widely used for information recovery of hazy images in many computer vision applications. Estimation of haze density is an essential task of visibility restoration techniques. However, conventional visibility restoration techniques often suffer from either the generation of serious artifacts or the loss of object information in the restored images due to uneven haze density, which usually means that the images contain heavy haze formation within their background regions and little haze formation within their foreground regions. This frequently occurs when the images feature real-world scenes with a deep depth of field. How to effectively and accurately estimate the haze density in the transmission map for these images is the most challenging aspect of the traditional state-of-the-art techniques. In response to this problem, this work proposes a novel visibility restoration approach that is based on Bi-Histogram modification, and which integrates a haze density estimation module and a haze formation removal module for effective and accurate estimation of haze density in the transmission map. As our experimental results demonstrate, the proposed approach achieves superior visibility restoration efficacy in comparison with the other state-of-the-art approaches based on both qualitative and quantitative evaluations. The proposed approach proves effective and accurate in terms of both background and foreground restoration of various hazy scenarios. © 2015 ACM.",Haze density; Transmission map; Visibility restoration,Computer vision; Graphic methods; Image reconstruction; Visibility; Accurate estimation; Computer vision applications; Histogram modification; Information recovery; Quantitative evaluation; Restoration techniques; State-of-the-art approach; State-of-the-art techniques; Restoration
MeTA: Characterization of medical treatments at different abstraction levels,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937905716&doi=10.1145%2f2700479&partnerID=40&md5=038043f502f2c868a220bd6e0c550c88,"Physicians and health care organizations always collect large amounts of data during patient care. These large and high-dimensional datasets are usually characterized by an inherent sparseness. Hence, analyzing these datasets to figure out interesting and hidden knowledge is a challenging task. This article proposes a new data mining framework based on generalized association rules to discover multiple-level correlations among patient data. Specifically, correlations among prescribed examinations, drugs, and patient profiles are discovered and analyzed at different abstraction levels. The rule extraction process is driven by a taxonomy to generalize examinations and drugs into their corresponding categories. To ease the manual inspection of the result, a worthwhile subset of rules (i.e., nonredundant generalized rules) is considered. Furthermore, rules are classified according to the involved data features (medical treatments or patient profiles) and then explored in a top-down fashion: from the small subset of high-level rules, a drill-down is performed to target more specific rules. The experiments, performed on a real diabetic patient dataset, demonstrate the effectiveness of the proposed approach in discovering interesting rule groups at different abstraction levels. © 2015 ACM.",Data mining; Generalized association rule mining; Health care informatics,Abstracting; Association rules; Health care; Hospital data processing; Data mining frameworks; Generalized association rule mining; Generalized association rules; Health care informatics; Healthcare organizations; High dimensional datasets; Large amounts of data; Rule extraction process; Data mining
Analyzing activity recognition uncertainties in smart home environments,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939791420&doi=10.1145%2f2651445&partnerID=40&md5=8c58993848dfc93983b35b9cd3ef3329,"In spite of the importance of activity recognition (AR) for intelligent human-computer interaction in emerging smart space applications, state-of-the-art AR technology is not ready or adequate for real-world deployments due to its insufficient accuracy. The accuracy limitation is directly attributed to uncertainties stemming from multiple sources in the AR system. Hence, one of the major goals of AR research is to improve system accuracy by minimizing or managing the uncertainties encountered throughout the AR process. As we cannot manage uncertainties well without measuring them, we must first quantify their impact. Nevertheless, such a quantification process is very challenging given that uncertainties come from diverse and heterogeneous sources. In this article, we propose an approach, which can account for multiple uncertainty sources and assess their impact on AR systems. We introduce several metrics to quantify the various uncertainties and their impact. We then conduct a quantitative impact analysis of uncertainties utilizing data collected from actual smart spaces that we have instrumented. The analysis is intended to serve as groundwork for developing ""diagnostic"" accuracy measures of AR systems capable of pinpointing the sources of accuracy loss. This is to be contrasted with the currently used accuracy measures. © 2015 ACM 2157-6904/2015/08-ART52 $15.00.",Human activity recognition and activity model; Uncertainty analysis,Automation; Human computer interaction; Intelligent buildings; Pattern recognition; Space applications; Accuracy limitations; Accuracy measures; Activity modeling; Activity recognition; Heterogeneous sources; Real world deployment; Smart space applications; Uncertainty sources; Uncertainty analysis
Design of a predictive scheduling system to improve assisted living services for elders,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938409767&doi=10.1145%2f2736700&partnerID=40&md5=9d6d2c8b4e29d86bf525a58c5440df13,"As the number of older adults increases, and with it the demand for dedicated care, geriatric residences face a shortage of caregivers, who themselves experience work overload, stress, and burden. We conducted a long-term field study in three geriatric residences to understand the work conditions of caregivers with the aim of developing technologies to assist them in their work and help them deal with their burdens. From this study, we obtained relevant requirements and insights to design, implement, and evaluate two prototypes for supporting caregivers' tasks (e.g., electronic recording and automatic notifications) in order to validate the feasibility of their implementation in situ and their technical requirements. The evaluation in situ of the prototypes was conducted for a period of 4 weeks. The results of the evaluation, together with the data collected from 6 months of use, motivated the design of a predictive schedule, which was iteratively improved and evaluated in participative sessions with caregivers. PRESENCE, the predictive schedule we propose, triggers real-time alerts of risky situations (e.g., falls, entering off-limits areas such as the infirmary or the kitchen) and informs caregivers of routine tasks that need to be performed (e.g., medication administration, diaper change, etc.). Moreover, PRESENCE helps caregivers to record caring tasks (such as diaper changes or medication) and well-being assessments (such as the mood) that are difficult to automate. This facilitates caregiver's shift handover and can help to train new caregivers by suggesting routine tasks and by sending reminders and timely information about residents. It can be seen as a tool to reduce theworkload of caregivers and medical staff. Instead of trying to substitute the caregiver with an automatic caring system, as proposed by others, we propose our predictive schedule system that blends caregiver assessments and measurements from sensors.We show the feasibility of predicting caregiver tasks and a formative evaluation with caregivers that provides preliminary evidence of its utility. © 2015 ACM.",Activities of daily living (ADL); Assistive living systems; Elderly care,Activities of Daily Living; Assistive living systems; Automatic notification; Elderly care; Electronic recording; Formative evaluation; Predictive scheduling; Technical requirement; Assisted living
Introduction to the ACM TIST special issue on intelligent healthcare informatics,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937044098&doi=10.1145%2f2791398&partnerID=40&md5=c209914b60a3c1d573f3058343b2390e,[No abstract available],,
Effective social graph deanonymization based on graph structure and descriptive information,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937041244&doi=10.1145%2f2700836&partnerID=40&md5=c787a153d98d1fab54fac4056b39577d,"The study of online social networks has attracted increasing interest. However, concerns are raised for the privacy risks of user data since they have been frequently shared among researchers, advertisers, and application developers. To solve this problem, a number of anonymization algorithms have been recently developed for protecting the privacy of social graphs. In this article, we proposed a graph node similarity measurement in consideration with both graph structure and descriptive information, and a deanonymization algorithm based on the measurement. Using the proposed algorithm, we evaluated the privacy risks of several typical anonymization algorithms on social graphs with thousands of nodes from Microsoft Academic Search, LiveJournal, and the Enron email dataset, and a social graph with millions of nodes from Tencent Weibo. Our results showed that the proposed algorithm was efficient and effective to deanonymize social graphs without any initial seed mappings. Based on the experiments, we also pointed out suggestions on how to better maintain the data utility while preserving privacy. © 2015 ACM.",Deanonymization; Privacy protection; Social network,Data privacy; Economic and social effects; Graph algorithms; Graph theory; Graphic methods; Social networking (online); Academic search; Application developers; Data utilities; Deanonymization; Descriptive information; Node similarities; On-line social networks; Privacy protection; Graph structures
Empowering patients and caregivers to manage healthcare via streamlined presentation of web objects selected by modeling learning benefits obtained by similar peers,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937045077&doi=10.1145%2f2700480&partnerID=40&md5=4913051f99822db085378b2b1078ad03,"In this article, we introduce a framework for selecting web objects (texts, videos, simulations) from a large online repository to present to patients and caregivers, in order to assist in their healthcare. Motivated by the paradigm of peer-based intelligent tutoring, we model the learning gains achieved by users when exposed to specific web objects in order to recommend those objects most likely to deliver benefit to new users. We are able to show that this streamlined presentation leads to effective knowledge gains, both through a process of simulated learning and through a user study, for the specific application of caring for children with autism. The value of our framework for peer-driven content selection of health information is emphasized through two additional roles for peers: attaching commentary to web objects and proposing subdivided objects for presentation, both of which are demonstrated to deliver effective learning gains, in simulations. In all, we are offering an opportunity for patients to navigate the deep waters of excessive online information towards effective management of healthcare, through content selection influenced by previous peer experiences. © 2015 ACM.",Computational support for patient-centred care; Cyber-based empowering of patients; Discovery of new knowledge for decision support; E-communities for patients and caregivers; Effective information retrieval for healthcare applications; Shareable health knowledge,Decision support systems; Health care; Computational support for patient-centred care; Cyber-based empowering of patients; Decision supports; E-community; Health care application; Shareable health knowledge; Learning systems
Recognition of patient-related named entities in Noisy Tele-Health Texts,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938375776&doi=10.1145%2f2651444&partnerID=40&md5=9d2d12692145c5513d9ffaef3cae7545,"We explore methods for effectively extracting information from clinical narratives that are captured in a public health consulting phone service called HealthLink. Our research investigates the application of stateof- the-art natural language processing and machine learning to clinical narratives to extract information of interest. The currently available data consist of dialogues constructed by nurses while consulting patients by phone. Since the data are interviews transcribed by nurses during phone conversations, they include a significant volume and variety of noise. When we extract the patient-related information from the noisy data, we have to remove or correct at least two kinds of noise: explicit noise, which includes spelling errors, unfinished sentences, omission of sentence delimiters, and variants of terms, and implicit noise, which includes non-patient information and patient's untrustworthy information. To filter explicit noise, we propose our own biomedical term detection/normalization method: it resolves misspelling, term variations, and arbitrary abbreviation of terms by nurses. In detecting temporal terms, temperature, and other types of named entities (which show patients' personal information such as age and sex), we propose a bootstrapping-based pattern learning process to detect a variety of arbitrary variations of named entities. To address implicit noise, we propose a dependency path-based filtering method. The result of our denoising is the extraction of normalized patient information, and we visualize the named entities by constructing a graph that shows the relations between named entities. The objective of this knowledge discovery task is to identify associations between biomedical terms and to clearly expose the trends of patients' symptoms and concern; the experimental results show that we achieve reasonable performance with our noise reduction methods. © 2015 ACM.",Biomedical text mining; Effective information retrieval; Named entity recognition; Tele-health mining,Character recognition; Clinical research; Filtration; Information retrieval; Learning systems; Noise abatement; Nursing; Telephone sets; Text mining; Biomedical text minings; Extract informations; Extracting information; Named entity recognition; NAtural language processing; Noise reduction methods; Path-based filtering; Telehealth; Natural language processing systems
Using health-consumer-contributed data to detect adverse drug reactions by association mining with temporal analysis,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937038752&doi=10.1145%2f2700482&partnerID=40&md5=dac36369c5f60472560aa75db04ef80b,"Since adverse drug reactions (ADRs) represent asignificant health problem all over the world, ADR detection has become an important research topic in drug safety surveillance. As many potential ADRs cannot be detected though premarketing review, drug safety currently depends heavily on postmarketing surveillance. Particularly, current postmarketing surveillance in the United States primarily relies on the FDA Adverse Event Reporting System (FAERS). However, the effectiveness of such spontaneous reporting systems for ADR detection is not as good as expected because of the extremely high under reporting ratio of ADRs. Moreover, it often takes the FDA years to complete the whole process of collecting reports, investigating cases, and releasing alerts. Given the prosperity of social media, many online health communities are publicly available for health consumers to share and discuss any healthcare experience such as ADRs they are suffering. Such health-consumer-contributed content is timely and informative, but this data source still remains untapped for postmarketing drug safety surveillance. In this study, we propose to use (1) association mining to identify the relations between a drug and an ADR and (2) temporal analysis to detect drug safety signals at the early stage. We collect data from MedHelp and use the FDA's alerts and information of drug labeling revision as the gold standard to evaluate the effectiveness of our approach. The experiment results show that health-related social media is a promising source for ADR detection, and our proposed techniques are effective to identify early ADR signals. © 2015 ACM.",Adverse drug reactions; Association mining; Drug safety signal detection; Health-consumer-contributed content; Postmarketing surveillance; Social media; Temporal analysis,Association reactions; Health; Monitoring; Signal detection; Social networking (online); Adverse drug reactions; Association mining; Drug safety; Social media; Temporal analysis; Pharmacodynamics
Smart colonography for distributed medical databases with Group Kernel Feature Analysis,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938363021&doi=10.1145%2f2668136&partnerID=40&md5=4bb9be0b6901fa5edd49cf1fe29bf6c5,"Computer-Aided Detection (CAD) of polyps in Computed Tomographic (CT) colonography is currently very limited since a single database at each hospital/institution doesn't provide sufficient data for training the CAD system's classification algorithm. To address this limitation, we propose to use multiple databases, (e.g., big data studies) to create multiple institution-wide databases using distributed computing technologies, which we call smart colonography. Smart colonography may be built by a larger colonography database networked through the participation of multiple institutions via distributed computing. The motivation herein is to create a distributed database that increases the detection accuracy of CAD diagnosis by covering many true-positive cases. Colonography data analysis is mutually accessible to increase the availability of resources so that the knowledge of radiologists is enhanced. In this article, we propose a scalable and efficient algorithm called Group Kernel Feature Analysis (GKFA), which can be applied to multiple cancer databases so that the overall performance of CAD is improved. The key idea behind the proposed GKFA method is to allow the feature space to be updated as the training proceeds with more data being fed from other institutions into the algorithm. Experimental results show that GKFA achieves very good classification accuracy. © 2015 ACM.",Computed tomographic colonography; Distributed databases; Group learning; Kernel feature analysis,Classification (of information); Computer aided diagnosis; Computerized tomography; Medical computing; Classification accuracy; Classification algorithm; Computed tomographic colonography; Computer-aided detection; Distributed computing technology; Distributed database; Feature analysis; Group learning; Distributed database systems
Automated pricing in a multiagent prediction market using a partially observable stochastic game,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939799624&doi=10.1145%2f2700488&partnerID=40&md5=1ebd030cd578bab81f024d33eddab5cd,"Prediction markets offer an efficient market-based mechanism to aggregate large amounts of dispersed or distributed information from different people to predict the possible outcome of future events. Recently, automated prediction markets where software trading agents perform market operations such as trading and updating beliefs on behalf of humans have been proposed. A challenging aspect in automated prediction markets is to develop suitable techniques that can be used by automated trading agents to update the price at which they should trade securities related to an event so that they can increase their profit. This problem is nontrivial, as the decision to trade and the price at which trading should occur depends on several dynamic factors, such as incoming information related to the event for which the security is being traded, the belief-update mechanism and risk attitude of the trading agent, and the trading decision and trading prices of other agents. To address this problem, we have proposed a new behavior model for trading agents based on a game-theoretic framework called partially observable stochastic game with information (POSGI). We propose a correlated equilibrium (CE)-based solution strategy for this game that allows each agent to dynamically choose an action (to buy or sell or hold) in the prediction market. We have also performed extensive simulation experiments using the data obtained from the Intrade prediction market for four different prediction markets. Our results show that our POSGI model and CE strategy produces prices that are strongly correlated with the prices of the real prediction markets. Results comparing our CE strategy with five other strategies commonly used in similar market show that our CE strategy improves price predictions and provides higher utilities to the agents compared to other existing strategies. © 2015 ACM 2157-6904/2015/07-ART48 $15.00.",Correlated equilibrium; Prediction market; Risk-averse traders; Stochastic game,Automation; Computation theory; Costs; Forecasting; Game theory; Software agents; Stochastic models; Stochastic systems; Automated trading; Correlated equilibria; Distributed information; Extensive simulations; Market-based mechanisms; Prediction markets; Risk averse; Stochastic game; Commerce
Extracting city traffic events from social streams,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937028264&doi=10.1145%2f2717317&partnerID=40&md5=7218f0c4e0fe5aa58aad163cc7ae1f4e,"Cities are composed of complex systems with physical, cyber, and social components. Current works on extracting and understanding city events mainly rely on technology-enabled infrastructure to observe and record events. In this work, we propose an approach to leverage citizen observations of various city systems and services, such as traffic, public transport, water supply, weather, sewage, and public safety, as a source of city events. We investigate the feasibility of using such textual streams for extracting city events from annotated text. We formalize the problem of annotating social streams such as microblogs as a sequence labeling problem. We present a novel training data creation process for training sequence labeling models. Our automatic training data creation process utilizes instance-level domain knowledge (e.g., locations in a city, possible event terms). We compare this automated annotation process to a state-of-the-art tool that needs manually created training data and show that it has comparable performance in annotation tasks. An aggregation algorithm is then presented for event extraction from annotated text. We carry out a comprehensive evaluation of the event annotation and event extraction on a real-world dataset consisting of event reports and tweets collected over 4 months from the San Francisco Bay Area. The evaluation results are promising and provide insights into the utility of social stream for extracting city events. © 2015 ACM.",Citizen sensing; City events; Event extraction; Physical-cyber-social systems; Smart cities; Tweets,Information analysis; Sewage; Smart city; Water supply; Citizen sensing; City events; Event extraction; Social systems; Tweets; Extraction
City-scale social event detection and evaluation with taxi traces,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930680547&doi=10.1145%2f2700478&partnerID=40&md5=18463e2136d9ebc658828ec15d64838b,"A social event is an occurrence that involves lots of people and is accompanied by an obvious rise in human flow. Analysis of social events has real-world importance because events bring about impacts onmany aspects of city life. Traditionally, detection and impact measurement of social events rely on social investigation, which involves considerable human effort. Recently, by analyzing messages in social networks, researchers can also detect and evaluate country-scale events. Nevertheless, the analysis of city-scale events has not been explored. In this article, we use human flow dynamics, which reflect the social activeness of a region, to detect social events and measure their impacts. We first extract human flow dynamics from taxi traces. Second, we propose a method that can not only discover the happening time and venue of events from abnormal social activeness, but also measure the scale of events through changes in such activeness. Third, we extract traffic congestion information from traces and use its change during social events to measure their impact. The results of experiments validate the effectiveness of both the event detection and impact measurement methods. © 2015.",Social event; social impact; taxi traces; traffic condition,Taxicabs; Traffic congestion; Event detection; Flow dynamics; Impact measurements; Social events; Social impact; taxi traces; Traffic conditions; Traffic congestion information; Economic and social effects
Trajectory data mining: An overview,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930640513&doi=10.1145%2f2743025&partnerID=40&md5=ed7ec30b7e4193b171f23a27460b346c,"The advances in location-acquisition and mobile computing techniques have generated massive spatial trajectory data, which represent the mobility of a diversity of moving objects, such as people, vehicles, and animals. Many techniques have been proposed for processing, managing, and mining trajectory data in the past decade, fostering a broad range of applications. In this article, we conduct a systematic survey on the major research into trajectory data mining, providing a panorama of the field as well as the scope of its research topics. Following a road map from the derivation of trajectory data, to trajectory data preprocessing, to trajectory data management, and to a variety of mining tasks (such as trajectory pattern mining, outlier detection, and trajectory classification), the survey explores the connections, correlations, and differences among these existing techniques. This survey also introduces the methods that transform trajectories into other data formats, such as graphs, matrices, and tensors, to which more data mining and machine learning techniques can be applied. Finally, some public trajectory datasets are presented. This survey can help shape the field of trajectory data mining, providing a quick understanding of this field to the community. © 2015.",Spatiotemporal data mining; trajectory classification; trajectory compression; trajectory data mining; trajectory indexing and retrieval; trajectory outlier detection; trajectory pattern mining; trajectory uncertainty; urban computing,Anomaly detection; Classification (of information); Data handling; Information management; Learning systems; Pattern recognition; Statistics; Surveys; Trajectories; Spatio-temporal data mining; Trajectory classification; Trajectory data minings; Trajectory indexing; Trajectory pattern; Urban computing; Data mining
Latent support vector machine modeling for sign language recognition with kinect,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927556900&doi=10.1145%2f2629481&partnerID=40&md5=383f5cd379d7c6c27ad257ee4659426d,"Vision-based sign language recognition has attracted more and more interest from researchers in the computer vision field. In this article, we propose a novel algorithm to model and recognize sign language performed in front of a Microsoft Kinect sensor. Under the assumption that some frames are expected to be both discriminative and representative in a sign language video, we first assign a binary latent variable to each frame in training videos for indicating its discriminative capability, then develop a latent support vector machine model to classify the signs, as well as localize the discriminative and representative frames in each video. In addition, we utilize the depth map together with the color image captured by the Kinect sensor to obtain a more effective and accurate feature to enhance the recognition accuracy. To evaluate our approach, we conducted experiments on both word-level sign language and sentence-level sign language. An American Sign Language dataset including approximately 2,000 word-level sign language phrases and 2,000 sentence-level sign language phrases was collected using the Kinect sensor, and each phrase contains color, depth, and skeleton information. Experiments on our dataset demonstrate the effectiveness of the proposed method for sign language recognition. © 2015 ACM.",Kinect sensor; Latent SVM; Sign language recognition,Image enhancement; Support vector machines; American sign language; Kinect sensors; Latent support vector machines; Latent SVM; Microsoft Kinect sensors; Recognition accuracy; Sign Language recognition; Vision based sign language recognition; Modeling languages
Depth error elimination for RGB-D cameras,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929164361&doi=10.1145%2f2735959&partnerID=40&md5=a7d2a69db98c1c5b89bd525346399dbf,"The rapid spreading of RGB-D cameras has led to wide applications of 3D videos in both academia and industry, such as 3D entertainment and 3D visual understanding. Under these circumstances, extensive research efforts have been dedicated to RGB-D camera-oriented topics. In these topics, quality promotion of depth videos with the temporal characteristic is emerging and important. Due to the limited exposure time of RGB-D cameras, object movement can easily lead to motion blurs in intensive images, which can further result in obvious artifacts (holes or fake boundaries) in the corresponding depth frames. With regard to this problem, we propose a depth error elimination method based on time series analysis to remove the artifacts in depth images. In this method, we first locate the regions with erroneous depths in intensive images by using motion blur detection based on a time series analysis model. This is based on the fact that the depth image is calculated by intensive color images that are captured synchronously by RGB-D cameras. Then, the artifacts, such as holes or fake boundaries, are fixed by a depth error elimination method. To evaluate the performance of the proposed method, we conducted experiments on 250 images. Experimental results demonstrate that the proposed method can locate the error regions correctly and eliminate these artifacts effectively. The quality of depth video can be improved significantly by using the proposed method. © 2015 ACM.",Depth error; Depth video; RGB-D cameras; Time series analysis,Cameras; Harmonic analysis; Image analysis; Color images; Depth errors; Depth videos; Exposure-time; Object movements; Research efforts; Rgb-d cameras; Temporal characteristics; Time series analysis
"Kinect depth recovery using a color-guided, region-adaptive, and depth-selective framework",2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927563289&doi=10.1145%2f2700475&partnerID=40&md5=2325fafd923b4f88ab3fd32196f430bf,"Considering that the existing depth recovery approaches have different limitations when applied to Kinect depth data, in this article, we propose to integrate their effective features including adaptive support region selection, reliable depth selection, and color guidance together under an optimization framework for Kinect depth recovery. In particular, we formulate our depth recovery as an energy minimization problem, which solves the depth hole filling and denoising simultaneously. The energy function consists of a fidelity term and a regularization term, which are designed according to the Kinect characteristics. Our framework inherits and improves the idea of guided filtering by incorporating structure information and prior knowledge of the Kinect noise model. Through analyzing the solution to the optimization framework, we also derive a local filtering version that provides an efficient and effective way of improving the existing filtering techniques. Quantitative evaluations on our developed synthesized dataset and experiments on real Kinect data show that the proposed method achieves superior performance in terms of recovery accuracy and visual quality. © 2015 ACM.",Depth recovery; Kinect; Variational framework,Recovery; Depth recovery; Energy minimization problem; Kinect; Optimization framework; Quantitative evaluation; Regularization terms; Structure information; Variational framework; Information filtering
A combined approach toward consistent reconstructions of indoor spaces based on 6D RGB-D odometry and KinectFusion,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927553232&doi=10.1145%2f2629673&partnerID=40&md5=63844e7af0f54a39b0a9f7b1903fa6b3,"We propose a 6D RGB-D odometry approach that finds the relative camera pose between consecutive RGB-D frames by keypoint extraction and feature matching both on the RGB and depth image planes. Furthermore, we feed the estimated pose to the highly accurate KinectFusion algorithm, which uses a fast ICP (Iterative Closest Point) to fine-tune the frame-to-frame relative pose and fuse the depth data into a global implicit surface. We evaluate our method on a publicly available RGB-D SLAM benchmark dataset by Sturm et al. The experimental results show that our proposed reconstructionmethod solely based on visual odometry and KinectFusion outperforms the state-of-the-art RGB-D SLAM system accuracy. Moreover, our algorithm outputs a ready-to-use polygonmesh (highly suitable for creating 3D virtual worlds) without any postprocessing steps. © 2015 ACM.",Benchmark datasets; Evaluation; Indoor mapping; Kinect,Iterative methods; Virtual reality; 3d virtual worlds; Benchmark datasets; Consistent reconstruction; Evaluation; Implicit surfaces; Iterative Closest Points; Kinect; Relative camera pose; Indoor positioning systems
Data mining of online genealogy datasets for revealing lifespan patterns in human population,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927556738&doi=10.1145%2f2700464&partnerID=40&md5=cd5bc865cb958071d61f4989109d9ad8,"Online genealogy datasets contain extensive information about millions of people and their past and present family connections. This vast amount of data can help identify various patterns in the human population. In this study, we present methods and algorithms that can assist in identifying variations in lifespan distributions of the human population in the past centuries, in detecting social and genetic features that correlate with the human lifespan, and in constructing predictive models of human lifespan based on various features that can easily be extracted from genealogy datasets. We have evaluated the presented methods and algorithms on a large online genealogy dataset with over a million profiles and over 9 million connections, all of which were collected from the WikiTree website. Our findings indicate that significant but small positive correlations exist between the parents' lifespan and their children's lifespan. Additionally, we found slightly higher and significant correlations between the lifespans of spouses. We also discovered a very small positive and significant correlation between longevity and reproductive success in males, and a small and significant negative correlation between longevity and reproductive success in females. Moreover, our predictive models presented results with a Mean Absolute Error as low as 13.18 in predicting the lifespans of individuals who outlived the age of 10, and our classification models presented better than random classification results in predicting which people who outlive the age of 50 will also outlive the age of 80. We believe that this study will be the first of many studies to utilize the wealth of data on human populations, existing in online genealogy datasets, to better understand factors that influence the human lifespan. Understanding these factors can assist scientists in providing solutions for successful aging. © 2015 ACM.",Data mining; Genealogy data mining; Human population lifespan; Lifespan prediction; Machine learning; WikiTree,Forecasting; History; Large dataset; Learning systems; Population statistics; Predictive analytics; Classification models; Classification results; Life span; Mean absolute error; Negative correlation; Positive correlations; Reproductive success; WikiTree; Data mining
TerraFly GeoCloud: An online spatial data analysis and visualization system,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929144591&doi=10.1145%2f2700494&partnerID=40&md5=7c8f785c5bf711e3cf565c2a82163a00,"With the exponential growth of the usage of web map services, geo-data analysis has become more and more popular. This article develops an online spatial data analysis and visualization system, TerraFly GeoCloud, which helps end-users visualize and analyze spatial data and share the analysis results. Built on the TerraFly Geo spatial database, TerraFly GeoCloud is an extra layer running upon the TerraFly map and can efficiently support many different visualization functions and spatial data analysis models. Furthermore, users can create unique URLs to visualize and share the analysis results. TerraFly GeoCloud also enables the MapQL technology to customize map visualization using SQL-like statements. The system is available at http://terrafly.fiu.edu/GeoCloud/. © 2015 ACM.",Big data; Geospatial analysis; GIS; Visualization,Data handling; Data visualization; Flow visualization; Geographic information systems; Information analysis; Social networking (online); Spatial variables measurement; Visualization; Exponential growth; Geo-spatial analysis; Geo-spatial database; Map visualizations; Spatial data analysis; Visualization functions; Visualization system; Web map services; Big data
An event-driven QoI-aware participatory sensing framework with energy and budget constraints,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928942004&doi=10.1145%2f2630074&partnerID=40&md5=eaabe381089e4654adbeeccd602140eb,"Participatory sensing systems can be used for concurrent event monitoring applications, like noise levels, fire, and pollutant concentrations. However, they are facing new challenges as to how to accurately detect the exact boundaries of these events, and further, to select the most appropriate participants to collect the sensing data. On the one hand, participants' handheld smart devices are constrained with different energy conditions and sensing capabilities, and they move around with uncontrollable mobility patterns in their daily life. On the other hand, these sensing tasks are within time-varying quality-of-information (QoI) requirements and budget to afford the users' incentive expectations. Toward this end, this article proposes an event-driven QoI-aware participatory sensing framework with energy and budget constraints. The main method of this framework is event boundary detection. For the former, a two-step heuristic solution is proposed where the coarse-grained detection step finds its approximation and the fine-grained detection step identifies the exact location. Participants are selected by explicitly considering their mobility pattern, required QoI of multiple tasks, and users' incentive requirements, under the constraint of an aggregated task budget. Extensive experimental results, based on a real trace in Beijing, show the effectiveness and robustness of our approach, while comparing with existing schemes. © 2015 ACM 2157-6904/2015/04-ART42$15.00.",Energy efficiency; Event boundary detection; Participatory sensing,Aggregates; Energy efficiency; Human computer interaction; Budget constraint; Event boundary detection; Event monitoring; Heuristic solutions; Mobility pattern; Participatory Sensing; Pollutant concentration; Quality of informations (QoI); Budget control
Real-time system for driver fatigue detection by RGB-D camera,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927521223&doi=10.1145%2f2629482&partnerID=40&md5=a2f0cd3ceada6f034891a763892717ab,"Drowsy driving is one of the major causes of fatal traffic accidents. In this article, we propose a real-time system that utilizes RGB-D cameras to automatically detect driver fatigue and generate alerts to drivers. By introducing RGB-D cameras, the depth data can be obtained, which provides extra evidence to benefit the task of head detection and head pose estimation. In this system, two important visual cues (head pose and eye state) for driver fatigue detection are extracted and leveraged simultaneously. We first present a real-time 3D head pose estimation method by leveraging RGB and depth data. Then we introduce a novel method to predict eye states employing the WLBP feature, which is a powerful local image descriptor that is robust to noise and illumination variations. Finally, we integrate the results from both head pose and eye states to generate the overall conclusion. The combination and collaboration of the two types of visual cues can reduce the uncertainties and resolve the ambiguity that a single cue may induce. The experiments were performed using an inside-car environment during the day and night, and theyfully demonstrate the effectiveness and robustness of our system as well as the proposed methods of predicting head pose and eye states. © 2015 ACM.",Driver fatigue detection system; Eye state; Head pose; RGB-D cameras,Cameras; Interactive computer systems; Driver fatigue; Eye state; Fatal traffic accidents; Head pose; Head Pose Estimation; Illumination variation; Local image descriptors; Rgb-d cameras; Real time systems
Activity sensor: Check-in usage mining for local recommendation,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929152481&doi=10.1145%2f2700468&partnerID=40&md5=fc0cb12bb3d94ed537649bb5c2879d35,"While on the go, people are using their phones as a personal concierge discovering what is around and deciding what to do. Mobile phone has become a recommendation terminal customized for individuals-capable of recommending activities and simplifying the accomplishment of related tasks. In this article, we conduct usagemining on the check-in data, with summarized statistics identifying the local recommendation challenges of huge solution space, sparse available data, and complicated user intent, and discovered observations to motivate the hierarchical, contextual, and sequential solution.We present a point-of-interest (POI) categorytransition- based approach, with a goal of estimating the visiting probability of a series of successive POIs conditioned on current user context and sensor context. A mobile local recommendation demo application is deployed. The objective and subjective evaluations validate the effectiveness in providing mobile users both accurate recommendation and favorable user experience. © 2015 ACM.",Check-in; Local recommendation; Location-based service; Usage mining,Mobile telecommunication systems; Telecommunication services; Telephone sets; Check-in; Local recommendation; Objective and subjective evaluations; Point of interest; Solution space; Usage mining; User context; User experience; Location based services
Fusing multiple features for depth-based action recognition,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927551270&doi=10.1145%2f2629483&partnerID=40&md5=db9d409b4be77317ad6f567c9ff7328c,"Human action recognition is a very active research topic in computer vision and pattern recognition. Recently, it has shown a great potential for human action recognition using the three-dimensional (3D) depth data captured by the emerging RGB-D sensors. Several features and/or algorithms have been proposed for depthbased action recognition. A question is raised: Can we find some complementary features and combine them to improve the accuracy significantly for depth-based action recognition? To address the question and have a better understanding of the problem, we study the fusion of different features for depth-based action recognition. Although data fusion has shown great success in other areas, it has not been well studied yet on 3D action recognition. Some issues need to be addressed, for example, whether the fusion is helpful or not for depth-based action recognition, and how to do the fusion properly. In this article, we study different fusion schemes comprehensively, using diverse features for action characterization in depth videos. Two different levels of fusion schemes are investigated, that is, feature level and decision level. Various methods are explored at each fusion level. Four different features are considered to characterize the depth action patterns from different aspects. The experiments are conducted on four challenging depth action databases, in order to evaluate and find the best fusion methods generally. Our experimental results show that the four different features investigated in the article can complement each other, and appropriate fusion methods can improve the recognition accuracies significantly over each individual feature. More importantly, our fusion-based action recognition outperforms the state-of-the-art approaches on these challenging databases. © 2015 ACM.",4D descriptor; Action recognition; Data fusion; Decision level; Depth maps; Feature level; Feature selection; RGB-D sensor; Skeleton; Spatiotemporal features,Data fusion; Feature extraction; Sensor data fusion; Action recognition; Decision levels; Depth Map; Descriptors; Feature level; Rgb-d sensors; Skeleton; Spatio temporal features; Pattern recognition
Significant correlation pattern mining in smart homes,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928552467&doi=10.1145%2f2700484&partnerID=40&md5=cd229777de817c8f4cc21a0295c159ed,"Owing to the great advent of sensor technology, the usage data of appliances in a house can be logged and collected easily today. However, it is a challenge for the residents to visualize how these appliances are used. Thus, mining algorithms are much needed to discover appliance usage patterns. Most previous studies on usage pattern discovery are mainly focused on analyzing the patterns of single appliance rather than mining the usage correlation among appliances. In this article, a novel algorithm, namely Correlation Pattern Miner (CoPMiner), is developed to capture the usage patterns and correlations among appliances probabilistically. CoPMiner also employs four pruning techniques and a statistical model to reduce the search space and filter out insignificant patterns, respectively. Furthermore, the proposed algorithm is applied on a real-world dataset to show the practicability of correlation pattern mining. © 2015 ACM.",Correlation pattern; Sequential pattern; Smart home; Time interval-based data; Usage representation,Automation; Data mining; Intelligent buildings; Correlation patterns; Sequential patterns; Smart homes; Time interval; Usage representation; Filtration
Visual understanding with RGB-D sensors: An introduction to the special issue,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927520111&doi=10.1145%2f2732265&partnerID=40&md5=d56f9993dfa8a0012cf10f43b402000a,[No abstract available],,
Ohmage: A general and extensible end-to-end participatory sensing platform,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928574862&doi=10.1145%2f2717318&partnerID=40&md5=e6ddc3e6c8e9f9bf4c61fec5c1b6bf1a,"Participatory sensing (PS) is a distributed data collection and analysis approach where individuals, acting alone or in groups, use their personal mobile devices to systematically explore interesting aspects of their lives and communities [Burke et al. 2006]. Thesemobile devices can be used to capture diverse spatiotemporal data through both intermittent self-report and continuous recording from on-board sensors and applications. Ohmage (http://ohmage.org) is a modular and extensible open-source, mobile to Web PS platform that records, stores, analyzes, and visualizes data from both prompted self-report and continuous data streams. These data streams are authorable and can dynamically be deployed in diverse settings. Feedback from hundreds of behavioral and technology researchers, focus group participants, and end users has been integrated into ohmage through an iterative participatory design process. Ohmage has been used as an enabling platform in more than 20 independent projects in many disciplines. We summarize the PS requirements, challenges and key design objectives learned through our design process, and ohmage system architecture to achieve those objectives. The flexibility, modularity, and extensibility of ohmage in supporting diverse deployment settings are presented through three distinct case studies in education, health, and clinical research. ©2015 ACM 2157-6904/2015/04-ART38 $15.00.",Citizen science; Clinical research; Data sharing; Educational tool; Experience sampling; Mobile data collection; Mobile sensing; Participatory sensing,Behavioral research; Clinical research; Data Sharing; Data streams; Human computer interaction; Mobile telecommunication systems; Citizen science; Educational tools; Experience sampling; Mobile data collections; Mobile sensing; Participatory Sensing; Data acquisition
Who will retweet This? Detecting strangers from twitter to retweet information,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928535429&doi=10.1145%2f2700466&partnerID=40&md5=c502747720600ab60063895e8606a18e,"There has been much effort on studying how social media sites, such as Twitter, help propagate information in different situations, including spreading alerts and SOS messages in an emergency. However, existing work has not addressed how to actively identify and engage the right strangers at the right time on social media to help effectively propagate intended information within a desired time frame. To address this problem, we have developed three models: (1) a feature-based model that leverages people's exhibited social behavior, including the content of their tweets and social interactions, to characterize their willingness and readiness to propagate information on Twitter via the act of retweeting; (2) a wait-time model based on a user's previous retweeting wait times to predict his or her next retweeting time when asked; and (3) a subset selection model that automatically selects a subset of people from a set of available people using probabilities predicted by the feature-based model and maximizes retweeting rate. Based on these three models, we build a recommender system that predicts the likelihood of a stranger to retweet information when asked, within a specific time window, and recommends the top-N qualified strangers to engage with. Our experiments, including live studies in the real world, demonstrate the effectiveness of our work. © 2015 ACM.",Personality; Retweet; Social media; Twitter; Willingness,Personality; Retweet; Social media; Twitter; Willingness; Social networking (online)
User-specific feature-based similarity models for top-n recommendation of new items,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929177487&doi=10.1145%2f2700495&partnerID=40&md5=a826918951bc4a3a2d8bcc89eb898f34,"Recommending new items for suitable users is an important yet challenging problem due to the lack of preference history for the new items. Noncollaborative user modeling techniques that rely on the item features can be used to recommend new items. However, they only use the past preferences of each user to provide recommendations for that user. They do not utilize information from the past preferences of other users, which can potentially be ignoring useful information. More recent factor models transfer knowledge across users using their preference information in order to provide more accurate recommendations. These methods learn a low-rank approximation for the preference matrix, which can lead to loss of information. Moreover, they might not be able to learn useful patterns given very sparse datasets. In this work, we present UFSM, a method for top-n recommendation of new items given binary user preferences. UFSM learns User-specific Feature-based item-Similarity Models, and its strength lies in combining two points: (1) exploiting preference information across all users to learn multiple global item similarity functions and (2) learning user-specific weights that determine the contribution of each global similarity function in generating recommendations for each user. UFSM can be considered as a sparse high-dimensional factor model where the previous preferences of each user are incorporated within his or her latent representation. This way, UFSM combines the merits of item similarity models that capture local relations among items and factor models that learn global preference patterns. A comprehensive set of experiments was conduced to compare UFSM against state-of-the-art collaborative factor models and noncollaborative user modeling techniques. Results show that UFSM outperforms other techniques in terms of recommendation quality. UFSM manages to yield better recommendations even with very sparse datasets. Results also show that UFSM can efficiently handle high-dimensional as well as low-dimensional item feature spaces. © 2015 ACM.",Cold start; Item content; Item features; Item similarity; Recommender systems; Top-n,Approximation theory; Recommender systems; Cold start; Feature-based similarities; Item content; Item features; Item similarity; Low rank approximations; Preference information; Preference pattern; Information use
Pattern matching techniques for replacing missing sections of audio streamed across wireless networks,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927547782&doi=10.1145%2f2663358&partnerID=40&md5=aa06e82b98110b24c977c30a34d809c2,"Streaming media on the Internet can be unreliable. Services such as audio-on-demand drastically increase the loads on networks; therefore, new, robust, and highly efficient coding algorithms are necessary. One method overlooked to date, which can work alongside existing audio compression schemes, is that which takes into account the semantics and natural repetition of music. Similarity detection within polyphonic audio has presented problematic challenges within the field of music information retrieval. One approach to deal with bursty errors is to use self-similarity to replace missing segments. Many existing systems exist based on packet loss and replacement on a network level, but none attempt repairs of large dropouts of 5 seconds or more. Music exhibits standard structures that can be used as a forward error correction (FEC) mechanism. FEC is an area that addresses the issue of packet loss with the onus of repair placed as much as possible on the listener's device. We have developed a server-client-based framework (SoFI) for automatic detection and replacement of large packet losses on wireless networks when receiving timedependent streamed audio. Whenever dropouts occur, SoFI swaps audio presented to the listener between a live stream and previous sections of the audio stored locally. Objective and subjective evaluations of SoFI where subjects were presented with other simulated approaches to audio repair together with simulations of replacements including varying lengths of time in the repair give positive results. © 2015 ACM.",Audio repair; Data compaction and compression; Forward error correction; Streaming media,Error correction; Forward error correction; Media streaming; Packet loss; Pattern matching; Semantics; Wireless networks; Automatic Detection; Coding algorithms; Data compaction; Music information retrieval; Objective and subjective evaluations; Pattern-matching technique; Similarity detection; Streaming media; Audio acoustics
An association-based unified framework for mining features and opinion words,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927560383&doi=10.1145%2f2663359&partnerID=40&md5=edb220afa7ebfbbc12d694528095da0f,"Mining features and opinion words is essential for fine-grained opinion analysis of customer reviews. It is observed that semantic dependencies naturally exist between features and opinion words, even among features or opinion words themselves. In this article, we employ a corpus statistics association measure to quantify the pairwise word dependencies and propose a generalized association-based unified framework to identify features, including explicit and implicit features, and opinion words from reviews. We first extract explicit features and opinion words via an association-based bootstrapping method (ABOOT). ABOOT starts with a small list of annotated feature seeds and then iteratively recognizes a large number of domain-specific features and opinion words by discovering the corpus statistics association between each pair of words on a given review domain. Two instances of this ABOOT method are evaluated based on two particular association models, likelihood ratio tests (LRTs) and latent semantic analysis (LSA). Next, we introduce a natural extension to identify implicit features by employing the recognized known semantic correlations between features and opinion words. Experimental results illustrate the benefits of the proposed association-based methods for identifying features and opinion words versus benchmark methods. © 2015 ACM.",Association; Feature; Implicit feature; Opinion mining; Opinion word,Association reactions; Semantics; Sentiment analysis; Bootstrapping method; Feature; Generalized associations; Implicit features; Latent Semantic Analysis; Likelihood ratio tests; Opinion mining; Opinion word; Iterative methods
An evaluation of gamesourced data for human pose estimation,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927551079&doi=10.1145%2f2629465&partnerID=40&md5=b4de3e42e7d9ea3ad8ae96713d9e695f,"Gamesourcing has emerged as an approach for rapidly acquiring labeled data for learning-based, computer vision recognition algorithms. In this article, we present an approach for using RGB-D sensors to acquire annotated training data for human pose estimation from 2D images. Unlike other gamesourcing approaches, our method does not require a specific game, but runs alongside any gesture-based game using RGB-D sensors. The automatically generated datasets resulting from this approach contain joint estimates within a few pixel units of manually labeled data, and a gamesourced dataset created using a relatively small number of players, games, and locations performs as well as large-scale, manually annotated datasets when used as training data with recent learning-based human pose estimation methods for 2D images. © 2015 ACM.",Automatic annotation; Crowdsourcing; Data collection; Dataset generation; Datasets; Depth images; Kinect,Crowdsourcing; Labeled data; Automatic annotation; Data collection; Dataset generation; Datasets; Depth image; Kinect; Large dataset
An introduction to the special issue on participatory sensing and crowd intelligence,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928539200&doi=10.1145%2f2745712&partnerID=40&md5=d4e59be95ed4e6f571e0381db7b3771c,"Participatory sensing [Burke 2006] is an emerging computing paradigm that tasks everyday mobile devices to form participatory sensor networks. It allows the increasing number of mobile phone users to share local knowledge acquired by their sensorenhanced devices, such as monitoring of pollution or noise levels and traffic conditions. The sensing data from volunteer contributors can be further analyzed and processed to form crowd intelligence [Zhang et al. 2011], which can be elaborated into three dimensions: personal awareness, social awareness, and urban awareness. Layered on these concepts, we have raised the new term mobile crowd sensing and computing (MCSC) to characterize crowd intelligence extraction from large-scale and heterogeneous usercontributed data [Guo et al. 2014]. A formal definition of MCSC is as follows: a new sensing paradigm that empowers ordinary citizens to contribute data sensed or generated from their mobile devices, then aggregates and fuses the data in the cloud for crowd intelligence extraction and human-centric service delivery. It has the following three features compared to participatory sensing: ©2015 ACM 2157-6904/2015/04-ART36 $15.00.",,Extraction; Human computer interaction; Mobile telecommunication systems; Sensor networks; Emerging computing paradigm; Formal definition; Mobile-phone users; Participatory Sensing; Participatory sensor networks; Service delivery; Three dimensions; Traffic conditions; Data mining
Sensing the pulse of urban refueling behavior:A perspective from taxi mobility,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928551049&doi=10.1145%2f2644828&partnerID=40&md5=5ab5877768407331046086bcc329cfae,"Urban transportation is an important factor in energy consumption and pollution, and is of increasing concern due to its complexity and economic significance. Its importance will only increase as urbanization continues around the world. In this article, we explore drivers' refueling behavior in urban areas. Compared to questionnaire-based methods of the past, we propose a complete data-driven system that pushes towards real-time sensing of individual refueling behavior and citywide petrol consumption. Our system provides the following: detection of individual refueling events (REs) from which refueling preference can be analyzed; estimates of gas station wait times from which recommendations can be made; an indication of overall fuel demand from which macroscale economic decisions can be made, and a spatial, temporal, and economic view of urban refueling characteristics. For individual behavior, we use reported trajectories from a fleet of GPS-equipped taxicabs to detect gas station visits. For time spent estimates, to solve the sparsity issue along time and stations, we propose context-aware tensor factorization (CATF), a factorization model that considers a variety of contextual factors (e.g., price, brand, and weather condition) that affect consumers' refueling decision. For fuel demand estimates, we apply a queue model to calculate the overall visits based on the time spent inside the station. We evaluated our system on large-scale and real-world datasets, which contain 4-month trajectories of 32,476 taxicabs, 689 gas stations, and the self-reported refueling details of 8,326 online users. The results show that our system can determine REs with an accuracy of more than 90%, estimate time spent with less than 2 minutes of error, and measure overall visits in the same order of magnitude with the records in the field study. ©2015 ACM 2157-6904/2015/04-ART37 $15.00.",Arrival rate; Expected duration; Refueling event; Spatial-temporal unit,Energy utilization; Factorization; Fleet operations; Gas plants; Large dataset; Surveys; Taxicabs; Arrival rates; Economic decisions; Expected duration; Factorization model; Individual behavior; Real-world datasets; Spatial temporals; Tensor factorization; Urban transportation
Simplifying data disclosure configurations in a cloud computing environment,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929153098&doi=10.1145%2f2700472&partnerID=40&md5=23026230a68fdc0ec1672ce219bb1827,"Cloud computing offers a compelling vision of computation, enabling an unprecedented level of data distribution and sharing. Beyond improving the computing infrastructure, cloud computing enables a higher level of interoperability between information systems, simplifying tasks such as sharing documents between coworkers or enabling collaboration between an organization and its suppliers. While these abilities may result in significant benefits to users and organizations, they also present privacy challenges due to unwanted exposure of sensitive information. As information-sharing processes in cloud computing are complex and domain specific, configuring these processes can be an overwhelming and burdensome task for users. This article investigates the feasibility of configuring sharing processes through a small and representative set of canonical configuration options. For this purpose, we present a generic method, named SCON-UP (Simplified CON-figuration of User Preferences). SCON-UP simplifies configuration interfaces by using a clustering algorithm that analyzes a massive set of sharing preferences and condenses them into a small number of discrete disclosure levels. Thus, the user is provided with a usable configuration model while guaranteeing adequate privacy control.We describe the algorithm and empirically evaluate our model using data collected in two user studies (n = 121 and n = 352). Our results show that when provided with three canonical configuration options, on average, 82% of the population can be covered by at least one option. We exemplify the feasibility of discretizing sharing levels and discuss the tradeoff between coverage and simplicity in discrete configuration options. © 2015 ACM.",Artificial intelligence (ai); Cloud computing; Data protection; Human-computer interaction; Information disclosure; Intelligent agents; Preference clustering; Privacy,Artificial intelligence; Cloud computing; Clustering algorithms; Computer privacy; Data privacy; Human computer interaction; Intelligent agents; Interoperability; Security of data; Cloud computing environments; Computing infrastructures; Configuration options; Data protection; Discrete configurations; Information disclosure; Preference clustering; Sensitive informations; Distributed computer systems
EEMC: Enabling energy-efficient mobile crowdsensing with anonymous participants,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929152831&doi=10.1145%2f2644827&partnerID=40&md5=a013639951680ccdfd3544b8013112be,"Mobile Crowdsensing (MCS) requires users to be motivated to participate. However, concerns regarding energy consumption and privacy-among other things-may compromise their willingness to join such a crowd. Our preliminary observations and analysis of common MCS applications have shown that the data transfer in MCS applications may incur significant energy consumption due to the 3G connection setup. However, if data are transferred in parallel with a traditional phone call, then such transfer can be done almost ""for free"": With only an insignificant additional amount of energy required to piggy-back the data- usually incoming task assignments and outgoing sensor results-on top of the call. Here, we present an Energy-Efficient Mobile Crowdsensing (EEMC) framework where task assignments and sensing results are transferred in parallel with phone calls. The main objective, and the principal contribution of this article, is an MCS task assignment scheme that guarantees that aminimum number of anonymous participants return sensor results within a specified time frame, while also minimizing the waste of energy due to redundant task assignments and considering privacy concerns of participants. Evaluations with a large-scale real-world phone call dataset show that our proposed EEMC framework outperforms the baseline approaches, and it can reduce overall energy consumption in data transfer by 54-66% when compared to the 3G-based solution. © 2015 ACM.",Anonymous participants; Energy efficiency; Mobile crowdsensing; Task assignment decision making,Data transfer; Decision making; Energy utilization; Telephone sets; Anonymous participants; Connection setup; Energy efficient; Mobile crowdsensing; Phone calls; Privacy concerns; Task assignment; Time frame; Energy efficiency
Identifying authorities in online communities,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929160209&doi=10.1145%2f2700481&partnerID=40&md5=bca45c0a05095b79f6d06053cfb1a102,"Several approaches have been proposed for the problem of identifying authoritative actors in online communities. However, the majority of existing methods suffer from one or more of the following limitations: (1) There is a lack of an automatic mechanism to formally discriminate between authoritative and nonau-thoritative users. In fact, a common approach to authoritative user identification is to provide a ranked list of users expecting authorities to come first. A major problem of such an approach is the question of where to stop reading the ranked list of users. How many users should be chosen as authoritative? (2) Supervised learning approaches for authoritative user identification suffer from their dependency on the training data. The problem here is that labeled samples are more difficult, expensive, and time consuming to obtain than unlabeled ones. (3) Several approaches rely on some user parameters to estimate an authority score. Detection accuracy of authoritative users can be seriously affected if incorrect values are used. In this article, we propose a parameterless mixture model-based approach that is capable of addressing the three aforementioned issues in a single framework. In our approach, we first represent each user with a feature vector composed of information related to its social behavior and activity in an online community. Next, we propose a statistical framework, based on the multivariate beta mixtures, in order to model the estimated set of feature vectors. The probability density function is therefore estimated and the beta component that corresponds to the most authoritative users is identified. The suitability of the proposed approach is illustrated on real data extracted from the Stack Exchange question-answering network and Twitter. © 2015 ACM.",Authoritative users; Mixture model; Multivariate beta; Online communities; Unsupervised learning,Mixtures; Probability density function; Social networking (online); Unsupervised learning; Websites; Authoritative users; Automatic mechanisms; Mixture model; Multivariate beta; On-line communities; Statistical framework; Supervised learning approaches; User identification; Online systems
Robust multiview feature learning for RGB-D image understanding,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927517773&doi=10.1145%2f2735521&partnerID=40&md5=398a5db58a7d3fd2b6e98b0a99f01352,"The availability of massive RGB-depth (RGB-D) images poses a compelling need for effective RGB-D content understanding techniques. RGB-D images provide synchronized information from multiple views (e.g., color and depth) of real-world objects and scenes. This work proposes learning compact and discriminative features from the multiple views of RGB-D content toward effective feature representation for RGB-D image understanding. In particular, a robust multiview feature learning approach is developed, which exploits the intrinsic relations among multiple views. The feature learning in multiple views is jointly optimized in an integrated formulation. The joint optimization essentially exploits the intrinsic relations among the views, leading to effective features and making the learning process robust to noises. The feature learning function is formulated as a robust nonnegative graph embedding function over multiple graphs in various views. The graphs characterize the local geometric and discriminating structure of the multiview data. The joint sparsity in l1-norm graph embedding and l21-norm data factorization further enhances the robustness of feature learning. We derive an efficient computational solution for the proposed approach and provide rigorous theoretical proof with regard to its convergence. We apply the proposed approach to two RGB-D image understanding tasks: RGB-D object classification and RGB-D scene categorization. We conduct extensive experiments on two real-world RGB-D image datasets. The experimental results have demonstrated the effectiveness of the proposed approach. © 2015 ACM.",Image understanding; Multiview feature learning; RGB-D content,Embeddings; Image understanding; Computational solutions; Discriminative features; Feature learning; Feature representation; Intrinsic relation; Object classification; RGB-D content; Scene categorization; Machine learning
CEPR:A collaborative exploration and periodically returning model for location prediction,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928563654&doi=10.1145%2f2629557&partnerID=40&md5=1b52c9b2ad783c3f293336b932016838,"With the growing popularity of location-based social networks, numerous location visiting records (e.g., checkins) continue to accumulate over time. The more these records are collected, the better we can understand users' mobility patterns and the more accurately we can predict their future locations. However, due to the personality trait of neophilia, people also show propensities of novelty seeking in human mobility, that is, exploring unvisited but tailored locations for them to visit. As such, the existing prediction algorithms, mainly relying on regular mobility patterns, face severe challenges because such behavior is beyond the reach of regularity. As a matter of fact, the prediction of this behavior not only relies on the forecast of novelty-seeking tendency but also depends on how to determine unvisited candidate locations. To this end, we put forward a Collaborative Exploration and Periodically Returning model (CEPR), based on a novel problem, Exploration Prediction (EP), which forecasts whether people will seek unvisited locations to visit, in the following. When people are predicted to do exploration, a state-of-the-art recommendation algorithm, armed with collaborative social knowledge and assisted by geographical influence, will be applied for seeking the suitable candidates; otherwise, a traditional prediction algorithm, incorporating both regularity and the Markov model, will be put into use for figuring out the most possible locations to visit. We then perform case studies on check-ins and evaluate them on two large-scale check-in datasets with 6M and 36M records, respectively. The evaluation results show that EP achieves a roughly 20% classification error rate on both datasets, greatly outperforming the baselines, and that CEPR improves performances by as much as 30% compared to the traditional location prediction algorithms. Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering, information filtering;H.2.8 [Database Management]: Database Applications-Data mining, spatial databases and GIS ©c 2015 ACM Additional.",Exploration prediction; Location prediction; Location recommendation; Location-based services; Social network,Classification (of information); Data mining; Database systems; Digital storage; Error analysis; Forecasting; Information filtering; Information management; Large dataset; Location; Markov processes; Social networking (online); Telecommunication services; Classification error rate; H.2.8 [database management]: database applications - data minings; H.3.3 [information storage and retrieval]: information search and retrievals; Location prediction; Location-based social networks; Prediction algorithms; Recommendation algorithms; Spatial databases and GIS; Location based services
A sparse projection and low-rank recovery framework for handwriting representation and salient stroke feature extraction,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925238812&doi=10.1145%2f2601408&partnerID=40&md5=255a07409ca020ed117a6c2f61acd358,"In this article, we consider the problem of simultaneous low-rank recovery and sparse projection. More specifically, a new Robust Principal Component Analysis (RPCA)-based framework called Sparse Projection and Low-Rank Recovery (SPLRR) is proposed for handwriting representation and salient stroke feature extraction. In addition to achieving a low-rank component encoding principal features and identify errors or missing values from a given data matrix as RPCA, SPLRR also learns a similarity-preserving sparse projection for extracting salient stroke features and embedding new inputs for classification. These properties make SPLRR applicable for handwriting recognition and stroke correction and enable online computation. A cosine-similarity-style regularization term is incorporated into the SPLRR formulation for encoding the similarities of local handwriting features. The sparse projection and low-rank recovery are calculated from a convex minimization problem that can be efficiently solved in polynomial time. Besides, the supervised extension of SPLRR is also elaborated. The effectiveness of our SPLRR is examined by extensive handwritten digital repairing, stroke correction, and recognition based on benchmark problems. Compared with other related techniques, SPLRR delivers strong generalization capability and state-of-the-art performance for handwriting representation and recognition. © 2015 ACM.",Handwriting representation and recognition; Low-rank recovery; Salient stroke feature extraction; Similarity preservation; Sparse projection,Encoding (symbols); Extraction; Feature extraction; Polynomial approximation; Recovery; Signal encoding; Generalization capability; Handwriting recognition; Handwriting representation and recognition; Low-rank recoveries; Robust principal component analysis; Sparse projection; State-of-the-art performance; Stroke feature extraction; Character recognition
Modeling the thermal dynamics of buildings: A latent-force-model-based approach,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925252082&doi=10.1145%2f2629674&partnerID=40&md5=bbaa6a84e3dbb4d4fdd90d3c399c5b92,"Minimizing the energy consumed by heating, ventilation, and air conditioning (HVAC) systems of residential buildings without impacting occupants' comfort has been highlighted as an important artificial intelligence (AI) challenge. Typically, approaches that seek to address this challenge use a model that captures the thermal dynamics within a building, also referred to as a thermal model. Among thermal models, gray-box models are a popular choice for modeling the thermal dynamics of buildings. They combine knowledge of the physical structure of a building with various data-driven inputs and are accurate estimators of the state (internal temperature). However, existing gray-box models require a detailed specification of all the physical elements that can affect the thermal dynamics of a building a priori. This limits their applicability, particularly in residential buildings, where additional dynamics can be induced by human activities such as cooking, which contributes additional heat, or opening of windows, which leads to additional leakage of heat. Since the incidence of these additional dynamics is rarely known, their combined effects cannot readily be accommodated within existing models. To overcome this limitation and improve the general applicability of gray-box models, we introduce a novel model, which we refer to as a latent force thermal model of the thermal dynamics of a building, or LFM-TM. Our model is derived from an existing gray-box thermal model, which is augmented with an extra term referred to as the learned residual. This term is capable of modeling the effect of any a priori unknown additional dynamic, which, if not captured, appears as a structure in a thermal model's residual (the error induced by the model). More importantly, the learned residual can also capture the effects of physical elements such as a building's envelope or the lags in a heating system, leading to a significant reduction in complexity compared to existing models. To evaluate the performance of LFM-TM, we apply it to two independent data sources. The first is an established dataset, referred to as the FlexHouse data, which was previously used for evaluating the efficacy of existing gray-box models [Bacher and Madsen 2011]. The second dataset consists of heating data logged within homes located on the University of Southampton campus, which were specifically instrumented to collect data for our thermal modeling experiments. On both datasets, we show that LFM-TM outperforms existing models in its ability to accurately fit the observed data, generate accurate day-ahead internal temperature predictions, and explain a large amount of the variability in the future observations. This, along with the fact that we also use a corresponding efficient sequential inference scheme for LFM-TM, makes it an ideal candidate for model-based predictive control, where having accurate online predictions of internal temperatures is essential for high-quality solutions. © 2015 ACM.",Artificial intelligence; Energy savings; HVAC; Latent force models; Residential buildings; Smart homes; Sustainability; Thermal models,Air conditioning; Artificial intelligence; Automation; Dynamics; Energy conservation; Housing; Intelligent buildings; Large dataset; Model predictive control; Sustainable development; Thermography (temperature measurement); Force model; High-quality solutions; Internal temperature; Model based predictive control; Residential building; Smart homes; Thermal model; University of Southampton; HVAC
Accurate and robust moving-object segmentation for telepresence systems,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927513112&doi=10.1145%2f2629480&partnerID=40&md5=b3333e6485ab90aa412b631f2e372b05,"Moving-object segmentation is the key issue of Telepresence systems. With monocular camera-based segmentation methods, desirable segmentation results are hard to obtain in challenging scenes with ambiguous color, illumination changes, and shadows. Approaches based on depth sensors often cause holes inside the object and missegmentations on the object boundary due to inaccurate and unstable estimation of depth data. This work proposes an adaptive multi-cue decision fusion method based on Kinect (which integrates a depth sensor with an RGB camera). First, the algorithm obtains an initial foreground mask based on the depth cue. Second, the algorithm introduces a postprocessing framework to refine the segmentation results, which consists of two main steps: (1) automatically adjusting the weight of two weak decisions to identify foreground holes based on the color and contrast cue separately; and (2) refining the object boundary by integrating the motion probability weighted temporal prior, color likelihood, and smoothness constraint. The extensive experiments we conducted demonstrate that our method can segment moving objects accurately and robustly in various situations in real time. © 2015 ACM.",Adaptivemulti-cue decision fusion; Foreground hole detection; Foreground segmentation; Object boundary refining,Cameras; Color; Refining; Visual communication; Decision fusion; Decision fusion methods; Foreground segmentation; Hole detection; Moving object segmentation; Object boundaries; Smoothness constraints; Tele-presence systems; Image segmentation
When location meets social multimedia: A survey on vision-based recognition and mining for geo-social multimedia analytics,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929151037&doi=10.1145%2f2597181&partnerID=40&md5=44678794a2a852b00286a5829a60decd,"Coming with the popularity of multimedia sharing platforms such as Facebook and Flickr, recent years have witnessed an explosive growth of geographical tags on social multimedia content. This trend enables a wide variety of emerging applications, for example, mobile location search, landmark recognition, scene reconstruction, and touristic recommendation, which range from purely research prototype to commercial systems. In this article, we give a comprehensive survey on these applications, covering recent advances in recognition and mining of geographical-aware social multimedia. We review related work in the past decade regarding to location recognition, scene summarization, tourism suggestion, 3D building modeling, mobile visual search and city navigation. At the end, we further discuss potential challenges, future topics, as well as open issues related to geo-social multimedia computing, recognition, mining, and analytics. © 2015 ACM.",Algorithms; Image analysis; Internet; Knowledge representation; Multimedia systems,Algorithms; Image analysis; Internet; Knowledge representation; Multimedia systems; Surveys; Emerging applications; Landmark recognition; Location recognition; Mobile visual searches; Multimedia computing; Multimedia contents; Scene reconstruction; Scene summarizations; Data mining
On discovery of spatiotemporal influence-based moving clusters,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925242195&doi=10.1145%2f2631926&partnerID=40&md5=797db42a60fd75391578b629c479037f,"A moving object cluster is a set of objects that move close to each other for a long time interval. Existing works have utilized object trajectories to discover moving object clusters efficiently. In this article, we define a spatiotemporal influence-based moving cluster that captures spatiotemporal influence spread over a set of spatial objects. A spatiotemporal influence-based moving cluster is a sequence of spatial clusters, where each cluster is a set of nearby objects, such that each object in a cluster influences at least one object in the next immediate cluster and is also influenced by an object from the immediate preceding cluster. Reallife examples of spatiotemporal influence-based moving clusters include diffusion of infectious diseases and spread of innovative ideas. We study the discovery of spatiotemporal influence-based moving clusters in a database of spatiotemporal events. While the search space for discovering all spatiotemporal influencebased moving clusters is prohibitively huge, we design a method, STIMer, to efficiently retrieve the maximal answer. The algorithm STIMer adopts a top-down recursive refinement method to generate the maximal spatiotemporal influence-based moving clusters directly. Empirical studies on the real data as well as large synthetic data demonstrate the effectiveness and efficiency of our method. © 2015 ACM.",Spatiotemporal events; Spatiotemporal influence-based moving clusters,Effectiveness and efficiencies; Empirical studies; Infectious disease; Innovative ideas; Object trajectories; Refinement methods; Spatio-temporal events; Spatiotemporal influence-based moving clusters; Fuzzy clustering
From RGB-D images to RGB images: Single labeling for mining visual models,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927518365&doi=10.1145%2f2629701&partnerID=40&md5=e440e171bc870b26b0abe6536194d28e,"Mining object-level knowledge, that is, building a comprehensive category model base, from a large set of cluttered scenes presents a considerable challenge to the field of artificial intelligence. How to initiate model learning with the least human supervision (i.e., manual labeling) and how to encode the structural knowledge are two elements of this challenge, as they largely determine the scalability and applicability of any solution. In this article, we propose a model-learning method that starts from a single-labeled object for each category, and mines further model knowledge from a number of informally captured, cluttered scenes. However, in these scenes, target objects are relatively small and have large variations in texture, scale, and rotation. Thus, to reduce the model bias normally associated with less supervised learning methods, we use the robust 3D shape in RGB-D images to guide our model learning, then apply the properly trained category models to both object detection and recognition in more conventional RGB images. In addition to model training for their own categories, the knowledge extracted from the RGB-D images can also be transferred to guide model learning for a new category, in which only RGB images without depth information in the new category are provided for training. Preliminary testing shows that the proposed method performs as well as fully supervised learning methods. © 2015 ACM.",Big visual data; Computer vision; Data mining; RGB-D sensor; Transfer learning; Visual knowledge base; Visual mining,Computer vision; Data mining; Knowledge based systems; Object detection; Object recognition; Supervised learning; Textures; Transfer learning; Well testing; Depth information; Object detection and recognition; Rgb-d sensors; Structural knowledge; Supervised learning methods; Visual data; Visual knowledge; Visual mining; Learning systems
Exploring spatial correlation for visual object retrieval,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923287525&doi=10.1145%2f2641576&partnerID=40&md5=10234b9b0765a6720aff28944f29fea2,"Bag-of-visual-words (BOVW)-based image representation has received intense attention in recent years and has improved content-based image retrieval (CBIR) significantly. BOVW does not consider the spatial correlation between visual words in natural images and thus biases the generated visual words toward noise when the corresponding visual features are not stable. This article outlines the construction of a visual word co-occurrence matrix by exploring visual word co-occurrence extracted from small affine-invariant regions in a large collection of natural images. Based on this co-occurrence matrix, we first present a novel high-order predictor to accelerate the generation of spatially correlated visual words and a penalty tree (PTree) to continue generating the words after the prediction. Subsequently, we propose two methods of co-occurrence weighting similarity measure for image ranking: Co-Cosine and Co-TFIDF. These two new schemes downweight the contributions of the words that are less discriminative because of frequent co-occurrences with other words.We conduct experiments on Oxford and Paris Building datasets, in which the ImageNet dataset is used to implement a large-scale evaluation. Cross-dataset evaluations between the Oxford and Paris datasets and Oxford and Holidays datasets are also provided. Thorough experimental results suggest that our method outperforms the state of the art without adding much additional cost to the BOVW model. © 2015 ACM.",BOVW; Co-Cosine; Co-TFIDF; High-order predictor; Penalty tree; Spatial correlation,Correlation; Data Bases; Image Analysis; Information Retrieval; Content based retrieval; Forestry; Object recognition; BOVW; Co-occurrence-matrix; Content-Based Image Retrieval; Cross-dataset evaluation; High-order; Image representations; Penalty tree; Spatial correlations; Image retrieval
A hybrid multigroup coclustering recommendation framework based on information fusion,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927519201&doi=10.1145%2f2700465&partnerID=40&md5=4bc484ad59d5dff52fbc10d96e79fc39,"Collaborative Filtering (CF) is one of the most successful algorithms in recommender systems. However, it suffers from data sparsity and scalability problems. Although many clustering techniques have been incorporated to alleviate these two problems, most of them fail to achieve further significant improvement in recommendation accuracy. First of all, most of them assume each user or item belongs to a single cluster. Since usually users can hold multiple interests and items may belong to multiple categories, it is more reasonable to assume that users and items can join multiple clusters (groups), where each cluster is a subset of like-minded users and items they prefer. Furthermore, most of the clustering-based CF models only utilize historical rating information in the clustering procedure but ignore other data resources in recommender systems such as the social connections of users and the correlations between items. In this article, we propose HMCoC, a Hybrid Multigroup CoClustering recommendation framework, which can cluster users and items into multiple groups simultaneously with different information resources. In our framework, we first integrate information of user-item rating records, user social networks, and item features extracted from the DBpedia knowledge base. We then use an optimization method to mine meaningful user-item groups with all the information. Finally, we apply the conventional CF method in each cluster to make predictions. By merging the predictions from each cluster, we generate the top-n recommendations to the target users for return. Extensive experimental results demonstrate the superior performance of our approach in top-n recommendation in terms of MAP, NDCG, and F1 compared with other clustering-based CF models. © 2015 ACM.",Coclustering; Collaborative filtering; Data sparsity; Information fusion; Recommender systems,Cluster analysis; Collaborative filtering; Information fusion; Knowledge based systems; Clustering procedure; Clustering techniques; Co-clustering; Data sparsity; Information resource; Optimization method; Recommendation accuracy; Scalability problems; Recommender systems
Resources sequencing using automatic prerequisite-outcome annotation,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925263959&doi=10.1145%2f2505349&partnerID=40&md5=d0096f216bce1a9d2e7bf7d32a640185,"The objective of any tutoring system is to provide resources to learners that are adapted to their current state of knowledge. With the availability of a large variety of online content and the disjunctive nature of results provided by traditional search engines, it becomes crucial to provide learners with adapted learning paths that propose a sequence of resources that match their learning objectives. In an ideal case, the sequence of documents provided to the learner should be such that each new document relies on concepts that have been already defined in previous documents. Thus, the problem of determining an effective learning path from a corpus of web documents depends on the accurate identification of outcome and prerequisite concepts in these documents and on their ordering according to this information. Until now, only a few works have been proposed to distinguish between prerequisite and outcome concepts, and to the best of our knowledge, no method has been introduced so far to benefit from this information to produce a meaningful learning path. To this aim, this article first describes a concept annotation method that relies on machine-learning techniques to predict the class of each concept-prerequisite or outcome-on the basis of contextual and local features. Then, this categorization is exploited to produce an automatic resource sequencing on the basis of different representations and scoring functions that transcribe the precedence relation between learning resources. Experiments conducted on a real dataset built from online resources show that our concept annotation approach outperforms the baseline method and that the learning paths automatically generated are consistent with the ground truth provided by the author of the online content. © 2015 ACM.",Adapted learning path; Classification; Resource sequencing; Scoring function; Tutoring system,Classification (of information); Search engines; Automatically generated; Effective learning; Learning objectives; Learning paths; Precedence relations; Resource sequencing; Scoring functions; Tutoring system; Learning systems
A real-time hand posture recognition system using deep neural networks,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927514857&doi=10.1145%2f2735952&partnerID=40&md5=070b3e4c35a29df771ec0f2747a8dec0,"Hand posture recognition (HPR) is quite a challenging task, due to both the difficulty in detecting and tracking hands with normal cameras and the limitations of traditional manually selected features. In this article, we propose a two-stage HPR system for Sign Language Recognition using a Kinect sensor. In the first stage, we propose an effective algorithm to implement hand detection and tracking. The algorithm incorporates both color and depth information, without specific requirements on uniform-colored or stable background. It can handle the situations in which hands are very close to other parts of the body or hands are not the nearest objects to the camera and allows for occlusion of hands caused by faces or other hands. In the second stage, we apply deep neural networks (DNNs) to automatically learn features from hand posture images that are insensitive to movement, scaling, and rotation. Experiments verify that the proposed system works quickly and accurately and achieves a recognition accuracy as high as 98.12%. © 2015 ACM.",Deep neural networks; Hand tracking; Kinect; Posture recognition,Cameras; Deep neural networks; Palmprint recognition; Depth information; Effective algorithms; Hand posture recognition; Hand tracking; Kinect; Posture recognition; Recognition accuracy; Sign Language recognition; Neural networks
Identifying controversial wikipedia articles using editor collaboration networks,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928562232&doi=10.1145%2f2630075&partnerID=40&md5=463d65949be6dc5a2aa0ff2908e4da89,"Wikipedia is probably the most commonly used knowledge reference nowadays, and the high quality of its articles is widely acknowledged. Nevertheless, disagreement among editors often causes some articles to become controversial over time. These articles span thousands of popular topics, including religion, history, and politics, to name a few, and are manually tagged as controversial by the editors, which is clearly suboptimal.Moreover, disagreement, bias, and conflict are expressed quite differently inWikipedia compared to other social media, rendering previous approaches ineffective. On the other hand, the social process of editing Wikipedia is partially captured in the edit history of the articles, opening the door for novel approaches. This article describes a novel controversy model that builds on the interaction history of the editors and not only predicts controversy but also sheds light on the process that leads to controversy. The model considers the collaboration history of pairs of editors to predict their attitude toward one another. This is done in a supervisedway, where the votes ofWikipedia administrator elections are used as labels indicating agreement (i.e., support vote) or disagreement (i.e., oppose vote). From each article, a collaboration network is built, capturing the pairwise attitude among editors, allowing the accurate detection of controversy. Extensive experimental results establish the superiority of this approach compared to previous work and very competitive baselines on a wide range of settings. © 2015 ACM.",Classification; Collaboration; Controversial articles; Wikipedia,Classification (of information); Collaboration; Collaboration network; Controversial articles; High quality; Interaction history; Social process; Wikipedia; Wikipedia articles; Social networking (online)
An approach to ballet dance training through MS kinect and visualization in a CAVE virtual reality environment,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927511803&doi=10.1145%2f2735951&partnerID=40&md5=06dcc5f45a798290413b542c2a35c8ad,"This article proposes a novel framework for the real-time capture, assessment, and visualization of ballet dance movements as performed by a student in an instructional, virtual reality (VR) setting. The acquisition of human movement data is facilitated by skeletal joint tracking captured using the popular Microsoft (MS) Kinect camera system, while instruction and performance evaluation are provided in the form of 3D visualizations and feedback through a CAVE virtual environment, in which the student is fully immersed. The proposed framework is based on the unsupervised parsing of ballet dance movement into a structured posture space using the spherical self-organizing map (SSOM). A unique feature descriptor is proposed to more appropriately reflect the subtleties of ballet dance movements, which are represented as gesture trajectories through posture space on the SSOM. This recognition subsystem is used to identify the category of movement the student is attempting when prompted (by a virtual instructor) to perform a particular dance sequence. The dance sequence is then segmented and cross-referenced against a library of gestural components performed by the teacher. This facilitates alignment and score-based assessment of individual movements within the context of the dance sequence. An immersive interface enables the student to review his or her performance from a number of vantage points, each providing a unique perspective and spatial context suggestive of how the student mightmake improvements in training. An evaluation of the recognition and virtual feedback systems is presented. © 2015 ACM.","Ballet; Dance; Gesture recognition; Human-computer interaction; Immersive training and simulation; MS Kinect; Self-organizing maps, CAVE; Virtual reality",Caves; Conformal mapping; E-learning; Gesture recognition; Human computer interaction; Self organizing maps; Students; Three dimensional computer graphics; Visualization; Ballet; Dance; Gestural components; Gesture trajectories; MS-Kinect; Training and simulations; Virtual instructors; Virtual-reality environment; Virtual reality
Transfer learning across feature-rich heterogeneous feature spaces via feature-space remapping (FSR),2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925250435&doi=10.1145%2f2629528&partnerID=40&md5=26a8a3967fa2162ff90c5dd1316fac22,"Transfer learning aims to improve performance on a target task by utilizing previous knowledge learned from source tasks. In this paper we introduce a novel heterogeneous transfer learning technique, Feature- Space Remapping (FSR), which transfers knowledge between domains with different feature spaces. This is accomplished without requiring typical feature-feature, feature instance, or instance-instance co-occurrence data. Instead we relate features in different feature-spaces through the construction of metafeatures. We show how these techniques can utilize multiple source datasets to construct an ensemble learner which further improves performance.We apply FSR to an activity recognition problem and a document classification problem. The ensemble technique is able to outperform all other baselines and even performs better than a classifier trained using a large amount of labeled data in the target domain. These problems are especially difficult because, in addition to having different feature-spaces, the marginal probability distributions and the class labels are also different. This work extends the state of the art in transfer learning by considering large transfer across dramatically different spaces. © 2015 ACM.",Activity recognition; Domain adaption; Heterogeneous transfer learning; Text classification,Character recognition; Classification (of information); Information retrieval systems; Probability distributions; Text processing; Activity recognition; Document Classification; Domain adaptions; Ensemble techniques; Heterogeneous features; Improve performance; Learning techniques; Text classification; Transfer learning
A fast parallel stochastic gradient method for matrix factorization in shared memory systems,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925237532&doi=10.1145%2f2668133&partnerID=40&md5=b438d9021e773dc6373a957f5c0cb31e,"Matrix factorization is known to be an effective method for recommender systems that are given only the ratings from users to items. Currently, stochastic gradient (SG) method is one of the most popular algorithms for matrix factorization. However, as a sequential approach, SG is difficult to be parallelized for handling web-scale problems. In this article, we develop a fast parallel SG method, FPSG, for shared memory systems. By dramatically reducing the cache-miss rate and carefully addressing the load balance of threads, FPSG is more efficient than state-of-the-art parallel algorithms for matrix factorization. © 2015 ACM.",Matrix factorization; Parallel computing; Recommender system; Shared memory algorithm; Stochastic gradient descent,Factorization; Gradient methods; Memory architecture; Parallel processing systems; Recommender systems; Stochastic systems; Cache miss rates; Matrix factorizations; Sequential approach; Shared memory algorithms; Shared memory system; Stochastic gradient; Stochastic gradient descent; Stochastic gradient methods; Matrix algebra
Combining sketching and traditional diagram editing tools,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925238469&doi=10.1145%2f2631925&partnerID=40&md5=a9cb46a89c193d0163caf97cd7635dfc,"The least cognitively demanding way to create a diagram is to draw it with a pen. Yet there is also a need for more formal visualizations, that is, diagrams created using both traditional keyboard andmouse interaction. Our objective is to allow the creation of diagrams using traditional and stylus-based input. Having two diagram creation interfaces requires that changes to a diagram should be automatically rendered in the other visualization. Because sketches are imprecise, there is always the possibility that conversion between visualizations results in a lack of syntactic consistency between the two visualizations. We propose methods for converting diagrams between forms, checking them for equivalence, and rectifying inconsistencies. As a result of our theoretical contributions, we present an intelligent software system allowing users to create and edit diagrams in sketch or formal mode. Our proof-of-concept tool supports diagrams with connected and spatial syntactic elements. Two user studies show that this approach is viable and participants found the software easy to use. We conclude that supporting such diagram creation is now possible in practice. © 2015 ACM.",Diagram editing; Sketching interfaces; Tools for visual languages,Syntactics; Visual languages; Diagram editing; Editing tools; Intelligent software systems; Proof of concept; Sketching interface; Tool support; User study; Visualization
Improved approaches with calibrated neighboring joint density to steganalysis and seam-carved forgery detection in JPEG images,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925137351&doi=10.1145%2f2560365&partnerID=40&md5=522f5c8644a491200b07710820b62d35,"Steganalysis and forgery detection in image forensics are generally investigated separately. We have designed a method targeting the detection of both steganography and seam-carved forgery in JPEG images. We analyze the neighboring joint density of the DCT coefficients and reveal the difference between the untouched image and the modified version. In realistic detection, the untouched image and themodified version may not be obtained at the same time, and different JPEG images may have different neighboring joint density features. By exploring the self-calibration under different shift recompressions, we propose calibrated neighboring joint density-based approaches with a simple feature set to distinguish steganograms and tampered images from untouched ones. Our study shows that this approach has multiple promising applications in image forensics. Compared to the state-of-the-art steganalysis detectors, our approach delivers better or comparable detection performances with a much smaller feature set while detecting several JPEG-based steganographic systems including DCT-embedding-based adaptive steganography and Yet Another Steganographic Scheme (YASS). Our approach is also effective in detecting seam-carved forgery in JPEG images. By integrating calibrated neighboring density with spatial domain rich models that were originally designed for steganalysis, the hybrid approach obtains the best detection accuracy to discriminate seam-carved forgery from an untouched image. Our study also offers a promising manner to explore steganalysis and forgery detection together. ©2014 ACM.",Calibration; Image tampering; JPEG; Neighboring joint density; Seam carving; Steganalysis; Steganography; YASS,Calibration; Steganography; Image tampering; Joint densities; JPEG; Seam carving; Steganalysis; YASS; Feature extraction
Sponsored search auctions: Recent advances and future directions,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925164786&doi=10.1145%2f2668108&partnerID=40&md5=bea7f2f49154def95e59c1a13cab435d,"Sponsored search has been proven to be a successful business model, and sponsored search auctions have become a hot research direction. There have been many exciting advances in this field, especially in recent years, while at the same time, there are also many open problems waiting for us to resolve. In this article, we provide a comprehensive review of sponsored search auctions in hopes of helping both industry practitioners and academic researchers to become familiar with this field, to know the state of the art, and to identify future research topics. Specifically, we organize the article into two parts. In the first part, we review research works on sponsored search auctions with basic settings, where fully rational advertisers without budget constraints, preknown click-through rates (CTRs) without interdependence, and exact match between queries and keywords are assumed. Under these assumptions, we first introduce the generalized second price (GSP) auction, which is the most popularly used auction mechanism in the industry. Then we give the definitions of several well-studied equilibria and review the latest results on GSP's efficiency and revenue in these equilibria. In the second part, we introduce some advanced topics on sponsored search auctions. In these advanced topics, one or more assumptions made in the basic settings are relaxed. For example, the CTR of an ad could be unknown and dependent on other ads; keywords could be broadly matched to queries before auctions are executed; and advertisers are not necessarily fully rational, could have budget constraints, and may prefer rich bidding languages. Given that the research on these advanced topics is still immature, in each section of the second part, we provide our opinions on how to make further advances, in addition to describing what has been done by researchers in the corresponding direction. © 2015 ACM.",Efficiency and revenue.; Equilibrium concepts; Generalized second price auction; Mechanism design; Sponsored search auctions,Budget control; Machine design; Auction mechanisms; Bidding languages; Budget constraint; Click-through rate; Mechanism design; Second-price auction; Sponsored search auctions; Sponsored searches; Commerce
Introduction to the special issue on online advertising,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925082482&doi=10.1145%2f2668123&partnerID=40&md5=d77a74eb1ee964ae026118cb15abe618,[No abstract available],,
Community discovery from social media by low-rank matrix recovery,2015,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925655894&doi=10.1145%2f2668110&partnerID=40&md5=7ed7882629007aae8addfdeb7c4146b8,"The pervasive usage and reach of socialmedia have attracted a surge of attention in the multimedia research community. Community discovery from socialmedia has therefore become an important yet challenging issue. However, due to the subjective generating process, the explicitly observed communities (e.g., group-user and user-user relationship) are often noisy and incomplete in nature. This paper presents a novel approach to discovering communities from social media, including the group membership and user friend structure, by exploring a low-rank matrix recovery technique. In particular, we take Flickr as one exemplary social media platform. We first model the observed indicator matrix of the Flickr community as a summation of a lowrank true matrix and a sparse error matrix. We then formulate an optimization problem by regularizing the true matrix to coincide with the available rich context and content (i.e., photos and their associated tags). An iterative algorithm is developed to recover the true community indicator matrix. The proposed approach leads to a variety of social applications, including community visualization, interest group refinement, friend suggestion, and influential user identification. The evaluations on a large-scale testbed, consisting of 4,919 Flickr users, 1,467 interest groups, and over five million photos, show that our approach opens a new yet effective perspective to solve social network problems with sparse learning technique. Despite being focused on Flickr, our technique can be applied in any other social media community. © 2015 ACM.",Community discovery; Context information; Low-rank matrix; Social media; Social networks,Algorithms; Computer system recovery; Iterative methods; Optimization; Recovery; Community discoveries; Context information; Large scale testbed; Low-rank matrices; Low-rank matrix recoveries; Optimization problems; Social media; Social media platforms; Social networking (online)
Snap and play: Auto-generated personalized find-the-difference game,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919625060&doi=10.1145%2f2668109&partnerID=40&md5=2db3fe46a1867f75f9f0d0e512d16246,"In this article, by taking a popular game, the Find-the-Difference (FiDi) game, as a concrete example, we explore how state-of-the-art image processing techniques can assist in developing a personalized, automatic, and dynamic game. Unlike the traditional FiDi game, where image pairs (source image and target image) with five different patches are manually produced by professional game developers, the proposed Personalized FiDi (P-FiDi) electronic game can be played in a fully automatic Snap &Play mode. Snap means that players first take photos with their digital cameras. The newly captured photos are used as source images and fed into the P-FiDi system to autogenerate the counterpart target images for users to play. Four steps are adopted to autogenerate target images: enhancing the visual quality of source images, extracting some changeable patches from the source image, selecting the most suitable combination of changeable patches and difference styles for the image, and generating the differences on the target image with state-of-theart image processing techniques. In addition, the P-FiDi game can be easily redesigned for the im-game advertising. Extensive experiments show that the P-FiDi electronic game is satisfying in terms of player experience, seamless advertisement, and technical feasibility. © 2014 ACM.",,Dynamic game; Electronic games; Image processing technique; Player experience; Source images; State of the art; Target images; Visual qualities; Image enhancement
Diversifying citation recommendations,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919684221&doi=10.1145%2f2668106&partnerID=40&md5=ded9b162ebf6afeea4ce66dba0566351,"Literature search is one of the most important steps of academic research. With more than 100,000 papers published each year just in computer science, performing a complete literature search becomes a Herculean task. Some of the existing approaches and tools for literature search cannot compete with the characteristics of today's literature, and they suffer from ambiguity and homonymy. Techniques based on citation information are more robust to the mentioned issues. Thus, we recently built a Web service called theadvisor, which provides personalized recommendations to researchers based on their papers of interest. Since most recommendation methods may return redundant results, diversifying the results of the search process is necessary to increase the amount of information that one can reach via an automated search. This article targets the problem of result diversification in citation-based bibliographic search, assuming that the citation graph itself is the only information available and no categories or intents are known. The contribution of this work is threefold. We survey various random walk-based diversification methods and enhance them with the direction awareness property to allow users to reach either old, foundational (possibly well-cited and well-known) research papers or recent (most likely less-known) ones. Next, we propose a set of novel algorithms based on vertex selection and query refinement. A set of experiments with various evaluation criteria shows that the proposed ã-RLM algorithm performs better than the existing approaches and is suitable for real-time bibliographic search in practice. © 2014 ACM.",Bibliographic search; Direction awareness; Diversity,Web services; Amount of information; Bibliographic search; Citation information; Direction awareness; Diversity; Evaluation criteria; Personalized recommendation; Recommendation methods; Bibliographies
Location- and query-aware modeling of browsing and click behavior in sponsored search,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937131009&doi=10.1145%2f2534398&partnerID=40&md5=0d165d250a9e630c5816867a07d35ddd,"An online advertisement's clickthrough rate provides a fundamental measure of its quality, which is widely used in ad selection strategies. Unfortunately, ads placed in contexts where they are rarely viewed- or where users are unlikely to be interested in commercial results-may receive few clicks regardless of their quality. In this article, we model the variability of a user's browsing behavior for the purpose of click analysis and prediction in sponsored search. Our model incorporates several important contextual factors that influence ad clickthrough rates, including the user's query and adplacementon search engine result pages.We formally model these factors with respect to the list of ads displayed on a result page, the probability that the user will initiate browsing of this list, and the persistence of the user in browsing the list. We incorporate these factors into existing click models by augmenting them with appropriate query and location biases. Using expectation maximization, we learn the parameters of these augmented models from click signals recorded in the logs of a commercial search engine. To evaluate the performance of the models and to compare them with state-of-the-art performance, we apply standard evaluation metrics, including log-likelihood and perplexity. Our evaluation results indicate that, through the incorporation of query and location biases, significant improvements can be achieved in predicting browsing and click behavior in sponsored search. In addition, we explore the extent to which these biases actually reflect varying behavioral patterns. Our observations confirm that correlations exist between the biases and user search behavior. © 2014 ACM.",Bayesian inference; Click model; Clickthrough; Contextual factors; Query log; Sponsored search,Bayesian networks; Inference engines; Information retrieval; Location; Maximum principle; Search engines; Bayesian inference; Clickthrough; Contextual factors; Query logs; Sponsored searches; Behavioral research
Choosing a candidate using efficient allocation of biased information,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938079235&doi=10.1145%2f2558327&partnerID=40&md5=2086850fcf0cd45a48195e4f031a983b,"This article deals with a decision-making problem concerning an agent who wants to choose a partner from multiple candidates for long-term collaboration. To choose the best partner, the agent can rely on prior information he knows about the candidates. However, to improve his decision, he can request additional information from information sources. Nonetheless, acquiring information from external information sources about candidates may be biased due to different personalities of the agent searching for a partner and the information source. In addition, information may be costly. Considering the bias and the cost of the information sources, the optimization problem addressed in this article is threefold: (1) determining the necessary amount of additional information, (2) selecting information sources from which to request the information, and (3) choosing the candidates on whom to request the additional information. We propose a heuristic to solve this optimization problem. The results of experiments on simulated and real-world domains demonstrate the efficiency of our algorithm. © 2014 ACM.",Decision theory; Multiagent systems,Decision making; Decision theory; Multi agent systems; Optimization; Biased information; Decision-making problem; Efficient allocations; External informations; Information sources; Optimization problems; Prior information; Real world domain; Problem solving
Identifying points of interest using heterogeneous features,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925585718&doi=10.1145%2f2668111&partnerID=40&md5=82911e66bacce0f85fe0064bd67a2718,"Deducing trip-related information from web-scale datasets has received large amounts of attention recently. Identifying points of interest (POIs) in geo-tagged photos is one of these problems. The problem can be viewed as a standard clustering problem of partitioning two-dimensional objects. In this work, we study spectral clustering, which is the first attempt for the identification of POIs. However, there is no unified approach to assigning the subjective clustering parameters, and these parameters vary immensely in different metropolitans and locations. To address this issue, we study a self-tuning technique that can properly determine the parameters for the clustering needed. Besides geographical information, web photos inherently store other rich information. Such heterogenous information can be used to enhance the identification accuracy. Thereby, we study a novel refinement framework that is based on the tightness and cohesion degree of the additional information. We thoroughly demonstrate our findings by web-scale datasets collected from Flickr. © 2014 ACM.",,Clustering algorithms; Social networking (online); Clustering problems; Geographical information; Heterogeneous features; Identification accuracy; Self-tuning technique; Spectral clustering; Subjective clustering; Two-dimensional objects; Web images; Large dataset; Identification (control systems)
Simple and scalable response prediction for display advertising,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937149669&doi=10.1145%2f2532128&partnerID=40&md5=afac07fdc434ed930761e22e66e7c758,"Clickthrough and conversation rates estimation are two core predictions tasks in display advertising. We present in this article a machine learning framework based on logistic regression that is specifically designed to tackle the specifics of display advertising. The resulting system has the following characteristics: It is easy to implement and deploy, it is highly scalable (we have trained it on terabytes of data), and it provides models with state-of-the-art accuracy. © 2014 ACM.",Click prediction; Display advertising; Distributed learning; Feature selection; Hashing; Machine learning,Artificial intelligence; Feature extraction; Learning systems; Display advertisings; Distributed learning; Hashing; Logistic regressions; Response prediction; State of the art; Marketing
Accurate and novel recommendations: An algorithm based on popularity forecasting,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919632368&doi=10.1145%2f2668107&partnerID=40&md5=9cbff7a4e579bcdd09b137454d6a1216,"Recommender systems are in the center of network science, and they are becoming increasingly important in individual businesses for providing efficient, personalized services and products to users. Previous research in the field of recommendation systems focused on improving the precision of the system through designing more accurate recommendation lists. Recently, the community has been paying attention to diversity and novelty of recommendation lists as key characteristics of modern recommender systems. In many cases, novelty and precision do not go hand in hand, and the accuracy-novelty dilemma is one of the challenging problems in recommender systems, which needs efforts in making a trade-off between them. In this work, we propose an algorithm for providing novel and accurate recommendation to users. We consider the standard definition of accuracy and an effective self-information-based measure to assess novelty of the recommendation list. The proposed algorithm is based on item popularity, which is defined as the number of votes received in a certain time interval. Wavelet transform is used for analyzing popularity time series and forecasting their trend in future timesteps. We introduce two filtering algorithms based on the information extracted from analyzing popularity time series of the items. The popularity-based filtering algorithm gives a higher chance to items that are predicted to be popular in future timesteps. The other algorithm, denoted as a novelty and population-based filtering algorithm, is to move toward items with low popularity in past timesteps that are predicted to become popular in the future. The introduced filters can be applied as adds-on to any recommendation algorithm. In this article, we use the proposed algorithms to improve the performance of classic recommenders, including item-based collaborative filtering and Markovbased recommender systems. The experiments show that the algorithms could significantly improve both the accuracy and effective novelty of the classic recommenders. © 2014 ACM.",,Economic and social effects; Recommender systems; Signal filtering and prediction; Time series; Wavelet transforms; Filtering algorithm; Item popularities; Item-based collaborative filtering; Key characteristics; Personalized service; Recommendation algorithms; Self information; Standard definitions; Collaborative filtering
Strategic information disclosure to people with multiple alternatives,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938095778&doi=10.1145%2f2558397&partnerID=40&md5=37182f8864988e6c82943ed07da07305,"In this article, we study automated agents that are designed to encourage humans to take some actions over others by strategically disclosing key pieces of information. To this end, we utilize the framework of persuasion games-a branch of game theory that deals with asymmetric interactions where one player (Sender) possesses more information about the world, but it is only the other player (Receiver) who can take an action. In particular, we use an extended persuasion model, where the Sender's information is imperfect and the Receiver has more than two alternative actions available. We design a computational algorithm that, from the Sender's standpoint, calculates the optimal information disclosure rule. The algorithm is parameterized by the Receiver's decision model (i.e., what choice he will make based on the information disclosed by the Sender) and can be retuned accordingly. We then provide an extensive experimental study of the algorithm's performance in interactions with human Receivers. First, we consider a fully rational (in the Bayesian sense) Receiver decision model and experimentally show the efficacy of the resulting Sender's solution in a routing domain. Despite the discrepancy in the Sender's and the Receiver's utilities from each of the Receiver's choices, our Sender agent successfully persuaded human Receivers to select an option more beneficial for the agent. Dropping the Receiver's rationality assumption, we introduce a machine learning procedure that generates a more realistic human Receiver model. We then show its significant benefit to the Sender solution by repeating our routing experiment. To complete our study, we introduce a second (supply-demand) experimental domain and, by contrasting it with the routing domain, obtain general guidelines for a Sender on how to construct a Receiver model. © 2014 ACM.",Human modeling; Human-agent interaction; Information disclosure; Persuasion,Artificial intelligence; Behavioral research; Computation theory; Game theory; Learning systems; Algorithm's performance; Asymmetric interaction; Computational algorithm; Human agent interactions; Human Model; Information disclosure; Persuasion; Receiver modeling; Algorithms
On unexpectedness in recommender systems: Or how to better expect the unexpected,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919756227&doi=10.1145%2f2559952&partnerID=40&md5=0ba76d5a0daed820556dd9207157f8fd,"Although the broad social and business success of recommender systems has been achieved across several domains, there is still a long way to go in terms of user satisfaction. One of the key dimensions for significant improvement is the concept of unexpectedness. In this article, we propose a method to improve user satisfaction by generating unexpected recommendations based on the utility theory of economics. In particular, we propose a new concept of unexpectedness as recommending to users those items that depart from what they would expect from the system - the consideration set of each user. We define and formalize the concept of unexpectedness and discuss how it differs from the related notions of novelty, serendipity, and diversity. In addition, we suggest several mechanisms for specifying the users' expectations and propose specific performance metrics to measure the unexpectedness of recommendation lists. We also take into consideration the quality of recommendations using certain utility functions and present an algorithm for providing users with unexpected recommendations of high quality that are hard to discover but fairly match their interests. Finally, we conduct several experiments on ""real-world"" datasets and compare our recommendation results with other methods. The proposed approach outperforms these baseline methods in terms of unexpectedness and other important metrics, such as coverage, aggregate diversity and dispersion, while avoiding any accuracy loss. © 2014 ACM.",Diversity; Evaluation; Novelty; Recommendations; Recommender systems; Serendipity; Unexpectedness; Utility theory,Diversity; Evaluation; Novelty; Recommendations; Serendipity; Unexpectedness; Utility theory; Recommender systems
Mining mobile user preferences for personalized context-aware recommendation,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919608533&doi=10.1145%2f2532515&partnerID=40&md5=df392bba8e5b15d5e08649b33618c6df,"Recent advances in mobile devices and their sensing capabilities have enabled the collection of rich contextual information and mobile device usage records through the device logs. These context-rich logs open a venue for mining the personal preferences of mobile users under varying contexts and thus enabling the development of personalized context-aware recommendation and other related services, such asmobile online advertising. In this article, we illustrate how to extract personal context-aware preferences from the context-rich device logs, or context logs for short, and exploit these identified preferences for building personalized contextaware recommender systems. A critical challenge along this line is that the context log of each individual user may not contain sufficient data for mining his or her context-aware preferences. Therefore, we propose to first learn common context-aware preferences from the context logs of many users. Then, the preference of each user can be represented as a distribution of these common context-aware preferences. Specifically, we develop two approaches for mining common context-aware preferences based on two different assumptions, namely, context-independent and context-dependent assumptions, which can fit into different application scenarios. Finally, extensive experiments on a real-world dataset show that both approaches are effective and outperform baselines with respect to mining personal context-aware preferences for mobile users. © 2014 ACM.",,Application scenario; Context dependent; Context independent; Context-aware recommendations; Context-aware recommender systems; Contextual information; Critical challenges; Online advertising; Mobile telecommunication systems
Real-Time Bid Optimization for Group-Buying Ads,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919684238&doi=10.1145%2f2532441&partnerID=40&md5=caeb499d898ab534224174f7290a42fa,"Group-buying ads seeking a minimum number of customers before the deal expiry are increasingly used by daily-deal providers. Unlike traditional web ads, the advertiser's profits for group-buying ads depend on the time to expiry and additional customers needed to satisfy the minimum group size. Since both these quantities are time-dependent, optimal bid amounts to maximize profits change with every impression. Consequently, traditional static bidding strategies are far from optimal. Instead, bid values need to be optimized in real-time to maximize expected bidder profits. This online optimization of deal profits is made possible by the advent of ad exchanges offering real-time (spot) bidding. To this end, we propose a realtime bidding strategy for group-buying deals based on the online optimization of bid values. We derive the expected bidder profit of deals as a function of the bid amounts and dynamically vary the bids to maximize profits. Furthermore, to satisfy time constraints of the online bidding, we present methods of minimizing computation timings. Subsequently, we derive the real-time ad selection, admissibility, and realtime bidding of the traditional ads as the special cases of the proposed method. We evaluate the proposed bidding, selection, and admission strategies on amultimillion click stream of 935 ads. The proposed real-time bidding, selection, and admissibility show significant profit increases over the existing strategies. Further experiments illustrate the robustness of the bidding and acceptable computation timings. © 2014 ACM.",,Customer satisfaction; Ad exchanges; Ad selections; Bid optimizations; Bidding strategy; Online bidding; Online optimization; Time constraints; Time dependent; Profitability
Multiobjective Pareto-efficient approaches for recommender systems,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937125986&doi=10.1145%2f2629350&partnerID=40&md5=aae07fcab3c9d36d89aa806a2ba5f5f5,"Recommender systems are quickly becoming ubiquitous in applications such as e-commerce, social media channels, and content providers, among others, acting as an enabling mechanism designed to overcome the information overload problem by improving browsing and consumption experience. A typical task in many recommender systems is to output a ranked list of items, so that items placed higher in the rank are more likely to be interesting to the users. Interestingness measures include how accurate, novel, and diverse are the suggested items, and the objective is usually to produce ranked lists optimizing one of these measures. Suggesting items that are simultaneously accurate, novel, and diverse is much more challenging, since this may lead to a conflicting-objective problem, in which the attempt to improve a measure further may result in worsening other measures. In this article, we propose new approaches for multiobjective recommender systems based on the concept of Pareto efficiency-a state achieved when the system is devised in the most efficient manner in the sense that there is no way to improve one of the objectives without making any other objective worse off. Given that existing multiobjective recommendation algorithms differ in their level of accuracy, diversity, and novelty, we exploit the Pareto-efficiency concept in two distinct manners: (i) the aggregation of ranked lists produced by existing algorithms into a single one, which we call Pareto-efficient ranking, and (ii) the weighted combination of existing algorithms resulting in a hybrid one, which we call Pareto-efficient hybridization. Our evaluation involves two real application scenarios: music recommendation with implicit feedback (i.e., Last.fm) and movie recommendation with explicit feedback (i.e., MovieLens). We show that the proposed Pareto-efficient approaches are effective in suggesting items that are likely to be simultaneously accurate, diverse, and novel. We discuss scenarios where the system achieves high levels of diversity and novelty without compromising its accuracy. Further, comparison against multiobjective baselines reveals improvements in terms of accuracy (from 10.4% to 10.9%), novelty (from 5.7% to 7.5%), and diversity (from 1.6% to 4.2%). © 2014 ACM.",Multiobjective recommender systems; Pareto efficiency,Algorithms; Efficiency; Electronic commerce; Recommender systems; Conflicting objectives; Information overloads; Interestingness measures; Movie recommendations; Multi objective; Music recommendation; Pareto efficiency; Recommendation algorithms; Pareto principle
Introduction to the special issue on diversity and discovery in recommender systems,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919650417&doi=10.1145%2f2668113&partnerID=40&md5=18bff6c62766abf4810acfe674d5971d,[No abstract available],,
Personalized recommendations of locally interesting venues to tourists via cross-region community matching,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907434311&doi=10.1145%2f2532439&partnerID=40&md5=53be61ebe7a291b6cce5cd81ad2b2190,"You are in a new city. You are not familiar with the places and neighborhoods. You want to know all about the exciting sights, food outlets, and cultural venues that the locals frequent, in particular those that suit your personal interests. Even though there exist many mapping, local search, and travel assistance sites, they mostly provide popular and famous listings such as Statue of Liberty and Eiffel Tower, which are well-known places but may not suit your personal needs or interests. Therefore, there is a gap between what tourists want and what dominant tourism resources are providing. In this work, we seek to provide a solution to bridge this gap by exploiting the rich user-generated location contents in location-based social networks in order to offer tourists the most relevant and personalized local venue recommendations. In particular, we first propose a novel Bayesian approach to extract the social dimensions of people at different geographical regions to capture their latent local interests. We next mine the local interest communities in each geographical region.We then represent each local community using aggregated behaviors of community members. Finally, we correlate communities across different regions and generate venue recommendations to tourists via cross-region community matching.We have sampled a representative subset of check-ins from Foursquare and experimentally verified the effectiveness of our proposed approaches. © 2014 ACM 2157-6904/2014/09-ART48 $15.00.",Cross-region community matching; Locally interesting venues; Location-based social networks; Social dimensions,Bayesian networks; Location; Bayesian approaches; Cross-region community matching; Interest communities; Local community; Locally interesting venues; Location-based social networks; Personalized recommendation; Social dimensions; Geographical regions
"Check-ins in ""blau space"": Applying blau's macrosociological theory to foursquare check-ins from new york city",2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907434831&doi=10.1145%2f2566617&partnerID=40&md5=e922f14e9964bdc052c19d51dd214e83,"Peter Blau was one of the first to define a latent social space and utilize it to provide concrete hypotheses. Blau defines social structure via social ""parameters"" (constraints). Actors that are closer together (more homogenous) in this social parameter space are more likely to interact. One of Blau's most important hypotheses resulting from this work was that the consolidation of parameters could lead to isolated social groups. For example, the consolidation of race and income might lead to segregation. In the present work, we use Foursquare data from New York City to explore evidence of homogeneity along certain social parameters and consolidation that breeds social isolation in communities of locations checked in to by similar users.; More specifically, we first test the extent to which communities detected via Latent Dirichlet Allocation are homogenous across a set of four social constraints-racial homophily, income homophily, personal interest homophily and physical space. Using a bootstrapping approach, we find that 14 (of 20) communities are statistically, and all but one qualitatively, homogenous along one of these social constraints, showing the relevance of Blau's latent space model in venue communities determined via user check-in behavior. We then consider the extent to which communities with consolidated parameters, those homogenous on more than one parameter, represent socially isolated populations. We find communities homogenous on multiple parameters, including a homosexual community and a ""hipster"" community, that show support for Blau's hypothesis that consolidation breeds social isolation. We consider these results in the context of mediated communication, in particular in the context of self-representation on social media. © 2014 ACM 2157-6904/2014/09-ART44 $15.00.",Community structure; Foursquare; Latent social space; Urban analytics,Statistics; Community structures; Foursquare; Latent Dirichlet allocation; Latent space models; Mediated Communication; Multiple parameters; Social spaces; Urban analytics; Behavioral research
Model-based count series clustering for bike sharing system usage mining: A case study with the vélib' system of Paris,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907911507&doi=10.1145%2f2560188&partnerID=40&md5=155584cd9339517e47bccddcbbb748d3,"Today, more and more bicycle sharing systems (BSSs) are being introduced in big cities. These transportation systems generate sizable transportation data, the mining of which can reveal the underlying urban phenomenon linked to city dynamics. This article presents a statistical model to automatically analyze the trip data of a bike sharing system. The proposed solution partitions (i.e., clusters) the stations according to their usage profiles. To do so, count series describing the stations's usage through departure/arrival counts per hour throughout the day are built and analyzed. The model for processing these count series is based on Poisson mixtures and introduces a station scaling factor that handles the differences between the stations's global usage. Differences between weekday and weekend usage are also taken into account. This model identifies the latent factors that shape the geography of trips, and the results may thus offer insights into the relationships between station neighborhood type (its amenities, its demographics, etc.) and the generated mobility patterns. In other words, the proposed method brings to light the different functions in different areas that induce specific patterns in BSS data. These potentials are demonstrated through an in-depth analysis of the results obtained on the Paris Vélib' large-scale bike sharing system. ©2014 ACM 2157-6904/2014/07-ART39 $15.00.",,Bicycles; Bicycle sharing; In-depth analysis; Mobility pattern; Model-based OPC; Poisson mixtures; Scaling factors; Statistical modeling; Transportation system; Urban transportation
Mining user check-in behavior with a random walk for urban point-of-interest recommendations,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907428660&doi=10.1145%2f2523068&partnerID=40&md5=0611972148866684f8975c408080595c,"In recent years, research into the mining of user check-in behavior for point-of-interest (POI) recommendations has attracted a lot of attention. Existing studies on this topic mainly treat such recommendations in a traditional manner-that is, they treat POIs as items and check-ins as ratings. However, users usually visit a place for reasons other than to simply say that they have visited. In this article, we propose an approach referred to as Urban POI-Walk (UPOI-Walk), which takes into account a user's social-triggered intentions (SI), preference-triggered intentions (PreI), and popularity-triggered intentions (PopI), to estimate the probability of a user checking-in to a POI. The core idea of UPOI-Walk involves building a HITS-based random walk on the normalized check-in network, thus supporting the prediction of POI properties related to each user's preferences. To achieve this goal, we define several user-POI graphs to capture the key properties of the check-in behavior motivated by user intentions. In our UPOI-Walk approach, we propose a new kind of random walk model-Dynamic HITS-based Random Walk-which comprehensively considers the relevance between POIs and users from different aspects. On the basis of similitude, we make an online recommendation as to the POI the user intends to visit. To the best of our knowledge, this is the first work on urban POI recommendations that considers user check-in behavior motivated by SI, PreI, and PopI in location-based social network data. Through comprehensive experimental evaluations on two real datasets, the proposed UPOI-Walk is shown to deliver excellent performance. © 2014 ACM 2157-6904/2014/09-ART40 $15.00.","Location-based social network; Point-of-interest recommendation; Urban computing, data mining; User preference mining",Data mining; Random processes; Social networking (online); Social sciences computing; Experimental evaluation; Location-based social networks; Point of interest; Preference mining; Random walk modeling; Real data sets; Urban computing; User's preferences; Behavioral research
Object-oriented travel package recommendation,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907908063&doi=10.1145%2f2542665&partnerID=40&md5=a7e56e844299b0c666c0dbfc623b1a1f,"Providing better travel services for tourists is one of the important applications in urban computing. Though many recommender systems have been developed for enhancing the quality of travel service, most of them lack a systematic and open framework to dynamically incorporate multiple types of additional context information existing in the tourism domain, such as the travel area, season, and price of travel packages. To that end, in this article, we propose an open framework, the Objected-Oriented Recommender System (ORS), for the developers performing personalized travel package recommendations to tourists. This framework has the ability to import all the available additional context information to the travel package recommendation process in a cost-effective way. Specifically, the different types of additional information are extracted and uniformly represented as feature-value pairs. Then, we define the Object, which is the collection of the feature-value pairs. We propose two models that can be used in the ORS framework for extracting the implicit relationships among Objects. The Objected-Oriented Topic Model (OTM) can extract the topics conditioned on the intrinsic feature-value pairs of the Objects. The Objected-Oriented Bayesian Network (OBN) can effectively infer the cotravel probability of two tourists by calculating the co-occurrence time of feature-value pairs belonging to different kinds of Objects. Based on the relationships mined by OTM or OBN, the recommendation list is generated by the collaborative filtering method. Finally, we evaluate these two models and the ORS framework on real-world travel package data, and the experimental results show that the ORS framework is more flexible in terms of incorporating additional context information, and thus leads to better performances for travel package recommendations. Meanwhile, for feature selection in ORS, we define the feature information entropy, and the experimental results demonstrate that using features with lower entropies usually leads to better recommendation results. © 2014 ACM 2157-6904/2014/09-ART40 $15.00.",Bayesian; Collaborative filtering; Object-oriented; Topic model; Travel,Bayesian networks; Cost effectiveness; Recommender systems; Bayesian; Collaborative filtering methods; Context information; Feature information; Implicit relationships; Object oriented; Topic Modeling; Travel; Collaborative filtering
Introduction to the special section on Urban computing,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995680432&doi=10.1145%2f2642650&partnerID=40&md5=50645a0b61d48b01a220561589da4e7f,[No abstract available],,
Home location identification of twitter users,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907448549&doi=10.1145%2f2528548&partnerID=40&md5=03a0e46181de6d4bb975dd2e1d8e2b56,"We present a new algorithm for inferring the home location of Twitter users at different granularities, including city, state, time zone, or geographic region, using the content of users' tweets and their tweeting behavior. Unlike existing approaches, our algorithm uses an ensemble of statistical and heuristic classifiers to predict locations and makes use of a geographic gazetteer dictionary to identify place-name entities. We find that a hierarchical classification approach, where time zone, state, or geographic region is predicted first and city is predicted next, can improve prediction accuracy. We have also analyzed movement variations of Twitter users, built a classifier to predict whether a user was travelling in a certain period of time, and use that to further improve the location detection accuracy. Experimental evidence suggests that our algorithm works well in practice and outperforms the best existing algorithms for predicting the home location of Twitter users. © 2014 ACM 2157-6904/2014/09-ART44 $15.00.",Location; Time zone; Tweets,Forecasting; Social networking (online); Different granularities; Experimental evidence; Hierarchical classification; Location detection; Location identification; Prediction accuracy; Time zones; Tweets; Location
Charging and storage infrastructure design for electric vehicles,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907431278&doi=10.1145%2f2513567&partnerID=40&md5=f5ac2bbdaabb9e113784a27922c67316,"Ushered by recent developments in various areas of science and technology, modern energy systems are going to be an inevitable part of our societies. Smart grids are one of these modern systems that have attracted many research activities in recent years. Before utilizing the next generation of smart grids, we should have a comprehensive understanding of the interdependent energy networks and processes. Nextgeneration energy systems networks cannot be effectively designed, analyzed, and controlled in isolation from the social, economic, sensing, and control contexts in which they operate. In this article, we present a novel framework to support charging and storage infrastructure design for electric vehicles. We develop coordinated clustering techniques to work with network models of urban environments to aid in placement of charging stations for an electrical vehicle deployment scenario. Furthermore, we evaluate the network before and after the deployment of charging stations, to recommend the installation of appropriate storage units to overcome the extra load imposed on the network by the charging stations. We demonstrate the multiple factors that can be simultaneously leveraged in our framework to achieve practical urban deployment. Our ultimate goal is to help realize sustainable energy system management in urban electrical infrastructure by modeling and analyzing networks of interactions between electric systems and urban populations. © 2014 ACM 2157-6904/2014/09-ART40 $15.00.",Charging stations; Clustering; Coordinated clustering; Data mining; Electric vehicles; Smart grids; Storage; Synthetic populations,Charging (batteries); Data mining; Digital storage; Electric power transmission networks; Electric vehicles; Energy storage; Population statistics; Charging station; Clustering; Coordinated clustering; Smart grid; Synthetic populations; Smart power grids
"Urban computing: Concepts, methodologies, and applications",2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907023884&doi=10.1145%2f2629592&partnerID=40&md5=7f3203667e2646f9cb8d6f3084e4f80c,"Urbanization's rapid progress has modernized many people's lives but also engendered big issues, such as traffic congestion, energy consumption, and pollution. Urban computing aims to tackle these issues by using the data that has been generated in cities (e.g., traffic flow, human mobility, and geographical data). Urban computing connects urban sensing, data management, data analytics, and service providing into a recurrent process for an unobtrusive and continuous improvement of people's lives, city operation systems, and the environment. Urban computing is an interdisciplinary field where computer sciences meet conventional city-related fields, like transportation, civil engineering, environment, economy, ecology, and sociology in the context of urban spaces. This article first introduces the concept of urban computing, discussing its general framework and key challenges from the perspective of computer sciences. Second, we classify the applications of urban computing into seven categories, consisting of urban planning, transportation, the environment, energy, social, economy, and public safety and security, presenting representative scenarios in each category. Third, we summarize the typical technologies that are needed in urban computing into four folds, which are about urban sensing, urban data management, knowledge fusion across heterogeneous data, and urban data visualization. Finally, we give an outlook on the future of urban computing, suggesting a few research topics that are somehow missing in the community. © 2014 ACM 2157-6904/2014/09-ART38 $15.00.",Big data; City dynamics; Computing with heterogeneous data; Human mobility; Knowledge fusion; Trajectories; Urban computing; Urban informatics; Urban sensing,Big data; Data Analytics; Data fusion; Data visualization; Energy utilization; Information management; Sociology; Traffic congestion; Trajectories; Heterogeneous data; Human mobility; Knowledge fusion; Urban computing; Urban Informatics; Urban sensing; Urban transportation
Traffic information publication with privacy preservation,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907441582&doi=10.1145%2f2542666&partnerID=40&md5=baa844c6eec3927c7d678562f5d9cfc7,"We are experiencing the expanding use of location-based services such as AT&T's TeleNavGPS Navigator and Intel's Thing Finder. Existing location-based services have collected a large amount of location data, which has great potential for statistical usage in applications like traffic flow analysis, infrastructure planning, and advertisement dissemination. The key challenge is how to wisely use the data without violating each user's location privacy concerns. In this article, we first identify a new privacy problem, namely, the inference-route problem, and then present our anonymization algorithms for privacy-preserving trajectory publishing. The experimental results have demonstrated that our approach outperforms the latest related work in terms of both efficiency and effectiveness. © 2014 ACM 2157-6904/2014/09-ART44 $15.00.",Location privacy; Road-network constraint; Trajectory anonymization,Inference engines; Location; Telecommunication services; Traffic control; Anonymization; Infrastructure planning; Location privacy; Privacy preservation; Privacy preserving; Road network; Traffic flow analysis; Traffic information; Location based services
Intelligent interface for textual attitude analysis,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907445778&doi=10.1145%2f2535912&partnerID=40&md5=e6d9121ab6fd515267b7c26123260ea7,"This article describes a novel intelligent interface for attitude sensing in text driven by a robust computational tool for the analysis of fine-grained attitudes (emotions, judgments, and appreciations) expressed in text. The module responsible for textual attitude analysis was developed using a compositional linguistic approach based on the attitude-conveying lexicon, the analysis of syntactic and dependency relations between words in a sentence, the compositionality principle applied at various grammatical levels, the rules elaborated for semantically distinct verb classes, and a method considering the hierarchy of concepts. The performance of this module was evaluated on sentences from personal stories about life experiences. The developed web-based interface supports recognition of nine emotions, positive and negative judgments, and positive and negative appreciations conveyed in text. It allows users to adjust parameters, to enable or disable various functionality components of the algorithm, and to select the format of text annotation and attitude statistics visualization. © 2014 ACM 2157-6904/2014/09-ART48 $15.00.",Affective computing; Affective user interface; Attitude analysis in text,Character recognition; Multimedia systems; Affective Computing; Affective user interfaces; Attitude analysis; Computational tools; Dependency relation; Intelligent interface; Linguistic approach; Web-based interface; User interfaces
VSRank: A novel framework for ranking-based collaborative filtering,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907446973&doi=10.1145%2f2542048&partnerID=40&md5=e139d33c6f9782cac2ef61baf92282bb,"Collaborative filtering (CF) is an effective technique addressing the information overload problem. CF approaches generally fall into two categories: rating based and ranking based. The former makes recommendations based on historical rating scores of items and the latter based on their rankings. Ranking-based CF has demonstrated advantages in recommendation accuracy, being able to capture the preference similarity between users even if their rating scores differ significantly. In this study, we propose VSRank, a novel framework that seeks accuracy improvement of ranking-based CF through adaptation of the vector space model. In VSRank, we consider each user as a document and his or her pairwiserelative preferences as terms. We then use a novel degree-specialty weighting scheme resembling TF-IDF to weight the terms. Extensive experiments on benchmarks in comparison with the state-of-the-art approaches demonstrate the promise of our approach. © 2014 ACM 2157-6904/2014/09-ART48 $15.00.",Collaborative filtering; Ranking-based collaborative filtering; Recommender systems; Term weighting; Vector space model,Recommender systems; Vector spaces; Accuracy Improvement; Information overloads; Recommendation accuracy; State-of-the-art approach; Term weighting; Vector space models; Weighting scheme; Collaborative filtering
Using digital footprints for a city-scale traffic simulation,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907448234&doi=10.1145%2f2517028&partnerID=40&md5=3ac818b3d613304e8c56272cc73a3c6a,"This article introduces a microsimulation of urban traffic flows within a large-scale scenario implemented for the Greater Dublin region in Ireland. Traditionally, the data available for traffic simulations come from a population census and dedicated road surveys that only partly cover shopping, leisure, or recreational trips. To account for the latter, the presented traffic modeling framework exploits the digital footprints of city inhabitants on services such as Twitter and Foursquare. We enriched the model with findings from our previous studies on geographical layout of communities in a country-wide mobile phone network to account for socially related journeys. These datasets were used to calibrate a variant of a radiation model of spatial choice, which we introduced in order to drive individuals' decisions on trip destinations within an assigned daily activity plan.We observed that given the distribution of population, the workplace locations, a comprehensive set of urban facilities, and a list of typical activity sequences of city dwellers collected withina national travel survey, the developed microsimulation reproduces not only the journey statistics such as peak travel periods but also the traffic volumes at main road segments with surprising accuracy. © 2014 ACM 2157-6904/2014/09-ART40 $15.00.",Social networks; Traffic simulation; Urban analysis,Digital storage; Population statistics; Roads and streets; Social networking (online); Traffic control; Geographical layout; Mobile phone networks; National travel surveys; Radiation modeling; Traffic simulations; Urban analysis; Urban traffic flow; Workplace locations; Surveys
Measuring and recommending time-sensitive routes from location-based data,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907912489&doi=10.1145%2f2542668&partnerID=40&md5=0925ed3244986994dc18e506ab4ce89a,"Location-based services allow users to perform geospatial recording actions, which facilitates the mining of the moving activities of human beings. This article proposes to recommend time-sensitive trip routes consisting of a sequence of locations with associated timestamps based on knowledge extracted from largescale timestamped location sequence data (e.g., check-ins and GPS traces). We argue that a good route should consider (a) the popularity of places, (b) the visiting or er of places, (c) the proper visiting time of each place, and (d) the proper transit time from one place to another. By devising a statistical model, we integrate these four factors into a route goodness function that aims to measure the quality of a route. Equipped with the route goodness, we recommend time-sensitive routes for two scenarios. The first is about constructing the route based on the user-specified source location with the starting time. The second is about composing the route between the specified source location and the destination location given a starting time. To handle these queries, we propose a search method, Guidance Search, which consists of a novel heuristic satisfaction function that guides the search toward the destination location and a backward checking mechanism to boost the effectiveness of the constructed route. Experiments on the Gowalla check-in datasets demonstrate the effectiveness of our model on detecting real routes and performing cloze test of routes, comparing with other baseline methods. We also develop a system TripRouter as a real-time demo platform. © 2014 ACM 2157-6904/2014/09-ART44 $15.00.",Location-based data; Time-sensitive route; Trip recommendation,Heuristic methods; Location; Query processing; Telecommunication services; Baseline methods; Destination location; Location based; Satisfaction functions; Source location; Statistical modeling; Time-sensitive route; Trip recommendation; Location based services
A unified geolocation framework for web videos,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907441012&doi=10.1145%2f2533989&partnerID=40&md5=c975fa0fc89260206b22b4af243ec9ca,"In this article, we propose a unified geolocation framework to automatically determine where on the earth a web video was shot. We analyze different social, visual, and textual relationships from a real-world dataset and find four relationships with apparent geography clues that can be used for web video geolocation. Then, the geolocation process is formulated as an optimization problem that simultaneously takes the social, visual, and textual relationships into consideration. The optimization problem is solved by an iterative procedure, which can be interpreted as a propagation of the geography information among the web video social network. Extensive experiments on a real-world dataset clearly demonstrate the effectiveness of our proposed framework, with the geolocation accuracy higher than state-of-the-art approaches. © 2014 ACM 2157-6904/2014/09-ART48 $15.00.",Geotag; Unified geolocation framework; Web video,Optimization; Geo-tags; Geolocations; Optimization problems; Real-world; State-of-the-art approach; Web video; Multimedia systems
Joint link prediction and attribute inference using a social-attribute network,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899766960&doi=10.1145%2f2594455&partnerID=40&md5=ce507b4572402169ace6cd783293e386,"The effects of social influence and homophily suggest that both network structure and node-attribute information should inform the tasks of link prediction and node-attribute inference. Recently, Yin et al. [2010a, 2010b] proposed an attribute-augmented social network model, which we call Social-Attribute Network (SAN), to integrate network structure and node attributes to perform both link prediction and attribute inference. They focused on generalizing the random walk with a restart algorithm to the SAN framework and showed improved performance. In this article, we extend the SAN framework with several leading supervised and unsupervised link-prediction algorithms and demonstrate performance improvement for each algorithm on both link prediction and attribute inference. Moreover, we make the novel observation that attribute inference can help inform link prediction, that is, link-prediction accuracy is further improved by first inferring missing attributes. We comprehensively evaluate these algorithms and compare them with other existing algorithms using a novel, large-scale Google+ dataset, which we make publicly available (http://www.cs.berkeley.edu/~stevgong/gplus.html). © 2014 ACM.",Attribute inference; Heterogeneous network; Link prediction; Social-attribute network,Algorithms; Data processing; Forecasting; Heterogeneous networks; Attribute inference; Google; Joint links; Link prediction; Network structures; Node attribute; Random Walk; Social influence; Inference engines
AutoLCA: A framework for sustainable redesign and assessment of products,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899768352&doi=10.1145%2f2505270&partnerID=40&md5=a658de1814a8e564d71780341bb4063f,"With increasing public consciousness regarding sustainability, companies are ever more eager to introduce eco-friendly products and services. Assessing environmental footprints and designing sustainable products are challenging tasks since they require analysis of each component of a product through their life cycle. To achieve sustainable design of products, companies need to evaluate the environmental impact of their system, identify the major contributors to the footprint, and select the design alternative with the lowest environmental footprint. In this article, we formulate sustainable design as a series of clustering and classification problems, and propose a framework called AutoLCA that simplifies the effort of estimating the environmental footprint of a product bill of materials by more than an order of magnitude over current methods, which are mostly labor intensive. We apply AutoLCA to real data from a large computer manufacturer. We conduct a case study on bill of materials of four different products, perform a ""hotspot"" assessment analysis to identify major contributors to carbon footprint, and determine design alternatives that can reduce the carbon footprint from 1% to 36%. © 2014 ACM.",Disparate clustering; Life cycle analysis; Sustainable redesign,Carbon footprint; Environmental impact; Industry; Life cycle; Product design; Design alternatives; Disparate clustering; Eco-friendly products; Environmental footprints; Life cycle analysis; Sustainable design; Sustainable products; Sustainable redesign; Sustainable development
Description-driven community detection,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899750161&doi=10.1145%2f2517088&partnerID=40&md5=7bf2c4c71d82b215b0ff62f9a82cdfc1,"Traditional approaches to community detection, as studied by physicists, sociologists, and more recently computer scientists, aim at simply partitioning the social network graph. However, with the advent of online social networking sites, richer data has become available: beyond the link information, each user in the network is annotated with additional information, for example, demographics, shopping behavior, or interests. In this context, it is therefore important to develop mining methods which can take advantage of all available information. In the case of community detection, this means finding good communities (a set of nodes cohesive in the social graph) which are associated with good descriptions in terms of user information (node attributes). Having good descriptions associated to our models make them understandable by domain experts and thus more useful in real-world applications. Another requirement dictated by real-world applications, is to develop methods that can use, when available, any domain-specific background knowledge. In the case of community detection the background knowledge could be a vague description of the communities sought in a specific application, or some prototypical nodes (e.g., good customers in the past), that represent what the analyst is looking for (a community of similar users). Towards this goal, in this article, we define and study the problem of finding a diverse set of cohesive communities with concise descriptions. We propose an effective algorithm that alternates between two phases: a hill-climbing phase producing (possibly overlapping) communities, and a description induction phase which uses techniques from supervised pattern set mining. Our framework has the nice feature of being able to build well-described cohesive communities starting from any given description or seed set of nodes, which makes it very flexible and easily applicable in real-world applications. Our experimental evaluation confirms that the proposed method discovers cohesive communities with concise descriptions in realistic and large online social networks such as DELICIOUS, FLICKR, and LASTFM. © 2014 ACM.",Behavioral and demographic information; Community detection; Description; Domain knowledge; Social networks; Social patterns,Mining; Population statistics; Social networking (online); Community detection; Demographic information; Description; Domain knowledge; Social patterns; Population dynamics
Introduction to the special issue on linking social granularity and functions,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899760646&doi=10.1145%2f2594452&partnerID=40&md5=fe764501a028f62d34f5ff2e2df9f328,[No abstract available],,
Multi-label classification based on multi-objective optimization,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899736696&doi=10.1145%2f2505272&partnerID=40&md5=53c13d870c808eefa658c97cc05d2a7d,"Multi-label classification refers to the task of predicting potentially multiple labels for a given instance. Conventional multi-label classification approaches focus on single objective setting, where the learning algorithm optimizes over a single performance criterion (e.g., Ranking Loss) or a heuristic function. The basic assumption is that the optimization over one single objective can improve the overall performance of multilabel classification and meet the requirements of various applications. However, in many real applications, an optimal multi-label classifier may need to consider the trade-offs among multiple inconsistent objectives, such as minimizing Hamming Loss while maximizing Micro F1. In this article, we study the problem of multi-objective multi-label classification and propose a novel solution (calledMOML) to optimize overmultiple objectives simultaneously. Note that optimization objectives may be inconsistent, even conflicting, thus one cannot identify a single solution that is optimal on all objectives. Our MOML algorithm finds a set of nondominated solutions which are optimal according to different trade-offs among multiple objectives. So users can flexibly construct various predictive models from the solution set, which provides more meaningful classification results in different application scenarios. Empirical studies on real-world tasks demonstrate that the MOML can effectively boost the overall performance of multi-label classification by optimizing over multiple objectives simultaneously. © 2014 ACM.",Classification; Classifier design and evaluation; Model selection; Multi-label classification; Multi-objective optimization; Pattern analysis,Classification (of information); Commerce; Heuristic algorithms; Application scenario; Classification results; Classifier design and evaluation; Model Selection; Multi-label classifications; Nondominated solutions; Pattern analysis; Performance criterion; Multiobjective optimization
CIM: Community-based influence maximization in social networks,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899737385&doi=10.1145%2f2532549&partnerID=40&md5=9e1ce745e334c46922b7c6e52feec9da,"Given a social graph, the problem of influence maximization is to determine a set of nodes that maximizes the spread of influences. While some recent research has studied the problem of influence maximization, these works are generally too time consuming for practical use in a large-scale social network. In this article, we develop a new framework, community-based influence maximization (CIM), to tackle the influence maximization problem with an emphasis on the time efficiency issue. Our proposed framework, CIM, comprises three phases: (i) community detection, (ii) candidate generation, and (iii) seed selection. Specifically, phase (i) discovers the community structure of the network; phase (ii) uses the information of communities to narrow down the possible seed candidates; and phase (iii) finalizes the seed nodes from the candidate set. By exploiting the properties of the community structures, we are able to avoid overlapped information and thus efficiently select the number of seeds to maximize information spreads. The experimental results on both synthetic and real datasets show that the proposed CIM algorithm significantly outperforms the state-of-The-art algorithms in terms of efficiency and scalability, with almost no compromise of effectiveness. © 2014 ACM.",Community detection; Diffusion models; Influence maximization; Social network analysis,Algorithms; Population dynamics; Candidate generation; Community detection; Community structures; Diffusion model; Influence maximizations; Recent researches; State-of-the-art algorithms; Time efficiencies; Social networking (online)
Mining check-in history for personalized location naming,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899713461&doi=10.1145%2f2490890&partnerID=40&md5=ec92605942b8c44293d8d12c55ba3380,"Many innovative location-based services have been established to offer users greater convenience in their everyday lives. These services usually cannot map user's physical locations into semantic names automatically. The semantic names of locations provide important context for mobile recommendations and advertisements. In this article, we proposed a novel location naming approach which can automatically provide semantic names for users given their locations and time. In particular, when a user opens a GPS device and submits a query with her physical location and time, she will be returned the most appropriate semantic name. In our approach, we drew an analogy between location naming and local search, and designed a local search framework to propose a spatiotemporal and user preference (STUP) model for location naming. STUP combined three components, user preference (UP), spatial preference (SP), and temporal preference (TP), by leveraging learning-to-rank techniques. We evaluated STUP on 466,190 check-ins of 5,805 users from Shanghai and 135,052 check-ins of 1,361 users from Beijing. The results showed that SP was most effective among three components and that UP can provide personalized semantic names, and thus it was a necessity for location naming. Although TP was not as discriminative as the others, it can still be beneficial when integrated with SP and UP. Finally, according to the experimental results, STUP outperformed the proposed baselines and returned accurate semantic names for 23.6% and 26.6% of the testing queries from Beijing and Shanghai, respectively. © 2014 ACM.",Location naming; Location-based services; Location-based social network,Location based services; Check-in; Learning to rank; Local search; Location-based social networks; Mobile recommendations; Physical locations; Three component; Semantics
Conveying semantics through visual metaphor,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899714809&doi=10.1145%2f2589483&partnerID=40&md5=ea2fe2fb545f2b8ab6a3604b4d2cffeb,"In the field of visual art, metaphor is a way to communicate meaning to the viewer. We present a computational system for communicating visual metaphor that can identify adjectives for describing an image based on a low-level visual feature representation of the image. We show that the system can use this visuallinguistic association to render source images that convey the meaning of adjectives in a way consistent with human understanding. Our conclusions are based on a detailed analysis of how the system's artifacts cluster, how these clusters correspond to the semantic relationships of adjectives as documented in WordNet, and how these clusters correspond to human opinion. © 2014 ACM.",Clustering; Evolutionary art; Neural networks; Visual metaphor,Neural networks; Clustering; Computational system; Evolutionary arts; Human understanding; Semantic relationships; Source images; Visual feature; Visual metaphor; Semantics
Exploiting user preference for online learning in web content optimization systems,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899749479&doi=10.1145%2f2493259&partnerID=40&md5=19c97005120260e6ed95712fd944c2cb,"Web portal services have become an important medium to deliver digital content (e.g. news, advertisements, etc.) to Web users in a timely fashion. To attract more users to various content modules on the Web portal, it is necessary to design a recommender system that can effectively achieve Web portal content optimization by automatically estimating content item attractiveness and relevance to user interests. The state-of-The-art online learningmethodology adapts dedicated pointwise models to independently estimate the attractiveness score for each candidate content item. Although such pointwise models can be easily adapted for online recommendation, there still remain a few critical problems. First, this pointwise methodology fails to use invaluable user preferences between content items.Moreover, the performance of pointwise models decreases drastically when facing the problem of sparse learning samples. To address these problems, we propose exploring a new dynamic pairwise learning methodology for Web portal content optimization in which we exploit dynamic user preferences extracted based on users' actions on portal services to compute the attractiveness scores of content items. In this article, we introduce two specific pairwise learning algorithms, a straightforward graph-based algorithm and a formalized Bayesian modeling one. Experiments on largescale data from a commercialWeb portal demonstrate the significant improvement of pairwisemethodologies over the baseline pointwise models. Further analysis illustrates that our new pairwise learning approaches can benefit personalized recommendationmore than pointwise models, since the data sparsity is more critical for personalized content optimization. © 2014 ACM.",Bayesianmodel; Content optimization; Pairwise learning; User preference,E-learning; Portals; Bayesianmodel; Critical problems; Graph-based algorithms; Optimization system; Pairwise learning; Personalized content; User preference; Web portal services; Optimization
Structure and overlaps of ground-truth communities in networks,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899760660&doi=10.1145%2f2594454&partnerID=40&md5=ea3816a8247af0f021e861723fb52593,"One of the main organizing principles in real-world networks is that of network communities, where sets of nodes organize into densely linked clusters. Even though detection of such communities is of great interest, understanding the structure communities in large networks remains relatively limited. In particular, due to the unavailability of labeled ground-truth data, it was traditionally very hard to develop accurate models of network community structure. Here we use six large social, collaboration, and information networks where nodes explicitly state their ground-truth community memberships. For example, nodes in social networks join into explicitly defined interest based groups, and we use such groups as explicitly labeled ground-truth communities. We use such ground-truth communities to study their structural signatures by analyzing how ground-truth communities emerge in networks and how they overlap. We observe some surprising phenomena. First, ground-truth communities contain high-degree hub nodes that reside in community overlaps and link to most of the members of the community. Second, the overlaps of communities are more densely connected than the non-overlapping parts of communities. We show that this in contrast to the conventional wisdom that community overlaps are more sparsely connected than the non-overlapping parts themselves. We then show that many existing models of network communities do not capture dense community overlaps. This in turn means that most present models and community detection methods confuse overlaps as separate communities. In contrast, we present the community-affiliation graph model (AGM), a conceptual model of network community structure.We demonstrate that AGMreliably captures the overall structure of networks as well as the overlapping and hierarchical nature of network communities. © 2014 ACM.",Affiliation networks; Network communities; Social networks,Information services; Community detection; Conceptual model; Information networks; Large networks; Network communities; Network community structures; Real-world networks; Structural signatures; Social networking (online)
A framework for effectively choosing between alternative candidate partners,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899767213&doi=10.1145%2f2589482&partnerID=40&md5=715253c15207204b906592d95454e771,"Many multi-agent settings require that agents identify appropriate partners or teammates with whom to work on tasks. When selecting potential partners, agents may benefit from obtaining information about the alternatives, for instance, through gossip (i.e., by consulting others) or reputation systems. When information is uncertain and associated with cost, deciding on the amount of information needed is a hard optimization problem. This article defines a statistical model, the Information-Acquisition Source Utility model (IASU), by which agents, operating in an uncertain world, can determine (1) which information sources they should request for information, and (2) the amount of information to collect about potential partners from each source. To maximize the expected gain from the choice, IASU computes the utility of choosing a partner by estimating the benefit of additional information. The article presents empirical studies through a simulation domain as well as a real-world domain of restaurants. We compare the IASU model to other relevant models and show that the use of the IASU model significantly increases agents' overall utility. © 2014 ACM.",AI technologies; Decision theory; Multi-agent systems,Decision theory; AI Technologies; Amount of information; Information sources; Multi-agent setting; Optimization problems; Reputation systems; Request for informations; Statistical modeling; Multi agent systems
Learning probabilistic hierarchical task networks as probabilistic context-free grammars to capture user preferences,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899719995&doi=10.1145%2f2589481&partnerID=40&md5=a5b2adcde79150d43fb0032a9689309b,"We introduce an algorithm to automatically learn probabilistic hierarchical task networks (pHTNs) that capture a user's preferences on plans by observing only the user's behavior. HTNs are a common choice of representation for a variety of purposes in planning, including work on learning in planning. Our contributions are twofold. First, in contrast with prior work, which employs HTNs to represent domain physics or search control knowledge, we use HTNs to model user preferences. Second, while most prior work on HTN learning requires additional information (e.g., annotated traces or tasks) to assist the learning process, our system only takes plan traces as input. Initially, we will assume that users carry out preferred plans more frequently, and thus the observed distribution of plans is an accurate representation of user preference. We then generalize to the situation where feasibility constraints frequently prevent the execution of preferred plans. Taking the prevalent perspective of viewing HTNs as grammars over primitive actions, we adapt an expectation-maximization (EM) technique from the discipline of probabilistic grammar induction to acquire probabilistic context-free grammars (pCFG) that capture the distribution on plans. To account for the difference between the distributions of possible and preferred plans, we subsequently modify this core EM technique by rescaling its input. We empirically demonstrate that the proposed approaches are able to learn HTNs representing user preferences better than the inside-outside algorithm. Furthermore, when feasibility constraints are obfuscated, the algorithm with rescaled input performs better than the algorithm with the original input. © 2014 ACM.",AI Technology; Hierarchical task networks; Learning user preferences; Planning,Behavioral research; Planning; AI Technologies; Expectation Maximization; Hierarchical task networks; Learning user preferences; Primitive actions; Probabilistic context free grammars; Probabilistic grammars; User's preferences; Algorithms
Infer user interests via link structure regularization,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899769379&doi=10.1145%2f2499380&partnerID=40&md5=77e0972a025a99170f509a3878b41ebe,"Learning user interests from online social networks helps to better understand user behaviors and provides useful guidance to design user-centric applications. Apart from analyzing users' online content, it is also important to consider users' social connections in the social Web. Graph regularization methods have been widely used in various text mining tasks, which can leverage the graph structure information extracted from data. Previously, graph regularization methods operate under the cluster assumption that nearby nodes are more similar and nodes on the same structure (typically referred to as a cluster or a manifold) are likely to be similar. We argue that learning user interests from complex, sparse, and dynamic social networks should be based on the link structure assumption under which node similarities are evaluated based on the local link structures instead of explicit links between two nodes. We propose a regularization framework based on the relation bipartite graph, which can be constructed from any type of relations. Using Twitter as our case study, we evaluate our proposed framework from social networks built from retweet relations. Both quantitative and qualitative experiments show that our proposed method outperforms a few competitive baselines in learning user interests over a set of predefined topics. It also gives superior results compared to the baselines on retweet prediction and topical authority identification. © 2014 ACM.",Graph regularization; Link structure; User interests,Behavioral research; Data mining; Social networking (online); Dynamic social networks; Graph regularization; Link structure; On-line social networks; Qualitative experiments; Regularization framework; Regularization methods; User interests; Graph theory
Detecting social media hidden communities using dynamic stochastic blockmodel with temporal dirichlet process,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899732724&doi=10.1145%2f2517085&partnerID=40&md5=862570318ac7917a347e109204e587b8,"Detecting evolving hidden communities within dynamic social networks has attracted significant attention recently due to its broad applications in e-commerce, online social media, security intelligence, public health, and other areas. Many community network detection techniques employ a two-stage approach to identify and detect evolutionary relationships between communities of two adjacent time epochs. These techniques often identify communities with high temporal variation, since the two-stage approach detects communities of each epoch independently without considering the continuity of communities across two time epochs. Other techniques require identification of a predefined number of hidden communities which is not realistic in many applications. To overcome these limitations, we propose the Dynamic Stochastic Blockmodel with Temporal Dirichlet Process, which enables the detection of hidden communities and tracks their evolution simultaneously from a network stream. The number of hidden communities is automatically determined by a temporal Dirichlet process without human intervention.We tested our proposed technique on three different testbeds with results identifying a high performance level when compared to the baseline algorithm. © 2014 ACM.",Dynamic Community Detection; Stochastic Blockmodel; Temporal Dirichlet Process,Social networking (online); Community networks; Dynamic communities; Dynamic social networks; Evolutionary relationships; Online social medias; Stochastic block models; Temporal dirichlet process; Two-stage approaches; Stochastic systems
Cluster-based collaborative filtering for sign prediction in social networks with positive and negative links,2014,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899710039&doi=10.1145%2f2501977&partnerID=40&md5=b30c176245a371281ce9a8b4f5e161b8,"Social network analysis and mining get ever-increasingly important in recent years, which is mainly due to the availability of large datasets and advances in computing systems. A class of social networks is those with positive and negative links. In such networks, a positive link indicates friendship (or trust), whereas links with a negative sign correspond to enmity (or distrust). Predicting the sign of the links in these networks is an important issue and hasmany applications, such as friendship recommendation and identifyingmalicious nodes in the network. In this manuscript, we proposed a new method for sign prediction in networks with positive and negative links. Our algorithm is based first on clustering the network into a number of clusters and then applying a collaborative filtering algorithm. The clusters are such that the number of intra-cluster negative links and inter-cluster positive links are minimal, that is, the clusters are socially balanced as much as possible (a signed graph is socially balanced if it can be divided into clusters with all positive links inside the clusters and all negative links between them). We then used similarity between the clusters (based on the links between them) in a collaborative filtering algorithm. Our experiments on a number of real datasets showed that the proposedmethod outperformed previousmethods, including those based on social balance and status theories and one based on a machine learning framework (logistic regression in this work). © 2014 ACM.",Cluster identification; Collaborative filtering; Signed networks; Social balance theory; Social networks; Social status theory,Collaborative filtering; Forecasting; Signal filtering and prediction; Collaborative filtering algorithms; Learning frameworks; Logistic regressions; Number of clusters; Signed networks; Social balances; Social network analysis and minings; Social status; Social networking (online)
Introduction to special section on intelligent mobile knowledge discovery and management systems,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891765469&doi=10.1145%2f2542182.2542183&partnerID=40&md5=a13db62c025b233469f78fa7fc5196e9,[No abstract available],,
Personalized emerging topic detection based on a term aging model,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891781315&doi=10.1145%2f2542182.2542189&partnerID=40&md5=6fc8264410082a100a18c51d191fe620,"Twitter is a popular microblogging service that acts as a ground-level information news flashes portal where people with different background, age, and social condition provide information about what is happening in front of their eyes. This characteristic makes Twitter probably the fastest information service in the world. In this article, we recognize this role of Twitter and propose a novel, user-aware topic detection technique that permits to retrieve, in real time, the most emerging topics of discussion expressed by the community within the interests of specific users. First, we analyze the topology of Twitter looking at how the information spreads over the network, taking into account the authority/influence of each active user. Then, we make use of a novel term aging model to compute the burstiness of each term, and provide a graph-based method to retrieve the minimal set of terms that can represent the corresponding topic. Finally, since any user can have topic preferences inferable from the shared content, we leverage such knowledge to highlight the most emerging topics within her foci of interest. As evaluation we then provide several experiments together with a user study proving the validity and reliability of the proposed approach. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Aging theory; Social network analysis; Topic detection and tracking; Trends; Twitter,Information services; Knowledge management; Aging theories; Emerging topic detections; Graph-based methods; Micro-blogging services; Social conditions; Topic detection and tracking; Trends; Twitter; Social networking (online)
Personalized tag recommendation based on generalized rules,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891796924&doi=10.1145%2f2542182.2542194&partnerID=40&md5=025e2fac90afec3b5eceb77d009787e2,"Tag recommendation is focused on recommending useful tags to a user who is annotating a Web resource. A relevant research issue is the recommendation of additional tags to partially annotated resources, whichmay be based on either personalized or collective knowledge. However, since the annotation process is usually not driven by any controlled vocabulary, the collections of user-specific and collective annotations are often very sparse. Indeed, the discovery of the most significant associations among tags becomes a challenging task. This article presents a novel personalized tag recommendation system that discovers and exploits generalized association rules, that is, tag correlations holding at different abstraction levels, to identify additional pertinent tags to suggest. The use of generalized rules relevantly improves the effectiveness of traditional rule-based systems in coping with sparse tag collections, because: (i) correlations hidden at the level of individual tags may be anyhow figured out at higher abstraction levels and (ii) low-level tag associations discovered from collective data may be exploited to specialize high-level associations discovered in the userspecific context. The effectiveness of the proposed system has been validated against other personalized approaches on real-life and benchmark collections retrieved from the popular photo-sharing system Flickr. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Flickr; Generalized association rule mining; Tag recommendation,Abstraction level; Flickr; Generalized association rule mining; Generalized association rules; Research issues; Rule-based system; Tag associations; Tag recommendations; Abstracting
Community detection and visualization in social networks: Integrating structural and semantic information,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891751331&doi=10.1145%2f2542182.2542193&partnerID=40&md5=4df22b2124498dc3d0332821a94ae82c,"Due to the explosion of social networking and the information sharing among their users, the interest in analyzing social networks has increased over the recent years. Two general interests in this kind of studies are community detection and visualization. In the first case, most of the classic algorithms for community detection use only the structural information to identify groups, that is, how clusters are formed according to the topology of the relationships. However, these methods do not take into account any semantic information which could guide the clustering process, and which may add elements to conduct further analyses. In the second case most of the layout algorithms for clustered graphs have been designed to differentiate the groups within the graph, but they are not designed to analyze the interactions between such groups. Identifying these interactions gives an insight into the way different communities exchange messages or information, and allows the social network researcher to identify key actors, roles, and paths from one community to another. This article presents a novel model to use, in a conjoint way, the semantic information from the social network and its structural information to, first, find structurally and semantically related groups of nodes, and second, a layout algorithm for clustered graphs which divides the nodes into two types, one for nodes with edges connecting other communities and another with nodes connecting nodes only within their own community. With this division the visualization tool focuses on the connections between groups facilitating deep studies of augmented social networks. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Clustered graphs layout; Community detection; Graph clustering; Knowledge extraction; Social network analysis; Social networks; Unsupervised learning,Clustering algorithms; Population dynamics; Semantics; Unsupervised learning; Visualization; Clustered graph; Community detection; Graph clustering; Information sharing; Knowledge extraction; Semantic information; Structural information; Visualization tools; Social networking (online)
Active learning strategies for rating elicitation in collaborative filtering: A system-wide perspective,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891751057&doi=10.1145%2f2542182.2542195&partnerID=40&md5=ec30d83471a1ab134ab7ce0f04082fed,"The accuracy of collaborative-filtering recommender systems largely depends on three factors: the quality of the rating prediction algorithm, and the quantity and quality of available ratings. While research in the field of recommender systems often concentrates on improving prediction algorithms, even the best algorithms will fail if they are fed poor-quality data during training, that is, garbage in, garbage out. Active learning aims to remedy this problem by focusing on obtaining better-quality data that more aptly reflects a user's preferences. However, traditional evaluation of active learning strategies has two major flaws, which have significant negative ramifications on accurately evaluating the system's performance (prediction error, precision, and quantity of elicited ratings). (1) Performance has been evaluated for each user independently (ignoring system-wide improvements). (2) Active learning strategies have been evaluated in isolation from unsolicited user ratings (natural acquisition). In this article we show that an elicited rating has effects across the system, so a typical user-centric evaluation which ignores any changes of rating prediction of other users also ignores these cumulative effects, which may be more influential on the performance of the system as a whole (system centric). We propose a new evaluationmethodology and use it to evaluate some novel and state-of-the-art rating elicitation strategies. We found that the system-wide effectiveness of a rating elicitation strategy depends on the stage of the rating elicitation process, and on the evaluation measures (MAE, NDCG, and Precision). In particular, we show that using some common user-centric strategies may actually degrade the overall performance of a system. Finally, we show that the performance of many common active learning strategies changes significantly when evaluated concurrently with the natural acquisition of ratings in recommender systems. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Active learning; Cold start; Rating elicitation; Recommender systems,Algorithms; Collaborative filtering; Learning systems; Recommender systems; Active Learning; Active learning strategies; Cold start; Cumulative effects; Evaluation measures; Prediction algorithms; System's performance; User-centric evaluations; Rating
Forecasting with twitter data,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891765968&doi=10.1145%2f2542182.2542190&partnerID=40&md5=c429e09cf44a3c6cc7c8bc9c911023ef,"The dramatic rise in the use of social network platforms such as Facebook or Twitter has resulted in the availability of vast and growing user-contributed repositories of data. Exploiting this data by extracting useful information from it has become a great challenge in data mining and knowledge discovery. A recently popular way of extracting useful information from social network platforms is to build indicators, often in the form of a time series, of general public mood by means of sentiment analysis. Such indicators have been shown to correlate with a diverse variety of phenomena. In this article we follow this line of work and set out to assess, in a rigorous manner, whether a public sentiment indicator extracted from daily Twitter messages can indeed improve the forecasting of social, economic, or commercial indicators. To this end we have collected and processed a large amount of Twitter posts from March 2011 to the present date for two very different domains: stock market and movie box office revenue. For each of these domains, we build and evaluate forecasting models for several target time series both using and ignoring the Twitter-related data. If Twitter does help, then this should be reflected in the fact that the predictions of models that use Twitter-related data are better than the models that do not use this data. By systematically varying the models that we use and their parameters, together with other tuning factors such as lag or the way in which we build our Twitter sentiment index, we obtain a large dataset that allows us to test our hypothesis under different experimental conditions. Using a novel decision-tree-based technique that we call summary tree we are able to mine this large dataset and obtain automatically those configurations that lead to an improvement in the prediction power of our forecasting models. As a general result, we have seen that nonlinear models do take advantage of Twitter data when forecasting trends in volatility indices, while linear ones fail systematically when forecasting any kind of financial time series. In the case of predicting box office revenue trend, it is support vector machines that make best use of Twitter data. In addition, we conduct statistical tests to determine the relation between our Twitter time series and the different target time series. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Box office; Forecasting; Sentiment index; Stock market; Twitter,Commerce; Data mining; Economics; Finance; Financial data processing; Forecasting; Information use; Motion pictures; Statistical tests; Time series; Box office; Data mining and knowledge discovery; Experimental conditions; Financial time series; Sentiment analysis; Sentiment index; Stock market; Twitter; Social networking (online)
A SAT-based approach to cost-sensitive temporally expressive planning,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891749370&doi=10.1145%2f2542182.2542200&partnerID=40&md5=df21451f01c20a97b0b61723c908330f,"Complex features, such as temporal dependencies and numerical cost constraints, are hallmarks of realworld planning problems. In this article, we consider the challenging problem of cost-sensitive temporally expressive (CSTE) planning, which requires concurrency of durative actions and optimization of action costs. We first propose a scheme to translate a CSTE planning problem to a minimum cost (MinCost) satisfiability (SAT) problem and to integrate with a relaxed parallel planning semantics for handling true temporal expressiveness. Our scheme finds solution plans that optimize temporal makespan, and also minimize total action costs at the optimal makespan. We propose two approaches for solving MinCost SAT. The first is based on a transformation of a MinCost SAT problem to a weighted partial Max-SAT (WPMax-SAT), and the second, called BB-CDCL, is an integration of the branch-and-bound technique and the conflict driven clause learning (CDCL) method. We also develop a CSTE customized variable branching scheme for BB-CDCL which can significantly improve the search efficiency. Our experiments on the existing CSTE benchmark domains show that our planner compares favorably to the state-of-the-art temporally expressive planners in both efficiency and quality. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Numerical cost constraint; Planning; Satisfiability; Temporal expressiveness,Benchmarking; Formal logic; Optimization; Planning; Scheduling algorithms; Semantics; Benchmark domains; Branch and bounds; Numerical costs; Real-world planning problem; Satisfiability; Satisfiability problems; Search efficiency; Temporal expressiveness; Costs
Analyzing user behavior across social sharing environments,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891753470&doi=10.1145%2f2535526&partnerID=40&md5=b3f6e3a9b7740e58d758d86d5b93aad3,"In this work we present an in-depth analysis of the user behaviors on different Social Sharing systems. We consider three popular platforms, Flickr, Delicious and StumbleUpon, and, by combining techniques from social network analysis with techniques from semantic analysis, we characterize the tagging behavior as well as the tendency to create friendship relationships of the users of these platforms. The aim of our investigation is to see if (and how) the features and goals of a given Social Sharing system reflect on the behavior of its users and, moreover, if there exists a correlation between the social and tagging behavior of the users. We report our findings in terms of the characteristics of user profiles according to three different dimensions: (i) intensity of user activities, (ii) tag-based characteristics of user profiles, and (iii) semantic characteristics of user profiles. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Folksonomies; Semantic analysis; Social networks; Social systems; User modeling,Semantics; Social networking (online); User interfaces; Combining techniques; Folksonomies; In-depth analysis; Popular platform; Semantic analysis; Social systems; User behaviors; User Modeling; Behavioral research
Introduction to the special issue on social web mining,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891792082&doi=10.1145%2f2542182.2542187&partnerID=40&md5=20b1e622a75943b404f0eb11669180c7,[No abstract available],,
Computationally efficient link prediction in a variety of social networks,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891823014&doi=10.1145%2f2542182.2542192&partnerID=40&md5=d49bad1baed126eed781f20727d56852,"Online social networking sites have become increasingly popular over the last few years. As a result, new interdisciplinary research directions have emerged in which social network analysis methods are applied to networks containing hundreds of millions of users. Unfortunately, links between individuals may be missing either due to an imperfect acquirement process or because they are not yet reflected in the online network (i.e., friends in the real world did not form a virtual connection). The primary bottleneck in link prediction techniques is extracting the structural features required for classifying links. In this article, we propose a set of simple, easy-to-compute structural features that can be analyzed to identifymissing links.We show that by using simple structural features, a machine learning classifier can successfully identify missing links, even when applied to a predicament of classifying links between individuals with at least one common friend. We also present a method for calculating the amount of data needed in order to build more accurate classifiers. The new Friends measure and Same community features we developed are shown to be good predictors for missing links. An evaluation experiment was performed on ten large social networks datasets: Academia.edu, DBLP, Facebook, Flickr, Flixster, Google+, Gowalla, TheMarker, Twitter, and YouTube. Our methods can provide social network site operators with the capability of helping users to find known, offline contacts and to discover new friends online. They may also be used for exposing hidden links in online social networks. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Academia.edu; DBLP; Facebook; Flickr; Flixster; Google; Hidden links; Imbalanced dataset; Link prediction; Social networks; Supervised learning; The Marker Cafe; Training set size; Twitter; YouTube,Forecasting; Learning systems; Online systems; Supervised learning; Academia.edu; DBLP; Facebook; Flickr; Flixster; Google; Hidden links; Imbalanced dataset; Link prediction; The Marker Cafe; Training sets; Twitter; YouTube; Social networking (online)
Mining geographic-temporal-semantic patterns in trajectories for location prediction,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891810623&doi=10.1145%2f2542182.2542184&partnerID=40&md5=3402f0bd4eddd8d76dfeb6774c7635a1,"In recent years, research on location predictions by mining trajectories of users has attracted a lot of attention. Existing studies on this topic mostly treat such predictions as just a type of location recommendation, that is, they predict the next location of a user using location recommenders. However, an user usually visits somewhere for reasons other than interestingness. In this article, we propose a novel mining-based location prediction approach called Geographic-Temporal-Semantic-based Location Prediction (GTS-LP), which takes into account a user's geographic-triggered intentions, temporal-triggered intentions, and semantic-triggered intentions, to estimate the probability of the user in visiting a location. The core idea underlying our proposal is the discovery of trajectory patterns of users, namely GTS patterns, to capture frequent movements triggered by the three kinds of intentions. To achieve this goal, we define a new trajectory pattern to capture the key properties of the behaviors that are motivated by the three kinds of intentions from trajectories of users. In our GTS-LP approach, we propose a series of novel matching strategies to calculate the similarity between the current movement of a user and discovered GTS patterns based on various moving intentions. On the basis of similitude, we make an online prediction as to the location the user intends to visit. To the best of our knowledge, this is the first work on location prediction based on trajectory pattern mining that explores the geographic, temporal, and semantic properties simultaneously. By means of a comprehensive evaluation using various real trajectory datasets, we show that our proposed GTS-LP approach delivers excellent performance and significantly outperforms existing state-of-the-art location prediction methods. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Frequent movement patterns; Location prediction; Semantic trajectory; Trajectory mining,Semantics; Trajectories; Comprehensive evaluation; Location prediction; Movement pattern; Online prediction; Semantic properties; Semantic trajectories; Trajectory minings; Trajectory pattern; Forecasting
Effective and efficient microprocessor design space exploration using unlabeled design configurations,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891783012&doi=10.1145%2f2542182.2542202&partnerID=40&md5=6b47a58ea0b74f79100ad13531cf9cd6,"Ever-increasing design complexity and advances of technology impose great challenges on the design of modern microprocessors. One such challenge is to determine promising microprocessor configurations to meet specific design constraints, which is called Design Space Exploration (DSE). In the computer architecture community, supervised learning techniques have been applied to DSE to build regression models for predicting the qualities of design configurations. For supervised learning, however, considerable simulation costs are required for attaining the labeled design configurations. Given limited resources, it is difficult to achieve high accuracy. In this article, inspired by recent advances in semisupervised learning and active learning, we propose the COAL approach which can exploit unlabeled design configurations to significantly improve the models. Empirical study demonstrates that COAL significantly outperforms a state-of-the-art DSE technique by reducing mean squared error by 35% to 95%, and thus, promising architectures can be attained more efficiently. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Active learning; Design space exploration; Machine learning; Microprocessor design; Semisupervised learning; Simulation,Computer architecture; Design; Learning systems; Microprocessor chips; Regression analysis; Supervised learning; Active Learning; Design space exploration; Microprocessor designs; Semi- supervised learning; Simulation; Learning algorithms
Trust and matching algorithms for selecting suitable agents,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880531389&doi=10.1145%2f2542182.2542198&partnerID=40&md5=f842208e680a45ba6238a8098ef5008e,"This article addresses the problem of finding suitable agents to collaborate with for a given interaction in distributed open systems, such as multiagent and P2P systems. The agent in question is given the chance to describe its confidence in its own capabilities. However, since agents may be malicious, misinformed, suffer from miscommunication, and so on, one also needs to calculate how much trusted is that agent. This article proposes a novel trust model that calculates the expectation about an agent's future performance in a given context by assessing both the agent's willingness and capability through the semantic comparison of the current context in question with the agent's performance in past similar experiences. The proposed mechanism for assessing trust may be applied to any real world application where past commitments are recorded and observations are made that assess these commitments, and the model can then calculate one's trust in another with respect to a future commitment by assessing the other's past performance. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Semantic matching; Trust and reputation,Multi agent systems; Distributed open systems; Future performance; Matching algorithm; P2P system; Semantic matching; Trust and reputation; Trust modeling; Semantics
Relational term-suggestion graphs incorporating multipartite concept and expertise networks,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891817906&doi=10.1145%2f2542182.2542201&partnerID=40&md5=b484401240a489cc55e3ed6b1ec99ac3,"Term suggestions recommend query terms to a user based on his initial query. Suggesting adequate terms is a challenging issue. Most existing commercial search engines suggest search terms based on the frequency of prior used terms that match the leading alphabets the user types. In this article, we present a novel mechanism to construct semantic term-relation graphs to suggest relevant search terms in the semantic level. We built term-relation graphs based on multipartite networks of existing social media, especially from Wikipedia. The multipartite linkage networks of contributor-term, term-category, and term-term are extracted from Wikipedia to eventually form term relation graphs. For fusing these multipartite linkage networks, we propose to incorporate the contributor-category networks to model the expertise of the contributors. Based on our experiments, this step has demonstrated clear enhancement on the accuracy of the inferred relatedness of the term-semantic graphs. Experiments on keyword-expanded search based on 200 TREC-5 ad-hoc topics showed obvious advantage of our algorithms over existing approaches. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Keyword expansion reranking; Social network,Experiments; Search engines; Semantics; Social networking (online); Expertise networks; Multipartite networks; Re-ranking; Search terms; Search-based; Semantic levels; Suggest searches; Term suggestion; Graphic methods
Mondrian tree: A fast index for spatial alarm processing,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891800493&doi=10.1145%2f2542182.2542186&partnerID=40&md5=21122f39fd978eb026e520e0d29b3bf1,"With ubiquitous wireless connectivity and technological advances in mobile devices, we witness the growing demands and increasing market shares of mobile intelligent systems and technologies for real-time decision making and location-based knowledge discovery. Spatial alarms are considered as one of the fundamental capabilities for intelligent mobile location-based systems. Like time-based alarms that remind us the arrival of a future time point, spatial alarms remind us the arrival of a future spatial point. Existing approaches for scaling spatial alarm processing are focused on computing Alarm-Free Regions (AFR) and Alarm-Free Period (AFP) such that mobile objects traveling within an AFR can safely hibernate the alarm evaluation process for the computed AFP, to save battery power, until approaching the nearest alarm of interest. A key technical challenge in scaling spatial alarm processing is to efficiently compute AFR and AFP such that mobile objects traveling within an AFR can safely hibernate the alarm evaluation process during the computed AFP, while maintaining high accuracy. In this article we argue that on-demand computation of AFR is expensive and may not scale well for dense populations of mobile objects. Instead, we propose to maintain an index for both spatial alarms and empty regions (AFR) such that for a given mobile user's location, we can find relevant spatial alarms and whether it is in an alarm-free region more efficiently. We also show that conventional spatial indexing methods, such as R-tree family, k-d tree, Quadtree, and Grid, are by design not well suited to index empty regions. We present Mondrian Tree - a region partitioning tree for indexing both spatial alarms and alarm-free regions. We first introduce the Mondrian Tree indexing algorithms, including index construction, search, and maintenance. Then we describe a suite of Mondrian Tree optimizations to further enhance the performance of spatial alarm processing. Our experimental evaluation shows that the Mondrian Tree index, as an intelligent technology for mobile systems, outperforms traditional index methods, such as R-tree, Quadtree, and k-d tree, for spatial alarm processing. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Location-based systems; Spatial index; Spatial query processing,Alarms; Coefficients; Competition; Data Processing; Decision Making; Forestry; Competition; Decision trees; Forestry; Indexing (of information); Intelligent systems; Mobile devices; Experimental evaluation; Intelligent technology; Location-based systems; On-demand computations; Real-time decision making; Spatial indexes; Spatial query processing; Wireless connectivities; Alarm systems
Audio classification with low-rank matrix representation features,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891776309&doi=10.1145%2f2542182.2542197&partnerID=40&md5=6d7dbba4261bd1d335ed746591dafc93,"In this article, a novel framework based on trace norm minimization for audio classification is proposed. In this framework, both the feature extraction and classification are obtained by solving corresponding convex optimization problem with trace norm regularization. For feature extraction, robust principle component analysis (robust PCA) via minimization a combination of the nuclear norm and the 1-norm is used to extract low-rank matrix features which are robust to white noise and gross corruption for audio signal. These low-rank matrix features are fed to a linear classifier where the weight and bias are learned by solving similar trace norm constrained problems. For this linear classifier, most methods find the parameters, that is the weight matrix and bias in batch-mode, which makes it inefficient for large scale problems. In this article, we propose a parallel online framework using accelerated proximal gradient method. This framework has advantages in processing speed andmemory cost. In addition, as a result of the regularization formulation of matrix classification, the Lipschitz constant was given explicitly, and hence the step size estimation of the general proximal gradient method was omitted, and this part of computing burden is saved in our approach. Extensive experiments on real data sets for laugh/non-laugh and applause/non-applause classification indicate that this novel framework is effective and noise robust. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Audio classification; Low-rank matrix; Matrix classification; Online learning; Proximal gradient; Robust principle component analysis; Trace norm minimization,Audio acoustics; Classification (of information); Convex optimization; Feature extraction; Gradient methods; Principal component analysis; White noise; Audio classification; Low-rank matrices; Online learning; Principle component analysis; Trace-norms; Matrix algebra
Campaign extraction from social media,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891821713&doi=10.1145%2f2542182.2542191&partnerID=40&md5=39f73b05d37fc1ea7401e54f28930d2f,"In this manuscript, we study the problem of detecting coordinated free text campaigns in large-scale social media. These campaigns-ranging from coordinated spam messages to promotional and advertising campaigns to political astro-turfing-are growing in significance and reach with the commensurate rise in massive-scale social systems. Specifically, we propose and evaluate a content-driven framework for effectively linking free text posts with common ''talking points'' and extracting campaigns from large-scale social media. Three of the salient features of the campaign extraction framework are: (i) first, we investigate graph mining techniques for isolating coherent campaigns from large message-based graphs; (ii) second, we conduct a comprehensive comparative study of text-based message correlation in message and user levels; and (iii) finally, we analyze temporal behaviors of various campaign types. Through an experimental study over millions of Twitter messages we identify five major types of campaigns-namely Spam, Promotion, Template, News, and Celebrity campaigns-and we show how these campaigns may be extracted with high precision and recall. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Campaign detection; Social media,Advertising campaign; Comparative studies; Precision and recall; Salient features; Social media; Social systems; Temporal behavior; Text-based messages; Extraction
Monitoring business constraints with the event calculus,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891781134&doi=10.1145%2f2542182.2542199&partnerID=40&md5=80e67004f3023c2a5f782617e6c3ef70,"Today, large business processes are composed of smaller, autonomous, interconnected subsystems, achieving modularity and robustness. Quite often, these large processes comprise software components as well as human actors, they face highly dynamic environments and their subsystems are updated and evolve independently of each other. Due to their dynamic nature and complexity, it might be difficult, if not impossible, to ensure at design-time that such systems will always exhibit the desired/expected behaviors. This, in turn, triggers the need for runtime verification and monitoring facilities. These are needed to check whether the actual behavior complies with expected business constraints, internal/external regulations and desired best practices. In this work, we present Mobucon EC, a novel monitoring framework that tracks streams of events and continuously determines the state of business constraints. In Mobucon EC, business constraints are defined using the declarative language Declare. For the purpose of this work, Declare has been suitably extended to support quantitative time constraints and non-atomic, durative activities. The logic-based language Event Calculus (EC) has been adopted to provide a formal specification and semantics to Declare constraints, while a light-weight, logic programming-based EC tool supports dynamically reasoning about partial, evolving execution traces. To demonstrate the applicability of our approach, we describe a case study about maritime safety and security and provide a synthetic benchmark to evaluate its scalability. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Business constraints; Declarative processmodels; Event calculus; Monitoring; Operational decision support; Process mining; Runtime verification,Decision support systems; Monitoring; Semantics; Business constraints; Event calculus; Operational decision support; Process mining; Process-models; Run-time verification; Logic programming
A framework of traveling companion discovery on trajectory data streams,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891749445&doi=10.1145%2f2542182.2542185&partnerID=40&md5=fc5b5e0ba3a3b7f0ba4c5203ecea0841,"The advance of mobile technologies leads to huge volumes of spatio-temporal data collected in the form of trajectory data streams. In this study, we investigate the problem of discovering object groups that travel together (i.e., traveling companions) from trajectory data streams. Such technique has broad applications in the areas of scientific study, transportation management, and military surveillance. To discover traveling companions, the monitoring system should cluster the objects of each snapshot and intersect the clustering results to retrieve moving-together objects. Since both clustering and intersection steps involve high computational overhead, the key issue of companion discovery is to improve the efficiency of algorithms. We propose the models of closed companion candidates and smart intersection to accelerate data processing. A data structure termed traveling buddy is designed to facilitate scalable and flexible companion discovery from trajectory streams. The traveling buddies are microgroups of objects that are tightly bound together. By only storing the object relationships rather than their spatial coordinates, the buddies can be dynamically maintained along the trajectory stream with low cost. Based on traveling buddies, the system can discover companions without accessing the object details. In addition, we extend the proposed framework to discover companions on more complicated scenarios with spatial and temporal constraints, such as on the road network and battlefield. The proposed methods are evaluated with extensive experiments on both real and synthetic datasets. Experimental results show that our proposed buddy-based approach is an order of magnitude faster than the baselines and achieves higher accuracy in companion discovery. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Clustering; Data stream; Trajectory,Clustering algorithms; Data communication systems; Data processing; Detectors; Clustering; Computational overheads; Data stream; Military surveillance; Spatial coordinates; Spatio-temporal data; Temporal constraints; Transportation management; Trajectories
Norms as a basis for governing sociotechnical systems,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891809035&doi=10.1145%2f2542182.2542203&partnerID=40&md5=2eb7d7c55030fb75094de1873823b783,"We understand a sociotechnical system as a multistakeholder cyber-physical system. We introduce governance as the administration of such a system by the stakeholders themselves. In this regard, governance is a peer-to-peer notion and contrasts with traditional management, which is a top-down hierarchical notion. Traditionally, there is no computational support for governance and it is achieved through out-of-band interactions among system administrators. Not surprisingly, traditional approaches simply do not scale up to large sociotechnical systems. We develop an approach for governance based on a computational representation of norms in organizations. Our approach is motivated by the Ocean Observatory Initiative, a thirty-year 400 million project, which supports a variety of resources dealing with monitoring and studying the world's oceans. These resources include autonomous underwater vehicles, ocean gliders, buoys, and other instrumentation as well as more traditional computational resources. Our approach has the benefit of directly reflecting stakeholder needs and assuring stakeholders of the correctness of the resulting governance decisions while yielding adaptive resource allocation in the face of changes in both stakeholder needs and physical circumstances. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Adaptation; Governance; Sociotechnical systems,Autonomous underwater vehicles; Adaptation; Adaptive resource allocations; Computational resources; Cyber physical systems (CPSs); Governance; Large socio-technical systems; Sociotechnical systems; Traditional approaches; Embedded systems
COM: A method for mining and monitoring human activity patterns in home-based health monitoring systems,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885674772&doi=10.1145%2f2508037.2508045&partnerID=40&md5=3122ee0fe5e328fbfce4244e2e0f3f5f,"The increasing aging population in the coming decades will result in many complications for society and in particular for the healthcare system due to the shortage of healthcare professionals and healthcare facilities. To remedy this problem, researchers have pursued developing remote monitoring systems and assisted living technologies by utilizing recent advances in sensor and networking technology, as well as in the data mining and machine learning fields. In this article, we report on our fully automated approach for discovering and monitoring patterns of daily activities. Discovering and tracking patterns of daily activities can provide unprecedented opportunities for health monitoring and assisted living applications, especially for older adults and individuals with mental disabilities. Previous approaches usually rely on preselected activities or labeled data to track and monitor daily activities. In this article, we present a fully automated approach by discovering natural activity patterns and their variations in real-life data. We will show how our activity discovery component can be integrated with an activity recognition component to track and monitor various daily activity patterns. We also provide an activity visualization component to allow caregivers to visually observe and examine the activity patterns using a user-friendly interface. We validate our algorithms using real-life data obtained from two apartments during a three-month period. © 2013 ACM.",Assisted living technology; Health monitoring; Sequence mining; Smart environments,Health care; Assisted living technologies; Daily activity patterns; Health care professionals; Health monitoring; Health monitoring system; Remote monitoring system; Sequence mining; Smart environment; Monitoring
A machine learning approach to college drinking prediction and risk factor identification,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885659402&doi=10.1145%2f2508037.2508053&partnerID=40&md5=44b96a6627a15c4c38bfaceebcd783dc,"Alcohol misuse is one of the most serious public health problems facing adolescents and young adults in the United States. National statistics shows that nearly 90% of alcohol consumed by youth under 21 years of age involves binge drinking and 44% of college students engage in high-risk drinking activities. Conventional alcohol intervention programs, which aim at installing either an alcohol reduction norm or prohibition against underage drinking, have yielded little progress in controlling college binge drinking over the years. Existing alcohol studies are deductive where data are collected to investigate a psychological/behavioral hypothesis, and statistical analysis is applied to the data to confirm the hypothesis. Due to this confirmatory manner of analysis, the resulting statistical models are cohort-specific and typically fail to replicate on a different sample. This article presents two machine learning approaches for a secondary analysis of longitudinal data collected in college alcohol studies sponsored by the National Institute on Alcohol Abuse and Alcoholism. Our approach aims to discover knowledge, from multiwave cohort-sequential daily data, which may or may not align with the original hypothesis but quantifies predictive models with higher likelihood to generalize to new samples. We first propose a so-called temporally-correlated support vector machine to construct a classifier as a function of daily moods, stress, and drinking expectancies to distinguish days with nighttime binge drinking from days without for individual students. We then propose a combination of cluster analysis and feature selection, where cluster analysis is used to identify drinking patterns based on averaged daily drinking behavior and feature selection is used to identify risk factors associated with each pattern. We evaluate our methods on two cohorts of 530 total college students recruited during the Spring and Fall semesters, respectively. Cross validation on these two cohorts and further on 100 random partitions of the total students demonstrate that our methods improve the model generalizability in comparison with traditional multilevel logistic regression. The discovered risk factors and the interaction of these factors delineated in our models can set a potential basis and offer insights to a new design of more effective college alcohol interventions. © 2013 ACM.",Clustering; College student alcohol consumption; Feature selection; Longitudinal data analysis; Machine learning,Cluster analysis; Feature extraction; Learning systems; Alcohol consumption; Alcohol reduction; Clustering; Intervention programs; Longitudinal data; Machine learning approaches; Multi-level logistics; National statistics; Students
Early prediction of the highest workload in incremental cardiopulmonary tests,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885674757&doi=10.1145%2f2508037.2508051&partnerID=40&md5=79dfc83b5f2b8f388922889c7e700681,"Incremental tests are widely used in cardiopulmonary exercise testing, both in the clinical domain and in sport sciences. The highest workload (denotedWpeak) reached in the test is key information for assessing the individual body response to the test and for analyzing possible cardiac failures and planning rehabilitation, and training sessions. Being physically very demanding, incremental tests can significantly increase the body stress on monitored individuals and may cause cardiopulmonary overload. This article presents a new approach to cardiopulmonary testing that addresses these drawbacks. During the test, our approach analyzes the individual body response to the exercise and predicts the Wpeak value that will be reached in the test and an evaluation of its accuracy. When the accuracy of the prediction becomes satisfactory, the test can be prematurely stopped, thus avoiding its entire execution. To predict Wpeak, we introduce a new index, the CardioPulmonary Efficiency Index (CPE), summarizing the cardiopulmonary response of the individual to the test. Our approach analyzes the CPE trend during the test, together with the characteristics of the individual, and predicts W peak. A K-nearest-neighbor-based classifier and an ANN-based classier are exploited for the prediction. The experimental evaluation showed that theWpeak value can be predicted with a limited error from the first steps of the test. © 2013 ACM.",Classification techniques; Highest workload prediction; Incremental test; Multivariate data; Physiological signals analysis,Forecasting; Classification technique; Early prediction; Efficiency index; Experimental evaluation; Multivariate data; Physiological signals; Training sessions; Workload predictions; Testing
Introduction to the special section on intelligent systems for health informatics,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885656757&doi=10.1145%2f2508037.2508043&partnerID=40&md5=772e8cfbd5c9c4105b4b503dd3875dda,[No abstract available],,
Dealing with uncertainty: Robust workflow navigation in the healthcare domain,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885645610&doi=10.1145%2f2508037.2508046&partnerID=40&md5=70d6fcea6c17de693fa1ead6bd20c763,"Processes in the healthcare domain are characterized by coarsely predefined recurring procedures that are flexibly adapted by the personnel to suite-specific situations. In this setting, a workflow management system that gives guidance and documents the personnel's actions can lead to a higher quality of care, fewer mistakes, and higher efficiency. However, most existing workflow management systems enforce rigid inflexible workflows and rely on direct manual input. Both are inadequate for healthcare processes. In particular, direct manual input is not possible in most cases since (1) it would distract the personnel even in critical situations and (2) it would violate fundamental hygiene principles by requiring disinfected doctors and nurses to touch input devices. The solution could be activity recognition systems that use sensor data (e.g., audio and acceleration data) to infer the current activities by the personnel and provide input to a workflow (e.g., informing it that a certain activity is finished now). However, state-of-the-art activity recognition technologies have difficulties in providing reliable information. We describe a comprehensive framework tailored for flexible human-centric healthcare processes that improves the reliability of activity recognition data. We present a set of mechanisms that exploit the application knowledge encoded in workflows in order to reduce the uncertainty of this data, thus enabling unobtrusive robust healthcare workflows. We evaluate our work based on a real-world case study and show that the robustness of unobtrusive healthcare workflows can be increased to an absolute value of up to 91% (compared to only 12% with a classical workflow system). This is a major breakthrough that paves the way towards future IT-enabled healthcare systems. © 2013 ACM.",Activity recognition; Bayesian networks; Business process management; Particle filters; Subjective logic; Uncertain real-world context; Workflow mining,Bayesian networks; Distributed computer systems; Human resource management; Pattern recognition; Work simplification; Activity recognition; Business process management; Particle filter; Real-world; Subjective Logic; Workflow mining; Health care
Web media semantic concept retrieval via tag removal and model fusion,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885653186&doi=10.1145%2f2508037.2508042&partnerID=40&md5=db4d8e3a739956649e6281522d075e89,"Multimedia data on social websites contain rich semantics and are often accompanied with user-defined tags. To enhance Web media semantic concept retrieval, the fusion of tag-based and content-based models can be used, though it is very challenging. In this article, a novel semantic concept retrieval framework that incorporates tag removal and model fusion is proposed to tackle such a challenge. Tags with useful information can facilitate media search, but they are often imprecise, which makes it important to apply noisy tag removal (by deleting uncorrelated tags) to improve the performance of semantic concept retrieval. Therefore, a multiple correspondence analysis (MCA)-based tag removal algorithm is proposed, which utilizes MCA's ability to capture the relationships among nominal features and identify representative and discriminative tags holding strong correlations with the target semantic concepts. To further improve the retrieval performance, a novel model fusion method is also proposed to combine ranking scores from both tag-based and content-based models, where the adjustment of ranking scores, the reliability of models, and the correlations between the intervals divided on the ranking scores and the semantic concepts are all considered. Comparative results with extensive experiments on the NUS-WIDE-LITE as well as the NUS-WIDE-270K benchmark datasets with 81 semantic concepts show that the proposed framework outperforms baseline results and the other comparison methods with each component being evaluated separately. © 2013 ACM.",Model fusion; Multimedia semantic concept retrieval; Multiple correspondence analysis (MCA); Noisy tag removal; Social tags,Algorithms; Information retrieval; Semantics; Model fusion; Multimedia semantics; Multiple correspondence analysis; Social Tags; Tag removal; Semantic Web
Customized prediction of respiratory motion with clustering from multiple patient interaction,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885668554&doi=10.1145%2f2508037.2508050&partnerID=40&md5=7b4217c9fe20eb62af48308b5c3bf255,"Information processing of radiotherapy systems has become an important research area for sophisticated radiation treatment methodology. Geometrically precise delivery of radiotherapy in the thorax and upper abdomen is compromised by respiratory motion during treatment. Accurate prediction of the respiratory motion would be beneficial for improving tumor targeting. However, a wide variety of breathing patterns can make it difficult to predict the breathing motion with explicit models. We proposed a respiratory motion predictor, that is, customized prediction with multiple patient interactions using neural network (CNN). For the preprocedure of prediction for individual patient, we construct the clustering based on breathing patterns of multiple patients using the feature selection metrics that are composed of a variety of breathing features. In the intraprocedure, the proposed CNN used neural networks (NN) for a part of the prediction and the extended Kalman filter (EKF) for a part of the correction. The prediction accuracy of the proposed method was investigated with a variety of prediction time horizons using normalized root mean squared error (NRMSE) values in comparison with the alternate recurrent neural network (RNN). We have also evaluated the prediction accuracy using the marginal value that can be used as the reference value to judge how many signals lie outside the confidence level. The experimental results showed that the proposed CNN can outperform RNN with respect to the prediction accuracy with an improvement of 50%. © 2013 ACM.",Breathing prediction; Intelligent systems; Medical signal analysis; Multilayer perceptron; Recurrent neural networks; Respiratory motion,Data processing; Intelligent systems; Radiotherapy; Recurrent neural networks; Respiratory mechanics; Feature selection metrics; Multi layer perceptron; Patient interaction; Prediction time horizon; Radiation treatments; Recurrent neural network (RNN); Respiratory motions; Root mean squared errors; Forecasting
Mining search and browse logs for web search: A survey,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885653457&doi=10.1145%2f2508037.2508038&partnerID=40&md5=c8db717402795f74e40342e06c6246be,"Huge amounts of search log data have been accumulated at Web search engines. Currently, a popular Web search engine may receive billions of queries and collect terabytes of records about user search behavior daily. Beside search log data, huge amounts of browse log data have also been collected through client-side browser plugins. Suchmassive amounts of search and browse log data provide great opportunities formining the wisdom of crowds and improvingWeb search. At the same time, designing effective and efficient methods to clean, process, and model log data also presents great challenges. In this survey,we focus on mining search and browse log data forWeb search.We start with an introduction to search and browse log data and an overview of frequently-used data summarizations in log mining. We then elaborate how log mining applications enhance the five major components of a search engine, namely, query understanding, document understanding, document ranking, user understanding, and monitoring and feedback. For each aspect, we survey the major tasks, fundamental principles, and state-of-the-art methods. © 2013 ACM.",Browse log; Document ranking; Document understanding; Feedbacks; Log mining; Monitoring; Query understanding; Search logs; Survey; User understanding; Web search,Behavioral research; Feedback; Information retrieval; Monitoring; Surveying; Surveys; Websites; Browse log; Document ranking; Document understanding; Log mining; Query understanding; Search logs; User understanding; Web searches; Search engines
LocateMe: Magnetic-fields-based indoor localization using smartphones,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885644913&doi=10.1145%2f2508037.2508054&partnerID=40&md5=dacc8e855993eb332b3d6eac1fcbe38a,"Fine-grained localization is extremely important to accurately locate a user indoors. Although innovative solutions have already been proposed, there is no solution that is universally accepted, easily implemented, user centric, and, most importantly, works in the absence of GSM coverage or WiFi availability. The advent of sensor rich smartphones has paved a way to develop a solution that can cater to these requirements. By employing a smartphone's built-in magnetic field sensor, magnetic signatures were collected inside buildings. These signatures displayed a uniqueness in their patterns due to the presence of different kinds of pillars, doors, elevators, etc., that consist of ferromagnetic materials like steel or iron. We theoretically analyze the cause of this uniqueness and then present an indoor localization solution by classifying signatures based on their patterns. However, to account for user walking speed variations so as to provide an application usable to a variety of users, we follow a dynamic time-warping-based approach that is known to work on similar signals irrespective of their variations in the time axis. Our approach resulted in localization distances of approximately 2m-6m with accuracies between 80- 100% implying that it is sufficient to walk short distances across hallways to be located by the smartphone. The implementation of the application on different smartphones yielded response times of less than five secs, thereby validating the feasibility of our approach and making it a viable solution. © 2013 ACM.",Indoor localization; Magnetic fields; Smartphones; Ubiquitous,Magnetic fields; Signal encoding; Indoor localization; Innovative solutions; Magnetic field sensors; Magnetic signatures; Ubiquitous; User-centric; Viable solutions; Walking speed; Smartphones
A survey of appearance models in visual object tracking,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885606175&doi=10.1145%2f2508037.2508039&partnerID=40&md5=0cf2d638a9cb0c8da918164ffd5d839f,"Visual object tracking is a significant computer vision task which can be applied to many domains, such as visual surveillance, human computer interaction, and video compression. Despite extensive research on this topic, it still suffers from difficulties in handling complex object appearance changes caused by factors such as illumination variation, partial occlusion, shape deformation, and camera motion. Therefore, effective modeling of the 2D appearance of tracked objects is a key issue for the success of a visual tracker. In the literature, researchers have proposed a variety of 2D appearance models. To help readers swiftly learn the recent advances in 2D appearance models for visual object tracking, we contribute this survey, which provides a detailed review of the existing 2D appearance models. In particular, this survey takes a module-based architecture that enables readers to easily grasp the key points of visual object tracking. In this survey, we first decompose the problem of appearance modeling into two different processing stages: visual representation and statistical modeling. Then, different 2D appearance models are categorized and discussed with respect to their composition modules. Finally, we address several issues of interest as well as the remaining challenges for future research on this topic. The contributions of this survey are fourfold. First, we review the literature of visual representations according to their feature-construction mechanisms (i.e., local and global). Second, the existing statistical modeling schemes for tracking-by-detection are reviewed according to their model-constructionmechanisms: generative, discriminative, and hybrid generative-discriminative. Third, each type of visual representations or statisticalmodeling techniques is analyzed and discussed from a theoretical or practical viewpoint. Fourth, the existing benchmark resources (e.g., source codes and video datasets) are examined in this survey. © 2013 ACM.",Appearance model; Features; Statistical modeling; Visual object tracking,Research; Statistical methods; Appearance modeling; Features; Illumination variation; Partial occlusions; Statistical modeling; Visual object tracking; Visual representations; Visual surveillance; Surveys
A semantic framework for intelligent matchmaking for clinical trial eligibility criteria,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885601282&doi=10.1145%2f2508037.2508052&partnerID=40&md5=cc309b9490c6560b1383a1f8ac2c447b,"An integral step in the discovery of new treatments for medical conditions is the matching of potential subjects with appropriate clinical trials. Eligibility criteria for clinical trials are typically specified as inclusion and exclusion criteria for each study in freetext form. While this is sufficient for a human to guide a recruitment interview, it cannot be reliably and computationally construed to identify potential subjects. Standardization of the representation of eligibility criteria can enhance the efficiency and accuracy of this process. This article presents a semantic framework that facilitates intelligent matchmaking by identifying a minimal set of eligibility criteria with maximal coverage of clinical trials. In contrast to existing top-down manual standardization efforts, a bottom-up data driven approach is presented to find a canonical nonredundant representation of an arbitrary collection of clinical trial criteria. The methodology has been validated with a corpus of 709 clinical trials related to Generalized Anxiety Disorder containing 2,760 inclusion and 4,871 exclusion eligibility criteria. This corpus is well represented by a relatively small number of 126 inclusion clusters and 175 exclusion clusters, each of which corresponds to a semantically distinct criterion. Internal and external validation measures provide an objective evaluation of the method. An eligibility criteria ontology has been constructed based on the clustering. The resulting model has been incorporated into the development of the MindTrial clinical trial recruiting system. The prototype for clinical trial recruitment illustrates the effectiveness of the methodology in characterizing clinical trials and subjects and accurate matching between them. © 2013 ACM.",Clinical trials; Clustering; Eligibility criteria; Intelligent matchmaking; Ontology; Ontology to DB mapping; Search engine,Experiments; Ontology; Search engines; Semantics; Standardization; Clinical trial; Clustering; Data-driven approach; Eligibility criterion; Inclusion and exclusions; Intelligent matchmaking; Objective evaluation; Semantic framework; Medical applications
Social semantic query expansion,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885602404&doi=10.1145%2f2508037.2508041&partnerID=40&md5=d27618c174a728e0d44fbda0740ea4eb,"Weak semantic techniques rely on the integration of Semantic Web techniques with social annotations and aim to embrace the strengths of both. In this article, we propose a novel weak semantic technique for query expansion. Traditional query expansion techniques are based on the computation of two-dimensional co-occurrence matrices. Our approach proposes the use of three-dimensional matrices, where the added dimension is represented by semantic classes (i.e., categories comprising all the terms that share a semantic property) related to the folksonomy extracted from social bookmarking services, such as delicious and StumbleUpon. The results of an indepth experimental evaluation performed on both artificial datasets and real users show that our approach outperforms traditional techniques, such as relevance feedback and personalized PageRank, so confirming the validity and usefulness of the categorization of the user needs and preferences in semantic classes. We also present the results of a questionnaire aimed to know the users opinion regarding the system. As one drawback of several query expansion techniques is their high computational costs, we also provide a complexity analysis of our system, in order to show its capability of operating in real time. © 2013 ACM.",Information retrieval; Query expansion; Social semantic web,Information retrieval; Co-occurrence-matrix; Experimental evaluation; Personalized PageRank; Query expansion; Query expansion techniques; Semantic-Web techniques; Social semantic webs; Traditional techniques; Expansion
CUDIA: Probabilistic cross-level imputation using individual auxiliary information,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885634519&doi=10.1145%2f2508037.2508047&partnerID=40&md5=7ae977d507ae915fe337e28658f422e7,"In healthcare-related studies, individual patient or hospital data are not often publicly available due to privacy restrictions, legal issues, or reporting norms. However, such measures may be provided at a higher or more aggregated level, such as state-level, county-level summaries or averages over health zones, such as hospital referral regions (HRR) or hospital service areas (HSA). Such levels constitute partitions over the underlying individual level data, which may not match the groupings that would have been obtained if one clustered the data based on individual-level attributes. Moreover, treating aggregated values as representatives for the individuals can result in the ecological fallacy. How can one run data mining procedures on such data where different variables are available at different levels of aggregation or granularity? In this article, we seek a better utilization of variably aggregated datasets, which are possibly assembled from different sources. We propose a novel cross-level imputation technique that models the generative process of such datasets using a Bayesian directed graphical model. The imputation is based on the underlying data distribution and is shown to be unbiased. This imputation can be further utilized in a subsequent predictive modeling, yielding improved accuracies. The experimental results using a simulated dataset and the Behavioral Risk Factor Surveillance System (BRFSS) dataset are provided to illustrate the generality and capabilities of the proposed framework. © 2013 ACM.",BRFSS; Clustering; Privacy preserving data mining,Hospitals; Auxiliary information; BRFSS; Clustering; Imputation techniques; Predictive modeling; Privacy preserving data mining; Privacy restrictions; Surveillance systems; Aggregates
Random walks down the mention graphs for event coreference resolution,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885599757&doi=10.1145%2f2508037.2508055&partnerID=40&md5=054c67a27ddac902ebff247c52cd93f5,"Event coreference is an important task in event extraction and other natural language processing tasks. Despite its importance, it was merely discussed in previous studies. In this article, we present a global coreference resolution system dedicated to various sophisticated event coreference phenomena. First, seven resolvers are utilized to resolve different event and object coreferencemention pairs with a new instance selection strategy and new linguistic features. Second, a global solution-a modified random walk partitioning-is employed for the chain formation. Being the first attempt to apply the random walk model for coreference resolution, the revised model utilizes a sampling method, termination criterion, and stopping probability to greatly improve the effectiveness of random walk model for event coreference resolution. Last but not least, the new model facilitates a convenient way to incorporate sophisticated linguistic constraints and preferences, the related object mention graph, as well as pronoun coreference information not used in previous studies for effective chain formation. In total, these techniques impose more than 20% F-score improvement over the baseline system. © 2013 ACM.",Anaphora resolution; Competing classifiers; Coreference resolution; Event coreference; Instance selection; Random walks partitioning; Self-interacting walks,Chains; Linguistics; Random processes; Anaphora resolution; Co-reference resolutions; Coreference; Instance selection; Random Walk; Self-interacting walks; Natural language processing systems
Reliable medical recommendation systems with patient privacy,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885617136&doi=10.1145%2f2508037.2508048&partnerID=40&md5=5a61e7156a861b010c0d0d8c26559374,"One of the concerns patients have when confronted with a medical condition is which physician to trust. Any recommendation system that seeks to answer this question must ensure that any sensitive medical information collected by the system is properly secured. In this article, we codify these privacy concerns in a privacy-friendly framework and present two architectures that realize it: the Secure Processing Architecture (SPA) and the Anonymous Contributions Architecture (ACA). In SPA, patients submit their ratings in a protected form without revealing any information about their data and the computation of recommendations proceeds over the protected data using secure multiparty computation techniques. In ACA, patients submit their ratings in the clear, but no link between a submission and patient data can be made.We discuss various aspects of both architectures, including techniques for ensuring reliability of computed recommendations and system performance, and provide their comparison. © 2013 ACM.",Framework; Privacy; Recommendation systems,Data privacy; Hospital data processing; Recommender systems; Framework; Medical conditions; Medical information; Patient data; Patient privacies; Privacy concerns; Processing architectures; Secure multi-party computation; Architecture
Perspectives in semantic adaptive social web,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885598244&doi=10.1145%2f2501603&partnerID=40&md5=632d358a72e6b2ee061666316beeac04,"The Social Web is now a successful reality with its quickly growing number of users and applications. Also the Semantic Web, which started with the objective of describing Web resources in a machine-processable way, is now outgrowing the research labs and is being massively exploited in many websites, incorporating high-quality user-generated content and semantic annotations. The primary goal of this special section is to showcase some recent research at the intersection of the Social Web and the Semantic Web that explores the benefits that adaptation and personalization have to offer in the Web of the future, the so-called Social Adaptive Semantic Web. We have selected two articles out of fourteen submissions based on the quality of the articles and we present the main lessons learned from the overall analysis of these submissions. © 2013 ACM.",Adaptation; Semantic web; Social web,Adaptation; Personalizations; Recent researches; Semantic annotations; Social webs; Special sections; User-generated content; Web resources; Semantic Web
Validation of an ontological medical decision support system for patient treatment using a repository of patient data: Insights into the value of machine learning,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885623290&doi=10.1145%2f2508037.2508049&partnerID=40&md5=1b9cf208dde32df37fe8622e66f32359,"In this article, we begin by presenting OMeD, a medical decision support system, and argue for its value over purely probabilistic approaches that reason about patients for time-critical decision scenarios. We then progress to present Holmes, a Hybrid Ontological and Learning MEdical System which supports decision making about patient treatment. This system is introduced in order to cope with the case of missing data. We demonstrate its effectiveness by operating on an extensive set of real-world patient health data from the CDC, applied to the decision-making scenario of administering sleeping pills. In particular, we clarify how the combination of semantic, ontological representations, and probabilistic reasoning together enable the proposal of effective patient treatments. Our focus is thus on presenting an approach for interpreting medical data in the context of real-time decision making. This constitutes a comprehensive framework for the design of medical recommendation systems for potential use by medical professionals and patients both, with the end result being personalized patient treatment. We conclude with a discussion of the value of our particular approach for such diverse considerations as coping with misinformation provided by patients, performing effectively in time-critical environments where real-time decisions are necessary, and potential applications facilitating patient information gathering. © 2013 ACM.",Automated knowledge inference; Machine learning; Medical decision support system; Ontology-based knowledge representation,Artificial intelligence; Decision making; Decision support systems; Hospital data processing; Knowledge representation; Learning systems; Ontology; Pelletizing; Semantics; Automated knowledge inference; Medical decision support system; Medical professionals; Ontological representation; Ontology-based; Probabilistic approaches; Probabilistic reasoning; Real-time decision making; Patient treatment
A temporal pattern mining approach for classifying electronic health record data,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885587994&doi=10.1145%2f2508037.2508044&partnerID=40&md5=cf3fd2d433ac3953f12d67ba4ee3f426,"We study the problem of learning classificationmodels from complex multivariate temporal data encountered in electronic health record systems. The challenge is to define a good set of features that are able to represent well the temporal aspect of the data. Our method relies on temporal abstractions and temporal pattern mining to extract the classification features. Temporal pattern mining usually returns a large number of temporal patterns, most of which may be irrelevant to the classification task. To address this problem, we present the Minimal Predictive Temporal Patterns framework to generate a small set of predictive and nonspurious patterns.We apply our approach to the real-world clinical task of predicting patients who are at risk of developing heparin-induced thrombocytopenia. The results demonstrate the benefit of our approach in efficiently learning accurate classifiers, which is a key step for developing intelligent clinical monitoring systems. © 2013 ACM.",Classification; Multivariate time series; Temporal abstractions; Temporal pattern mining; Time-interval patterns,Abstracting; Classification features; Electronic health record; Electronic health record systems; Minimal predictive temporal patterns; Multivariate time series; Temporal abstraction; Temporal pattern minings; Time-interval patterns; Classification (of information)
Using qualitative reasoning for social simulation of crowds,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880213970&doi=10.1145%2f2483669.2483687&partnerID=40&md5=7554ba8cb83243aff4357f8717f5e8b9,"The ability to model and reason about the potential violence level of a demonstration is important to the police decision making process. Unfortunately, existing knowledge regarding demonstrations is composed of partial qualitative descriptions without complete and precise numerical information. In this article we describe a first attempt to use qualitative reasoning techniques to model demonstrations. To our knowledge, such techniques have never been applied to modeling and reasoning regarding crowd behaviors, nor in particular demonstrations. We develop qualitative models consistent with the partial, qualitative social science literature, allowing us to model the interactions between different factors that influence violence in demonstrations. We then utilize qualitative simulation to predict the potential eruption of violence, at various levels, based on a description of the demographics, environmental settings, and police responses. We incrementally present and compare three such qualitative models. The results show that while two of these models fail to predict the outcomes of real-world events reported and analyzed in the literature, one model provides good results.We also examine whether a popular machine learning algorithm (decision tree learning) can be used. While the results show that the decision trees provide improved predictions, we show that the QR models can be more sensitive to changes, and can account for what-if scenarios, in contrast to decision trees. Moreover, we introduce a novel analysis algorithm that analyzes the QR simulations, to automatically determine the factors that are most important in influencing the outcome in specific real-world demonstrations. We show that the algorithm identifies factors that correspond to experts' analysis of these events. ©2013 ACM.",Demonstrations; Qualitative reasoning; Social simulation,Artificial intelligence; Behavioral research; Decision making; Decision trees; Factor analysis; Law enforcement; Learning algorithms; Analysis algorithms; Decision making process; Decision tree learning; Numerical information; Qualitative reasoning; Qualitative simulation; Social simulations; What-if scenarios; Demonstrations
An inference-based model of word meaning in context as a paraphrase distribution,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880231437&doi=10.1145%2f2483669.2483675&partnerID=40&md5=ac71743e8f4054a25522e9164f0b0399,"Graded models of word meaning in context characterize the meaning of individual usages (occurrences) without reference to dictionary senses. We introduce a novel approach that frames the task of computing word meaning in context as a probabilistic inference problem. The model represents the meaning of a word as a probability distribution over potential paraphrases, inferred using an undirected graphical model. Evaluated on paraphrasing tasks, the model achieves state-of-the-art performance. ©2013 ACM.",Lexical semantics; Loopy belief propagation; Paraphrases; Probabilistic graphical models; Probabilistic inference; Semantics,Probability distributions; Speech recognition; Lexical semantics; Loopy belief propagation; Paraphrases; Probabilistic graphical models; Probabilistic inference; Semantics
Analysis of friendship network and its role in explaining obesity,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880202547&doi=10.1145%2f2483669.2483689&partnerID=40&md5=06783e06013315d56f8c4ae760a12fc0,"We employ Add Health data to show that friendship networks, constructed from mutual friendship nominations, are important in building weight perception, setting weight goals, and measuring social marginalization among adolescents and young adults. We study the relationship between individuals' perceived weight status, actual weight status, weight status relative to friends' weight status, and weight goals. This analysis helps us understand how individual weight perceptions might be formed, what these perceptions do to the weight goals, and how friends' relative weight affects weight perception and weight goals. Combining this information with individuals' friendship network helps determine the influence of social relationships on weight-related variables. Multinomial logistic regression results indicate that relative status is indeed a significant predictor of perceived status, and perceived status is a significant predictor of weight goals. We also address the issue of causality between actual weight status and social marginalization (as measured by the number of friends) and show that obesity precedes social marginalization in time rather than the other way around. This lends credence to the hypothesis that obesity leads to social marginalization not vice versa. Attributes of the friendship network can provide new insights into effective interventions for combating obesity since adolescent friendships provide an important social context for weight-related behaviors. ©2013 ACM.",Add Health; Causality; Friendship network; Obesity; Perceived weight; Relative weight,Logistics; Causality; Friendship networks; Obesity; Perceived weight; Relative weights; Nutrition
Paraphrase acquisition via crowdsourcing and machine learning,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880207564&doi=10.1145%2f2483669.2483676&partnerID=40&md5=f44a87e57d7ef3db265fdac4c9134f52,"To paraphrase means to rewrite content while preserving the original meaning. Paraphrasing is important in fields such as text reuse in journalism, anonymizing work, and improving the quality of customer-written reviews. This article contributes to paraphrase acquisition and focuses on two aspects that are not addressed by current research: (1) acquisition via crowdsourcing, and (2) acquisition of passage-level samples. The challenge of the first aspect is automatic quality assurance; without such a means the crowdsourcing paradigm is not effective, and without crowdsourcing the creation of test corpora is unacceptably expensive for realistic order of magnitudes. The second aspect addresses the deficit that most of the previous work in generating and evaluating paraphrases has been conducted using sentence-level paraphrases or shorter; these short-sample analyses are limited in terms of application to plagiarism detection, for example. We present the Webis Crowd Paraphrase Corpus 2011 (Webis-CPC-11), which recently formed part of the PAN 2010 international plagiarism detection competition. This corpus comprises passage-level paraphrases with 4067 positive samples and 3792 negative samples that failed our criteria, using Amazon's Mechanical Turk for crowdsourcing. In this article, we review the lessons learned at PAN 2010, and explain in detail the method used to construct the corpus. The empirical contributions include machine learning experiments to explore if passage-level paraphrases can be identified in a two-class classification problem using paraphrase similarity features, and we find that a k-nearest-neighbor classifier can correctly distinguish between paraphrased and nonparaphrased samples with 0.980 precision at 0.523 recall. This result implies that just under half of our samples must be discarded (remaining 0.477 fraction), but our cost analysis shows that the automation we introduce results in a 18% financial saving and over 100 hours of time returned to the researchers when repeating a similar corpus design. On the other hand, when building an unrelated corpus requiring, say, 25% training data for the automated component, we show that the financial outcome is cost neutral, while still returning over 70 hours of time to the researchers. The work presented here is the first to join the paraphrasing and plagiarism communities. ©2013 ACM.",Corpus; Cost analysis; Mechanical Turk; Paraphrase generation; Plagiarism,Cost accounting; Learning systems; Quality assurance; Research; Corpus; Cost analysis; Mechanical turks; Paraphrase generation; Plagiarism; Intellectual property
Nontrivial landmark recommendation using geotagged photos,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880198679&doi=10.1145%2f2483669.2483680&partnerID=40&md5=9ca8d501610329307b5f32e2b72af060,"Online photo-sharing sites provide a wealth of information about user behavior and their potential is increasing as it becomes ever-more common for images to be associated with location information in the form of geotags. In this article, we propose a novel approach that exploits geotagged images from an online community for the purpose of personalized landmark recommendation. Under our formulation of the task, recommended landmarks should be relevant to user interests and additionally they should constitute nontrivial recommendations. In other words, recommendations of landmarks that are highly popular and frequently visited and can be easily discovered through other information sources such as travel guides should be avoided in favor of recommendations that relate to users' personal interests. We propose a collaborative filtering approach to the personalized landmark recommendation task within a matrix factorization framework. Our approach, WMF-CR, combines weighted matrix factorization and category-based regularization. The integrated weights emphasize the contribution of nontrivial landmarks in order to focus the recommendation model specifically on the generation of nontrivial recommendations. They support the judicious elimination of trivial landmarks from consideration without also discarding information valuable for recommendation. Category-based regularization addresses the sparse data problem, which is arguably even greater in the case of our landmark recommendation task than in other recommendation scenarios due to the limited amount of travel experience recorded in the online image set of any given user. We use category information extracted from Wikipedia in order to provide the system with a method to generalize the semantics of landmarks and allow the model to relate them not only on the basis of identity, but also on the basis of topical commonality. The proposed approach is computational scalable, that is, its complexity is linear with the number of observed preferences in the user-landmark preference matrix and the number of nonzero similarities in the category-based landmark similarity matrix. We evaluate the approach on a large collection of geotagged photos gathered from Flickr. Our experimental results demonstrate that WMF-CR outperforms several state-of-the-art baseline approaches in recommending nontrivial landmarks. Additionally, they demonstrate that the approach is well suited for addressing data sparseness and provides particular performance improvement in the case of users who have limited travel experience, that is, have visited only few cities or few landmarks. ©2013 ACM.",Collaborative filtering; Geotag; Location-based recommendation; Nontrivial recommendation; Social media application,Collaborative filtering; Matrix algebra; Semantics; Websites; Geo-tags; Location based; Location information; Matrix factorizations; Nontrivial recommendation; Performance improvements; Social media; Wealth of information; Behavioral research
Using targeted paraphrasing and monolingual crowdsourcing to improve translation,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880226746&doi=10.1145%2f2483669.2483671&partnerID=40&md5=b85d75961c04e37e076fc0626ba0f6f2,"Targeted paraphrasing is a new approach to the problem of obtaining cost-effective, reasonable quality translation, which makes use of simple and inexpensive human computations by monolingual speakers in combination with machine translation. The key insight behind the process is that it is possible to spot likely translation errors with only monolingual knowledge of the target language, and it is possible to generate alternative ways to say the same thing (i.e., paraphrases) with only monolingual knowledge of the source language. Formal evaluation demonstrates that this approach can yield substantial improvements in translation quality, and the idea has been integrated into a broader framework for monolingual collaborative translation that produces fully accurate, fully fluent translations for a majority of sentences in a real-world translation task, with no involvement of human bilingual speakers. ©2013 ACM.",Crowdsourcing; Human computation; Machine translation; Monolingual; Paraphrase; Translation; Translation interface; Wisdom of crowds,Translation (languages); Crowdsourcing; Human computation; Machine translations; Monolingual; Paraphrase; Wisdom of crowds; Computer aided language translation
Pervasive social context: Taxonomy and survey,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880204731&doi=10.1145%2f2483669.2483679&partnerID=40&md5=c64492bdde49bd259f71f360fae4b626,"As pervasive computing meets social networks, there is a fast growing research field called pervasive social computing. Applications in this area exploit the richness of information arising out of people using sensorequipped pervasive devices in their everyday life combined with intense use of different social networking services.We call this set of information pervasive social context.We provide a taxonomy to classify pervasive social context along the dimensions space, time, people, and information source (STiPI) as well as commenting on the type and reason for creating such context. A survey of recent research shows the applicability and usefulness of the taxonomy in classifying and assessing applications and systems in the area of pervasive social computing. Finally, we present some research challenges in this area and illustrate how they affect the systems being surveyed. ©2013 ACM.",Context awareness; Pervasive computing; Social networks; Survey; Taxonomy,Research; Social networking (online); Social sciences computing; Surveying; Surveys; Ubiquitous computing; Context- awareness; Information sources; Pervasive devices; Pervasive Social Computing; Recent researches; Research challenges; Research fields; Social context; Taxonomies
Generating targeted paraphrases for improved translation,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880234761&doi=10.1145%2f2483669.2483673&partnerID=40&md5=9886bf87554e07ce846eaf9ba9f6e3aa,"Today's Statistical Machine Translation (SMT) systems require high-quality human translations for parameter tuning, in addition to large bitexts for learning the translation units. This parameter tuning usually involves generating translations at different points in the parameter space and obtaining feedback against human-authored reference translations as to how good the translations. This feedback then dictates what point in the parameter space should be explored next. To measure this feedback, it is generally considered wise to have multiple (usually 4) reference translations to avoid unfair penalization of translation hypotheses which could easily happen given the large number of ways in which a sentence can be translated from one language to another. However, this reliance on multiple reference translations creates a problem since they are labor intensive and expensive to obtain. Therefore, most current MT datasets only contain a single reference. This leads to the problem of reference sparsity. In our previously published research, we had proposed the first paraphrase-based solution to this problem and evaluated its effect on Chinese-English translation. In this article, we first present extended results for that solution on additional source languages. More importantly, we present a novel way to generate ""targeted"" paraphrases that yields substantially larger gains (up to 2.7 BLEU points) in translation quality when compared to our previous solution (up to 1.6 BLEU points). In addition, we further validate these improvements by supplementing with human preference judgments obtained via Amazon Mechanical Turk. ©2013 ACM.",Machine translation; Natural language processing; Paraphrasing,Amazon mechanical turks; Machine translations; Multiple references; NAtural language processing; Parameter-tuning; Paraphrasing; Statistical machine translation; Translation quality; Natural language processing systems
Connecting people through physical proximity and physical resources at a conference,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880232901&doi=10.1145%2f2483669.2483683&partnerID=40&md5=4a2628e5c12b77e3be682b355e080940,"This work investigates how to bridge the gap between offline and online behaviors at a conference and how the physical resources in the conference (the physical objects used in the conference for gathering attendees together in engaging an activity such as rooms, sessions, and papers) can be used to help facilitate social networking.We build Find and Connect, a system that integrates offline activities and interactions captured in real time with online connections in a conference environment, to provide a list of potential people one should connect to for forming an ephemeral social network. We investigate how social connections can be established and integrated with physical resources through positioning technology, and the relationship between physical proximity encounters and online social connections. Results from our two datasets of two trials, one at the UIC/ATC 2010 conference and GCJK internal marketing event, show that social connections that are reciprocal in relationship, such as friendship and exchanged contacts, have tighter, denser, and highly clustered networks compared to unidirectional relationships such as follow. We discover that there is a positive relationship between physical proximity encounters and online social connections before the social connection is made for friends, but a negative relationship for after the social connection is made. The first indicates social selection is strong, and the second indicates social influence is weak. Even though our dataset is sparse, nonetheless we believe our work is promising and novel which is worthy of future research. ©2013 ACM.",Ephemeral social network; Mobile social network; Physical proximity; Resource; Social networking,Clustered networks; Ephemeral social networks; Mobile social networks; Physical proximity; Physical resources; Positioning technologies; Resource; Social connection; Social networking (online)
An abstractive approach to sentence compression,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880227825&doi=10.1145%2f2483669.2483674&partnerID=40&md5=cb0645b273c934d554dac6f734b3cbd7,"In this article we generalize the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present an experimental study showing that humans can naturally create abstractive sentences using a variety of rewrite operations, not just deletion. We next create a new corpus that is suited to the abstractive compression task and formulate a discriminative tree-to-tree transduction model that can account for structural and lexical mismatches. The model incorporates a grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression-specific loss functions. ©2013 ACM.",Language generation; Language models; Machine translation; Paraphrases; Sentence compression; Synchronous grammars; Transduction,Forestry; Languages; Models; Bacteriophages; Forestry; Language generation; Language model; Machine translations; Paraphrases; Sentence compression; Synchronous grammars; Transduction; Computational linguistics
Markov models of social dynamics: Theory and applications,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880196157&doi=10.1145%2f2483669.2483686&partnerID=40&md5=f2e0b1c499937a90e874a60418a9962a,[No abstract available],,
Multitechnique paraphrase alignment: A contribution to pinpointing sub-sentential paraphrases,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880191660&doi=10.1145%2f2483669.2483677&partnerID=40&md5=79aa54f34aa623c3aa729be1d995f437,"This work uses parallel monolingual corpora for a detailed study of the task of sub-sentential paraphrase acquisition. We argue that the scarcity of this type of resource is compensated by the fact that it is the most suited type for studies on paraphrasing. We propose a large exploration of this task with experiments on two languages with five different acquisition techniques, selected for their complementarity, their combinations, as well as four monolingual corpus types of varying comparability. We report, under all conditions, a significant improvement over all techniques by validating candidate paraphrases using a maximum entropy classifier. An important result of our study is the identification of difficult-to-acquire paraphrase pairs, which are classified and quantified in a bilingual typology. ©2013 ACM.",Paraphrase acquisition; Paraphrase corpora,Entropy classifiers; Paraphrase acquisition; Paraphrase corpus; Mergers and acquisitions
Optimization-based influencing of village social networks in a counterinsurgency,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880248252&doi=10.1145%2f2483669.2483685&partnerID=40&md5=675caeed442ec43aa73b3f5e287e3d16,"This article considers the nonlethal targeting assignment problem in the counterinsurgency in Afghanistan, the problem of deciding on the people whom U.S. forces should engage through outreach, negotiations, meetings, and other interactions in order to ultimately win the support of the population in their area of operations. We propose two models: (1) the Afghan counterinsurgency (COIN) social influence model, to represent how attitudes of local leaders are affected by repeated interactions with other local leaders, insurgents, and counterinsurgents, and (2) the nonlethal targeting model, a NonLinear Programming (NLP) optimization formulation that identifies a strategy for assigning k U.S. agents to produce the greatest arithmetic mean of the expected long-term attitude of the population. We demonstrate in an experiment the merits of the optimization model in nonlethal targeting, which performs significantly better than both doctrine-based and random methods of assignment in a large network. ©2013 ACM.",Agent modeling; Counterinsurgency; Network optimization; Opinion dynamics; Social network,Optimization; Social networking (online); Agent modeling; Assignment problems; Counterinsurgency; Network optimization; Opinion dynamics; Optimization formulations; Optimization models; Social influence model; Nonlinear programming
Semantic trajectories: Mobility data computation and annotation,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880228087&doi=10.1145%2f2483669.2483682&partnerID=40&md5=b2e392a6dd8f0cc15037cbbb62529bb9,"With the large-scale adoption of GPS equipped mobile sensing devices, positional data generated by moving objects (e.g., vehicles, people, animals) are being easily collected. Such data are typically modeled as streams of spatio-temporal (x,y,t) points, called trajectories. In recent years trajectory management research has progressed significantly towards efficient storage and indexing techniques, as well as suitable knowledge discovery. These works focused on the geometric aspect of the raw mobility data. We are now witnessing a growing demand in several application sectors (e.g., from shipment tracking to geo-social networks) on understanding the semantic behavior of moving objects. Semantic behavior refers to the use of semantic abstractions of the raw mobility data, including not only geometric patterns but also knowledge extracted jointly from the mobility data and the underlying geographic and application domains information. The core contribution of this article lies in a semantic model and a computation and annotation platform for developing a semantic approach that progressively transforms the raw mobility data into semantic trajectories enriched with segmentations and annotations. We also analyze a number of experiments we did with semantic trajectories in different domains. ©2013 ACM.",Hidden Markov model; Map matching; Spatial join; Spatio-temporal/structured/semantic trajectory; Trajectory annotation; Trajectory computing; Trajectory segmentation,Hidden Markov models; Semantics; Sensors; Trajectories; Geo-social networks; Indexing techniques; Management research; Map matching; Semantic trajectories; Spatial join; Spatio-temporal; Trajectory segmentation; Digital storage
Introduction to the special section on intelligent systems for socially aware computing,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880205804&doi=10.1145%2f2483669.2483678&partnerID=40&md5=501e0375ff4e562697ccc964793e9b4c,[No abstract available],,
Distributional phrasal paraphrase generation for statistical machine translation,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880202531&doi=10.1145%2f2483669.2483672&partnerID=40&md5=a49de29ee17e09b37335c08282c0d07b,"Paraphrase generation has been shown useful for various natural language processing tasks, including statistical machine translation. A commonly used method for paraphrase generation is pivoting [Callison-Burch et al. 2006], which benefits from linguistic knowledge implicit in the sentence alignment of parallel texts, but has limited applicability due to its reliance on parallel texts. Distributional paraphrasing [Marton et al. 2009a] has wider applicability, is more language-independent, but doesn't benefit from any linguistic knowledge. Nevertheless, we show that using distributional paraphrasing can yield greater gains in translation tasks. We report method improvements leading to higher gains than previously published, of almost 2 BLEU points, and provide implementation details, complexity analysis, and further insight into this method. ©2013 ACM.",Paraphrase generation; Semantic distance; Semantic similarity; SMT; Statistical machine translation,Natural language processing systems; Semantics; Surface mount technology; Complexity analysis; Linguistic knowledge; Method improvement; NAtural language processing; Paraphrase generation; Semantic distance; Semantic similarity; Statistical machine translation; Linguistics
"Introduction to the special section on social computing, behavioral-cultural modeling, and prediction",2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880222127&doi=10.1145%2f2483669.2483684&partnerID=40&md5=8027db21d5cbcb420138eb99c6c4fd80,[No abstract available],,
Detecting changes in information diffusion patterns over social networks,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880236127&doi=10.1145%2f2483669.2483688&partnerID=40&md5=bcdcafd8885cf2857e2966eb3fb39fbc,"We addressed the problem of detecting the change in behavior of information diffusion over a social network which is caused by an unknown external situation change using a small amount of observation data in a retrospective setting. The unknown change is assumed effectively reflected in changes in the parameter values in the probabilistic information diffusion model, and the problem is reduced to detecting where in time and how long this change persisted and how big this change is.We solved this problem by searching the change pattern that maximizes the likelihood of generating the observed information diffusion sequences, and in doing so we devised a very efficient general iterative search algorithm using the derivative of the likelihood which avoids parameter value optimization during each search step. This is in contrast to the naive learning algorithm in that it has to iteratively update the patten boundaries, each requiring the parameter value optimization and thus is very inefficient.We tested this algorithm for two instances of the probabilistic information diffusion model which has different characteristics. One is of information push style and the other is of information pull style. We chose Asynchronous Independent Cascade (AsIC) model as the former and Value-weighted Voter (VwV) model as the latter. The AsIC is the model for general information diffusion with binary states and the parameter to detect its change is diffusion probability and the VwV is the model for opinion formation with multiple states and the parameter to detect its change is opinion value. The results tested on these two models using four real-world network structures confirm that the algorithm is robust enough and can efficiently identify the correct change pattern of the parameter values. Comparison with the naive method that finds the best combination of change boundaries by an exhaustive search through a set of randomly selected boundary candidates shows that the proposed algorithm far outperforms the native method both in terms of accuracy and computation time. ©2013 ACM.",Change point detection; Information diffusion; Parameter learning; Social networks,Algorithms; Iterative methods; Optimization; Social networking (online); Change point detection; General information; Information diffusion; Opinion formation; Parameter learning; Probabilistic information; Real-world networks; Search Algorithms; Parameter estimation
Introduction to special section on paraphrasing,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880216593&doi=10.1145%2f2483669.2483670&partnerID=40&md5=4340e765f8b9b75a32401539c4b2b104,[No abstract available],,
Exploring pattern-aware travel routes for trajectory search,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880194316&doi=10.1145%2f2483669.2483681&partnerID=40&md5=29aad851b519f05eebbcc9d5047d9d6b,"With the popularity of positioning devices, Web 2.0 technology, and trip sharing services, many users are willing to log and share their trips on the Web. Thus, trip planning Web sites are able to provide some new services by inferring Regions-Of-Interest (ROIs) and recommending popular travel routes from trip trajectories. We argue that simply providing some travel routes consisting of popular ROIs to users is not sufficient. To tour around a wide geographical area, for example, a city, some users may prefer a trip to visit as many ROIs as possible, while others may like to stop by only a few ROIs for an in-depth visit. We refer to a trip fitting the former user group as an in-breadth trip and a trip suitable for the latter user group as an in-depth trip. Prior studies on trip planning have focused on mining ROIs and travel routes without considering these different preferences. In this article, given a spatial range and a user preference of depth/breadth specified by a user, we develop a Pattern-Aware Trajectory Search (PATS) framework to retrieve the top K trajectories passing through popular ROIs. PATS is novel because the returned travel trajectories, discovered from travel patterns hidden in trip trajectories, may represent the most valuable travel experiences of other travelers fitting the user's trip preference in terms of depth or breadth. The PATS framework comprises two components: travel behavior exploration and trajectory search. The travel behavior exploration component determines a set of ROIs along with their attractive scores by considering not only the popularity of the ROIs but also the travel sequential relationships among the ROIs. To capture the travel sequential relationships among ROIs and to derive their attractive scores, a user movement graph is constructed. For the trajectory search component of PATS, we formulate two trajectory score functions, the depth-trip score function and the breadth-trip score function, by taking into account the number of ROIs in a trajectory and their attractive scores. Accordingly, we propose an algorithm, namely, Bounded Trajectory Search (BTS), to efficiently retrieve the top K trajectories based on the two trajectory scores. The PATS framework is evaluated by experiments and user studies using a real dataset. The experimental results demonstrate the effectiveness and the efficiency of the proposed PATS framework. ©2013 ACM.",Data mining; Route planning; Trajectory pattern mining; Trajectory search,Data mining; Transportation; Bounded trajectories; Positioning devices; Regions of interest; Route planning; Trajectory pattern; Trajectory searches; Travel experiences; Web 2.0 Technologies; Trajectories
LONET: An Interactive Search Network for Intelligent Lecture Path Generation,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024254019&doi=10.1145%2f2438653.2438665&partnerID=40&md5=c815c6c034869811df6abdcd5e4a2f2a,"Sharing resources and information on the Internet has become an important activity for education. In distance learning, instructors can benefit from resources, also known as Learning Objects (LOs), to create plenteous materials for specific learning purposes. Our repository (called the MINE Registry) has been developed for storing and sharing learning objects, around 22,000 in total, in the past few years. To enhance reusability, one significant concept named Reusability Tree was implemented to trace the process of changes. Also, weighting and ranking metrics have been proposed to enhance the searchability in the repository. Following the successful implementation, this study goes further to investigate the relationships between LOs from a perspective of social networks. The LONET (Learning Object Network), as an extension of Reusability Tree, is newly proposed and constructed to clarify the vague reuse scenario in the past, and to summarize collaborative intelligence through past interactive usage experiences.We define a social structure in our repository based on past usage experiences from instructors, by proposing a set of metrics to evaluate the interdependency such as prerequisites and references. The structure identifies usage experiences and can be graphed in terms of implicit and explicit relations among learning objects. As a practical contribution, an adaptive algorithm is proposed to mine the social structure in our repository. The algorithm generates adaptive routes, based on past usage experiences, by computing possible interactive input, such as search criteria and feedback from instructors, and assists them in generating specific lectures. Copyright © 2013, ACM. All rights reserved.",Algorithms; distance learning; Human Factors; interactive search; Learning object network; lecture path; Management; Performance; ranking; repository; SCORM; social network analysis; Standardization,
Learning Image-to-Class Distance Metric for Image Classification,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024243443&doi=10.1145%2f2438653.2438669&partnerID=40&md5=52a3de418c0a8f5f12ea56ee1964b3a2,"Image-To-Class (I2C) distance is a novel distance used for image classification and has successfully handled datasets with large intra-class variances. However, it uses Euclidean distance for measuring the distance between local features in different classes, which may not be the optimal distance metric in real image classification problems. In this article, we propose a distance metric learning method to improve the performance of I2C distance by learning per-class Mahalanobis metrics in a large margin framework. Our I2C distance is adaptive to different classes by combining with the learned metric for each class. These multiple per-class metrics are learned simultaneously by forming a convex optimization problem with the constraints that the I2C distance from each training image to its belonging class should be less than the distances to other classes by a large margin. A subgradient descent method is applied to efficiently solve this optimization problem. For efficiency and scalability to large-scale problems, we also show how to simplify the method to learn a diagonal matrix for each class. We show in experiments that our learned Mahalanobis I2C distance can significantly outperform the original Euclidean I2C distance as well as other distance metric learning methods in several prevalent image datasets, and our simplified diagonal matrices can preserve the performance but significantly speed up the metric learning procedure for large-scale datasets. We also show in experiment that our method is able to correct the class imbalance problem, which usually leads the NN-based methods toward classes containing more training images. Copyright © 2013, ACM. All rights reserved.",Algorithms; distance metric learning; Experimentation; image classification; Image-to-class distance; nearest-neighbor classification,
Introduction to special section on trust in multiagent systems,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876152255&doi=10.1145%2f2438653.2438658&partnerID=40&md5=57d33d863045f8c2447a319bcd6f38cd,[No abstract available],Trust,
From manifesta to krypta: The relevance of categories for trusting others,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876158472&doi=10.1145%2f2438653.2438662&partnerID=40&md5=d4cad570c9afa8ffca059e9ed36bb37d,"In this article we consider the special abilities needed by agents for assessing trust based on inference and reasoning. We analyze the case in which it is possible to infer trust towards unknown counterparts by reasoning on abstract classes or categories of agents shaped in a concrete application domain. We present a scenario of interacting agents providing a computational model implementing different strategies to assess trust. Assuming a medical domain, categories, including both competencies and dispositions of possible trustees, are exploited to infer trust towards possibly unknown counterparts. The proposed approach for the cognitive assessment of trust relies on agents' abilities to analyze heterogeneous information sources along different dimensions. Trust is inferred based on specific observable properties (manifesta), namely explicitly readable signals indicating internal features (krypta) regulating agents' behavior and effectiveness on specific tasks. Simulative experiments evaluate the performance of trusting agents adopting different strategies to delegate tasks to possibly unknown trustees, while experimental results show the relevance of this kind of cognitive ability in the case of open multiagent systems. © 2013 ACM.",Cognitive analysis; Fuzzy cognitive maps; Open systems; Trust by reasoning,Multi agent systems; Open systems; Cognitive analysis; Cognitive assessments; Computational model; Concrete applications; Fuzzy cognitive map; Heterogeneous information sources; Open multi-agent system; Trust by reasoning; Abstracting
Learning image-to-class distance metric for image classification,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876105426&doi=10.1145%2f2438653.2438659&partnerID=40&md5=f8d99bdc576c51c35c3c2cc38e0f69bc,"Image-To-Class (I2C) distance is a novel distance used for image classification and has successfully handled datasets with large intra-class variances. However, it uses Euclidean distance for measuring the distance between local features in different classes, which may not be the optimal distance metric in real image classification problems. In this article, we propose a distance metric learning method to improve the performance of I2C distance by learning per-class Mahalanobis metrics in a large margin framework. Our I2C distance is adaptive to different classes by combining with the learned metric for each class. These multiple per-class metrics are learned simultaneously by forming a convex optimization problem with the constraints that the I2C distance from each training image to its belonging class should be less than the distances to other classes by a large margin. A subgradient descent method is applied to efficiently solve this optimization problem. For efficiency and scalability to large-scale problems, we also show how to simplify the method to learn a diagonal matrix for each class. We show in experiments that our learned Mahalanobis I2C distance can significantly outperform the original Euclidean I2C distance as well as other distance metric learning methods in several prevalent image datasets, and our simplified diagonal matrices can preserve the performance but significantly speed up the metric learning procedure for large-scale datasets. We also show in experiment that our method is able to correct the class imbalance problem, which usually leads the NN-based methods toward classes containing more training images. © 2013 ACM.",Distance metric learning; Image classification; Image-to-class distance; Nearest-neighbor classification,Convex optimization; Experiments; Learning systems; Optimization; Problem solving; Class imbalance problems; Convex optimization problems; Distance Metric Learning; Image-to-class distance; Large-scale datasets; Nearest-neighbor classifications; Optimization problems; Subgradient descent; Image classification
LONET: An interactive search network for intelligent lecture path generation,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876115367&doi=10.1145%2f2438653.2438668&partnerID=40&md5=76c56e8dd6ed6e5bf42a6277073973e4,"Sharing resources and information on the Internet has become an important activity for education. In distance learning, instructors can benefit from resources, also known as Learning Objects (LOs), to create plenteous materials for specific learning purposes. Our repository (called the MINE Registry) has been developed for storing and sharing learning objects, around 22,000 in total, in the past few years. To enhance reusability, one significant concept named Reusability Tree was implemented to trace the process of changes. Also, weighting and ranking metrics have been proposed to enhance the searchability in the repository. Following the successful implementation, this study goes further to investigate the relationships between LOs from a perspective of social networks. The LONET (Learning Object Network), as an extension of Reusability Tree, is newly proposed and constructed to clarify the vague reuse scenario in the past, and to summarize collaborative intelligence through past interactive usage experiences.We define a social structure in our repository based on past usage experiences from instructors, by proposing a set of metrics to evaluate the interdependency such as prerequisites and references. The structure identifies usage experiences and can be graphed in terms of implicit and explicit relations among learning objects. As a practical contribution, an adaptive algorithm is proposed to mine the social structure in our repository. The algorithm generates adaptive routes, based on past usage experiences, by computing possible interactive input, such as search criteria and feedback from instructors, and assists them in generating specific lectures. © 2013 ACM.",Distance learning; Interactive search; Learning object network; Lecture path; Ranking; Repository; SCORM; Social network analysis,Algorithms; Education; Forestry; Information Retrieval; Internet; Adaptive algorithms; Distance education; Forestry; Reusability; Interactive search; Learning objects; Lecture path; Ranking; Repository; SCORM; Social Network Analysis; Social networking (online)
Personalized reading support for second-language web documents,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876155058&doi=10.1145%2f2438653.2438666&partnerID=40&md5=62aef9a122e6b2cda8af6a28023a05de,"A novel intelligent interface eases the browsing of Web documents written in the second languages of users. It automatically predicts words unfamiliar to the user by a collective intelligence method and glosses them with their meaning in advance. If the prediction succeeds, the user does not need to consult a dictionary; even if it fails, the user can correct the prediction. The correction data are collected and used to improve the accuracy of further predictions. The prediction is personalized in that every user's language ability is estimated by a state-of-the-art language testing model, which is trained in a practical response time with only a small sacrifice of prediction accuracy. The system was evaluated in terms of prediction accuracy and reading simulation. The reading simulation results show that this system can reduce the number of clicks for most readers with insufficient vocabulary to read documents and can significantly reduce the remaining number of unfamiliar words after the prediction and glossing for all users. © 2013 ACM.",Glossing systems; Item response theory; Logistic regression; Reading support; Web pages,Ability testing; Logistics; Websites; World Wide Web; Collective intelligences; Intelligent interface; Item response theory; Language testing; Logistic regressions; Prediction accuracy; Second language; Web document; Forecasting
Formalizing and verifying protocol refinements,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876124077&doi=10.1145%2f2438653.2438656&partnerID=40&md5=3fec48c0b16606ed9787428d2f7c8556,"A (business) protocol describes, in high-level terms, a pattern of communication between two or more participants, specifically via the creation and manipulation of the commitments between them. In this manner, a protocol offers both flexibility and rigor: a participant may communicate in anyway it chooses as long as it discharges all of its activated commitments. Protocols thus promise benefits in engineering cross-organizational business processes. However, software engineering using protocols presupposes a formalization of protocols and a notion of the refinement of one protocol by another. Refinement for protocols is both intuitively obvious (e.g., PayViaCheck is clearly a kind of Pay) and technically nontrivial (e.g., compared to Pay, PayViaCheck involves different participants exchanging different messages). This article formalizes protocols and their refinement. It develops Proton, an analysis tool for protocol specifications that overlays a model checker to compute whether one protocol refines another with respect to a stated mapping. Proton and its underlying theory are evaluated by formalizing several protocols from the literature and verifying all and only the expected refinements. © 2013 ACM.",Agent communication; Commitments; Verification of multiagent systems,Communication; Model checking; Multi agent systems; Software engineering; Agent communications; Analysis tools; Commitments; Cross-organizational business process; Model checker; Protocol refinement; Protocol specifications; Several protocols; Protons
Building and using social structures: A case study using the agent ART testbed,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876104348&doi=10.1145%2f2438653.2438660&partnerID=40&md5=972d21493ec360dcc0ed860b5e088d15,"This article investigates the conjecture that agents who make decisions in scenarios where trust is important can benefit from the use of a social structure, representing the social relationships that exist between agents. We propose techniques that can be used by agents to initially build and then progressively update such a structure in the light of experience. We describe an implementation of our techniques in the domain of the Agent ART testbed: we take two existing agents for this domain (""Simplet"" and ""Connected"") and compare their performance with versions that use our social structure (""SocialSimplet"" and ""SocialConnected""). We show that SocialSimplet and SocialConnected outperform their counterparts with respect to the quality of the interactions, the number of rounds won in a competition, and the total utility gained. © 2013 ACM.",Agents; Trading competition; Trust,Agents; Social relationships; Social structure; Trust; Testbeds
"A fully online and unsupervised system for large and high-density area surveillance: Tracking, semantic scene learning and abnormality detection",2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876134773&doi=10.1145%2f2438653.2438670&partnerID=40&md5=7026706bb775cdbf9aca027b37a1fc65,"For reasons of public security, an intelligent surveillance system that can cover a large, crowded public area has become an urgent need. In this article, we propose a novel laser-based system that can simultaneously perform tracking, semantic scene learning, and abnormality detection in a fully online and unsupervised way. Furthermore, these three tasks cooperate with each other in one framework to improve their respective performances. The proposed system has the following key advantages over previous ones: (1) It can cover quite a large area (more than 60×35m), and simultaneously perform robust tracking, semantic scene learning, and abnormality detection in a high-density situation. (2) The overall system can vary with time, incrementally learn the structure of the scene, and perform fully online abnormal activity detection and tracking. This feature makes our system suitable for real-time applications. (3) The surveillance tasks are carried out in a fully unsupervised manner, so that there is no need for manual labeling and the construction of huge training datasets.We successfully apply the proposed system to the JR subway station in Tokyo, and demonstrate that it can cover an area of 60×35m, robustly track more than 150 targets at the same time, and simultaneously perform online semantic scene learning and abnormality detection with no human intervention. © 2013 ACM.",Abnormality detection; Multitarget tracking; Semantic scene learning; Surveillance,Monitoring; Space surveillance; Subway stations; Abnormal activity detection; Abnormality detection; Human intervention; Intelligent surveillance systems; Laser-based systems; Multi-target tracking; Real-time application; Surveillance task; Semantics
Introduction to the special section on agent communication,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876151267&doi=10.1145%2f2438653.2438654&partnerID=40&md5=c67f823ab760ff2ce1ddb2e238a142de,[No abstract available],,
A framework for trust modeling in multiagent electronic marketplaces with buying advisors to consider varying seller behavior and the limiting of seller bids,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876130135&doi=10.1145%2f2438653.2438659&partnerID=40&md5=3766a27da9910ba0f8efb390af59c63b,"In this article, we present a framework of use in electronic marketplaces that allows buying agents to model the trustworthiness of selling agents in an effective way, making use of seller ratings provided by other buying agents known as advisors. The trustworthiness of the advisors is also modeled, using an approach that combines both personal and public knowledge and allows the relative weighting to be adjusted over time. Through a series of experiments that simulate e-marketplaces, including ones where sellers may vary their behavior over time, we are able to demonstrate that our proposed framework delivers effective seller recommendations to buyers, resulting in important buyer profit. We also propose limiting seller bids as a method for promoting seller honesty, thus facilitating successful selection of sellers by buyers, and demonstrate the value of this approach through experimental results. Overall, this research is focused on the technological aspects of electronic commerce and specifically on technology that would be used to manage trust. © 2013 ACM.",Electronic commerce applications; Multiagent systems; Social networks; Trust modeling,Electronic commerce; Multi agent systems; Profitability; Sales; E-marketplaces; Electronic commerce applications; Electronic marketplaces; Public knowledge; Social Networks; Technological aspects; Trust modeling; Intelligent agents
Intelligent systems and technology for integrative and predictive medicine: An ACP approach,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876136924&doi=10.1145%2f2438653.2438667&partnerID=40&md5=b77ae5b10304d2508e33ee5468064e58,"One of the principal goals in medicine is to determine and implement the best treatment for patients through fastidious estimation of the effects and benefits of therapeutic procedures. The inherent complexities of physiological and pathological networks that span across orders of magnitude in time and length scales, however, represent fundamental hurdles in determining effective treatments for patients. Here we argue for a new approach, called the ACP-based approach, that combines artificial (societies), computational (experiments), and parallel (execution) methods in intelligent systems and technology for integrative and predictive medicine, or more generally, precision medicine and smart health management. The advent of artificial societies that collect the clinically relevant information in prognostics and therapeutics provides a promising platform for organizing and experimenting complex physiological systems toward integrative medicine. The ability of computational experiments to analyze distinct, interactive systems such as the host mechanisms, pathological pathways, and therapeutic strategies, as well as other factors using the artificial systems, will enable control and management through parallel execution of real and arficial systems concurrently within the integrative medicine context. The development of this framework in integrative medicine, fueled by close collaborations between physicians, engineers, and scientists, will result in preventive and predictive practices of a personal, proactive, and precise nature, including rational combinatorial treatments, adaptive therapeutics, and patient-oriented disease management. © 2013 ACM.",,Complex networks; Experiments; Intelligent systems; Physiology; Artificial societies; Computational experiment; Control and management; Orders of magnitude; Physiological systems; Relevant informations; Therapeutic procedures; Therapeutic strategy; Patient treatment
Tractable POMDP representations for intelligent tutoring systems,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876121023&doi=10.1145%2f2438653.2438664&partnerID=40&md5=8967d796508e3d4093a1222b5ad743f6,"With Partially Observable Markov Decision Processes (POMDPs), Intelligent Tutoring Systems (ITSs) can model individual learners from limited evidence and plan ahead despite uncertainty. However, POMDPs need appropriate representations to become tractable in ITSs that model many learner features, such as mastery of individual skills or the presence of specific misconceptions. This article describes two POMDP representations-state queues and observation chains-that take advantage of ITS task properties and let POMDPs scale to represent over 100 independent learner features. A real-world military training problem is given as one example. A human study (n = 14) provides initial validation for the model construction. Finally, evaluating the experimental representations with simulated students helps predict their impact on ITS performance. The compressed representations can model a wide range of simulated problems with instructional efficacy equal to lossless representations. With improved tractability, POMDP ITSs can accommodate more numerous or more detailed learner states and inputs. © 2013 ACM.",Computer-based training; Intelligent tutoring systems; Partially observable Markov decision processes,Computer aided instruction; Computer based training; Human study; Individual skills; Intelligent tutoring system; Intelligent tutoring system (ITSs); Military training; Model construction; Partially observable Markov decision process; Computer simulation
Stereotypical trust and bias in dynamic multiagent systems,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876149284&doi=10.1145%2f2438653.2438661&partnerID=40&md5=0bc305000c7a63443907a0687be66e5b,"Large-scale multiagent systems have the potential to be highly dynamic. Trust and reputation are crucial concepts in these environments, as it may be necessary for agents to rely on their peers to perform as expected, and learn to avoid untrustworthy partners. However, aspects of highly dynamic systems introduce issues which make the formation of trust relationships difficult. For example, they may be short-lived, precluding agents from gaining the necessary experiences to make an accurate trust evaluation. This article describes a new approach, inspired by theories of human organizational behavior, whereby agents generalize their experiences with previously encountered partners as stereotypes, based on the observable features of those partners and their behaviors. Subsequently, these stereotypes are applied when evaluating new and unknown partners. Furthermore, these stereotypical opinions can be communicated within the society, resulting in the notion of stereotypical reputation. We show how this approach can complement existing state-of-the-art trust models, and enhance the confidence in the evaluations that can be made about trustees when direct and reputational information is lacking or limited. Furthermore, we show how a stereotyping approach can help agents detect unwanted biases in the reputational opinions they receive from others in the society. © 2013 ACM.",Multiagent systems; Stereotypes; Trust,Behavioral research; New approaches; Organizational behavior; Stereotypes; Trust; Trust and reputation; Trust evaluation; Trust models; Trust relationship; Multi agent systems
A parts-based approach for automatic 3D shape categorization using belief functions,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876146330&doi=10.1145%2f2438653.2438668&partnerID=40&md5=32072a2eb1504276787dc9d150ed3537,"Grouping 3D objects into (semantically) meaningful categories is a challenging and important problem in 3D mining and shape processing. Here, we present a novel approach to categorize 3D objects. The method described in this article, is a belief-function-based approach and consists of two stages: the training stage, where 3D objects in the same category are processed and a set of representative parts is constructed, and the labeling stage, where unknown objects are categorized. The experimental results obtained on the Tosca- Sumner and the Shrec07 datasets show that the system efficiently performs in categorizing 3D models. © 2013 ACM.",3D categorization; Belief functions; Classification; Multimedia data; Object recognition,Classification (of information); Content based retrieval; Object recognition; Three dimensional computer graphics; Uncertainty analysis; 3-D shape; 3D categorization; 3D models; Belief function; Multimedia data; Parts-based approaches; Shape processing; Unknown objects; Three dimensional
Introduction to the special section on intelligent tutoring and coaching systems,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876133477&doi=10.1145%2f2438653.2438663&partnerID=40&md5=af95e747be1f1a2cd7722da35663910a,[No abstract available],,
A conformant planner based on approximation: CPA(H),2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876128871&doi=10.1145%2f2438653.2438671&partnerID=40&md5=16c0b60368731582ed1851d72832f276,"This article describes the planner CPA(H), the recipient of the Best Nonobservable Nondeterministic Planner Award in the ""Uncertainty Track"" of the 6th International Planning Competition (IPC), 2008. The article presents the various techniques that helpCPA(H) to achieve the level of performance and scalability exhibited in the competition. The article also presents experimental results comparing CPA(H) with state-of-the-art conformant planners. © 2013 ACM.",Approximations; Conformant planning; International planning competition; Plan generation,Approximations; Conformant planning; International Planning Competitions; Non-observable; Performance and scalabilities; Plan generation
Constitutive and regulative specifications of commitment protocols: A decoupled approach,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876136635&doi=10.1145%2f2438653.2438657&partnerID=40&md5=0dd7137f7e092e1bdbf02c4f84f99102,"Interaction protocols play a fundamental role in multiagent systems. In this work, after analyzing the trends that are emerging not only from research on multiagent interaction protocols but also from neighboring fields, like research on workflows and business processes, we propose a novel definition of commitment-based interaction protocols, that is characterized by the decoupling of the constitutive and the regulative specifications and that explicitly foresees a representation of the latter based on constraints among commitments. A clear distinction between the two representations has many advantages, mainly residing in a greater openness of multiagent systems, and an easier reuse of protocols and of action definitions. A language, named 2CL, for writing regulative specifications is also given together with a designer-oriented graphical notation. © 2013 ACM.",Commitments; Constitutive and regulative specifications; Constraints among commitments; Interaction protocols,Multi agent systems; Business Process; Commitment protocols; Commitments; Constraints among commitments; Graphical notation; Inter-action protocols; Multi-agent interaction; Work-flows; Specifications
Dynamic joint sentiment-topic model,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891775892&doi=10.1145%2f2542182.2542188&partnerID=40&md5=afcc6e4625ee606dbdd805e050eb9ef1,"Social media data are produced continuously by a large and uncontrolled number of users. The dynamic nature of such data requires the sentiment and topic analysis model to be also dynamically updated, capturing the most recent language use of sentiments and topics in text. We propose a dynamic Joint Sentiment-Topic model (dJST) which allows the detection and tracking of views of current and recurrent interests and shifts in topic and sentiment. Both topic and sentiment dynamics are captured by assuming that the current sentiment-topic-specific word distributions are generated according to the word distributions at previous epochs. We study three different ways of accounting for such dependency information: (1) Sliding window where the current sentiment-topic word distributions are dependent on the previous sentiment-topic-specific word distributions in the last S epochs; (2) skip model where history sentiment topic word distributions are considered by skipping some epochs in between; and (3) multiscale model where previous long- and shorttimescale distributions are taken into consideration. We derive efficient online inference procedures to sequentially update the model with newly arrived data and show the effectiveness of our proposed model on the Mozilla add-on reviews crawled between 2007 and 2011. © 2013 ACM 2157-6904/2013/12-ART5 $ 15.00.",Dynamic joint sentiment-topic model; Opinion mining; Sentiment analysis; Topic model,Sentiment analysis; Dependency informations; Detection and tracking; Dynamic nature; Multi-scale Modeling; Online inferences; Social media datum; Topic analysis; Topic Modeling; Data mining
Generating virtual ratings from chinese reviews to augment online recommendations,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873616070&doi=10.1145%2f2414425.2414434&partnerID=40&md5=5ac98557fbb80953707a6deb6722260d,"Collaborative filtering (CF) recommenders based on User-Item rating matrix as explicitly obtained from end users have recently appeared promising in recommender systems. However, User-Item rating matrix is not always available or very sparse in some web applications, which has critical impact to the application of CF recommenders. In this article we aim to enhance the online recommender system by fusing virtual ratings as derived from user reviews. Specifically, taking into account of Chinese reviews' characteristics, we propose to fuse the self-supervised emotion-integrated sentiment classification results into CF recommenders, by which the User-Item Rating Matrix can be inferred by decomposing item reviews that users gave to the items. The main advantage of this approach is that it can extend CF recommenders to some web applications without user rating information. In the experiments, we have first identified the self-supervised sentiment classification's higher precision and recall by comparing it with traditional classification methods .Furthermore, the classification results, as behaving as virtual ratings, were incorporated into both user-based and item-based CF algorithms. We have also conducted an experiment to evaluate the proximity between the virtual and real ratings and clarified the effectiveness of the virtual ratings. The experimental results demonstrated the significant impact of virtual ratings on increasing system's recommendation accuracy in different data conditions (i.e., conditions with real ratings and without). © 2013 ACM.",Information retrieval; Online recommendation; Sentiment analysis,Collaborative filtering; Experiments; Information retrieval; Rating; World Wide Web; Classification methods; Classification results; End users; Item-based CF; Online recommendation; Online recommender systems; Precision and recall; Recommendation accuracy; Sentiment analysis; Sentiment classification; Significant impacts; User rating; User reviews; WEB application; Recommender systems
Reorder user's tweets,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873683980&doi=10.1145%2f2414425.2414431&partnerID=40&md5=0fdcc116e418e9fc9b61da36a999ec7f,"Twitter displays the tweets a user received in a reversed chronological order, which is not always the best choice. As Twitter is full of messages of very different qualities, many informative or relevant tweets might be flooded or displayed at the bottom while some nonsense buzzes might be ranked higher. In this work, we present a supervised learning method for personalized tweets reordering based on user interests. User activities on Twitter, in terms of tweeting, retweeting, and replying, are leveraged to obtain the training data for reordering models. Through exploring a rich set of social and personalized features, we model the relevance of tweets by minimizing the pairwise loss of relevant and irrelevant tweets. The tweets are then reordered according to the predicted relevance scores. Experimental results with real twitter user activities demonstrated the effectiveness of our method. The new method achieved above 30% accuracy gain compared with the default ordering in twitter based on time. © 2013 ACM.",Personalization; Reorder; Retweet; Twitter,Best choice; Chronological order; Personalizations; Relevance score; Reorder; Reordering models; Retweet; Supervised learning methods; Training data; Twitter; User activity; User interests; Social networking (online)
Improving recency ranking using twitter data,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873618242&doi=10.1145%2f2414425.2414429&partnerID=40&md5=3877fbe25db282a1b369e7a58e6fb530,"In Web search and vertical search, recency ranking refers to retrieving and ranking documents by both relevance and freshness. As impoverished in-links and click information is the the biggest challenge for recency ranking, we advocate the use of Twitter data to address the challenge in this article. We propose a method to utilize Twitter TinyURL to detect fresh and high-quality documents, and leverage Twitter data to generate novel and effective features for ranking. The empirical experiments demonstrate that the proposed approach effectively improves a commercial search engine for both Web search ranking and tweet vertical ranking. © 2013 ACM.",Recency ranking; Tweet ranking; Twitter,Information retrieval; Search engines; World Wide Web; Empirical experiments; High quality; Recency ranking; Tweet ranking; Twitter; Vertical searches; Web search rankings; Web searches; Social networking (online)
Introduction to the special section on twitter and microblogging services,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873695563&doi=10.1145%2f2414425.2414426&partnerID=40&md5=32d0966b2a551c580e6ce5b6ef61676a,[No abstract available],,
"An empirical comparison of social, collaborative filtering, and hybrid recommenders",2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873636017&doi=10.1145%2f2414425.2414439&partnerID=40&md5=45b15a9c141a4378fcc17cecfdcf608d,"In the Social Web, a number of diverse recommendation approaches have been proposed to exploit the user generated contents available in the Web, such as rating, tagging, and social networking information. In general, these approaches naturally require the availability of a wide amount of these user preferences. This may represent an important limitation for real applications, and may be somewhat unnoticed in studies focusing on overall precision, in which a failure to produce recommendations gets blurred when averaging the obtained results or, even worse, is just not accounted for, as users with no recommendations are typically excluded from the performance calculations. In this article, we propose a coverage metric that uncovers and compensates for the incompleteness of performance evaluations based only on precision. We use this metric together with precision metrics in an empirical comparison of several social, collaborative filtering, and hybrid recommenders. The obtained results show that a better balance between precision and coverage can be achieved by combining social-based filtering (high accuracy, low coverage) and collaborative filtering (low accuracy, high coverage) recommendation techniques. We thus explore several hybrid recommendation approaches to balance this trade-off. In particular, we compare, on the one hand, techniques integrating collaborative and social information into a single model, and on the other, linear combinations of recommenders. For the last approach, we also propose a novel strategy to dynamically adjust the weight of each recommender on a user-basis, utilizing graph measures as indicators of the target user's connectedness and relevance in a social network. © 2013 ACM.",Collaborative filtering; Graph theory; Hybrid recommenders; Random walk; Recommender Systems; Social networks; User coverage,Graph theory; Recommender systems; Social networking (online); Empirical - comparisons; Hybrid recommendation approaches; Hybrid recommenders; Linear combinations; Novel strategies; Performance calculation; Performance evaluation; Random Walk; Real applications; Recommendation techniques; Single models; Social information; Social Networks; Social webs; Social-based; User coverage; User-generated content; Collaborative filtering
Research directions in agent communication,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873180315&doi=10.1145%2f2438653.2438655&partnerID=40&md5=0bf88e4d08ba96ec57f9ca0dc3634023,"Increasingly, software engineering involves open systems consisting of autonomous and heterogeneous participants or agents who carry out loosely coupled interactions. Accordingly, understanding and specifying communications among agents is a key concern. A focus on ways to formalize meaning distinguishes agent communication from traditional distributed computing: meaning provides a basis for flexible interactions and compliance checking. Over the years, a number of approaches have emerged with some essential and some irrelevant distinctions drawn among them. As agent abstractions gain increasing traction in the software engineering of open systems, it is important to resolve the irrelevant and highlight the essential distinctions, so that future research can be focused in the most productive directions. This article is an outcome of extensive discussions among agent communication researchers, aimed at taking stock of the field and at developing, criticizing, and refining their positions on specific approaches and future challenges. This article serves some important purposes, including identifying (1) points of broad consensus; (2) points where substantive differences remain; and (3) interesting directions of future work. © 2013 ACM.",Communication,Communication; Compliance control; Distributed computer systems; Open systems; Agent abstractions; Agent communications; Compliance checking; Future challenges; Loosely coupled; Autonomous agents
An approach to social recommendation for context-aware mobile services,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873670541&doi=10.1145%2f2414425.2414435&partnerID=40&md5=046ce0a609461b8cc6121ac900386656,"Nowadays, several location-based services (LBSs) allow their users to take advantage of information from the Web about points of interest (POIs) such as cultural events or restaurants. To the best of our knowledge, however, none of these provides information taking into account user preferences, or other elements, in addition to location, that contribute to define the context of use. The provided suggestions do not consider, for example, time, day of week, weather, user activity or means of transport. This article describes a social recommender system able to identify user preferences and information needs, thus suggesting personalized recommendations related to POIs in the surroundings of the user's current location. The proposed approach achieves the following goals: (i) to supply, unlike the current LBSs, a methodology for identifying user preferences and needs to be used in the information filtering process; (ii) to exploit the ever-growing amount of information from social networking, user reviews, and local search Web sites; (iii) to establish procedures for defining the context of use to be employed in the recommendation of POIs with low effort. The flexibility of the architecture is such that our approach can be easily extended to any category of POI. Experimental tests carried out on real users enabled us to quantify the benefits of the proposed approach in terms of performance improvement. © 2013 ACM.",Social recommender system; Ubiquitous computing; User modeling,Location based services; Ubiquitous computing; Amount of information; Context of use; Context-aware mobile services; Cultural events; Experimental test; Information need; Local search; Performance improvements; Personalized recommendation; Points of interest; Social recommender systems; User activity; User Modeling; User reviews; Recommender systems
Mathematical description and analysis of adaptive risk choice behavior,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873657471&doi=10.1145%2f2414425.2414442&partnerID=40&md5=c0e20af5546c3bde5ac13cb0d2df0924,"Which risk should one choose when facing alternatives with different levels of risk?We discuss here adaptive processes in such risk choice behavior by generalizing the study of Roos et al. [2010].We deal with an n-choice game in which every player sequentially chooses n times of lotteries of which there are two types: a safe lottery and a risky lottery. We analyze this model in more detail by elaborating the game. Based on the results of mathematical analysis, replicator dynamics analysis, and numerical simulations, we derived some salient features of risk choice behavior. We show that all the risk strategies can be divided into two groups: persistence and nonpersistence. We also proved that the dynamics with perturbation in which a mutation is installed is globally asymptotically stable to a unique equilibrium point for any initial population. The numerical simulations clarify that the number of persistent strategies seldom increases regardless of the increase in n, and suggest that a rarity of dominant choice strategies is widely observed in many social contexts. These facts not only go hand-in-hand with some well-known insights from prospect theory, but may also provide some theoretical hypotheses for various fields such as behavioral economics, ecology, sociology, and consumer behavioral theory. © 2013 ACM.",Adaptive process; Decision theory; Mutation; Replicator dynamics; Risk; Risk choice strategy; Sequentiality; Winner-takes-all,Computer simulation; Decision theory; Dynamics; Adaptive process; Mutation; Replicator dynamics; Sequentiality; Winner-takes-all; Risks
Introduction to the special section on social recommender systems,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873659529&doi=10.1145%2f2414425.2414432&partnerID=40&md5=56220e4d9dc8ea87d0f259fac961af68,[No abstract available],,
Introduction to special section on CAMRa2010: Movie recommendation in context,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873629230&doi=10.1145%2f2414425.2414438&partnerID=40&md5=373817d7f3aa59544d2ed1f31b80bd1f,"The challenge and workshop on Context-Aware Movie Recommendation (CAMRa2010) were conducted jointly in 2010 with the Recommender Systems conference. The challenge focused on three context-aware recommendation scenarios: time-based, mood-based, and social recommendation. The participants were provided with anonymized datasets from two real-world online movie recommendation communities and competed against each other for obtaining the highest accuracy of recommendations. The datasets contained contextual features, such as tags, annotation, social relationsips, and comments, normally not available in public recommendation datasets. More than 40 teams from 21 countries participated in the challenge. Their participation was summarized by 10 papers published by the workshop, which have been extended and revised for this special section. In this preface we overview the challenge datasets, tasks, evaluation metrics, and the obtained outcomes. © 2013 ACM.",Context-aware recommendations; Context-awareness; Datasets; Recommender systems; Social network analysis; User modeling,Recommender systems; Social networking (online); Context-aware recommendations; Context-awareness; Datasets; Social Network Analysis; User Modeling; Motion pictures
Improving recommendation accuracy based on item-specific tag preferences,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873616145&doi=10.1145%2f2414425.2414436&partnerID=40&md5=1d3645a530228bd23d045485ed53ca3a,"In recent years, different proposals have been made to exploit Social Web tagging information to build more effective recommender systems. The tagging data, for example, were used to identify similar users or were viewed as additional information about the recommendable items. Recent research has indicated that ""attaching feelings to tags"" is experienced by users as a valuable means to express which features of an item they particularly like or dislike. When following such an approach, users would therefore not only add tags to an item as in usual Web 2.0 applications, but also attach a preference (affect) to the tag itself, expressing, for example, whether or not they liked a certain actor in a given movie. In this work, we show how this additional preference data can be exploited by a recommender system to make more accurate predictions. In contrast to previous work, which also relied on so-called tag preferences to enhance the predictive accuracy of recommender systems, we argue that tag preferences should be considered in the context of an item. We therefore propose new schemes to infer and exploit context-specific tag preferences in the recommendation process. An evaluation on two different datasets reveals that our approach is capable of providing more accurate recommendations than previous tag-based recommender algorithms and recent tag-agnostic matrix factorization techniques. © 2013 ACM.",Algorithms; Recommender systems; Social web; Tagging,Algorithms; Recommender systems; Accurate prediction; Matrix factorizations; Predictive accuracy; Preference data; Recent researches; Recommendation accuracy; Recommender algorithms; Social webs; Tag-based; Tagging; Web 2.0 applications; User interfaces
A modified random walk framework for handling negative ratings and generating explanations,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873683556&doi=10.1145%2f2414425.2414437&partnerID=40&md5=baba102c50cbba4a423689d6d78b68c1,"The concept of random walk (RW) has been widely applied in the design of recommendation systems. RW-based approaches are effective in handling locality problem and taking extra information, such as the relationships between items or users, into consideration. However, the traditional RW-based approach has a serious limitation in handling bidirectional opinions. The propagation of positive and negative information simultaneously in a graph is nontrivial using random walk. To address the problem, this article presents a novel and efficientRW-based model that can handle both positive and negative comments with the guarantee of convergence. Furthermore, we argue that a good recommendation system should provide users not only a list of recommended items but also reasonable explanations for the decisions. Therefore, we propose a technique that generates explanations by backtracking the influential paths and subgraphs. The results of experiments on the MovieLens and Netflix datasets show that our model significantly outperforms stateof- the-art RW-based algorithms, and is capable of improving the overall performance in the ensemble with other models. © 2013 ACM.",Collaborative Filtering; Explanation; Integration; Random Walk; Ranking,Collaborative filtering; Integration; Recommender systems; Explanation; Movielens; Negative information; Netflix; Random Walk; Ranking; Subgraphs; Random processes
Social temporal collaborative ranking for context aware movie recommendation,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873649993&doi=10.1145%2f2414425.2414440&partnerID=40&md5=fdec8ef9e7aa33717adfd24826621a0b,"Most existing collaborative filtering models only consider the use of user feedback (e.g., ratings) and meta data (e.g., content, demographics). However, in most real world recommender systems, context information, such as time and social networks, are also very important factors that could be considered in order to produce more accurate recommendations. In this work, we address several challenges for the context aware movie recommendation tasks in CAMRa 2010: (1) how to combine multiple heterogeneous forms of user feedback? (2) how to cope with dynamic user and item characteristics? (3) how to capture and utilize social connections among users? For the first challenge, we propose a novel ranking based matrix factorization model to aggregate explicit and implicit user feedback. For the second challenge, we extend this model to a sequential matrix factorization model to enable time-aware parametrization. Finally, we introduce a network regularization function to constrain user parameters based on social connections. To the best of our knowledge, this is the first study that investigates the collective modeling of social and temporal dynamics. Experiments on the CAMRa 2010 dataset demonstrated clear improvements over many baselines. © 2013 ACM.",Collaborative filtering; Context awareness; Recommender systems; User feedback,Collaborative filtering; Recommender systems; Context information; Context- awareness; Context-Aware; Matrix factorizations; Movie recommendations; Parametrizations; Regularization function; Social connection; Social Networks; Temporal dynamics; User feedback; Face recognition
Named entity recognition for tweets,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873632407&doi=10.1145%2f2414425.2414428&partnerID=40&md5=622291dc8b976bec1a51fdf2d32a15d0,"Two main challenges of Named Entity Recognition (NER) for tweets are the insufficient information in a tweet and the lack of training data. We propose a novel method consisting of three core elements: (1) normalization of tweets; (2) combination of a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model; and (3) semisupervised learning framework. The tweet normalization preprocessing corrects common ill-formed words using a global linear model. The KNN-based classifier conducts prelabeling to collect global coarse evidence across tweets while the CRF model conducts sequential labeling to capture fine-grained information encoded in a tweet. The semisupervised learning plus the gazetteers alleviate the lack of training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of normalization, KNN, and semisupervised learning. © 2013 ACM.",Model combination; Semisupervised learning; Tweet normalization,Supervised learning; Conditional random field; Core elements; Crf models; K nearest neighbor (KNN); Model combination; Named entity recognition; Semi- supervised learning; Training data; Tweet normalization; Text processing
Lexical normalization for social media text,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873657672&doi=10.1145%2f2414425.2414430&partnerID=40&md5=b3523ac15813978dc5a6e0563632a06c,"Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this article, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalizing lexical variants. Our method uses a classifier to detect lexical variants, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn't require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter. © 2013 ACM.",Lexical normalization; Microblog; Short text message; Text analysis; Text preprocessing,Telephone systems; Lexical normalization; Micro-blog; Short text messages; Text analysis; Text preprocessing; Social networking (online)
A content-driven framework for geolocating microblog users,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873658809&doi=10.1145%2f2414425.2414427&partnerID=40&md5=218d0024ca943a8e7c4c20ec4ea99fe1,"Highly dynamic real-time microblog systems have already published petabytes of real-time human sensor data in the form of status updates. However, the lack of user adoption of geo-based features per user or per post signals that the promise of microblog services as location-based sensing systems may have only limited reach and impact. Thus, in this article, we propose and evaluate a probabilistic framework for estimating a microblog user's location based purely on the content of the user's posts. Our framework can overcome the sparsity of geo-enabled features in these services and bring augmented scope and breadth to emerging location-based personalized information services. Three of the key features of the proposed approach are: (i) its reliance purely on publicly available content; (ii) a classification component for automatically identifying words in posts with a strong local geo-scope; and (iii) a lattice-based neighborhood smoothing model for refining a user's location estimate. On average we find that the location estimates converge quickly, placing 51% of users within 100 miles of their actual location. © 2013 ACM.",Location-based estimation; Microblog; Spatial data mining; Text mining; Twitter,Data mining; Digital storage; Estimation; Information services; Sensors; Location based; Micro-blog; Spatial data mining; Text mining; Twitter; Location based services
Mining contextual movie similarity with matrix factorization for context-aware recommendation,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873621495&doi=10.1145%2f2414425.2414441&partnerID=40&md5=a33a4821a7eaf600c492e5d32b04300f,"Context-aware recommendation seeks to improve recommendation performance by exploiting various information sources in addition to the conventional user-item matrix used by recommender systems. We propose a novel context-aware movie recommendation algorithm based on joint matrix factorization (JMF). We jointly factorize the user-item matrix containing general movie ratings and other contextual movie similarity matrices to integrate contextual information into the recommendation process. The algorithm was developed within the scope of the mood-aware recommendation task that was offered by the Moviepilot mood track of the 2010 context-aware movie recommendation (CAMRa) challenge. Although the algorithm could generalize to other types of contextual information, in this work, we focus on two: movie mood tags and movie plot keywords. Since the objective in this challenge track is to recommend movies for a user given a specified mood, we devise a novel mood-specific movie similarity measure for this purpose.We enhance the recommendation based on this measure by also deploying the second movie similarity measure proposed in this article that takes into account the movie plot keywords.We validate the effectiveness of the proposed JMF algorithm with respect to the recommendation performance by carrying out experiments on the Moviepilot challenge dataset. We demonstrate that exploiting contextual information in JMF leads to significant improvement over several state-of-the-art approaches that generate movie recommendations without using contextual information. We also demonstrate that our proposed mood-specific movie similarity is better suited for the task than the conventional mood-based movie similarity measures. Finally, we show that the enhancement provided by the movie similarity capturing the plot keywords is particularly helpful in improving the recommendation to those users who are significantly more active in rating the movies than other users. © 2013 ACM.",Collaborative filtering; Context-aware recommendation; Matrix factorization; Mood-specific movie similarity; Recommender systems,Algorithms; Collaborative filtering; Recommender systems; Context-Aware; Context-aware recommendations; Contextual information; Information sources; Joint matrix factorizations; Matrix factorizations; Mood-specific movie similarity; Movie ratings; Movie recommendations; Recommendation performance; Similarity measure; State-of-the-art approach; User-item matrix; Motion pictures
"An online system for multiple interacting targets tracking: Fusion of laser and vision, tracking and learning",2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873666009&doi=10.1145%2f2414425.2414443&partnerID=40&md5=b1dc7e3b097483751ad7974e1036c7f4,"Multitarget tracking becomes significantly more challenging when the targets are in close proximity or frequently interact with each other. This article presents a promising online system to deal with these problems. The novelty of this system is that laser and vision are integrated with tracking and online learning to complement each other in one framework: when the targets do not interact with each other, the laser-based independent trackers are employed and the visual information is extracted simultaneously to train some classifiers online for ""possible interacting targets"". When the targets are in close proximity, the classifiers learned online are used alongside visual information to assist in tracking. Therefore, this mode of cooperation not only deals with various tough problems encountered in tracking, but also ensures that the entire process can be completely online and automatic. Experimental results demonstrate that laser and vision fully display their respective advantages in our system, and it is easy for us to obtain a good trade-off between tracking accuracy and the time-cost factor. © 2013 ACM.",Multiple targets tracking; Sensor fusion,Close proximity; Laser-based; Multi-target tracking; Multiple targets tracking; Online learning; Sensor fusion; Targets tracking; Time cost; Tracking accuracy; Visual information
Social factors in group recommender systems,2013,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873643151&doi=10.1145%2f2414425.2414433&partnerID=40&md5=b35efcdf803e59fc8610be0ac3cf6f6c,"In this article we review the existing techniques in group recommender systems and we propose some improvement based on the study of the different individual behaviors when carrying out a decision-making process. Our method includes an analysis of group personality composition and trust between each group member to improve the accuracy of group recommenders. This way we simulate the argumentation process followed by groups of people when agreeing on a common activity in a more realistic way. Moreover, we reflect how they expect the system to behave in a long term recommendation process. This is achieved by including a memory of past recommendations that increases the satisfaction of users whose preferences have not been taken into account in previous recommendations. © 2013 ACM.",Memory; Personality; Recommender systems; Social networks; Trust,Data storage equipment; Decision making process; Group members; Group recommender systems; Group recommenders; Individual behavior; Long-term recommendations; Personality; Social factor; Social Networks; Trust; Recommender systems
Bootstrapping a game with a purpose for commonsense collection,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867412461&doi=10.1145%2f2337542.2337544&partnerID=40&md5=fd5ff4595c8ca7097f3d31e65af710c5,"Text mining has been very successful in extracting huge amounts of commonsense knowledge from data, but the extracted knowledge tends to be extremely noisy. Manual construction of knowledge repositories, on the other hand, tends to produce high-quality data in very small amounts. We propose an architecture to combine the best of both worlds: A game with a purpose that induces humans to clean up data automatically extracted by text mining. First, a text miner trained on a set of known commonsense facts harvests many more candidate facts from corpora. Then, a simple slot-machine-with-a-purpose game presents these candidate facts to the players for verification by playing. As a result, a new dataset of high precision commonsense knowledge is created. This combined architecture is able to produce significantly better commonsense facts than the state-of-the-art text miner alone. Furthermore, we report that bootstrapping (i.e., training the text miner on the output of the game) improves the subsequent performance of the text miner. © 2012 ACM.",Commonsense; Facebook; Games with a purpose; Knowledge extraction; Natural language processing,Miners; Natural language processing systems; Commonsense; Facebook; Games with a purpose; Knowledge extraction; NAtural language processing; Data mining
"Twitter, MySpace, Digg: Unsupervised sentiment analysis in social media",2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867420933&doi=10.1145%2f2337542.2337551&partnerID=40&md5=1d68b449154401ce5d77a14f9cef1166,"Sentiment analysis is a growing area of research with significant applications in both industry and academia. Most of the proposed solutions are centered around supervised, machine learning approaches and review-oriented datasets. In this article, we focus on the more common informal textual communication on the Web, such as online discussions, tweets and social network comments and propose an intuitive, less domain-specific, unsupervised, lexicon-based approach that estimates the level of emotional intensity contained in text in order to make a prediction. Our approach can be applied to, and is tested in, two different but complementary contexts: subjectivity detection and polarity classification. Extensive experiments were carried on three real-world datasets, extracted from online social Web sites and annotated by human evaluators, against state-of-the-art supervised approaches. The results demonstrate that the proposed algorithm, even though unsupervised, outperforms machine learning solutions in the majority of cases, overall presenting a very robust and reliable solution for sentiment analysis of informal communication on theWeb. © 2012 ACM.",Opinion mining; Sentiment analysis; Social media,Communication; Data mining; Learning systems; Data sets; Domain specific; Informal communication; Learning approach; Online discussions; Opinion mining; Polarity classification; Real-world datasets; Sentiment analysis; Social media; Social Networks; Social networking (online)
Information retrieval in the commentsphere,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861043331&doi=10.1145%2f2337542.2337553&partnerID=40&md5=0c307396c09e72180ed7e216472e3c35,"This article studies information retrieval tasks related to Web comments. Prerequisite of such a study and a main contribution of the article is a unifying survey of the research field. We identify the most important retrieval tasks related to comments, namely filtering, ranking, and summarization. Within these tasks, we distinguish two paradigms according to which comments are utilized and which we designate as commenttargeting and comment-exploiting. Within the first paradigm, the comments themselves form the retrieval targets. Within the second paradigm, the commented items form the retrieval targets (i.e., comments are used as an additional information source to improve the retrieval performance for the commented items). We report on four case studies to demonstrate the exploration of the commentsphere under information retrieval aspects: comment filtering, comment ranking, comment summarization and cross-media retrieval. The first three studies deal primarily with comment-targeting retrieval, while the last one deals with commentexploiting retrieval. Throughout the article, connections to information retrieval research are pointed out. © 2012 ACM.",Comment-based retrieval; Commentsphere; Survey; Web comments,Research; Surveys; Comment-based retrieval; Commentsphere; Cross-media retrieval; Information retrieval research; Information sources; Research fields; Retrieval performance; Web comments; Information retrieval
Introduction to the special section on computational models of collective intelligence in the social web,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867425786&doi=10.1145%2f2337542.2337543&partnerID=40&md5=67e40b60dd68b961597896d589a7af45,[No abstract available],,
Identify online store review spammers via social review graph,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867408740&doi=10.1145%2f2337542.2337546&partnerID=40&md5=75b50dde58e607608a8e23a2de382f5e,"Online shopping reviews provide valuable information for customers to compare the quality of products store services, and many other aspects of future purchases. However, spammers are joining this community trying to mislead consumers by writing fake or unfair reviews to confuse the consumers. Previous attempts have used reviewers' behaviors such as text similarity and rating patterns, to detect spammers. These studies are able to identify certain types of spammers, for instance, those who post many similar reviews about one target. However, in reality, there are other kinds of spammers who can manipulate their behaviors to act just like normal reviewers, and thus cannot be detected by the available techniques. In this article, we propose a novel concept of review graph to capture the relationships among all reviewers, reviews and stores that the reviewers have reviewed as a heterogeneous graph. We explore how interactions between nodes in this graph could reveal the cause of spam and propose an iterative computation model to identify suspicious reviewers. In the review graph, we have three kinds of nodes, namely, reviewer, review, and store. We capture their relationships by introducing three fundamental concepts, the trustiness of reviewers, the honesty of reviews, and the reliability of stores, and identifying their interrelationships: a reviewer is more trustworthy if the person has written more honesty reviews; a store is more reliable if it has more positive reviews from trustworthy reviewers; and a review is more honest if many other honest reviews support it. This is the first time such intricate relationships have been identified for spam detection and captured in a graph model. We further develop an effective computation method based on the proposed graph model. Different from any existing approaches, we do not use an review text information. Our model is thus complementary to existing approaches and able to find more difficult and subtle spamming activities, which are agreed upon by human judges after they evaluate our results. © 2012 ACM.",Review graph; Spammer detection,Electronic commerce; Internet; Iterative methods; Computation methods; Fundamental concepts; Graph model; Heterogeneous graph; Iterative computation; Novel concept; Online shopping; Online store; Quality of product; Rating pattern; Spam detection; Spammers; Text information; Text similarity; Graph theory
Nowcasting events from the social web with statistical learning,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867410703&doi=10.1145%2f2337542.2337557&partnerID=40&md5=65d003d0304846fad62689b847846f33,"We present a general methodology for inferring the occurrence and magnitude of an event or phenomenon by exploring the rich amount of unstructured textual information on the social part of the Web. Having geotagged user posts on the microblogging service of Twitter as our input data, we investigate two case studies. The first consists of a benchmark problem, where actual levels of rainfall in a given location and time are inferred from the content of tweets. The second one is a real-life task, where we infer regional Influenzalike Illness rates in the effort of detecting timely an emerging epidemic disease. Our analysis builds on a statistical learning framework, which performs sparse learning via the bootstrapped version of LASSO to select a consistent subset of textual features from a large amount of candidates. In both case studies, selected features indicate close semantic correlation with the target topics and inference, conducted by regression, has a significant performance, especially given the short length -approximately one year- of Twitter's data time series. © 2012 ACM.",Event detection; Feature selection; LASSO; Social network mining; Sparse learning; Twitter,Epidemiology; Feature extraction; Semantics; Event detection; LASSO; Social Networks; Sparse learning; Twitter; Social networking (online)
Evaluation of folksonomy induction algorithms,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864066772&doi=10.1145%2f2337542.2337559&partnerID=40&md5=061c7ce07fe46b5f8d42788ea08720f7,"Algorithms for constructing hierarchical structures from user-generated metadata have caught the interest of the academic community in recent years. In social tagging systems, the output of these algorithms is usually referred to as folksonomies (from folk-generated taxonomies). Evaluation of folksonomies and folksonomy induction algorithms is a challenging issue complicated by the lack of golden standards, lack of comprehensive methods and tools as well as a lack of research and empirical/simulation studies applying these methods. In this article, we report results from a broad comparative study of state-of-the-art folksonomy induction algorithms that we have applied and evaluated in the context of five social tagging systems. In addition to adopting semantic evaluation techniques, we present and adopt a new technique that can be used to evaluate the usefulness of folksonomies for navigation. Our work sheds new light on the properties and characteristics of state-of-the-art folksonomy induction algorithms and introduces a new pragmatic approach to folksonomy evaluation, while at the same time identifying some important limitations and challenges of folksonomy evaluation. Our results show that folksonomy induction algorithms specifically developed to capture intuitions of social tagging systems outperform traditional hierarchical clustering techniques. To the best of our knowledge, this work represents the largest and most comprehensive evaluation study of state-of-the-art folksonomy induction algorithms to date. © 2012 ACM.",Evaluation; Folksonomies; Social tagging systems; Taxonomies,Metadata; Semantic Web; Semantics; Social networking (online); Taxonomies; Academic community; Comparative studies; Comprehensive evaluation; Comprehensive method; Evaluation; Folksonomies; Hier-archical clustering; Hierarchical structures; Induction algorithms; Social tagging; Algorithms
On the relationship between novelty and popularity of user-generated content,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867425920&doi=10.1145%2f2337542.2337554&partnerID=40&md5=c3c18b8f983a79d183ba4fea2fbfc359,"This work deals with the task of predicting the popularity of user-generated content. We demonstrate how the novelty of newly published content plays an important role in affecting its popularity. More specifically, we study three dimensions of novelty. The first one, termed contemporaneous novelty, models the relative novelty embedded in a new post with respect to contemporary content that was generated by others. The second type of novelty, termed self novelty, models the relative novelty with respect to the user's own contribution history. The third type of novelty, termed discussion novelty, relates to the novelty of the comments associated by readers with respect to the post content. We demonstrate the contribution of the new novelty measures to estimating blog-post popularity by predicting the number of comments expected for a fresh post. We further demonstrate how novelty based measures can be utilized for predicting the citation volume of academic papers. © 2012 ACM 2157-6904/2012/09-ART69.",Novelty; Popularity; User-generated content,Academic paper; Novelty; Novelty measure; Popularity; Three dimensions; User-generated content; Forecasting
Latent geospatial semantics of social media,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867411579&doi=10.1145%2f2337542.2337549&partnerID=40&md5=26ef4e29e9da9ca1d4720cde01410661,"Multimodal understanding of shared content is an important success factor for many Web 2.0 applications and platforms. This article addresses the fundamental question of geo-spatial awareness in social media applications. In this context, we introduce an approach for improved characterization of social media by combining text features (e.g., tags as a prominent example of short, unstructured text labels) with spatial knowledge (e.g., geotags, coordinates of images, and videos). Our model-based framework GeoFolk combines these two aspects in order to construct better algorithms for content management, retrieval, and sharing. We demonstrate in systematic studies the benefits of this combination for a broad spectrum of scenarios related to social media: recommender systems, automatic content organization and filtering, and event detection. Furthermore, we establish a simple and technically sound model that can be seen as a reference baseline for future research in the field of geotagged social media. © 2012 ACM.",Geotagging; Social media; Tagging; Web2.0,Knowledge engineering; Web services; Automatic content; Broad spectrum; Content management; Event detection; Geo-spatial; Geo-tags; Geotagging; Multi-modal; Social media; Spatial knowledge; Success factors; Systematic study; Tagging; Text feature; Text labels; Web 2.0 applications; Web2.0; Semantics
Entity-relationship queries over wikipedia,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867419572&doi=10.1145%2f2337542.2337555&partnerID=40&md5=e8480e7118b9a8650488fd60514a9d93,"Wikipedia is the largest user-generated knowledge base. We propose a structured query mechanism, entityrelationship query, for searching entities in the Wikipedia corpus by their properties and interrelationships. An entity-relationship query consists of multiple predicates on desired entities. The semantics of each predicate is specified with keywords. Entity-relationship query searches entities directly over text instead of preextracted structured data stores. This characteristic brings two benefits: (1) Query semantics can be intuitively expressed by keywords; (2) It only requires rudimentary entity annotation, which is simpler than explicitly extracting and reasoning about complex semantic information before query-time. We present a ranking framework for general entity-relationship queries and a position-based Bounded Cumulative Model (BCM) for accurate ranking of query answers. We also explore various weighting schemes for further improving the accuracy of BCM. We test our ideas on a 2008 version of Wikipedia using a collection of 45 queries pooled from INEX entity ranking track and our own crafted queries. Experiments show that the ranking and weighting schemes are both effective, particularly on multipredicate queries. © 2012 ACM.",Entity search and ranking; Structured entity query; Wikipedia,Data mining; Knowledge based systems; Semantics; Entity ranking; Entity search; Entity-relationship; Knowledge base; Multiple predicates; Query search; Query semantics; Semantic information; Structured data; Structured entity query; Structured queries; Weighting scheme; Wikipedia; Websites
Leveraging social bookmarks from partially tagged corpus for improved web page clustering,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867406810&doi=10.1145%2f2337542.2337552&partnerID=40&md5=cce1002c678009c91bedf7e9c3a5f21c,"Automatic clustering of Web pages helps a number of information retrieval tasks, such as improving user interfaces, collection clustering, introducing diversity in search results, etc. Typically, Web page clustering algorithms use only features extracted from the page-text. However, the advent of social-bookmarking Web sites, such as StumbleUpon.com and Delicious.com, has led to a huge amount of user-generated content such as the social tag information that is associated with the Web pages. In this article, we present a subspace based feature extraction approach that leverages the social tag information to complement the page-contents of a Web page for extracting beter features, with the goal of improved clustering performance. In our approach, we consider page-text and tags as two separate views of the data, and learn a shared subspace that maximizes the correlation between the two views. Any clustering algorithm can then be applied in this subspace. We then present an extension that allows our approach to be applicable even if the Web page corpus is only partially tagged, that is, when the social tags are present for not all, but only for a small number of Web pages. We compare our subspace based approach with a number of baselines that use tag information in various other ways, and show that the subspace based approach leads to improved performance on theWeb page clustering task. We also discuss some possible future work including an active learning extension that can help in choosing which Web pages to get tags for, if we only can get the social tags for only a small number of Web pages. © 2012 ACM.",Canonical correlation analysis; Information retrieval; Kernel methods; Social bookmarking; Web page clustering,Clustering algorithms; Data mining; Feature extraction; Information retrieval; User interfaces; Active Learning; Automatic clustering; Canonical correlation analysis; Kernel methods; Possible futures; Search results; Social bookmarking; Subspace based; User-generated content; Web page clustering; Websites
Folksonomy-based term extraction for word cloud generation,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867411746&doi=10.1145%2f2337542.2337545&partnerID=40&md5=6e7a45734c2099e00698eb9c9e720ec0,"In this work we study the task of term extraction for word cloud generation in sparsely tagged domains, in which manual tags are scarce. We present a folksonomy-based term extraction method, called tag-boost, which boosts terms that are frequently used by the public to tag content. Our experiments with tag-boost based term extraction over different domains demonstrate tremendous improvement in word cloud quality, as reflected by the agreement between manual tags of the testing items and the cloud's terms extracted from the items' content. Moreover, our results demonstrate the high robustness of this approach, as compared to alternative cloud generation methods that exhibit a high sensitivity to data sparseness. Additionally, we show that tag-boost can be effectively applied even in nontagged domains, by using an external rich folksonomy borrowed from a well-tagged domain. © 2012 ACM.",Keyword extraction; Tag-boost; Tag-cloud generation,Data sparseness; Different domains; Folksonomies; Generation method; High robustness; High sensitivity; Keyword extraction; Tag-boost; Tag-cloud generation; Term extraction; Data mining
Ranking user influence in healthcare social media,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867408820&doi=10.1145%2f2337542.2337558&partnerID=40&md5=d21fa5bef0fc795c04e188fd217f3dc8,"Due to the revolutionary development of Web 2.0 technology, individual users have become major contributors of Web content in online social media. In light of the growing activities, how to measure a user's influence to other users in online social media becomes increasingly important. This research need is urgent especially in the online healthcare community since positive influence can be beneficial while negative influence may cause-negative impact on other users of the same community. In this article, a research framework was proposed to study user influence within the online healthcare community. We proposed a new approach to incorporate users' reply relationship, conversation content and response immediacy which capture both explicit and implicit interaction between users to identify influential users of online healthcare community. A weighted social network is developed to represent the influence between users. We tested our proposed techniques thoroughly on two medical support forums. Two algorithms UserRank and Weighted in-degree are benchmarked with PageRank and in-degree. Experiment results demonstrated the validity and effectiveness of our proposed approaches. © 2012 ACM.",Online healthcare community; Ranking algorithm; Social computing; Social media analytics; Social network; User influence; Web mining,Algorithms; Health care; Social sciences computing; World Wide Web; Online healthcare community; Ranking algorithm; Social computing; Social media; Social Networks; User influence; Web Mining; Social networking (online)
Using stochastic models to describe and predict social dynamics of web users,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867426679&doi=10.1145%2f2337542.2337547&partnerID=40&md5=d0b838501d1ccbff4f7cf75c0b82217a,"The popularity of content in social media is unequally distributed, with some items receiving a disproportionate share of attention from users. Predicting which newly-submitted items will become popular is critically important for both the hosts of social media content and its consumers. Accurate and timely prediction would enable hosts to maximize revenue through differential pricing for access to content or ad placement. Prediction would also give consumers an important tool for filtering the content. Predicting the popularity of content in social media is challenging due to the complex interactions between content quality and how the social media site highlights its content. Moreover, most social media sites selectively present content that has been highly rated by similar users, whose similarity is indicated implicitly by their behavior or explicitly by links in a social network. While these factors make it difficult to predict popularity a priori, stochastic models of user behavior on these sites can allow predicting popularity based on early user reactions to new content. By incorporating the various mechanisms through which web sites display content, such models improve on predictions that are based on simply extrapolating from the early votes. Specifically, for one such site, the news aggregator Digg, we show how a stochastic model distinguishes the effect of the increased visibility due to the network from how interested users are in the content. We find a wide range of interest, distinguishing stories primarily of interest to users in the network (""niche interests"") from those of more general interest to the user community. This distinction is useful for predicting a story's eventual popularity from users' early reactions to the story. © 2012 ACM.",Modeling; Prediction; Social dynamics; Social media; Social networks; Social news,Behavioral research; Dynamics; Economics; Models; Social networking (online); Stochastic models; Complex interaction; Content qualities; Differential pricing; News aggregators; Social dynamics; Social media; Social Networks; Social news; User behaviors; User communities; Web users; Forecasting
EachWiki: Facilitating wiki authoring by annotation suggestion,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867424060&doi=10.1145%2f2337542.2337556&partnerID=40&md5=7cafcc7cf5a8ef3e54b57dd3f9c74b7a,"Wikipedia, one of the best-known wikis and the world's largest free online encyclopedia, has embraced the power of collaborative editing to harness collective intelligence. However, using such a wiki to create high-quality articles is not as easy as people imagine, given for instance the difficulty of reusing knowledge already available in Wikipedia. As a result, the heavy burden of upbuilding and maintaining the evergrowing online encyclopedia still rests on a small group of people. In this article, we aim at facilitating wiki authoring by providing annotation recommendations, thus lightening the burden of both contributors and administrators. We leverage the collective wisdom of the users by exploiting Semantic Web technologies with Wikipedia data and adopt a unified algorithm to support link, category, and semantic relation recommendation. A prototype system named EachWiki is proposed and evaluated. The experimental results show that it has achieved considerable improvements in terms of effectiveness, efficiency and usability. The proposed approach can also be applied to other wiki-based collaborative editing systems. © 2012 ACM.",Category suggestion; Link suggestion; Semantic relation suggestion,Category suggestion; Collaborative Editing; Collaborative editing systems; Collective intelligences; High quality; Link suggestion; Online encyclopedia; Prototype system; Semantic relations; Semantic Web technology; Unified algorithm; Wikipedia; Websites
Latent community topic analysis: Integration of community discovery with topic modeling,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867415920&doi=10.1145%2f2337542.2337548&partnerID=40&md5=f82a5feb0f4786c6af21065be97142d0,"This article studies the problem of latent community topic analysis in text-associated graphs. With the development of social media, a lot of user-generated content is available with user networks. Along with rich information in networks, user graphs can be extended with text information associated with nodes. Topic modeling is a classic problem in text mining and it is interesting to discover the latent topics in text-associated graphs. Different from traditional topic modeling methods considering links, we incorporate community discovery into topic analysis in text-associated graphs to guarantee the topical coherence in the communities so that users in the same community are closely linked to each other and share common latent topics. We handle topic modeling and community discovery in the same framework. In our model we separate the concepts of community and topic, so one community can correspond to multiple topics and multiple communities can share the same topic. We compare different methods and perform extensive experiments on two real datasets. The results confirm our hypothesis that topics could help understand community structure, while community structure could help model topics. © 2012 ACM.",Community discovery; Topic modeling,Data mining; Graphic methods; Social sciences; Community discovery; Community structures; HELP model; Real data sets; Social media; Text information; Text mining; Topic analysis; User networks; User-generated content; Websites
Surface sulfur detection via remote sensing and onboard classification,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867411048&doi=10.1145%2f2337542.2337562&partnerID=40&md5=de516815324882ed31d513f9cde731a1,"Orbital remote sensing provides a powerful way to efficiently survey targets such as the Earth and other planets and moons for features of interest. One such feature of astrobiological relevance is the presence of surface sulfur deposits. These deposits have been observed to be associated with microbial activity at the Borup Fiord glacial springs in Canada, a location that may provide an analogue to other icy environments such as Europa. This article evaluates automated classifiers for detecting sulfur in remote sensing observations by the hyperion spectrometer on the EO-1 spacecraft. We determined that a data-driven machine learning solution was needed because the sulfur could not be detected by simply matching observations to sulfur lab spectra. We also evaluated several methods (manual and automated) for identifying the most relevant attributes (spectral wavelengths) needed for successful sulfur detection. Our findings include (1) the Borup Fiord sulfur deposits were best modeled as containing two sub-populations: sulfur on ice and sulfur on rock; (2) as expected, classifiers using Gaussian kernels outperformed those based on linear kernels, and should be adopted when onboard computational constraints permit; and (3) Recursive Feature Elimination selected sensible and effective features for use in the computationally constrained environment onboard EO-1. This study helped guide the selection of algorithm parameters and configuration for the classification system currently operational on EO-1. Finally, we discuss implications for a similar onboard classification system for a future Europa orbiter. © 2012 ACM.",Feature selection; Remote sensing; Support vector machines,Deposits; Feature extraction; Remote sensing; Sulfur deposits; Support vector machines; Algorithm parameters; Automated classifiers; Classification system; Computational constraints; Europa Orbiter; Gaussian kernels; Hyperion; Linear kernel; Microbial activities; Recursive feature elimination; Sub-populations; Sulfur detection; Sulfur
An ensemble architecture for learning complex problem-solving techniques from demonstration,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864766711&doi=10.1145%2f2337542.2337560&partnerID=40&md5=1f8a90d1b8cdd14c0bf9561a08d92753,"We present a novel ensemble architecture for learning problem-solving techniques from a very small number of expert solutions and demonstrate its effectiveness in a complex real-world domain. The key feature of our ""Generalized Integrated Learning Architecture"" (GILA) is a set of heterogeneous independent learning and reasoning (ILR) components, coordinated by a central meta-reasoning executive (MRE). The ILRs are weakly coupled in the sense that all coordination during learning and performance happens through the MRE. Each ILR learns independently from a small number of expert demonstrations of a complex task. During performance, each ILR proposes partial solutions to subproblems posed by the MRE, which are then selected from and pieced together by the MRE to produce a complete solution. The heterogeneity of the learner-reasoners allows both learning and problem solving to be more effective because their abilities and biases are complementary and synergistic. We describe the application of this novel learning and problem solving architecture to the domain of airspace management, where multiple requests for the use of airspaces need to be deconflicted, reconciled, and managed automatically. Formal evaluations show that our system performs as well as or better than humans after learning from the same training data. Furthermore, GILA outperforms any individual ILR run in isolation, thus demonstrating the power of the ensemble architecture for learning and problem solving. © 2012 ACM.",Complex problemsolving; Ensemble architecture; Learning from demonstration,Airspace management; Complete solutions; Complex task; Integrated learning; Key feature; Learning from demonstration; Metareasoning; Real world domain; Sub-problems; Training data
Learning causal relations in multivariate time series data,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867423782&doi=10.1145%2f2337542.2337561&partnerID=40&md5=1d6b0abf57cae46884dedbcd3b8ee169,"Many applications naturally involve time series data and the vector autoregression (VAR), and the structural VAR (SVAR) are dominant tools to investigate relations between variables in time series. In the first part of this work, we show that the SVAR method is incapable of identifying contemporaneous causal relations for Gaussian process. In addition, least squares estimators become unreliable when the scales of the problems are large and observations are limited. In the remaining part, we propose an approach to apply Bayesian network learning algorithms to identify SVARs from time series data in order to capture both temporal and contemporaneous causal relations, and avoid high-order statistical tests. The difficulty of applying Bayesian network learning algorithms to time series is that the sizes of the networks corresponding to time series tend to be large, and high-order statistical tests are required by Bayesian network learning algorithms in this case. To overcome the difficulty, we show that the search space of conditioning sets d-separating two vertices should be a subset of the Markov blankets. Based on this fact, we propose an algorithm enabling us to learn Bayesian networks locally, and make the largest order of statistical tests independent of the scales of the problems. Empirical results show that our algorithm outperforms existing methods in terms of both efficiency and accuracy. © 2012 ACM.",Bayesian networks; Causal modeling; Graphical models; VAR,Learning algorithms; Regression analysis; Statistical tests; Time series; Value engineering; Bayesian network learning; Causal modeling; Causal relations; Gaussian Processes; GraphicaL model; High-order; Least-squares estimator; Markov Blankets; Multivariate time series; Search spaces; Time-series data; VAR; Vector autoregressions; Bayesian networks
Machine recognition of music emotion: A review,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864517995&doi=10.1145%2f2168752.2168754&partnerID=40&md5=5a37decb0a332be69878e6cca4c500d0,"The proliferation of MP3 players and the exploding amount of digital music content call for novel ways of music organization and retrieval to meet the ever-increasing demand for easy and effective information access. As almost every music piece is created to convey emotion, music organization and retrieval by emotion is a reasonable way of accessing music information. A good deal of effort has been made in the music information retrieval community to train a machine to automatically recognize the emotion of a music signal. A central issue of machine recognition of music emotion is the conceptualization of emotion and the associated emotion taxonomy. Different viewpoints on this issue have led to the proposal of different ways of emotion annotation, model training, and result visualization. This article provides a comprehensive review of the methods that have been proposed for music emotion recognition. Moreover, as music emotion recognition is still in its infancy, there are many open issues. We review the solutions that have been proposed to address these issues and conclude with suggestions for further research. © 2012 ACM 2157-6904/2012/05-ART40 $10.00.",Music emotion recognition,Visualization; Digital music; Emotion recognition; Information access; Machine recognition; Model training; MP-3 players; Music information; Music information retrieval; Music signals; Information retrieval
Introduction to the Special Section on Distance Metric Learning in Intelligent Systems,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024274176&doi=10.1145%2f2168752.2168766&partnerID=40&md5=9de9ac2e5b7324a5207ac6bf07e39869,[No abstract available],,
Introduction to the special section on intelligent multimedia systems and technology part II,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863617830&doi=10.1145%2f2168752.2168753&partnerID=40&md5=1f21fe4f5f67b401ef47bea55c3e2332,[No abstract available],,
Dynamic landmarking for surface feature identification and change detection,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863621374&doi=10.1145%2f2168752.2168763&partnerID=40&md5=ae10b72b8f31826130bc31571d6bd9d3,"Given the large volume of images being sent back from remote spacecraft, there is a need for automated analysis techniques that can quickly identify interesting features in those images. Feature identification in individual images and automated change detection in multiple images of the same target are valuable for scientific studies and can inform subsequent target selection. We introduce a new approach to orbital image analysis called dynamic landmarking. It focuses on the identification and comparison of visually salient features in images. We have evaluated this approach on images collected by five Mars orbiters. These evaluations were motivated by three scientific goals: to study fresh impact craters, dust devil tracks, and dark slope streaks on Mars. In the process we also detected a different kind of surface change that may indicate seasonally exposed bedforms. These experiences also point the way to how this approach could be used in an onboard setting to analyze and prioritize data as it is collected. © 2012 ACM 2157-6904/2012/05-ART49 $10.00.",Change detection; Image analysis; Salience,Image analysis; Automated analysis; Bedforms; Change detection; Dust devils; Feature identification; Impact craters; Landmarking; Multiple image; Remote spacecraft; Salience; Salient features; Scientific studies; Surface changes; Surface feature; Target selection; Signal detection
Transfer metric learning with semi-supervised extension,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863633782&doi=10.1145%2f2168752.2168768&partnerID=40&md5=183971aa7ddc5afbdf43a76233114115,"Distance metric learning plays a very crucial role in many data mining algorithms because the performance of an algorithm relies heavily on choosing a good metric. However, the labeled data available in many applications is scarce, and hence the metrics learned are often unsatisfactory. In this article, we consider a transfer-learning setting in which some related source tasks with labeled data are available to help the learning of the target task. We first propose a convex formulation for multitask metric learning by modeling the task relationships in the form of a task covariance matrix. Then we regard transfer learning as a special case of multitask learning and adapt the formulation of multitask metric learning to the transfer-learning setting for our method, called transfer metric learning (TML). In TML, we learn the metric and the task covariances between the source tasks and the target task under a unified convex formulation. To solve the convex optimization problem, we use an alternating method in which each subproblem has an efficient solution. Moreover, in many applications, some unlabeled data is also available in the target task, and so we propose a semi-supervised extension of TML called STML to further improve the generalization performance by exploiting the unlabeled data based on the manifold assumption. Experimental results on some commonly used transfer-learning applications demonstrate the effectiveness of our method. © 2012 ACM 2157-6904/2012/05-ART54 $10.00.",Metric learning; Multitask learning; Semi-supervised learning; Transfer learning,Algorithms; Convex optimization; Covariance matrix; Supervised learning; Alternating method; Convex optimization problems; Data mining algorithm; Distance Metric Learning; Generalization performance; Labeled data; Metric learning; Multitask learning; Semi-supervised; Semi-supervised learning; Transfer learning; Unlabeled data; Data mining
AEGIS automated science targeting for the MER Opportunity rover,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863631241&doi=10.1145%2f2168752.2168764&partnerID=40&md5=f9de896e687d49432e2e9cdbf7353d30,"The Autonomous Exploration for Gathering Increased Science (AEGIS) system enables automated data collection by planetary rovers. AEGIS software was uploaded to the Mars Exploration Rover (MER) mission's Opportunity rover in December 2009 and has successfully demonstrated automated onboard targeting based on scientist-specified objectives. Prior to AEGIS, images were transmitted from the rover to the operations team on Earth; scientists manually analyzed the images, selected geological targets for the rover's remotesensing instruments, and then generated a command sequence to execute the new measurements. AEGIS represents a significant paradigm shift-by using onboard data analysis techniques, the AEGIS software uses scientist input to select high-quality science targets with no human in the loop. This approach allows the rover to autonomously select and sequence targeted observations in an opportunistic fashion, which is particularly applicable for narrow field-of-view instruments (such as the MER Mini-TES spectrometer, the MER Panoramic camera, and the 2011 Mars Science Laboratory (MSL) ChemCam spectrometer). This article provides an overview of the AEGIS automated targeting capability and describes how it is currently being used onboard the MER mission Opportunity rover. © 2012 ACM 2157-6904/2012/05-ART50 $10.00.",Autonomous science; Data analysis; Spacecraft autonomy,Data reduction; Martian surface analysis; Planetary landers; Spectrometers; Spectrometry; Automated data collection; Autonomous exploration; Autonomous science; ChemCam; Command sequences; Field of views; High quality; Human-in-the-loop; Mars exploration rover missions; Mars science laboratory; Mini-TES; Onboard data analysis; Panoramic cameras; Planetary rovers; Software use; Spacecraft autonomy; Targeting capability; Automation
Multiview metric learning with Global consistency and Local smoothness,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863632577&doi=10.1145%2f2168752.2168767&partnerID=40&md5=28eae67597f818aefe37b780a026a630,"In many real-world applications, the same object may have different observations (or descriptions) from multiview observation spaces, which are highly related but sometimes look different from each other. Conventional metric-learning methods achieve satisfactory performance on distance metric computation of data in a single-view observation space, but fail to handle well data sampled from multiview observation spaces, especially those with highly nonlinear structure. To tackle this problem, we propose a new method called Multiview Metric Learning with Global consistency and Local smoothness (MVML-GL) under a semisupervised learning setting, which jointly considers global consistency and local smoothness. The basic idea is to reveal the shared latent feature space of the multiview observations by embodying global consistency constraints and preserving local geometric structures. Specifically, this framework is composed of two main steps. In the first step, we seek a global consistent shared latent feature space, which not only preserves the local geometric structure in each space but also makes those labeled corresponding instances as close as possible. In the second step, the explicit mapping functions between the input spaces and the shared latent space are learned via regularized locally linear regression. Furthermore, these two steps both can be solved by convex optimizations in closed form. Experimental results with application to manifold alignment on real-world datasets of pose and facial expression demonstrate the effectiveness of the proposed method. © 2012 ACM 2157-6904/2012/05-ART53 $10.00.",Global consistency; Local smoothness; Metric Learning; Multiview learning,Convex optimization; Face recognition; Closed form; Distance metrics; Facial Expressions; Feature space; Geometric structure; Global consistency; Highly nonlinear; Input space; Local smoothness; Mapping functions; Metric learning; Multi-view learning; Multi-views; Observation space; Real-world application; Real-world datasets; Semi-supervised learning; Well data; Geometry
Context-aware semi-local feature detector,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863631474&doi=10.1145%2f2168752.2168758&partnerID=40&md5=f4fafc19f9aebaf7aa04547aa5251439,"How can interest point detectors benefit from contextual cues? In this articles, we introduce a context-aware semi-local detector (CASL) framework to give a systematic answer with three contributions: (1) We integrate the context of interest points to recurrently refine their detections. (2) This integration boosts interest point detectors from the traditionally local scale to a semi-local scale to discover more discriminative salient regions. (3) Such context-aware structure further enables us to bring forward category learning (usually in the subsequent recognition phase) into interest point detection to locate category-aware, meaningful salient regions. Our CASL detector consists of two phases. The first phase accumulates multiscale spatial correlations of local features into a difference of contextual Gaussians (DoCG) field. DoCG quantizes detector context to highlight contextually salient regions at a semi-local scale, which also reveals visual attentions to a certain extent. The second phase locates contextual peaks by mean shift search over the DoCG field, which subsequently integrates contextual cues into feature description. This phase enables us to integrate category learning into mean shift search kernels. This learning-based CASL mechanism produces more categoryaware features, which substantially benefits the subsequent visual categorization process. We conducted experiments in image search, object characterization, and feature detector repeatability evaluations, which reported superior discriminability and comparable repeatability to state-of-the-art works. © 2012 ACM 2157-6904/2012/05-ART44 $10.00.",Context-aware feature; Contextual gaussian field; Image analysis; Internet; Knowledge representation; Learning-based feature extraction; Mean shift; Multimedia systems; Semi-local feature; Supervised kernel learning,Feature extraction; Image analysis; Internet; Knowledge representation; Multimedia systems; Context-Aware; Gaussian field; Kernel learning; Mean shift; Semi-local feature; Detectors
Video human motion recognition using a knowledge-based hybrid method based on a hidden Markov model,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863632564&doi=10.1145%2f2168752.2168756&partnerID=40&md5=3b65a3dd253cc16f4d7471d491abe361,"Human motion recognition in video data has several interesting applications in fields such as gaming, senior/assisted-living environments, and surveillance. In these scenarios, we may have to consider adding new motion classes (i.e., new types of human motions to be recognized), as well as new training data (e.g., for handling different type of subjects). Hence, both the accuracy of classification and training time for the machine learning algorithms become important performance parameters in these cases. In this article, we propose a knowledge-based hybrid (KBH) method that can compute the probabilities for hidden Markov models (HMMs) associated with different human motion classes. This computation is facilitated by appropriately mixing features from two different media types (3D motion capture and 2D video). We conducted a variety of experiments comparing the proposed KBH for HMMs and the traditional Baum-Welch algorithms. With the advantage of computing the HMM parameter in a noniterative manner, the KBH method outperforms the Baum-Welch algorithm both in terms of accuracy as well as in reduced training time. Moreover, we show in additional experiments that the KBH method also outperforms the linear support vector machine (SVM). © 2012 ACM 2157-6904/2012/05-ART42 $10.00.",3D motion capture; Hidden Markov model; Human-computer interaction; Video human motion recognition,Experiments; Hidden Markov models; Knowledge based systems; Learning algorithms; Motion estimation; Support vector machines; 2D video; 3D motion capture; Baum-Welch algorithms; Hidden markov models (HMMs); Human motion recognition; Human motions; Hybrid method; In-field; Linear Support Vector Machines; Media types; Non-iterative; Performance parameters; Training data; Training time; Video data; Three dimensional
Mining travel patterns from geotagged photos,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863632961&doi=10.1145%2f2168752.2168770&partnerID=40&md5=fc5021f3aab90e663e67b1ec5fde6e1a,"Recently, the phenomenal advent of photo-sharing services, such as Flickr and Panoramio, have led to volumous community-contributed photos with text tags, timestamps, and geographic references on the Internet. The photos, together with their time- and geo-references, become the digital footprints of photo takers and implicitly document their spatiotemporal movements. This study aims to leverage the wealth of these enriched online photos to analyze people's travel patterns at the local level of a tour destination. Specifically, we focus our analysis on two aspects: (1) tourist movement patterns in relation to the regions of attractions (RoA), and (2) topological characteristics of travel routes by different tourists. To do so, we first build a statistically reliable database of travel paths from a noisy pool of community-contributed geotagged photos on the Internet. We then investigate the tourist traffic flow among different RoAs by exploiting the Markov chain model. Finally, the topological characteristics of travel routes are analyzed by performing a sequence clustering on tour routes. Testings on four major cities demonstrate promising results of the proposed system. © 2012 ACM 2157-6904/2012/05-ART56 $10.00.",Geotagged photos; Travel pattern mining,Topology; Geotagged photos; Major cities; Markov chain models; Movement pattern; Time stamps; Traffic flow; Travel patterns; Travel routes; Internet
Introduction to the special section on artificial intelligence in space,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863628592&doi=10.1145%2f2168752.2168762&partnerID=40&md5=b9a938e2a1b5fcf4899dfa5958e8ed39,[No abstract available],,
Using clustering and metric learning to improve science return of remote sensed imagery,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863630618&doi=10.1145%2f2168752.2168765&partnerID=40&md5=2a025bef9217f56b552174f9c69cf6e6,"Current and proposed remote space missions, such as the proposed aerial exploration of Titan by an aerobot, often can collect more data than can be communicated back to Earth. Autonomous selective downlink algorithms can choose informative subsets of data to improve the science value of these bandwidth-limited transmissions. This requires statistical descriptors of the data that reflect very abstract and subtle distinctions in science content. We propose a metric learning strategy that teaches algorithms how best to cluster new data based on training examples supplied by domain scientists. We demonstrate that clustering informed by metric learning produces results that more closely match multiple scientists' labelings of aerial data than do clusterings based on random or periodic sampling. A new metric-learning strategy accommodates training sets produced by multiple scientists with different and potentially inconsistent mission objectives. Our methods are fit for current spacecraft processors (e.g., RAD750) and would further benefit from more advanced spacecraft processor architectures, such as OPERA. © 2012 ACM 2157-6904/2012/05-ART51 $10.00. © 2012 Association for Computing Machinery.",Clustering; Information retrieval; Onboard data analysis; Selective data return; Space exploration,Algorithms; Information retrieval; Interplanetary flight; Remote sensing; Space research; Spacecraft; Aerobots; Clustering; Clusterings; Labelings; Metric learning; Mission objectives; Onboard data analysis; Periodic sampling; Processor architectures; Remote sensed imagery; Selective data return; Space explorations; Space missions; Statistical descriptors; Training example; Training sets; Computer architecture
Metric learning for estimating psychological similarities,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863620538&doi=10.1145%2f2168752.2168769&partnerID=40&md5=117eddc1cd8bbed11b6a4432d5b70b38,"An important problem in cognitive psychology is to quantify the perceived similarities between stimuli. Previous work attempted to address this problem with multidimensional scaling (MDS) and its variants. However, there are several shortcomings of the MDS approaches. We propose Yada, a novel general metric-learning procedure based on two-alternative forced-choice behavioral experiments. Our method learns forward and backward nonlinear mappings between an objective space in which the stimuli are defined by the standard feature vector representation and a subjective space in which the distance between a pair of stimuli corresponds to their perceived similarity. We conduct experiments on both synthetic and real human behavioral datasets to assess the effectiveness of Yada. The results show that Yada outperforms several standard embedding and metric-learning algorithms, both in terms of likelihood and recovery error. © 2012 ACM 2157-6904/2012/05-ART55 $10.00.",Embedding; Metric learning; Subjective distance,Learning algorithms; Behavioral experiment; Cognitive psychology; Data sets; Embedding; Feature vectors; Metric learning; Multi-dimensional scaling; Nonlinear mappings; Objective space; Subjective distance; Behavioral research
Intelligent social media indexing and sharing using an adaptive indexing search engine,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863632991&doi=10.1145%2f2168752.2168761&partnerID=40&md5=e26d8af3885cbddb1bf2aef2234a6269,"Effective sharing of diverse social media is often inhibited by limitations in their search and discovery mechanisms, which are particularly restrictive for media that do not lend themselves to automatic processing or indexing. Here, we present the structure and mechanism of an adaptive search engine which is designed to overcome such limitations. The basic framework of the adaptive search engine is to capture human judgment in the course of normal usage from user queries in order to develop semantic indexes which link search terms to media objects semantics. This approach is particularly effective for the retrieval of multimedia objects, such as images, sounds, and videos, where a direct analysis of the object features does not allow them to be linked to search terms, for example, nontextual/icon-based search, deep semantic search, or when search terms are unknown at the time the media repository is built. An adaptive search architecture is presented to enable the index to evolve with respect to user feedback, while a randomized query-processing technique guarantees avoiding local minima and allows the meaningful indexing of new media objects and new terms. The present adaptive search engine allows for the efficient community creation and updating of social media indexes, which is able to instill and propagate deep knowledge into social media concerning the advanced search and usage of media resources. Experiments with various relevance distribution settings have shown efficient convergence of such indexes, which enable intelligent search and sharing of social media resources that are otherwise hard to discover. © 2012 ACM 2157-6904/2012/05-ART47 $10.00.",Adaptive indexing; Evolutionary computation; Genetic algorithms; Multimedia semantics; Relevance feedback; Social media,Evolutionary algorithms; Feedback; Genetic algorithms; Indexing (of information); Search engines; Semantics; Adaptive search; Automatic processing; Deep knowledge; Direct analysis; Human judgments; Intelligent search; Local minimums; Media objects; Multimedia object; Multimedia semantics; New media; Relevance feedback; Search terms; Semantic search; Social media; User feedback; User query; Indexing (materials working)
Distinguishing facial features for ethnicity-based 3D face recognition,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863625895&doi=10.1145%2f2168752.2168759&partnerID=40&md5=e4485528f9a2dd604b26ac986983fd39,"Among different approaches for 3D face recognition, solutions based on local facial characteristics are very promising, mainly because they can manage facial expression variations by assigning different weights to different parts of the face. However, so far, a few works have investigated the individual relevance that local features play in 3D face recognition with very simple solutions applied in the practice. In this article, a local approach to 3D face recognition is combined with a feature selection model to study the relative relevance of different regions of the face for the purpose of discriminating between different subjects. The proposed solution is experimented using facial scans of the Face Recognition Grand Challenge dataset. Results of the experimentation are two-fold: they quantitatively demonstrate the assumption that different regions of the face have different relevance for face discrimination and also show that the relevance of facial regions changes for different ethnic groups. © 2012 ACM 2157-6904/2012/05-ART45 $10.00.",3D face recognition; Ethnicity-based learning; Feature selection; Iso-geodesic stripes,Feature extraction; Three dimensional; 3D face recognition; Data sets; Ethnic groups; Ethnicity-based learning; Face recognition grand challenges; Facial Expressions; Facial feature; Facial regions; Iso-geodesic stripes; Local approaches; Local feature; Face recognition
Robust visual tracking using an effective appearance model based on sparse coding,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863626473&doi=10.1145%2f2168752.2168757&partnerID=40&md5=0be4902abce183e4ebe82e44d90b8033,"Intelligent video surveillance is currently one of the most active research topics in computer vision, especially when facing the explosion of video data captured by a large number of surveillance cameras. As a key step of an intelligent surveillance system, robust visual tracking is very challenging for computer vision. However, it is a basic functionality of the human visual system (HVS). Psychophysical findings have shown that the receptive fields of simple cells in the visual cortex can be characterized as being spatially localized, oriented, and bandpass, and it forms a sparse, distributed representation of natural images. In this article, motivated by these findings, we propose an effective appearance model based on sparse coding and apply it in visual tracking. Specifically, we consider the responses of general basis functions extracted by independent component analysis on a large set of natural image patches as features and model the appearance of the tracked target as the probability distribution of these features. In order to make the tracker more robust to partial occlusion, camouflage environments, pose changes, and illumination changes, we further select features that are related to the target based on an entropy-gain criterion and ignore those that are not. The target is finally represented by the probability distribution of those related features. The target search is performed by minimizing the Matusita distance between the distributions of the target model and a candidate using Newton-style iterations. The experimental results validate that the proposed method is more robust and effective than three state-of-the-art methods. © 2012 ACM 2157-6904/2012/05-ART43 $10.00.",Appearance model; Intelligent visual surveillance; Sparse coding,Discrete cosine transforms; Independent component analysis; Probability distributions; Security systems; Appearance models; Band pass; Basis functions; Distributed representation; Human visual systems; Illumination changes; Intelligent surveillance systems; Intelligent video surveillance; Natural images; Partial occlusions; Psychophysical; Receptive fields; Research topics; Simple cell; Sparse coding; State-of-the-art methods; Surveillance cameras; Target model; Target search; Video data; Visual cortexes; Visual surveillance; Visual Tracking; Computer vision
Factorization machines with libFM,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863614151&doi=10.1145%2f2168752.2168771&partnerID=40&md5=da4799c14aee5d149925aa2efd4ac018,"Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented. Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM. © 2012 ACM 2157-6904/2012/05-ART57 $10.00.",Collaborative filtering; Factorization machine; Factorization model; Matrix factorization; Recommender system; Tensor factorization,Bayesian networks; Inference engines; Learning algorithms; Markov processes; Models; Recommender systems; Bayesian inference; Categorical variables; Collaborative filtering; Expert knowledge; Factorization approach; Factorization model; Generic approach; Large domain; Least Square; Matrix factorizations; Non-trivial tasks; Prediction problem; Software implementation; Stochastic gradient descent; Tensor factorization; Factorization
A generic approach for systematic analysis of sports videos,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863626808&doi=10.1145%2f2168752.2168760&partnerID=40&md5=ccc223f6a2400bd61661e50c8cda915f,"Various innovative and original works have been applied and proposed in the field of sports video analysis. However, individual works have focused on sophisticated methodologies with particular sport types and there has been a lack of scalable and holistic frameworks in this field. This article proposes a solution and presents a systematic and generic approach which is experimented on a relatively large-scale sports consortia. The system aims at the event detection scenario of an input video with an orderly sequential process. Initially, domain knowledge-independent local descriptors are extracted homogeneously from the input video sequence. Then the video representation is created by adopting a bag-of-visual-words (BoW) model. The video's genre is first identified by applying the k-nearest neighbor (k-NN) classifiers on the initially obtained video representation, and various dissimilarity measures are assessed and evaluated analytically. Subsequently, an unsupervised probabilistic latent semantic analysis (PLSA)-based approach is employed at the same histogram-based video representation, characterizing each frame of video sequence into one of four view groups, namely closed-up-view, mid-view, long-view, and outer-field-view. Finally, a hidden conditional random field (HCRF) structured prediction model is utilized for interesting event detection. From experimental results, k-NN classifier using KL-divergence measurement demonstrates the best accuracy at 82.16% for genre categorization. Supervised SVM and unsupervised PLSA have average classification accuracies at 82.86% and 68.13%, respectively. The HCRF model achieves 92.31% accuracy using the unsupervised PLSA based label input, which is comparable with the supervised SVM based input at an accuracy of 93.08%. In general, such a systematic approach can be widely applied in processing massive videos generically. © 2012 ACM 2157-6904/2012/05-ART46 $10.00.",,Image retrieval; Text processing; Video recording; Bag-of-visual-words; Classification accuracy; Dissimilarity measures; Event detection; Generic approach; Hidden conditional random fields; Input videos; K-nearest neighbors; k-NN classifier; KL-divergence; Local descriptors; Probabilistic latent semantic analysis; Sequential process; Sports video; Sports video analysis; Structured prediction; Systematic analysis; Video representations; Video sequences; SportS
Robust video content analysis via transductive learning,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863608680&doi=10.1145%2f2168752.2168755&partnerID=40&md5=b8a5bedb8547791895192d584368e7cd,"Reliable video content analysis is an essential prerequisite for effective video search. An important current research question is how to develop robust video content analysis methods that produce satisfactory results for a large variety of video sources, distribution platforms, genres, and content. The work presented in this article exploits the observation that the appearance of objects and events is often related to a particular video sequence, episode, program, or broadcast. This motivates our idea of considering the content analysis task for a single video or episode as a transductive setting: the final classification model must be optimal for the given video only, and not in general, as expected for inductive learning. For this purpose, the unlabeled video test data have to be used in the learning process. In this article, a transductive learning framework for robust video content analysis based on feature selection and ensemble classification is presented. In contrast to related transductive approaches for video analysis (e.g., for concept detection), the framework is designed in a general manner and not only for a single task. The proposed framework is applied to the following video analysis tasks: shot boundary detection, face recognition, semantic video retrieval, and semantic indexing of computer game sequences. Experimental results for diverse video analysis tasks and large test sets demonstrate that the proposed transductive framework improves the robustness of the underlying state-of-the-art approaches, whereas transductive support vector machines do not solve particular tasks in a satisfactory manner. © 2012 ACM 2157-6904/2012/05-ART41 $10.00.",Concept detection; Cut detection; Ensemble classification; Face recognition; Robust video content analysis; Robustness; Self-supervised learning; Transductive learning,Face recognition; Learning systems; Robustness (control systems); Semantics; Concept detection; Cut detection; Ensemble classification; Self-supervised learning; Transductive learning; Video-content analysis; Video recording
DClusterE: A framework for evaluating and understanding document clustering using visualization,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863262996&doi=10.1145%2f2089094.2089100&partnerID=40&md5=f1c95f1de183f20c264fd73274e60cfd,"Over the last decade, document clustering, as one of the key tasks in information organization and navigation, has been widely studied. Many algorithms have been developed for addressing various challenges in document clustering and for improving clustering performance. However, relatively few research efforts have been reported on evaluating and understanding document clustering results. In this article, we present DClusterE, a comprehensive and effective framework for document clustering evaluation and understanding using information visualization. DClusterE integrates cluster validation with user interactions and offers rich visualization tools for users to examine document clustering results from multiple perspectives. In particular, through informative views including force-directed layout view, matrix view, and cluster view, DClusterE provides not only different aspects of document inter/intra-clustering structures, but also the corresponding relationship between clustering results and the ground truth. Additionally, DClusterE supports general user interactions such as zoom in/out, browsing, and interactive access of the documents at different levels. Two new techniques are proposed to implement DClusterE: (1) A novel multiplicative update algorithm (MUA) for matrix reordering to generate narrow-banded (or clustered) nonzero patterns from documents. Combined with coarse seriation, MUA is able to provide better visualization of the cluster structures. (2) A Mallows-distance-based algorithm for establishing the relationship between the clustering results and the ground truth, which serves as the basis for coloring schemes. Experiments and user studies are conducted to demonstrate the effectiveness and efficiency of DClusterE. © 2012 ACM.",Clustering; Document analysis; Performance evaluation; Visualization,Cluster analysis; Flow visualization; Information retrieval; Information science; Information systems; Visualization; Cluster structure; Cluster validation; Clustering; Clustering results; Coloring schemes; Document analysis; Document Clustering; Force-directed layout; Ground truth; Information organization; Information visualization; Multiple perspectives; Multiplicative updates; Performance evaluation; Research efforts; User interaction; User study; Visualization tools; Clustering algorithms
Mining recurring concept drifts with limited labeled streaming data,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863283905&doi=10.1145%2f2089094.2089105&partnerID=40&md5=348605cfcc264ffe4f82887e6bdf4f80,"Tracking recurring concept drifts is a significant issue for machine learning and data mining that frequently appears in real-world stream classification problems. It is a challenge for many streaming classification algorithms to learn recurring concepts in a data stream environment with unlabeled data, and this challenge has received little attention from the research community. Motivated by this challenge, this article focuses on the problem of recurring contexts in streaming environments with limited labeled data. We propose a semi-supervised classification algorithm for data streams with REcurring concept Drifts and Limited LAbeled data, called REDLLA, in which a decision tree is adopted as the classification model. When growing a tree, a clustering algorithm based on k-means is installed to produce concept clusters and unlabeled data are labeled in the method of majority-class at leaves. In view of deviations between history and new concept clusters, potential concept drifts are distinguished and recurring concepts are maintained. Extensive studies on both synthetic and real-world data confirm the advantages of our REDLLA algorithm over three state-of-the-art online classification algorithms of CVFDT, DWCDS, and CDRDT and several known online semi-supervised algorithms, even in the case with more than 90% unlabeled data. © 2012 ACM.",Clustering; Concept drift; Data stream; Decision tree,Clustering algorithms; Data communication systems; Decision trees; Trees (mathematics); Classification algorithm; Classification models; Clustering; Concept drifts; Data stream; Decision trees (DTs); K-means; Labeled data; On-line classification; Real world data; Research communities; Semi-supervised algorithm; Semi-supervised classification; Stream classification; Streaming data; Unlabeled data; Data mining
Advertising keywords recommendation for short-text web pages using wikipedia,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863293446&doi=10.1145%2f2089094.2089112&partnerID=40&md5=2a3bc3e818aa31ea9505d7cb5a40e386,"Advertising keywords recommendation is an indispensable component for online advertising with the keywords selected from the target Web pages used for contextual advertising or sponsored search. Several ranking-based algorithms have been proposed for recommending advertising keywords. However, for most of them performance is still lacking, especially when dealing with short-text target Web pages, that is, those containing insufficient textual information for ranking. In some cases, short-text Web pages may not even contain enough keywords for selection. A natural alternative is then to recommend relevant keywords not present in the target Web pages. In this article, we propose a novel algorithm for advertising keywords recommendation for short-text Web pages by leveraging the contents of Wikipedia, a user-contributed online encyclopedia. Wikipedia contains numerous entities with related entities on a topic linked to each other. Given a target Web page, we propose to use a content-biased PageRank on the Wikipedia graph to rank the related entities. Furthermore, in order to recommend high-quality advertising keywords, we also add an advertisement-biased factor into our model. With these two biases, advertising keywords that are both relevant to a target Web page and valuable for advertising are recommended. In our experiments, several state-of-the-art approaches for keyword recommendation are compared. The experimental results demonstrate that our proposed approach produces substantial improvement in the precision of the top 20 recommended keywords on short-text Web pages over existing approaches. © 2012 ACM.",Advertising keywords recommendation; Contextual advertising; Topic-sensitive PageRank; Wikipedia,Algorithms; Marketing; Contextual advertisings; High quality; Novel algorithm; Online advertising; Online encyclopedia; PageRank; Sponsored searches; State-of-the-art approach; Textual information; Wikipedia; Websites
"Mining the ""voice of the customer"" for business prioritization",2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863273276&doi=10.1145%2f2089094.2089114&partnerID=40&md5=a2cee8207d0c811540d479d148c146bd,"To gain competitiveness and sustained growth in the 21st century, most businesses are on a mission to become more customer-centric. In order to succeed in this endeavor, it is crucial not only to synthesize and analyze the VOC (the VOice of the Customer) data (i.e., the feedbacks or requirements raised by customers), but also to quickly turn these data into actionable knowledge. Although there are many technologies being developed in this complex problem space, most existing approaches in analyzing customer requests are ad hoc, time-consuming, error-prone, people-based processes which hardly scale well as the quantity of customer information explodes. This often results in the slow response to customer requests. In this article, in order to mine VOC to extract useful knowledge for the best product or service quality, we develop a hybrid framework that integrates domain knowledge with data-driven approaches to analyze the semistructured customer requests. The framework consists of capturing functional features, discovering the overlap or correlation among the features, and identifying the evolving feature trend by using the knowledge transformation model. In addition, since understanding the relative importance of the individual customer request is very critical and has a direct impact on the effective prioritization in the development process, we develop a novel semantic enhanced link-based ranking (SELRank) algorithm for relatively rating/ranking both customer requests and products. The framework has been successfully applied on Xerox Office Group Feature Enhancement Requirements (XOG FER) datasets to analyze customer requests. © 2012 ACM.",Business prioritization; Ranking; Text mining; Voice of the customer,Competition; Customer satisfaction; Data mining; Semantics; Complex problems; Customer information; Customer-centric; Data sets; Data-driven approach; Development process; Direct impact; Domain knowledge; Error prones; Feature enhancement; Functional features; Hybrid framework; Individual customers; Knowledge transformation models; Link-based ranking; Prioritization; Ranking; Semi-structured; Service Quality; Text mining; Voice of the customer; Sales
Learning to infer the status of heavy-duty sensors for energy-efficient context-sensing,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863262427&doi=10.1145%2f2089094.2089111&partnerID=40&md5=1be10c74565b262c0ab962ebefdaaa0b,"With the prevalence of smart mobile devices with multiple sensors, the commercial application of intelligent context-aware services becomes more and more attractive. However, limited by the battery capacity, the energy efficiency of context-sensing is the bottleneck for the success of context-aware applications. Though several previous studies for energy-efficient context-sensing have been reported, none of themcan be applied to multiple types of high-energy-consuming sensors. Moreover, applying machine learning technologies to energy-efficient context-sensing is underexplored too. In this article, we propose to leverage machine learning technologies for improving the energy efficiency of multiple high-energy-consuming context sensors by trading off the sensing accuracy. To be specific, we try to infer the status of high-energy-consuming sensors according to the outputs of software-based sensors and the physical sensors that are necessary to work all the time for supporting the basic functions of mobile devices. If the inference indicates the highenergy- consuming sensor is in a stable status, we avoid the unnecessary invocation and instead use the latest invoked value as the estimation. The experimental results on real datasets show that the energy efficiency of GPS sensing and audio-level sensing are significantly improved by the proposed approach while the sensing accuracy is over 90%. © 2012 ACM.",Context-sensing; Energy efficiency; Machine learning,Energy efficiency; High energy physics; Information services; Learning systems; Mobile devices; Basic functions; Battery capacity; Commercial applications; Context aware applications; Context aware services; Context-sensing; Energy efficient; High energy; Machine learning technology; Machine-learning; Multiple sensors; Physical sensors; Real data sets; Sensing accuracy; Software-based; Sensors
Introduction to the special section on intelligent visual interfaces for text analysis,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863249824&doi=10.1145%2f2089094.2089095&partnerID=40&md5=cdd39f1705a347c69ef156b56518759e,[No abstract available],,
Batch mode active learning for networked data,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863237145&doi=10.1145%2f2089094.2089109&partnerID=40&md5=c8695c69a3e5556224b5b6363352c0d8,"We study a novel problem of batch mode active learning for networked data. In this problem, data instances are connected with links and their labels are correlated with each other, and the goal of batch mode active learning is to exploit the link-based dependencies and node-specific content information to actively select a batch of instances to query the user for learning an accurate model to label unknown instances in the network. We present three criteria (i.e., minimum redundancy, maximum uncertainty, and maximum impact) to quantify the informativeness of a set of instances, and formalize the batch mode active learning problem as selecting a set of instances by maximizing an objective function which combines both link and content information. As solving the objective function is NP-hard, we present an efficient algorithm to optimize the objective function with a bounded approximation rate. To scale to real large networks, we develop a parallel implementation of the algorithm. Experimental results on both synthetic datasets and real-world datasets demonstrate the effectiveness and efficiency of our approach. © 2012 ACM.",Batch mode active learning; Combine link and content; Network classification,Algorithms; Active Learning; Approximation rates; Batch modes; Content information; Efficient algorithm; Informativeness; Large networks; Network classification; Networked datum; NP-hard; Objective functions; Parallel implementations; Real-world datasets; Synthetic datasets; Content based retrieval
Introduction to the special section on the 2nd Asia Conference on Machine Learning (ACML 2010),2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858257720&doi=10.1145%2f2089094.2089103&partnerID=40&md5=7a4ccea9bd65dc5838b09809f56d6d78,[No abstract available],,
Adversarial geospatial abduction problems,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858242881&doi=10.1145%2f2089094.2089110&partnerID=40&md5=9f4e4335fd5b817543ef92a90913436f,"Geospatial Abduction Problems (GAPs) involve the inference of a set of locations that ""best explain"" a given set of locations of observations. For example, the observations might include locations where a serial killer committed murders or where insurgents carried out Improvised Explosive Device (IED) attacks. In both these cases, we would like to infer a set of locations that explain the observations, for example, the set of locations where the serial killer lives/works, and the set of locations where insurgents locate weapons caches. However, unlike all past work on abduction, there is a strong adversarial component to this; an adversary actively attempts to prevent us from discovering such locations. We formalize such abduction problems as a two-player game where both players (an ""agent"" and an ""adversary"") use a probabilistic model of their opponent (i.e., a mixed strategy). There is asymmetry as the adversary can choose both the locations of the observations and the locations of the explanation, while the agent (i.e., us) tries to discover these. In this article, we study the problem from the point of view of both players. We define reward functions axiomatically to capture the similarity between two sets of explanations (one corresponding to the locations chosen by the adversary, one guessed by the agent). Many different reward functions can satisfy our axioms. We then formalize the Optimal Adversary Strategy (OAS) problem and the Maximal Counter-Adversary strategy (MCA) and show that both are NP-hard, that their associated counting complexity problems are #P-hard, and that MCA has no fully polynomial approximation scheme unless P=NP. We show that approximation guarantees are possible for MCA when the reward function satisfies two simple properties (zero-starting and monotonicity) which many natural reward functions satisfy. We develop a mixed integer linear programming algorithm to solve OAS and two algorithms to (approximately) compute MCA; the algorithms yield different approximation guarantees and one algorithm assumes a monotonic reward function. Our experiments use real data about IED attacks over a 21-month period in Baghdad. We are able to show that both the MCA algorithms work well in practice; while MCA-GREEDY-MONO is both highly accurate and slightly faster than MCA-LS, MCA-LS (to our surprise) always completely and correctly maximized the expected benefit to the agent while running in an acceptable time period. © 2012 ACM.",Abduction; Spatial reasoning,Data storage equipment; Polynomial approximation; Abduction; Baghdad; Counting complexity; Geo-spatial; Improvised explosive devices; Mixed integer linear programming; Mixed strategy; Monotonicity; NP-hard; Probabilistic models; Reward function; Running-in; Spatial reasoning; Time-periods; Two-player games; Weapons caches; Approximation algorithms
Leveraging auxiliary data for learning to rank,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858182777&doi=10.1145%2f2089094.2089113&partnerID=40&md5=76e43d2457eebcf69afab24f6c6f3c7a,"In learning to rank, both the quality and quantity of the training data have significant impacts on the performance of the learned ranking functions. However, in many applications, there are usually not sufficient labeled training data for the construction of an accurate ranking model. It is therefore desirable to leverage existing training data from other tasks when learning the ranking function for a particular task, an important problem which we tackle in this article utilizing a boosting framework with transfer learning. In particular, we propose to adaptively learn transferable representations called super-features from the training data of both the target task and the auxiliary task. Those super-features and the coefficients for combining them are learned in an iterative stage-wise fashion. Unlike previous transfer learning methods, the super-features can be adaptively learned by weak learners from the data. Therefore, the proposed framework is sufficiently flexible to deal with complicated common structures among different learning tasks. We evaluate the performance of the proposed transfer learning method for two datasets from the Letor collection and one dataset collected from a commercial search engine, and we also compare our methods with several existing transfer learning methods. Our results demonstrate that the proposed method can enhance the ranking functions of the target tasks utilizing the training data from the auxiliary tasks. © 2012 ACM.",Convergence analysis; Experimental evaluation; Learning to rank; Relevance; Search engine; Super-features; Transfer learning,Information retrieval; Learning algorithms; Learning systems; Search engines; Convergence analysis; Experimental evaluation; Learning to rank; Relevance; Super-features; Transfer learning; Data processing
Visual abstraction and ordering in faceted browsing of text collections,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858221357&doi=10.1145%2f2089094.2089097&partnerID=40&md5=b072fbb5ac65a4e5f57ebf36725d7b0b,"Faceted navigation is a technique for the exploration and discovery of a collection of resources, which can be of various types including text documents. While being information-rich resources, documents are usually not treated as content-bearing items in faceted browsing interfaces, and yet the required clean metadata is not always available or matches users' interest. In addition, the existing linear listing paradigm for representing result items from the faceted filtering process makes it difficult for users to traverse or compare across facet values in different orders of importance to them. In this context, we report in this article a visual support toward faceted browsing of a collection of documents based on a set of entities of interest to users. Our proposed approach involves using a multi-dimensional visualization as an alternative to the linear listing of focus items. In this visualization, visual abstraction based on a combination of a conceptual structure and the structural equivalence of documents can be simultaneously used to deal with a large number of items. Furthermore, the approach also enables visual ordering based on the importance of facet values to support prioritized, cross-facet comparisons of focus items. A user study was conducted and the results suggest that interfaces using the proposed approach can support users better in exploratory tasks and were also well-liked by the participants of the study, with the hybrid interface combining the multidimensional visualization with the linear listing receiving the most favorable ratings. © 2012 ACM.",Faceted browsing; Text collections; Visual exploration,Abstracting; Metadata; Collection of documents; Conceptual structures; Different order; Faceted browsing; Filtering process; Hybrid interface; Multidimensional visualization; Structural equivalence; Text collection; Text document; User study; Users' interests; Visual exploration; Visualization
Ensembles of restricted Hoeffding trees,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858171410&doi=10.1145%2f2089094.2089106&partnerID=40&md5=cc78f6625b76d7627c875731e4b7c4ff,"The success of simple methods for classification shows that is is often not necessary to model complex attribute interactions to obtain good classification accuracy on practical problems. In this article, we propose to exploit this phenomenon in the data stream context by building an ensemble of Hoeffding trees that are each limited to a small subset of attributes. In this way, each tree is restricted to model interactions between attributes in its corresponding subset. Because it is not known a priori which attribute subsets are relevant for prediction, we build exhaustive ensembles that consider all possible attribute subsets of a given size. As the resulting Hoeffding trees are not all equally important, we weigh them in a suitable manner to obtain accurate classifications. This is done by combining the log-odds of their probability estimates using sigmoid perceptrons, with one perceptron per class. We propose a mechanism for setting the perceptrons' learning rate using the ADWIN change detection method for data streams, and also use ADWIN to reset ensemble members (i.e., Hoeffding trees) when they no longer perform well. Our experiments show that the resulting ensemble classifier outperforms bagging for data streams in terms of accuracy when both are used in conjunction with adaptive naive Bayes Hoeffding trees, at the expense of runtime and memory consumption. We also show that our stacking method can improve the performance of a bagged ensemble. © 2012 ACM.",Data streams; Decision trees; Ensemble methods,Communication; Cybernetics; Data Processing; Decision Theory; Experimentation; Forestry; Mathematical Models; Pattern Recognition; Set; Cybernetics; Data communication systems; Decision trees; Pattern recognition systems; Set theory; Attribute interactions; Change detection; Classification accuracy; Data stream; Decision trees (DTs); Ensemble classifiers; Ensemble members; Ensemble methods; Learning rates; Memory consumption; Model complexes; Model interaction; Naive bayes; Perceptron; Practical problems; Probability estimate; Runtimes; SIMPLE method; Stacking method; Forestry
Conceptual imitation learning in a human-robot interaction paradigm,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858181867&doi=10.1145%2f2089094.2089104&partnerID=40&md5=d70bd816427de6873b2d3ab58456b2b3,"In general, imitation is imprecisely used to address different levels of social learning from high-level knowledge transfer to low-level regeneration of motor commands. However, true imitation is based on abstraction and conceptualization. This article presents a model for conceptual imitation through interaction with the teacher to abstract spatio-temporal demonstrations based on their functional meaning. Abstraction, concept acquisition, and self-organization of proto-symbols are performed through an incremental and gradual learning algorithm. In this algorithm, Hidden Markov Models (HMMs) are used to abstract perceptually similar demonstrations. However, abstract (relational) concepts emerge as a collection of HMMs irregularly scattered in the perceptual space but showing the same functionality. Performance of the proposed algorithm is evaluated in two experimental scenarios. The first one is a human-robot interaction task of imitating signs produced by hand movements. The second one is a simulated interactive task of imitating whole body motion patterns of a humanoid model. Experimental results show efficiency of our model for concept extraction, proto-symbol emergence, motion pattern recognition, prediction, and generation. © 2012 ACM.",Concept learning; Hidden Markov model; Human-robot interaction; Imitation,Abstracting; Computer simulation; Hidden Markov models; Human computer interaction; Knowledge management; Learning algorithms; Man machine systems; Pattern recognition; Time and motion study; Concept extraction; Concept learning; Gradual learning algorithms; Hand movement; Hidden markov models (HMMs); High level knowledge; Humanoid model; Imitation; Imitation learning; Motor commands; Social learning; Spatio-temporal; Whole-body motion; Human robot interaction
"TIARA: Interactive, topic-based visual text summarization and analysis",2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863233384&doi=10.1145%2f2089094.2089101&partnerID=40&md5=2031e1d6ef45723c29aa3397cce92917,"We are building an interactive visual text analysis tool that aids users in analyzing large collections of text. Unlike existing work in visual text analytics, which focuses either on developing sophisticated text analytic techniques or inventing novel text visualization metaphors, ours tightly integrates state-of-the-art text analytics with interactive visualization to maximize the value of both. In this article, we present our work from two aspects. We first introduce an enhanced, LDA-based topic analysis technique that automatically derives a set of topics to summarize a collection of documents and their content evolution over time. To help users understand the complex summarization results produced by our topic analysis technique, we then present the design and development of a time-based visualization of the results. Furthermore, we provide users with a set of rich interaction tools that help them further interpret the visualized results in context and examine the text collection from multiple perspectives. As a result, our work offers three unique contributions. First, we present an enhanced topic modeling technique to provide users with a time-sensitive and more meaningful text summary. Second, we develop an effective visual metaphor to transform abstract and often complex text summarization results into a comprehensible visual representation. Third, we offer users flexible visual interaction tools as alternatives to compensate for the deficiencies of current text summarization techniques. We have applied our work to a number of text corpora and our evaluation shows promise, especially in support of complex text analyses. © 2012 ACM.",Interactive text visualization; Stacked graph; Text analytics; Text summarization; Text trend chart; Topic model,Visualization; Interactive texts; Stacked graph; Text analytics; Text summarization; Text trend chart; Topic model; Text processing
Feature-based visual sentiment analysis of text document streams,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858212695&doi=10.1145%2f2089094.2089102&partnerID=40&md5=a1d43a6ed04b78a676196e9c52f59b3e,"This article describes automatic methods and interactive visualizations that are tightly coupled with the goal to enable users to detect interesting portions of text document streams. In this scenario the interestingness is derived from the sentiment, temporal density, and context coherence that comments about features for different targets (e.g., persons, institutions, product attributes, topics, etc.) have. Contributions are made at different stages of the visual analytics pipeline, including novel ways to visualize salient temporal accumulations for further exploration. Moreover, based on the visualization, an automatic algorithm aims to detect and preselect interesting time interval patterns for different features in order to guide analysts. The main target group for the suggested methods are business analysts who want to explore time-stamped customer feedback to detect critical issues. Finally, application case studies on two different datasets and scenarios are conducted and an extensive evaluation is provided for the presented intelligent visual interface for feature-based sentiment exploration over time. © 2012 ACM.",Document time series; Sentiment analysis; Text mining; Visual analytics,Data mining; Text processing; Automatic algorithms; Automatic method; Business analysts; Critical issues; Customer feedback; Data sets; Feature-based; Interactive visualizations; Interestingness; Product attributes; Sentiment analysis; Target group; Temporal density; Text document; Text mining; Tightly-coupled; Time-interval patterns; Visual analytics; Visual Interface; Visualization
A fuzzy logic system for bargaining in information markets,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858177823&doi=10.1145%2f2089094.2089108&partnerID=40&md5=737c82745c4b4120cff5616ec6bc0a6a,Future Web business models involve virtual environments where entities interact in order to sell or buy information goods. Such environments are known as Information Markets (IMs). Intelligent agents are used in IMs for representing buyers or information providers (sellers). We focus on the decisions taken by the buyer in the purchase negotiation process with sellers. We propose a reasoning mechanism on the offers (prices of information goods) issued by sellers based on fuzzy logic. The buyer's knowledge on the negotiation process is modeled through fuzzy sets. We propose a fuzzy inference engine dealing with the decisions that the buyer takes on each stage of the negotiation process. The outcome of the proposed reasoning method indicates whether the buyer should accept or reject the sellers' offers. Our findings are very promising for the efficiency of automated transactions undertaken by intelligent agents. © 2012 ACM.,Fuzzy systems; Negotiation process,Artificial intelligence; Fuzzy logic; Fuzzy sets; Fuzzy systems; Intelligent agents; Virtual reality; Business models; Fuzzy logic system; Information goods; Information market; Information provider; Negotiation process; Reasoning mechanism; Reasoning methods; Sales
PhC: Multiresolution visualization and exploration of text corpora with parallel hierarchical coordinates,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985034765&doi=10.1145%2f2089094.2089098&partnerID=40&md5=4c6ff75d2739a87ab7d8dddf042ca547,"The high-dimensional nature of the textual data complicates the design of visualization tools to support exploration of large document corpora. In this article, we first argue that the Parallel Coordinates (PC) technique, which can map multidimensional vectors onto a 2D space in such a way that elements with similar values are represented as similar poly-lines or curves in the visualization space, can be used to help users discern patterns in document collections. The inherent reduction in dimensionality during the mapping from multidimensional points to 2D lines, however, may result in visual complications. For instance, the lines that correspond to clusters of objects that are separate in the multidimensional space may overlap each other in the 2D space; the resulting increase in the number of crossings would make it hard to distinguish the individual document clusters. Such crossings of lines and overly dense regions are significant sources of visual clutter, thus avoiding them may help interpret the visualization. In this article, we note that visual clutter can be significantly reduced by adjusting the resolution of the individual term coordinates by clustering the corresponding values. Such reductions in the resolution of the individual term-coordinates, however, will lead to a certain degree of information loss and thus the appropriate resolution for the term-coordinates has to be selected carefully. Thus, in this article we propose a controlled clutter reduction approach, called Parallel hierarchical Coordinates (or PhC), for reducing the visual clutter in PC-based visualizations of text corpora. We define visual clutter and information loss measures and provide extensive evaluations that show that the proposed PhC provides significant visual gains (i.e., multiple orders of reductions in visual clutter) with small information loss during visualization and exploration of document collections. © 2012 ACM.",Clutter reduction; Document set visualization; Parallel coordinates,Clutter (information theory); Reduction; Vector spaces; Visualization; Clutter Reduction; Document set visualizations; Information loss measures; Large document corpora; Multi-dimensional space; Multi-dimensional vectors; Multiresolution visualization; Parallel coordinates; Data visualization
Topicnets: Visual analysis of large text corpora with topic modeling,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858270593&doi=10.1145%2f2089094.2089099&partnerID=40&md5=077ecff102ddaf80e6a1d836d857b2c4,"We present TopicNets, a Web-based system for visual and interactive analysis of large sets of documents using statistical topic models. A range of visualization types and control mechanisms to support knowledge discovery are presented. These include corpus- And document-specific views, iterative topic modeling, search, and visual filtering. Drill-down functionality is provided to allow analysts to visualize individual document sections and their relations within the global topic space. Analysts can search across a dataset through a set of expansion techniques on selected document and topic nodes. Furthermore, analysts can select relevant subsets of documents and perform real-time topic modeling on these subsets to interactively visualize topics at various levels of granularity, allowing for a better understanding of the documents. A discussion of the design and implementation choices for each visual analysis technique is presented. This is followed by a discussion of three diverse use cases in which TopicNets enables fast discovery of information that is otherwise hard to find. These include a corpus of 50,000 successful NSF grant proposals, 10,000 publications from a large research center, and single documents including a grant proposal and a PhD thesis. © 2012 ACM.",Graph visualization; Text visualization; Topic modeling,Software agents; Visualization; Control mechanism; Data sets; Drill-down; Graph visualization; Interactive analysis; PhD thesis; Research center; Text corpora; Text visualization; Topic model; Visual analysis; Web-based system; Information retrieval systems
A reliable people counting system via multiple cameras,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858214853&doi=10.1145%2f2089094.2089107&partnerID=40&md5=fff1c56118142c22e710da5b9828f3c6,"Reliable and real-time people counting is crucial in many applications. Most previous works can only count moving people from a single camera, which cannot count still people or can fail badly when there is a crowd (i.e., heavy occlusion occurs). In this article, we build a system for robust and fast people counting under occlusion through multiple cameras. To improve the reliability of human detection from a single camera, we use a dimensionality reduction method on the multilevel edge and texture features to handle the large variations in human appearance and poses. To accelerate the detection speed, we propose a novel two-stage cascade-of-rejectors method. To handle the heavy occlusion in crowded scenes, we present a fusion method with error tolerance to combine human detection from multiple cameras. To improve the speed and accuracy of moving people counting, we combine our multiview fusion detection method with particle tracking to count the number of people moving in/out the camera view (""border control""). Extensive experiments and analyses show that our method outperforms state-of-the-art techniques in single- and multicamera datasets for both speed and reliability. We also design a deployed system for fast and reliable people (still or moving) counting by using multiple cameras. © 2012 ACM.",Human detection; Multiple cameras; People counting; Video surveillance,Border control; Camera view; Data sets; Deployed systems; Detection methods; Dimensionality reduction method; Error tolerance; Fusion methods; Heavy occlusion; Human detection; Moving peoples; Multi-cameras; Multi-views; Multiple cameras; Number of peoples; Particle tracking; People counting; Single cameras; Texture features; Two stage; Video surveillance; Cameras
Introduction to the special section on search and mining user-generated content,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867420634&doi=10.1145%2f2337542.2337550&partnerID=40&md5=358d248b78ce1f9c81b1bc61464d8b77,"The primary goal of this special section of ACM Transactions on Intelligent Systems and Technology is to foster research in the interplay between Social Media, Data/Opinion Mining and Search, aiming to reflect the actual developments in technologies that exploit user-generated content. © 2012 ACM.",Data mining; Information retrieval; Opinion mining; Search; Social media; Text mining; User-generated contents,Information retrieval; Intelligent systems; Search engines; Sentiment analysis; Social networking (online); Search; Social media; Special sections; Text mining; User-generated content; Data mining
Watch the story unfold with textwheel: Visualization of large-scale news streams,2012,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863275728&doi=10.1145%2f2089094.2089096&partnerID=40&md5=967fa232db1e373e898009046fdd8650,"Keyword-based searching and clustering of news articles have been widely used for news analysis. However, news articles usually have other attributes such as source, author, date and time, length, and sentiment which should be taken into account. In addition, news articles and keywords have complicated macro/micro relations, which include relations between news articles (i.e., macro relation), relations between keywords (i.e., micro relation), and relations between news articles and keywords (i.e., macro-micro relation). These macro/micro relations are time varying and pose special challenges for news analysis. In this article we present a visual analytics system for news streams which can bring multiple attributes of the news articles and the macro/micro relations between news streams and keywords into one coherent analytical context, all the while conveying the dynamic natures of news streams. We introduce a new visualization primitive called TextWheel which consists of one or multiple keyword wheels, a document transportation belt, and a dynamic system which connects the wheels and belt. By observing the TextWheel and its content changes, some interesting patterns can be detected. We use our system to analyze several news corpora related to some major companies and the results demonstrate the high potential of our method. © 2012 ACM.",Document analysis; Macro-micro relation; Text visualization,Information services; Wheels; Analytical context; Document analysis; Dynamic nature; Macro micro; Multiple attributes; Text visualization; Transportation belt; Visual analytics systems; Visualization
Mining concept sequences from large-scale search logs for context-aware query suggestion,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150165&doi=10.1145%2f2036264.2036281&partnerID=40&md5=ba13dc0d95ee4856bce6132e248518ee,"Query suggestion plays an important role in improving usability of search engines. Although some recently proposed methods provide query suggestions by mining query patterns from search logs, none of them models the immediately preceding queries as context systematically, and uses context information effectively in query suggestions. Context-aware query suggestion is challenging in both modeling context and scaling up query suggestion using context. In this article, we propose a novel context-aware query suggestion approach. To tackle the challenges, our approach consists of two stages. In the first, offline model-learning stage, to address data sparseness, queries are summarized into concepts by clustering a click-through bipartite. A concept sequence suffix tree is then constructed from session data as a context-aware query suggestionmodel. In the second, online query suggestion stage, a user's search context is captured by mapping the query sequence submitted by the user to a sequence of concepts. By looking up the context in the concept sequence suffix tree, we suggest to the user context-aware queries. We test our approach on large-scale search logs of a commercial search engine containing 4.0 billion Web queries, 5.9 billion clicks, and 1.87 billion search sessions. The experimental results clearly show that our approach outperforms three baseline methods in both coverage and quality of suggestions.© 2011 ACM.",Click-through data; Context-aware; Query suggestion; Session data,Information retrieval; Plant extracts; Query processing; Trees (mathematics); User interfaces; World Wide Web; Baseline methods; Clickthrough data; Context information; Context-aware; Data sparseness; Offline; Query patterns; Query sequence; Query suggestion; Scaling-up; Search logs; Search sessions; Session data; Suffix-trees; Two stage; Search engines
Two-word collocation extraction using monolingual word alignment method,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150163&doi=10.1145%2f2036264.2036280&partnerID=40&md5=afd60eb32ad47c0a0ca00a6dc17c87ce,"Statistical bilingual word alignment has been well studied in the field of machine translation. This article adapts the bilingual word alignment algorithm into a monolingual scenario to extract collocations from monolingual corpus, based on the fact that the words in a collocation tend to co-occur in similar contexts as in bilingual word alignment. First, the monolingual corpus is replicated to generate a parallel corpus, in which each sentence pair consists of two identical sentences. Next, the monolingual word alignment algorithm is employed to align potentially collocated words. Finally, the aligned word pairs are ranked according to the alignment scores and candidates with higher scores are extracted as collocations. We conducted experiments on Chinese and English corpora respectively. Compared to previous approaches that use association measures to extract collocations from co-occurrence word pairs within a given window, our method achieves higher precision and recall. According to human evaluation, our method achieves precisions of 62% on a Chinese corpus and 64% on an English corpus. In particular, we can extract collocations with longer spans, achieving a higher precision of 83% on the long-span (> 6 words) Chinese collocations. © 2011 ACM.",Collocation extraction; Statistical word alignment,Algorithms; Natural language processing systems; Software agents; Association measures; Chinese corpus; Co-occurrence; Collocation extraction; Human evaluation; Long span; Machine translations; Parallel corpora; Precision and recall; Statistical word alignment; Word alignment; Alignment
Interactive image search by color map,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150193&doi=10.1145%2f2036264.2036276&partnerID=40&md5=ae52bf352251488267e81ddb869de37e,"The availability of large-scale images from the Internet has made the research on image search attract a lot of attention. Text-based image search engines, for example, Google/Microsoft Bing/Yahoo! image search engines using the surrounding text, have been developed and widely used. However, they suffer from an inability to search image content. In this article, we present an interactive image search system, image search by color map, which can be applied to, but not limited to, enhance text-based image search. This system enables users to indicate how the colors are spatially distributed in the desired images, by scribbling a few color strokes, or dragging an image and highlighting a few regions of interest in an intuitive way. In contrast to the conventional sketch-based image retrieval techniques, our system searches images based on colors rather than shapes, and we, technically, propose a simple but effective scheme to mine the latent search intention from the user's input, and exploit the dominant color filter strategy to make our system more efficient. We integrate our system to existing Web image search engines to demonstrate its superior performance over text-based image search. The user study shows that our system can indeed help users conveniently find desired images.© 2011 ACM.",Color map; Image search; Intention map,Color; Image retrieval; User interfaces; World Wide Web; Color map; Dominant color; Image search; Image search engine; Image search system; Intention map; Regions of interest; Search image; Sketch-based image retrievals; User study; Web image search; Search engines
Cross-lingual adaptation using structural correspondence learning,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150190&doi=10.1145%2f2036264.2036277&partnerID=40&md5=d06755863135b8a11cf7e73ec0a9e4a6,"Cross-lingual adaptation is a special case of domain adaptation and refers to the transfer of classification knowledge between two languages. In this article we describe an extension of Structural Correspondence Learning (SCL), a recently proposed algorithm for domain adaptation, for cross-lingual adaptation in the context of text classification. The proposed method uses unlabeled documents from both languages, along with a word translation oracle, to induce a cross-lingual representation that enables the transfer of classification knowledge from the source to the target language. The main advantages of this method over existing methods are resource efficiency and task specificity. We conduct experiments in the area of cross-language topic and sentiment classification involving English as source language and German, French, and Japanese as target languages. The results show a significant improvement of the proposed method over a machine translation baseline, reducing the relative error due to cross-lingual adaptation by an average of 30% (topic classification) and 59% (sentiment classification). We further report on empirical analyses that reveal insights into the use of unlabeled data, the sensitivity with respect to important hyperparameters, and the nature of the induced cross-lingual word correspondences. © 2011 ACM.",Cross-language text classification; Cross-lingual adaptation; Structural correspondence learning,Algorithms; Information retrieval systems; Knowledge representation; Linguistics; Text processing; Cross-lingual; Cross-lingual adaptation; Domain adaptation; Empirical analysis; Hyperparameters; Machine translations; Relative errors; Resource efficiencies; Sentiment classification; Source language; Structural correspondence learning; Target language; Text classification; Topic Classification; Unlabeled data; Unlabeled documents; Word translation; Translation (languages)
Group profiling for understanding social structures,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150191&doi=10.1145%2f2036264.2036279&partnerID=40&md5=dbedf8c64c2d7a2153b0ee7b8dd62142,"The prolific use of participatory Web and social networking sites is reshaping the ways in which people interact with one another. It has become a vital part of human social life in both the developed and developing world. People sharing certain similarities or affiliates tend to form communities within social media. At the same time, they participate in various online activities: content sharing, tagging, posting status updates, etc. These diverse activities leave behind traces of their social life, providing clues to understand changing social structures. A large body of existing work focuses on extracting cohesive groups based on network topology. But little attention is paid to understanding the changing social structures. In order to help explain the formation of a group, we explore different group-profiling strategies to construct descriptions of a group. This research can assist network navigation, visualization, and analysis, as well as monitoring and tracking the ebbs and tides of different groups in evolving networks. By exploiting information collected from real-world social media sites, extensive experiments are conducted to evaluate group-profiling results. The pros and cons of different group-profiling strategies are analyzed with concrete examples. We also show some potential applications based on group profiling. Interesting findings with discussions are reported. © 2011 ACM.",Community; Group formation; Group profiling; Social media; Social structure,Developing countries; Electric network topology; User interfaces; Visualization; Community; Group formation; Group profiling; Social media; Social structure; Social networking (online)
Fair seeding in knockout tournaments,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155142201&doi=10.1145%2f2036264.2036273&partnerID=40&md5=f414b879f856120637da13effe3dc5ea,"We investigated the existence of fair seeding in knockout tournaments. We define two fairness criteria, both adapted from the literature: envy-freeness and order preservation. We show how to achieve the first criterion in tournaments whose structure is unconstrained, and prove an impossibility result for balanced tournaments. For the second criterion we have a similar result for unconstrained tournaments, but not for the balanced case. We provide instead a heuristic algorithm which we show through experiments to be efficient and effective. This suggests that the criterion is achievable also in balanced tournaments. However, we prove that it again becomes impossible to achieve when we add a weak condition guarding against the phenomenon of tournament dropout. © 2011 ACM.",Heuristic algorithm; Knockout tournament,Fairness criteria; Impossibility results; Knockout tournament; Order preservation; Show through; Heuristic algorithms
Activity recognition for dynamic multi-agent teams,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155142169&doi=10.1145%2f2036264.2036282&partnerID=40&md5=a927362512672ddd0d3ed5616cf797c4,"This article addresses the problem of activity recognition for dynamic, physically embodied agent teams. We define team activity recognition as the process of identifying team behaviors from traces of agent positions over time; for many physical domains, military or athletic, coordinated team behaviors create distinctive spatio-temporal patterns that can be used to identify low-level action sequences. This article focuses on the novel problem of recovering agent-to-team assignments for complex team tasks where team composition, the mapping of agents into teams, changes over time. We suggest two methods for improving the computational efficiency of the multi-agent plan recognition process in these cases of changing team composition; our proposed approach is robust to sensor observation noise and errors in behavior classification. © 2011 ACM.",Activity recognition; Multi-agent systems; Plan recognition; Teamwork,Computational efficiency; Multi agent systems; Action sequences; Activity recognition; Behavior classification; Coordinated teams; Embodied agent; Multiagent plans; Multiagent teams; Observation noise; Physical domain; Plan recognition; Spatiotemporal patterns; Team activity recognition; Team composition; Teamwork; Intelligent agents
Efficient tag recommendation for real-life data,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155142204&doi=10.1145%2f2036264.2036266&partnerID=40&md5=df14bbdf33162f25a86c52ef693c1a4c,"Despite all of the advantages of tags as an easy and flexible information management approach, tagging is a cumbersome task. A set of descriptive tags has to be manually entered by users whenever they post a resource. This process can be simplified by the use of tag recommendation systems. Their objective is to suggest potentially useful tags to the user. We present a hybrid tag recommendation system together with a scalable, highly efficient system architecture. The system is able to utilize user feedback to tune its parameters to specific characteristics of the underlying tagging system and adapt the recommendation models to newly added content. The evaluation of the system on six real-life datasets demonstrated the system's ability to combine tags from various sources (e.g., resource content or tags previously used by the user) to achieve the best quality of recommended tags. It also confirmed the importance of parameter tuning and content adaptation. A series of additional experiments allowed us to better understand the characteristics of the system and tagging datasets and to determine the potential areas for further system development. © 2011 ACM.",Broad folksonomies; Collaborative tagging; Folksonomies; Hybrid systems; Narrow folksonomies; Tag recommendation,Hybrid systems; Information management; Recommender systems; Collaborative tagging; Content adaptation; Data sets; Efficient systems; Folksonomies; Parameter-tuning; Real life data; Real life datasets; System development; Tag recommendation; Tagging systems; User feedback; User interfaces
Web page summarization for just-in-time contextual advertising,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155187412&doi=10.1145%2f2036264.2036278&partnerID=40&md5=32191f20b64b40558cb906ac9b02cc5d,"Contextual advertising is a type of Web advertising, which, given the URL of aWeb page, aims to embed into the page the most relevant textual ads available. For static pages that are displayed repeatedly, the matching of ads can be based on prior analysis of their entire content; however, often ads need to be matched to new or dynamically created pages that cannot be processed ahead of time. Analyzing the entire content of such pages on-the-fly entails prohibitive communication and latency costs. To solve the three-horned dilemma of either low relevance or high latency or high load, we propose to use text summarization techniques paired with external knowledge (exogenous to the page) to craft short page summaries in real time. Empirical evaluation proves that matching ads on the basis of such summaries does not sacrifice relevance, and is competitive with matching based on the entire page content. Specifically, we found that analyzing a carefully selected 6% fraction of the page text can sacrifice only 1%-3% in ad relevance. Furthermore, our summaries are fully compatible with the standard JavaScript mechanisms used for ad placement: they can be produced at ad-display time by simple additions to the usual script, and they only add 500-600 bytes to the usual request. We also compared our summarization approach, which is based on structural properties of the HTML content of the page, with a more principled one based on one of the standard text summarization tools (MEAD), and found their performance to be comparable. © 2011 ACM.",Text classification; Text summarization,User interfaces; Websites; Contextual advertisings; Empirical evaluations; External knowledge; Fully compatible; High load; Javascript; Just in time; Latency costs; On-the-fly; Real time; Static pages; Text classification; Text summarization; Web advertising; Web page summarization; Marketing
Who is doing what and when: Social map-based recommendation for content-centric social web sites,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155177622&doi=10.1145%2f2036264.2036269&partnerID=40&md5=76de224d16cc7f37fa92c6944975e3c9,"Content-centric social Web sites, such as discussion forums and blog sites, have flourished during the past several years. These sites often contain overwhelming amounts of information that are also being updated rapidly. To help users locate their interests at such sites (e.g., interesting blogs to read or discussion forums to join), researchers have developed a number of recommendation technologies. However, it is difficult to make effective recommendations for new users (a.k.a. the cold start problem) due to a lack of user information (e.g., preferences and interests). Furthermore, the complexity of recommendation algorithms often prevents users from comprehending let alone trusting the recommended results. To tackle these above two challenges, we are building a social map-based recommender system called Pharos. A social map summarizes users' content-related social behavior over time (e.g., reading, writing, and commenting behavior during the past week) as a set of latent communities. For a given time interval, each community is characterized by the theme of the content being discussed and the key people involved. By discovering, ranking, and displaying the most popular latent communities at different time intervals, Pharos creates a time-sensitive, visual social map of a Web site. This enables new users to obtain a quick overview of the site, alleviating the cold start problem. Furthermore, we use the social map as a context to help explain Pharos-recommended content and people. Users can also interactively explore the social map to locate the content in which they are interested or people that are not being explicitly recommended, compensating for the imperfections in the recommendation algorithms. We have developed several Pharos applications, one of which is deployed within our company. Our preliminary evaluation of the deployed application shows the usefulness of Pharos. © 2011 ACM.",Cold start; Recommender systems; Social map; Trust; Visual explanation,Algorithms; Internet; Recommender systems; Cold start; Cold start problems; Community IS; Discussion forum; Recommendation algorithms; Social behavior; Social map; Time interval; Trust; User information; Visual explanation; User interfaces
Introduction,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155142222&doi=10.1145%2f2036264.2036265&partnerID=40&md5=574e2b9b51d6b2d40c71ee5b29b15c02,[No abstract available],,
Introduction,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155177627&doi=10.1145%2f2036264.2036270&partnerID=40&md5=5d1fb1689ea6bed37aa8d9daac6811cb,[No abstract available],,
An adaptive agent for negotiating with people in different cultures,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155151641&doi=10.1145%2f2036264.2036272&partnerID=40&md5=48941cc96ea5a96d5f6abef9173093fa,"The rapid dissemination of technology such as the Internet across geographical and ethnic lines is opening up opportunities for computer agents to negotiate with people of diverse cultural and organizational affiliations. To negotiate proficiently with people in different cultures, agents need to be able to adapt to the way behavioral traits of other participants change over time. This article describes a new agent for repeated bilateral negotiation that was designed to model and adapt its behavior to the individual traits exhibited by its negotiation partner. The agent's decision-making model combined a social utility function that represented the behavioral traits of the other participant, as well as a rule-based mechanism that used the utility function to make decisions in the negotiation process. The agent was deployed in a strategic setting in which both participants needed to complete their individual tasks by reaching agreements and exchanging resources, the number of negotiation rounds was not fixed in advance and agreements were not binding. The agent negotiated with human subjects in the United States and Lebanon in situations that varied the dependency relationships between participants at the onset of negotiation. There was no prior data available about the way people would respond to different negotiation strategies in these two countries. Results showed that the agent was able to adopt a different negotiation strategy to each country. Its average performance across both countries was equal to that of people. However, the agent outperformed people in the United States, because it learned to make offers that were likely to be accepted by people, while being more beneficial to the agent than to people. In contrast, the agent was outperformed by people in Lebanon, because it adopted a high reliability measure which allowed people to take advantage of it. These results provide insight for human-computer agent designers in the types of multicultural settings that we considered, showing that adaptation is a viable approach towards the design of computer agents to negotiate with people when there is no prior data of their behavior. © 2011 ACM.",Cultural modeling; Human-agent decision making,Human computer interaction; Adaptive agents; Behavioral traits; Bilateral negotiations; Computer agents; Cultural modeling; Decision making models; Dependency relationship; High reliability; Human subjects; Human-agent decision making; Human-computer; Lebanon; Negotiation process; Negotiation strategy; Rule based; Social utility functions; Utility functions; Decision making
Scalable affiliation recommendation using auxiliary networks,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155151645&doi=10.1145%2f2036264.2036267&partnerID=40&md5=fbd0035a12c2b98892afbb0d53fa6fed,"Social network analysis has attracted increasing attention in recent years. In many social networks, besides friendship links among users, the phenomenon of users associating themselves with groups or communities is common. Thus, two networks exist simultaneously: the friendship network among users, and the affiliation network between users and groups. In this article, we tackle the affiliation recommendation problem, where the task is to predict or suggest new affiliations between users and communities, given the current state of the friendship and affiliation networks. More generally, affiliations need not be community affiliations-they can be a user's taste, so affiliation recommendation algorithms have applications beyond community recommendation. In this article, we show that information from the friendship network can indeed be fruitfully exploited in making affiliation recommendations. Using a simple way of combining these networks, we suggest two models of user-community affinity for the purpose of making affiliation recommendations: one based on graph proximity, and another using latent factors to model users and communities. We explore the affiliation recommendation algorithms suggested by these models and evaluate these algorithms on two real-world networks, Orkut and Youtube. In doing so, we motivate and propose a way of evaluating recommenders, by measuring how good the top 50 recommendations are for the average user, and demonstrate the importance of choosing the right evaluation strategy. The algorithms suggested by the graph proximity model turn out to be the most effective. We also introduce scalable versions of these algorithms, and demonstrate their effectiveness. This use of link prediction techniques for the purpose of affiliation recommendation is, to our knowledge, novel. © 2011 ACM.",Scalability,Algorithms; Electric network analysis; Auxiliary network; Evaluation strategies; Latent factor; Link prediction; Real-world networks; Recommendation algorithms; Social Network Analysis; Social Networks; YouTube; Mathematical models
A case study of collaboration and reputation in social web search,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155151643&doi=10.1145%2f2036264.2036268&partnerID=40&md5=c93c51266a2afa795e19627be0ae752d,"Although collaborative searching is not supported by mainstream search engines, recent research has highlighted the inherently collaborative nature of many Web search tasks. In this article, we describe HeyStaks,1 a collaborativeWeb search framework that is designed to complement mainstreamsearch engines. At search time, HeyStaks learns from the search activities of other users and leverages this information to generate recommendations based on results that others have found relevant for similar searches. The key contribution of this article is to extend the HeyStaks social search model by considering the search expertise, or reputation, of HeyStaks users and using this information to enhance the result recommendation process. In particular, we propose a reputation model for HeyStaks users that utilise the implicit collaboration events that take place between users as recommendations are made and selected. We describe a live-user trial of HeyStaks that demonstrates the relevance of its core recommendations and the ability of the reputation model to further improve recommendation quality. Our findings indicate that incorporating reputation into the recommendation process further improves the relevance of HeyStaks recommendations by up to 40%. © 2011 ACM.",HeyStaks; Reputation; Social search; Trust,Information retrieval; Search engines; User interfaces; HeyStaks; Reputation; Search activity; Search time; Social search; Trust; Web searches; World Wide Web
GAPs: Geospatial abduction problems,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155151638&doi=10.1145%2f2036264.2036271&partnerID=40&md5=44c81f97e4bc4d8acfb1528e08e4ddd5,"There are many applications where we observe various phenomena in space (e.g., locations of victims of a serial killer), and where we want to infer ""partner"" locations (e.g., the location where the killer lives) that are geospatially related to the observed phenomena. In this article, we define geospatial abduction problems (GAPs for short). We analyze the complexity of GAPs, develop exact and approximate algorithms (often with approximation guarantees) for these problems together with analyses of these algorithms, and develop a prototype implementation of our GAP framework. We demonstrate accuracy of our algorithms on a real world data set consisting of insurgent IED (improvised explosive device) attacks against U.S. forces in Iraq (the observations were the locations of the attacks, while the ""partner"" locations we were trying to infer were the locations of IED weapons caches). © 2011 ACM.",Abduction; Complexity analysis; Heuristic algorithms,Approximation algorithms; Explosives; Abduction; Approximate algorithms; Complexity analysis; Geo-spatial; Improvised explosive devices; Prototype implementations; Real world data; Weapons caches; Heuristic algorithms
"Understanding, manipulating and searching hand-drawn concept maps",2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155151636&doi=10.1145%2f2036264.2036275&partnerID=40&md5=a54cd2cc5aba1282c6cacf2a012d8fb9,"Concept maps are an important tool to organize, represent, and share knowledge. Building a concept map involves creating text-based concepts and specifying their relationships with line-based links. Current concept map tools usually impose specific task structures for text and link construction, and may increase cognitive burden to generate and interact with concept maps. While pen-based devices (e.g., tablet PCs) offer users more freedom in drawing concept maps with a pen or stylus more naturally, the support for hand-drawn concept map creation and manipulation is still limited, largely due to the lack of methods to recognize the components and structures of hand-drawn concept maps. This article proposes a method to understand hand-drawn concept maps. Our algorithm can extract node blocks, or concept blocks, and link blocks of a hand-drawn concept map by combining dynamic programming and graph partitioning, recognize the text content of each concept node, and build a concept-map structure by relating concepts and links. We also design an algorithm for concept map retrieval based on hand-drawn queries. With our algorithms, we introduce structure-based intelligent manipulation techniques and ink-based retrieval techniques to support the management and modification of hand-drawn concept maps. Results from our evaluation study show high structure recognition accuracy in real time of our method, and good usability of intelligent manipulation and retrieval techniques. © 2011 ACM.",Hand-drawn concept map; Intelligent manipulation; Recognition; Retrieval,Algorithms; Dynamic programming; Intelligent robots; Personal computers; Concept maps; Evaluation study; Graph Partitioning; Intelligent manipulation; Pen-based; Real time; Recognition; Retrieval; Retrieval techniques; Share knowledge; Specific tasks; Structure recognition; Structure-based; Tablet PCs; Text content; Character recognition
Geographic information systems and spatial agent-based model simulations for sustainable development,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155151632&doi=10.1145%2f2036264.2036274&partnerID=40&md5=f75cf3dbe133826a6a6b2ae48737b78e,"In recent years the interdisciplinary field of Computational Social Science has developed theory and methodologies for building spatial Agent-Based Social Simulation (ABSS) models of human societies that are situated in ecosystems with land cover and climate. This article explains the needs and demand for Geographic Information Systems (GIS) in these types of agent-based models, with an emphasis on models applied to Eastern Africa and Inner Asia and relevance for understanding and analyzing development issues. The models are implemented with the MASON (Multi-Agent Simulator Of Networks and Neighborhoods) system, an open-source simulation environment in the Java language and suitable for developing ABSS models with GIS for representing spatial features. © 2011 ACM.",Computational social science; Eastern Africa; Geographic information systems (GIS); Inner Asia; Multi-agent simulator of networks and neighborhoods (MASON); Multi-agent systems (MAS); Spatial agent-based modeling (ABM),Behavioral research; Climate models; Computational methods; Computer simulation; Computer simulation languages; Geographic information systems; Information systems; Intelligent agents; Java programming language; Social sciences; Agent-based modeling; Computational social science; Eastern Africa; Geographic information; Inner Asia; Multi-Agent; Multi agent systems
Spatiotemporal correlations in criminal offense records,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960830307&doi=10.1145%2f1989734.1989742&partnerID=40&md5=84f0763eb622ef4edf789fb83e692d83,"With the increased availability of rich behavioral datasets, we present a novel application of tools to analyze this information. Using criminal offense records as an example, we employ cross-correlation measures, eigenvalue spectrum analysis, and results from random matrix theory to identify spatiotemporal patterns on multiple scales. With these techniques, we show that most significant correlation exists on the time scale of weeks and identify clusters of neighborhoods whose crime rates are affected simultaneously by external forces. © 2011 ACM.",Big data; Computational social science; Computational sustainability; Criminology; Engineering social systems,Eigenvalues and eigenfunctions; Social sciences; Spectrum analysis; Big data; Computational social science; Computational sustainability; Criminology; Cross correlations; Data sets; Eigenvalue spectra; External force; Multiple scale; Novel applications; Random matrix theory; Spatiotemporal correlation; Spatiotemporal patterns; Time-scales; Crime
Sustainable biomass power plant location in the Italian Emilia-Romagna region,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960817161&doi=10.1145%2f1989734.1989737&partnerID=40&md5=c6df357fc9e760b34782731e07dece59,"Biomass power plants are very promising for reducing carbon oxides emissions, because they provide energy with a carbon-neutral process. Biomass comes from trees and vegetables, so they provide a renewable type of energy. However, biomass plants location, along with their provisioning basins, are heavily regulated by economical aspects, often without careful consideration of their environmental footprint. For example, some Italian biomass plants import from overseas palm-tree oil that is economically convenient. However, the energy consumed for the oil transportation is definitely greater than the energy produced by the palm-tree oil burning. In this way biomass power plants turn out to be environmentally inefficient, even if they produce renewable energy. We propose an Integer Linear Programming approach for defining the energy and cost-efficient biomass plant location along with the corresponding provisioning basin. In addition, the model enables to evaluate existing plants and their energy and cost efficiency. Our study is based on real data gathered in the Emilia-Romagna region of Italy. Finally, this optimization tool is just a small part of a wider perspective that is aimed to define decision support tools for the improvement of regional planning and its precise strategic environmental assessment. © 2011 ACM.",Computational sustainability; Facility location,Decision support systems; Energy efficiency; Environmental impact; Environmental impact assessments; Integer programming; Location; Petroleum transportation; Plant extracts; Power plants; Regional planning; Biomass plants; Biomass power plants; Carbon oxide; Computational sustainability; Cost-efficient; Decision support tools; Economical aspects; Energy and cost; Environmental footprints; Facility location; Integer Linear Programming; Oil transportation; Optimization tools; PaLM-tree; Renewable energies; Strategic environmental assessments; Biomass
Planning solar array operations on the international space station,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960807028&doi=10.1145%2f1989734.1989745&partnerID=40&md5=40c4946b35d392dbd4556bf773ab3adc,"Flight controllers manage the orientation and modes of eight large solar arrays that power the International Space Station (ISS). The task requires generating plans that balance complex constraints and preferences. These considerations include context-dependent constraints on viable solar array configurations, temporal limits on transitions between configurations, and preferences on which considerations have priority. The Solar Array Constraint Engine (SACE) treats this operations planning problem as a sequence of tractable constrained optimization problems. SACE uses constraint management and automated planning capabilities to reason about the constraints, to find optimal array configurations subject to these constraints and solution preferences, and to automatically generate solar array operations plans. SACE further provides flight controllers with real-time situational awareness and what-if analysis capabilities. SACE is built on the Extensible Universal Remote Operations Planning Architecture (EUROPA) model-based planning system. EUROPA facilitated SACE development by providing model-based planning, built-in constraint reasoning capability, and extensibility. This article formulates the planning problem, explains how EUROPA solves the problem, and provides performance statistics from several planning scenarios. SACE reduces a highly manual process that takes weeks to an automated process that takes tens of minutes. © 2011 ACM.",Constraint satisfaction; Optimization; Planning; Scheduling; Space mission operations,Automation; Constrained optimization; Manned space flight; Solar cell arrays; Space stations; Analysis capabilities; Automated planning; Automated process; Constrained optimization problems; Constraint management; Constraint reasoning; Constraint Satisfaction; Context dependent; Flight controllers; International Space stations; Manual process; Operations planning; Optimal arrays; Performance statistics; Planning problem; Planning systems; Situational awareness; Solar arrays; Space mission operations; Universal remote; Planning
Submodularity and its applications in optimized information gathering,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960793022&doi=10.1145%2f1989734.1989736&partnerID=40&md5=c00b345025d8f6ed9684e5114e09cac5,"Where should we place sensors to efficiently monitor natural drinking water resources for contamination? Which blogs should we read to learn about the biggest stories on the Web? These problems share a fundamental challenge: How can we obtain the most useful information about the state of the world, at minimum cost? Such information gathering, or active learning, problems are typically NP-hard, and were commonly addressed using heuristics without theoretical guarantees about the solution quality. In this article, we describe algorithms which efficiently find provably near-optimal solutions to large, complex information gathering problems. Our algorithms exploit submodularity, an intuitive notion of diminishing returns common tomany sensing problems: themore sensors we have already deployed, the less we learn by placing another sensor. In addition to identifying the most informative sensing locations, our algorithms can handle more challenging settings, where sensors need to be able to reliably communicate over lossy links, where mobile robots are used for collecting data, or where solutions need to be robust against adversaries and sensor failures. We also present results applying our algorithms to several real-world sensing tasks, including environmental monitoring using robotic sensors, activity recognition using a built sensing chair, a sensor placement challenge, and deciding which blogs to read on the Web. © 2011 ACM.",Active learning; Blogs; Computational sustainability; Environmental monitoring; Information gathering; Information overload; Sensor networks; Submodular functions,Algorithms; Environmental engineering; Internet; Optimization; Potable water; Sensor networks; Sensors; User interfaces; Water resources; Active Learning; Computational sustainability; Environmental Monitoring; Information gathering; Information overloads; Submodular functions; Information dissemination
MoveMine: Mining moving object data for discovery of animal movement patterns,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960782524&doi=10.1145%2f1989734.1989741&partnerID=40&md5=c88710fdca20a0ed5d13ef7ed80f0813,"With thematurity and wide availability of GPS, wireless, telecommunication, andWeb technologies,massive amounts of object movement data have been collected from various moving object targets, such as animals, mobile devices, vehicles, and climate radars. Analyzing such data has deep implications in many applications, such as, ecological study, traffic control, mobile communication management, and climatological forecast. In this article, we focus our study on animal movement data analysis and examine advanced data mining methods for discovery of various animal movement patterns. In particular, we introduce a moving object data mining system,MoveMine, which integrates multiple data mining functions, including sophisticated pattern mining and trajectory analysis. In this system, two interesting moving object pattern mining functions are newly developed: (1) periodic behavior mining and (2) swarm pattern mining. For mining periodic behaviors, a reference location-based method is developed, which first detects the reference locations, discovers the periods in complex movements, and then finds periodic patterns by hierarchical clustering. For mining swarm patterns, an efficient method is developed to uncover flexible moving object clusters by relaxing the popularly-enforced collective movement constraints. In the MoveMine system, a set of commonly used moving object mining functions are built and a userfriendly interface is provided to facilitate interactive exploration of moving object data mining and flexible tuning of the mining constraints and parameters. MoveMine has been tested on multiple kinds of real datasets, especially forMoveBank applications and other moving object data analysis. The systemwill benefit scientists and other users to carry out versatile analysis tasks to analyze object movement regularities and anomalies. Moreover, it will benefit researchers to realize the importance and limitations of current techniques and promote future studies on moving object data mining. As expected, a mastery of animal movement patterns and trends will improve our understanding of the interactions between and the changes of the animal world and the ecosystem and therefore help ensure the sustainability of our ecosystem. © 2011 ACM.",Computational sustainability; Moving objects; Pattern mining; Periodic behavior; Swarm pattern,Animals; Data reduction; Ecosystems; Mobile devices; Mobile telecommunication systems; Sustainable development; User interfaces; Computational sustainability; Moving objects; Pattern mining; Periodic behavior; Swarm pattern; Data mining
RECYCLE: Learning looping workflows from annotated traces,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960801696&doi=10.1145%2f1989734.1989746&partnerID=40&md5=918bae892506d9a8758ffc07f60aae37,"A workflow is a model of a process that systematically describes patterns of activity. Workflows capture a sequence of operations, their enablement conditions, and data flow dependencies among them. It is hard to design a complete and correct workflow from scratch, while it is much easier for humans to demonstrate the solution than to state the solution declaratively. This article presents RECYCLE, our approach to learning workflow models from example demonstration traces. RECYCLE captures control flow, data flow, and enablement conditions of an underlying workflow process. Unlike prior work from workflow mining and AI planning literature, (1) RECYCLE can learn from a single demonstration trace with loops, (2) RECYCLE learns both loop and conditional branch structure, and (3) RECYCLE handles data flow among actions. In this article, we describe the phases of RECYCLE's learning algorithm: substructure analysis and node abstraction. To ground the discussion, we present a simplified flight reservation system with some of the important characteristics of the real domains we worked with. We present some results from a patient transport domain. © 2011 ACM.",Hierarchical Task Network learning; Learning from demonstration; Learning from traces; Process mining; Workflow learning,Data transfer; Demonstrations; Learning algorithms; Reservation systems; Hierarchical task networks; Learning from demonstration; Learning from traces; Process mining; Workflow learning; Recycling
Monitoring global forest cover using data mining,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960791695&doi=10.1145%2f1989734.1989740&partnerID=40&md5=e7d1e377191dcfb471c68f8e2bdc6b02,"Forests are a critical component of the planet's ecosystem. Unfortunately, there has been significant degradation in forest cover over recent decades as a result of logging, conversion to crop, plantation, and pasture land, or disasters (natural or man made) such as forest fires, floods, and hurricanes. As a result, significant attention is being given to the sustainable use of forests. A key to effective forest management is quantifiable knowledge about changes in forest cover. This requires identification and characterization of changes and the discovery of the relationship between these changes and natural and anthropogenic variables. In this article, we present our preliminary efforts and achievements in addressing some of these tasks along with the challenges and opportunities that need to be addressed in the future. At a higher level, our goal is to provide an overview of the exciting opportunities and challenges in developing and applying data mining approaches to provide critical information for forest and land use management. © 2011 ACM.",Computational sustainability; Forest cover change; Land change; Remote sensing,Agriculture; Data Processing; Deforestation; Degradation; Disasters; Forest Management; Forests; Land Use; Remote Sensing; Agriculture; Deforestation; Remote sensing; Computational sustainability; Critical component; Critical information; Forest cover; Forest cover change; Forest fires; Land change; Land-use management; Pasture lands; Sustainable use; Data mining
Temporal data mining approaches for sustainable chiller management in data centers,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960806016&doi=10.1145%2f1989734.1989738&partnerID=40&md5=8d7375af48cbf28ad7223c2c3a5c2c45,"Practically every large IT organization hosts data centers-a mix of computing elements, storage systems, networking, power, and cooling infrastructure-operated either in-house or outsourced to major vendors. A significant element of modern data centers is their cooling infrastructure, whose efficient and sustainable operation is a key ingredient to the ""always-on"" capability of data centers. We describe the design and implementation of CAMAS (Chiller Advisory and MAnagement System), a temporal data mining solution to mine and manage chiller installations. CAMAS embodies a set of algorithms for processing multivariate time-series data and characterizes sustainability measures of the patterns mined.We demonstrate three key ingredients of CAMAS-motif mining, association analysis, and dynamic Bayesian network inference-that help bridge the gap between low-level, raw, sensor streams, and the high-level operating regions and features needed for an operator to efficiently manage the data center. The effectiveness of CAMAS is demonstrated by its application to a real-life production data center managed by HP. © 2011 ACM.",Chillers; Clustering; Data centers; Frequent episodes; Motifs; Sustainability,Bayesian networks; Cooling systems; Data mining; Data storage equipment; Inference engines; Mathematical operators; Satellite communication systems; Sustainable development; Time series analysis; Chillers; Clustering; Data centers; Frequent episodes; Motifs; Information management
Agent-based homeostatic control for green energy in the smart grid,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960812405&doi=10.1145%2f1989734.1989739&partnerID=40&md5=7b5a56249080b6da33e7cdf54f21e0ae,"With dwindling nonrenewable energy reserves and the adverse effects of climate change, the development of the smart electricity grid is seen as key to solving global energy security issues and to reducing carbon emissions. In this respect, there is a growing need to integrate renewable (or green) energy sources in the grid. However, the intermittency of these energy sources requires that demand must also be made more responsive to changes in supply, and a number of smart grid technologies are being developed, such as highcapacity batteries and smart meters for the home, to enable consumers to be more responsive to conditions on the grid in real time. Traditional solutions based on these technologies, however, tend to ignore the fact that individual consumers will behave in such a way that best satisfies their own preferences to use or store energy (as opposed to that of the supplier or the grid operator). Hence, in practice, it is unclear how these solutions will cope with large numbers of consumers using their devices in this way. Against this background, in this article, we develop novel control mechanisms based on the use of autonomous agents to better incorporate consumer preferences in managing demand. These agents, residing on consumers' smart meters, can both communicate with the grid and optimize their owner's energy consumption to satisfy their preferences. More specifically, we provide a novel control mechanism that models and controls a system comprising of a green energy supplier operating within the grid and a number of individual homes (each possibly owning a storage device). This control mechanism is based on the concept of homeostasis whereby control signals are sent to individual components of a system, based on their continuous feedback, in order to change their state so that the system may reach a stable equilibrium. Thus, we define a new carbon-based pricing mechanism for this green energy supplier that takes advantage of carbon-intensity signals available on the Internet in order to provide real-time pricing. The pricing scheme is designed in such a way that it can be readily implemented using existing communication technologies and is easily understandable by consumers. Building upon this, we develop new control signals that the supplier can use to incentivize agents to shift demand (using their storage device) to times when green energy is available. Moreover, we show how these signals can be adapted according to changes in supply and to various degrees of penetration of storage in the system.We empirically evaluate our system and show that, when all homes are equipped with storage devices, the supplier can significantly reduce its reliance on other carbon-emitting power sources to cater for its own shortfalls. By so doing, the supplier reduces the carbon emission of the system by up to 25% while the consumer reduces its costs by up to 14.5%. Finally, we demonstrate that our homeostatic control mechanism is not sensitive to small prediction errors and the supplier is incentivized to accurately predict its green production to minimize costs. © 2011 ACM.",Agentbased control; Agents; Computational sustainability; Electricity; Multiagent systems,Alternative fuels; Autonomous agents; Climate change; Climate control; Costs; Energy utilization; Grid computing; Multi agent systems; State feedback; Virtual storage; Adverse effect; Agent based; Agent-based control; Carbon emissions; Carbon-based; Communication technologies; Computational sustainability; Consumer preferences; Control mechanism; Control signal; Electricity grids; Energy source; Global energy; Green energy; Green energy suppliers; Green production; Grid operators; High-capacity; Homeostatic control; Individual components; Intermittency; Non-renewable energy; Power sources; Prediction errors; Pricing mechanism; Pricing scheme; Real time; Real time pricing; Smart grid; Smart meters; Stable equilibrium; Smart power grids
Subkilometer crater discovery with boosting and transfer learning,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960772320&doi=10.1145%2f1989734.1989743&partnerID=40&md5=7c31cd0a018315045a2ef7c38ee83bf3,"Counting craters in remotely sensed images is the only tool that provides relative dating of remote planetary surfaces. Surveying craters requires counting a large amount of small subkilometer craters, which calls for highly efficient automatic crater detection. In this article, we present an integrated framework on autodetection of subkilometer craters with boosting and transfer learning. The framework contains three key components. First, we utilize mathematical morphology to efficiently identify crater candidates, the regions of an image that can potentially contain craters. Only those regions occupying relatively small portions of the original image are the subjects of further processing. Second, we extract and select image texture features, in combination with supervised boosting ensemble learning algorithms, to accurately classify crater candidates into craters and noncraters. Third, we integrate transfer learning into boosting, to enhance detection performance in the regions where surface morphology differs from what is characterized by the training set. Our framework is evaluated on a large test image of 37, 500 × 56, 250 m2 on Mars, which exhibits a heavily cratered Martian terrain characterized by nonuniform surface morphology. Empirical studies demonstrate that the proposed crater detection framework can achieve an F1 score above 0.85, a significant improvement over the other crater detection algorithms. © 2011 ACM.",Classification; Feature selection; Planetary and space science; Spatial data mining; Transfer learning,Feature extraction; Learning algorithms; Morphology; Remote sensing; Surface morphology; Auto-detection; Boosting ensembles; Detection algorithm; Detection framework; Detection performance; Empirical studies; Image texture; Integrated frameworks; Key component; Original images; Planetary surfaces; Remotely sensed images; Space science; Spatial data mining; Test images; Training sets; Transfer learning; Mathematical morphology
PTIME: Personalized assistance for calendaring,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960775941&doi=10.1145%2f1989734.1989744&partnerID=40&md5=c50547442135a1f3e2621fd3b4e41ed5,"In a world of electronic calendars, the prospect of intelligent, personalized time management assistance seems a plausible and desirable application of AI. PTIME (Personalized Time Management) is a learning cognitive assistant agent that helps users handle email meeting requests, reserve venues, and schedule events. PTIME is designed to unobtrusively learn scheduling preferences, adapting to its user over time. The agent allows its user to flexibly express requirements for new meetings, as they would to an assistant. It interfaces with commercial enterprise calendaring platforms, and it operates seamlessly with users who do not have PTIME. This article overviews the system design and describes the models and technical advances required to satisfy the competing needs of preference modeling and elicitation, constraint reasoning, and machine learning. We further report on a multifaceted evaluation of the perceived usefulness of the system. © 2011 ACM.",Calendaring; Machine learning; Personal assistant agents; Preference modeling,Learning systems; Systems analysis; Calendaring; Commercial enterprise; Constraint reasoning; Machine-learning; Multifaceted evaluation; Perceived usefulness; Personal assistant agents; Preference modeling; Technical advances; Time management; Management
Introduction to special issue on computational sustainability,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960811063&doi=10.1145%2f1989734.1989735&partnerID=40&md5=a0e81b0b5512fc01da667f88bb86610b,[No abstract available],,
A helpfulness modeling framework for electronic word-of-mouth on consumer opinion platforms,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955679664&doi=10.1145%2f1961189.1961195&partnerID=40&md5=d29ce3444192c1089e85c2b7ed478a5c,"Electronic Word-of-Mouth (eWOM) is growing exponentially with the rapid development of electronic commerce. As a result, consumers are increasingly crowded by a huge amount of eWOM contents and therefore there is a need to automatically recommend eWOM contents that are helpful to them. Existing helpfulness assessment approaches that deterministically estimate the helpfulness of eWOM contents lack a generative formulation and are limited to the training set that has been voted by many readers. This article presents a rigorous probabilistic framework for inferring the ""helpfulness"" of eWOM contents which can build a ""helpfulness"" model from a low number of votes on eWOM contents. Furthermore, we introduce a measurement, ""helpfulness"" bias, as the benchmark for the ""helpfulness"" of eWOM documents. We also propose a model that exploits the graphical model and expectation maximization algorithm, under this probabilistic framework, to demonstrate the versatility of our framework. Our algorithm is compared experimentally to other existing helpfulness discovering algorithms and the experimental results show that our framework can effectively model the helpfulness of eWOM contents better than other approaches, and therefore indicate the capability of our framework to recommend helpful eWOMs to potential consumers. © 2011 ACM.",Online product reviews; Ranking; Recommender systems,Online systems; Recommender systems; Assessment approaches; Expectation-maximization algorithms; GraphicaL model; Modeling frameworks; Online products; Probabilistic framework; Ranking; Rapid development; Training sets; Algorithms
Introduction to special issue on large-scale machine learning,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955700741&doi=10.1145%2f1961189.1961197&partnerID=40&md5=7b66ff25bfb05864555f5c483fa0d9b6,[No abstract available],,
Learning to recommend with explicit and implicit social relations,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955688653&doi=10.1145%2f1961189.1961201&partnerID=40&md5=9a5fd7f3473202fac51311480aa10f9d,"Recommender systems have been well studied and developed, both in academia and in industry recently. However, traditional recommender systems assume that all the users are independent and identically distributed; this assumption ignores the connections among users, which is not consistent with the real-world observations where we always turn to our trusted friends for recommendations. Aiming at modeling recom- mender systems more accurately and realistically, we propose a novel probabilistic factor analysis framework which naturally fuses the users' tastes and their trusted friends' favors together. The proposed framework is quite general, and it can also be applied to pure user-item rating matrix even if we do not have explicit social trust information among users. In this framework, we coin the term social trust ensemble to represent the formulation of the social trust restrictions on the recommender systems. The complexity analysis indicates that our approach can be applied to very large datasets since it scales linearly with the number of observations, while the experimental results show that our method outperforms state-of-the-art approaches. ©2011 ACM.",Matrix factorization; Recommender systems; Social network; Social trust ensemble,Factorization; Recommender systems; Complexity analysis; Large datasets; matrix; Matrix factorizations; Probabilistic factors; Real-world; Social network; Social relations; Social trust ensemble; State-of-the-art approach; Matrix algebra
PLDA+: Parallel latent dirichlet allocation with data placement and pipeline processing,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955694310&doi=10.1145%2f1961189.1961198&partnerID=40&md5=d61624b98664888b5f29de259971227c,"Previous methods of distributed Gibbs sampling for LDA run into either memory or communication bottlenecks. To improve scalability, we propose four strategies: data placement, pipeline processing, word bundling, and priority-based scheduling. Experiments show that our strategies significantly reduce the unparalleliz- able communication bottleneck and achieve good load balancing, and hence improve scalability of LDA. © 2011 ACM.",Distributed parallel computations; Gibbs sampling; Latent dirichlet allocation; Topic models,Electric power system interconnection; Scalability; Data placement; Gibbs sampling; Latent dirichlet allocation; Load-Balancing; Parallel Computation; Pipeline processing; Priority-based scheduling; Topic models; Pipeline processing systems
LIBSVM: A Library for support vector machines,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955702502&doi=10.1145%2f1961189.1961199&partnerID=40&md5=dd03b423a92c61178036037cfd9e2444,"LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail. © 2011 ACM.",Classification LIBSVM optimization regression support vector machines SVM,Optimization; Vectors; Machine-learning; Multi-class classification; Parameter selection; Probability estimate; Regression support vector machines; SVM-optimization; Support vector machines
Social network analysis and mining for business applications,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955676843&doi=10.1145%2f1961189.1961194&partnerID=40&md5=97b300501c1f9cc42dac802f5170207d,"Social network analysis has gained significant attention in recent years, largely due to the success of online social networking and media-sharing sites, and the consequent availability of a wealth of social network data. In spite of the growing interest, however, there is little understanding of the potential business applications of mining social networks. While there is a large body of research on different problems and methods for social network mining, there is a gap between the techniques developed by the research community and their deployment in real-world applications. Therefore the potential business impact of these techniques is still largely unexplored. In this article we use a business process classiication framework to put the research topics in a business context and provide an overview of what we consider key problems and techniques in social network analysis and mining from the perspective of business applications. In particular, we discuss data acquisition and preparation, trust, expertise, community structure, network dynamics, and information propagation. In each case we present a brief overview of the problem, describe state-of-the art approaches, discuss business application examples, and map each of the topics to a business process classification framework. In addition, we provide insights on prospective business applications, challenges, and future research directions. The main contribution of this article is to provide a state-of-the-art overview of current techniques while providing a critical perspective on business applications of social network analysis and mining.© 2011 ACM.",Community structure; Expert finding; Influence propagation; Networks dynamics and evolution; Social networks; Viral marketing,Arts computing; Marketing; Research; Social networking (online); Social sciences; Community structures; Expert finding; Influence propagation; Networks dynamics and evolution; Social Networks; Viral marketing; Computer aided network analysis
Learning to detect malicious URLs,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955678470&doi=10.1145%2f1961189.1961202&partnerID=40&md5=ba2d5b31147ce82d4428ed5e57f29f0f,"Malicious Web sites are a cornerstone of Internet criminal activities. The dangers of these sites have created a demand for safeguards that protect end-users from visiting them. This article explores how to detect malicious Web sites from the lexical and host-based features of their URLs. We show that this problem lends itself naturally to modern algorithms for online learning. Online algorithms not only process large numbers of URLs more efficiently than batch algorithms, they also adapt more quickly to new features in the continuously evolving distribution of malicious URLs. We develop a real-time system for gathering URL features and pair it with a real-time feed of labeled URLs from a large Web mail provider. From these features and labels, we are able to train an online classifier that detects malicious Web sites with 99% accuracy over a balanced dataset. © 2011 ACM.",Malicious Web sites; Online learning,Algorithms; Computer crime; Real time systems; Batch algorithms; Criminal activities; Data sets; End-users; Host-based; On-line algorithms; On-line classifier; Online learning; Web mail; E-learning
A learning-based contrarian trading strategy via a dual-classifier model,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955676997&doi=10.1145%2f1961189.1961192&partnerID=40&md5=5f7927b4d177952ca22ac1550a71923a,"Behavioral finance is a relatively new and developing research field which adopts cognitive psychology and emotional bias to explain the inefficient market phenomenon and some irrational trading decisions. Unlike the experts in this field who tried to reason the price anomaly and applied empirical evidence in many different financial markets, we employ the advanced binary classification algorithms, such as AdaBoost and support vector machines, to precisely model the overreaction and strengthen the portfolio compositions of the contrarian trading strategies. The novelty of this article is to discover the financial time-series patterns through a high-dimensional and nonlinear model which is constructed by integrated knowledge of finance and machine learning techniques. We propose a dual-classifier learning framework to select candidate stocks from the past results of original contrarian trading strategies based on the defined learning targets. Three different feature extraction methods, including wavelet transformation, historical return distribution, and various technical indicators, are employed to represent these learning samples in a 381-dimensional financial time-series feature space. Finally, we construct the classifier models with four different learning kernels and prove that the proposed methods could improve the returns dramatically, such as the 3-year return that improved from 26.79% to 53.75%. The experiments also demonstrate significantly higher portfolio selection accuracy, improved from 57.47% to 66.41%, than the original contrarian trading strategy. To sum up, all these experiments show that the proposed method could be extended to an effective trading system in the historical stock prices of the leading U.S. companies of S&P 100 index. © 2011 ACM.",Behavioral finance; Classification; Machine learning,Adaptive boosting; Experiments; Feature extraction; Finance; Financial data processing; Learning systems; AdaBoost; Behavioral finance; Binary classification; Classification; Classifier learning; Classifier models; Cognitive psychology; Empirical evidence; Feature extraction methods; Feature space; Financial market; High-dimensional; Learning kernels; Learning samples; Machine learning techniques; Machine-learning; Non-linear model; Portfolio selection; Research fields; Stock price; Technical indicator; Trading strategies; Trading systems; Wavelet transformations; Commerce
CORN: Correlation-driven nonparametric learning approach for portfolio selection,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955690700&doi=10.1145%2f1961189.1961193&partnerID=40&md5=89d58b7dc55abdb0c8590aa538be9298,"Machine learning techniques have been adopted to select portfolios from financial markets in some emerging intelligent business applications. In this article, we propose a novel learning-to-trade algorithm termed CO Rrelation-driven Nonparametric learning strategy (CORN) for actively trading stocks. CORN effectively exploits statistical relations between stock market windows via a nonparametric learning approach. We evaluate the empirical performance of our algorithm extensively on several large historical and latest real stock markets, and show that it can easily beat both the market index and the best stock in the market sub-stantially (without or with small transaction costs), and also surpass a variety of state-of-the-art techniques significantly. © 2011 ACM.",Correlation coefficient; Nonparametric learning; Online portfolio selection,Algorithms; Commerce; Finance; Business applications; Correlation coefficient; Empirical performance; Financial market; Learning approach; Learning strategy; Machine learning techniques; Non-parametric; Nonparametric learning; Online portfolio selection; Portfolio selection; Statistical relations; Stock market; Transaction cost; E-learning
Introduction to special issue on machine learning for business applications,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955681657&doi=10.1145%2f1961189.1961190&partnerID=40&md5=1f7fff87291ec7ea6a4364449ae826b3,[No abstract available],,
Prediction in financial markets: The case for small disjuncts,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955699855&doi=10.1145%2f1961189.1961191&partnerID=40&md5=92c56c92a11bf2280f3618a2c3a2bc46,"Predictive models in regression and classification problems typically have a single model that covers most, if not all, cases in the data. At the opposite end of the spectrum is a collection of models, each of which covers a very small subset of the decision space. These are referred to as ""small disjuncts."" The trade-offs between the two types of models have been well documented. Single models, especially linear ones, are easy to interpret and explain. In contrast, small disjuncts do not provides as clean or as simple an interpretation of the data, and have been shown by several researchers to be responsible for a disproportionately large number of errors when applied to out-of-sample data. This research provides a counterpoint, demonstrating that a portfolio of ""simple"" small disjuncts provides a credible model for financial market prediction, a problem with a high degree of noise. A related novel contribution of this article is a simple method for measuring the ""yield"" of a learning system, which is the percentage of in-sample performance that the learned model can be expected to realize on out-of-sample data. Curiously, such a measure is missing from the literature on regression learning algorithms. Pragmatically, the results suggest that for problems characterized by a high degree of noise and lack of a stable knowledge base it makes sense to reconstruct the portfolio of small rules periodically. © 2011 ACM.",Financial markets; Machine learning; Predictive modeling; Time-series prediction,Commerce; Forecasting; Knowledge based systems; Learning algorithms; Learning systems; Decision space; Degree of noise; Financial market; Knowledge base; Machine-learning; Predictive modeling; Predictive models; Sample data; SIMPLE method; Small disjuncts; Time-series prediction; Finance
Batch and online learning algorithms for nonconvex Neyman-Pearson classification,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955682771&doi=10.1145%2f1961189.1961200&partnerID=40&md5=d8b58c299aaafcddb3c7f9b4de0fb355,We describe and evaluate two algorithms for Neyman-Pearson (NP) classification problem which has been recently shown to be of a particular importance for bipartite ranking problems. NP classification is a non- convex problem involving a constraint on false negatives rate. We investigated batch algorithm based on DC programming and stochastic gradient method well suited for large-scale datasets. Empirical evidences illustrate the potential of the proposed methods. © 2011 ACM.,DC algorithm; Neyman-Pearson; Nonconvex SVM; Online learning,E-learning; Gradient methods; Batch algorithms; D-C programming; DC algorithm; Empirical evidence; False negatives; Large-scale datasets; Neyman-Pearson; Neyman-Pearson classification; Nonconvex; Nonconvex problem; Nonconvex SVM; Online learning; Online learning algorithms; Ranking problems; Stochastic gradient methods; Learning algorithms
Multifocal learning for customer problem analysis,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955694175&doi=10.1145%2f1961189.1961196&partnerID=40&md5=087fd9db7a58ea00126d1acb80712817,"In this study, we formalize a multifocal learning problem, where training data are partitioned into several different focal groups and the prediction model will be learned within each focal group. The multifocal learning problem is motivated by numerous real-world learning applications. For instance, for the same type of problems encountered in a customer service center, the problem descriptions from different customers can be quite different. Experienced customers usually give more precise and focused descriptions about the problem. In contrast, inexperienced customers usually provide diverse descriptions. In this case, the examples from the same class in the training data can be naturally in different focal groups. Therefore, it is necessary to identify those natural focal groups and exploit them for learning at different focuses. Along this line, the key development challenge is how to identify those focal groups in the training data. As a case study, we exploit multifocal learning for profiling customer problems. Also, we provide an empirical study about how the performance of multifocal learning is affected by the quality of focal groups. The results on real-world customer problem logs show that multifocal learning can significantly boost the performance of many existing classification algorithms, such as Support Vector Machines (SVMs), for classifying customer problems and there is strong correlation between the quality of focal groups and the learning performance. © 2011 ACM.",Customer service support; Multi-focal learning,Focusing; Mathematical models; Sales; Support vector machines; Classification algorithm; Customer problems; Customer service centers; Customer services; Empirical studies; Learning performance; Learning problem; Multi-focal learning; Multifocal; Prediction model; Problem description; Real-world; Real-world learning; Strong correlation; Training data; Customer satisfaction
Probabilistic temporal multimedia data mining,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952429317&doi=10.1145%2f1899412.1899421&partnerID=40&md5=8efd1ed09c7273dc8534805242ef7bc8,"Existing sequence pattern mining techniques assume that the obtained events from event detectors are accurate. However, in reality, event detectors label the events from different modalities with a certain probability over a time-interval. In this article, we consider for the first time Probabilistic Temporal Multimedia (PTM) Event data to discover accurate sequence patterns. PTM event data considers the start time, end time, event label and associated probability for the sequence pattern discovery. As the existing sequence pattern mining techniques cannot work on such realistic data, we have developed a novel framework for performing sequence pattern mining on probabilistic temporal multimedia event data. We perform probability fusion to resolve the redundancy among detected events from different modalities, considering their cross-modal correlation.We propose a novel sequence pattern mining algorithm called Probabilistic Interval based Event Miner (PIE-Miner) for discovering frequent sequence patterns from interval based events. PIE-Miner has a new support counting mechanism developed for PTM data. Existing sequence pattern mining algorithms have event label level support counting mechanism, whereas we have developed event cluster level support counting mechanism. We discover the complete set of all possible temporal relationships based on Allen's interval algebra. The experimental results showed that the discovered sequence patterns are more useful than the patterns discovered with state-of-the-art sequence pattern mining algorithms. © 2011.",Cross-modal correlation; Multimedia datamining; Multimodal datamining; Probabilistic interval based event mining; Sequence pattern mining,Data mining; Miners; Probability; Cross-modal correlation; Multimedia datamining; Multimodal datamining; Probabilistic interval based event mining; Sequence pattern mining; Clustering algorithms
Collection-based sparse label propagation and its application on social group suggestion from photos,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952383577&doi=10.1145%2f1899412.1899416&partnerID=40&md5=37b5d86dfa420f0d862b928aa7b1219e,"Online social network services pose great opportunities and challenges for many research areas. In multimedia content analysis, automatic social group recommendation for images holds the promise to expand one's social network through media sharing. However, most existing techniques cannot generate satisfactory social group suggestions when the images are classified independently. In this article, we present novel methods to produce accurate suggestions of suitable social groups from a user's personal photo collection. First, an automatic clustering process is designed to estimate the group similarities, select the optimal number of clusters and categorize the social groups. Both visual content and textual annotations are integrated to generate initial predictions of the group categories for the images. Next, the relationship among images in a user's collection is modeled as a sparse graph. A collection-based sparse label propagation method is proposed to improve the group suggestions. Furthermore, the sparse graph-based collection model can be readily exploited to select the most influential and informative samples for active relevance feedback, which can be integrated with the label propagation process without the need for classifier retraining. The proposed methods have been tested on group suggestion tasks for real user collections and demonstrated superior performance over the state-of-the-art techniques. © 2011.",Active relevance feedback; Collection-based sparse label propagation; Group recommendation; Social image,Clustering algorithms; Automatic clustering; Collection-based sparse label propagation; Group recommendation; Label propagation; Multimedia content analysis; Novel methods; Online social networks; Optimal number; Personal photo collection; Relevance feedback; Research areas; Social groups; Social image; Social Networks; Sparse graphs; Textual annotations; Visual content; Feedback
Image annotation by k nn-sparse graph-based label propagation over noisily tagged web images,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952402835&doi=10.1145%2f1899412.1899418&partnerID=40&md5=e89c8153d3afb009eeba69de5f273200,"In this article, we exploit the problem of annotating a large-scale image corpus by label propagation over noisily tagged web images. To annotate the images more accurately, we propose a novel kNN-sparse graphbased semi-supervised learning approach for harnessing the labeled and unlabeled data simultaneously. The sparse graph constructed by datum-wise one-vs-kNN sparse reconstructions of all samples can remove most of the semantically unrelated links among the data, and thus it is more robust and discriminative than the conventional graphs. Meanwhile, we apply the approximate k nearest neighbors to accelerate the sparse graph construction without loosing its effectiveness. More importantly, we propose an effective training label refinement strategy within this graph-based learning framework to handle the noise in the training labels, by bringing in a dual regularization for both the quantity and sparsity of the noise. We conduct extensive experiments on a real-world image database consisting of 55,615 Flickr images and noisily tagged training labels. The results demonstrate both the effectiveness and efficiency of the proposed approach and its capability to deal with the noise in the training labels. © 2011.",KNN; Label propagation; Noisy tags; Semi-supervised learning; Sparse graph; Web image,KNN; Label propagation; Noisy tags; Semi-supervised learning; Sparse graphs; Web images; Supervised learning
Variational inference with graph regularization for image annotation,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952373233&doi=10.1145%2f1899412.1899415&partnerID=40&md5=55313aab588c10e8619355be978eea2a,"Image annotation is a typical area where there are multiple types of attributes associated with each individual image. In order to achieve better performance, it is important to develop effective modeling by utilizing prior knowledge. In this article, we extend the graph regularization approaches to a more general case where the regularization is imposed on the factorized variational distributions, instead of posterior distributions implicitly involved in EM-like algorithms. In this way, the problem modeling can be more flexible, and we can choose any factor in the problem domain to impose graph regularization wherever there are similarity constraints among the instances. We formulate the problem formally and show its geometrical background in manifold learning.We also design two practically effective algorithms and analyze their properties such as the convergence. Finally, we apply our approach to image annotation and show the performance improvement of our algorithm. © 2011.",Automatic image annotation; Graph regularization; Laplacian regularization; Semantic indexing; Semi-supervised learning; Variational inference,Algorithms; Convergence of numerical methods; Indexing (of information); Laplace transforms; Semantics; Supervised learning; Automatic image annotation; Graph regularization; Laplacian regularization; Semantic indexing; Semi-supervised learning; Variational inference; Image analysis
Introduction to the special issue on intelligent multimedia systems and technology,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952411411&doi=10.1145%2f1899412.1899413&partnerID=40&md5=715a3d14a9bb3beab7b53e09654e1224,[No abstract available],,
"Automatic player labeling, tracking and field registration and trajectory mapping in broadcast soccer video",2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952412905&doi=10.1145%2f1899412.1899419&partnerID=40&md5=b4c0aa6c6275e453c97c962d8d1b00b7,"In this article, we present a method to perform automatic player trajectories mapping based on player detection, unsupervised labeling, efficient multi-object tracking, and playfield registration in broadcast soccer videos. Player detector determines the players' positions and scales by combining the ability of dominant color based background subtraction and a boosting detector with Haar features. We first learn the dominant color with accumulate color histogram at the beginning of processing, then use the player detector to collect hundreds of player samples, and learn player appearance codebook by unsupervised clustering. In a soccer game, a player can be labeled as one of four categories: two teams, referee or outlier. The learning capability enables the method to be generalized well to different videos without any manual initialization. With the dominant color and player appearance model, we can locate and label each player. After that, we perform multi-object tracking by using Markov Chain Monte Carlo (MCMC) data association to generate player trajectories. Some data driven dynamics are proposed to improve the Markov chain's efficiency, such as label consistency, motion consistency, and track length, etc. Finally, we extract key-points and find the mapping from an image plane to the standard field model, and then map players' position and trajectories to the field. A large quantity of experimental results on FIFA World Cup 2006 videos demonstrate that this method can reach high detection and labeling precision, reliably tracking in scenes of player occlusion, moderate camera motion and pose variation, and yield promising field registration results. © 2011.",Boosting; Codebook; Field registration; MCMC; Multiple player tracking; Player labeling; Sports video; Video analysis,Color; Face recognition; Mapping; Markov processes; Trajectories; Video streaming; Boosting; Codebook; Field registration; MCMC; Multiple player tracking; Player labeling; Sports video; Video analysis; Detectors
Distance metric learning from uncertain side information for automated photo tagging,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952428167&doi=10.1145%2f1899412.1899417&partnerID=40&md5=cb95850001bb725ac160d9682592d075,"Automated photo tagging is an important technique for many intelligent multimedia information systems, for example, smart photo management system and intelligent digital media library. To attack the challenge, several machine learning techniques have been developed and applied for automated photo tagging. For example, supervised learning techniques have been applied to automated photo tagging by training statistical classifiers from a collection of manually labeled examples. Although the existing approaches work well for small testbeds with relatively small number of annotation words, due to the long-standing challenge of object recognition, they often perform poorly in large-scale problems. Another limitation of the existing approaches is that they require a set of high-quality labeled data, which is not only expensive to collect but also time consuming. In this article, we investigate a social image based annotation scheme by exploiting implicit side information that is available for a large number of social photos from the social web sites. The key challenge of our intelligent annotation scheme is how to learn an effective distance metric based on implicit side information (visual or textual) of social photos. To this end, we present a novel ""Probabilistic Distance Metric Learning"" (PDML) framework, which can learn optimized metrics by effectively exploiting the implicit side information vastly available on the social web. We apply the proposed technique to photo annotation tasks based on a large social image testbed with over 1 million tagged photos crawled from a social photo sharing portal. Encouraging results show that the proposed technique is effective and promising for social photo based annotation tasks. © 2011.",Automated photo tagging; Content-based image retrieval; Distance metric learning; Social images; Uncertain side information,Automation; Classification (of information); Content based retrieval; Digital libraries; Digital storage; Information management; Learning algorithms; Object recognition; Supervised learning; Test facilities; Testbeds; Automated photo tagging; Content based image retrieval; Distance Metric Learning; Social images; Uncertain side information; Computer graphics
Active learning in multimedia annotation and retrieval: A survey,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952368651&doi=10.1145%2f1899412.1899414&partnerID=40&md5=d4ce9c47d553e1febd6284840b2d43a8,"Active learning is a machine learning technique that selects the most informative samples for labeling and uses them as training data. It has been widely explored in multimedia research community for its capability of reducing human annotation effort. In this article, we provide a survey on the efforts of leveraging active learning in multimedia annotation and retrieval. We mainly focus on two application domains: image/video annotation and content-based image retrieval. We first briefly introduce the principle of active learning and then we analyze the sample selection criteria. We categorize the existing sample selection strategies used in multimedia annotation and retrieval into five criteria: risk reduction, uncertainty, diversity, density and relevance. We then introduce several classification models used in active learning-based multimedia annotation and retrieval, including semi-supervised learning, multilabel learning and multiple instance learning. We also provide a discussion on several future trends in this research direction. In particular, we discuss cost analysis of human annotation and large-scale interactive multimedia annotation. © 2011.",Active learning; Content-based image retrieval; Image annotation; Model learning; Sample selection; Video annotation,Cost accounting; Image analysis; Interactive computer systems; Multimedia systems; Supervised learning; Surveys; Active Learning; Content based image retrieval; Image annotation; Model learning; Sample selection; Video annotation; Content based retrieval
Neighboring joint density-based JPEG steganalysis,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952391306&doi=10.1145%2f1899412.1899420&partnerID=40&md5=5ca936e310a1e24a607c3c58d0dbc7b1,"The threat posed by hackers, spies, terrorists, and criminals, etc. using steganography for stealthy communications and other illegal purposes is a serious concern of cyber security. Several steganographic systems that have been developed and made readily available utilize JPEG images as carriers. Due to the popularity of JPEG images on the Internet, effective steganalysis techniques are called for to counter the threat of JPEG steganography. In this article, we propose a new approach based on feature mining on the discrete cosine transform (DCT) domain and machine learning for steganalysis of JPEG images. First, neighboring joint density features on both intra-block and inter-block are extracted from the DCT coefficient array and the absolute array, respectively; then a support vector machine (SVM) is applied to the features for detection. An evolving neural-fuzzy inference system is employed to predict the hiding amount in JPEG steganograms. We also adopt a feature selection method of support vector machine recursive feature elimination to reduce the number of features. Experimental results show that, in detecting several JPEG-based steganographic systems, our method prominently outperforms the well-known Markov-process based approach. © 2011.",Classification; JPEG; Neighboring joint density; Nuero-fuzzy; Steganalysis; Steganography; SVM; SVMRFE,Cosine transforms; Crime; Discrete cosine transforms; Feature extraction; Fuzzy inference; Personal computing; Support vector machines; Classification; Joint densities; JPEG; Nuero-fuzzy; Steganalysis; SVM; SVM-RFE; Steganography
Performance metrics for activity recognition,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955140421&doi=10.1145%2f1889681.1889687&partnerID=40&md5=9aa5172c3d910c5ab6fba1b4ca202695,"In this article, we introduce and evaluate a comprehensive set of performance metrics and visualisations for continuous activity recognition (AR). We demonstrate how standard evaluation methods, often borrowed from related pattern recognition problems, fail to capture common artefacts found in continuous AR-specifically event fragmentation, event merging and timing offsets. We support our assertion with an analysis on a set of recently published AR papers. Building on an earlier initial work on the topic, we develop a frame-based visualisation and corresponding set of class-skew invariant metrics for the one class versus all evaluation. These are complemented by a new complete set of event-based metrics that allow a quick graphical representation of system performance-showing events that are correct, inserted, deleted, fragmented, merged and those which are both fragmented and merged. We evaluate the utility of our approach through comparison with standard metrics on data from three different published experiments. This shows that where event-and frame-based precision and recall lead to an ambiguous interpretation of results in some cases, the proposed metrics provide a consistently unambiguous explanation. © 2011 ACM.",Activity recognition; Metrics; Performance evaluation,Pattern recognition; A-frames; Activity recognition; Common artefacts; Event-based; Graphical representations; Metrics; Pattern recognition problems; Performance evaluation; Performance metrics; Precision and recall; Skew invariant; Standard evaluations; Standard metrics; Timing offsets; Visualisation; Visualization
Inferring colocation and conversation networks from privacy-sensitive audio with implications for computational social science,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955150614&doi=10.1145%2f1889681.1889688&partnerID=40&md5=458d705a400b09946d7fe5c071ce00a9,"New technologies have made it possible to collect information about social networks as they are acted and observed in the wild, instead of as they are reported in retrospective surveys. These technologies offer opportunities to address many new research questions: How can meaningful information about social interaction be extracted from automatically recorded raw data on human behavior? What can we learn about social networks from such fine-grained behavioral data? And how can all of this be done while protecting privacy? With the goal of addressing these questions, this article presents new methods for inferring colocation and conversation networks from privacysensitive audio. These methods are applied in a study of face-to-face interactions among 24 students in a graduate school cohort during an academic year. The resulting analysis shows that networks derived from colocation and conversation inferences are quite different. This distinction can inform future research in computational social science, especially work that only measures colocation or employs colocation data as a proxy for conversation networks. © 2011 ACM.",Mobile sensing; Social networks,Engineering research; Social networking (online); Social sciences computing; Students; Teaching; Wireless networks; Behavioral data; Colocations; Computational social science; Face-to-face interaction; Graduate schools; Human behaviors; Mobile sensing; New technologies; Research questions; Social interactions; Social networks; Behavioral research
Learning travel recommendations from user-generated GPS traces,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955148008&doi=10.1145%2f1889681.1889683&partnerID=40&md5=089e9922912c2cb3470c528c9d802b11,"The advance of GPS-enabled devices allows people to record their location histories with GPS traces, which imply human behaviors and preferences related to travel. In this article, we perform two types of travel recommendations by mining multiple users' GPS traces. The first is a generic one that recommends a user with top interesting locations and travel sequences in a given geospatial region. The second is a personalized recommendation that provides an individual with locations matching her travel preferences. To achieve the first recommendation, we model multiple users' location histories with a tree-based hierarchical graph (TBHG). Based on the TBHG, we propose a HITS (Hypertext Induced Topic Search)-based model to infer the interest level of a location and a user's travel experience (knowledge). In the personalized recommendation, we first understand the correlation between locations, and then incorporate this correlation into a collaborative filtering (CF)-based model, which predicts a user's interests in an unvisited location based on her locations histories and that of others. We evaluated our system based on a real-world GPS trace dataset collected by 107 users over a period of one year. As a result, our HITS-based inference model outperformed baseline approaches like rank-by-count and rank-by-frequency. Meanwhile, we achieved a better performance in recommending travel sequences beyond baselines like rank-by-count. Regarding the personalized recommendation, our approach is more effective than the weighted Slope One algorithm with a slightly additional computation, and is more efficient than the Pearson correlation-based CF model with the similar effectiveness. © 2011 ACM.",Collaborative filtering; GeoLife; GPS trace; Location history; Location recommendation,Computational efficiency; Correlation methods; Hypertext systems; Trees (mathematics); Collaborative filtering; Data sets; Geo-spatial; GeoLife; GPS trace; Hierarchical graphs; Human behaviors; Hypertext induced topic search; Inference models; Interest level; Location based; Multiple user; Pearson correlation; Personalized recommendation; Real-world; System-based; Tree-based; User's interest; Behavioral research
Recognizing pair-activities by causality analysis,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955133021&doi=10.1145%2f1889681.1889686&partnerID=40&md5=d6972b3517fd477563a43f370c065cc7,"In this article, beyond solo-activity analysis for single object, we study the more complicated pairactivity recognition problem by exploring the relationship between two active objects based on their trajectory clues obtained from video sensor. Our contributions are three-fold. First, we design two sets of features for representing the pair-activities encoded as length-variable trajectory pairs. One set characterizes the strength of causality between two trajectories, for example, the causality ratio and feedback ratio based on the Granger Causality Test (GCT), and another set describes the style of causality between two trajectories, for example, the sampled frequency responses of the digital filter with these two trajectories as the input and output discrete signals respectively. These features along with conventional velocity and position features of a trajectory-pair are essentially of multi-modalities, and may be greatly different in scales and importance. To make full use of them, we then develop a novel feature fusing procedure to learn the coefficients for weighting these features by maximizing the discriminating power measured by weighted correlation. Finally, we collected a pair-activity database of five popular categories, each of which consists of about 170 instances. The extensive experiments on this database validate the effectiveness of the designed features for pair-activity representation, and also demonstrate that the proposed feature fusing procedure significantly boosts the pair-activity classification accuracy. © 2011 ACM.",Activity analysis; Causality analysis; Digital filter; Frequency responses,Digital filters; Trajectories; Active object; Activity analysis; Activity representation; Causality analysis; Classification accuracy; Discrete signal; Discriminating power; Feedback ratio; Granger causality test; Input and outputs; Single object; Video sensors; Weighted correlation; Frequency response
Discovering routines from large-scale human locations using probabilistic topic models,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955141605&doi=10.1145%2f1889681.1889684&partnerID=40&md5=cec15ff1427baf90577ab467811b74a0,"In this work, we discover the daily location-driven routines that are contained in a massive real-life human dataset collected by mobile phones. Our goal is the discovery and analysis of human routines that characterize both individual and group behaviors in terms of location patterns. We develop an unsupervised methodology based on two differing probabilistic topic models and apply them to the daily life of 97 mobile phone users over a 16-month period to achieve these goals. Topic models are probabilistic generative models for documents that identify the latent structure that underlies a set of words. Routines dominating the entire group's activities, identified with a methodology based on the Latent Dirichlet Allocation topic model, include ""going to work late"", ""going home early"", ""working nonstop"" and ""having no reception (phone off)"" at different times over varying time-intervals. We also detect routines which are characteristic of users, with a methodology based on the Author-Topic model. With the routines discovered, and the two methods of characterizing days and users, we can then perform various tasks. We use the routines discovered to determine behavioral patterns of users and groups of users. For example, we can find individuals that display specific daily routines, such as ""going to work early"" or ""turning off the mobile (or having no reception) in the evenings"". We are also able to characterize daily patterns by determining the topic structure of days in addition to determining whether certain routines occur dominantly on weekends or weekdays. Furthermore, the routines discovered can be used to rank users or find subgroups of users who display certain routines. We can also characterize users based on their entropy. We compare our method to one based on clustering using K-means. Finally, we analyze an individual's routines over time to determine regions with high variations, which may correspond to specific events. © 2011 ACM.",Human activity modeling; Reality mining; Topic models,Mobile phones; Telecommunication equipment; Telephone; Telephone sets; Behavioral patterns; Daily lives; Generative model; Group behavior; Human activities; Human dataset; K-means; Latent Dirichlet allocation; Latent structures; Mobile-phone users; Probabilistic topic models; Reality minings; Topic model; Topic structures; Models
Probabilistic models for concurrent chatting activity recognition,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955151444&doi=10.1145%2f1889681.1889685&partnerID=40&md5=4be8137bfccf7d4a4b6808f54a0522fe,"Recognition of chatting activities in social interactions is useful for constructing human social networks. However, the existence of multiple people involved in multiple dialogues presents special challenges. To model the conversational dynamics of concurrent chatting behaviors, this article advocates Factorial Conditional Random Fields (FCRFs) as a model to accommodate co-temporal relationships among multiple activity states. In addition, to avoid the use of inefficient Loopy Belief Propagation (LBP) algorithm, we propose using Iterative Classification Algorithm (ICA) as the inference method for FCRFs. We designed experiments to compare our FCRFs model with two dynamic probabilistic models, Parallel Condition Random Fields (PCRFs) and Hidden Markov Models (HMMs), in learning and decoding based on auditory data. The experimental results show that FCRFs outperform PCRFs and HMMs-like models. We also discover that FCRFs using the ICA inference approach not only improves the recognition accuracy but also takes significantly less time than the LBP inference method. © 2011 ACM.",Chatting activity recognition; Factorial conditional random fields; Iterative classification; Loopy belief propagation,Algorithms; Content based retrieval; Face recognition; Independent component analysis; Inference engines; Activity recognition; Conditional random field; Designed experiments; Human social networks; Inference methods; Iterative classification; Iterative classification algorithms; Loopy belief propagation; Multiple people; Parallel condition; Probabilistic models; Random fields; Recognition accuracy; Social interactions; Temporal relationships; Hidden Markov models
FolderPredictor: Reducing the cost of reaching the right folder,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955148225&doi=10.1145%2f1889681.1889689&partnerID=40&md5=c9eacc7d56192639cc5f836839c1bbff,"Helping computer users rapidly locate files in their folder hierarchies is a practical research problem involving both intelligent systems and user interface design. This article reports on FolderPredictor, a software system that can reduce the cost of locating files in hierarchical folders. FolderPredictor applies a cost-sensitive prediction algorithm to the user's previous file access information to predict the next folder that will be accessed. Experimental results show that, on average, FolderPredictor reduces the number of clicks spent on locating a file by 50%. Several variations of the cost-sensitive prediction algorithm are discussed. An experimental study shows that the best algorithm among them is a mixture of the most recently used (MRU) folder and the cost-sensitive predictions. Furthermore, FolderPredictor does not require users to adapt to a new interface, but rather meshes with the existing interface for opening files on the Windows platform. © 2011 ACM.",Activities; Directories; Folders; Intelligent systems; Intelligent user interfaces; Prediction; Recommendation; Shortcuts; Tasks; User interface,Algorithms; Cost reduction; Forecasting; Intelligent systems; Activities; Directories; Folders; Intelligent User Interfaces; Prediction; Recommendation; Shortcuts; Tasks; User interfaces
Introduction to the Special Issue on intelligent systems for Activity recognition,2011,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955146676&doi=10.1145%2f1889681.1889682&partnerID=40&md5=8cc62b4a0c7f82c5fe5ea141e3a37d7e,[No abstract available],,
CORALS: A real-time planner for anti-air defense operations,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955065455&doi=10.1145%2f1869397.1869402&partnerID=40&md5=db3690b27149b4f1acddf1591638eca8,"Forces involved in modern conflicts may be exposed to a variety of threats, including coordinated raids of advanced ballistic and cruise missiles. To respond to these, a defending force will rely on a set of combat resources. Determining an efficient allocation and coordinated use of these resources, particularly in the case of multiple simultaneous attacks, is a very complex decision-making process in which a huge amount of data must be dealt with under uncertainty and time pressure. This article presents CORALS (COmbat Resource ALlocation Support), a real-time planner developed to support the command team of a naval force defending against multiple simultaneous threats. In response to such multiple threats, CORALS uses a local planner to generate a set of local plans, one for each threat considered apart, and then combines and coordinates them into a single optimized, conflict-free global plan. The coordination is performed through an iterative process of plan merging and conflict detection and resolution, which acts as a plan repair mechanism. Such an incremental plan repair approach also allows adapting previously generated plans to account for dynamic changes in the tactical situation. © 2010 ACM.",Anti-air defense operations; Decision support; Planning,Ballistic missiles; Coordination reactions; Decision making; Decision support systems; Air-defense operations; Complex decision; Conflict detection and resolution; Conflict free; Cruise missile; Decision supports; Dynamic changes; Iterative process; Naval forces; Repair mechanism; Time pressures; Planning
Preface to special issue on applications of automated planning,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955070895&doi=10.1145%2f1869397.1869398&partnerID=40&md5=2e6ed03a9c438432a599e01ab04ef4a1,[No abstract available],,
Planning interventions in biological networks,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955064800&doi=10.1145%2f1869397.1869400&partnerID=40&md5=4098839c5d9ef0850275cf07f7daea6b,"Modeling the dynamics of biological processes has recently become an important research topic in computational biology and systems engineering. One of the most important reasons to model a biological process is to enable high-throughput in-silico experiments that attempt to predict or intervene in the process. These experiments can help accelerate the design of therapies through their rapid and inexpensive replication and alteration. While some techniques exist for reasoning with biological processes, few take advantage of the flexible and scalable algorithms popular in AI research. In reasoning about interventions in biological processes, where scalability is crucial for feasible application, we apply AI planning-based search techniques and demonstrate their advantage over existing enumerative methods. We also present a novel formulation of intervention planning that relies on models that characterize and attempt to change the phenotype of a system. We study three biological systems: the yeast cell cycle, a model of the human aging process, and the Wnt5a network governing the metastasis of melanoma in humans. The contribution of our investigation is in demonstrating that: (i) prior approaches, based on dynamic programming, cannot scale as well as heuristic search, and (ii) the newly found scalability enables us to plan previously unknown sequences of interventions that reveal novel and biologically significant responses in the systems which are consistent with biological knowledge in the literature. © 2010 ACM.",Planning; Search; Systems biology,Bioinformatics; Dynamic programming; Experiments; Scalability; AI planning; Biological networks; Biological process; Computational biology; Enumerative method; Heuristic search; High-throughput; Human aging; In-silico; Intervention planning; Research topics; Scalable algorithms; Search; Search technique; Systems biology; Yeast cell cycles; Biological systems
Structured coalitions in resource selection games,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955073994&doi=10.1145%2f1858948.1858952&partnerID=40&md5=f9ab0c073e967b4a04ff82471c3ca8e5,"We study stability against coalitional deviations in resource selection games where the coalitions have a certain structure. In particular, the agents are partitioned into coalitions, and only deviations by the prescribed coalitions are considered. This is in contrast to the classical concept of strong equilibrium according to which any subset of the agents may deviate. In resource selection games, each agent selects a resource from a set of resources, and its payoff is an increasing (or nondecreasing) function of the number of agents selecting its resource. While it has been shown that a strong equilibrium always exists in resource selection games, a closer look reveals severe limitations to the applicability of the existence result even in the simplest case of two identical resources with increasing cost functions. First, these games do not possess a super strong equilibrium in which a fruitful deviation benefits at least one deviator without hurting any other deviator. Second, a strong equilibrium may not exist when the game is played repeatedly. We prove that for any given partition, there exists a super strong equilibrium for resource selection games of identical resources with increasing cost functions. In addition, we show similar existence results for a variety of other classes of resource selection games. For the case of repeated games, we characterize partitions that guarantee the existence of a strong equilibrium. Together, our work introduces a natural concept, which turns out to lead to positive and applicable results in one of the basic domains studied in the literature. © 2010 ACM.",Coalitions; Partition; Repeated games; Resource selection games; Strong equilibrium,Coalitions; Partition; Repeated games; Resource selection games; Strong equilibrium; Cost functions
Applying planning to interactive storytelling: Narrative control using state constraints,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955059589&doi=10.1145%2f1869397.1869399&partnerID=40&md5=92339061a01e96fdb08257f91b8242ce,"We have seen ten years of the application of AI planning to the problem of narrative generation in Interactive Storytelling (IS). In that time planning has emerged as the dominant technology and has featured in a number of prototype systems. Nevertheless key issues remain, such as how best to control the shape of the narrative that is generated (e.g., by using narrative control knowledge, i.e., knowledge about narrative features that enhance user experience) and also how best to provide support for real-time interactive performance in order to scale up to more realistic sized systems. Recent progress in planning technology has opened up new avenues for IS and we have developed a novel approach to narrative generation that builds on this. Our approach is to specify narrative control knowledge for a given story world using state trajectory constraints and then to treat these state constraints as landmarks and to use them to decompose narrative generation in order to address scalability issues and the goal of real-time performance in larger story domains. This approach to narrative generation is fully implemented in an interactive narrative based on the ""Merchant of Venice."" The contribution of the work lies both in our novel use of state constraints to specify narrative control knowledge for interactive storytelling and also our development of an approach to narrative generation that exploits such constraints. In the article we show how the use of state constraints can provide a unified perspective on important problems faced in IS. © 2010 ACM.",Agents in games and virtual environments; Interactive storytelling; Narrative modeling; Planning,Virtual reality; AI planning; Interactive narrative; Interactive storytelling; Key issues; Narrative modeling; Prototype system; Real time performance; Recent progress; Scalability issue; Scale-up; Sized systems; State constraints; State trajectory; User experience; Virtual environments; Real time systems
Planning for human-robot teaming in open worlds,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955055871&doi=10.1145%2f1869397.1869403&partnerID=40&md5=46f6b91f5bf264b9598c24bdd3a8052e,"As the number of applications for human-robot teaming continue to rise, there is an increasing need for planning technologies that can guide robots in such teaming scenarios. In this article, we focus on adapting planning technology to Urban Search And Rescue (USAR) with a human-robot team. We start by showing that several aspects of state-of-the-art planning technology, including temporal planning, partial satisfaction planning, and replanning, can be gainfully adapted to this scenario. We then note that human-robot teaming also throws up an additional critical challenge, namely, enabling existing planners, which work under closed-world assumptions, to cope with the open worlds that are characteristic of teaming problems such as USAR. In response, we discuss the notion of conditional goals, and describe how we represent and handle a specific class of them called open world quantified goals. Finally, we describe how the planner, and its open world extensions, are integrated into a robot control architecture, and provide an empirical evaluation over USAR experimental runs to establish the effectiveness of the planning components. © 2010 ACM.",Automated planning; Planner; Robot; Search and rescue,Planning; Robots; Automated planning; Critical challenges; Empirical evaluations; Guide robots; Human-robot-team; Open world; Planner; Re-planning; Robot control architecture; Search and rescue; Temporal planning; Urban search and rescue; Robot programming
Virtual worlds as cultural models,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955060029&doi=10.1145%2f1858948.1858951&partnerID=40&md5=1c7ba895f080c8d4e56e0570360c8005,"Thirteen gamelike virtual worlds illustrate issues that overlap social science and information science, because they embody rather clear theories of society and culture: World of Warcraft, Lord of the Rings Online, Dark Age of Camelot, Age of Conan, Pirates of the Burning Sea, ATaleinthe Desert, Entropia Universe, Anarchy Online, The Matrix Online, Tabula Rasa, EVE Online, Star Trek Online, and Dungeons and Dragons Online. A fourteenth, Star Wars Galaxies, illustrates the possibility that not all virtual worlds embody clear theories. After describing the thirteen, this essay discusses their economic systems, social systems, communication challenges, and the ways in which autonomous agents and semi-autonomous secondary avatars enrich interactive complexity.",Culture; Game; Virtual world,Autonomous agents; Interactive computer graphics; Social networking (online); Stars; Cultural models; Culture; Economic system; Game; matrix; Social systems; Star treks; Star wars; Virtual world; Virtual worlds; Virtual reality
Opinion formation under costly expression,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955071565&doi=10.1145%2f1858948.1858953&partnerID=40&md5=164c433e5fc46c22af5326447e27a2ba,"Opinions play an important role in trust building and the creation of consensus about issues and products and a number of studies have focused on the design, evaluation, and utilization of online opinion systems. However, little effort has been spent on the dynamic aspects of online opinion formation. In this article, we study the dynamics of online opinion expression by analyzing the temporal evolution of vey large sets of user views and determine that in the course of time, later opinions tend to show a big difference with earlier opinions, which moderates the average opinion to the less extreme. Online posters also tend to disagree with previous opinions when the cost of expression is high. © 2010 ACM.",Costly expression; Opinion formation,Costly expression; Dynamic aspects; On-line opinions; Opinion formation; Temporal evolution; Trust building; User view; Online systems
Social media as crisis platform: The future of community maps/crisis maps,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952409325&doi=10.1145%2f1858948.1858955&partnerID=40&md5=9fcb9633214273699eb6b76ea1072355,"Social media provides the means for creating new communities and for reenergizing old communities. Recently, a new kind of quickly formulated, powerful community has formed as existing social media communities, news organizations, and users have converged in social media spaces to respond to sudden tragedies. This article addresses the ad-hoc crisis community, whith uses the social madia as a crisis platform to generate community crisis maps.",Crisis mapping; Disaster management; Haiti; Information tools; Maps; Mumbai; Social networking; Visualization,Disaster prevention; Disasters; Crisis mapping; Disaster management; Haiti; Information tools; Mumbai; Social networking; Visualization
A constraint-based approach to scheduling an individual's activities,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955052538&doi=10.1145%2f1869397.1869401&partnerID=40&md5=c5a5aeae4d2cac47e9cb10e16598d8fd,"The goal of helping to automate the management of an individual's time is ambitious in terms both of knowledge engineering and of the quality of the plans produced by an AI system. Modeling an individual's activities is itself a challenge, due to the variety of activity, constraint, and preference types involved. Activities might be simple or interruptible; they might have fixed or variable durations, constraints over their temporal domains, and binary constraints between them. Activities might require the individual being at specific locations in order, whereas traveling time should be taken into account. Some activities might require exclusivity, whereas others can be overlapped with compatible concurrent activities. Finally, while scheduled activities generate utility for the individual, extra utility might result from the way activities are scheduled in time, individually and in conjunction. This article presents a rigorous, expressive model to represent an individual's activities, that is, activities whose scheduling is not contingent on any other person. Joint activities such as meetings are outside our remit; it is expected that these are arranged manually or through negotiation mechanisms and they are considered as fixed busy times in the individual's calendar. The model, formulated as a constraint optimization problem, is general enough to accommodate a variety of situations. We present a scheduler that operates on this rich model, based on the general squeaky wheel optimization framework and enhanced with domain-dependent heuristics and forward checking. Our empirical evaluation demonstrates both the efficiency and the effectiveness of the selected approach. Part of the work described has been implemented in the SelfPlanner system, a Web-based intelligent calendar application that utilizes Google Calendar. © 2010 ACM.",,Knowledge engineering; Optimization; Scheduling; AI systems; Binary constraints; Concurrent activities; Constraint optimization problems; Constraint-based; Empirical evaluations; Forward checking; Joint activity; Negotiation mechanism; Specific location; Squeaky wheel optimizations; Temporal domain; Traveling time; Model checking
Evolution of state-dependent risk preferences,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955054887&doi=10.1145%2f1858948.1858954&partnerID=40&md5=e71d59719ac35f1642b9ab3244489da5,"Researchers have invested much effort in constructing models of the state-dependent (sometimes risk-averse and sometimes risk-prone) nature of human decision making. An important open question is how state-dependent risk behavior can arise and remain prominent in populations. We believe that one part of the answer is the interplay between risk-taking and sequentiality of choices in populations subject to evolutionary population dynamics. To support this hypothesis, we provide simulation and analytical results for evolutionary lottery games, including results on evolutionary stability. We consider a parameterized class of imitation dynamics in which the parameter 0 < α < 1 yields the replicator dynamic with α = 1and the imitate-the-better dynamic with α = 0. Our results demonstrate that for every population dynamic in this class except for the replicator dynamic, the interplay between risk-taking and sequentiality of choices allows state-dependent risk behavior to have an evolutionary advantage over expected-value maximization. © 2010 ACM.",Decision theory; Evolutionary games; Population dynamics; Risk,Decision making; Dynamics; Game theory; Population dynamics; Analytical results; Constructing models; Evolutionary games; Evolutionary population; Evolutionary stability; Human decision making; Imitation dynamics; Parameterized; Replicator dynamics; Risk behavior; Risk preference; Risk taking; Sequentiality; State-dependent; Decision theory
Introduction to the ACM TIST special issue al in social computing and cultural modeling,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955056760&doi=10.1145%2f1858948.1858950&partnerID=40&md5=074cc7dd406778dc5ebcb07d98ce9679,[No abstract available],,
Introduction to ACM TIST,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955054062&doi=10.1145%2f1858948.1858949&partnerID=40&md5=9ece09ceb382be007d50ce5e119b4aa4,[No abstract available],,
Accessible image search for colorblindness,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955057179&doi=10.1145%2f1858948.1858956&partnerID=40&md5=2005deba94f9fcfb4d18a94581d20c23,"This article introduces an intelligent system that accommodates colorblind users in image search. Color plays an important role in the human perception and recognition of images. However, there are about 8% of men and 0.8% of women suffering from colorblindness. We show that the existing image search techniques cannot provide satisfactory results for these users since many images will not be well perceived by them due to the loss of color information. To deal with this difficulty, we introduce a system named Accessible Image Search (AIS) to accommodate these users. Different from the general image search scheme that aims at returning more relevant results, AIS further takes into account the colorblind accessibilities of the returned results, that is, the image qualities in the eyes of colorblind users. The system contains three components: accessibility assessment, accessibility improvement, and color indication. The accessibility assessment component measures the accessibility scores of images, and consequently different reranking methods can be performed to prioritize images with high accessibilities. In the accessibility improvement component, we propose an efficient recoloring algorithm to modify the colors of the images such that they can be better perceived by colorblind users. Color indication aims to indicate the name of the interesting color in an image. We evaluate the introduced system with more than 60 queries and 20 anonymous colorblind users, and the empirical results demonstrate its effectiveness and usefulness.© 2010 ACM.",Colorblindness; Image search,Color; Image quality; Intelligent systems; Rating; Color information; Colorblindness; Empirical results; Human perception; Image search; Re-ranking; Recoloring algorithms; Three component; Search engines
Human-aware task planning: An application to mobile robots,2010,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955060590&doi=10.1145%2f1869397.1869404&partnerID=40&md5=41c75af5059c156c6d1aa20086083b46,"Consider a house cleaning robot planning its activities for the day. Assume that the robot expects the human inhabitant to first dress, then have breakfast, and finally go out. Then, it should plan not to clean the bedroom while the human is dressing, and to clean the kitchen after the human has had breakfast. In general, robots operating in inhabited environments, like households and future factory floors, should plan their behavior taking into account the actions that will be performed by the humans sharing the same environment. This would improve human-robot cohabitation, for example, by avoiding undesired situations for the human. Unfortunately, current task planners only consider the robot's actions and unexpected external events in the planning process, and cannot accommodate expectations about the actions of the humans. In this article, we present a human-aware planner able to address this problem. Our planner supports alternative hypotheses of the human plan, temporal duration for the actions of both the robot and the human, constraints on the interaction between robot and human, partial goal achievement and, most importantly, the possibility to use observations of human actions in the policy generated for the robot. Our planner has been tested both as a stand-alone component and within a full framework for human-robot interaction in a real environment. © 2010 ACM.",Human-aware planning; Human-robot interaction,Human computer interaction; Man machine systems; Planning; Robot programming; Cleaning robot; Factory floors; Human actions; Human inhabitants; Human-aware; Human-aware planning; Planning process; Real environments; Robot interactions; Stand -alone; Task planner; Task planning; Temporal durations; Human robot interaction
CAFE and SOUP: Toward Adaptive VDI Workload Prediction,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146448352&doi=10.1145%2f3529536&partnerID=40&md5=703eaa35e205918bfee399dbeed37cfb,"For Virtual Desktop Infrastructure (VDI) system, effective resource management is rather important where turning off spare virtual machines would help save running cost while maintaining sufficient virtual machines is essential to secure satisfactory user experience. Current VDI resource management strategy works in a passive manner by either reactively driving available capacity based on user demands or following manually configured schedules, which may lead to unnecessary running costs or unsatisfactory user experience. In this article, we propose a first attempt toward proactive VDI resource management, where two adaptive learning approaches for VDI workload prediction are proposed by learning from multi-grained historical features. For non-persistent desktop pool, based on the aggregation session count of pool-sharing users, the CAFE approach induces a pool-level workload predictive model by utilizing coarse-to-fine historical features extracted from aggregation workload data. For persistent desktop pool, based on the session connection status of individual users within the same pool, the SOUP approach induces user-level workload predictive model by incorporating encoded multi-grained features extracted from the logon behavior of individual users into an aggregation pool-level model. Extensive experiments on datasets of real VDI customers and electricity load evidently verify the effectiveness of the proposed adaptive approaches for VDI workload prediction as well as other workload prediction tasks.  © 2022 Association for Computing Machinery.",adaptive modeling; multi-grained features; VDI resource management; workload prediction,Forecasting; Natural resources management; Network security; Resource allocation; Virtual machine; Adaptive models; Infrastructure resources; Multi-grained feature; Pool-based; Resource management; Running cost; Users' experiences; Virtual desktop infrastructure resource management; Virtual desktops; Workload predictions; Lakes
Two-Level Optimization to Reduce Waiting Time at Locks in Inland Waterway Transportation,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146471586&doi=10.1145%2f3527822&partnerID=40&md5=e7994bdcb06c1266186844d96a59ef5d,"Inland vessels often have to cross numerous locks before reaching their final destination, which leads to a significant delay and sometimes represents as much as half of the total travel time. The delay affects shipment costs and can affect other parts of the transport chain, adversely impacting this transportation mode's growth. Therefore, this work presents a two-level solution to ensure a shorter waiting time at locks and improve inland waterway transport. On the one hand, the first level focuses on making infrastructural modifications by proposing an efficient Lock Automation Decision Making (Lock-ADM) method. The problem modeling consists of using a three-stage algorithm. Firstly, we calculate the optimal number of locks while minimizing the investment costs using the exact solver, CPLEX. Secondly, we measure the importance of locks in the network, and finally, we select the best locks to automate using the Genetic Algorithm (GA) metaheuristic. Based on real data, we achieved an average reduction of 33.7% in overall lock waiting time at a low cost. On the other hand, the second level proposes a Dynamic Lock Scheduling (Lock-DS) to efficiently manage vessels scheduling at locks by minimizing their waiting time and optimizing their speed. We achieve an average reduction of 69.9% in vessel waiting time and a reduction of 48.03% in total fuel consumption compared to existing scheduling methods. Automating the most important locks with Lock-ADM and managing their crossing with Lock-DS ensure shorter vessels' waiting time and represent a significant first step towards the automation of inland navigation.  © 2022 Association for Computing Machinery.",dynamic scheduling; Inland waterway navigation; locks; multi-objective optimization; waiting time,Decision making; Genetic algorithms; Multiobjective optimization; Travel time; Waterway transportation; % reductions; Dynamic scheduling; Inland waterway navigation; Inland waterway transportation; Lock-in; Multi-objectives optimization; Travel-time; Two-level optimization; Waiting time; Waterway navigation; Locks (fasteners)
CoPhy-PGNN: Learning Physics-guided Neural Networks with Competing Loss Functions for Solving Eigenvalue Problems,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138555270&doi=10.1145%2f3530911&partnerID=40&md5=cbcafcfc597404d9d4c76b3596d1872f,"Physics-guided Neural Networks (PGNNs) represent an emerging class of neural networks that are trained using physics-guided (PG) loss functions (capturing violations in network outputs with known physics), along with the supervision contained in data. Existing work in PGNNs has demonstrated the efficacy of adding single PG loss functions in the neural network objectives, using constant tradeoff parameters, to ensure better generalizability. However, in the presence of multiple PG functions with competing gradient directions, there is a need to adaptively tune the contribution of different PG loss functions during the course of training to arrive at generalizable solutions. We demonstrate the presence of competing PG losses in the generic neural network problem of solving for the lowest (or highest) eigenvector of a physics-based eigenvalue equation, which is commonly encountered in many scientific problems. We present a novel approach to handle competing PG losses and demonstrate its efficacy in learning generalizable solutions in two motivating applications of quantum mechanics and electromagnetic propagation. All the code and data used in this work are available at https://github.com/jayroxis/Cophy-PGNN.  © 2022 Copyright held by the owner/author(s).",Electromagnetic propagation; Ising model; ML; PGML; quantum physics,Backpropagation; Ising model; Learning systems; Quantum theory; Eigenvalue problem; Electromagnetic propagation; In networks; Learning physics; Loss functions; ML; Neural network learning; Neural-networks; PGML; Quantum physics; Eigenvalues and eigenfunctions
A New Similarity Space Tailored for Supervised Deep Metric Learning,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150015764&doi=10.1145%2f3559766&partnerID=40&md5=5f53dc349d0e102b3c303e3de6f82d15,"We propose a novel deep metric learning method. Differently from many works in this area, we define a novel latent space obtained through an autoencoder. The new space, namely S-space, is divided into different regions describing positions where pairs of objects are similar/dissimilar. We locate makers to identify these regions and estimate the similarities between objects through a kernel-based Cauchy distribution to measure the markers' distance and the new data representation. In our approach, we simultaneously estimate the markers' position in the S-space and represent the objects in the same space. Moreover, we propose a new regularization function to prevent similar markers from collapsing altogether. Our method emphasizes the group property (separability) while preserving instance representativity. We present evidence that our proposal can represent complex spaces, for instance, when groups of similar objects are located in disjoint regions. We compare our proposal to nine different distance metric learning approaches (four of them are based on deep learning) on 28 real-world heterogeneous datasets. According to the four quantitative metrics used, our method overcomes all of the nine strategies from the literature. © 2022 Association for Computing Machinery.",deep metric learning; latent feature space; regularization function; Similarity space,Learning systems; Auto encoders; Cauchy distribution; Deep metric learning; Feature space; Latent feature space; Learning methods; Metric learning; Regularization function; Similarity between objects; Similarity spaces; Deep learning
Multitask Balanced and Recalibrated Network for Medical Code Prediction,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141788827&doi=10.1145%2f3563041&partnerID=40&md5=0968ec11d0227c2aad760a33a01d1b97,"Human coders assign standardized medical codes to clinical documents generated during patients' hospitalization, which is error prone and labor intensive. Automated medical coding approaches have been developed using machine learning methods, such as deep neural networks. Nevertheless, automated medical coding is still challenging because of complex code association, noise in lengthy documents, and the imbalanced class problem. We propose a novel neural network, called the Multitask Balanced and Recalibrated Neural Network, to solve these issues. Significantly, the multitask learning scheme shares the relationship knowledge between different coding branches to capture code association. A recalibrated aggregation module is developed by cascading convolutional blocks to extract high-level semantic features that mitigate the impact of noise in documents. Also, the cascaded structure of the recalibrated module can benefit learning from lengthy notes. To solve the imbalanced class problem, we deploy focal loss to redistribute the attention on low- and high-frequency medical codes. Experimental results show that our proposed model outperforms competitive baselines on a real-world clinical dataset called the Medical Information Mart for Intensive Care (MIMIC-III). © 2022 Copyright held by the owner/author(s).",balanced and recalibrated network; imbalanced class problem; Medical code prediction; multitask learning,Bioinformatics; Learning systems; Network coding; Semantics; Balanced and recalibrated network; Code predictions; Complex codes; Error prones; Imbalanced class; Imbalanced class problem; Labour-intensive; Machine learning methods; Medical code prediction; Multitask learning; Deep neural networks
Adversarial Learning for Cross Domain Recommendations,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150016643&doi=10.1145%2f3548776&partnerID=40&md5=70fcf56ce3fd058deeef490e2f474f2e,"Existing cross domain recommender systems typically assume homogeneous user preferences across multiple domains to capture similarities of user-item interactions and to provide cross domain recommendations accordingly. Meanwhile, the heterogeneity of user behaviors is usually not well studied and captured during the recommendation process, where users might have vastly different interests in different domains. In addition, previous models focus primarily on recommendation tasks between domain pairs, and cannot be naturally extended to serve for multiple domain recommendation applications. To address these challenges, we propose to utilize the idea of adversarial learning to intelligently incorporate global user preferences and domain-specific user preferences for providing satisfying cross domain recommendations. In particular, our proposed Adversarial Cross Domain Recommendation (ACDR) model first obtains the latent representations of global user preferences from their explicit feature information, and then transforms them into domain-specific user embeddings, where we take into account user behaviors and their heterogeneous preferences among different domains. By doing so, we address the differences among user representations in the domain-specific latent space while also preserving global user preferences, as we effectively segment the distributions of domain-specific user embeddings in the shared latent space. The convergence of our proposed model is theoretically guaranteed. The proposed ACDR model leads to significant and consistent improvements in cross domain recommendation performance over the state-of-the-art baseline models, which we demonstrate through extensive experiments on three real-world datasets. In addition, we show that the improvements are greater on those datasets that are smaller and more sparse, on those users that have fewer interaction records in the dataset, and when user interactions from more product domains are included in the cross domain recommendation model. © 2022 Association for Computing Machinery.",adversarial learning; Cross domain recommendation; user preferences,Behavioral research; Embeddings; Adversarial learning; Cross-domain; Cross-domain recommendations; Different domains; Domain specific; Embeddings; Multiple domains; User behaviors; User domains; User's preferences; Recommender systems
How Is the Stroke? Inferring Shot Influence in Badminton Matches via Long Short-term Dependencies,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143392082&doi=10.1145%2f3551391&partnerID=40&md5=966955329720c2ad9efa513f66462637,"Identifying significant shots in a rally is important for evaluating players' performance in badminton matches. While there are several studies that have quantified player performance in other sports, analyzing badminton data has remained untouched. In this article, we introduce a badminton language to fully describe the process of the shot, and we propose a deep-learning model composed of a novel short-term extractor and a long-term encoder for capturing a shot-by-shot sequence in a badminton rally by framing the problem as predicting a rally result. Our model incorporates an attention mechanism to enable the transparency between the action sequence and the rally result, which is essential for badminton experts to gain interpretable predictions. Experimental evaluation based on a real-world dataset demonstrates that our proposed model outperforms the strong baselines. We also conducted case studies to show the ability to enhance players' decision-making confidence and to provide advanced insights for coaching, which benefits the badminton analysis community and bridges the gap between the field of badminton and computer science. © 2022 Association for Computing Machinery.",attention mechanism; badminton language representation; shot influence; Sport analytics,Decision making; Deep learning; Action sequences; Attention mechanisms; Badminton language representation; Experimental evaluation; Learning models; Performance; Real-world datasets; Shot influence; Sport analytic; Term dependency; Sports
Decentralized Online Learning: Take Benefits from Others' Data without Sharing Your Own to Track Global Trend,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147464820&doi=10.1145%2f3559765&partnerID=40&md5=4a7c3660f02068c0cb0919922f8a2e6c,"Decentralized online learning (online learning in decentralized networks) has been attracting more and more attention, since it is believed that decentralized online learning can help data providers cooperatively better solve their online problems without sharing their private data to a third party or other providers. Typically, the cooperation is achieved by letting the data providers exchange their models between neighbors, e.g., recommendation model. However, the best regret bound for a decentralized online learning algorithm is (n T), where n is the number of nodes (or users) and T is the number of iterations. This is clearly insignificant, since this bound can be achieved without any communication in the networks. This reminds us to ask a fundamental question: Can people really get benefit from the decentralized online learning by exchanging information? In this article, we studied when and why the communication can help the decentralized online learning to reduce the regret. Specifically, each loss function is characterized by two components: the adversarial component and the stochastic component. Under this characterization, we show that decentralized online gradient enjoys a regret bound , where G measures the magnitude of the adversarial component in the private data (or equivalently the local loss function) and σ measures the randomness within the private data. This regret suggests that people can get benefits from the randomness in the private data by exchanging private information. Another important contribution of this article is to consider the dynamic regret - a more practical regret to track users' interest dynamics. Empirical studies are also conducted to validate our analysis. © 2022 Association for Computing Machinery.",Decentralized online learning; dynamic regret; online gradient descent,E-learning; Gradient methods; Learning algorithms; Random processes; Decentralised; Decentralized online learning; Dynamic regret; Global trends; Gradient-descent; Loss functions; Online gradient descent; Online learning; Private data; Regret bounds; Stochastic systems
Incorporation of Data-Mined Knowledge into Black-Box SVM for Interpretability,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146692299&doi=10.1145%2f3548775&partnerID=40&md5=fc498f5dff3e79e8325bdbdc4e8345f1,"The lack of interpretability often makes black-box models challenging to be applied in many practical domains. For this reason, the current work, from the black-box model input port, proposes to incorporate data-mined knowledge into the black-box soft-margin SVM model to enhance accuracy and interpretability. The concept and incorporation mechanism of data-mined knowledge are successively developed, based on which a partially interpretable soft-margin SVM (pTsm-SVM) optimization model is designed and then solved through reformulating the optimization problem as standard quadratic programming. An algorithm for mining linear positive (negative) class knowledge from general data sets is also proposed, which generates a linear two-dimensional discriminative rule with specificity (sensitivity) equal to 1 and the highest possible sensitivity (specificity) among all two-dimensional feature spaces. The knowledge-integrated pTsm-SVM works by achieving a good trade-off among the ""large margin"", ""high specificity"", and ""high sensitivity"". Our experimental results on eight UCI datasets demonstrate the superiority of the proposed pTsm-SVM over the standard soft-margin SVM both in terms of accuracy and interpretability.  © 2022 Association for Computing Machinery.",black-box; data-mined; interpretability; Knowledge; soft-margin SVM,Data mining; Economic and social effects; Support vector machines; 'current; Black box modelling; Black boxes; Data-mined; Input port; Interpretability; Knowledge; Model inputs; Soft margins; Soft-margin SVM; Quadratic programming
Trustworthy AI: A Computational Perspective,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149731731&doi=10.1145%2f3546872&partnerID=40&md5=bd99397814713263686b6e603ca7ca9a,"In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone's daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies. A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Nondiscrimination & Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability & Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.  © 2022 Association for Computing Machinery.",Accountability; Artificial intelligence; Environmental Well-being; Explainability; Fairness; Privacy; Robustness,Robustness (control systems); Safety engineering; Accountability; Artificial intelligence technologies; Daily lives; Environmental well-being; Explainability; Fairness; Human society; Privacy; Robustness; Well being; Artificial intelligence
Redundant Label Learning via Subspace Representation and Global Disambiguation,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150057489&doi=10.1145%2f3558547&partnerID=40&md5=0bc90c811b6ebafc510cb1d3487aa700,"Redundant Label Learning (RLL) aims at inducing a robust model from training data, where each example is associated with a set of candidate labels, among which some of them are incorrect. Most existing approaches deal with such problem by disambiguating the candidate labels first and then inducing the predictive model from the disambiguated data. However, these approaches only focus on disambiguation for each instance' candidate label set, while the global label context tends to be ignored. Meanwhile, these approaches usually induce the objective model by directly utilizing the original feature information, which may lead to the model overfitting due to high-dimensional redundant features. To tackle the above issues, we propose a novel feature SubspacE Representation and label Global DisambiguatIOn (SERGIO) approach, which improves the generalization ability of the learning system from the perspective of both feature space and label space. Specifically, we project the original high-dimensional feature space into a low-dimensional subspace, where the projection matrix is regularized with an orthogonality constraint to make the subspace more compact. Meanwhile, we introduce a label confidence matrix and constrain it with ĝ.,""1-norm and trace-norm regularization simultaneously, which are utilized to explore global label correlations and further well in accordance with the nature of single-label classification and multi-label classification problem, respectively. Extensive experiments on both single-label and multi-label RLL datasets demonstrate that our proposed method achieves competitive performance against state-of-the-art approaches. © 2022 Association for Computing Machinery.",feature Subspace Representation; label Global Disambiguation; multi-label classification; Redundant Label Learning; single-label classification,Data mining; Knowledge management; Learning systems; Matrix algebra; Feature subspace; Feature subspace representation; Label global disambiguation; Multi-label classifications; Predictive models; Redundant label learning; Robust modeling; Single-label classification; Subspace representation; Training data; Classification (of information)
FLAG: A Feedback-aware Local and Global Model for Heterogeneous Sequential Recommendation,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150025944&doi=10.1145%2f3557046&partnerID=40&md5=2abb73b194d94ae8d07f7e52cf565cec,"Heterogeneous sequential recommendation that models sequences of items associated with more than one type of feedback such as examinations and purchases is an emerging topic in the research community, which is also an important problem in many real-world applications. Though there are some methods proposed to exploit different types of feedback in item sequences such as RLBL, RIB, and BINN, they are based on RNN and may not be very competitive in capturing users' complex and dynamic preferences. And most existing advanced sequential recommendation methods such as the CNN- and attention-based methods are often designed for making use of item sequences with one single type of feedback, which thus can not be applied to the studied problem directly. As a response, we propose a novel feedback-aware local and global (FLAG) preference learning model for heterogeneous sequential recommendation. Our FLAG contains four modules, including (i) a local preference learning module for capturing a user's short-term interest, which adopts a novel feedback-aware self-attention block to distinguish different types of feedback; (ii) a global preference learning module for modeling a user's global preference; (iii) a local intention learning module, which takes a user's real feedback in the next step, i.e., the user's intention at the current step, as the query vector in a self-attention block to figure out the items that match the user's intention well; and (iv) a prediction module for preference integration and final prediction. We then conduct extensive experiments on three public datasets and find that our FLAG significantly outperforms 13 very competitive baselines in terms of two commonly used ranking-oriented metrics in most cases. We also include ablation studies and sensitivity analysis of our FLAG to have more in-depth insights. © 2022 Association for Computing Machinery.",global preference; Heterogeneous sequential recommendation; local intention; local preference; self-attention,Data mining; Learning systems; User profile; Global models; Global preference; Heterogeneous sequential recommendation; Learning modules; Local intention; Local model; Local preference; Preference learning; Self-attention; User's intentions; Sensitivity analysis
Deep Learning Embedded into Smart Traps for Fruit Insect Pests Detection,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143782939&doi=10.1145%2f3552435&partnerID=40&md5=600f390e1fb9c039e14f6c6141ff80f7,"This article presents a novel approach to identify two species of fruit insect pests as part of a network of intelligent traps designed to monitor the population of these insects in a plantation. The proposed approach uses a simple Digital Image Processing technique to detect regions in the image that are likely the monitored pests and an Artificial Neural Network to classify the regions into the right class given their characteristics. This identification is done essentially by a Convolutional Neural Network (CNN), which learns the characteristics of the insects based on their images made from the adhesive floor inside a trap. We have trained several CNN architectures, with different configurations, through a data set of images collected in the field. We aimed to find the model with the highest precision and the lowest time needed for the classification. The best performance in classification was achieved by ResNet18, with a precision of 93.55% and 91.28% for the classification of the pests focused on this study, named Ceratitis capitata and Grapholita molesta, respectively, and a 90.72%overall accuracy. Yet, the classification must be embedded on a resource-constrained system inside the trap, then we exploited SqueezeNet, MobileNet, and MNASNet architectures to achieve a model with lesser inference time and small losses in accuracy when compared to the models we assessed. We also attempted to quantize our highest precision model to reduce even more inference time in embedded systems, which achieved a precision of 88.76% and 89.73% for C. capitata and G. molesta, respectively; notwithstanding, a decrease of roughly 2% on the overall accuracy was endured. According to the expertise of our partner company, our results are worthwhile for a real-world application, since general human laborers have a precision of about 85%. © 2022 Association for Computing Machinery.",deep learning; integrated pest management; Intelligent traps; model compression; pests,Convolutional neural networks; Deep learning; Embedded systems; Fruits; Image processing; Network architecture; Convolutional neural network; Deep learning; Digital image processing technique; Insects pests; Integrated Pest Management; Intelligent trap; Model compression; Overall accuracies; Pest; Simple++; Adhesives
Describing UI Screenshots in Natural Language,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149650525&doi=10.1145%2f3564702&partnerID=40&md5=3d8f21fc779364b6f51022609166b8d3,"Being able to describe any user interface (UI) screenshot in natural language can promote understanding of the main purpose of the UI, yet currently it cannot be accomplished with state-of-the-art captioning systems. We introduce XUI, a novel method inspired by the global precedence effect to create informative descriptions of UIs, starting with an overview and then providing fine-grained descriptions about the most salient elements. XUI builds upon computational models for topic classification, visual saliency prediction, and natural language generation (NLG). XUI provides descriptions with up to three different granularity levels that, together, describe what is in the interface and what the user can do with it. We found that XUI descriptions are highly readable, are perceived to accurately describe the UI, and score similarly to human-generated UI descriptions. XUI is available as open-source software. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Captioning; deep learning; natural language processing; visual saliency,Deep learning; Interface states; Natural language processing systems; Open source software; Open systems; Visual languages; Captioning; Deep learning; Language processing; Natural language processing; Natural languages; Novel methods; Precedence effect; Screenshots; State of the art; Visual saliency; User interfaces
No Free Lunch Theorem for Security and Utility in Federated Learning,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149742408&doi=10.1145%2f3563219&partnerID=40&md5=6bf01a7c975ca9268eba937c9f8dd375,"In a federated learning scenario where multiple parties jointly learn a model from their respective data, there exist two conflicting goals for the choice of appropriate algorithms. On one hand, private and sensitive training data must be kept secure as much as possible in the presence of semi-honest partners; on the other hand, a certain amount of information has to be exchanged among different parties for the sake of learning utility. Such a challenge calls for the privacy-preserving federated learning solution, which maximizes the utility of the learned model and maintains a provable privacy guarantee of participating parties' private data.This article illustrates a general framework that (1) formulates the trade-off between privacy loss and utility loss from a unified information-theoretic point of view, and (2) delineates quantitative bounds of the privacy-utility trade-off when different protection mechanisms including randomization, sparsity, and homomorphic encryption are used. It was shown that in general there is no free lunch for the privacy-utility trade-off, and one has to trade the preserving of privacy with a certain degree of degraded utility. The quantitative analysis illustrated in this article may serve as the guidance for the design of practical federated learning algorithms. © 2022 Association for Computing Machinery.",divergence; Federated learning; optimization; privacy-preserving computing; security; trade-off; utility,Computation theory; Economic and social effects; Information theory; Learning systems; Privacy-preserving techniques; Sensitive data; Divergence; Federated learning; Learning scenarios; No free lunch theorem; Optimisations; Privacy preserving; Privacy-preserving computing; Security; Trade off; Utility; Learning algorithms
Hierarchical Multi-agent Model for Reinforced Medical Resource Allocation with Imperfect Information,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150067428&doi=10.1145%2f3552436&partnerID=40&md5=9288c88866fbc5e4f540d17bfa0faf57,"With the advent of the COVID-19 pandemic, the shortage in medical resources became increasingly more evident. Therefore, efficient strategies for medical resource allocation are urgently needed. However, conventional rule-based methods employed by public health experts have limited capability in dealing with the complex and dynamic pandemic-spreading situation. In addition, model-based optimization methods such as dynamic programming (DP) fail to work since we cannot obtain a precise model in real-world situations most of the time. Model-free reinforcement learning (RL) is a powerful tool for decision-making; however, three key challenges exist in solving this problem via RL: (1) complex situations and countless choices for decision-making in the real world; (2) imperfect information due to the latency of pandemic spreading; and (3) limitations on conducting experiments in the real world since we cannot set up pandemic outbreaks arbitrarily. In this article, we propose a hierarchical RL framework with several specially designed components. We design a decomposed action space with a corresponding training algorithm to deal with the countless choices, ensuring efficient and real-time strategies. We design a recurrent neural network-based framework to utilize the imperfect information obtained from the environment. We also design a multi-agent voting method, which modifies the decision-making process considering the randomness during model training and, thus, improves the performance. We build a pandemic-spreading simulator based on real-world data, serving as the experimental platform. We then conduct extensive experiments. The results show that our method outperforms all baselines, which reduces infections and deaths by 14.25% on average without the multi-agent voting method and up to 15.44% with it. © 2022 Association for Computing Machinery.",COVID-19 pandemic; Hierarchical reinforcement learning; imperfect information; medical resource allocation; multi-agent reinforcement learning,Complex networks; Decision making; Dynamic programming; Learning systems; Multi agent systems; Recurrent neural networks; Reinforcement learning; Resource allocation; COVID-19 pandemic; Decisions makings; Hierarchical reinforcement learning; Imperfect information; Medical resource allocation; Multi agent; Multi-agent reinforcement learning; Real-world; Reinforcement learnings; Resources allocation; COVID-19
Domain Generalization for Activity Recognition via Adaptive Feature Fusion,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149763054&doi=10.1145%2f3552434&partnerID=40&md5=ad5223fb4abda7e665c83f55fbb7d71c,"Human activity recognition requires the efforts to build a generalizable model using the training datasets with the hope to achieve good performance in test datasets. However, in real applications, the training and testing datasets may have totally different distributions due to various reasons such as different body shapes, acting styles, and habits, damaging the model's generalization performance. While such a distribution gap can be reduced by existing domain adaptation approaches, they typically assume that the test data can be accessed in the training stage, which is not realistic. In this article, we consider a more practical and challenging scenario: domain-generalized activity recognition (DGAR) where the test dataset cannot be accessed during training. To this end, we propose Adaptive Feature Fusion for Activity Recognition (AFFAR), a domain generalization approach that learns to fuse the domain-invariant and domain-specific representations to improve the model's generalization performance. AFFAR takes the best of both worlds where domain-invariant representations enhance the transferability across domains and domain-specific representations leverage the model discrimination power from each domain. Extensive experiments on three public HAR datasets show its effectiveness. Furthermore, we apply AFFAR to a real application, i.e., the diagnosis of Children's Attention Deficit Hyperactivity Disorder (ADHD), which also demonstrates the superiority of our approach. © 2022 Association for Computing Machinery.",domain generalization; Human activity recognition; transfer learning,Deep learning; Pattern recognition; Transfer learning; Activity recognition; Adaptive features; Domain generalization; Features fusions; Generalisation; Generalization performance; Human activity recognition; Model generalization; Real applications; Transfer learning; Statistical tests
Self-supervised Discriminative Representation Learning by Fuzzy Autoencoder,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149923274&doi=10.1145%2f3555777&partnerID=40&md5=00f4956926582c4d9a5a0d9e77f91603,"Representation learning based on autoencoders has received great concern for its potential ability to capture valuable latent information. Conventional autoencoders pursue minimal reconstruction error, but in most machine learning tasks such as classification and clustering, the discrimination of feature representation is also important. To address this limitation, an enhanced self-supervised discriminative fuzzy autoencoder (FAE) is innovatively proposed, which focuses on exploring information within data to guide the unsupervised training process and enhancing feature discrimination in a self-supervised manner. In FAE, fuzzy membership is applied to provide a means of self-supervised, which allows FAE can not only utilize AE's outstanding representation learning capabilities but can also transform the original data into another space with improved discrimination. First, the objective function corresponding to FAE is proposed by reconstruction loss and clustering oriented loss simultaneously. Subsequently, Mini-Batch Gradient Descent is applied to infer the objective function and the detailed process is illustrated step by step. Finally, empirical studies on clustering tasks have demonstrated the superiority of FAE over the state of the art.  © 2022 Association for Computing Machinery.",Autoencoders; discriminative representation learning; fuzzy clustering; self-supervised learning,Fuzzy clustering; Learning systems; Supervised learning; Auto encoders; Clusterings; Discriminative representation learning; Latent information; Learning tasks; Machine-learning; Objective functions; Potential ability; Reconstruction error; Self-supervised learning; Gradient methods
Gray-Box Shilling Attack: An Adversarial Learning Approach,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128081327&doi=10.1145%2f3512352&partnerID=40&md5=524603a5cbf0e2b7980f9da4a9bd4724,"Recommender systems are essential components of many information services, which aim to find relevant items that match user preferences. Several studies have shown that shilling attacks can significantly weaken the robustness of recommender systems by injecting fake user profiles. Traditional shilling attacks focus on creating hand-engineered fake user profiles, but these profiles can be detected effortlessly by advanced detection methods. Adversarial learning, which has emerged in recent years, can be leveraged to generate powerful and intelligent attack models. To this end, in this article we explore potential risks of recommender systems and shed light on a gray-box shilling attack model based on generative adversarial networks, named GSA-GANs. Specifically, we aim to generate fake user profiles that can achieve two goals: unnoticeable and offensive. Toward these goals, there are several challenges that we need to address: (1) learning complex user behaviors from user-item rating data, and (2) adversely influencing the recommendation results without knowing the underlying recommendation algorithms. To tackle these challenges, two essential GAN modules are respectively designed to make generated fake profiles more similar to real ones and harmful to recommendation results. Experimental results on three public datasets demonstrate that the proposed GSA-GANs framework outperforms baseline models in attack effectiveness, transferability, and camouflage. In the end, we also provide several possible defensive strategies against GSA-GANs. The exploration and analysis in our work will contribute to the defense research of recommender systems. © 2022 Association for Computing Machinery.",adversarial learning; GANs; Shilling attack,Behavioral research; Fake detection; Generative adversarial networks; Information services; User profile; Advanced detections; Adversarial learning; Attack modeling; Detection methods; GAN; Grey-box; Learning approach; Shilling attack; User's preferences; User's profiles; Recommender systems
PSDF: Privacy-aware IoV Service Deployment with Federated Learning in Cloud-Edge Computing,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148070992&doi=10.1145%2f3501810&partnerID=40&md5=d59a5935487af8eb5f045e3c77f15909,"Through the collaboration of cloud and edge, cloud-edge computing allows the edge that approximates end-users undertakes those non-computationally intensive service processing of the cloud, reducing the communication overhead and satisfying the low latency requirement of Internet of Vehicle (IoV). With cloud-edge computing, the computing tasks in IoV is able to be delivered to the edge servers (ESs) instead of the cloud and rely on the deployed services of ESs for a series of processing. Due to the storage and computing resource limits of ESs, how to dynamically deploy partial services to the edge is still a puzzle. Moreover, the decision of service deployment often requires the transmission of local service requests from ESs to the cloud, which increases the risk of privacy leakage. In this article, a method for privacy-aware IoV service deployment with federated learning in cloud-edge computing, named PSDF, is proposed. Technically, federated learning secures the distributed training of deployment decision network on each ES by the exchange and aggregation of model weights, avoiding the original data transmission. Meanwhile, homomorphic encryption is adopted for the uploaded weights before the model aggregation on the cloud. Besides, a service deployment scheme based on deep deterministic policy gradient is proposed. Eventually, the performance of PSDF is evaluated by massive experiments. © 2022 Association for Computing Machinery.",Cloud-edge computing; federated learning; IoV; privacy preservation,Cryptography; Digital storage; Vehicle to vehicle communications; Cloud-edge computing; Edge clouds; Edge computing; Edge server; Federated learning; Internet of vehicle; Privacy aware; Privacy preservation; Service deployment; Vehicle service; Edge computing
"FLEE: A Hierarchical Federated Learning Framework for Distributed Deep Neural Network over Cloud, Edge, and End Device",2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137821587&doi=10.1145%2f3514501&partnerID=40&md5=1c4230176ea1ccc520e80f554393d46d,"With the development of smart devices, the computing capabilities of portable end devices such as mobile phones have been greatly enhanced. Meanwhile, traditional cloud computing faces great challenges caused by privacy-leakage and time-delay problems, there is a trend to push models down to edges and end devices. However, due to the limitation of computing resource, it is difficult for end devices to complete complex computing tasks alone. Therefore, this article divides the model into two parts and deploys them on multiple end devices and edges, respectively. Meanwhile, an early exit is set to reduce computing resource overhead, forming a hierarchical distributed architecture. In order to enable the distributed model to continuously evolve by using new data generated by end devices, we comprehensively consider various data distributions on end devices and edges, proposing a hierarchical federated learning framework FLEE, which can realize dynamical updates of models without redeploying them. Through image and sentence classification experiments, we verify that it can improve model performances under all kinds of data distributions, and prove that compared with other frameworks, the models trained by FLEE consume less global computing resource in the inference stage. © 2022 Association for Computing Machinery.",distributed neural network; early exit of inference; Federated learning,Edge computing; Image enhancement; Learning systems; Cloud-computing; Computing capability; Computing resource; Data distribution; Distributed neural networks; Early exit of inference; End-devices; Federated learning; Learning frameworks; Smart devices; Deep neural networks
Representation Learning on Variable Length and Incomplete Wearable-Sensory Time Series,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146476825&doi=10.1145%2f3531228&partnerID=40&md5=9493b487f63b33de68f5cd4b05fe26aa,"The prevalence of wearable sensors (e.g., smart wristband) is creating unprecedented opportunities to not only inform health and wellness states of individuals, but also assess and infer personal attributes, including demographic and personality attributes. However, the data captured from wearables, such as heart rate or number of steps, present two key challenges: (1) the time series is often of variable length and incomplete due to different data collection periods (e.g., wearing behavior varies by person); and (2) there is inter-individual variability to external factors like stress and environment. This article addresses these challenges and brings us closer to the potential of personalized insights about an individual, taking the leap from quantified self to qualified self. Specifically, HeartSpace proposed in this article learns embedding of the time-series data with variable length and missing values via the integration of a time-series encoding module and a pattern aggregation network. Additionally, HeartSpace implements a Siamese-triplet network to optimize representations by jointly capturing intra- and inter-series correlations during the embedding learning process. The empirical evaluation over two different real-world data presents significant performance gains over state-of-the-art baselines in a variety of applications, including user identification, personality prediction, demographics inference, job performance prediction, and sleep duration estimation.  © 2022 Association for Computing Machinery.",Representation learning; wearable-sensory time series,Population statistics; Time series; Wearable technology; Data collection; Embeddings; Health and wellness; Heart-rate; Individual variability; Personal attributes; Representation learning; Times series; Variable length; Wearable-sensory time series; Embeddings
Modeling Continuous Time Sequences with Intermittent Observations using Marked Temporal Point Processes,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145886580&doi=10.1145%2f3545118&partnerID=40&md5=de4843a577307c1083c9021c0785e19e,"A large fraction of data generated via human activities such as online purchases, health records, spatial mobility, etc. can be represented as a sequence of events over a continuous-time. Learning deep learning models over these continuous-time event sequences is a non-trivial task as it involves modeling the ever-increasing event timestamps, inter-event time gaps, event types, and the influences between different events within and across different sequences. In recent years, neural enhancements to marked temporal point processes (MTPP) have emerged as a powerful framework to model the underlying generative mechanism of asynchronous events localized in continuous time. However, most existing models and inference methods in the MTPP framework consider only the complete observation scenario i.e., the event sequence being modeled is completely observed with no missing events - an ideal setting that is rarely applicable in real-world applications. A recent line of work which considers missing events while training MTPP utilizes supervised learning techniques that require additional knowledge of missing or observed label for each event in a sequence, which further restricts its practicability as in several scenarios the details of missing events is not known a priori. In this work, we provide a novel unsupervised model and inference method for learning MTPP in presence of event sequences with missing events. Specifically, we first model the generative processes of observed events and missing events using two MTPP, where the missing events are represented as latent random variables. Then, we devise an unsupervised training method that jointly learns both the MTPP by means of variational inference. Such a formulation can effectively impute the missing data among the observed events, which in turn enhances its predictive prowess, and can identify the optimal position of missing events in a sequence. Experiments with eight real-world datasets show that IMTPP outperforms the state-of-the-art MTPP frameworks for event prediction and missing data imputation, and provides stable optimization.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Marked temporal point processes; missing data,Continuous time systems; Data mining; Deep learning; Learning systems; Continous time; Event sequence; Inference methods; Intermittent observation; Marked temporal point process; Missing data; Model method; Point process; Process framework; Time sequences; Supervised learning
FLeet: Online Federated Learning via Staleness Awareness and Performance Prediction,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142499202&doi=10.1145%2f3527621&partnerID=40&md5=9303c6d074c984d873d67a98a24938aa,"Federated learning (FL) is very appealing for its privacy benefits: essentially, a global model is trained with updates computed on mobile devices while keeping the data of users local. Standard FL infrastructures are however designed to have no energy or performance impact on mobile devices, and are therefore not suitable for applications that require frequent (online) model updates, such as news recommenders. This article presents FLeet, the first Online FL system, acting as a middleware between the Android operating system and the machine learning application. FLeet combines the privacy of Standard FL with the precision of online learning thanks to two core components: (1) I-Prof, a new lightweight profiler that predicts and controls the impact of learning tasks on mobile devices, and (2) AdaSGD, a new adaptive learning algorithm that is resilient to delayed updates. Our extensive evaluation shows that Online FL, as implemented by FLeet, can deliver a 2.3× quality boost compared to Standard FL while only consuming 0.036% of the battery per day. I-Prof can accurately control the impact of learning tasks by improving the prediction accuracy by up to 3.6× in terms of computation time, and by up to 19× in terms of energy. AdaSGD outperforms alternative FL approaches by 18.4% in terms of convergence speed on heterogeneous data.  © 2022 Association for Computing Machinery.",asynchronous gradient descent; Federated learning; mobile Android devices; online learning; profiling,E-learning; Gradient methods; Learning algorithms; Learning systems; Middleware; Quality control; Asynchronoi gradient descent; Energy impact; Federated learning; Global models; Gradient-descent; Learning tasks; Mobile android device; Online learning; Performance prediction; Profiling; Android (operating system)
Privacy-preserving Collaborative Filtering by Distributed Mediation,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146436355&doi=10.1145%2f3542950&partnerID=40&md5=90a0e63a8894a07918711e7fcc86fa31,"Recommender systems have become very influential in our everyday decision making, e.g., helping us choose a movie from a content platform, or offering us suitable products on e-commerce websites. While most vendors who utilize recommender systems rely exclusively on training data consisting of past transactions that took place through them, it would be beneficial to base recommendations on the rating data of more than one vendor. However, enlarging the training data by means of sharing information between different vendors may jeopardize the privacy of users. We devise here secure multi-party protocols that enable the practice of Collaborative Filtering (CF) in a manner that preserves the privacy of the vendors and users. Shmueli and Tassa [38] introduced privacy-preserving protocols of CF that involved a mediator; namely, an external entity that assists in performing the computations. They demonstrated the significant advantages of mediation in that context. We take here the mediation approach into the next level by using several independent mediators. Such distributed mediation maintains all of the advantages that were identified by Shmueli and Tassa, and offers additional ones, in comparison with the single-mediator protocols: stronger security and dramatically shorter runtimes. In addition, while all prior art assumed limited and unrealistic settings, in which each user can purchase any given item through only one vendor, we consider here a general and more realistic setting, which encompasses all previously considered settings, where users can choose between different competing vendors. We demonstrate the appealing performance of our protocols through extensive experimentation.  © 2022 Association for Computing Machinery.",Collaborative Filtering; distributed computing; privacy; Recommender systems; the mediated model,Collaborative filtering; Decision making; Distributed computer systems; Electronic commerce; Privacy-preserving techniques; Decisions makings; E-commerce websites; Multi-party protocols; Privacy; Privacy preserving; Privacy-preserving protocols; Sharing information; Strong securities; The mediated model; Training data; Recommender systems
Performance Evaluation of Aggregation-based Group Recommender Systems for Ephemeral Groups,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137250478&doi=10.1145%2f3542804&partnerID=40&md5=38b00029ca82f8f5dbd8ecd1ee528d33,"Recommender Systems (RecSys) provide suggestions in many decision-making processes. Given that groups of people can perform many real-world activities (e.g., a group of people attending a conference looking for a place to dine), the need for recommendations for groups has increased. A wide range of Group Recommender Systems (GRecSys) has been developed to aggregate individual preferences to group preferences. We analyze 175 studies related to GRecSys. Previous works evaluate their systems using different types of groups (sizes and cohesiveness), and most of such works focus on testing their systems using only one type of item, called Experience Goods (EG). As a consequence, it is hard to get consistent conclusions about the performance of GRecSys. We present the aggregation strategies and aggregation functions that GRecSys commonly use to aggregate group members' preferences. This study experimentally compares the performance (i.e., accuracy, ranking quality, and usefulness) using four metrics (Hit Ratio, Normalize Discounted Cumulative Gain, Diversity, and Coverage) of eight representative RecSys for group recommendations on ephemeral groups. Moreover, we use two different aggregation strategies, 10 different aggregation functions, and two different types of items on two types of datasets (EG and Search Goods (SG)) containing real-life datasets. The results show that the evaluation of GRecSys needs to use both EG and SG types of data, because the different characteristics of datasets lead to different performance. GRecSys using Singular Value Decomposition or Neural Collaborative Filtering methods work better than others. It is observed that the Average aggregation function is the one that produces better results.  © 2022 Association for Computing Machinery.",aggregation strategies; Group recommender systems; recommendation scenarios,Aggregates; Behavioral research; Collaborative filtering; Decision making; Function evaluation; Singular value decomposition; Aggregation functions; Aggregation strategy; Decision-making process; Experience goods; Group recommender systems; Performance; Performances evaluation; Real-world activities; Recommendation scenario; Search goods; Recommender systems
AggEnhance: Aggregation Enhancement by Class Interior Points in Federated Learning with Non-IID Data,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146426609&doi=10.1145%2f3544495&partnerID=40&md5=3981fe4928ace77d17e282193fe80e29,"Federated learning (FL) is a privacy-preserving paradigm for multi-institutional collaborations, where the aggregation is an essential procedure after training on the local datasets. Conventional aggregation algorithms often apply a weighted averaging of the updates generated from distributed machines to update the global model. However, while the data distributions are non-IID, the large discrepancy between the local updates might lead to a poor averaged result and a lower convergence speed, i.e., more iterations required to achieve a certain performance. To solve this problem, this article proposes a novel method named AggEnhance for enhancing the aggregation, where we synthesize a group of reliable samples from the local models and tune the aggregated result on them. These samples, named class interior points (CIPs) in this work, bound the relevant decision boundaries that ensure the performance of aggregated result. To the best of our knowledge, this is the first work to explicitly design an enhancing method for the aggregation in prevailing FL pipelines. A series of experiments on real data demonstrate that our method has noticeable improvements of the convergence in non-IID scenarios. In particular, our approach reduces the iterations by 31.87% on average for the CIFAR10 dataset and 43.90% for the PASCAL VOC dataset. Since our method does not modify other procedures of FL pipelines, it is easy to apply to most existing FL frameworks. Furthermore, it does not require additional data transmitted from the local clients to the global server, thus holding the same security level as the original FL algorithms.  © 2022 Copyright held by the owner/author(s).",aggregation enhancement; class interior points; communication; Federated learning; non-IID,Privacy-preserving techniques; Aggregation algorithms; Aggregation enhancement; Class interior point; Federated learning; IID data; Institutional collaboration; Interior point; Non-IID; Performance; Privacy preserving; Pipelines
MetaDetector: Meta Event Knowledge Transfer for Fake News Detection,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142918297&doi=10.1145%2f3532851&partnerID=40&md5=33d8a394c030e66b3f7cd235c6188aff,"The blooming of fake news on social networks has devastating impacts on society, the economy, and public security. Although numerous studies are conducted for the automatic detection of fake news, the majority tend to utilize deep neural networks to learn event-specific features for superior detection performance on specific datasets. However, the trained models heavily rely on the training datasets and are infeasible to apply to upcoming events due to the discrepancy between event distributions. Inspired by domain adaptation theories, we propose an end-to-end adversarial adaptation network, dubbed as MetaDetector, to transfer meta knowledge (event-shared features) between different events. Specifically, MetaDetector pushes the feature extractor and event discriminator to eliminate event-specific features and preserve required meta knowledge by adversarial training. Furthermore, the pseudo-event discriminator is utilized to evaluate the importance of news records in historical events to obtain partial knowledge that are discriminative for detecting fake news. Under the coordinated optimization among all the submodules, MetaDetector accurately transfers the meta knowledge of historical events to the upcoming event for fact checking. We conduct extensive experiments on two real-world datasets collected from Sina Weibo and Twitter. The experimental results demonstrate that MetaDetector outperforms the state-of-the-art methods, especially when the distribution discrepancy between events is significant.  © 2022 Association for Computing Machinery.",Fake news detection; knowledge transfer; weighted adversarial domain adaptation,Fake detection; Knowledge management; Automatic Detection; Domain adaptation; Economy security; Event-specific; Fake news detection; Knowledge transfer; Learn+; Meta-knowledge; Public security; Weighted adversarial domain adaptation; Deep neural networks
Steering-by-example for Progressive Visual Analytics,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146482916&doi=10.1145%2f3531229&partnerID=40&md5=e81583da69f36a0c8a3955c00b446488,"Progressive visual analytics allows users to interact with early, partial results of long-running computations on large datasets. In this context, computational steering is often brought up as a means to prioritize the progressive computation. This is meant to focus computational resources on data subspaces of interest so as to ensure their computation is completed before all others. Yet, current approaches to select a region of the view space and then to prioritize its corresponding data subspace either require a one-to-one mapping between view and data space, or they need to establish and maintain computationally costly index structures to trace complex mappings between view and data space. We present steering-by-example, a novel interactive steering approach for progressive visual analytics, which allows prioritizing data subspaces for the progression by generating a relaxed query from a set of selected data items. Our approach works independently of the particular visualization technique and without additional index structures. First benchmark results show that steering-by-example considerably improves Precision and Recall for prioritizing unprocessed data for a selected view region, clearly outperforming random uniform sampling.  © 2022 Copyright held by the owner/author(s).",Computational steering; interactive data exploration; progressive computation,Database systems; Mapping; Visualization; 'current; Computational resources; Computational steering; Data space; Index structure; Interactive data exploration; Large datasets; Partial results; Progressive computation; Visual analytics; Large dataset
Defending against Poisoning Backdoor Attacks on Federated Meta-learning,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148101901&doi=10.1145%2f3523062&partnerID=40&md5=4e744808beef482c0d84ecbb95237068,"Federated learning allows multiple users to collaboratively train a shared classification model while preserving data privacy. This approach, where model updates are aggregated by a central server, was shown to be vulnerable to poisoning backdoor attacks: a malicious user can alter the shared model to arbitrarily classify specific inputs from a given class. In this article, we analyze the effects of backdoor attacks on federated meta-learning, where users train a model that can be adapted to different sets of output classes using only a few examples. While the ability to adapt could, in principle, make federated learning frameworks more robust to backdoor attacks (when new training examples are benign), we find that even one-shot attacks can be very successful and persist after additional training. To address these vulnerabilities, we propose a defense mechanism inspired by matching networks, where the class of an input is predicted from the similarity of its features with a support set of labeled examples. By removing the decision logic from the model shared with the federation, the success and persistence of backdoor attacks are greatly reduced. © 2022 Association for Computing Machinery.",attention mechanism; backdoor attacks; Federated learning; matching networks; meta-learning; poisoning attacks; security and privacy,Privacy-preserving techniques; Attention mechanisms; Backdoor attack; Backdoors; Classification models; Federated learning; Matching networks; Metalearning; Multiple user; Poisoning attacks; Security and privacy; Learning systems
Budget Distributed Support Vector Machine for Non-ID Federated Learning Scenarios,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141407785&doi=10.1145%2f3539734&partnerID=40&md5=6246e1d148c921428cfd530c437b93b5,"In recent years, there has been remarkable growth in Federated Learning (FL) approaches because they have proven to be very effective in training large Machine Learning (ML) models and also serve to preserve data confidentiality, as recommended by the GDPR or other business confidentiality restrictions that may apply. Despite the success of FL, performance is greatly reduced when data is not distributed identically (non-ID) across participants, as local model updates tend to diverge from the optimal global solution and thus the model averaging procedure in the aggregator is less effective. Kernel methods such as Support Vector Machines (SVMs) have not seen an equivalent evolution in the area of privacy preserving edge computing because they suffer from inherent computational, privacy and scalability issues. Furthermore, non-linear SVMs do not naturally lead to federated schemes, since locally trained models cannot be passed to the aggregator because they reveal training data (they are built on Support Vectors), and the global model cannot be updated at every worker using gradient descent. In this article, we explore the use of a particular controlled complexity (""Budget"") Distributed SVM (BDSVM) in the FL scenario with non-ID data, which is the least favorable situation, but very common in practice. The proposed BDSVM algorithm is as follows: model weights are broadcasted to workers, which locally update some kernel Gram matrices computed according to a common architectural base and send them back to the aggregator, which finally combines them, updates the global model, and repeats the procedure until a convergence criterion is met. Experimental results using synthetic 2D datasets show that the proposed method can obtain maximal margin decision boundaries even when the data is non-ID distributed. Further experiments using real-world datasets with non-ID data distribution show that the proposed algorithm provides better performance with less communication requirements than a comparable Multilayer Perceptron (MLP) trained using FedAvg. The advantage is more remarkable for a larger number of edge devices. We have also demonstrated the robustness of the proposed method against information leakage, membership inference attacks, and situations with dropout or straggler participants. Finally, in experiments run on separate processes/machines interconnected via the cloud messaging service developed in the context of the EU-H2020 MUSKETEER project, BDSVM is able to train better models than FedAvg in about half the time.  © 2022 Association for Computing Machinery.",budget; distributed; EU-H2020; federated learning; Kernel methods; MUSKETEER; non-ID; support vector machine,Budget control; Gradient methods; Learning systems; Privacy-preserving techniques; Vectors; Budget; Distributed; EU-h2020; Federated learning; Global models; Kernel-methods; Learning scenarios; MUSKETEER; Non-ID; Support vectors machine; Support vector machines
Intrinsic Performance Influence-based Participant Contribution Estimation for Horizontal Federated Learning,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146449358&doi=10.1145%2f3523059&partnerID=40&md5=d7cd622b9537dab028b5ee3ffa9c371c,"The rapid development of modern artificial intelligence technique is mainly attributed to sufficient and high-quality data. However, in the data collection, personal privacy is at risk of being leaked. This issue can be addressed by federated learning, which is proposed to achieve efficient model training among multiple data providers without direct data access and aggregation. To encourage more parties owning high-quality data to participate in the federated learning, it is important to evaluate and reward the participant contribution in a reasonable, robust, and efficient manner. To achieve this goal, we propose a novel contribution estimation method: Intrinsic Performance Influence-based Contribution Estimation (IPICE). In particular, the class-level intrinsic performance influence is adopted as the contribution estimation criteria in IPICE, and a neural network is employed to exploit the non-linear relationship between the performance change and estimated contribution. Extensive experiments are conducted on various datasets, and the results demonstrate that IPICE is more accurate and stable than the counterpart in various data distribution settings. The computational complexity is significantly reduced in our IPICE, especially when a new party joins the federation. IPICE assigns small contributions to bad/garbage data and thus prevent them from participating and deteriorating the learning ecosystem.  © 2022 Association for Computing Machinery.",Federated learning; neural network; participant contribution estimation,Artificial intelligence techniques; Data collection; Federated learning; High quality data; Intrinsic performance; Model training; Multiple data; Neural-networks; Participant contribution estimation; Personal privacy; Learning systems
DeepExpress: Heterogeneous and Coupled Sequence Modeling for Express Delivery Prediction,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146441113&doi=10.1145%2f3526087&partnerID=40&md5=89ddd4e22f32660fdc9db3e1cb219a8a,"The prediction of express delivery sequence, i.e., modeling and estimating the volumes of daily incoming and outgoing parcels for delivery, is critical for online business, logistics, and positive customer experience, and specifically for resource allocation optimization and promotional activity arrangement. A precise estimate of consumer delivery requests has to involve sequential factors such as shopping behaviors, weather conditions, events, business campaigns, and their couplings. Despite that various methods have integrated external features to enhance the effects, extant works fail to address complex feature-sequence couplings in the following aspects: weaken the inter-dependencies when processing heterogeneous data and ignore the cumulative and evolving situation of coupling relationships. To address these issues, we propose DeepExpress - a deep-learning-based express delivery sequence prediction model, which extends the classic seq2seq framework to learn feature-sequence couplings. DeepExpress leverages an express delivery seq2seq learning, a carefully designed heterogeneous feature representation, and a novel joint training attention mechanism to adaptively handle heterogeneity issues and capture feature-sequence couplings for accurate prediction. Experimental results on real-world data demonstrate that the proposed method outperforms both shallow and deep baseline models.  © 2022 Association for Computing Machinery.",express delivery; feature-sequence coupling; heterogeneous data; Sequence prediction,Consumer behavior; Data handling; Data mining; Deep learning; Forecasting; Business logistics; Customer experience; Express delivery; Feature sequence; Feature-sequence coupling; Heterogeneous data; Online business; Resource allocation optimization; Sequence models; Sequence prediction; Couplings
Cross-Silo Federated Learning for Multi-Tier Networks with Vertical and Horizontal Data Partitioning,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146120515&doi=10.1145%2f3543433&partnerID=40&md5=7f1af52d212acb8f6544c3e49db2f54e,"We consider federated learning in tiered communication networks. Our network model consists of a set of silos, each holding a vertical partition of the data. Each silo contains a hub and a set of clients, with the silo's vertical data shard partitioned horizontally across its clients. We propose Tiered Decentralized Coordinate Descent (TDCD), a communication-efficient decentralized training algorithm for such two-tiered networks. The clients in each silo perform multiple local gradient steps before sharing updates with their hub to reduce communication overhead. Each hub adjusts its coordinates by averaging its workers' updates, and then hubs exchange intermediate updates with one another. We present a theoretical analysis of our algorithm and show the dependence of the convergence rate on the number of vertical partitions and the number of local updates. We further validate our approach empirically via simulation-based experiments using a variety of datasets and objectives.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Coordinate descent; federated learning; machine learning; stochastic gradient descent,Data handling; Gradient methods; Learning systems; Stochastic systems; Communications networks; Coordinate descent; Data partitioning; Decentralised; Federated learning; Machine-learning; Multi-tier networks; Network models; Stochastic gradient descent; Training algorithms; Machine learning
Shifting Capsule Networks from the Cloud to the Deep Edge,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144650055&doi=10.1145%2f3544562&partnerID=40&md5=984f7ae5ff65843df8d740df7d773408,"Capsule networks (CapsNets) are an emerging trend in image processing. In contrast to a convolutional neural network, CapsNets are not vulnerable to object deformation, as the relative spatial information of the objects is preserved across the network. However, their complexity is mainly related to the capsule structure and the dynamic routing mechanism, which makes it almost unreasonable to deploy a CapsNet, in its original form, in a resource-constrained device powered by a small microcontroller (MCU). In an era where intelligence is rapidly shifting from the cloud to the edge, this high complexity imposes serious challenges to the adoption of CapsNets at the very edge. To tackle this issue, we present an API for the execution of quantized CapsNets in Arm Cortex-M and RISC-V MCUs. Our software kernels extend the Arm CMSIS-NN and RISC-V PULP-NN to support capsule operations with 8-bit integers as operands. Along with it, we propose a framework to perform post-training quantization of a CapsNet. Results show a reduction in memory footprint of almost 75%, with accuracy loss ranging from 0.07% to 0.18%. In terms of throughput, our Arm Cortex-M API enables the execution of primary capsule and capsule layers with medium-sized kernels in just 119.94 and 90.60 ms, respectively (STM32H755ZIT6U, Cortex-M7 @ 480 MHz). For the GAP-8 SoC (RISC-V RV32IMCXpulp @ 170 MHz), the latency drops to 7.02 and 38.03 ms, respectively.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",capsule network quantization; Capsule networks; cloud; CMSIS-NN; edge; PULP-NN,ARM processors; Convolutional neural networks; Image processing; System-on-chip; Arm cortices; Capsule network; Capsule network quantization; CMSIS-NN; Convolutional neural network; Edge; Emerging trends; Images processing; PULP-NN; Quantisation; Complex networks
An Efficient Learning Framework for Federated XGBoost Using Secret Sharing and Distributed Optimization,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143571624&doi=10.1145%2f3523061&partnerID=40&md5=39bf1e277e54713502ed3deee8a2d87b,"XGBoost is one of the most widely used machine learning models in the industry due to its superior learning accuracy and efficiency. Targeting at data isolation issues in the big data problems, it is crucial to deploy a secure and efficient federated XGBoost (FedXGB) model. Existing FedXGB models either have data leakage issues or are only applicable to the two-party setting with heavy communication and computation overheads. In this article, a lossless multi-party federated XGB learning framework is proposed with a security guarantee, which reshapes the XGBoost's split criterion calculation process under a secret sharing setting and solves the leaf weight calculation problem by leveraging distributed optimization. Remarkably, a thorough analysis of model security is provided as well, and multiple numerical results showcase the superiority of the proposed FedXGB compared with the state-of-the-art models on benchmark datasets. © 2022 Association for Computing Machinery.",boosting; ensemble methods; gradient descent; privacy; Vertical federated learning,Cryptography; Gradient methods; Learning systems; Boosting; Distributed optimization; Efficient learning; Ensemble methods; Gradient-descent; Learning frameworks; Machine learning models; Privacy; Secret-sharing; Vertical federated learning; Optimization
Jointly Optimizing Expressional and Residual Models for 3D Facial Expression Removal,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146431178&doi=10.1145%2f3533312&partnerID=40&md5=ba636df5d02e7b3651cb2364d839bbea,"This article proposes a facial expression removal method to recover a 3D neutral face from a single 3D expressional or non-neutral face. We treat a 3D non-neutral face as the sum of its neutral one and the residual. This can be satisfied if the correspondence between 3D vertices of expressional faces and those of neutral faces is established. We propose a non-rigid deformation method to establish the correspondence between 3D faces. Then, according to algebra inequality, the minimization of a neutral face model can be replaced by the minimization of its upper bound, i.e., the errors of an expressional face model and a residual model. Thus, we co-optimize the representation errors of the latter two models and build the relationship between the representation coefficients of the two models. Given an expressional face as the input, its corresponding neutral face can be inferred by the associative representation parameters in these two models. In the testing stage, we use an iterative joint fitting scheme to obtain a more accurate recovery. Extensive experiments are conducted to evaluate our method. The results show that our method obtains considerably better performance than existing methods in terms of average root mean square errors and recognition rates, and also better visual effects. © 2022 Association for Computing Machinery.",3D face; expression removal; expressional residual; non-rigid alignment,Iterative methods; Mean square error; 3-d facial expressions; 3D faces; Expression removal; Expressional residual; Face models; Minimisation; Non-neutral; Non-rigid; Non-rigid alignment; Residual model; Errors
A Holistic Approach for Role Inference and Action Anticipation in Human Teams,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146465282&doi=10.1145%2f3531230&partnerID=40&md5=7435d6409fe73c1a76d4146e4a8e70f2,"The ability to anticipate human actions is critical to many cyber-physical systems, such as robots and autonomous vehicles. Computer vision and sensing algorithms to date have focused on extracting and predicting visual features that are explicit in the scene, such as color, appearance, actions, positions, and velocities, using video and physical measurements, such as object depth and motion. Human actions, however, are intrinsically influenced and motivated by many implicit factors such as context, human roles and interactions, past experience, and inner goals or intentions. For example, in a sport team, the team strategy, player role, and dynamic circumstances driven by the behavior of the opponents, all influence the actions of each player. This article proposes a holistic framework for incorporating visual features, as well as hidden information, such as social roles, and domain knowledge. The approach, relying on a novel dynamic Markov random field (DMRF) model, infers the instantaneous team strategy and, subsequently, the players' roles that are temporally evolving throughout the game. The results from the DMRF inference stage are then integrated with instantaneous visual features, such as individual actions and position, in order to perform holistic action anticipation using a multi-layer perceptron (MLP). The approach is demonstrated on the team sport of volleyball, by first training the DMRF and MLP offline with past videos, and, then, by applying them to new volleyball videos online. These results show that the method is able to infer the players' roles with an average accuracy of 86.99%, and anticipate future actions over a sequence of up to 46 frames with an average accuracy of 80.50%. Additionally, the method predicts the onset and duration of each action achieving a mean relative error of 14.57% and 15.67%, respectively.  © 2022 Association for Computing Machinery.",action anticipation; computer vision; domain knowledge; hidden variables; Human; Markov random field; multi-layer perceptron; role inference; sports; teams,Domain Knowledge; Dynamics; Embedded systems; Image segmentation; Markov processes; Sports; Action anticipations; Domain knowledge; Dynamic markov random fields; Hidden variable; Human; Markov Random Fields; Multilayers perceptrons; Role inference; Team; Visual feature; Computer vision
Impact of Driving Behavior on Commuter's Comfort During Cab Rides: Towards a New Perspective of Driver Rating,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146450745&doi=10.1145%2f3523063&partnerID=40&md5=a0a20fcfea437857efc86f055828cae1,"Commuter comfort in cab rides affects driver rating as well as the reputation of ride-hailing firms like Uber/Lyft. Existing research has revealed that commuter comfort not only varies at a personalized level but also is perceived differently on different trips for the same commuter. Furthermore, there are several factors, including driving behavior and driving environment, affecting the perception of comfort. Automatically extracting the perceived comfort level of a commuter due to the impact of the driving behavior is crucial for a timely feedback to the drivers, which can help them to meet the commuter's satisfaction. In light of this, we surveyed around 200 commuters who usually take such cab rides and obtained a set of features that impact comfort during cab rides. Following this, we develop a system Ridergo which collects smartphone sensor data from a commuter, extracts the spatial time series feature from the data, and then computes the level of commuter comfort on a five-point scale with respect to the driving. Ridergo uses a Hierarchical Temporal Memory model-based approach to observe anomalies in the feature distribution and then trains a multi-task learning-based neural network model to obtain the comfort level of the commuter at a personalized level. The model also intelligently queries the commuter to add new data points to the available dataset and, in turn, improve itself over periodic training. Evaluation of Ridergo on 30 participants shows that the system could provide efficient comfort score with high accuracy when the driving impacts the perceived comfort.  © 2022 Copyright held by the owner/author(s).",Commuter comfort; Hierarchical Temporal Memory; multi-task learning; sparse data,Automobile drivers; Behavioral research; Learning systems; Comfort level; Commuter comfort; Driving behaviour; Driving environment; Hierarchical temporal memory; Multitask learning; Sets of features; Sparse data; Temporal memory; Timely feedback; Neural network models
Detecting Extreme Traffic Events Via a Context Augmented Graph Autoencoder,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143898509&doi=10.1145%2f3539735&partnerID=40&md5=3af8910698401a68ff95d32eb69fbaa4,"Accurate and timely detection of large events on urban transportation networks enables informed mobility management. This work tackles the problem of extreme event detection on large-scale transportation networks using origin-destination mobility data, which is now widely available. Such data is highly structured in time and space, but high dimensional and sparse. Current multivariate time series anomaly detection methods cannot fully address these challenges. To exploit the structure of mobility data, we formulate the event detection problem in a novel way, as detecting anomalies in a set of time-dependent directed weighted graphs. We further propose a Context augmented Graph Autoencoder (Con-GAE) model to solve the problem, which leverages graph embedding and context embedding techniques to capture the spatial and temporal patterns. Con-GAE adopts an autoencoder framework and detects anomalies via semi-supervised learning. The performance of the method is assessed on several city-scale travel-time datasets from Uber Movement, New York taxis, and Chicago taxis and compared to state-of-the-art approaches. The proposed Con-GAE can achieve an improvement in the area under the curve score as large as 0.15 over the second best method. We also discuss real-world traffic anomalies detected by Con-GAE.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Anomaly detection; autoencoder; graph neural network; transportation,Directed graphs; Graph embeddings; Graph neural networks; Machine learning; Travel time; Urban transportation; Anomaly detection; Augmented graph; Auto encoders; Events detection; Extreme events; Graph neural networks; Mobility datum; Mobility management; Traffic event; Urban transportation networks; Anomaly detection
FedBERT: When Federated Learning Meets Pre-training,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135862757&doi=10.1145%2f3510033&partnerID=40&md5=3aa2c98f16e23bfae1a0810fbeaa0d92,"The fast growth of pre-trained models (PTMs) has brought natural language processing to a new era, which has become a dominant technique for various natural language processing (NLP) applications. Every user can download the weights of PTMs, then fine-tune the weights for a task on the local side. However, the pre-training of a model relies heavily on accessing a large-scale of training data and requires a vast amount of computing resources. These strict requirements make it impossible for any single client to pre-train such a model. To grant clients with limited computing capability to participate in pre-training a large model, we propose a new learning approach, FedBERT, that takes advantage of the federated learning and split learning approaches, resorting to pre-training BERT in a federated way. FedBERT can prevent sharing the raw data information and obtain excellent performance. Extensive experiments on seven GLUE tasks demonstrate that FedBERT can maintain its effectiveness without communicating to the sensitive local data of clients.  © 2022 Association for Computing Machinery.",BERT; Federated learning; NLP; pre-training,Learning systems; BERT; Fast growths; Federated learning; Language processing; Large-scales; Learning approach; Natural language processing; Natural language processing applications; Natural languages; Pre-training; Natural language processing systems
Federated Social Recommendation with Graph Neural Network,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137680057&doi=10.1145%2f3501815&partnerID=40&md5=8ba5583c9b80cbc74bfbb28f5f881f7a,"Recommender systems have become prosperous nowadays, designed to predict users' potential interests in items by learning embeddings. Recent developments of the Graph Neural Networks (GNNs) also provide recommender systems (RSs) with powerful backbones to learn embeddings from a user-item graph. However, only leveraging the user-item interactions suffers from the cold-start issue due to the difficulty in data collection. Hence, current endeavors propose fusing social information with user-item interactions to alleviate it, which is the social recommendation problem. Existing work employs GNNs to aggregate both social links and user-item interactions simultaneously. However, they all require centralized storage of the social links and item interactions of users, which leads to privacy concerns. Additionally, according to strict privacy protection under General Data Protection Regulation, centralized data storage may not be feasible in the future, urging a decentralized framework of social recommendation.As a result, we design a federated learning recommender system for the social recommendation task, which is rather challenging because of its heterogeneity, personalization, and privacy protection requirements. To this end, we devise a novel framework Fedrated Social recommendation with Graph neural network (FeSoG). Firstly, FeSoG adopts relational attention and aggregation to handle heterogeneity. Secondly, FeSoG infers user embeddings using local data to retain personalization. Last but not least, the proposed model employs pseudo-labeling techniques with item sampling to protect the privacy and enhance training. Extensive experiments on three real-world datasets justify the effectiveness of FeSoG in completing social recommendation and privacy protection. We are the first work proposing a federated learning framework for social recommendation to the best of our knowledge.  © 2022 Association for Computing Machinery.",Federated learning; graph neural network; recommender system; social recommendation,Data privacy; Digital storage; Embeddings; Graph neural networks; Centralised; Cold-start; Data collection; Embeddings; Federated learning; Graph neural networks; Learn+; Personalizations; Privacy protection; Social recommendation; Recommender systems
"Introduction to the Special Issue on the Federated Learning: Algorithms, Systems, and Applications: Part 1",2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137684013&doi=10.1145%2f3514223&partnerID=40&md5=6fd89354401d13cca985125ee437cb78,[No abstract available],,
"Preface to Federated Learning: Algorithms, Systems, and Applications: Part 2",2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148102685&doi=10.1145%2f3536420&partnerID=40&md5=240a195cf327c58dde97515cabf9d562,[No abstract available],,
Communication-Efficient Federated Learning with Adaptive Quantization,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134930872&doi=10.1145%2f3510587&partnerID=40&md5=ad0c8d0d719f66b76ee8008ba1d93be8,"Federated learning (FL) has attracted tremendous attentions in recent years due to its privacy-preserving measures and great potential in some distributed but privacy-sensitive applications, such as finance and health. However, high communication overloads for transmitting high-dimensional networks and extra security masks remain a bottleneck of FL. This article proposes a communication-efficient FL framework with an Adaptive Quantized Gradient (AQG), which adaptively adjusts the quantization level based on a local gradient’s update to fully utilize the heterogeneity of local data distribution for reducing unnecessary transmissions. In addition, client dropout issues are taken into account and an Augmented AQG is developed, which could limit the dropout noise with an appropriate amplification mechanism for transmitted gradients. Theoretical analysis and experiment results show that the proposed AQG leads to 18% to 50% of additional transmission reduction as compared with existing popular methods, including Quantized Gradient Descent (QGD) and Lazily Aggregated Quantized (LAQ) gradient-based methods without deteriorating convergence properties. Experiments with heterogenous data distributions corroborate a more significant transmission reduction compared with independent identical data distributions. The proposed AQG is robust to a client dropping rate up to 90% empirically, and the Augmented AQG manages to further improve the FL system’s communication efficiency with the presence of moderate-scale client dropouts commonly seen in practical FL scenarios. © 2022 Association for Computing Machinery.",communication efficiency; Federated learning; information compression,Data reduction; Gradient methods; Privacy-preserving techniques; Adaptive quantization; Communication efficiency; Communication overload; Data distribution; Federated learning; High dimensional networks; Information compression; Privacy preserving; Sensitive application; Transmission reduction; Efficiency
SignDS-FL: Local Differentially Private Federated Learning with Sign-based Dimension Selection,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144209751&doi=10.1145%2f3517820&partnerID=40&md5=0c17e01b7ec0118693bb7361ff1eea10,"Federated Learning (FL) [31] is a decentralized learning mechanism that has attracted increasing attention due to its achievements in computational efficiency and privacy preservation. However, recent research highlights that the original FL framework may still reveal sensitive information of clients' local data from the exchanged local updates and the global model parameters. Local Differential Privacy (LDP), as a rigorous definition of privacy, has been applied to Federated Learning to provide formal privacy guarantees and prevent potential privacy leakage. However, previous LDP-FL solutions suffer from considerable utility loss with an increase of model dimensionality. Recent work [29] proposed a two-stage framework that mitigates the dimension-dependency problem by first selecting one ""important""dimension for each local update and then perturbing the dimension value to construct the sparse privatized update. However, the framework may still suffer from utility loss because of the insufficient per-stage privacy budget and slow model convergence. In this article, we propose an improved framework, SignDS-FL, which shares the concept of dimension selection with Reference [29], but saves the privacy cost for the value perturbation stage by assigning random sign values to the selected dimensions. Besides using the single-dimension selection algorithms in Reference [29], we propose an Exponential Mechanism-based Multi-Dimension Selection algorithm that further improves model convergence and accuracy. We evaluate the framework on a number of real-world datasets with both simple logistic regression models and deep neural networks. For training logistic regression models on structured datasets, our framework yields only a 1%-2% accuracy loss in comparison to a 5%-15% decrease of accuracy for the baseline methods. For training deep neural networks on image datasets, the accuracy loss of our framework is less than and at best only. Extensive experimental results show that our framework significantly outperforms the previous LDP-FL solutions and enjoys an advanced utility-privacy balance. © 2022 Copyright held by the owner/author(s).",Federated learning; local differential privacy,Budget control; Computational efficiency; Privatization; Regression analysis; Accuracy loss; Decentralized learning; Differential privacies; Dimension selection; Federated learning; Learning mechanism; Local differential privacy; Logistic Regression modeling; Model convergence; Selection algorithm; Deep neural networks
Federated Multi-task Graph Learning,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146275390&doi=10.1145%2f3527622&partnerID=40&md5=334084a11fa4f5c22976919f9cfae952,"Distributed processing and analysis of large-scale graph data remain challenging because of the high-level discrepancy among graphs. This study investigates a novel subproblem: the distributed multi-task learning on the graph, which jointly learns multiple analysis tasks from decentralized graphs. We propose a federated multi-task graph learning (FMTGL) framework to solve the problem within a privacy-preserving and scalable scheme. Its core is an innovative data-fusion mechanism and a low-latency distributed optimization method. The former captures multi-source data relatedness and generates universal task representation for local task analysis. The latter enables the quick update of our framework with gradients sparsification and tree-based aggregation. As a theoretical result, the proposed optimization method has a convergence rate interpolates between and, up to logarithmic terms. Unlike previous studies, our work analyzes the convergence behavior with adaptive stepsize selection and non-convex assumption. Experimental results on three graph datasets verify the effectiveness and scalability of FMTGL.  © 2022 Association for Computing Machinery.",Federated learning; graph learning; multi-task learning,Data fusion; Job analysis; Distributed analysis; Distributed processing; Federated learning; Graph data; Graph learning; Large-scales; Multi tasks; Multitask learning; Optimization method; Tasks graph; Learning systems
Dynamic Probabilistic Graphical Model for Progressive Fake News Detection on Social Media Platform,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141653885&doi=10.1145%2f3523060&partnerID=40&md5=1a364d80ad8f1a9b21083f4ee87f642f,"Recently, fake news has been readily spread by massive amounts of users in social media, and automatic fake news detection has become necessary. The existing works need to prepare the overall data to perform detection, losing important information about the dynamic evolution of crowd opinions, and usually neglect the issue of uneven arrival of data in the real world. To address these issues, in this article, we focus on a kind of approach for fake news detection, namely progressive detection, which can be achieved by the dynamic Probabilistic Graphical Model. Based on the observation on real-world datasets, we adaptively improve the Kalman Filter to the Labeled Variable Dimension Kalman Filter (LVDKF) that learns two universal patterns from true and fake news, respectively, which can capture the temporal information of time-series data that arrive unevenly. It can take sequential data as input, distill the dynamic evolution knowledge regarding a post, and utilize crowd wisdom from users' responses to achieve progressive detection. Then we derive the formulas using the Forward, Backward, and EM Algorithm, and we design a dynamic detection algorithm using Bayes' theorem. Finally, we design experimental scenarios simulating progressive detection and evaluate LVDKF on two public datasets. It outperforms the baseline methods in these experimental scenarios, which indicates that it is adequate for progressive detection. © 2022 Association for Computing Machinery.",dynamic evolution; dynamic Probabilistic Graphical Model; Kalman Filter; Progressive fake news detection; uneven arrival,Fake detection; Graphic methods; Social networking (online); Dynamic evolution; Dynamic probabilistic graphical model; Probabilistic graphical models; Progressive fake news detection; Real-world; Real-world datasets; Social media; Social media platforms; Uneven arrival; Variable-dimension; Kalman filters
Semi-Synchronous Federated Learning for Energy-Efficient Training and Accelerated Convergence in Cross-Silo Settings,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136067129&doi=10.1145%2f3524885&partnerID=40&md5=6fc846e6e9e8f7b4c880ec5efead478e,"There are situations where data relevant to machine learning problems are distributed across multiple locations that cannot share the data due to regulatory, competitiveness, or privacy reasons. Machine learning approaches that require data to be copied to a single location are hampered by the challenges of data sharing. Federated Learning (FL) is a promising approach to learn a joint model over all the available data across silos. In many cases, the sites participating in a federation have different data distributions and computational capabilities. In these heterogeneous environments existing approaches exhibit poor performance: synchronous FL protocols are communication efficient, but have slow learning convergence and high energy cost; conversely, asynchronous FL protocols have faster convergence with lower energy cost, but higher communication. In this work, we introduce a novel energy-efficient Semi-Synchronous Federated Learning protocol that mixes local models periodically with minimal idle time and fast convergence. We show through extensive experiments over established benchmark datasets in the computer-vision domain as well as in real-world biomedical settings that our approach significantly outperforms previous work in data and computationally heterogeneous environments. © 2022 Association for Computing Machinery.",communication protocols; distributed execution; Federated Learning,Deep learning; Accelerated convergence; Communications protocols; Distributed execution; Energy efficient; Fast convergence; Federated learning; Heterogeneous environments; Learning protocols; Machine learning approaches; Machine learning problem; Energy efficiency
Federated Learning for Electronic Health Records,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135990329&doi=10.1145%2f3514500&partnerID=40&md5=f9a4df7083fec70b5b38ce73a154a4f2,"In data-driven medical research, multi-center studies have long been preferred over single-center ones due to a single institute sometimes not having enough data to obtain sufficient statistical power for certain hypothesis testings as well as predictive and subgroup studies. The wide adoption of electronic health records (EHRs) has made multi-institutional collaboration much more feasible. However, concerns over infrastructures, regulations, privacy, and data standardization present a challenge to data sharing across healthcare institutions. Federated Learning (FL), which allows multiple sites to collaboratively train a global model without directly sharing data, has become a promising paradigm to break the data isolation. In this study, we surveyed existing works on FL applications in EHRs and evaluated the performance of current state-of-the-art FL algorithms on two EHR machine learning tasks of significant clinical importance on a real world multi-center EHR dataset. © 2022 Copyright held by the owner/author(s).",electronic health records; Federated learning; healthcare; neural networks,E-learning; Health care; Learning systems; Machine learning; Well testing; Data driven; Electronic health; Electronic health record; Federated learning; Health records; Healthcare; Medical research; Multi-centre study; Neural-networks; Statistical power; Records management
INN: An Interpretable Neural Network for AI Incubation in Manufacturing,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147941195&doi=10.1145%2f3519313&partnerID=40&md5=b488f28582c0168e12bea93e65067890,"Both artificial intelligence (AI) and domain knowledge from human experts play an important role in manufacturing decision making. Smart manufacturing emphasizes a fully automated data-driven decision-making; however, the AI incubation process involves human experts to enhance AI systems by integrating domain knowledge for modeling, data collection and annotation, and feature extraction. Such an AI incubation process not only enhances the domain knowledge discovery but also improves the interpretability and trustworthiness of AI methods. In this article, we focus on the knowledge transfer from human experts to a supervised learning problem by learning domain knowledge as interpretable features and rules, which can be used to construct rule-based systems to support manufacturing decision making, such as process modeling and quality inspection. Although many advanced statistical and machine learning methods have shown promising modeling accuracy and efficiency, rule-based systems are still highly preferred and widely adopted due to their interpretability for human experts to comprehend. However, most of the existing rule-based systems are constructed based on deterministic human-crafted rules, whose parameters, such as thresholds of decision rules, are suboptimal. Yet the machine learning methods, such as tree models or neural networks, can learn a decision rule based structure without much interpretation or agreement with domain knowledge. Therefore, the traditional machine learning models and human experts' domain knowledge cannot be directly improved by learning from data. In this research, we propose an interpretable neural network (INN) model with a center-adjustable sigmoid activation function to efficiently optimize the rule-based systems. Using the rule-based system from domain knowledge to regulate the INN architecture not only improves the prediction accuracy with optimized parameters but also ensures the interpretability by adopting the interpretable rule-based systems from domain knowledge. The proposed INN will be effective for supervised learning problems when rule-based systems are available. The merits of the INN model are demonstrated via a simulation study and a real case study in the quality modeling of a semiconductor manufacturing process. © 2022 Association for Computing Machinery.",AI incubation; interpretable neural network; multi-layer perceptron; rule-based systems; semiconductor manufacturing,Domain Knowledge; III-V semiconductors; Knowledge management; Learning systems; Multilayer neural networks; Network layers; Semiconductor device manufacture; Artificial intelligence incubation; Decisions makings; Domain knowledge; Human expert; Interpretability; Interpretable neural network; Multilayers perceptrons; Neural-networks; Rules based systems; Semiconductor manufacturing; Decision making
A Foraging Strategy with Risk Response for Individual Robots in Adversarial Environments,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148085224&doi=10.1145%2f3514499&partnerID=40&md5=c8ebc84b5e867ee73a2eba79ad8557b0,"As an essential problem in robotics, foraging means that robots collect objects from a given environment and return them to a specified location. On many occasions, robots are required to perform foraging tasks in adversarial environments, such as battlefield rescue, where potential adversaries may damage robots with a certain probability. The longer an individual robot moves through adversarial environments, the higher the probability of being damaged by adversaries. The robot system can gain utility only when the robot brings carried objects back to a predetermined home station. Such a risk of being damaged makes returning home at different locations potentially relevant to the expected utility produced by the robot. Thus, the individual robot faces a dilemma when it responds to the potential risks in adversarial environments: whether to return the carried resources home or continue foraging tasks. In this article, two fundamental environment settings are discussed, homogeneous cases and heterogeneous cases. The former is analyzed as having both the optimal substructure property and the non-aftereffect property. Then, we present a dynamic programming (DP) algorithm that can find an optimal solution with polynomial time complexity. For the latter, it is proven that finding an optimal solution is -hard. We then propose a heuristic algorithm: A division hierarchical path planning (DHPP) algorithm that is based on the idea of dividing the foraging routes generated initially into a certain number of subroutes to dilute risks. Finally, these algorithms are extensively evaluated in simulations, concluding that in adversarial environments, they can significantly improve the productivity of an individual robot before it is damaged. © 2022 Association for Computing Machinery.",dynamic programming; heuristic; integer programming; Mobile robot foraging; path planning; robotics in adversarial environments,Heuristic algorithms; Integer programming; Motion planning; Optimal systems; Polynomial approximation; Robot programming; Adversarial environments; Essential problems; Foraging task; Heuristic; Integer Program- ming; Mobile robot foraging; Optimal solutions; Property; Risk response; Robotic in adversarial environment; Dynamic programming
Efficient Federated Matrix Factorization Against Inference Attacks,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137759717&doi=10.1145%2f3501812&partnerID=40&md5=d6b959cf2fdab5c47fdb21e746281e98,"Recommender systems typically require the revelation of users' ratings to the recommender server, which will subsequently use these ratings to provide personalized services. However, such revelations make users vulnerable to a broader set of inference attacks, allowing the recommender server to learn users' private attributes, e.g., age and gender. Therefore, in this paper, we propose an efficient federated matrix factorization method that protects users against inference attacks. The key idea is that we obfuscate one user's rating to another such that the private attribute leakage is minimized under the given distortion budget, which bounds the recommending loss and overhead of system efficiency. During the obfuscation, we apply differential privacy to control the information leakage between the users. We also adopt homomorphic encryption to protect the intermediate results during training. Our framework is implemented and tested on real-world datasets. The result shows that our method can reduce up to 16.7% of inference attack accuracy compared to using no privacy protections.  © 2022 Association for Computing Machinery.",Federated learning; inference attack; matrix factorization,Budget control; Cryptography; Matrix algebra; Recommender systems; Differential privacies; Factorization methods; Federated learning; Inference attacks; Information leakage; Learn+; Matrix factorizations; Personalized service; System efficiency; User rating; Matrix factorization
Auto-weighted Robust Federated Learning with Corrupted Data Sources,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146545321&doi=10.1145%2f3517821&partnerID=40&md5=dd88c23f73ddf1047f6e786216966416,"Federated learning provides a communication-efficient and privacy-preserving training process by enabling learning statistical models with massive participants without accessing their local data. Standard federated learning techniques that naively minimize an average loss function are vulnerable to data corruptions from outliers, systematic mislabeling, or even adversaries. In this article, we address this challenge by proposing Auto-weighted Robust Federated Learning (ARFL), a novel approach that jointly learns the global model and the weights of local updates to provide robustness against corrupted data sources. We prove a learning bound on the expected loss with respect to the predictor and the weights of clients, which guides the definition of the objective for robust federated learning. We present an objective that minimizes the weighted sum of empirical risk of clients with a regularization term, where the weights can be allocated by comparing the empirical risk of each client with the average empirical risk of the best clients. This method can downweight the clients with significantly higher losses, thereby lowering their contributions to the global model. We show that this approach achieves robustness when the data of corrupted clients is distributed differently from the benign ones. To optimize the objective function, we propose a communication-efficient algorithm based on the blockwise minimization paradigm. We conduct extensive experiments on multiple benchmark datasets, including CIFAR-10, FEMNIST, and Shakespeare, considering different neural network models. The results show that our solution is robust against different scenarios, including label shuffling, label flipping, and noisy features, and outperforms the state-of-the-art methods in most scenarios. © 2022 Association for Computing Machinery.",Auto-weighted; distributed learning; Federated learning; neural networks; robustness,Neural network models; Privacy-preserving techniques; Auto-weighted; Corrupted data; Data-source; Distributed learning; Empirical risks; Federated learning; Global models; Neural-networks; Privacy preserving; Robustness; Learning systems
CLC: A Consensus-based Label Correction Approach in Federated Learning,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139897989&doi=10.1145%2f3519311&partnerID=40&md5=7cc2a59be087897d05fe5173a3446629,"Federated learning (FL) is a novel distributed learning framework where multiple participants collaboratively train a global model without sharing any raw data to preserve privacy. However, data quality may vary among the participants, the most typical of which is label noise. The incorrect label would significantly damage the performance of the global model. In FL, the inaccessibility of raw data makes this issue more challenging. Previously published studies are limited to using a task-specific benchmark-trained model to evaluate the relevance between the benchmark dataset in the server and the local one on the participants' side. However, such approaches have failed to exploit the cooperative nature of FL itself and are not practical. This paper proposes a Consensus-based Label Correction approach (CLC) in FL, which tries to correct the noisy labels using the developed consensus method among the FL participants. The consensus-defined class-wise information is used to identify the noisy labels and correct them with pseudo-labels. Extensive experiments are conducted on several public datasets in various settings. The experimental results prove the advantage over the state-of-art methods.  © 2022 Association for Computing Machinery.",consensus mechanism; data evaluation; Federated learning,Consensus mechanism; Correction approaches; Data evaluation; Data quality; Distributed learning; Federated learning; Global models; Learning frameworks; Noisy labels; Performance
Federated Multi-view Learning for Private Medical Data Integration and Analysis,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137651421&doi=10.1145%2f3501816&partnerID=40&md5=0fca64d2c712ab7c60038e348aec7371,"Along with the rapid expansion of information technology and digitalization of health data, there is an increasing concern on maintaining data privacy while garnering the benefits in the medical field. Two critical challenges are identified: First, medical data is naturally distributed across multiple local sites, making it difficult to collectively train machine learning models without data leakage. Second, in medical applications, data are often collected from different sources and views, resulting in heterogeneity and complexity that requires reconciliation. In this article, we present a generic Federated Multi-view Learning (FedMV) framework for multi-view data leakage prevention. Specifically, we apply this framework to two types of problems based on local data availability: Vertical Federated Multi-view Learning (V-FedMV) and Horizontal Federated Multi-view Learning (H-FedMV). We experimented with real-world keyboard data collected from BiAffect study. Our results demonstrated that the proposed approach can make full use of multi-view data in a privacy-preserving way, and both V-FedMV and H-FedMV perform better than their single-view and pairwise counterparts. Besides, the framework can be easily adapted to deal with multi-view sequential data. We have developed a sequential model (S-FedMV) that takes sequence of multi-view data as input and demonstrated it experimentally. To the best of our knowledge, this framework is the first to consider both vertical and horizontal diversification in the multi-view setting, as well as their sequential federated learning.  © 2022 Copyright held by the owner/author(s).",Federated learning; medical data; multi-view learning; privacy preserving; sequential data,Horizontal wells; Medical applications; Privacy-preserving techniques; Federated learning; Health data; Medical data; Medical fields; Multi-view datum; Multi-view learning; Multi-views; Privacy preserving; Rapid expansion; Sequential data; Data integration
FedCTR: Federated Native Ad CTR Prediction with Cross-platform User Behavior Data,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125029117&doi=10.1145%2f3506715&partnerID=40&md5=44946807d50b91fbb06300e312a46308,"Native ad is a popular type of online advertisement that has similar forms with the native content displayed on websites. Native ad click-through rate (CTR) prediction is useful for improving user experience and platform revenue. However, it is challenging due to the lack of explicit user intent, and user behaviors on the platform with native ads may be insufficient to infer users' interest in ads. Fortunately, user behaviors exist on many online platforms that can provide complementary information for user-interest mining. Thus, leveraging multi-platform user behaviors is useful for native ad CTR prediction. However, user behaviors are highly privacy-sensitive, and the behavior data on different platforms cannot be directly aggregated due to user privacy concerns and data protection regulations. Existing CTR prediction methods usually require centralized storage of user behavior data for user modeling, which cannot be directly applied to the CTR prediction task with multi-platform user behaviors. In this article, we propose a federated native ad CTR prediction method named FedCTR, which can learn user-interest representations from cross-platform user behaviors in a privacy-preserving way. On each platform a local user model learns user embeddings from the local user behaviors on that platform. The local user embeddings from different platforms are uploaded to a server for aggregation, and the aggregated ones are sent to the ad platform for CTR prediction. Besides, we apply local differential privacy and differential privacy to the local and aggregated user embeddings, respectively, for better privacy protection. Moreover, we propose a federated framework for collaborative model training with distributed models and user behaviors. Extensive experiments on real-world dataset show that FedCTR can effectively leverage multi-platform user behaviors for native ad CTR prediction in a privacy-preserving manner.  © 2022 Association for Computing Machinery.",CTR Prediction; Federated Learning; Native Ad; Privacy-preserving,Behavioral research; Digital storage; Forecasting; Privacy-preserving techniques; Sensitive data; User profile; Click-through rate prediction; Clickthrough rates (CTR); Embeddings; Federated learning; Multi-platform; Native ad; Privacy preserving; Rate predictions; User behaviors; Users' interests; Embeddings
A Review on Source Code Documentation,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140250976&doi=10.1145%2f3519312&partnerID=40&md5=4dd116a1bf0359725351293767a7e9d2,Context: Coding is an incremental activity where a developer may need to understand a code before making suitable changes in the code. Code documentation is considered one of the best practices in software development but requires significant efforts from developers. Recent advances in natural language processing and machine learning have provided enough motivation to devise automated approaches for source code documentation at multiple levels. Objective: The review aims to study current code documentation practices and analyze the existing literature to provide a perspective on their preparedness to address the stated problem and the challenges that lie ahead. Methodology: We provide a detailed account of the literature in the area of automated source code documentation at different levels and critically analyze the effectiveness of the proposed approaches. This also allows us to infer gaps and challenges to address the problem at different levels. Findings: (1) The research community focused on method-level summarization. (2) Deep learning has dominated the past five years of this research field. (3) Researchers are regularly proposing bigger corpora for source code documentation. (4) Java and Python are the widely used programming languages as corpus. (5) Bilingual Evaluation Understudy is the most favored evaluation metric for the research persons. © 2022 Association for Computing Machinery.,deep learning; name prediction; software documentation; source code; Summarization; summary generation,Codes (symbols); Deep learning; Learning algorithms; Learning systems; Natural language processing systems; Software design; Best practices; Deep learning; Language processing; Machine-learning; Name prediction; Natural languages; Software documentation; Source codes; Summarization; Summary generation; Python
Crowd Flow Prediction for Irregular Regions with Semantic Graph Attention Network,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137729514&doi=10.1145%2f3501805&partnerID=40&md5=eac913eb0c65963e40d1436558769f9f,"It is essential to predict crowd flow precisely in a city, which is practically partitioned into irregular regions based on road networks and functionality. However, prior works mainly focus on grid-based crowd flow prediction, where a city is divided into many regular grids. Although Convolutional Neural Netwok (CNN) is powerful to capture spatial dependence from grid-based Euclidean data, it fails to tackle non-Euclidean data, which reflect the correlations among irregular regions. Besides, prior works fail to jointly capture the hierarchical spatio-temporal dependence from both regular and irregular regions. Finally, the correlations among regions are time-varying and functionality-related. However, the combination of dynamic and semantic attributes of regions are ignored by related works. To address the above challenges, in this article, we propose a novel model to tackle the flow prediction task for irregular regions. First, we employ CNN and Graph Neural Network (GNN) to capture micro and macro spatial dependence among grid-based regions and irregular regions, respectively. Further, we think highly of the dynamic inter-region correlations and propose a location-aware and time-aware graph attention mechanism named Semantic Graph Attention Network (Semantic-GAT), based on dynamic node attribute embedding and multi-view graph reconstruction. Extensive experimental results based on two real-life datasets demonstrate that our model outperforms 10 baselines by reducing the prediction error around 8%. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",crowd flow; graph neural networks; regions; Traffic prediction,Convolution; Flow graphs; Forecasting; Graphic methods; Semantic Web; Semantics; Traffic control; Crowd flows; Flow prediction; Graph neural networks; Grid-based; Region; Region-based; Road network; Semantic graphs; Spatial dependence; Traffic prediction; Graph neural networks
The OARF Benchmark Suite: Characterization and Implications for Federated Learning Systems,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137812167&doi=10.1145%2f3510540&partnerID=40&md5=907d76448db9bfe13eeb9b65a0254887,"This article presents and characterizes an Open Application Repository for Federated Learning (OARF), a benchmark suite for federated machine learning systems. Previously available benchmarks for federated learning (FL) have focused mainly on synthetic datasets and use a limited number of applications. OARF mimics more realistic application scenarios with publicly available datasets as different data silos in image, text, and structured data. Our characterization shows that the benchmark suite is diverse in data size, distribution, feature distribution, and learning task complexity. The extensive evaluations with reference implementations show the future research opportunities for important aspects of FL systems. We have developed reference implementations, and evaluated the important aspects of FL, including model accuracy, communication cost, throughput, and convergence time. Through these evaluations, we discovered some interesting findings such as FL can effectively increase end-to-end throughput. The code of OARF is publicly available on GitHub.1  © 2022 Association for Computing Machinery.",benchmark; dataset; Federated learning; framework; machine learning,Benchmarking; Benchmark; Benchmark suites; Dataset; Federated learning; Federated learning system; Framework; Machine learning systems; Machine-learning; Reference implementation; Synthetic datasets; Machine learning
Dynamic-Aware Federated Learning for Face Forgery Video Detection,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134101788&doi=10.1145%2f3501814&partnerID=40&md5=57bfad922c30fb0d8521fd30d43d8300,"The spread of face forgery videos is a serious threat to information credibility, calling for effective detection algorithms to identify them. Most existing methods have assumed a shared or centralized training set. However, in practice, data may be distributed on devices of different enterprises that cannot be centralized to share due to security and privacy restrictions. In this article, we propose a Federated Learning face forgery detection framework to train a global model collaboratively while keeping data on local devices. In order to make the detection model more robust, we propose a novel Inconsistency-Capture module (ICM) to capture the dynamic inconsistencies between adjacent frames of face forgery videos. The ICM contains two parallel branches. The first branch takes the whole face of adjacent frames as input to calculate a global inconsistency representation. The second branch focuses only on the inter-frame variation of critical regions to capture the local inconsistency. To the best of our knowledge, this is the first work to apply federated learning to face forgery video detection, which is trained with decentralized data. Extensive experiments show that the proposed framework achieves competitive performance compared with existing methods that are trained with centralized data, with higher-level security and privacy guarantee.  © 2022 Association for Computing Machinery.",decentralized data; DeepFake detection; dynamic inconsistency; federated learning,Adjacent frames; Centralised; Decentralised; Decentralized data; Deepfake detection; Dynamic inconsistency; Federated learning; Information credibilities; Security and privacy; Video detection
Improving Availability of Vertical Federated Learning: Relaxing Inference on Non-overlapping Data,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132195121&doi=10.1145%2f3501817&partnerID=40&md5=48ab5b4fb269ae79da1a425d056103a6,"Vertical Federated Learning (VFL) enables multiple parties to collaboratively train a machine learning model over vertically distributed datasets without data privacy leakage. However, there is a limitation of the current VFL solutions: current VFL models fail to conduct inference on non-overlapping samples during inference. This limitation seriously damages the VFL model’s availability because, in practice, overlapping samples may only take up a small portion of the whole data at each party which means a large part of inference tasks will fail. In this article, we propose a novel VFL framework which enables federated inference on non-overlapping data. Our framework regards the distributed features as privileged information which is available in the training period but disappears during inference. We distill the knowledge of such privileged features and transfer them to the parties’ local model which only processes local features. Furthermore, we adopt Oblivious Transfer (OT) to preserve data ID privacy during training and inference. Empirically, we evaluate the model on the real-world dataset collected from Criteo and Taobao. Besides, we also provide a security analysis of the proposed framework. © 2022 Association for Computing Machinery.",Availability; privacy; vertical federated learning,Availability; Learning systems; 'current; Distributed features; Large parts; Learning frameworks; Learning models; Machine learning models; Overlapping data; Privacy; Privacy leakages; Vertical federated learning; Data privacy
Self-supervised Short-text Modeling through Auxiliary Context Generation,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130285248&doi=10.1145%2f3511712&partnerID=40&md5=8864fdb9d04d6ab7dcdbc042f1f3f7df,"Short text is ambiguous and often relies predominantly on the domain and context at hand in order to attain semantic relevance. Existing classification models perform poorly on short text due to data sparsity and inadequate context. Auxiliary context, which can often provide sufficient background regarding the domain, is typically available in several application scenarios. While some of the existing works aim to leverage real-world knowledge to enhance short-text representations, they fail to place appropriate emphasis on the auxiliary context. Such models do not harness the full potential of the available context in auxiliary sources. To address this challenge, we reformulate short-text classification as a dual channel self-supervised learning problem (that leverages auxiliary context) with a generation network and a corresponding prediction model. We propose a self-supervised framework, Pseudo-Auxiliary Context generation network for Short-text Modeling (PACS), to comprehensively leverage auxiliary context and it is jointly learned with a prediction network in an end-to-end manner. Our PACS model consists of two sub-networks: a Context Generation Network (CGN) that models the auxiliary context's distribution and a Prediction Network (PN) to map the short-text features and auxiliary context distribution to the final class label. Our experimental results on diverse datasets demonstrate that PACS outperforms formidable state-of-the-art baselines. We also demonstrate the performance of our model on cold-start scenarios (where contextual information is non-existent) during prediction. Furthermore, we perform interpretability and ablation studies to analyze various representational features captured by our model and the individual contribution of its modules to the overall performance of PACS, respectively.  © 2022 held by the owner/author(s).",context learning; Self-attention; self-supervision; short-text classification,Classification (of information); Knowledge management; Semantics; Text processing; Classification models; Context distribution; Context learning; Performance; Self-attention; Self-supervision; Semantic relevance; Short text classifications; Short texts; Text modeling; Forecasting
A Computational Framework for Modeling Biobehavioral Rhythms from Mobile and Wearable Data Streams,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130274227&doi=10.1145%2f3510029&partnerID=40&md5=b76b770c8e56257dbc35f7cf06e89e99,"This paper presents a computational framework for modeling biobehavioral rhythms - the repeating cycles of physiological, psychological, social, and environmental events - from mobile and wearable data streams. The framework incorporates four main components: mobile data processing, rhythm discovery, rhythm modeling, and machine learning. We evaluate the framework with two case studies using datasets of smartphone, Fitbit, and OURA smart ring to evaluate the framework's ability to (1) detect cyclic biobehavior, (2) model commonality and differences in rhythms of human participants in the sample datasets, and (3) predict their health and readiness status using models of biobehavioral rhythms. Our evaluation demonstrates the framework's ability to generate new knowledge and findings through rigorous micro- and macro-level modeling of human rhythms from mobile and wearable data streams collected in the wild and using them to assess and predict different life and health outcomes.  © 2022 held by the owner/author(s).",biobehavioral rhythms; Computational modeling; human behavior modeling; machine learning; mental health,Behavioral research; Health; Machine components; Machine learning; Wearable technology; Biobehavioral rhythm; Case-studies; Computational framework; Computational modelling; Data stream; Human behaviour modelling; Mental health; Mobile data; Rhythm modeling; Social and environmental; Data handling
Weakly Supervised Video Object Segmentation via Dual-attention Cross-branch Fusion,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130227756&doi=10.1145%2f3506716&partnerID=40&md5=694b3b92f4d56e8253fa6958402c28d3,"Recently, concerning the challenge of collecting large-scale explicitly annotated videos, weakly supervised video object segmentation (WSVOS) using video tags has attracted much attention. Existing WSVOS approaches follow a general pipeline including two phases, i.e., a pseudo masks generation phase and a refinement phase. To explore the intrinsic property and correlation buried in the video frames, most of them focus on the later phase by introducing optical flow as temporal information to provide more supervision. However, these optical flow-based studies are greatly affected by illumination and distortion and lack consideration of the discriminative capacity of multi-level deep features. In this article, with the goal of capturing more effective temporal information and investigating a temporal information fusion strategy accordingly, we propose a unified WSVOS model by adopting a two-branch architecture with a multi-level cross-branch fusion strategy, named as dual-attention cross-branch fusion network (DACF-Net). Concretely, the two branches of DACF-Net, i.e., a temporal prediction subnetwork (TPN) and a spatial segmentation subnetwork (SSN), are used for extracting temporal information and generating predicted segmentation masks, respectively. To perform the cross-branch fusion between TPN and SSN, we propose a dual-attention fusion module that can be plugged into the SSN flexibly. We also pose a cross-frame coherence loss (CFCL) to achieve smooth segmentation results by exploiting the coherence of masks produced by TPN and SSN. Extensive experiments demonstrate the effectiveness of proposed approach compared with the state-of-the-arts on two challenging datasets, i.e., Davis-2016 and YouTube-Objects.  © 2022 Association for Computing Machinery.",attention; temporal information; video object segmentation; weakly supervised,Image segmentation; Motion compensation; Optical correlation; Attention; Fusion strategies; Large-scales; Multilevels; Subnetworks; Temporal information; Temporal prediction; Two phase; Video objects segmentations; Weakly supervised; Optical flows
Integrating Algorithmic Sampling-Based Motion Planning with Learning in Autonomous Driving,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128530724&doi=10.1145%2f3469086&partnerID=40&md5=f3284a3126527a9b74a0226af3e6026c,"Sampling-based motion planning (SBMP) is a major algorithmic trajectory planning approach in autonomous driving given its high efficiency and outstanding performance in practice. However, driving safety still calls for further refinement of SBMP. In this article we organically integrate algorithmic motion planning with learning models to improve SBMP in highway traffic scenarios from the following two perspectives. First, given the number of points to be sampled, we develop a new model to sample ""important""points for SBMP by predicting the intention of surrounding vehicles and learning the distribution of human drivers' trajectory. Second, we empirically study the relationship between the number of sample points and the environment, which is largely ignored in conventional SBMP. Then, we provide a guideline to select the appropriate number of points to be sampled under different scenarios to guarantee efficiency. The simulation experiments are conducted based on the vehicle trajectory dataset NGSIM. The results show that the proposed sampling strategy outperforms existing sampling strategies in terms of the computing time, traveling time, and smoothness of the trajectory.  © 2022 Association for Computing Machinery.",Autonomous driving; imitation learning; sampling-based motion planning; vehicle intention prediction,Efficiency; Learning systems; Motion planning; Trajectories; Algorithmics; Autonomous driving; Higher efficiency; Imitation learning; Intention predictions; Performance; Sampling strategies; Sampling-based motion planning; Trajectory Planning; Vehicle intention prediction; Autonomous vehicles
Algorithms for Trajectory Points Clustering in Location-based Social Networks,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130225343&doi=10.1145%2f3480972&partnerID=40&md5=ff9e280591d6422a19a187cd1025763f,"Recent advances in localization techniques have fundamentally enhanced social networking services, allowing users to share their locations and location-related contents. This has further increased the popularity of location-based social networks (LBSNs) and produces a huge amount of trajectories composed of continuous and complex spatio-temporal points from people's daily lives. How to accurately aggregate large-scale trajectories is an important and challenging task. Conventional clustering algorithms (e.g., k-means or k-mediods) cannot be directly employed to process trajectory data due to their serialization, triviality and redundancy. Aiming to overcome the drawbacks of traditional k-means algorithm and k-mediods, including their sensitivity to the selection of the initial k value, the cluster centers and easy convergence to a locally optimal solution, we first propose an optimized k-means algorithm (namely OKM) to obtain k optimal initial clustering centers based on the density of trajectory points. Second, because k-means is sensitive to noisy points, we propose an improved k-mediods algorithm called IKMD based on an acceptable radius r by considering users' geographic location in LBSNs. The value of k can be calculated based on r, and the optimal k points are selected as the initial clustering centers with high densities to reduce the cost of distance calculation. Thirdly, we thoroughly analyze the advantages of IKMD by comparing it with the commonly used clustering approaches through illustrative examples. Last, we conduct extensive experiments to evaluate the performance of IKMD against seven clustering approaches including the proposed optimized k-means algorithm, k-mediods algorithm, traditional density-based k-mediods algorithm and the state-of-the-arts trajectory clustering methods. The results demonstrate that IKMD significantly outperforms existing algorithms in the cost of distance calculation and the convergence speed. The methods proposed is proved to contribute to a larger effort targeted at advancing the study of intelligent trajectory data analytics.  © 2022 Association for Computing Machinery.",density-based clustering; k-mediods; location-based social networks; similarity measurement; Trajectory clustering,Data Analytics; Location; Location based services; Social networking (online); Trajectories; Density-based Clustering; Initial clustering centers; K-mean algorithms; K-means; K-mediods; Location-based social networks; Similarity measurements; Trajectories datum; Trajectory clustering; Trajectory points; K-means clustering
Utility-aware and Privacy-preserving Trajectory Synthesis Model that Resists Social Relationship Privacy Attacks,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130241855&doi=10.1145%2f3495160&partnerID=40&md5=f2311f68b46b1049462d7878ad8045c8,"For academic research and business intelligence, trajectory data has been widely collected and analyzed. Releasing trajectory data to a third party may lead to serious privacy leakage, which has spawned considerable researches on trajectory privacy protection technology. However, existing work suffers from several shortcomings. They either focus on point-based location privacy, ignoring the spatio-temporal correlations among locations within a trajectory, or they protect the privacy of each user separately without considering privacy leakage of the social relationship between trajectories of different users. Besides, they fail to balance privacy protection and data utility. Motivated by these limitations, in this article, we propose S3T-Trajectory, which is a utility-aware and privacy-preserving trajectory synthesis model that Resists social relationship privacy attacks. Specifically, we first develop a time-dependent Markov chain based on an adaptive spatio-temporal discrete grid to efficiently and accurately capture human mobility behavior. Then, we propose three mobility feature metrics from spatio-temporal, semantic, and social dimensions. On the basis of the metrics, we construct a bi-level optimization problem to accomplish the utility-aware and privacy-preserving trajectory synthesizing. The upper-level objective guarantees data utility and the lower-level optimization problems (or upper-level constraints) provides two-layer privacy protection for S3T-Trajectory, i.e., resisting location inference attacks and social relationship privacy attacks. We conduct extensive experiments on large-scale real-world datasets loc-Gowalla and loc-Brightkite. The experimental results demonstrate the effectiveness and robustness of S3TTrajectory. Compared with the baseline models, S3TTrajectory achieves between 7.8% and 23.8% performance improvement in resisting social relationship privacy attacks and achieves at least 5.19% improvement regarding data utility.  © 2022 Association for Computing Machinery.",differential privacy; Privacy-preserving data publishing; social relationship privacy attacks; spatio-temporal dataset,Behavioral research; Large dataset; Location; Markov processes; Optimization; Privacy-preserving techniques; Semantics; Social aspects; Data utilities; Differential privacies; Privacy Attacks; Privacy preserving; Privacy Preserving Data Publishing; Privacy protection; Social relationship privacy attack; Social relationships; Spatiotemporal datasets; Trajectory synthesis; Trajectories
What Can Knowledge Bring to Machine Learning? - A Survey of Low-shot Learning for Structured Data,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130275222&doi=10.1145%2f3510030&partnerID=40&md5=7343cf01476edbf909ff97f3efb01ba0,"Supervised machine learning has several drawbacks that make it difficult to use in many situations. Drawbacks include heavy reliance on massive training data, limited generalizability, and poor expressiveness of high-level semantics. Low-shot Learning attempts to address these drawbacks. Low-shot learning allows the model to obtain good predictive power with very little or no training data, where structured knowledge plays a key role as a high-level semantic representation of human. This article will review the fundamental factors of low-shot learning technologies, with a focus on the operation of structured knowledge under different low-shot conditions. We also introduce other techniques relevant to low-shot learning. Finally, we point out the limitations of low-shot learning, the prospects and gaps of industrial applications, and future research directions.  © 2022 Association for Computing Machinery.",future directions; industrial applications; low-shot learning; Machine learning; structured knowledge,Industrial research; Semantics; Future direction; High level semantics; Low-shot learning; Machine-learning; Predictive power; Semantic representation; Structured data; Structured knowledge; Supervised machine learning; Training data; Supervised learning
Doing More with Less: Overcoming Data Scarcity for POI Recommendation via Cross-Region Transfer,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130287208&doi=10.1145%2f3511711&partnerID=40&md5=ee82751ce0aa6656fb29ec078aa4dbab,"Variability in social app usage across regions results in a high skew of the quantity and the quality of check-in data collected, which in turn is a challenge for effective location recommender systems. In this article, we present Axolotl (Automated crossLocation-network Transfer Learning), a novel method aimed at transferring location preference models learned in a data-rich region to significantly boost the quality of recommendations in a data-scarce region. Axolotl predominantly deploys two channels for information transfer: (1) a meta-learning based procedure learned using location recommendation as well as social predictions, and (2) a lightweight unsupervised cluster-based transfer across users and locations with similar preferences. Both of these work together synergistically to achieve improved accuracy of recommendations in data-scarce regions without any prerequisite of overlapping users and with minimal fine-tuning. We build Axolotl on top of a twin graph-attention neural network model used for capturing the user- and location-conditioned influences in a user-mobility graph for each region. We conduct extensive experiments on 12 user mobility datasets across the US, Japan, and Germany, using three as source regions and nine of them (that have much sparsely recorded mobility data) as target regions. Empirically, we show that Axolotl achieves up to 18% better recommendation performance than the existing state-of-the-art methods across all metrics.  © 2022 held by the owner/author(s). Publication rights licensed to ACM.",Cross-region transfer learning; mobility recommendation,Recommender systems; Check-in; Cross-region transfer learning; Data scarcity; Effective location; Mobility recommendation; Network transfers; Novel methods; Preference models; Transfer learning; Users' mobility; Location
Analyzing Trajectory Gaps to Find Possible Rendezvous Region,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130268261&doi=10.1145%2f3467977&partnerID=40&md5=88b7986a2e6114d5739ec3ddf3d3223c,"Given trajectory data with gaps, we investigate methods to identify possible rendezvous regions. The problem has societal applications such as improving maritime safety and regulatory enforcement. The challenges come from two aspects. First, gaps in trajectory data make it difficult to identify regions where moving objects may have rendezvoused for nefarious reasons. Hence, traditional linear or shortest path interpolation methods may not be able to detect such activities, since objects in a rendezvous may have traveled away from their usual routes to meet. Second, user detecting a rendezvous regions involve a large number of gaps and associated trajectories, making the task computationally very expensive. In preliminary work, we proposed a more effective way of handling gaps and provided examples to illustrate potential rendezvous regions. In this article, we are providing detailed experiments with both synthetic and real-world data. Experiments on synthetic data show that the accuracy improved by 50 percent, which is substantial as compared to the baseline approach. In this article, we propose a refined algorithm Temporal Selection Search for finding a potential rendezvous region and finding an optimal temporal range to improve computational efficiency. We also incorporate two novel spatial filters: (i) a Static Ellipse Intersection Filter and (ii) a Dynamic Circle Intersection Spatial Filter. Both the baseline and proposed approaches account for every possible rendezvous pattern. We provide a theoretical evaluation of the algorithms correctness and completeness along with a time complexity analysis. Experimental results on synthetic and real-world maritime trajectory data show that the proposed approach substantially improves the area pruning effectiveness and computation time over the baseline technique. We also performed experiments based on accuracy and precision on synthetic dataset on both proposed and baseline techniques.  © 2022 held by the owner/author(s). Publication rights licensed to ACM.",Spatial data mining; time geography; trajectory mining,Beamforming; Computational efficiency; Data mining; Filtration; Maritime safety; Moving objects; Path interpolations; Real-world; Short-path; Spatial data mining; Spatial filters; Time-geography; Trajectories datum; Trajectory minings; Trajectories
Efficient and Effective Similar Subtrajectory Search: A Spatial-aware Comprehension Approach,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130232096&doi=10.1145%2f3456723&partnerID=40&md5=5313845680c79131b372fd6bbfd837ca,"Although many applications take subtrajectories as basic units for analysis, there is little research on the similar subtrajectory search problem aiming to return a portion of a trajectory (i.e., subtrajectory), which is the most similar to a query trajectory. We find that in some special cases, when a grid-based metric is used, this problem can be formulated as a reading comprehension problem, which has been studied extensively in the field of natural language processing (NLP). By this formulation, we can obtain faster models with better performance than existing methods. However, due to the difference between natural language and trajectory (e.g., spatial relationship), it is impossible to directly apply NLP models to this problem. Therefore, we propose a Similar Subtrajectory Search with a Graph Neural Networks framework. This framework contains four modules including a spatial-aware grid embedding module, a trajectory embedding module, a query-context trajectory fusion module, and a span prediction module. Specifically, in the spatial-aware grid embedding module, the spatial-based grid adjacency is constructed and delivered to the graph neural network to learn spatial-aware grid embedding. The trajectory embedding module aims to model the sequential information of trajectories. The purpose of the query-context trajectory fusion module is to fuse the information of the query trajectory to each grid of the context trajectories. Finally, the span prediction module aims to predict the start and the end of a subtrajectory for the context trajectory, which is the most similar to the query trajectory. We conduct comprehensive experiments on two real world datasets, where the proposed framework outperforms the state-of-the-art baselines consistently and significantly.  © 2022 Association for Computing Machinery.",graph neural networks; reading comprehension; Similar subtrajectory search,Data mining; Embeddings; Forecasting; Graph neural networks; Natural language processing systems; Basic units; Embeddings; FAST model; Fusion modules; Graph neural networks; Grid-based; Query context; Reading comprehension; Search problem; Similar subtrajectory search; Trajectories
NEAR: Neighborhood Edge AggregatoR for Graph Classification,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130225275&doi=10.1145%2f3506714&partnerID=40&md5=e98a876c09e5901774fba53963dfe357,"Learning graph-structured data with graph neural networks (GNNs) has been recently emerging as an important field because of its wide applicability in bioinformatics, chemoinformatics, social network analysis, and data mining. Recent GNN algorithms are based on neural message passing, which enables GNNs to integrate local structures and node features recursively. However, past GNN algorithms based on 1-hop neighborhood neural message passing are exposed to a risk of loss of information on local structures and relationships. In this article, we propose Neighborhood Edge AggregatoR (NEAR), a framework that aggregates relations between the nodes in the neighborhood via edges. NEAR, which can be orthogonally combined with Graph Isomorphism Network (GIN), gives integrated information that describes which nodes in the neighborhood are connected. Therefore, NEAR can reflect additional information of a local structure of each node beyond the nodes themselves in 1-hop neighborhood. Experimental results on multiple graph classification tasks show that our algorithm makes a good improvement over other existing 1-hop based GNN-based algorithms.  © 2022 Association for Computing Machinery.",1-dimensional Weisfeiler-Lehman test; deep neural network; Graph classification; graph neural network,Data mining; Deep neural networks; Message passing; 1-dimensional weisfeile-lehman test; Chemoinformatics; Graph classification; Graph neural networks; Graph structured data; Learning graphs; Local structure; Message-passing; Neighbourhood; Neural networks algorithms; Graph neural networks
Privacy Preservation for Trajectory Publication Based on Differential Privacy,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130228664&doi=10.1145%2f3474839&partnerID=40&md5=db9e8872e3a5b34ff1aca2f23e4b670d,"With the proliferation of location-aware devices, trajectory data have been used widely in real-life applications. However, trajectory data are often associated with sensitive labels, such as users' purchase transactions and planned activities. As such, inappropriate sharing or publishing of these data could threaten users' privacy, especially when an adversary has sufficient background knowledge about a trajectory through other data sources, such as social media (check-in tags). Though differential privacy has been used to address the privacy of trajectory data, no existing method can protect the privacy of both trajectory data and sensitive labels. In this article, we propose a comprehensive trajectory publishing algorithm with three effective procedures. First, we apply density-based clustering to determine hotspots and outliers and then blur their locations by generalization. Second, we propose a graph-based model to efficiently capture the relationship among sensitive labels and trajectory points in all records and leverage Laplace noise to achieve differential privacy. Finally, we generate and publish trajectories by traversing and updating this graph until we travel all vertexes. Our experiments on synthetic and real-life datasets demonstrate that our algorithm effectively protects the privacy of both sensitive labels and location data in trajectory publication. Compared with existing works on trajectory publishing, our algorithm can also achieve higher data utility.  © 2022 Association for Computing Machinery.",differential privacy; privacy preservation; Trajectory publishing,Data privacy; Graphic methods; Location; Publishing; Background knowledge; Data-source; Differential privacies; Location-aware; Privacy preservation; Real-life applications; Social media; Trajectories datum; Trajectory publishing; User privacy; Trajectories
Introduction to the Special Issue on Intelligent Trajectory Analytics: Part II,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130240444&doi=10.1145%2f3510021&partnerID=40&md5=871c2055747b44bfd8b55c03f81df8b8,[No abstract available],,
Supply-Demand-aware Deep Reinforcement Learning for Dynamic Fleet Management,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130219760&doi=10.1145%2f3467979&partnerID=40&md5=22b1c72d98a4242ae9e55c4a1aa52793,"Online ride-hailing platforms have reduced significantly the amounts of the time that taxis are idle and that passengers spend on waiting. As a key component of these platforms, the fleet management problem can be naturally modeled as a Markov Decision Process, which enables us to use the deep reinforcement learning. However, existing studies are proposed based on simplified problem settings that fail to model the complicated supply-dynamics and restrict the performance in the real traffic environment. In this article, we propose a supply-demand-aware deep reinforcement learning algorithm for taxi dispatching, where we use a deep Q-network with action sampling policy, called AS-DQN, to learn an optimal dispatching policy. Furthermore, we utilize a dueling network architecture, called AS-DDQN, to improve the performance of AS-DQN. Extensive experiments on real-world datasets offer insight into the performance of our model and show that it is capable of outperforming the baseline approaches.  © 2022 Association for Computing Machinery.",deep reinforcement learning; fleet management; Trajectory,Fleet operations; Learning algorithms; Markov processes; Network architecture; Reinforcement learning; Taxicabs; Dynamic fleet management; Fleet management; Learn+; Management problems; Markov Decision Processes; Performance; Real traffic; Reinforcement learning algorithms; Supply-demand; Traffic environment; Deep learning
GPSClean: A Framework for Cleaning and Repairing GPS Data,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130234239&doi=10.1145%2f3469088&partnerID=40&md5=40c7f33bac7e0d0cf508e190e60c57f8,"The rise of GPS-equipped mobile devices has led to the emergence of big trajectory data. The collected raw data usually contain errors and anomalies information caused by device failure, sensor error, and environment influence. Low-quality data fails to support application requirements and therefore raw data will be comprehensively cleaned before usage. Existing methods are suboptimal to detect GPS data errors and do the repairing. To solve the problem, we propose a framework called GPSClean to analyze the anomalies data and develop effective methods to repair the data. There are primarily four modules in GPSClean: (i) data preprocessing, (ii) data filling, (iii) data repairing, and (iv) data conversion. For (i), we propose an approach named MDSort (Maximum Disorder Sorting) to efficiently solve the issue of data disorder. For (ii), we propose a method named NNF (Nearest Neighbor Filling) to fill missing data. For (iii), we design an approach named RCSWS (Range Constraints and Sliding Window Statistics) to repair anomalies and also improve the accuracy of data repairing by mak7ing use of driving direction. We use 45 million real trajectory data to evaluate our proposal in a prototype database system SECONDO. Experimental results show that the accuracy of RCSWS is three times higher than an alternative method SCREEN and nearly an order of magnitude higher than an alternative method EWMA.  © 2022 Association for Computing Machinery.",data cleaning; data detection; data repairing; Trajectory data,Data handling; Errors; Repair; Trajectories; Data cleaning; Data repairing; Data-detection; Device failures; Error influence; GPS data; Range constraints; Sensor errors; Sliding Window; Trajectories datum; Global positioning system
Traveling Transporter Problem: Arranging a New Circular Route in a Public Transportation System Based on Heterogeneous Non-Monotonic Urban Data,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130226688&doi=10.1145%2f3510034&partnerID=40&md5=6b599be66f5acd6a058d48792907d549,"Hybrid computational intelligent systems that synergize learning-based inference models and route planning strategies have thrived in recent years. In this article, we focus on the non-monotonicity originated from heterogeneous urban data, as well as heuristics based on neural networks, and thereafter formulate the traveling transporter problem (TTP). TTP is a multi-criteria optimization problem and may be applied to the circular route deployment in public transportation. In particular, TTP aims to find an optimized route that maximizes passenger flow according to a neural-network-based inference model and minimizes the length of the route given several constraints, including must-visit stations and the requirement for additional ones. As a variation of the traveling salesman problem (TSP), we propose a framework that first recommends new stations' location while considering the herding effect between stations, and thereafter combines state-of-the-art TSP solvers and a metaheuristic named Trembling Hand, which is inspired by self-efficacy for solving TTP. Precisely, the proposed Trembling Hand enhances the spatial exploration considering the structural patterns, previous actions, and aging factors. Evaluation conducted on two real-world mass transit systems, Tainan and Chicago, shows that the proposed framework can outperform other state-of-the-art methods by securing the Pareto-optimal toward the objectives of TTP among comparative methods under various constrained settings.  © 2022 Association for Computing Machinery.",Constrained route planning; multi-criteria optimization; non-monotonicity; public transportation system; traveling salesman problem (TSP),Constrained optimization; Intelligent systems; Light rail transit; Mass transportation; Pareto principle; Transportation routes; Traveling salesman problem; Urban transportation; Constrained route planning; Inference models; Monotonics; Multi-criteria optimisation; Multi-criterion optimization; Neural-networks; Non-monotonicity; Public transportation systems; Route planning; Traveling salesman problem; Multiobjective optimization
Deep Reinforcement Learning-based Trajectory Pricing on Ride-hailing Platforms,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130307883&doi=10.1145%2f3474841&partnerID=40&md5=086d1e9975e9e0bf63910d5684d619dc,"Dynamic pricing plays an important role in solving the problems such as traffic load reduction, congestion control, and revenue improvement. Efficient dynamic pricing strategies can increase capacity utilization, total revenue of service providers, and the satisfaction of both passengers and drivers. Many proposed dynamic pricing technologies focus on short-term optimization and face poor scalability in modeling long-term goals for the limitations of solution optimality and prohibitive computation. In this article, a deep reinforcement learning framework is proposed to tackle the dynamic pricing problem for ride-hailing platforms. A soft actor-critic (SAC) algorithm is adopted in the reinforcement learning framework. First, the dynamic pricing problem is translated into a Markov Decision Process (MDP) and is set up in continuous action spaces, which is no need for the discretization of action space. Then, a new reward function is obtained by the order response rate and the KL-divergence between supply distribution and demand distribution. Experiments and case studies demonstrate that the proposed method outperforms the baselines in terms of order response rate and total revenue.  © 2022 Association for Computing Machinery.",Reinforcement learning; traffic management; Trajectory dynamic pricing,Costs; Economics; Learning algorithms; Markov processes; Reinforcement learning; Surveys; Traffic congestion; Action spaces; Congestion revenue; Dynamic pricing; Load reduction; Pricing problems; Response rate; Total revenue; Traffic loads; Traffic management; Trajectory dynamic pricing; Deep learning
Multivariate Correlation-aware Spatio-temporal Graph Convolutional Networks for Multi-scale Traffic Prediction,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130305224&doi=10.1145%2f3469087&partnerID=40&md5=2ddde50d616d2eb4a25c5f46a115b5c5,"Traffic flow prediction based on vehicle trajectories collected from the installed GPS devices is critically important to Intelligent Transportation Systems (ITS). One limitation of existing traffic prediction models is that they mostly focus on predicting road-segment level traffic conditions, which can be considered as a fine-grained prediction. In many scenarios, however, a coarse-grained prediction, such as predicting the traffic flows among different urban areas covering multiple road links, is also required to help government have a better understanding on traffic conditions from the macroscopic point of view. This is especially useful in the applications of urban planning and public transportation planning. Another limitation is that the correlations among different types of traffic-related features are largely ignored. For example, the traffic flow and traffic speed are usually negatively correlated. Existing works regard these traffic-related features as independent features without considering their correlations. In this article, we for the first time study the novel problem of multivariate correlation-aware multi-scale traffic flow predicting, and we propose a feature correlation-aware spatio-temporal graph convolutional networks named MC-STGCN to effectively address it. Specifically, given a road graph, we first construct a coarse-grained road graph based on both the topology closeness and the traffic flow similarity among the nodes (road links). Then a cross-scale spatial-temporal feature learning and fusion technique is proposed for dealing with both the fine- and coarse-grained traffic data. In the spatial domain, a cross-scale GCN is proposed to learn the multi-scale spatial features jointly and fuse them together. In the temporal domain, a cross-scale temporal network that is composed of a hierarchical attention is designed for effectively capturing intra- and inter-scale temporal correlations. To effectively capture the feature correlations, a feature correlation learning component is also designed. Finally, a structural constraint is introduced to make the predictions on the two scale traffic data consistent. We conduct extensive evaluations over two real traffic datasets, and the results demonstrate the superior performance of the proposal on both fine- and coarse-grained traffic predictions.  © 2022 Association for Computing Machinery.",Graph Convolutional Networks; spatio-temporal data; Traffic prediction,Convolution; Convolutional neural networks; Flow graphs; Forecasting; Graph neural networks; Intelligent systems; Intelligent vehicle highway systems; Motor transportation; Roads and streets; Urban planning; Urban transportation; Coarse-grained; Convolutional networks; Feature correlation; Graph convolutional network; Multi-scales; Multivariate correlation; Spatio-temporal data; Spatio-temporal graphs; Traffic flow; Traffic prediction; Graphic methods
Toward Scalable and Privacy-preserving Deep Neural Network via Algorithmic-Cryptographic Co-design,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137148147&doi=10.1145%2f3501809&partnerID=40&md5=8f115717fd5c9f1efe0f9d1d6df81cd9,"Deep Neural Networks (DNNs) have achieved remarkable progress in various real-world applications, especially when abundant training data are provided. However, data isolation has become a serious problem currently. Existing works build privacy-preserving DNN models from either algorithmic perspective or cryptographic perspective. The former mainly splits the DNN computation graph between data holders or between data holders and server, which demonstrates good scalability but suffers from accuracy loss and potential privacy risks. In contrast, the latter leverages time-consuming cryptographic techniques, which has strong privacy guarantee but poor scalability. In this article, we propose SPNN-a Scalable and Privacy-preserving deep Neural Network learning framework, from an algorithmic-cryptographic co-perspective. From algorithmic perspective, we split the computation graph of DNN models into two parts, i.e., the private-data-related computations that are performed by data holders and the rest heavy computations that are delegated to a semi-honest server with high computation ability. From cryptographic perspective, we propose using two types of cryptographic techniques, i.e., secret sharing and homomorphic encryption, for the isolated data holders to conduct private-data-related computations privately and cooperatively. Furthermore, we implement SPNN in a decentralized setting and introduce user-friendly APIs. Experimental results conducted on real-world datasets demonstrate the superiority of our proposed SPNN.  © 2022 Association for Computing Machinery.",deep neural network; homomorphic encryption; Privacy-preserving; secret sharing,Neural network models; Privacy-preserving techniques; Scalability; Algorithmics; Co-designs; Cryptographic techniques; CryptoGraphics; Ho-momorphic encryptions; Homomorphic-encryptions; Neural network model; Privacy preserving; Private data; Secret-sharing; Deep neural networks
Federated Dynamic Graph Neural Networks with Secure Aggregation for Video-based Distributed Surveillance,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132483641&doi=10.1145%2f3501808&partnerID=40&md5=21c0b3eedcce6614da9608c2d96095c9,"Distributed surveillance systems have the ability to detect, track, and snapshot objects moving around in a certain space. The systems generate video data from multiple personal devices or street cameras. Intelligent video-analysis models are needed to learn dynamic representation of the objects for detection and tracking. Can we exploit the structural and dynamic information without storing the spatiotemporal video data at a central server that leads to a violation of user privacy? In this work, we introduce Federated Dynamic Graph Neural Network (Feddy), a distributed and secured framework to learn the object representations from graph sequences: (1) It aggregates structural information from nearby objects in the current graph as well as dynamic information from those in the previous graph. It uses a self-supervised loss of predicting the trajectories of objects. (2) It is trained in a federated learning manner. The centrally located server sends the model to user devices. Local models on the respective user devices learn and periodically send their learning to the central server without ever exposing the user's data to server. (3) Studies showed that the aggregated parameters could be inspected though decrypted when broadcast to clients for model synchronizing, after the server performed a weighted average. We design an appropriate aggregation mechanism of secure aggregation primitives that can protect the security and privacy in federated learning with scalability. Experiments on four video camera datasets as well as simulation demonstrate that Feddy achieves great effectiveness and security.  © 2022 Association for Computing Machinery.",distributed surveillance; federated learning; Graph neural network; secure aggregation,Graph neural networks; Learning systems; Monitoring; Network security; Object detection; Video cameras; Video recording; Central servers; Distributed surveillance; Dynamic graph; Dynamic information; Federated learning; Graph neural networks; Learn+; Secure aggregations; Structural information; Video data; Security systems
Federated Learning for Healthcare: Systematic Review and Architecture Proposal,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129082709&doi=10.1145%2f3501813&partnerID=40&md5=798f3a1bbed5b10c01c704e0cb04a350,"The use of machine learning (ML) with electronic health records (EHR) is growing in popularity as a means to extract knowledge that can improve the decision-making proceß in healthcare. Such methods require training of high-quality learning models based on diverse and comprehensive datasets, which are hard to obtain due to the sensitive nature of medical data from patients. In this context, federated learning (FL) is a methodology that enables the distributed training of machine learning models with remotely hosted datasets without the need to accumulate data and, therefore, compromise it. FL is a promising solution to improve ML-based systems, better aligning them to regulatory requirements, improving trustworthineß and data sovereignty. However, many open questions must be addreßed before the use of FL becomes widespread. This article aims at presenting a systematic literature review on current research about FL in the context of EHR data for healthcare applications. Our analysis highlights the main research topics, proposed solutions, case studies, and respective ML methods. Furthermore, the article discußes a general architecture for FL applied to healthcare data based on the main insights obtained from the literature review. The collected literature corpus indicates that there is extensive research on the privacy and confidentiality aspects of training data and model sharing, which is expected given the sensitive nature of medical data. Studies also explore improvements to the aggregation mechanisms required to generate the learning model from distributed contributions and case studies with different types of medical data.  © 2022 Aßociation for Computing Machinery.",Electronic health records; federated learning; systematic review,E-learning; eHealth; Machine learning; Records management; Case-studies; Decisions makings; Electronic health; Electronic health record; Federated learning; Health records; Learning models; Machine-learning; Medical data; Systematic Review; Decision making
GTG-Shapley: Efficient and Accurate Participant Contribution Evaluation in Federated Learning,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125789590&doi=10.1145%2f3501811&partnerID=40&md5=530842c7ee26474e0e962b8973553629,"Federated Learning (FL) bridges the gap between collaborative machine learning and preserving data privacy. To sustain the long-term operation of an FL ecosystem, it is important to attract high-quality data owners with appropriate incentive schemes. As an important building block of such incentive schemes, it is essential to fairly evaluate participants' contribution to the performance of the final FL model without exposing their private data. Shapley Value (SV)-based techniques have been widely adopted to provide a fair evaluation of FL participant contributions. However, existing approaches incur significant computation costs, making them difficult to apply in practice. In this article, we propose the Guided Truncation Gradient Shapley (GTG-Shapley) approach to address this challenge. It reconstructs FL models from gradient updates for SV calculation instead of repeatedly training with different combinations of FL participants. In addition, we design a guided Monte Carlo sampling approach combined with within-round and between-round truncation to further reduce the number of model reconstructions and evaluations required. This is accomplished through extensive experiments under diverse realistic data distribution settings. The results demonstrate that GTG-Shapley can closely approximate actual Shapley values while significantly increasing computational efficiency compared with the state-of-the-art, especially under non-i.i.d. settings.  © 2022 Association for Computing Machinery.",contribution assessment; Federated learning; Shapley value,Game theory; Monte Carlo methods; Privacy-preserving techniques; Building blockes; Contribution assessment; Federated learning; High quality data; Incentive schemes; Learning ecosystems; Learning models; Machine-learning; Shapley; Shapley value; Computational efficiency
FedCVT: Semi-supervised Vertical Federated Learning with Cross-view Training,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136540040&doi=10.1145%2f3510031&partnerID=40&md5=808e53caa8957fa4d21ec3f5dda8e907,"Federated learning allows multiple parties to build machine learning models collaboratively without exposing data. In particular, vertical federated learning (VFL) enables participating parties to build a joint machine learning model based upon distributed features of aligned samples. However, VFL requires all parties to share a sufficient amount of aligned samples. In reality, the set of aligned samples may be small, leaving the majority of the non-aligned data unused. In this article, we propose Federated Cross-view Training (FedCVT), a semi-supervised learning approach that improves the performance of the VFL model with limited aligned samples. More specifically, FedCVT estimates representations for missing features, predicts pseudo-labels for unlabeled samples to expand the training set, and trains three classifiers jointly based upon different views of the expanded training set to improve the VFL model’s performance. FedCVT does not require parties to share their original data and model parameters, thus preserving data privacy. We conduct experiments on NUS-WIDE, Vehicle, and CIFAR10 datasets. The experimental results demonstrate that FedCVT significantly outperforms vanilla VFL that only utilizes aligned samples. Finally, we perform ablation studies to investigate the contribution of each component of FedCVT to the performance of FedCVT. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cross-view training; semi-supervised learning; Vertical federated learning,Learning algorithms; Privacy-preserving techniques; Supervised learning; Aligned samples; Cross-view training; Learning models; Machine learning models; Model-based OPC; Performance; Semi-supervised; Semi-supervised learning; Training sets; Vertical federated learning; Classification (of information)
Federated Learning for Personalized Humor Recognition,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137629506&doi=10.1145%2f3511710&partnerID=40&md5=aa098eabad44d9008fdf90a0abf4345b,"Computational understanding of humor is an important topic under creative language understanding and modeling. It can play a key role in complex human-AI interactions. The challenge here is that human perception of humorous content is highly subjective. The same joke may receive different funniness ratings from different readers. This makes it highly challenging for humor recognition models to achieve personalization in practical scenarios. Existing approaches are generally designed based on the assumption that users have a consensus on whether a given text is humorous or not. Thus, they cannot handle diverse humor preferences well. In this article, we propose the FedHumor approach for the recognition of humorous content in a personalized manner through Federated Learning (FL). Extending a pre-trained language model, FedHumor guides the fine-tuning process by considering diverse distributions of humor preferences from individuals. It incorporates a diversity adaptation strategy into the FL paradigm to train a personalized humor recognition model. To the best of our knowledge, FedHumor is the first text-based personalized humor recognition model through federated learning. Extensive experiments demonstrate the advantage of FedHumor in recognizing humorous texts compared to nine state-of-the-art humor recognition approaches with superior capability for handling the diversity in humor labels produced by users with diverse preferences.  © 2022 Association for Computing Machinery.",natural language understanding; personalization; Subjectivity,Character recognition; Learning systems; User profile; Adaptation strategies; Creatives; Fine tuning; Human perception; Language model; Language understanding; Natural language understanding; Personalizations; Recognition models; Subjectivity; Modeling languages
GRNN: Generative Regression Neural Network-A Data Leakage Attack for Federated Learning,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137807023&doi=10.1145%2f3510032&partnerID=40&md5=071b9f39942bcc4842c4af564c35e0cf,"Data privacy has become an increasingly important issue in Machine Learning (ML), where many approaches have been developed to tackle this challenge, e.g., cryptography (Homomorphic Encryption (HE), Differential Privacy (DP)) and collaborative training (Secure Multi-Party Computation (MPC), Distributed Learning, and Federated Learning (FL)). These techniques have a particular focus on data encryption or secure local computation. They transfer the intermediate information to the third party to compute the final result. Gradient exchanging is commonly considered to be a secure way of training a robust model collaboratively in Deep Learning (DL). However, recent researches have demonstrated that sensitive information can be recovered from the shared gradient. Generative Adversarial Network (GAN), in particular, has shown to be effective in recovering such information. However, GAN based techniques require additional information, such as class labels that are generally unavailable for privacy-preserved learning. In this article, we show that, in the FL system, image-based privacy data can be easily recovered in full from the shared gradient only via our proposed Generative Regression Neural Network (GRNN). We formulate the attack to be a regression problem and optimize two branches of the generative model by minimizing the distance between gradients. We evaluate our method on several image classification tasks. The results illustrate that our proposed GRNN outperforms state-of-the-art methods with better stability, stronger robustness, and higher accuracy. It also has no convergence requirement to the global FL model. Moreover, we demonstrate information leakage using face re-identification. Some defense strategies are also discussed in this work.  © 2022 Association for Computing Machinery.",data privacy; Federated learning; gradient leakage attack; image generation,Cryptography; Deep learning; Generative adversarial networks; Learning systems; Neural networks; Recovery; Regression analysis; Collaborative training; Data leakage; Differential privacies; Federated learning; Gradient leakage attack; Ho-momorphic encryptions; Homomorphic-encryptions; Image generations; Machine-learning; Regression neural networks; Data privacy
Graph Sequence Neural Network with an Attention Mechanism for Traffic Speed Prediction,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129490445&doi=10.1145%2f3470889&partnerID=40&md5=b5718186ba7f6dd2ef89c24aaefa598c,"Recent years have witnessed the emerging success of Graph Neural Networks (GNNs) for modeling graphical data. A GNN can model the spatial dependencies of nodes in a graph based on message passing through node aggregation. However, in many application scenarios, these spatial dependencies can change over time, and a basic GNN model cannot capture these changes. In this article, we propose a Graph Sequence neural network with an Attention mechanism (GSeqAtt) for processing graph sequences. More specifically, two attention mechanisms are combined: a horizontal mechanism and a vertical mechanism. GTransformer, which is a horizontal attention mechanism for handling time series, is used to capture the correlations between graphs in the input time sequence. The vertical attention mechanism, a Graph Network (GN) block structure with an attention mechanism (GNAtt), acts within the graph structure in each frame of the time series. Experiments show that our proposed model is able to handle information propagation for graph sequences accurately and efficiently. Moreover, results on real-world data from three road intersections show that our GSeqAtt outperforms state-of-the-art baselines on the traffic speed prediction task. © 2022 Association for Computing Machinery.",Graph neural network; self-attention; traffic speed prediction,Backpropagation; Forecasting; Graph neural networks; Graph theory; Graphic methods; Message passing; Time series; Traffic control; Attention mechanisms; Graph neural networks; Graph sequences; Neural-networks; Self-attention; Spatial dependencies; Speed prediction; Times series; Traffic speed; Traffic speed prediction; Speed
Location-Centered House Price Prediction: A Multi-Task Learning Approach,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129518771&doi=10.1145%2f3501806&partnerID=40&md5=22ecfcd3a45ef0cc9cbe2719cf42523a,"Accurate house prediction is of great significance to various real estate stakeholders such as house owners, buyers, and investors. We propose a location-centered prediction framework that differs from existing work in terms of data profiling and prediction model. Regarding data profiling, we make an important observation as follows - besides the in-house features such as floor area, the location plays a critical role in house price prediction. Unfortunately, existing work either overlooked it or had a coarse grained measurement of locations. Thereby, we define and capture a fine-grained location profile powered by a diverse range of location data sources, including transportation profile, education profile, suburb profile based on census data, and facility profile. Regarding the choice of prediction model, we observe that a variety of approaches either consider the entire data for modeling, or split the entire house data and model each partition independently. However, such modeling ignores the relatedness among partitions, and for all prediction scenarios, there may not be sufficient training samples per partition for the latter approach. We address this problem by conducting a careful study of exploiting the Multi-Task Learning (MTL) model. Specifically, we map the strategies for splitting the entire house data to the ways the tasks are defined in MTL, and select specific MTL-based methods with different regularization terms to capture and exploit the relatedness among tasks. Based on real-world house transaction data collected in Melbourne, Australia, we design extensive experimental evaluations, and the results indicate a significant superiority of MTL-based methods over state-of-the-art approaches. Meanwhile, we conduct an in-depth analysis on the impact of task definitions and method selections in MTL on the prediction performance, and demonstrate that the impact of task definitions on prediction performance far exceeds that of method selections. © 2022 Association for Computing Machinery.",linear regression; multi-task learning; multiple auxiliary information; Price prediction; real estate,Data mining; Forecasting; Houses; Linearization; Location; Population statistics; Auxiliary information; Data profiling; House's prices; Learning-based methods; Multiple auxiliary information; Multitask learning; Prediction modelling; Prediction performance; Price prediction; Real-estates; Learning systems
COVID-GAN+: Estimating Human Mobility Responses to COVID-19 through Spatio-temporal Generative Adversarial Networks with Enhanced Features,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129525571&doi=10.1145%2f3481617&partnerID=40&md5=3a3157944a2626b35d140fd126a2b87b,"Estimating human mobility responses to the large-scale spreading of the COVID-19 pandemic is crucial, since its significance guides policymakers to give Non-pharmaceutical Interventions, such as closure or reopening of businesses. It is challenging to model due to complex social contexts and limited training data. Recently, we proposed a conditional generative adversarial network (COVID-GAN) to estimate human mobility response under a set of social and policy conditions integrated from multiple data sources. Although COVID-GAN achieves a good average estimation accuracy under real-world conditions, it produces higher errors in certain regions due to the presence of spatial heterogeneity and outliers. To address these issues, in this article, we extend our prior work by introducing a new spatio-temporal deep generative model, namely, COVID-GAN+. COVID-GAN+ deals with the spatial heterogeneity issue by introducing a new spatial feature layer that utilizes the local Moran statistic to model the spatial heterogeneity strength in the data. In addition, we redesign the training objective to learn the estimated mobility changes from historical average levels to mitigate the effects of spatial outliers. We perform comprehensive evaluations using urban mobility data derived from cell phone records and census data. Results show that COVID-GAN+ can better approximate real-world human mobility responses than prior methods, including COVID-GAN. © 2022 Association for Computing Machinery.",conditional generative adversarial networks; COVID-19; Mobility estimation,Population statistics; Condition; Conditional generative adversarial network; COVID-19; Human mobility; Large-scales; Mobility estimation; Real-world; Spatial heterogeneity; Spatial outlier; Spatio-temporal; Generative adversarial networks
Introduction to the Special Issue on Deep Learning for Spatio-Temporal Data: Part 2,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129523860&doi=10.1145%2f3510023&partnerID=40&md5=c7fc840d989ab9d67651ee957dcad4d2,[No abstract available],,
Deep Spatio-temporal Adaptive 3D Convolutional Neural Networks for Traffic Flow Prediction,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129510140&doi=10.1145%2f3510829&partnerID=40&md5=2a7562508d6593f1bf690d6a80da4992,"Traffic flow prediction is the upstream problem of path planning, intelligent transportation system, and other tasks. Many studies have been carried out on the traffic flow prediction of the spatio-temporal network, but the effects of spatio-temporal flexibility (historical data of the same type of time intervals in the same location will change flexibly) and spatio-temporal correlation (different road conditions have different effects at different times) have not been considered at the same time. We propose the Deep Spatio-temporal Adaptive 3D Convolution Neural Network (ST-A3DNet), which is a new scheme to solve both spatio-temporal correlation and flexibility, and consider spatio-temporal complexity (complex external factors, such as weather and holidays). Different from other traffic forecasting models, ST-A3DNet captures the spatio-temporal relationship at the same time through the Adaptive 3D convolution module, assigns different weights flexibly according to the influence of historical data, and obtains the impact of external factors on the flow through the ex-mask module. Considering the holidays and weather conditions, we train our model for experiments in Xi'an and Chengdu. We evaluate the ST-A3DNet and the results show that we have better results than the other 11 baselines. © 2022 Association for Computing Machinery.",Convolutional neural networks; spatial-temporal information; traffic prediction,Complex networks; Convolution; Convolutional neural networks; Deep neural networks; Intelligent systems; Intelligent vehicle highway systems; Traffic control; Weather forecasting; Convolution neural network; Convolutional neural network; Historical data; Spatial temporals; Spatial-temporal information; Spatio-temporal; Temporal adaptive; Temporal information; Traffic flow prediction; Traffic prediction; Motion planning
Predicting Citywide Crowd Dynamics at Big Events: A Deep Learning System,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129481790&doi=10.1145%2f3472300&partnerID=40&md5=2de790f086a2c88d2d0c086551075406,"Event crowd management has been a significant research topic with high social impact. When some big events happen such as an earthquake, typhoon, and national festival, crowd management becomes the first priority for governments (e.g., police) and public service operators (e.g., subway/bus operator) to protect people's safety or maintain the operation of public infrastructures. However, under such event situations, human behavior will become very different from daily routines, which makes prediction of crowd dynamics at big events become highly challenging, especially at a citywide level. Therefore in this study, we aim to extract the ""deep""trend only from the current momentary observations and generate an accurate prediction for the trend in the short future, which is considered to be an effective way to deal with the event situations. Motivated by these, we build an online system called DeepUrbanEvent, which can iteratively take citywide crowd dynamics from the current one hour as input and report the prediction results for the next one hour as output. A novel deep learning architecture built with recurrent neural networks is designed to effectively model these highly complex sequential data in an analogous manner to video prediction tasks. Experimental results demonstrate the superior performance of our proposed methodology to the existing approaches. Lastly, we apply our prototype system to multiple big real-world events and show that it is highly deployable as an online crowd management system. © 2022 Association for Computing Machinery.",application and system; Crowd management; deep learning; ubiquitous and mobile computing,Behavioral research; Dynamics; Iterative methods; Recurrent neural networks; Ubiquitous computing; 'current; Application and system; Bus operators; Crowd dynamics; Crowd managements; Deep learning; Mobile-computing; Public services; Research topics; Social impact; Forecasting
Weakly Supervised Spatial Deep Learning for Earth Image Segmentation Based on Imperfect Polyline Labels,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129473019&doi=10.1145%2f3480970&partnerID=40&md5=519448e2ff2ed869fb9b1e07dab2f294,"In recent years, deep learning has achieved tremendous success in image segmentation for computer vision applications. The performance of these models heavily relies on the availability of large-scale high-quality training labels (e.g., PASCAL VOC 2012). Unfortunately, such large-scale high-quality training data are often unavailable in many real-world spatial or spatiotemporal problems in earth science and remote sensing (e.g., mapping the nationwide river streams for water resource management). Although extensive efforts have been made to reduce the reliance on labeled data (e.g., semi-supervised or unsupervised learning, few-shot learning), the complex nature of geographic data such as spatial heterogeneity still requires sufficient training labels when transferring a pre-trained model from one region to another. On the other hand, it is often much easier to collect lower-quality training labels with imperfect alignment with earth imagery pixels (e.g., through interpreting coarse imagery by non-expert volunteers). However, directly training a deep neural network on imperfect labels with geometric annotation errors could significantly impact model performance. Existing research that overcomes imperfect training labels either focuses on errors in label class semantics or characterizes label location errors at the pixel level. These methods do not fully incorporate the geometric properties of label location errors in the vector representation. To fill the gap, this article proposes a weakly supervised learning framework to simultaneously update deep learning model parameters and infer hidden true vector label locations. Specifically, we model label location errors in the vector representation to partially reserve geometric properties (e.g., spatial contiguity within line segments). Evaluations on real-world datasets in the National Hydrography Dataset (NHD) refinement application illustrate that the proposed framework outperforms baseline methods in classification accuracy. © 2022 Association for Computing Machinery.",Deep learning; earth imagery segmentation; imperfect labels; weakly supervised learning,Classification (of information); Deep neural networks; Errors; Geometry; Image segmentation; Information management; Location; Remote sensing; Semantics; Supervised learning; Water management; Water quality; Deep learning; Earth imagery segmentation; Geometric properties; High quality; Images segmentations; Imperfect label; Large-scales; Location errors; Quality training; Weakly supervised learning; Pixels
A Survey on Text Classification: From Traditional to Deep Learning,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129552120&doi=10.1145%2f3495162&partnerID=40&md5=909b08e29ab44a6092d1d86851175ca5,"Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021, focusing on models from traditional models to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area. © 2022 Association for Computing Machinery.",challenges; Deep learning; evaluation metrics; text classification; traditional models,Classification (of information); Deep learning; Natural language processing systems; Petroleum reservoir evaluation; Text processing; Benchmark datasets; Challenge; Comprehensive comparisons; Deep learning; Evaluation metrics; Feature extraction and classification; State-of-the-art approach; Technical development; Text classification; Traditional models; Surveys
Earth Imagery Segmentation on Terrain Surface with Limited Training Labels: A Semi-supervised Approach based on Physics-Guided Graph Co-Training,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129465733&doi=10.1145%2f3481043&partnerID=40&md5=0996ffe2a0215c6b3d1dc665416846ea,"Given earth imagery with spectral features on a terrain surface, this paper studies surface segmentation based on both explanatory features and surface topology. The problem is important in many spatial and spatiotemporal applications such as flood extent mapping in hydrology. The problem is uniquely challenging for several reasons: first, the size of earth imagery on a terrain surface is often much larger than the input of popular deep convolutional neural networks; second, there exists topological structure dependency between pixel classes on the surface, and such dependency can follow an unknown and non-linear distribution; third, there are often limited training labels. Existing methods for earth imagery segmentation often divide the imagery into patches and consider the elevation as an additional feature channel. These methods do not fully incorporate the spatial topological structural constraint within and across surface patches and thus often show poor results, especially when training labels are limited. Existing methods on semi-supervised and unsupervised learning for earth imagery often focus on learning representation without explicitly incorporating surface topology. In contrast, we propose a novel framework that explicitly models the topological skeleton of a terrain surface with a contour tree from computational topology, which is guided by the physical constraint (e.g., water flow direction on terrains). Our framework consists of two neural networks: a convolutional neural network (CNN) to learn spatial contextual features on a 2D image grid, and a graph neural network (GNN) to learn the statistical distribution of physics-guided spatial topological dependency on the contour tree. The two models are co-trained via variational EM. Evaluations on the real-world flood mapping datasets show that the proposed models outperform baseline methods in classification accuracy, especially when training labels are limited. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contour tree; Earth image segmentation; physic-guided; semi-supervised; terrain surface,Classification (of information); Convolution; Convolutional neural networks; Deep neural networks; Floods; Flow of water; Forestry; Landforms; Mapping; Recurrent neural networks; Topology; Tropics; Co-training; Contour trees; Earth image segmentation; Earth images; Images segmentations; Learn+; Physic-guided; Semi-supervised; Surface topology; Terrain surfaces; Image segmentation
CrimeTensor: Fine-Scale Crime Prediction via Tensor Learning with Spatiotemporal Consistency,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129521454&doi=10.1145%2f3501807&partnerID=40&md5=a9025882de00077edde715b61da80f0b,"Crime poses a major threat to human life and property, which has been recognized as one of the most crucial problems in our society. Predicting the number of crime incidents in each region of a city before they happen is of great importance to fight against crime. There has been a great deal of research focused on crime prediction, ranging from introducing diversified data sources to exploring various prediction models. However, most of the existing approaches fail to offer fine-scale prediction results and take little notice of the intricate spatial-temporal-categorical correlations contained in crime incidents. In this article, we propose a tailor-made framework called CrimeTensor to predict the number of crime incidents belonging to different categories within each target region via tensor learning with spatiotemporal consistency. In particular, we model the crime data as a tensor and present an objective function which tries to take full advantage of the spatial, temporal, and categorical correlations contained in crime incidents. Moreover, a well-designed optimization algorithm which transforms the objective into a compact form and then applies CP decomposition to find the optimal solution is elaborated to solve the objective function. Furthermore, we develop an enhanced framework which takes a set of pre-selected regions to conduct prediction so as to further improve the computational efficiency of the optimization algorithm. Finally, extensive experiments are performed on both proprietary and public datasets and our framework significantly outperforms all the baselines in terms of each evaluation metric. © 2022 Association for Computing Machinery.",Crime prediction; spatiotemporal correlation; tensor learning; urban crime data,Computational efficiency; Crime; Optimization; Tensors; Crime data; Crime prediction; Fine-scale; Objective functions; Optimization algorithms; Spatial temporals; Spatio-temporal consistencies; Spatiotemporal correlation; Tensor learning; Urban crime data; Forecasting
Make More Connections: Urban Traffic Flow Forecasting with Spatiotemporal Adaptive Gated Graph Convolution Network,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129547740&doi=10.1145%2f3488902&partnerID=40&md5=18a7baf8f16f39d29d29859b5446d043,"Urban traffic flow forecasting is a critical issue in intelligent transportation systems. Due to the complexity and uncertainty of urban road conditions, how to capture the dynamic spatiotemporal correlation and make accurate predictions is very challenging. In most of existing works, urban road network is often modeled as a fixed graph based on local proximity. However, such modeling is not sufficient to describe the dynamics of the road network and capture the global contextual information. In this paper, we consider constructing the road network as a dynamic weighted graph through attention mechanism. Furthermore, we propose to seek both spatial neighbors and semantic neighbors to make more connections between road nodes. We propose a novel Spatiotemporal Adaptive Gated Graph Convolution Network (STAG-GCN) to predict traffic conditions for several time steps ahead. STAG-GCN mainly consists of two major components: (1) multivariate self-attention Temporal Convolution Network (TCN) is utilized to capture local and long-range temporal dependencies across recent, daily-periodic and weekly-periodic observations; (2) mix-hop AG-GCN extracts selective spatial and semantic dependencies within multi-layer stacking through adaptive graph gating mechanism and mix-hop propagation mechanism. The output of different components are weighted fused to generate the final prediction results. Extensive experiments on two real-world large scale urban traffic dataset have verified the effectiveness, and the multi-step forecasting performance of our proposed models outperforms the state-of-the-art baselines. © 2022 Association for Computing Machinery.",graph neural network; spatiotemporal data; Traffic forecasting; urban computing,Backpropagation; Dynamics; Flow graphs; Forecasting; Graph neural networks; Graphic methods; Intelligent systems; Intelligent vehicle highway systems; Large dataset; Motor transportation; Roads and streets; Semantics; Traffic control; Critical issues; Graph neural networks; Intelligent transportation systems; Road network; Spatio-temporal data; Traffic flow forecasting; Traffic Forecasting; Uncertainty; Urban computing; Urban traffic flow; Convolution
Urban Traffic Dynamics Prediction - A Continuous Spatial-temporal Meta-learning Approach,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125310539&doi=10.1145%2f3474837&partnerID=40&md5=1aa343fbb15951e35f2d9e28a888aece,"Urban traffic status (e.g., traffic speed and volume) is highly dynamic in nature, namely, varying across space and evolving over time. Thus, predicting such traffic dynamics is of great importance to urban development and transportation management. However, it is very challenging to solve this problem due to spatial-temporal dependencies and traffic uncertainties. In this article, we solve the traffic dynamics prediction problem from Bayesian meta-learning perspective and propose a novel continuous spatial-temporal meta-learner (cST-ML), which is trained on a distribution of traffic prediction tasks segmented by historical traffic data with the goal of learning a strategy that can be quickly adapted to related but unseen traffic prediction tasks. cST-ML tackles the traffic dynamics prediction challenges by advancing the Bayesian black-box meta-learning framework through the following new points: (1) cST-ML captures the dynamics of traffic prediction tasks using variational inference, and to better capture the temporal uncertainties within tasks, cST-ML performs as a rolling window within each task; (2) cST-ML has novel designs in architecture, where CNN and LSTM are embedded to capture the spatial-temporal dependencies between traffic status and traffic-related features; (3) novel training and testing algorithms for cST-ML are designed. We also conduct experiments on two real-world traffic datasets (taxi inflow and traffic speed) to evaluate our proposed cST-ML. The experimental results verify that cST-ML can significantly improve the urban traffic prediction performance and outperform all baseline models especially when obvious traffic dynamics and temporal uncertainties are presented. © 2022 Association for Computing Machinery.",Bayesian meta-learning; spatial-temporal data; Traffic dynamics prediction,Data mining; Forecasting; Long short-term memory; Speed; Urban growth; Urban transportation; Bayesian; Bayesian meta-learning; Dynamic prediction; Meta-learner; Metalearning; Spatial temporals; Spatial-temporal data; Traffic dynamic prediction; Traffic dynamics; Traffic prediction; Taxicabs
12D Spatio-Temporal Prediction with Stochastic Adversarial Networks,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129511291&doi=10.1145%2f3458025&partnerID=40&md5=74c902e9a8e06429f17ed2c31149b548,"Spatio-temporal (ST) data is a collection of multiple time series data with different spatial locations and is inherently stochastic and unpredictable. An accurate prediction over such data is an important building block for several urban applications, such as taxi demand prediction, traffic flow prediction, and so on. Existing deep learning based approaches assume that outcome is deterministic and there is only one plausible future; therefore, cannot capture the multimodal nature of future contents and dynamics. In addition, existing approaches learn spatial and temporal data separately as they assume weak correlation between them. To handle these issues, in this article, we propose a stochastic spatio-temporal generative model (named D-GAN) which adopts Generative Adversarial Networks (GANs)-based structure for more accurate ST prediction in multiple time steps. D-GAN consists of two components: (1) spatio-temporal correlation network which models spatio-temporal joint distribution of pixels and supports a stochastic sampling of latent variables for multiple plausible futures; (2) a stochastic adversarial network to jointly learn generation and variational inference of data through implicit distribution modeling. D-GAN also supports fusion of external factors through explicit objective to improve the model learning. Extensive experiments performed on two real-world datasets show that D-GAN achieves significant improvements and outperforms baseline models. © 2022 Association for Computing Machinery.",deep learning; Generative adversarial networks; spatio-temporal prediction,Data mining; Deep learning; Forecasting; Stochastic models; Stochastic systems; Taxicabs; Adversarial networks; Deep learning; Learn+; Multiple time series; Spatial location; Spatio-temporal; Spatio-temporal data; Spatio-temporal prediction; Stochastics; Time-series data; Generative adversarial networks
Generative Adversarial Networks for Spatio-temporal Data: A Survey,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128294592&doi=10.1145%2f3474838&partnerID=40&md5=8a8fd9ad7f0b8504818bc3648081de32,"Generative Adversarial Networks (GANs) have shown remarkable success in producing realistic-looking images in the computer vision area. Recently, GAN-based techniques are shown to be promising for spatio-temporal-based applications such as trajectory prediction, events generation, and time-series data imputation. While several reviews for GANs in computer vision have been presented, no one has considered addressing the practical applications and challenges relevant to spatio-temporal data. In this article, we have conducted a comprehensive review of the recent developments of GANs for spatio-temporal data. We summarise the application of popular GAN architectures for spatio-temporal data and the common practices for evaluating the performance of spatio-temporal applications with GANs. Finally, we point out future research directions to benefit researchers in this area. © 2022 Association for Computing Machinery.",Generative adversarial nets; spatio-temporal data; time series; trajectory data,Computer vision; Time series; Data imputation; Network-based; Performance; Spatio-temporal; Spatio-temporal applications; Spatio-temporal data; Time-series data; Times series; Trajectories datum; Trajectory prediction; Generative adversarial networks
DeepRoute+: Modeling Couriers' Spatial-temporal Behaviors and Decision Preferences for Package Pick-up Route Prediction,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129561737&doi=10.1145%2f3481006&partnerID=40&md5=8d417e5bcaffd546a8715e88c8cb1503,"Over 10 billion packages are picked up every day in China. A fundamental task raised in the emerging intelligent logistics systems is the couriers' package pick-up route prediction, which is beneficial for package dispatching, arrival-time estimation and overdue-risk evaluation, by leveraging the predicted routes to improve those downstream tasks. In the package pick-up scene, the decision-making of a courier is affected by strict spatial-temporal constraints (e.g., package location, promised pick-up time, current time, and courier's current location). Furthermore, couriers have different decision preferences on various factors (e.g., time factor, distance factor, and balance of both), based on their own perception of the environments and work experience. In this article, we propose a novel model, named DeepRoute+, to predict couriers' future package pick-up routes according to the couriers' decision experience and preference learned from the historical behaviors. Specifically, DeepRoute+ consists of three layers: (1) The representation layer produces experience- and preference-aware representations for the unpicked-up packages, in which a decision preference module can dynamically adjust the importance of factors that affects the courier's decision under the current situation. (2) The transformer encoder layer encodes the representations of packages while considering the spatial-temporal correlations among them. (3) The attention-based decoder layer uses the attention mechanism to generate the whole pick-up route recurrently. Experiments on a real-world logistics dataset demonstrate the state-of-the-art performance of our model. © 2022 Association for Computing Machinery.",deep neural networks; package pick-up route prediction; Trajectory,Forecasting; Pickups; Risk perception; 'current; Arrival time; Decisions makings; Down-stream; Logistics system; Package pick-up route prediction; Risk evaluation; Route predictions; Spatial-temporal behavior; Time estimation; Deep neural networks
Data-driven Targeted Advertising Recommendation System for Outdoor Billboard,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129478469&doi=10.1145%2f3495159&partnerID=40&md5=8c58ffed7132f1a0b1611b768751637e,"In this article, we propose and study a novel data-driven framework for Targeted Outdoor Advertising Recommendation (TOAR) with a special consideration of user profiles and advertisement topics. Given an advertisement query and a set of outdoor billboards with different spatial locations and rental prices, our goal is to find a subset of billboards, such that the total targeted influence is maximum under a limited budget constraint. To achieve this goal, we are facing two challenges: (1) it is difficult to estimate targeted advertising influence in physical world; (2) due to NP hardness, many common search techniques fail to provide a satisfied solution with an acceptable time, especially for large-scale problem settings. Taking into account the exposure strength, advertisement matching degree, and advertising repetition effect, we first build a targeted influence model that can characterize that the advertising influence spreads along with users mobility. Subsequently, based on a divide-and-conquer strategy, we develop two effective approaches, i.e., a master-slave-based sequential optimization method, TOAR-MSS, and a cooperative co-evolution-based optimization method, TOAR-CC, to solve our studied problem. Extensive experiments on two real-world datasets clearly validate the effectiveness and efficiency of our proposed approaches. © 2022 Association for Computing Machinery.",graph model; Influence spread; large-scale optimization; outdoor advertising,Budget control; Knowledge management; Optimization; User profile; Advertizing; Budget constraint; Data driven; Graph model; Influence spread; Large-scale optimization; Outdoor advertisings; Spatial location; Targeted advertising; User's profiles; Marketing
Bayesian Attribute Bagging-Based Extreme Learning Machine for High-Dimensional Classification and Regression,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129459602&doi=10.1145%2f3495164&partnerID=40&md5=221d08bb3292cca65c21bdf1621e235b,"This article presents a Bayesian attribute bagging-based extreme learning machine (BAB-ELM) to handle high-dimensional classification and regression problems. First, the decision-making degree (DMD) of a condition attribute is calculated based on the Bayesian decision theory, i.e., the conditional probability of the condition attribute given the decision attribute. Second, the condition attribute with the highest DMD is put into the condition attribute group (CAG) corresponding to the specific decision attribute. Third, the bagging attribute groups (BAGs) are used to train an ensemble learning model of extreme learning machines (ELMs). Each base ELM is trained on a BAG which is composed of condition attributes that are randomly selected from the CAGs. Fourth, the information amount ratios of bagging condition attributes to all condition attributes is used as the weights to fuse the predictions of base ELMs in BAB-ELM. Exhaustive experiments have been conducted to compare the feasibility and effectiveness of BAB-ELM with seven other ELM models, i.e., ELM, ensemble-based ELM (EN-ELM), voting-based ELM (V-ELM), ensemble ELM (E-ELM), ensemble ELM based on multi-activation functions (MAF-EELM), bagging ELM, and simple ensemble ELM. Experimental results show that BAB-ELM is convergent with the increase of base ELMs and also can yield higher classification accuracy and lower regression error for high-dimensional classification and regression problems. © 2022 Association for Computing Machinery.",attribute bagging; Bayesian decision; Ensemble learning; extreme learning machine; re-substitution entropy,Decision making; Decision theory; Machine learning; Regression analysis; Attribute Bagging; Bayesian; Bayesian decision; Condition attributes; Decisions makings; Ensemble learning; High-dimensional; Higher-dimensional; Re-substitution entropy; Regression problem; Knowledge acquisition
Contrastive Trajectory Learning for Tour Recommendation,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125011866&doi=10.1145%2f3462331&partnerID=40&md5=d6d5a5e3f2864b4fc331b8f263be8582,"The main objective of Personalized Tour Recommendation (PTR) is to generate a sequence of point-of-interest (POIs) for a particular tourist, according to the user-specific constraints such as duration time, start and end points, the number of attractions planned to visit, and so on. Previous PTR solutions are based on either heuristics for solving the orienteering problem to maximize a global reward with a specified budget or approaches attempting to learn user visiting preferences and transition patterns with the stochastic process or recurrent neural networks. However, existing learning methodologies rely on historical trips to train the model and use the next visited POI as the supervised signal, which may not fully capture the coherence of preferences and thus recommend similar trips to different users, primarily due to the data sparsity problem and long-tailed distribution of POI popularity. This work presents a novel tour recommendation model by distilling knowledge and supervision signals from the trips in a self-supervised manner. We propose Contrastive Trajectory Learning for Tour Recommendation (CTLTR), which utilizes the intrinsic POI dependencies and traveling intent to discover extra knowledge and augments the sparse data via pre-training auxiliary self-supervised objectives. CTLTR provides a principled way to characterize the inherent data correlations while tackling the implicit feedback and weak supervision problems by learning robust representations applicable for tour planning. We introduce a hierarchical recurrent encoder-decoder to identify tourists' intentions and use the contrastive loss to discover subsequence semantics and their sequential patterns through maximizing the mutual information. Additionally, we observe that a data augmentation step as the preliminary of contrastive learning can solve the overfitting issue resulting from data sparsity. We conduct extensive experiments on a range of real-world datasets and demonstrate that our model can significantly improve the recommendation performance over the state-of-the-art baselines in terms of both recommendation accuracy and visiting orders.  © 2021 Association for Computing Machinery.",contrastive self-supervised learning; Tour recommendation; trip planning,Budget control; Knowledge management; Random processes; Semantics; Stochastic systems; World Wide Web; Contrastive self-supervised learning; Duration time; End-points; Learn+; Orienteering problem; Preference pattern; Start point; Time points; Tour recommendation; Trip planning; Recurrent neural networks
Self-Adaptive Feature Transformation Networks for Object Detection in low luminance Images,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125013486&doi=10.1145%2f3480973&partnerID=40&md5=dabd13b540e1b82627541a060dd8172c,"Despite the recent improvement of object detection techniques, many of them fail to detect objects in low-luminance images. The blurry and dimmed nature of low-luminance images results in the extraction of vague features and failure to detect objects. In addition, many existing object detection methods are based on models trained on both sufficient- and low-luminance images, which also negatively affect the feature extraction process and detection results. In this article, we propose a framework called Self-adaptive Feature Transformation Network (SFT-Net) to effectively detect objects in low-luminance conditions. The proposed SFT-Net consists of the following three modules: (1) feature transformation module, (2) self-adaptive module, and (3) object detection module. The purpose of the feature transformation module is to enhance the extracted feature through unsupervisely learning a feature domain projection procedure. The self-adaptive module is utilized as a probabilistic module producing appropriate features either from the transformed or the original features to further boost the performance and generalization ability of the proposed framework. Finally, the object detection module is designed to accurately detect objects in both low- and sufficient- luminance images by using the appropriate features produced by the self-adaptive module. The experimental results demonstrate that the proposed SFT-Net framework significantly outperforms the state-of-the-art object detection techniques, achieving an average precision (AP) of up to 6.35 and 11.89 higher on the sufficient- and low- luminance domain, respectively.  © 2022 Association for Computing Machinery.",GAN; neural networks; unsupervised learning,Extraction; Feature extraction; Image enhancement; Luminance; Object recognition; Unsupervised learning; Adaptive features; Adaptive modules; Feature transformations; Features extraction; GAN; Luminance images; Neural-networks; Object detection method; Objects detection; Transformation modules; Object detection
"Graph Neural Networks: Taxonomy, Advances, and Trends",2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125012033&doi=10.1145%2f3495161&partnerID=40&md5=fe0f333411e638b2cd416517435e4c3c,"Graph neural networks provide a powerful toolkit for embedding real-world graphs into low-dimensional spaces according to specific tasks. Up to now, there have been several surveys on this topic. However, they usually lay emphasis on different angles so that the readers cannot see a panorama of the graph neural networks. This survey aims to overcome this limitation and provide a systematic and comprehensive review on the graph neural networks. First of all, we provide a novel taxonomy for the graph neural networks, and then refer to up to 327 relevant literatures to show the panorama of the graph neural networks. All of them are classified into the corresponding categories. In order to drive the graph neural networks into a new stage, we summarize four future research directions so as to overcome the challenges faced. It is expected that more and more scholars can understand and exploit the graph neural networks and use them in their research community.  © 2022 Association for Computing Machinery.",graph attention mechanism; Graph convolutional neural network; graph neural network; graph pooling operator; graph recurrent neural network,Convolutional neural networks; Recurrent neural networks; Surveys; Taxonomies; Attention mechanisms; Classifieds; Embeddings; Future research directions; Graph attention mechanism; Graph neural networks; Graph pooling operator; Low-dimensional spaces; Real-world graphs; Specific tasks; Graph neural networks
Simultaneous Past and Current Social Interaction-aware Trajectory Prediction for Multiple Intelligent Agents in Dynamic Scenes,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125018416&doi=10.1145%2f3466182&partnerID=40&md5=df33c812fe03a7cf366431338ab9ddb9,"Trajectory prediction of multiple agents in a crowded scene is an essential component in many applications, including intelligent monitoring, autonomous robotics, and self-driving cars. Accurate agent trajectory prediction remains a significant challenge because of the complex dynamic interactions among the agents and between them and the surrounding scene. To address the challenge, we propose a decoupled attention-based spatial-temporal modeling strategy in the proposed trajectory prediction method. The past and current interactions among agents are dynamically and adaptively summarized by two separate attention-based networks and have proven powerful in improving the prediction accuracy. Moreover, it is optional in the proposed method to make use of the road map and the plan of the ego-agent for scene-compliant and accurate predictions. The road map feature is efficiently extracted by a convolutional neural network, and the features of the ego-agent's plan is extracted by a gated recurrent network with an attention module based on the temporal characteristic. Experiments on benchmark trajectory prediction datasets demonstrate that the proposed method is effective when the ego-agent plan and the the surrounding scene information are provided and achieves state-of-the-art performance with only the observed trajectories.  © 2021 Association for Computing Machinery.",autonomous robots; neural networks; Trajectory data analysis,Autonomous agents; Benchmarking; Forecasting; Intelligent agents; Intelligent robots; Maps; Multi agent systems; Trajectories; 'current; Agent plan; Dynamic scenes; Intelligent monitoring; Multiple agents; Neural-networks; Roadmap; Social interactions; Trajectory data analyse; Trajectory prediction; Recurrent neural networks
Exploring the Risky Travel Area and Behavior of Car-hailing Service,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125007892&doi=10.1145%2f3465059&partnerID=40&md5=757884abedfd9597a3047135650db94e,"Recent years have witnessed the rapid development of car-hailing services, which provide a convenient approach for connecting passengers and local drivers using their personal vehicles. At the same time, the concern on passenger safety has gradually emerged and attracted more and more attention. While car-hailing service providers have made considerable efforts on developing real-time trajectory tracking systems and alarm mechanisms, most of them only focus on providing rescue-supporting information rather than preventing potential crimes. Recently, the newly available large-scale car-hailing order data have provided an unparalleled chance for researchers to explore the risky travel area and behavior of car-hailing services, which can be used for building an intelligent crime early warning system. To this end, in this article, we propose a Risky Area and Risky Behavior Evaluation System (RARBEs) based on the real-world car-hailing order data. In RARBEs, we first mine massive multi-source urban data and train an effective area risk prediction model, which estimates area risk at the urban block level. Then, we propose a transverse and longitudinal double detection method, which estimates behavior risk based on two aspects, including fraud trajectory recognition and fraud patterns mining. In particular, we creatively propose a bipartite graph-based algorithm to model the implicit relationship between areas and behaviors, which collaboratively adjusts area risk and behavior risk estimation based on random walk regularization. Finally, extensive experiments on multi-source real-world urban data clearly validate the effectiveness and efficiency of our system.  © 2021 Association for Computing Machinery.",anomaly detection; bipartite graph optimization; fraud detection; order sequence syndrome; Risk analysis,Alarm systems; Crime; Graph theory; Graphic methods; Pattern recognition; Real time systems; Risk analysis; Risk assessment; Risk perception; Anomaly detection; Behavior evaluations; Bipartite graph optimization; Bipartite graphs; Fraud detection; Graph optimization; Order data; Order sequence syndrome; Real-world; Risky behaviors; Anomaly detection
Mining Willing-to-Pay Behavior Patterns from Payment Datasets,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125012328&doi=10.1145%2f3485848&partnerID=40&md5=fcc9929f13e82516c94549c45393fd30,"The customer base is the most valuable resource to E-commerce companies. A comprehensive understanding of customers' preferences and behavior is crucial to developing good marketing strategies, in order to achieve optimal customer lifetime values (CLVs). For example, by exploring customer behavior patterns, given a marketing plan with a limited budget, a set of potential customers is able to be identified to maximize profit. In other words, personalized campaigns at the right time and in the right place can be treated as the last stage of consumption. Moreover, effective future purchase estimation and recommendation help guide the customer to the up-selling stage. The proposed willing-to-pay prediction model (W2P) exploits the transaction data to predict customer payment behavior based on a probabilistic graphical model, which provides semantic explanation of the estimated results and deals with the sparsity of payment data from each customer. Existing work in this domain ranks the customers by their probabilities of purchase in different conditions. However, the customer with the highest purchase probability does not necessarily spend the most. Therefore, we propose a CLV maximization algorithm based on the prediction results. In addition, we improve the model by behavioral segmentation wherein we group the customers by payment behaviors to reduce the size of the offline models and enhance the accuracy for low-frequency customers. The experiment results show that our model outperforms the state-of-the-art methods in purchase behavior prediction.  © 2022 Association for Computing Machinery.",customer lifetime value prediction; data sparsity; Financial technology,Budget control; Data mining; Forecasting; Semantics; Strategic planning; Behaviour patterns; Customer behavior; Customer lifetime value; Customer lifetime value prediction; Customer preferences; Customerbase; Data sparsity; Marketing plan; Marketing strategy; Value prediction; Sales
Let Trajectories Speak Out the Traffic Bottlenecks,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125012739&doi=10.1145%2f3465058&partnerID=40&md5=ca7178b81f406054dfb5ab75c9134316,"Traffic bottlenecks are a set of road segments that have an unacceptable level of traffic caused by a poor balance between road capacity and traffic volume. A huge volume of trajectory data which captures realtime traffic conditions in road networks provides promising new opportunities to identify the traffic bottlenecks. In this paper, we define this problem as trajectory-driven traffic bottleneck identification: Given a road network R, a trajectory database T, find a representative set of seed edges of size K of traffic bottlenecks that influence the highest number of road segments not in the seed set. We show that this problem is NP-hard and propose a framework to find the traffic bottlenecks as follows. First, a traffic spread model is defined which represents changes in traffic volume for each road segment over time. Then, the traffic diffusion probability between two connected segments and the residual ratio of traffic volume for each segment can be computed using historical trajectory data. We then propose two different algorithmic approaches to solve the problem. The first one is a best-first algorithm BF, with an approximation ratio of 1-1/e. To further accelerate the identification process in larger datasets, we also propose a sampling-based greedy algorithm SG. Finally, comprehensive experiments using three different datasets compare and contrast various solutions, and provide insights into important efficiency and effectiveness trade-offs among the respective methods.  © 2021 Association for Computing Machinery.",road segments influence; traffic bottleneck; Traffic spread,Approximation algorithms; Economic and social effects; Motor transportation; Roads and streets; Real-time traffic conditions; Road capacity; Road network; Road segment influence; Road segments; Road traffic; Traffic bottleneck; Traffic spread; Traffic volumes; Trajectories datum; Trajectories
Passenger Mobility Prediction via Representation Learning for Dynamic Directed and Weighted Graphs,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125011925&doi=10.1145%2f3446344&partnerID=40&md5=88a89bf9b15684e4e74e2d2b15bace4b,"In recent years, ride-hailing services have been increasingly prevalent, as they provide huge convenience for passengers. As a fundamental problem, the timely prediction of passenger demands in different regions is vital for effective traffic flow control and route planning. As both spatial and temporal patterns are indispensable passenger demand prediction, relevant research has evolved from pure time series to graph-structured data for modeling historical passenger demand data, where a snapshot graph is constructed for each time slot by connecting region nodes via different relational edges (origin-destination relationship, geographical distance, etc.). Consequently, the spatiotemporal passenger demand records naturally carry dynamic patterns in the constructed graphs, where the edges also encode important information about the directions and volume (i.e., weights) of passenger demands between two connected regions. aspects in the graph-structure data. representation for DDW is the key to solve the prediction problem. However, existing graph-based solutions fail to simultaneously consider those three crucial aspects of dynamic, directed, and weighted graphs, leading to limited expressiveness when learning graph representations for passenger demand prediction. Therefore, we propose a novel spatiotemporal graph attention network, namely Gallat (Graph prediction with all attention) as a solution. In Gallat, by comprehensively incorporating those three intrinsic properties of dynamic directed and weighted graphs, we build three attention layers to fully capture the spatiotemporal dependencies among different regions across all historical time slots. Moreover, the model employs a subtask to conduct pretraining so that it can obtain accurate results more quickly. We evaluate the proposed model on real-world datasets, and our experimental results demonstrate that Gallat outperforms the state-of-the-art approaches.  © 2021 Association for Computing Machinery.",Dynamic graph; passenger demand prediction; representation learning,Data mining; Directed graphs; Forecasting; Demand prediction; Dynamic graph; Flow routes; Mobility predictions; Passenger demand prediction; Passenger demands; Representation learning; Timeslots; Traffic flow control; Weighted graph; Graphic methods
Origin-Aware Location Prediction Based on Historical Vehicle Trajectories,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125006621&doi=10.1145%2f3462675&partnerID=40&md5=753c8a0d07d94f717df94a3d6ccd48bd,"Next location prediction is of great importance for many location-based applications and provides essential intelligence to various businesses. In previous studies, a common approach to next location prediction is to learn the sequential transitions with massive historical trajectories based on conditional probability. Nevertheless, due to the time and space complexity, these methods (e.g., Markov models) only utilize the just passed locations to predict next locations, neglecting earlier passed locations in the trajectory. In this work, we seek to enhance the prediction performance by incorporating the travel time from all the passed locations in the query trajectory to each candidate next location. To this end, we propose a novel prediction method, namely the Travel Time Difference Model, which exploits the difference between the shortest travel time and the actual travel time to predict next locations. Moreover, we integrate the Travel Time Difference Model with a Sequential and Temporal Predictor to yield a joint model. The joint prediction model integrates local sequential transitions, temporal regularity, and global travel time information in the trajectory for the next location prediction problem. We have conducted extensive experiments on two real-world datasets: the vehicle passage record data and the taxi trajectory data. The experimental results demonstrate significant improvements in prediction accuracy over baseline methods.  © 2021 Association for Computing Machinery.",Next location prediction; traffic trajectory data; travel time difference model,Forecasting; Location; Markov processes; Taxicabs; Trajectories; Difference models; Location prediction; Next location predictions; Prediction-based; Time-differences; Traffic trajectory data; Trajectories datum; Travel time difference model; Travel-time; Vehicle trajectories; Travel time
FairSR: Fairness-aware Sequential Recommendation through Multi-Task Learning with Preference Graph Embeddings,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125016031&doi=10.1145%2f3495163&partnerID=40&md5=0180d696911f5d33781ea86b3436996f,"Sequential recommendation (SR) learns from the temporal dynamics of user-item interactions to predict the next ones. Fairness-aware recommendation mitigates a variety of algorithmic biases in the learning of user preferences. This article aims at bringing a marriage between SR and algorithmic fairness. We propose a novel fairness-aware sequential recommendation task, in which a new metric, interaction fairness, is defined to estimate how recommended items are fairly interacted by users with different protected attribute groups. We propose a multi-task learning-based deep end-to-end model, FairSR, which consists of two parts. One is to learn and distill personalized sequential features from the given user and her item sequence for SR. The other is fairness-aware preference graph embedding (FPGE). The aim of FPGE is two-fold: incorporating the knowledge of users' and items' attributes and their correlation into entity representations, and alleviating the unfair distributions of user attributes on items. Extensive experiments conducted on three datasets show FairSR can outperform state-of-the-art SR models in recommendation performance. In addition, the recommended items by FairSR also exhibit promising interaction fairness.  © 2022 Association for Computing Machinery.",Fairness-aware models; knowledge graph embedding; multi-task learning; sequential recommendation,Graph embeddings; Algorithmics; Fairness-aware model; Graph embeddings; Knowledge graph embedding; Knowledge graphs; Learn+; Multitask learning; Preference graph; Sequential recommendation; Temporal dynamics; Knowledge graph
An Uncertainty-based Neural Network for Explainable Trajectory Segmentation,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125019379&doi=10.1145%2f3467978&partnerID=40&md5=1e64607cbf2efe28b7f1477d1ea8622d,"As a variant task of time-series segmentation, trajectory segmentation is a key task in the applications of transportation pattern recognition and traffic analysis. However, segmenting trajectory is faced with challenges of implicit patterns and sparse results. Although deep neural networks have tremendous advantages in terms of high-level feature learning performance, deploying as a blackbox seriously limits the real-world applications. Providing explainable segmentations has significance for result evaluation and decision making. Thus, in this article, we address trajectory segmentation by proposing a Bayesian Encoder-Decoder Network (BED-Net) to provide accurate detection with explainability and references for the following active-learning procedures. BED-Net consists of a segmentation module based on Monte Carlo dropout and an explanation module based on uncertainty learning that provides results evaluation and visualization. Experimental results on both benchmark and real-world datasets indicate that BED-Net outperforms the rival methods and offers excellent explainability in the applications of trajectory segmentation.  © 2021 Association for Computing Machinery.",explainable neural network; time series; Trajectory segmentation; uncertainty learning,Benchmarking; Decision making; Deep neural networks; Pattern recognition; Time series analysis; Trajectories; Bayesian; Encoder-decoder; Explainable neural network; Module-based; Neural-networks; Result evaluation; Times series; Trajectory segmentation; Uncertainty; Uncertainty learning; Time series
Deep Siamese Metric Learning: A Highly Scalable Approach to Searching Unordered Sets of Trajectories,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125010052&doi=10.1145%2f3465057&partnerID=40&md5=1ca4bd159f244d0fc78b4fa6c47b2fae,"This work proposes metric learning for fast similarity-based scene retrieval of unstructured ensembles of trajectory data from large databases. We present a novel representation learning approach using Siamese Metric Learning that approximates a distance preserving low-dimensional representation and that learns to estimate reasonable solutions to the assignment problem. To this end, we employ a Temporal Convolutional Network architecture that we extend with a gating mechanism to enable learning from sparse data, leading to solutions to the assignment problem exhibiting varying degrees of sparsity.Our experimental results on professional soccer tracking data provides insights on learned features and embeddings, as well as on generalization, sensitivity, and network architectural considerations. Our low approximation errors for learned representations and the interactive performance with retrieval times several magnitudes smaller shows that we outperform previous state of the art.  © 2021 Association for Computing Machinery.",assignment problem; metric learning; Siamese neural networks,Computer vision; Deep neural networks; Knowledge management; Network architecture; Assignment problems; Large database; Learning approach; Low-dimensional representation; Metric learning; Neural-networks; Scalable approach; Scene retrieval; Siamese neural network; Trajectories datum; Combinatorial optimization
How Members of Covert Networks Conceal the Identities of Their Leaders,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125009509&doi=10.1145%2f3490462&partnerID=40&md5=f4fba7cd2f3d28794669910a447bd1f3,"Centrality measures are the most commonly advocated social network analysis tools for identifying leaders of covert organizations. While the literature has predominantly focused on studying the effectiveness of existing centrality measures or developing new ones, we study the problem from the opposite perspective, by focusing on how a group of leaders can avoid being identified by centrality measures as key members of a covert network. More specifically, we analyze the problem of choosing a set of edges to be added to a network to decrease the leaders' ranking according to three fundamental centrality measures, namely, degree, closeness, and betweenness. We prove that this problem is NP-complete for each measure. Moreover, we study how the leaders can construct a network from scratch, designed specifically to keep them hidden from centrality measures. We identify a network structure that not only guarantees to hide the leaders to a certain extent but also allows them to spread their influence across the network.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",centrality; complexity analysis.; covert networks; Social networks,Analysis tools; Betweenness; Centrality; Centrality measures; Complexity analyse.; Complexity analysis; Covert networks; NP Complete; Social network; Social Network Analysis
Predicting Future Locations with Semantic Trajectories,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125011972&doi=10.1145%2f3465060&partnerID=40&md5=dcd957b7a1d981cb01824f0af236fc68,"Location prediction has attracted much attention due to its important role in many location-based services, including taxi services, route navigation, traffic planning, and location-based advertisements. Traditional methods only use spatial-temporal trajectory data to predict where a user will go next. The divorce of semantic knowledge from the spatial-temporal one inhibits our better understanding of users' activities. Inspired by the architecture of Long Short Term Memory (LSTM), we design ST-LSTM, which draws on semantic trajectories to predict future locations. Semantic data add a new dimension to our study, increasing the accuracy of prediction. Since semantic trajectories are sparser than the spatial-temporal ones, we propose a strategic filling algorithm to solve this problem. In addition, as the prediction is based on the historical trajectories of users, the cold-start problem arises. We build a new virtual social network for users to resolve the issue. Experiments on two real-world datasets show that the performance of our method is superior to those of the baselines.  © 2022 Association for Computing Machinery.",cold-start; data sparsity; Location prediction; semantic information; trajectory pattern mining,Location; Location based services; Long short-term memory; Semantics; Taxicabs; Telecommunication services; Trajectories; Cold-start; Data sparsity; Location prediction; Location-based services; Pattern mining; Semantic trajectories; Semantics Information; Spatial temporals; Trajectory pattern; Trajectory pattern mining; Forecasting
Instant Basketball Defensive Trajectory Generation,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125015895&doi=10.1145%2f3460619&partnerID=40&md5=cc85bae7ec7b127ef0abead2d814fd2d,"Tactic learning in virtual reality (VR) has been proven to be effective for basketball training. Endowed with the ability of generating virtual defenders in real time according to the movement of virtual offenders controlled by the user, a VR basketball training system can bring more immersive and realistic experiences for the trainee. In this article, an autoregressive generative model for instantly producing basketball defensive trajectory is introduced. We further focus on the issue of preserving the diversity of the generated trajectories. A differentiable sampling mechanism is adopted to learn the continuous Gaussian distribution of player position. Moreover, several heuristic loss functions based on the domain knowledge of basketball are designed to make the generated trajectories assemble real situations in basketball games. We compare the proposed method with the state-of-the-art works in terms of both objective and subjective manners. The objective manner compares the average position, velocity, and acceleration of the generated defensive trajectories with the real ones to evaluate the fidelity of the results. In addition, more high-level aspects such as the empty space for offender and the defensive pressure of the generated trajectory are also considered in the objective evaluation. As for the subjective manner, visual comparison questionnaires on the proposed and other methods are thoroughly conducted. The experimental results show that the proposed method can achieve better performance than previous basketball defensive trajectory generation works in terms of different evaluation metrics.  © 2021 Association for Computing Machinery.",autoregressive model; Basketball; defensive strategies,Domain Knowledge; E-learning; Surveys; Trajectories; Virtual reality; Auto-regressive; Autoregressive modelling; Defensive strategies; Generated trajectories; Generative model; Immersive; Real- time; Sampling mechanisms; Training Systems; Trajectory generation; Sports
Introduction to the Special Issue on Intelligent Trajectory Analytics: Part I,2022,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125086078&doi=10.1145%2f3495230&partnerID=40&md5=4befe0a100efa9fb418aa7dbe3b47ea8,[No abstract available],,
Temporal Hierarchical Graph Attention Network for Traffic Prediction,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123960035&doi=10.1145%2f3446430&partnerID=40&md5=db647e783ca16808c226286882cc363f,"As a critical task in intelligent traffic systems, traffic prediction has received a large amount of attention in the past few decades. The early efforts mainly model traffic prediction as the time-series mining problem, in which the spatial dependence has been largely ignored. As the rapid development of deep learning, some attempts have been made in modeling traffic prediction as the spatiooral data mining problem in a road network, in which deep learning techniques can be adopted for modeling the spatial and temporal dependencies simultaneously. Despite the success, the spatial and temporal dependencies are only modeled in a regionless network without considering the underlying hierarchical regional structure of the spatial nodes, which is an important structure naturally existing in the real-world road network. Apart from the challenge of modeling the spatial and temporal dependencies like the existing studies, the extra challenge caused by considering the hierarchical regional structure of the road network lies in simultaneously modeling the spatial and temporal dependencies between nodes and regions and the spatial and temporal dependencies between regions. To this end, this article proposes a new Temporal Hierarchical Graph Attention Network (TH-GAT). The main idea lies in augmenting the original road network into a region-augmented network, in which the hierarchical regional structure can be modeled. Based on the region-augmented network, the region-aware spatial dependence model and the region-aware temporal dependence model can be constructed, which are two main components of the proposed TH-GAT model. In addition, in the region-aware spatial dependence model, the graph attention network is adopted, in which the importance of a node to another node, of a node to a region, of a region to a node, and of a region to another region, can be captured automatically by means of the attention coefficients. Extensive experiments are conducted on two real-world traffic datasets, and the results have confirmed the superiority of the proposed TH-GAT model. © 2021 Association for Computing Machinery.",gated recurrent unit; graph attention network; hierarchical; regional structure; Spatiooral; traffic prediction,Data mining; Deep learning; Graph theory; Learning systems; Motor transportation; Roads and streets; Traffic control; Dependence model; Gated recurrent unit; Graph attention network; Hierarchical; Hierarchical graphs; Regional structure; Road network; Spatial dependence; Spatiooral; Traffic prediction; Forecasting
Multi-Stage Fusion and Multi-Source Attention Network for Multi-Modal Remote Sensing Image Segmentation,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123911261&doi=10.1145%2f3484440&partnerID=40&md5=2d716155982804b28db767be3b11d340,"With the rapid development of sensor technology, lots of remote sensing data have been collected. It effectively obtains good semantic segmentation performance by extracting feature maps based on multi-modal remote sensing images since extra modal data provides more information. How to make full use of multi-model remote sensing data for semantic segmentation is challenging. Toward this end, we propose a new network called Multi-Stage Fusion and Multi-Source Attention Network ((MS)2-Net) for multi-modal remote sensing data segmentation. The multi-stage fusion module fuses complementary information after calibrating the deviation information by filtering the noise from the multi-modal data. Besides, similar feature points are aggregated by the proposed multi-source attention for enhancing the discriminability of features with different modalities. The proposed model is evaluated on publicly available multi-modal remote sensing data sets, and results demonstrate the effectiveness of the proposed method. © 2021 Association for Computing Machinery.",attention; feature fusion; multi-modal remote sensing images; Semantic segmentation,Image fusion; Information filtering; Modal analysis; Remote sensing; Semantic Segmentation; Attention; Features fusions; Images segmentations; Multi stage fusion; Multi-modal; Multi-modal remote sensing image; Multi-Sources; Remote sensing data; Remote sensing images; Semantic segmentation; Semantics
POLLA: Enhancing the Local Structure Awareness in Long Sequence Spatialoral Modeling,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123934007&doi=10.1145%2f3447987&partnerID=40&md5=e5e32ef0f42e801c5f9cbc12eb19abf4,"The spatialoral modeling on long sequences is of great importance in many real-world applications. Recent studies have shown the potential of applying the self-attention mechanism to improve capturing the complex spatialoral dependencies. However, the lack of underlying structure information weakens its general performance on long sequence spatialoral problem. To overcome this limitation, we proposed a novel method, named the Proximity-aware Long Sequence Learning framework, and apply it to the spatialoral forecasting task. The model substitutes the canonical self-attention by leveraging the proximity-aware attention, which enhances local structure clues in building long-range dependencies with a linear approximation of attention scores. The relief adjacency matrix technique can utilize the historical global graph information for consistent proximity learning. Meanwhile, the reduced decoder allows for fast inference in a non-autoregressive manner. Extensive experiments are conducted on five large-scale datasets, which demonstrate that our method achieves state-of-the-art performance and validates the effectiveness brought by local structure information. © 2021 Association for Computing Machinery.",graph; neural network; Spatiooral; time-series,Data mining; Graph neural networks; Attention mechanisms; Graph; Local structure; Long sequences; Neural-networks; Performance; Real-world; Spatiooral; Structure information; Times series; Large dataset
ACM TIST Special Issue on Deep Learning for Spatiooral Data: Part 1,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123913995&doi=10.1145%2f3495188&partnerID=40&md5=649a86d3415ffaef100453ee1b645b7f,[No abstract available],,
Causal Discovery with Confounding Cascade Nonlinear Additive Noise Models,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123945394&doi=10.1145%2f3482879&partnerID=40&md5=48a8f23682fa2c699730f49843ac7424,"Identification of causal direction between a causal-effect pair from observed data has recently attracted much attention. Various methods based on functional causal models have been proposed to solve this problem, by assuming the causal process satisfies some (structural) constraints and showing that the reverse direction violates such constraints. The nonlinear additive noise model has been demonstrated to be effective for this purpose, but the model class does not allow any confounding or intermediate variables between a cause pair-even if each direct causal relation follows this model. However, omitting the latent causal variables is frequently encountered in practice. After the omission, the model does not necessarily follow the model constraints. As a consequence, the nonlinear additive noise model may fail to correctly discover causal direction. In this work, we propose a confounding cascade nonlinear additive noise model to represent such causal influences-each direct causal relation follows the nonlinear additive noise model but we observe only the initial cause and final effect. We further propose a method to estimate the model, including the unmeasured confounding and intermediate variables, from data under the variational auto-encoder framework. Our theoretical results show that with our model, the causal direction is identifiable under suitable technical conditions on the data generation process. Simulation results illustrate the power of the proposed method in identifying indirect causal relations across various settings, and experimental results on real data suggest that the proposed model and method greatly extend the applicability of causal discovery based on functional causal models in nonlinear cases. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",additive noise model; Causal discovery; latent model,Additives; Data mining; Additive noise model; Causal discovery; Causal influences; Causal modeling; Causal relations; Latent models; Model constraints; Noise models; Observed data; Structural constraints; Additive noise
A Survey of AIOps Methods for Failure Management,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123922295&doi=10.1145%2f3483424&partnerID=40&md5=a399a4a52ea4c8e365864ee9fad47952,"Modern society is increasingly moving toward complex and distributed computing systems. The increase in scale and complexity of these systems challenges O&M teams that perform daily monitoring and repair operations, in contrast with the increasing demand for reliability and scalability of modern applications. For this reason, the study of automated and intelligent monitoring systems has recently sparked much interest across applied IT industry and academia. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to Machine Learning, AI, and Big Data. However, AIOps as a research topic is still largely unstructured and unexplored, due to missing conventions in categorizing contributions for their data requirements, target goals, and components. In this work, we focus on AIOps for Failure Management (FM), characterizing and describing 5 different categories and 14 subcategories of contributions, based on their time intervention window and the target problem being solved. We review 100 FM solutions, focusing on applicability requirements and the quantitative results achieved, to facilitate an effective application of AIOps solutions. Finally, we discuss current development problems in the areas covered by AIOps and delineate possible future trends for AI-based failure management. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AIOps; artificial intelligence; failure management; IT operations and maintenance,Failure analysis; Artificial intelligence for IT operation; Automated monitoring systems; Complex computing systems; Distributed computing systems; Failure management; IT operation and maintenance; Modern applications; Operation methods; Operations and maintenance; Repair operations; Artificial intelligence
TWIST-GAN: Towards Wavelet Transform and Transferred GAN for Spatiooral Single Image Super Resolution,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123956833&doi=10.1145%2f3456726&partnerID=40&md5=423b9bbc6a776996628949b45f5fa79d,"Single Image Super-resolution (SISR) produces high-resolution images with fine spatial resolutions from a remotely sensed image with low spatial resolution. Recently, deep learning and generative adversarial networks (GANs) have made breakthroughs for the challenging task of single image super-resolution (SISR). However, the generated image still suffers from undesirable artifacts such as the absence of texture-feature representation and high-frequency information. We propose a frequency domain-based spatiooral remote sensing single image super-resolution technique to reconstruct the HR image combined with generative adversarial networks (GANs) on various frequency bands (TWIST-GAN). We have introduced a new method incorporating Wavelet Transform (WT) characteristics and transferred generative adversarial network. The LR image has been split into various frequency bands by using the WT, whereas the transfer generative adversarial network predicts high-frequency components via a proposed architecture. Finally, the inverse transfer of wavelets produces a reconstructed image with super-resolution. The model is first trained on an external DIV2 K dataset and validated with the UC Merced Landsat remote sensing dataset and Set14 with each image size of 256 × 256. Following that, transferred GANs are used to process spatiooral remote sensing images in order to minimize computation cost differences and improve texture information. The findings are compared qualitatively and qualitatively with the current state-of-art approaches. In addition, we saved about 43% of the GPU memory during training and accelerated the execution of our simplified version by eliminating batch normalization layers. © 2021 Association for Computing Machinery.",neural networks; spatiooral; super resolution; Wavelet transform,Deep learning; Frequency domain analysis; Image compression; Image enhancement; Image reconstruction; Image resolution; Image texture; Remote sensing; Textures; Wavelet transforms; High-resolution images; Image super resolutions; Neural-networks; Remote-sensing; Remotely sensed images; Single images; Spatial resolution; Spatiooral; Superresolution; Wavelets transform; Generative adversarial networks
A Dynamic Convolutional Neural Network Based Shared-Bike Demand Forecasting Model,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123912702&doi=10.1145%2f3447988&partnerID=40&md5=e4f9e48833b6afde353bfe51b5ebf9e7,"Bike-sharing systems are becoming popular and generate a large volume of trajectory data. In a bike-sharing system, users can borrow and return bikes at different stations. In particular, a bike-sharing system will be affected by weather, the time period, and other dynamic factors, which challenges the scheduling of shared bikes. In this article, a new shared-bike demand forecasting model based on dynamic convolutional neural networks, called SDF, is proposed to predict the demand of shared bikes. SDF chooses the most relevant weather features from real weather data by using the Pearson correlation coefficient and transforms them into a two-dimensional dynamic feature matrix, taking into account the states of stations from historical data. The feature information in the matrix is extracted, learned, and trained with a newly proposed dynamic convolutional neural network to predict the demand of shared bikes in a dynamical and intelligent fashion. The phase of parameter update is optimized from three aspects: the loss function, optimization algorithm, and learning rate. Then, an accurate shared-bike demand forecasting model is designed based on the basic idea of minimizing the loss value. By comparing with classical machine learning models, the weight sharing strategy employed by SDF reduces the complexity of the network. It allows a high prediction accuracy to be achieved within a relatively short period of time. Extensive experiments are conducted on real-world bike-sharing datasets to evaluate SDF. The results show that SDF significantly outperforms classical machine learning models in prediction accuracy and efficiency. © 2021 Association for Computing Machinery.",artificial intelligence; Bike-sharing system; deep learning; dynamic convolutional neural network; optimization; scheduling,Convolution; Convolutional neural networks; Correlation methods; Deep learning; Forecasting; Learning algorithms; Learning systems; Matrix algebra; Bike-sharing system; Convolutional neural network; Deep learning; Demand forecasting; Dynamic convolutional neural network; Forecasting models; Machine learning models; Optimisations; Prediction accuracy; Sharing systems; Scheduling
PARP: A Parallel Traffic Condition Driven Route Planning Model on Dynamic Road Networks,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123914743&doi=10.1145%2f3459099&partnerID=40&md5=ea40322a757f50671615187ddb47532a,"The problem of route planning on road network is essential to many Location-Based Services (LBSs). Road networks are dynamic in the sense that the weights of the edges in the corresponding graph constantly change over time, representing evolving traffic conditions. Thus, a practical route planning strategy is required to supply the continuous route optimization considering the historic, current, and future traffic condition. However, few existing works comprehensively take into account these various traffic conditions during the route planning. Moreover, the LBSs usually suffer from extensive concurrent route planning requests in rush hours, which imposes a pressing need to handle numerous queries in parallel for reducing the response time of each query. However, this issue is also not involved by most existing solutions. We therefore investigate a parallel traffic condition driven route planning model on a cluster of processors. To embed the future traffic condition into the route planning, we employ a GCN model to periodically predict the travel costs of roads within a specified time period, which facilitates the robustness of the route planning model against the varying traffic condition. To reduce the response time, a Dual-Level Path (DLP) index is proposed to support a parallel route planning algorithm with the filter-and-refine principle. The bottom level of DLP partitions the entire graph into different subgraphs, and the top level is a skeleton graph that consists of all border vertices in all subgraphs. The filter step identifies a global directional path for a given query based on the skeleton graph. In the refine step, the overall route planning for this query is decomposed into multiple sub-optimizations in the subgraphs passed through by the directional path. Since the subgraphs are independently maintained by different processors, the sub-optimizations of extensive queries can be operated in parallel. Finally, extensive evaluations are conducted to confirm the effectiveness and superiority of the proposal. © 2021 Association for Computing Machinery.",parallel computing; road network; Route planning; travel cost prediction,Graph theory; Motor transportation; Musculoskeletal system; Parallel processing systems; Query processing; Roads and streets; Telecommunication services; Cost prediction; Location-based services; Parallel com- puting; Planning models; Road network; Route planning; Subgraphs; Traffic conditions; Travel cost prediction; Travel costs; Location based services
TARA-Net: A Fusion Network for Detecting Takeaway Rider Accidents,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123919633&doi=10.1145%2f3457218&partnerID=40&md5=54516791933a216c7ebdee498a5bc57f,"In the emerging business of food delivery, rider traffic accidents raise financial cost and social traffic burden. Although there has been much effort on traffic accident forecasting using temporal-spatial prediction models, none of the existing work studies the problem of detecting the takeaway rider accidents based on food delivery trajectory data. In this article, we aim to detect whether a takeaway rider meets an accident on a certain time period based on trajectories of food delivery and riders' contextual information. The food delivery data has a heterogeneous information structure and carries contextual information such as weather and delivery history, and trajectory data are collected as a spatialoral sequence. In this article, we propose a TakeAway Rider Accident detection fusion network TARA-Net to jointly model these heterogeneous and spatialoral sequence data. We utilize the residual network to extract basic contextual information features and take advantage of a transformer encoder to capture trajectory features. These embedding features are concatenated into a pyramidal feed-forward neural network. We jointly train the above three components to combine the benefits of spatialoral trajectory data and sparse basic contextual data for early detecting traffic accidents. Furthermore, although traffic accidents rarely happen in food delivery, we propose a sampling mechanism to alleviate the imbalance of samples when training the model. We evaluate the model on a transportation mode classification dataset Geolife and a real-world Ele.me dataset with over 3 million riders. The experimental results show that the proposed model is superior to the state-of-the-art. © 2021 Association for Computing Machinery.",deep learning; residual network; Traffic accident detection; trajectory data; transformer encoder,Accidents; Deep learning; Feedforward neural networks; Network coding; Trajectories; Accident detections; Contextual information; Deep learning; Financial costs; Food delivery; Residual network; Spatial prediction modeling; Traffic accident detection; Trajectories datum; Transformer encoder; Classification (of information)
Similar Trajectory Search with Spatiooral Deep Representation Learning,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123914343&doi=10.1145%2f3466687&partnerID=40&md5=0d4c33bb34dde3764b329b893c21e865,"Similar trajectory search is a crucial task that facilitates many downstream spatial data analytic applications. Despite its importance, many of the current literature focus solely on the trajectory's spatial similarity while neglecting the temporal information. Additionally, the few papers that use both the spatial and temporal features based their approach on a traditional point-to-point comparison. These methods model the importance of the spatial and temporal aspect of the data with only a single, pre-defined balancing factor for all trajectories, even though the relative spatial and temporal balance can change from trajectory to trajectory. In this article, we propose the first spatiooral, deep-representation-learning-based approach to similar trajectory search. Experiments show that utilizing both features offers significant improvements over existing point-to-point comparison and deep-representation-learning approach. We also show that our deep neural network approach is faster and performs more consistently compared to the point-to-point comparison approaches. © 2021 Association for Computing Machinery.",attention model; Deep neural networks; spatiooral; trajectories,Deep neural networks; 'current; Attention model; Data analytics; Down-stream; Spatial data; Spatial features; Spatial similarity; Spatiooral; Temporal information; Trajectory searches; Trajectories
Predicting Human Mobility with Reinforcement-Learning-Based Long-Term Periodicity Modeling,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123921260&doi=10.1145%2f3469860&partnerID=40&md5=7db38ccc66099a9c7586bec84edcd0fd,"Mobility prediction plays an important role in a wide range of location-based applications and services. However, there are three problems in the existing literature: (1) explicit high-order interactions of spatiooral features are not systemically modeled; (2) most existing algorithms place attention mechanisms on top of recurrent network, so they can not allow for full parallelism and are inferior to self-attention for capturing long-range dependence; (3) most literature does not make good use of long-term historical information and do not effectively model the long-term periodicity of users. To this end, we propose MoveNet and RLMoveNet. MoveNet is a self-attention-based sequential model, predicting each user's next destination based on her most recent visits and historical trajectory. MoveNet first introduces a cross-based learning framework for modeling feature interactions. With self-attention on both the most recent visits and historical trajectory, MoveNet can use an attention mechanism to capture the user's long-term regularity in a more efficient way. Based on MoveNet, to model long-term periodicity more effectively, we add the reinforcement learning layer and named RLMoveNet. RLMoveNet regards the human mobility prediction as a reinforcement learning problem, using the reinforcement learning layer as the regularization part to drive the model to pay attention to the behavior with periodic actions, which can help us make the algorithm more effective. We evaluate both of them with three real-world mobility datasets. MoveNet outperforms the state-of-the-art mobility predictor by around 10% in terms of accuracy, and simultaneously achieves faster convergence and over 4x training speedup. Moreover, RLMoveNet achieves higher prediction accuracy than MoveNet, which proves that modeling periodicity explicitly from the perspective of reinforcement learning is more effective. © 2021 Association for Computing Machinery.",Human mobility prediction; reinforcement learning; self-attention,Forecasting; Learning systems; Location based services; Recurrent neural networks; User profile; Attention mechanisms; High-order; Higher-order; Human mobility; Human mobility predictions; Location-based applications; Location-based services; Mobility predictions; Reinforcement learnings; Self-attention; Reinforcement learning
Classi-Fly: Inferring Aircraft Categories from Open Data,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123926396&doi=10.1145%2f3480969&partnerID=40&md5=cda61ccda71097f0e97cf33f0af95d0f,"In recent years, air traffic communication data has become easy to access, enabling novel research in many fields. Exploiting this new data source, a wide range of applications have emerged, from weather forecasting to stock market prediction, or the collection of intelligence about military and government movements. Typically, these applications require knowledge about the metadata of the aircraft, specifically its operator and the aircraft category.armasuisse Science + Technology, the R&D agency for the Swiss Armed Forces, has been developing Classi-Fly, a novel approach to obtain metadata about aircraft based on their movement patterns. We validate Classi-Fly using several hundred thousand flights collected through open source means, in conjunction with ground truth from publicly available aircraft registries containing more than 2 million aircraft. We show that we can obtain the correct aircraft category with an accuracy of greater than 88%. In cases, where no metadata is available, this approach can be used to create the data necessary for applications working with air traffic communication. Finally, we show that it is feasible to automatically detect particular sensitive aircraft such as police and surveillance aircraft using this method. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",air traffic control; knowledge engineering; object classification; Wireless sensor networks,Air navigation; Air transportation; Aircraft; Metadata; Military applications; Open Data; Weather forecasting; Wireless sensor networks; Air traffics; Armasuisse; Communications data; Data-source; Object classification; Open datum; Science technologies; Stock market prediction; Swiss Armed Forces; Traffic communication; Air traffic control
Route Optimization via Environment-Aware Deep Network and Reinforcement Learning,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123942638&doi=10.1145%2f3461645&partnerID=40&md5=e782387e6d5ed667d0aedd37bbe321aa,"Vehicle mobility optimization in urban areas is a long-standing problem in smart city and spatial data analysis. Given the complex urban scenario and unpredictable social events, our work focuses on developing a mobile sequential recommendation system to maximize the profitability of vehicle service providers (e.g., taxi drivers). In particular, we treat the dynamic route optimization problem as a long-term sequential decision-making task. A reinforcement-learning framework is proposed to tackle this problem, by integrating a self-check mechanism and a deep neural network for customer pick-up point monitoring. To account for unexpected situations (e.g., the COVID-19 outbreak), our method is designed to be capable of handling related environment changes with a self-adaptive parameter determination mechanism. Based on the yellow taxi data in New York City and vicinity before and after the COVID-19 outbreak, we have conducted comprehensive experiments to evaluate the effectiveness of our method. The results show consistently excellent performance, from hourly to weekly measures, to support the superiority of our method over the state-of-the-art methods (i.e., with more than 98% improvement in terms of the profitability for taxi drivers). © 2021 Association for Computing Machinery.",COVID-19; deep learning; reinforcement learning; route optimization; Route recommendation,Decision making; Deep neural networks; Profitability; Reinforcement learning; Taxicabs; Deep learning; Networks learning; Optimisations; Reinforcement learnings; Route optimization; Route recommendation; Standing problems; Taxi drivers; Urban areas; Vehicle mobility; COVID-19
TAML: A Traffic-aware Multi-task Learning Model for Estimating Travel Time,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123955942&doi=10.1145%2f3466686&partnerID=40&md5=cffebb1bf1e56c65bdc143aa937e3763,"Travel time estimation has been recognized as an important research topic that can find broad applications. Existing approaches aim to explore mobility patterns via trajectory embedding for travel time estimation. Though state-of-the-art methods utilize estimated traffic condition (by explicit features such as average traffic speed) for auxiliary supervision of travel time estimation, they fail to model their mutual influence and result in inaccuracy accordingly. To this end, in this article, we propose an improved traffic-aware model, called TAML, which adopts a multi-task learning network to integrate a travel time estimator and a traffic estimator in a shared space and improves the accuracy of estimation by enhanced representation of traffic condition, such that more meaningful implicit features are fully captured. In TAML, multi-task learning is further applied for travel time estimation in multi-granularities (including road segment, sub-path, and entire path). The multiple loss functions are combined by considering the homoscedastic uncertainty of each task. Extensive experiments on two real trajectory datasets demonstrate the effectiveness of our proposed methods. © 2021 Association for Computing Machinery.",deep learning; trajectory data mining; Travel time estimation,Data mining; Deep learning; Learning systems; Traffic control; Trajectories; Broad application; Deep learning; Learning models; Multitask learning; Research topics; Traffic aware; Traffic conditions; Trajectory data minings; Travel time estimation; Travel-time; Travel time
Spatial Variability Aware Deep Neural Networks (SVANN): A General Approach,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119172772&doi=10.1145%2f3466688&partnerID=40&md5=adf03487a38cee70faaca539f7685092,"Spatial variability is a prominent feature of various geographic phenomena such as climatic zones, USDA plant hardiness zones, and terrestrial habitat types (e.g., forest, grasslands, wetlands, and deserts). However, current deep learning methods follow a spatial-one-size-fits-all (OSFA) approach to train single deep neural network models that do not account for spatial variability. Quantification of spatial variability can be challenging due to the influence of many geophysical factors. In preliminary work, we proposed a spatial variability aware neural network (SVANN-I, formerly called SVANN) approach where weights are a function of location but the neural network architecture is location independent. In this work, we explore a more flexible SVANN-E approach where neural network architecture varies across geographic locations. In addition, we provide a taxonomy of SVANN types and a physics inspired interpretation model. Experiments with aerial imagery based wetland mapping show that SVANN-I outperforms OSFA and SVANN-E performs the best of all. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Neural networks; spatial variability,Aerial photography; Antennas; Arid regions; Learning systems; Location; Network architecture; Wetlands; Climatic zone; Geographic phenomena; Habitat type; Hardiness zone; Neural network architecture; Neural-networks; Prominent features; Spatial variability; Terrestrial habitat; Variability-Aware; Deep neural networks
Detecting and Analyzing Collusive Entities on YouTube,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122056016&doi=10.1145%2f3477300&partnerID=40&md5=506803e37df4fa999bea99700d672fd4,"YouTube sells advertisements on the posted videos, which in turn enables the content creators to monetize their videos. As an unintended consequence, this has proliferated various illegal activities such as artificial boosting of views, likes, comments, and subscriptions. We refer to such videos (gaining likes and comments artificially) and channels (gaining subscriptions artificially) as ""collusive entities.""Detecting such collusive entities is an important yet challenging task. Existing solutions mostly deal with the problem of spotting fake views, spam comments, fake content, and so on, and oftentimes ignore how such fake activities emerge via collusion. Here, we collect a large dataset consisting of two types of collusive entities on YouTube - videos submitted to gain collusive likes and comment requests and channels submitted to gain collusive subscriptions.We begin by providing an in-depth analysis of collusive entities on YouTube fostered by various blackmarket services. Following this, we propose models to detect three types of collusive YouTube entities: videos seeking collusive likes, channels seeking collusive subscriptions, and videos seeking collusive comments. The third type of entity is associated with temporal information. To detect videos and channels for collusive likes and subscriptions, respectively, we utilize one-class classifiers trained on our curated collusive entities and a set of novel features. The SVM-based model shows significant performance with a true positive rate of 0.911 and 0.910 for detecting collusive videos and collusive channels, respectively. To detect videos seeking collusive comments, we propose CollATe, a novel end-to-end neural architecture that leverages time-series information of posted comments along with static metadata of videos. CollATe is composed of three components: metadata feature extractor (which derives metadata-based features from videos), anomaly feature extractor (which utilizes the time-series data to detect sudden changes in the commenting activity), and comment feature extractor (which utilizes the text of the comments posted during collusion and computes a similarity score between the comments). Extensive experiments show the effectiveness of CollATe (with a true positive rate of 0.905) over the baselines.  © 2021 Association for Computing Machinery.",artificial boosting; blackmarket; collusion; OSNs; YouTube,Fake detection; Feature extraction; Large dataset; Support vector machines; Time series; Artificial boosting; Blackmarket; Collusion; Content creators; Feature extractor; Illegal activities; OSN; True positive rates; Unintended consequences; YouTube; Metadata
S3-Net: A Fast Scene Understanding Network by Single-Shot Segmentation for Autonomous Driving,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122092122&doi=10.1145%2f3470660&partnerID=40&md5=acf8ea4553b1e562275a31e8659764a1,"Real-time segmentation and understanding of driving scenes are crucial in autonomous driving. Traditional pixel-wise approaches extract scene information by segmenting all pixels in a frame, and hence are inefficient and slow. Proposal-wise approaches only learn from the proposed object candidates, but still require multiple steps on the expensive proposal methods. Instead, this work presents a fast single-shot segmentation strategy for video scene understanding. The proposed net, called S3-Net, quickly locates and segments target sub-scenes, and meanwhile extracts attention-aware time-series sub-scene features (ats-features) as inputs to an attention-aware spatio-temporal model (ASM). Utilizing tensorization and quantization techniques, S3-Net is intended to be lightweight for edge computing. Experiments results on CityScapes, UCF11, HMDB51, and MOMENTS datasets demonstrate that the proposed S3-Net achieves an accuracy improvement of 8.1% versus the 3D-CNN based approach on UCF11, a storage reduction of 6.9× and an inference speed of 22.8 FPS on CityScapes with a GTX1080Ti GPU.  © 2021 Association for Computing Machinery.",quantization; single-shot segmentation; spatio-temporal model; tensorization; time-series features; Video scene understanding,Image segmentation; Pixels; Time series; Quantisation; Scene understanding; Shot segmentation; Single-shot; Single-shot segmentation; Spatio-temporal models; Tensorization; Time series features; Video scene; Video scene understanding; Autonomous vehicles
DhCM: Dynamic and Hierarchical Event Categorization and Discovery for Social Media Stream,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122083622&doi=10.1145%2f3470888&partnerID=40&md5=374bcee4bdb58d55b8cd728adafee12a,"The online event discovery in social media based documents is useful, such as for disaster recognition and intervention. However, the diverse events incrementally identified from social media streams remain accumulated, ad hoc, and unstructured. They cannot assist users in digesting the tremendous amount of information and finding their interested events. Further, most of the existing work is challenged by jointly identifying incremental events and dynamically organizing them in an adaptive hierarchy. To address these problems, this article proposes dynamic and hierarchical Categorization Modeling (dhCM) for social media stream. Instead of manually dividing the timeframe, a multimodal event miner exploits a density estimation technique to continuously capture the temporal influence between documents and incrementally identify online events in textual, temporal, and spatial spaces. At the same time, an adaptive categorization hierarchy is formed to automatically organize the documents into proper categories at multiple levels of granularities. In a nonparametric manner, dhCM accommodates the increasing complexity of data streams with automatically growing the categorization hierarchy over adaptive growth. A sequential Monte Carlo algorithm is used for the online inference of the dhCM parameters. Extensive experiments show that dhCM outperforms the state-of-the-art models in terms of term coherence, category abstraction and specialization, hierarchical affinity, and event categorization and discovery accuracy.  © 2021 Association for Computing Machinery.",Bayesian nonparametrics; document stream; event categorization; event discovery; Hierarchical categorization; kernel estimation; online inference,Inference engines; Monte Carlo methods; Amount of information; Bayesian nonparametrics; Document stream; Event categorizations; Event discoveries; Hierarchical categorization; Kernel estimation; Multi-modal; Online inferences; Social media; Social networking (online)
KOMPOS: Connecting Causal Knots in Large Nonlinear Time Series with Non-Parametric Regression Splines,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122077989&doi=10.1145%2f3480971&partnerID=40&md5=0d25c3f141ac5184ee00f14e98fd9581,"Recovering causality from copious time series data beyond mere correlations has been an important contributing factor in numerous scientific fields. Most existing works assume linearity in the data that may not comply with many real-world scenarios. Moreover, it is usually not sufficient to solely infer the causal relationships. Identifying the correct time delay of cause-effect is extremely vital for further insight and effective policies in inter-disciplinary domains. To bridge this gap, we propose KOMPOS, a novel algorithmic framework that combines a powerful concept from causal discovery of additive noise models with graphical ones. We primarily build our structural causal model from multivariate adaptive regression splines with inherent additive local nonlinearities, which render the underlying causal structure more easily identifiable. In contrast to other methods, our approach is not restricted to Gaussian or non-Gaussian noise due to the non-parametric attribute of the regression method. We conduct extensive experiments on both synthetic and real-world datasets, demonstrating the superiority of the proposed algorithm over existing causal discovery methods, especially for the challenging cases of autocorrelated and non-stationary time series.  © 2021 Association for Computing Machinery.",additive noise model; Causal discovery; graphical causal model; stability selection; time series,Additive noise; Data mining; Gaussian noise (electronic); Regression analysis; Time series; Additive noise model; Causal discovery; Causal modeling; Graphical causal model; Noise models; Non-parametric regression; Nonlinear time series; Regression splines; Stability selections; Times series; Additives
Local Graph Edge Partitioning,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122064218&doi=10.1145%2f3466685&partnerID=40&md5=145da2d6833670b008a2acc176bc0f3e,"Graph edge partitioning, which is essential for the efficiency of distributed graph computation systems, divides a graph into several balanced partitions within a given size to minimize the number of vertices to be cut. Existing graph partitioning models can be classified into two categories: offline and streaming graph partitioning models. The former requires global graph information during the partitioning, which is expensive in terms of time and memory for large-scale graphs. The latter creates partitions based solely on the received graph information. However, the streaming model may result in a lower partitioning quality compared with the offline model. Therefore, this study introduces a Local Graph Edge Partitioning model, which considers only the local information (i.e., a portion of a graph instead of the entire graph) during the partitioning. Considering only the local graph information is meaningful because acquiring complete information for large-scale graphs is expensive. Based on the Local Graph Edge Partitioning model, two local graph edge partitioning algorithms - Two-stage Local Partitioning and Adaptive Local Partitioning - are given. Experimental results obtained on 14 real-world graphs demonstrate that the proposed algorithms outperform rival algorithms in most tested cases. Furthermore, the proposed algorithms are proven to significantly improve the efficiency of the real graph computation system GraphX.  © 2021 Association for Computing Machinery.",distributed graph computing; graph edge partitioning; Local information,Computational efficiency; Graph theory; Classifieds; Computation systems; Distributed graph computing; Graph edge partitioning; Graph edges; Graph information; Graph Partitioning; Large-scales; Local information; Offline; Efficiency
Significant DBSCAN+: Statistically Robust Density-based Clustering,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122095666&doi=10.1145%2f3474842&partnerID=40&md5=e592528384c95ee1e5b2624b8d496c55,"Cluster detection is important and widely used in a variety of applications, including public health, public safety, transportation, and so on. Given a collection of data points, we aim to detect density-connected spatial clusters with varying geometric shapes and densities, under the constraint that the clusters are statistically significant. The problem is challenging, because many societal applications and domain science studies have low tolerance for spurious results, and clusters may have arbitrary shapes and varying densities. As a classical topic in data mining and learning, a myriad of techniques have been developed to detect clusters with both varying shapes and densities (e.g., density-based, hierarchical, spectral, or deep clustering methods). However, the vast majority of these techniques do not consider statistical rigor and are susceptible to detecting spurious clusters formed as a result of natural randomness. On the other hand, scan statistic approaches explicitly control the rate of spurious results, but they typically assume a single ""hotspot""of over-density and many rely on further assumptions such as a tessellated input space. To unite the strengths of both lines of work, we propose a statistically robust formulation of a multi-scale DBSCAN, namely Significant DBSCAN+, to identify significant clusters that are density connected. As we will show, incorporation of statistical rigor is a powerful mechanism that allows the new Significant DBSCAN+ to outperform state-of-the-art clustering techniques in various scenarios. We also propose computational enhancements to speed-up the proposed approach. Experiment results show that Significant DBSCAN+ can simultaneously improve the success rate of true cluster detection (e.g., 10-20% increases in absolute F1 scores) and substantially reduce the rate of spurious results (e.g., from thousands/hundreds of spurious detections to none or just a few across 100 datasets), and the acceleration methods can improve the efficiency for both clustered and non-clustered data.  © 2021 Association for Computing Machinery.",Clustering; DBSCAN; statistical robustness,Data mining; Public health; Cluster detection; Clusterings; Datapoints; DBSCAN; Density-based Clustering; Detect density; Public safety; Safety transportation; Spatial cluster; Statistical robustness; Cluster analysis
In-Network Ensemble: Deep Ensemble Learning with Diversified Knowledge Distillation,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122094214&doi=10.1145%2f3473464&partnerID=40&md5=83e91b8c651b6ef2eaddb7f6af9a984e,"Ensemble learning is a widely used technique to train deep convolutional neural networks (CNNs) for improved robustness and accuracy. While existing algorithms usually first train multiple diversified networks and then assemble these networks as an aggregated classifier, we propose a novel learning paradigm, namely, ""In-Network Ensemble""(INE) that incorporates the diversity of multiple models through training a SINGLE deep neural network. Specifically, INE segments the outputs of the CNN into multiple independent classifiers, where each classifier is further fine-tuned with better accuracy through a so-called diversified knowledge distillation process. We then aggregate the fine-tuned independent classifiers using an Averaging-and-Softmax operator to obtain the final ensemble classifier. Note that, in the supervised learning settings, INE starts the CNN training from random, while, under the transfer learning settings, it also could start with a pre-trained model to incorporate the knowledge learned from additional datasets. Extensive experiments have been done using eight large-scale real-world datasets, including CIFAR, ImageNet, and Stanford Cars, among others, as well as common deep network architectures such as VGG, ResNet, and Wide ResNet. We have evaluated the method under two tasks: supervised learning and transfer learning. The results show that INE outperforms the state-of-the-art algorithms for deep ensemble learning with improved accuracy.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ensemble learning; knowledge distillation; Transfer learning,Convolutional neural networks; Distillation; Large dataset; Learning algorithms; Network architecture; Supervised learning; Transfer learning; Convolutional neural network; Distillation process; Ensemble learning; In networks; Independent classifiers; Knowledge distillation; Learning paradigms; Multiple-modeling; Network ensemble; Transfer learning; Deep neural networks
Collaborative Local-Global Learning for Temporal Action Proposal,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122091625&doi=10.1145%2f3466181&partnerID=40&md5=07af1ef4b0b89524190f0404a7a4f855,"Temporal action proposal generation is an essential and challenging task in video understanding, which aims to locate the temporal intervals that likely contain the actions of interest. Although great progress has been made, the problem is still far from being well solved. In particular, prevalent methods can handle well only the local dependencies (i.e., short-term dependencies) among adjacent frames but are generally powerless in dealing with the global dependencies (i.e., long-term dependencies) between distant frames. To tackle this issue, we propose CLGNet, a novel Collaborative Local-Global Learning Network for temporal action proposal. The majority of CLGNet is an integration of Temporal Convolution Network and Bidirectional Long Short-Term Memory, in which Temporal Convolution Network is responsible for local dependencies while Bidirectional Long Short-Term Memory takes charge of handling the global dependencies. Furthermore, an attention mechanism called the background suppression module is designed to guide our model to focus more on the actions. Extensive experiments on two benchmark datasets, THUMOS'14 and ActivityNet-1.3, show that the proposed method can outperform state-of-the-art methods, demonstrating the strong capability of modeling the actions with varying temporal durations.  © 2021 Association for Computing Machinery.",attention mechanism; long short-term memory; Temporal action proposal generation; untrimmed videos,Brain; Long short-term memory; Adjacent frames; Attention mechanisms; Global learning; Learning network; Long-term dependencies; Temporal action proposal generation; Temporal intervals; Term dependency; Untrimmed video; Video understanding; Convolution
An Attentive Survey of Attention Models,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122093497&doi=10.1145%2f3465055&partnerID=40&md5=758bdacfa3096f6cc81917d7b44e3f5a,"Attention Model has now become an important concept in neural networks that has been researched within diverse application domains. This survey provides a structured and comprehensive overview of the developments in modeling attention. In particular, we propose a taxonomy that groups existing techniques into coherent categories. We review salient neural architectures in which attention has been incorporated and discuss applications in which modeling attention has shown a significant impact. We also describe how attention has been used to improve the interpretability of neural networks. Finally, we discuss some future research directions in attention. We hope this survey will provide a succinct introduction to attention models and guide practitioners while developing approaches for their applications.  © 2021 Association for Computing Machinery.",Attention; attention models; neural networks,Computer programming; Applications domains; Attention; Attention model; Diverse applications; Future research directions; Interpretability; Neural architectures; Neural-networks; Surveys
A Comprehensive Survey of Grammatical Error Correction,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122074392&doi=10.1145%2f3474840&partnerID=40&md5=5ddd257e632cdecd2b4b9260ab0a18f7,"Grammatical error correction (GEC) is an important application aspect of natural language processing techniques, and GEC system is a kind of very important intelligent system that has long been explored both in academic and industrial communities. The past decade has witnessed significant progress achieved in GEC for the sake of increasing popularity of machine learning and deep learning. However, there is not a survey that untangles the large amount of research works and progress in this field. We present the first survey in GEC for a comprehensive retrospective of the literature in this area. We first give the definition of GEC task and introduce the public datasets and data annotation schema. After that, we discuss six kinds of basic approaches, six commonly applied performance boosting techniques for GEC systems, and three data augmentation methods. Since GEC is typically viewed as a sister task of Machine Translation (MT), we put more emphasis on the statistical machine translation (SMT)-based approaches and neural machine translation (NMT)-based approaches for the sake of their importance. Similarly, some performance-boosting techniques are adapted from MT and are successfully combined with GEC systems for enhancement on the final performance. More importantly, after the introduction of the evaluation in GEC, we make an in-depth analysis based on empirical results in aspects of GEC approaches and GEC systems for a clearer pattern of progress in GEC, where error type analysis and system recapitulation are clearly presented. Finally, we discuss five prospective directions for future GEC researches.  © 2021 Association for Computing Machinery.",Grammatical error correction; machine translation; natural language processing,Computational linguistics; Computer aided language translation; Deep learning; Intelligent systems; Learning algorithms; Machine translation; Natural language processing systems; Surveys; Academic community; Errors correction; Grammatical error correction; Grammatical errors; Industrial communities; Language processing techniques; Large amounts; Machine translations; Performance; Public dataset; Error correction
Fine-Grained Semantic Image Synthesis with Object-Attention Generative Adversarial Network,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122065997&doi=10.1145%2f3470008&partnerID=40&md5=3e520b1f65ff5fe3740c4ab135e35306,"Semantic image synthesis is a new rising and challenging vision problem accompanied by the recent promising advances in generative adversarial networks. The existing semantic image synthesis methods only consider the global information provided by the semantic segmentation mask, such as class label, global layout, and location, so the generative models cannot capture the rich local fine-grained information of the images (e.g., object structure, contour, and texture). To address this issue, we adopt a multi-scale feature fusion algorithm to refine the generated images by learning the fine-grained information of the local objects. We propose OA-GAN, a novel object-attention generative adversarial network that allows attention-driven, multi-fusion refinement for fine-grained semantic image synthesis. Specifically, the proposed model first generates multi-scale global image features and local object features, respectively, then the local object features are fused into the global image features to improve the correlation between the local and the global. In the process of feature fusion, the global image features and the local object features are fused through the channel-spatial-wise fusion block to learn gwhat' and gwhere' to attend in the channel and spatial axes, respectively. The fused features are used to construct correlation filters to obtain feature response maps to determine the locations, contours, and textures of the objects. Extensive quantitative and qualitative experiments on COCO-Stuff, ADE20K and Cityscapes datasets demonstrate that our OA-GAN significantly outperforms the state-of-the-art methods.  © 2021 Association for Computing Machinery.",attention-driven; channel-spatial-wise fusion; GAN; Semantic image synthesis,Image enhancement; Image fusion; Semantic Segmentation; Semantic Web; Semantics; Textures; Attention-driven; Channel-spatial-wise fusion; Features fusions; Fine grained; GAN; Image features; Images synthesis; Local object; Semantic image synthesis; Semantic images; Generative adversarial networks
Identifying Illicit Drug Dealers on Instagram with Large-scale Multimodal Data Fusion,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122079221&doi=10.1145%2f3472713&partnerID=40&md5=68e5a1639e28fcbc908c15baca0da340,"Illicit drug trafficking via social media sites such as Instagram have become a severe problem, thus drawing a great deal of attention from law enforcement and public health agencies. How to identify illicit drug dealers from social media data has remained a technical challenge for the following reasons. On the one hand, the available data are limited because of privacy concerns with crawling social media sites; on the other hand, the diversity of drug dealing patterns makes it difficult to reliably distinguish drug dealers from common drug users. Unlike existing methods that focus on posting-based detection, we propose to tackle the problem of illicit drug dealer identification by constructing a large-scale multimodal dataset named Identifying Drug Dealers on Instagram (IDDIG). Nearly 4,000 user accounts, of which more than 1,400 are drug dealers, have been collected from Instagram with multiple data sources including post comments, post images, homepage bio, and homepage images. We then design a quadruple-based multimodal fusion method to combine the multiple data sources associated with each user account for drug dealer identification. Experimental results on the constructed IDDIG dataset demonstrate the effectiveness of the proposed method in identifying drug dealers (almost 95% accuracy). Moreover, we have developed a hashtag-based community detection technique for discovering evolving patterns, especially those related to geography and drug types.  © 2021 Association for Computing Machinery.",drug dealer; Drug trafficking; Instagram; multimodal data fusion,Data fusion; Large dataset; Social networking (online); Drug dealers; Drug trafficking; Homepage; Illicit drug; Instagram; Large-scales; Multimodal data fusion; Multiple data sources; Public-health agencies; Social media; Crime
Multi-Graph Cooperative Learning towards Distant Supervised Relation Extraction,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122089819&doi=10.1145%2f3466560&partnerID=40&md5=2253378917f5b190d847cfb840f54b56,"The Graph Convolutional Network (GCN) is a universal relation extraction method that can predict relations of entity pairs by capturing sentences' syntactic features. However, existing GCN methods often use dependency parsing to generate graph matrices and learn syntactic features. The quality of the dependency parsing will directly affect the accuracy of the graph matrix and change the whole GCN's performance. Because of the influence of noisy words and sentence length in the distant supervised dataset, using dependency parsing on sentences causes errors and leads to unreliable information. Therefore, it is difficult to obtain credible graph matrices and relational features for some special sentences. In this article, we present a Multi-Graph Cooperative Learning model (MGCL), which focuses on extracting the reliable syntactic features of relations by different graphs and harnessing them to improve the representations of sentences. We conduct experiments on a widely used real-world dataset, and the experimental results show that our model achieves the state-of-the-art performance of relation extraction.  © 2021 Association for Computing Machinery.",distant supervision; graph convolutional network; information extraction; Natural language processing; relation extraction,Convolution; Information retrieval; Learning systems; Syntactics; Convolutional networks; Cooperative learning; Dependency parsing; Distant supervision; Extraction method; Graph convolutional network; matrix; Network methods; Relation extraction; Syntactic features; Natural language processing systems
BATS: A Spectral Biclustering Approach to Single Document Topic Modeling and Segmentation,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119310864&doi=10.1145%2f3468268&partnerID=40&md5=d4383e177da42b9d67e82a9ed2509425,"Existing topic modeling and text segmentation methodologies generally require large datasets for training, limiting their capabilities when only small collections of text are available. In this work, we reexamine the inter-related problems of ""topic identification""and ""text segmentation""for sparse document learning, when there is a single new text of interest. In developing a methodology to handle single documents, we face two major challenges. First is sparse information: with access to only one document, we cannot train traditional topic models or deep learning algorithms. Second is significant noise: a considerable portion of words in any single document will produce only noise and not help discern topics or segments. To tackle these issues, we design an unsupervised, computationally efficient methodology called Biclustering Approach to Topic modeling and Segmentation (BATS). BATS leverages three key ideas to simultaneously identify topics and segment text: (i) a new mechanism that uses word order information to reduce sample complexity, (ii) a statistically sound graph-based biclustering technique that identifies latent structures of words and sentences, and (iii) a collection of effective heuristics that remove noise words and award important words to further improve performance. Experiments on six datasets show that our approach outperforms several state-of-the-art baselines when considering topic coherence, topic diversity, segmentation, and runtime comparison metrics.  © 2021 Association for Computing Machinery.",Biclustering; text segmentation; topic modeling,Deep learning; Graphic methods; Learning algorithms; Biclustering; Large datasets; Model segmentations; Modelling and segmentations; New mechanisms; Text segmentation; Topic identification; Topic Modeling; Topic segmentations; Word orders; Large dataset
Quantized Adam with Error Feedback,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118987282&doi=10.1145%2f3470890&partnerID=40&md5=c55b9bce14eccdd77769a69f3b4fcca5,"In this article, we present a distributed variant of an adaptive stochastic gradient method for training deep neural networks in the parameter-server model. To reduce the communication cost among the workers and server, we incorporate two types of quantization schemes, i.e., gradient quantization and weight quantization, into the proposed distributed Adam. In addition, to reduce the bias introduced by quantization operations, we propose an error-feedback technique to compensate for the quantized gradient. Theoretically, in the stochastic nonconvex setting, we show that the distributed adaptive gradient method with gradient quantization and error feedback converges to the first-order stationary point, and that the distributed adaptive gradient method with weight quantization and error feedback converges to the point related to the quantized level under both the single-worker and multi-worker modes. Last, we apply the proposed distributed adaptive gradient methods to train deep neural networks. Experimental results demonstrate the efficacy of our methods.  © 2021 Association for Computing Machinery.",Adam; error feedback; quantized communication,Errors; Gradient methods; Stochastic models; Stochastic systems; Adam; Adaptive gradient methods; Communication cost; Error feedback; Quantisation; Quantization schemes; Quantized communications; Server modeling; Stochastic gradient methods; Workers'; Deep neural networks
Modeling Customer Experience in a Contact Center through Process Log Mining,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122542460&doi=10.1145%2f3468269&partnerID=40&md5=fb34e20bb451d55fed242339b1b10994,"The use of data mining and modeling methods in service industry is a promising avenue for optimizing current processes in a targeted manner, ultimately reducing costs and improving customer experience. However, the introduction of such tools in already established pipelines often must adapt to the way data is sampled and to its content. In this study, we tackle the challenge of characterizing and predicting customer experience having available only process log data with time-stamp information, without any ground truth feedback from the customers. As a case study, we consider the context of a contact center managed by TeleWare and analyze phone call logs relative to a two months span. We develop an approach to interpret the phone call process events registered in the logs and infer concrete points of improvement in the service management. Our approach is based on latent tree modeling and multi-class Naïve Bayes classification, which jointly allow us to infer a spectrum of customer experiences and test their predictability based on the current data sampling strategy. Moreover, such approach can overcome limitations in customer feedback collection and sharing across organizations, thus having wide applicability and being complementary to tools relying on more heavily constrained data. © 2021 Association for Computing Machinery.",contact center; Customer experience; latent tree model; process log data,Data mining; Forestry; Telephone sets; Trees (mathematics); Contact centers; Customer experience; Data mining methods; Latent tree model; Log data; Log mining; Model method; Phone calls; Process log data; Tree modeling; Sales
DILSA+: Predicting Urban Dispersal Events through Deep Survival Analysis with Enhanced Urban Features,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122524830&doi=10.1145%2f3469085&partnerID=40&md5=4e3d1e0feadceecdd76ae7c61e0160ae,"Urban dispersal events occur when an unexpectedly large number of people leave an area in a relatively short period of time. It is beneficial for the city authorities, such as law enforcement and city management, to have an advance knowledge of such events, as it can help them mitigate the safety risks and handle important challenges such as managing traffic, and so forth. Predicting dispersal events is also beneficial to Taxi drivers and/or ride-sharing services, as it will help them respond to an unexpected demand and gain competitive advantage. Large urban datasets such as detailed trip records and point of interest (POI) data make such predictions achievable. The related literature mainly focused on taxi demand prediction. The pattern of the demand was assumed to be repetitive and proposed methods aimed at capturing those patterns. However, dispersal events are, by definition, violations of those patterns and are, understandably, missed by the methods in the literature. We proposed a different approach in our prior work [32]. We showed that dispersal events can be predicted by learning the complex patterns of arrival and other features that precede them in time. We proposed a survival analysis formulation of this problem and proposed a two-stage framework (DILSA), where a deep learning model predicted the survival function at each point in time in the future. We used that prediction to determine the time of the dispersal event in the future, or its non-occurrence. However, DILSA is subject to a few limitations. First, based on evidence from the data, mobility patterns can vary through time at a given location. DILSA does not distinguish between different mobility patterns through time. Second, mobility patterns are also different for different locations. DILSA does not have the capability to directly distinguish between different locations based on their mobility patterns. In this article, we address these limitations by proposing a method to capture the interaction between POIs and mobility patterns and we create vector representations of locations based on their mobility patterns. We call our new method DILSA+. We conduct extensive case studies and experiments on the NYC Yellow taxi dataset from 2014 to 2016. Results show that DILSA+ can predict events in the next 5 hours with an F1-score of 0.66. It is significantly better than DILSA and the state-of-the-art deep learning approaches for taxi demand prediction. © 2021 Association for Computing Machinery.",Data mining; deep learning; dispersal events; survival analysis,Bioinformatics; Competition; Deep learning; Forecasting; Large dataset; Location; Taxicabs; City management; Deep learning; Demand prediction; Dispersal event; Location based; Mobility pattern; Number of peoples; Short periods; Survival analysis; Urban features; Data mining
Parallel Connected LSTM for Matrix Sequence Prediction with Elusive Correlations,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122501517&doi=10.1145%2f3469437&partnerID=40&md5=fdecfe0ba23c364df77c499d4af8f3dd,"This article is about a challenging problem called matrix sequence prediction, which is motivated from the application of taxi order prediction. Remarkably, the problem differs greatly from previous sequence prediction tasks in the sense that the time-wise correlations are quite elusive; namely, distant entries could be strongly correlated and nearby entries are unnecessarily related. Such distinct specifics make prevalent convolution-recurrence-based methods inadequate to apply. To remedy this trouble, we propose a novel architecture called Parallel Connected LSTM (PcLSTM), which integrates two new mechanisms, Multi-channel Linearized Connection (McLC) and Adaptive Parallel Unit (APU), into the framework of LSTM. Benefiting from the strengths of McLC and APU, our PcLSTM is able to handle well both the elusive correlations within each timestamp and the temporal dependencies across different timestamps, achieving state-of-the-art performance in a set of experiments demonstrated on synthetic and real-world datasets. © 2021 Association for Computing Machinery.",attention mechanism; long short-term memory; Sequence prediction; temporal dependencies,Data mining; Forecasting; Long short-term memory; Attention mechanisms; matrix; Multi channel; New mechanisms; Novel architecture; Parallel-connected; Prediction tasks; Sequence prediction; Temporal dependency; Time-stamp; Taxicabs
TLDS: A Transfer-Learning-Based Delivery Station Location Selection Pipeline,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122545304&doi=10.1145%2f3469084&partnerID=40&md5=f6003a173bc4a6c0771c8e76aaf3d43c,"Delivery stations play important roles in logistics systems. Well-designed delivery station planning can improve delivery efficiency significantly. However, existing delivery station locations are decided by experts, which requires much preliminary research and data collection work. It is not only time consuming but also expensive for logistics companies. Therefore, in this article, we propose a data-driven pipeline that can transfer expert knowledge among cities and automatically allocate delivery stations. Based on existing well-designed station location planning in the source city, we first train a model to learn the expert knowledge about delivery range selection for each station. Then we transfer the learned knowledge to a new city and design three strategies to select delivery stations for the new city. Due to the differences in characteristics among different cities, we adopt a transfer learning method to eliminate the domain difference so that the model can be adapted to a new city well. Finally, we conduct extensive experiments based on real-world datasets and find the proposed method can solve the problem well. © 2021 Association for Computing Machinery.",deep learning; knowledge transfer; station placement; Urban computing,Deep learning; Knowledge management; Location; Data collection; Deep learning; Expert knowledge; Knowledge transfer; Location selection; Logistics system; Station location; Station placement; Transfer learning; Urban computing; Pipelines
Linking Multiple User Identities of Multiple Services from Massive Mobility Traces,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122505209&doi=10.1145%2f3439817&partnerID=40&md5=1b3cf13741b869b175b897bb998f154a,"Understanding the linkability of online user identifiers (IDs) is critical to both service providers (for business intelligence) and individual users (for assessing privacy risks). Existing methods are designed to match IDs across two services but face key challenges of matching multiple services in practice, particularly when users have multiple IDs per service. In this article, we propose a novel system to link IDs across multiple services by exploring the spatial-temporal features of user activities, of which the core idea is that the same user's online IDs are more likely to repeatedly appear at the same location. Specifically, we first utilize a contact graph to capture the ""co-location""of all IDs across multiple services. Based on this graph, we propose a set-wise matching algorithm to discover candidate ID sets and use Bayesian inference to generate confidence scores for candidate ranking, which is proved to be optimal. We evaluate our system using two real-world ground-truth datasets from an Internet service provider (4 services, 815K IDs) and Twitter-Foursquare (2 services, 770 IDs). Extensive results show that our system significantly outperforms the state-of-the-art algorithms in accuracy (AUC is higher by 0.1-0.2), and it is highly robust against data quality, matching order, and number of services. © 2021 Association for Computing Machinery.",Identity linkage; online services; set-wise id matching; spatio-temporal trajectory,Bayesian networks; Internet service providers; Risk assessment; Web services; Identity linkage; Linkability; Matchings; Mobility traces; Multiple services; Multiple user; On-line service; Set-wise id matching; Spatio-temporal trajectories; User identity; Inference engines
StarFL: Hybrid Federated Learning Architecture for Smart Urban Computing,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122534701&doi=10.1145%2f3467956&partnerID=40&md5=60f719daaf9e73eaa0ffbfa444f6f53f,"From facial recognition to autonomous driving, Artificial Intelligence (AI) will transform the way we live and work over the next couple of decades. Existing AI approaches for urban computing suffer from various challenges, including dealing with synchronization and processing of vast amount of data generated from the edge devices, as well as the privacy and security of individual users, including their bio-metrics, locations, and itineraries. Traditional centralized-based approaches require data in each organization be uploaded to the central database, which may be prohibited by data protection acts, such as GDPR and CCPA. To decouple model training from the need to store the data in the cloud, a new training paradigm called Federated Learning (FL) is proposed. FL enables multiple devices to collaboratively learn a shared model while keeping the training data on devices locally, which can significantly mitigate privacy leakage risk. However, under urban computing scenarios, data are often communication-heavy, high-frequent, and asynchronized, posing new challenges to FL implementation. To handle these challenges, we propose a new hybrid federated learning architecture called StarFL. By combining with Trusted Execution Environment (TEE), Secure Multi-Party Computation (MPC), and (Beidou) satellites, StarFL enables safe key distribution, encryption, and decryption, and provides a verification mechanism for each participant to ensure the security of the local data. In addition, StarFL can provide accurate timestamp matching to facilitate synchronization of multiple clients. All these improvements make StarFL more applicable to the security-sensitive scenarios for the next generation of urban computing. © 2021 Association for Computing Machinery.",Federated learning; quantum key distribution; secure multi-party computation; trusted execution environment,Computer architecture; Data privacy; Quantum cryptography; Radio navigation; Trusted computing; Autonomous driving; Bio-metric; Centralised; Facial recognition; Federated learning; Learning architectures; Privacy and security; Secure multi-party computation; Trusted execution environments; Urban computing; Face recognition
Multiview Common Subspace Clustering via Coupled Low Rank Representation,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122492165&doi=10.1145%2f3465056&partnerID=40&md5=18c6995534ae62d19eaaa956608f8d87,"Multi-view subspace clustering (MVSC) finds a shared structure in latent low-dimensional subspaces of multi-view data to enhance clustering performance. Nonetheless, we observe that most existing MVSC methods neglect the diversity in multi-view data by considering only the common knowledge to find a shared structure either directly or by merging different similarity matrices learned for each view. In the presence of noise, this predefined shared structure becomes a biased representation of the different views. Thus, in this article, we propose a MVSC method based on coupled low-rank representation to address the above limitation. Our method first obtains a low-rank representation for each view, constrained to be a linear combination of the view-specific representation and the shared representation by simultaneously encouraging the sparsity of view-specific one. Then, it uses the k-block diagonal regularizer to learn a manifold recovery matrix for each view through respective low-rank matrices to recover more manifold structures from them. In this way, the proposed method can find an ideal similarity matrix by approximating clustering projection matrices obtained from the recovery structures. Hence, this similarity matrix denotes our clustering structure with exactly k connected components by applying a rank constraint on the similarity matrix's relaxed Laplacian matrix to avoid spectral post-processing of the low-dimensional embedding matrix. The core of our idea is such that we introduce dynamic approximation into the low-rank representation to allow the clustering structure and the shared representation to guide each other to learn cleaner low-rank matrices that would lead to a better clustering structure. Therefore, our approach is notably different from existing methods in which the local manifold structure of data is captured in advance. Extensive experiments on six benchmark datasets show that our method outperforms 10 similar state-of-the-art compared methods in six evaluation metrics. © 2021 Association for Computing Machinery.",adaptive clustering structure; block diagonal; Multiview low-rank representation; multiview subspace clustering,Clustering algorithms; Data mining; Matrix algebra; Adaptive clustering; Adaptive clustering structure; Block diagonal; Clusterings; Low-rank representations; Multi-views; Multiview low-rank representation; Multiview subspace clustering; Subspace clustering; Recovery
Nova: Value-based Negotiation of Norms,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122500845&doi=10.1145%2f3465054&partnerID=40&md5=83198cc06e924ead95a070dcd7b50c7d,"Specifying a normative multiagent system (nMAS) is challenging, because different agents often have conflicting requirements. Whereas existing approaches can resolve clear-cut conflicts, tradeoffs might occur in practice among alternative nMAS specifications with no apparent resolution. To produce an nMAS specification that is acceptable to each agent, we model the specification process as a negotiation over a set of norms. We propose an agent-based negotiation framework, where agents' requirements are represented as values (e.g., patient safety, privacy, and national security), and an agent revises the nMAS specification to promote its values by executing a set of norm revision rules that incorporate ontology-based reasoning. To demonstrate that our framework supports creating a transparent and accountable nMAS specification, we conduct an experiment with human participants who negotiate against our agent. Our findings show that our negotiation agent reaches better agreements (with small p-value and large effect size) faster than a baseline strategy. Moreover, participants perceive that our agent enables more collaborative and transparent negotiations than the baseline (with small p-value and large effect size in particular settings) toward reaching an agreement. © 2021 Association for Computing Machinery.",conflicting requirements; human-agent negotiation; Sociotechnical systems,Multi agent systems; National security; Ontology; Agent negotiations; Conflicting requirement; Effect size; Human agent; Human-agent negotiation; Normative multiagent systems; P-values; Sociotechnical systems; Systems specification; Value-based; Specifications
Mining Customers' Changeable Electricity Consumption for Effective Load Forecasting,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122532710&doi=10.1145%2f3466684&partnerID=40&md5=fb39c33d98a8217c93feda45dd669c88,"Most existing approaches for electricity load forecasting perform the task based on overall electricity consumption. However, using such a global methodology can affect load forecasting accuracy, as it does not consider the possibility that customers' consumption behavior may change at any time. Predicting customers' electricity consumption in the presence of unstable behaviors poses challenges to existing models. In this article, we propose a principled approach capable of handling customers' changeable electricity consumption. We devise a network-based method that first builds and tracks clusters of customer consumption patterns over time. Then, on the evolving clusters, we develop a framework that exploits long short-term memory recurrent neural network and survival analysis techniques to forecast electricity consumption. Our experiments on real electricity consumption datasets illustrate the suitability of the proposed approach.  © 2021 Association for Computing Machinery.",clustering; dynamic networks; forecasting; survival analysis; Time series,Bioinformatics; Electric power plant loads; Electric power utilization; Recurrent neural networks; Sales; Time series; Time series analysis; Clusterings; Dynamic network; Electricity load forecasting; Electricity-consumption; Forecasting accuracy; Load forecasting; Survival analysis; Task-based; Times series; Unstable behavior; Forecasting
MKEL: Multiple Kernel Ensemble Learning via Unified Ensemble Loss for Image Classification,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122538356&doi=10.1145%2f3457217&partnerID=40&md5=87ff968a3010e59ed2609c01dc031595,"In this article, a novel ensemble model, called Multiple Kernel Ensemble Learning (MKEL), is developed by introducing a unified ensemble loss. Different from the previous multiple kernel learning (MKL) methods, which attempt to seek a linear combination of basis kernels as a unified kernel, our MKEL model aims to find multiple solutions in corresponding Reproducing Kernel Hilbert Spaces (RKHSs) simultaneously. To achieve this goal, multiple individual kernel losses are integrated into a unified ensemble loss. Therefore, each model can co-optimize to learn its optimal parameters by minimizing a unified ensemble loss in multiple RKHSs. Furthermore, we apply our proposed ensemble loss into the deep network paradigm and take the sub-network as a kernel mapping from the original input space into a feature space, named Deep-MKEL (D-MKEL). Our D-MKEL model can utilize the diversified deep individual sub-networks into a whole unified network to improve the classification performance. With this unified loss design, our D-MKEL model can make our network much wider than other traditional deep kernel networks and more parameters are learned and optimized. Experimental results on several mediate UCI classification and computer vision datasets demonstrate that our MKEL model can achieve the best classification performance among comparative MKL methods, such as Simple MKL, GMKL, Spicy MKL, and Matrix-Regularized MKL. On the contrary, experimental results on large-scale CIFAR-10 and SVHN datasets concretely show the advantages and potentialities of the proposed D-MKEL approach compared to state-of-the-art deep kernel methods. © 2021 Association for Computing Machinery.",deep networks; ensemble learning; Ensemble loss; multiple kernel learning,Classification (of information); Image classification; Large dataset; Classification performance; Deep network; Ensemble learning; Ensemble loss; Kernel learning methods; Learning models; Multiple Kernel Learning; Multiple kernels; Reproducing Kernel Hilbert spaces; Subnetworks; Deep learning
VSumVis: Interactive Visual Understanding and Diagnosis of Video Summarization Model,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118674781&doi=10.1145%2f3458928&partnerID=40&md5=d6a9a2ae49fc018a76aad2141c4d1078,"With the rapid development of mobile Internet, the popularity of video capture devices has brought a surge in multimedia video resources. Utilizing machine learning methods combined with well-designed features, we could automatically obtain video summarization to relax video resource consumption and retrieval issues. However, there always exists a gap between the summarization obtained by the model and the ones annotated by users. How to help users understand the difference, provide insights in improving the model, and enhance the trust in the model remains challenging in the current study. To address these challenges, we propose VSumVis under a user-centered design methodology, a visual analysis system with multi-feature examination and multi-level exploration, which could help users explore and analyze video content, as well as the intrinsic relationship that existed in our video summarization model. The system contains multiple coordinated views, i.e., video view, projection view, detail view, and sequential frames view. A multi-level analysis process to integrate video events and frames are presented with clusters and nodes visualization in our system. Temporal patterns concerning the difference between the manual annotation score and the saliency score produced by our model are further investigated and distinguished with sequential frames view. Moreover, we propose a set of rich user interactions that enable an in-depth, multi-faceted analysis of the features in our video summarization model. We conduct case studies and interviews with domain experts to provide anecdotal evidence about the effectiveness of our approach. Quantitative feedback from a user study confirms the usefulness of our visual system for exploring the video summarization model. © 2021 Association for Computing Machinery.",machine learning; multimedia visual analysis; video summarization; video visualization; Visual Analytics,Machine learning; Multimedia systems; Video recording; Machine learning methods; Mobile Internet; Multimedia video; Multimedium visual analyse; Summarization models; Video capture; Video summarization; Video visualization; Visual analysis; Visual analytics; Visualization
Modeling Complementarity in Behavior Data with Multi-Type Itemset Embedding,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122511578&doi=10.1145%2f3458724&partnerID=40&md5=ce94d8df9571bee80d28ac4f8a669d6a,"People are looking for complementary contexts, such as team members of complementary skills for project team building and/or reading materials of complementary knowledge for effective student learning, to make their behaviors more likely to be successful. Complementarity has been revealed by behavioral sciences as one of the most important factors in decision making. Existing computational models that learn low-dimensional context representations from behavior data have poor scalability and recent network embedding methods only focus on preserving the similarity between the contexts. In this work, we formulate a behavior entry as a set of context items and propose a novel representation learning method, Multi-type Itemset Embedding, to learn the context representations preserving the itemset structures. We propose a measurement of complementarity between context items in the embedding space. Experiments demonstrate both effectiveness and efficiency of the proposed method over the state-of-the-art methods on behavior prediction and context recommendation. We discover that the complementary contexts and similar contexts are significantly different in human behaviors. © 2021 Association for Computing Machinery.",Behavior modeling; prediction; recommendation; representation learning,Data mining; Decision making; Knowledge management; Network embeddings; Students; Behaviour models; Context representation; Embeddings; Itemset; Learn+; Project team; Recommendation; Representation learning; Team building; Team members; Behavioral research
A Comprehensive Survey of the Key Technologies and Challenges Surrounding Vehicular Ad Hoc Networks,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118282133&doi=10.1145%2f3451984&partnerID=40&md5=4ad23b389c316497524fddcb40bb21d5,"Vehicular ad hoc networks (VANETs) and the services they support are an essential part of intelligent transportation. Through physical technologies, applications, protocols, and standards, they help to ensure traffic moves efficiently and vehicles operate safely. This article surveys the current state of play in VANETs development. The summarized and classified include the key technologies critical to the field, the resource-management and safety applications needed for smooth operations, the communications and data transmission protocols that support networking, and the theoretical and environmental constructs underpinning research and development, such as graph neural networks and the Internet of Things. Additionally, we identify and discuss several challenges facing VANETs, including poor safety, poor reliability, non-uniform standards, and low intelligence levels. Finally, we touch on hot technologies and techniques, such as reinforcement learning and 5G communications, to provide an outlook for the future of intelligent transportation systems. © 2021 Association for Computing Machinery.",deep learning; emergency message broadcast; graph neural networks; machine learning; reinforcement learning; VANETs; Vehicular ad hoc networks,5G mobile communication systems; Deep learning; Environmental technology; Information management; Intelligent systems; Reinforcement learning; Vehicular ad hoc networks; Deep learning; Emergency message broadcast; Emergency messages; Graph neural networks; Intelligent transportation; Key technologies; Message broadcasts; Technology application; Vehicular Adhoc Networks (VANETs); Surveys
A Camera Identity-guided Distribution Consistency Method for Unsupervised Multi-target Domain Person Re-identification,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121689709&doi=10.1145%2f3454130&partnerID=40&md5=8f1c412df9f7ffb910ae11d1d3181f26,"Unsupervised domain adaptation (UDA) for person re-identification (re-ID) is a challenging task due to large variations in human classes, illuminations, camera views, and so on. Currently, existing UDA methods focus on two-domain adaptation and are generally trained on one labeled source set and adapted on the other unlabeled target set. In this article, we put forward a new issue on person re-ID, namely, unsupervised multi-target domain adaptation (UMDA). It involves one labeled source set and multiple unlabeled target sets, which is more reasonable for practical real-world applications. Enabling UMDA has to learn the consistency for multiple domains, which is significantly different from the UDA problem. To ensure distribution consistency and learn the discriminative embedding, we further propose the Camera Identity-guided Distribution Consistency method that performs an alignment operation for multiple domains. The camera identities are encoded into the image semantic information to facilitate the adaptation of features. According to our knowledge, this is the first attempt on the unsupervised multi-target domain adaptation learning. Extensive experiments are executed on Market-1501, DukeMTMC-reID, MSMT17, PersonX, and CUHK03, and our method has achieved very competitive re-ID accuracy in multi-target domains against numerous state-of-the-art methods.  © 2021 Association for Computing Machinery.",camera identity; distribution consistency; Person re-identification; unsupervised multi-target domain adaptation,Semantics; Camera identity; Camera view; Distribution consistency; Domain adaptation; Learn+; Multi-targets; Multiple domains; Person re identifications; Target domain; Unsupervised multi-target domain adaptation; Cameras
Improved Fake Reviews Detection Model Based on Vertical Ensemble Tri-Training and Active Learning,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121497820&doi=10.1145%2f3450285&partnerID=40&md5=1b0c466654852dc731f3cab41eeb964e,"People's increasingly frequent online activity has generated a large number of reviews, whereas fake reviews can mislead users and harm their personal interests. In addition, it is not feasible to label reviews on a large scale because of the high cost of manual labeling. Therefore, to improve the detection performance by utilizing the unlabeled reviews, this article proposes a fake reviews detection model based on vertical ensemble tri-training and active learning (VETT-AL). The model combines the features of review text with the user behavior features as feature extraction. In the VETT-AL algorithm, the iterative process is divided into two parts: vertical integration within the group and horizontal integration among the groups. The intra-group integration is to integrate three original classifiers by using the previous iterative models of the classifiers. The inter-group integration is to adopt the active learning based on entropy to select the data with the highest confidence and label it, and as the result of that, the second generation classifiers are trained by the traditional process to improve the accuracy of the label. Experimental results show that the proposed model has a good classification performance.  © 2021 Association for Computing Machinery.",active learning; Fake reviews; iterative classifiers; label accuracy; tri-training,Artificial intelligence; Behavioral research; Fake detection; Integration; Learning systems; Active Learning; Detection models; Fake review; Fake review detections; Iterative classifier; Label accuracy; Large-scales; Model-based OPC; Online activities; Tri-training; Iterative methods
A scale and rotational invariant key-point detector based on sparse coding,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122616201&doi=10.1145%2f3452009&partnerID=40&md5=2b588ba5d9ff976116e11201bde86ad4,"Most popular hand-crafted key-point detectors such as Harris corner, SIFT, SURF aim to detect corners, blobs, junctions, or other human-defined structures in images. Though being robust with some geometric transformations, unintended scenarios or non-uniform lighting variations could significantly degrade their performance. Hence, a new detector that is flexible with context change and simultaneously robust with both geometric and non-uniform illumination variations is very desirable. In this article, we propose a solution to this challenging problem by incorporating Scale and Rotation Invariant design (named SRI-SCK) into a recently developed Sparse Coding based Key-point detector (SCK). The SCK detector is flexible in different scenarios and fully invariant to affine intensity change, yet it is not designed to handle images with drastic scale and rotation changes. In SRI-SCK, the scale invariance is implemented with an image pyramid technique, while the rotation invariance is realized by combining multiple rotated versions of the dictionary used in the sparse coding step of SCK. Techniques for calculation of key-points' characteristic scales and their sub-pixel accuracy positions are also proposed. Experimental results on three public datasets demonstrate that significantly high repeatability and matching score are achieved. © 2021 Association for Computing Machinery.",Feature detector; Interest point; Key-point; Sparse coding,Codes (symbols); Edge detection; Image coding; Rotation; Feature detector; Geometric transformations; Harris corners; Interest point; Key-point detectors; Keypoints; Rotational invariants; Scale and rotation; Scale-invariant; Sparse coding; Feature extraction
Vector-quantization-based topic modeling,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122632369&doi=10.1145%2f3450946&partnerID=40&md5=0afc06ae105b2809ee0476c2302dcb29,"With the purpose of learning and utilizing explicit and dense topic embeddings, we propose three variations of novel vector-quantization-based topic models (VQ-TMs): (1) Hard VQ-TM, (2) Soft VQ-TM, and (3) Multi-View Soft VQ-TM. The model family capitalize on vector quantization techniques, embedded input documents, and viewing words as mixtures of topics. Guided by a comprehensive set of evaluation metrics, we conduct systematic quantitative and qualitative empirical studies, and demonstrate the superior performance of VQ-TMs compared to important baseline models. Through a unique case study on code generation from natural language descriptions, we further illustrate the power of VQ-TMs in downstream tasks. © 2021 Association for Computing Machinery.",Deep learning; Knowledge discovery; Self-supervised learning,Data mining; Vector quantization; Baseline models; Deep learning; Embeddings; Empirical studies; Evaluation metrics; Multi-views; Performance; Self-supervised learning; Topic Modeling; Vector quantisation; Deep learning
PP-PG: Combining parameter perturbation with policy gradient methods for effective and efficient explorations in deep reinforcement learning,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122614572&doi=10.1145%2f3452008&partnerID=40&md5=fe899fcb069fd5ca04b21ecbcf8c7804,"Efficient and stable exploration remains a key challenge for deep reinforcement learning (DRL) operating in high-dimensional action and state spaces. Recently, a more promising approach by combining the exploration in the action space with the exploration in the parameters space has been proposed to get the best of both methods. In this article, we propose a new iterative and close-loop framework by combining the evolutionary algorithm (EA), which does explorations in a gradient-free manner directly in the parameters space with an actor-critic, and the deep deterministic policy gradient (DDPG) reinforcement learning algorithm, which does explorations in a gradient-based manner in the action space to make these two methods cooperate in a more balanced and efficient way. In our framework, the policies represented by the EA population (the parametric perturbation part) can evolve in a guided manner by utilizing the gradient information provided by the DDPG and the policy gradient part (DDPG) is used only as a fine-tuning tool for the best individual in the EA population to improve the sample efficiency. In particular, we propose a criterion to determine the training steps required for the DDPG to ensure that useful gradient information can be generated from the EA generated samples and the DDPG and EA part can work together in a more balanced way during each generation. Furthermore, within the DDPG part, our algorithm can flexibly switch between fine-tuning the same previous RL-Actor and fine-tuning a new one generated by the EA according to different situations to further improve the efficiency. Experiments on a range of challenging continuous control benchmarks demonstrate that our algorithm outperforms related works and offers a satisfactory trade-off between stability and sample efficiency. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Computing methodologies →Artificial intelligence; DDPG; Deep reinforcement learning; EA; Machine learning; Modeling and simulation; Parameter perturbation; Policy gradient,Economic and social effects; Efficiency; Evolutionary algorithms; Gradient methods; Learning algorithms; Parameter estimation; Reinforcement learning; Action spaces; Computing methodologies; Computing methodology →artificial intelligence; Deep deterministic policy gradient; Deterministics; Fine tuning; Model and simulation; Parameter perturbation; Policy gradient; Deep learning
Intelligent system of game-theory-based decision making in smart sports industry,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113857525&doi=10.1145%2f3447986&partnerID=40&md5=ec05c227148808ab58596dd0d4e8b5ee,"Internet of Things (IoT) technology backed by Artificial Intelligence (AI) techniques has been increasingly utilized for the realization of the Industry 4.0 vision. Conspicuously, this work provides a novel notion of the smart sports industry for provisioning efficient services in the sports arena. Specifically, an IoT-inspired framework has been proposed for real-time analysis of athlete performance. IoT data is utilized to quantify athlete performance in the terms of probability parameters of Probabilistic Measure of Performance (PMP) and Level of Performance Measure (LoPM). Moreover, a two-player game-theory-based mathematical framework has been presented for efficient decision modeling by the monitoring officials. The presented model is validated experimentally by deployment in District Sports Academy (DSA) for 60 days over four players. Based on the comparative analysis with state-of-the-art decision-modeling approaches, the proposed model acquired enhanced performance values in terms of Temporal Delay, Classification Efficiency, Statistical Efficacy, Correlation Analysis, and Reliability. © 2021 Association for Computing Machinery.",Game theory; Internet of Things (IoT); Level of performance measure (LoPM); Probabilistic measure of performance (PMP),Decision making; Decision theory; Intelligent systems; Internet of things; Reliability analysis; Sports; Decision modeling; Decisions makings; Internet of thing; Level of performance measure; Measure of performance; Performance; Performance measure; Probabilistic measure of performance; Probabilistic measures; Sports industries; Game theory
GTAE: Graph transformer based auto-encoders for linguistic-constrained text style transfer,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122617690&doi=10.1145%2f3448733&partnerID=40&md5=abbd029d8c10d954a5905a67f73fdd5c,"Non-parallel text style transfer has attracted increasing research interests in recent years. Despite successes in transferring the style based on the encoder-decoder framework, current approaches still lack the ability to preserve the content and even logic of original sentences, mainly due to the large unconstrained model space or too simplified assumptions on latent embedding space. Since language itself is an intelligent product of humans with certain grammars and has a limited rule-based model space by its nature, relieving this problem requires reconciling the model capacity of deep neural networks with the intrinsic model constraints from human linguistic rules. To this end, we propose a method called Graph Transformer-based Auto-Encoder, which models a sentence as a linguistic graph and performs feature extraction and style transfer at the graph level, to maximally retain the content and the linguistic structure of original sentences. Quantitative experiment results on three non-parallel text style transfer tasks show that our model outperforms state-of-the-art methods in content preservation, while achieving comparable performance on transfer accuracy and sentence naturalness. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph neural network; Natural language processing; Text style transfer,Data mining; Deep neural networks; Graph neural networks; Linguistics; Network coding; 'current; Auto encoders; Embeddings; Encoder-decoder; Graph neural networks; Model spaces; Parallel text; Research interests; Rule-based models; Text style transfer; Natural language processing systems
Improving action recognition via temporal and complementary learning,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122611983&doi=10.1145%2f3447686&partnerID=40&md5=f02cb702e5dfec43eacc5adbb865379b,"In this article, we study the problem of video-based action recognition. We improve the action recognition performance by finding an effective temporal and appearance representation. For capturing the temporal representation, we introduce two temporal learning techniques for improving long-term temporal information modeling, specifically Temporal Relational Network and Temporal Second-Order Pooling-based Network. Moreover, we harness the representation using complementary learning techniques, specifically Global-Local Network and Fuse-Inception Network. Performance evaluation on three datasets (UCF101, HMDB-51, and Mini-Kinetics-200) demonstrated the superiority of the proposed framework compared to the 2D Deep ConvNets-based state-of-the-art techniques. © 2021 Association for Computing Machinery.",Deep ConvNets; Two-stream networks,Learning systems; Action recognition; Complementary learning; Convnet; Deep convnet; Learning techniques; Performance; Stream networks; Temporal learning; Two-stream; Two-stream network; Learning algorithms
"A GDPR-compliant ecosystem for speech recognition with transfer, federated, and evolutionary learning",2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117033880&doi=10.1145%2f3447687&partnerID=40&md5=7038eb9b8f15ab9e531605d18c05b82e,"Automatic Speech Recognition (ASR) is playing a vital role in a wide range of real-world applications. However, Commercial ASR solutions are typically ""one-size-fits-all""products and clients are inevitably faced with the risk of severe performance degradation in field test. Meanwhile, with new data regulations such as the European Union's General Data Protection Regulation (GDPR) coming into force, ASR vendors, which traditionally utilize the speech training data in a centralized approach, are becoming increasingly helpless to solve this problem, since accessing clients' speech data is prohibited. Here, we show that by seamlessly integrating three machine learning paradigms (i.e., Transfer learning, Federated learning, and Evolutionary learning (TFE)), we can successfully build a win-win ecosystem for ASR clients and vendors and solve all the aforementioned problems plaguing them. Through large-scale quantitative experiments, we show that with TFE, the clients can enjoy far better ASR solutions than the ""one-size-fits-all""counterpart, and the vendors can exploit the abundance of clients' data to effectively refine their own ASR products. © 2021 Association for Computing Machinery.",Evolutionary learning; Federated learning; Speech recognition; Transfer learning,Ecosystems; Learning systems; Speech; Automatic speech recognition; Data regulations; Evolutionary Learning; Federated learning; Field test; General data protection regulations; In-field; Performance degradation; Real-world; Transfer learning; Speech recognition
MetaStore: A task-adaptative meta-learning model for optimal store placement with multi-city knowledge transfer,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122622156&doi=10.1145%2f3447271&partnerID=40&md5=feb7238e9f2c5b68f9db8243a6a4dec9,"Optimal store placement aims to identify the optimal location for a new brick-and-mortar store that can maximize its sale by analyzing and mining users' preferences from large-scale urban data. In recent years, the expansion of chain enterprises in new cities brings some challenges because of two aspects: (1) data scarcity in new cities, so most existing models tend to not work (i.e., overfitting), because the superior performance of these works is conditioned on large-scale training samples; (2) data distribution discrepancy among different cities, so knowledge learned from other cities cannot be utilized directly in new cities. In this article, we propose a task-adaptative model-agnostic meta-learning framework, namely, MetaStore, to tackle these two challenges and improve the prediction performance in new cities with insufficient data for optimal store placement, by transferring prior knowledge learned from multiple data-rich cities. Specifically, we develop a task-adaptative meta-learning algorithm to learn city-specific prior initializations from multiple cities, which is capable of handling the multimodal data distribution and accelerating the adaptation in new cities compared to other methods. In addition, we design an effective learning strategy for MetaStore to promote faster convergence and optimization by sampling high-quality data for each training batch in view of noisy data in practical applications. The extensive experimental results demonstrate that our proposed method leads to state-of-the-art performance compared with various baselines. © 2021 Association for Computing Machinery.",Knowledge transfer; Machine learning; Meta-learning; Optimal store placement; Urban computing,Data handling; Learning algorithms; Machine learning; Data distribution; Data scarcity; Knowledge transfer; Large-scales; Meta-learning models; Metalearning; Optimal locations; Optimal store placement; Urban computing; User's preferences; Knowledge management
MVGAN: Multi-view graph attention network for social event detection,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122616038&doi=10.1145%2f3447270&partnerID=40&md5=11c71e840e93d82a163ab0716412cb32,"Social networks are critical sources for event detection thanks to the characteristics of publicity and dissemination. Unfortunately, the randomness and semantic sparsity of the social network text bring significant challenges to the event detection task. In addition to text, time is another vital element in reflecting events since events are often followed for a while. Therefore, in this article, we propose a novel method named Multi-View Graph Attention Network (MVGAN) for event detection in social networks. It enriches event semantics through both neighbor aggregation and multi-view fusion in a heterogeneous social event graph. Specifically, we first construct a heterogeneous graph by adding the hashtag to associate the isolated short texts and describe events comprehensively. Then, we learn view-specific representations of events through graph convolutional networks from the perspectives of text semantics and time distribution, respectively. Finally, we design a hashtag-based multi-view graph attention mechanism to capture the intrinsic interaction across different views and integrate the feature representations to discover events. Extensive experiments on public benchmark datasets demonstrate that MVGAN performs favorably against many state-of-the-art social network event detection algorithms. It also proves that more meaningful signals can contribute to improving the event detection effect in social networks, such as published time and hashtags. © 2021 Association for Computing Machinery.",Event detection; Hashtag attention; Heterogeneous graph; Multi-view,Social networking (online); Detection tasks; Event graphs; Event semantics; Events detection; Hashtag attention; Hashtags; Heterogeneous graph; Multi-views; Novel methods; Social events; Semantics
Conditional Text Generation for Harmonious Human-Machine Interaction,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102787123&doi=10.1145%2f3439816&partnerID=40&md5=b7099a298657c8de8280e2ea164eaf0e,"In recent years, with the development of deep learning, text-generation technology has undergone great changes and provided many kinds of services for human beings, such as restaurant reservation and daily communication. The automatically generated text is becoming more and more fluent so researchers begin to consider more anthropomorphic text-generation technology, that is, the conditional text generation, including emotional text generation, personalized text generation, and so on. Conditional Text Generation (CTG) has thus become a research hotspot. As a promising research field, we find that much attention has been paid to exploring it. Therefore, we aim to give a comprehensive review of the new research trends of CTG. We first summarize several key techniques and illustrate the technical evolution route in the field of neural text generation, based on the concept model of CTG. We further make an investigation of existing CTG fields and propose several general learning models for CTG. Finally, we discuss the open issues and promising research directions of CTG.  © 2021 ACM.",conditional text generation; deep learning; dialog systems; Human-computer interaction; personalization,Automatically generated; Concept model; General learning; Human machine interaction; Research fields; Research trends; Technical evolution; Text generations; Deep learning
Causal Mechanism Transfer Network for Time Series Domain Adaptation in Mechanical Systems,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102805835&doi=10.1145%2f3445033&partnerID=40&md5=1c3b7bf18863ff08549600dd28deef01,"Data-driven models are becoming essential parts in modern mechanical systems, commonly used to capture the behavior of various equipment and varying environmental characteristics. Despite the advantages of these data-driven models on excellent adaptivity to high dynamics and aging equipment, they are usually hungry for massive labels, mostly contributed by human engineers at a high cost. Fortunately, domain adaptation enhances the model generalization by utilizing the labeled source data and the unlabeled target data. However, the mainstream domain adaptation methods cannot achieve ideal performance on time series data, since they assume that the conditional distributions are equal. This assumption works well in the static data but is inapplicable for the time series data. Even the first-order Markov dependence assumption requires the dependence between any two consecutive time steps. In this article, we assume that the causal mechanism is invariant and present our Causal Mechanism Transfer Network (CMTN) for time series domain adaptation. By capturing causal mechanisms of time series data, CMTN allows the data-driven models to exploit existing data and labels from similar systems, such that the resulting model on a new system is highly reliable even with limited data. We report our empirical results and lessons learned from two real-world case studies, on chiller plant energy optimization and boiler fault detection, which outperform the existing state-of-the-art method.  © 2021 ACM.",AI and engineering; general AI techniques,Fault detection; Mechanics; Time series; Conditional distribution; Data-driven model; Energy optimization; Environmental characteristic; Ideal performance; Mechanical systems; Model generalization; State-of-the-art methods; Mechanisms
Constraint-based Scheduling for Paint Shops in the Automotive Supply Industry,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102814265&doi=10.1145%2f3430710&partnerID=40&md5=ffbe8e985bcc76b2043584ea5df05f25,"Factories in the automotive supply industry paint a large number of items requested by car manufacturing companies on a daily basis. As these factories face numerous constraints and optimization objectives, finding a good schedule becomes a challenging task in practice, and full-time employees are expected to manually create feasible production plans. In this study, we propose novel constraint programming models for a real-life paint shop scheduling problem. We evaluate and compare our models experimentally by performing a series of benchmark experiments using real-life instances in the industry. We also show that the decision variant of the paint shop scheduling problem is NP-complete.  © 2021 ACM.",Constraint programming; exact methods; NP-complete; paint shop scheduling,Computer programming; Paint; Production control; Scheduling; Automotive supply industries; Benchmark experiments; Car manufacturing; Constraint programming model; Constraint-based scheduling; Full-time employees; NP Complete; Production plans; Constrained optimization
Attentive Excitation and Aggregation for Bilingual Referring Image Segmentation,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102817779&doi=10.1145%2f3446345&partnerID=40&md5=159d7c582338bb8961ea50fd0ffb5746,"The goal of referring image segmentation is to identify the object matched with an input natural language expression. Previous methods only support English descriptions, whereas Chinese is also broadly used around the world, which limits the potential application of this task. Therefore, we propose to extend existing datasets with Chinese descriptions and preprocessing tools for training and evaluating bilingual referring segmentation models. In addition, previous methods also lack the ability to collaboratively learn channel-wise and spatial-wise cross-modal attention to well align visual and linguistic modalities. To tackle these limitations, we propose a Linguistic Excitation module to excite image channels guided by language information and a Linguistic Aggregation module to aggregate multimodal information based on image-language relationships. Since different levels of features from the visual backbone encode rich visual information, we also propose a Cross-Level Attentive Fusion module to fuse multilevel features gated by language information. Extensive experiments on four English and Chinese benchmarks show that our bilingual referring image segmentation model outperforms previous methods.  © 2021 ACM.",Bilingual referring segmentation; channel excitation; spatial aggregation,Linguistics; Visual languages; Image channels; Image segmentation model; Language informations; Multi-modal information; Natural language expressions; Preprocessing tools; Segmentation models; Visual information; Image segmentation
Feature Grouping-based Trajectory Outlier Detection over Distributed Streams,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102822169&doi=10.1145%2f3444753&partnerID=40&md5=eadbf4b4ed52092cab247fb656b83c1f,"Owing to a wide variety of deployment of GPS-enabled devices, tremendous amounts of trajectories have been generated in distributed stream manner. It opens up new opportunities to track and analyze the moving behaviors of the entities. In this work, we focus on the issue of outlier detection over distributed trajectory streams, where the outliers refer to a few entities whose motion behaviors are significantly different from their local neighbors. In view of skewed distribution property and evolving nature of trajectory data, and on-the-fly detection requirement over distributed streams, we first design a high-efficiency outlier detection solution. It consists of identifying abnormal trajectory fragment and exceptional fragment cluster at the remote sites and then detecting abnormal evolving object at the coordinator site. Further, given that outlier detection accuracy would be damaged due to using inappropriate proximity thresholds or a few trajectory data not having sufficient neighbors at the remote sites, we extract proximity thresholds of different regions and spatial context relationship of each region from historical data to improve the precision. Built upon this is an improved version consisting of off-line modeling phase and on-line detection phase. During the on-line phase, the proximity thresholds that are derived from historical trajectories during the off-line phase are leveraged to assist in detecting abnormal trajectory fragments and exceptional fragment clusters at the remote sites. Additionally, at the coordinator site, the detection results of some remote sites can be refined by incorporating those of other remote sites with neighborhood relationship. Extensive experimental results on real data demonstrate that our proposed methods own high detection validity, less communication cost and linear scalability for online identifying outliers over distributed trajectory streams.  © 2021 ACM.",Distributed trajectory streams; feature-grouping; outlier detection; scalability,Anomaly detection; Data streams; Object detection; Statistics; Communication cost; Detection accuracy; Feature grouping; High-efficiency; Moving behavior; On-line detection; Skewed distribution; Trajectory streams; Trajectories
Dynamic Planning of Bicycle Stations in Dockless Public Bicycle-sharing System Using Gated Graph Neural Network,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102792488&doi=10.1145%2f3446342&partnerID=40&md5=04878d7c67017bd0ecaa5a13f1521c7f,"Benefiting from convenient cycling and flexible parking locations, the Dockless Public Bicycle-sharing (DL-PBS) network becomes increasingly popular in many countries. However, redundant and low-utility stations waste public urban space and maintenance costs of DL-PBS vendors. In this article, we propose a Bicycle Station Dynamic Planning (BSDP) system to dynamically provide the optimal bicycle station layout for the DL-PBS network. The BSDP system contains four modules: bicycle drop-off location clustering, bicycle-station graph modeling, bicycle-station location prediction, and bicycle-station layout recommendation. In the bicycle drop-off location clustering module, candidate bicycle stations are clustered from each spatio-temporal subset of the large-scale cycling trajectory records. In the bicycle-station graph modeling module, a weighted digraph model is built based on the clustering results and inferior stations with low station revenue and utility are filtered. Then, graph models across time periods are combined to create a graph sequence model. In the bicycle-station location prediction module, the GGNN model is used to train the graph sequence data and dynamically predict bicycle stations in the next period. In the bicycle-station layout recommendation module, the predicted bicycle stations are fine-tuned according to the government urban management plan, which ensures that the recommended station layout is conducive to city management, vendor revenue, and user convenience. Experiments on actual DL-PBS networks verify the effectiveness, accuracy, and feasibility of the proposed BSDP system.  © 2021 ACM.",Bicycle station layout; dockless PBS network; gated graph neural network; public bicycle-sharing system,Clustering algorithms; Drops; Forecasting; Graph theory; Location; Neural networks; Urban planning; Bicycle stations; Clustering results; Dynamic planning; Graph neural networks; Maintenance cost; Parking locations; Public bicycle sharing systems; Urban management; Bicycles
Indirectly Supervised Anomaly Detection of Clinically Meaningful Health Events from Smart Home Data,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102843122&doi=10.1145%2f3439870&partnerID=40&md5=487e20cc923e3624ada580a8377672f8,"Anomaly detection techniques can extract a wealth of information about unusual events. Unfortunately, these methods yield an abundance of findings that are not of interest, obscuring relevant anomalies. In this work, we improve upon traditional anomaly detection methods by introducing Isudra, an Indirectly Supervised Detector of Relevant Anomalies from time series data. Isudra employs Bayesian optimization to select time scales, features, base detector algorithms, and algorithm hyperparameters that increase true positive and decrease false positive detection. This optimization is driven by a small amount of example anomalies, driving an indirectly supervised approach to anomaly detection. Additionally, we enhance the approach by introducing a warm-start method that reduces optimization time between similar problems. We validate the feasibility of Isudra to detect clinically relevant behavior anomalies from over 2M sensor readings collected in five smart homes, reflecting 26 health events. Results indicate that indirectly supervised anomaly detection outperforms both supervised and unsupervised algorithms at detecting instances of health-related anomalies such as falls, nocturia, depression, and weakness.  © 2021 ACM.",Anomaly detection; Bayesian optimization; smart homes,Ambient intelligence; Automation; Health; Intelligent buildings; Anomaly detection methods; Bayesian optimization; False positive detection; Hyperparameters; Sensor readings; Time-series data; Unsupervised algorithms; Wealth of information; Anomaly detection
Active Learning for Effectively Fine-Tuning Transfer Learning to Downstream Task,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102801792&doi=10.1145%2f3446343&partnerID=40&md5=8d21d9c6272c67211e42ff3cb0e916d6,"Language model (LM) has become a common method of transfer learning in Natural Language Processing (NLP) tasks when working with small labeled datasets. An LM is pretrained using an easily available large unlabelled text corpus and is fine-tuned with the labelled data to apply to the target (i.e., downstream) task. As an LM is designed to capture the linguistic aspects of semantics, it can be biased to linguistic features. We argue that exposing an LM model during fine-tuning to instances that capture diverse semantic aspects (e.g., topical, linguistic, semantic relations) present in the dataset will improve its performance on the underlying task. We propose a Mixed Aspect Sampling (MAS) framework to sample instances that capture different semantic aspects of the dataset and use the ensemble classifier to improve the classification performance. Experimental results show that MAS performs better than random sampling as well as the state-of-the-art active learning models to abuse detection tasks where it is hard to collect the labelled data for building an accurate classifier.  © 2021 ACM.",active learning; hate speech; imbalanced dataset; Misogynistic tweet; topic model; transfer learning,Classification (of information); Learning systems; Natural language processing systems; Semantics; Classification performance; Ensemble classifiers; Labeled datasets; Linguistic features; NAtural language processing; Random sampling; Semantic relations; State of the art; Transfer learning
Disentangled Item Representation for Recommender Systems,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102828627&doi=10.1145%2f3445811&partnerID=40&md5=0d586374c01afc9297f8bb51a45b4748,"Item representations in recommendation systems are expected to reveal the properties of items. Collaborative recommender methods usually represent an item as one single latent vector. Nowadays the e-commercial platforms provide various kinds of attribute information for items (e.g., category, price, and style of clothing). Utilizing this attribute information for better item representations is popular in recent years. Some studies use the given attribute information as side information, which is concatenated with the item latent vector to augment representations. However, the mixed item representations fail to fully exploit the rich attribute information or provide explanation in recommender systems. To this end, we propose a fine-grained Disentangled Item Representation (DIR) for recommender systems in this article, where the items are represented as several separated attribute vectors instead of a single latent vector. In this way, the items are represented at the attribute level, which can provide fine-grained information of items in recommendation. We introduce a learning strategy, LearnDIR, which can allocate the corresponding attribute vectors to items. We show how DIR can be applied to two typical models, Matrix Factorization (MF) and Recurrent Neural Network (RNN). Experimental results on two real-world datasets show that the models developed under the framework of DIR are effective and efficient. Even using fewer parameters, the proposed model can outperform the state-of-the-art methods, especially in the cold-start situation. In addition, we make visualizations to show that our proposition can provide explanation for users in real-world applications.  © 2021 ACM.",attribute disentangling; recommender systems; Representation learning,Factorization; Recommender systems; Vectors; Attribute information; Attribute vectors; Collaborative recommender; Learning strategy; Matrix factorizations; Real-world datasets; Recurrent neural network (RNN); State-of-the-art methods; Recurrent neural networks
Flatter Is Better: Percentile Transformations for Recommender Systems,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102815160&doi=10.1145%2f3437910&partnerID=40&md5=223963cc8e711c17fffe9dc1980a2259,"It is well known that explicit user ratings in recommender systems are biased toward high ratings and that users differ significantly in their usage of the rating scale. Implementers usually compensate for these issues through rating normalization or the inclusion of a user bias term in factorization models. However, these methods adjust only for the central tendency of users' distributions. In this work, we demonstrate that a lack of flatness in rating distributions is negatively correlated with recommendation performance. We propose a rating transformation model that compensates for skew in the rating distribution as well as its central tendency by converting ratings into percentile values as a pre-processing step before recommendation generation. This transformation flattens the rating distribution, better compensates for differences in rating distributions, and improves recommendation performance. We also show that a smoothed version of this transformation can yield more intuitive results for users with very narrow rating distributions. A comprehensive set of experiments, with state-of-the-art recommendation algorithms in four real-world datasets, show improved ranking performance for these percentile transformations.  © 2021 ACM.",flatness; percentile transformation; rating distribution; Recommender systems,Central tendencies; Factorization model; Pre-processing step; Ranking performance; Real-world datasets; Recommendation algorithms; Recommendation performance; Transformation model; Recommender systems
RHUPS: Mining Recent High Utility Patterns with Sliding Window-based Arrival Time Control over Data Streams,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102813947&doi=10.1145%2f3430767&partnerID=40&md5=6d6b982af1b205dac9729b06e0d2401b,"Databases that deal with the real world have various characteristics. New data is continuously inserted over time without limiting the length of the database, and a variety of information about the items constituting the database is contained. Recently generated data has a greater influence than the previously generated data. These are called the time-sensitive non-binary stream databases, and they include databases such as web-server click data, market sales data, data from sensor networks, and network traffic measurement. Many high utility pattern mining and stream pattern mining methods have been proposed so far. However, they have a limitation that they are not suitable to analyze these databases, because they find valid patterns by analyzing a database with only some of the features described above. Therefore, knowledge-based software about how to find meaningful information efficiently by analyzing databases with these characteristics is required. In this article, we propose an intelligent information system that calculates the influence of the insertion time of each batch in a large-scale stream database by applying the sliding window model and mines recent high utility patterns without generating candidate patterns. In addition, a novel list-based data structure is suggested for a fast and efficient management of the time-sensitive stream databases. Moreover, our technique is compared with state-of-the-art algorithms through various experiments using real datasets and synthetic datasets. The experimental results show that our approach outperforms the previously proposed methods in terms of runtime, memory usage, and scalability.  © 2021 ACM.",evolutionary time-fading factor; Recent high utility pattern; sliding window; stream database,Data mining; Data streams; Database systems; Knowledge based systems; Large scale systems; Sensor networks; Efficient managements; Intelligent information systems; Knowledge-based softwares; Network traffic measurement; Sliding window models; Sliding window-based; State-of-the-art algorithms; Synthetic datasets; Information management
Industrial federated topic modeling,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101209140&doi=10.1145%2f3418283&partnerID=40&md5=f37e46fc311da7249406349c80590129,"Probabilistic topic modeling has been applied in a variety of industrial applications. Training a high-quality model usually requires a massive amount of data to provide comprehensive co-occurrence information for the model to learn. However, industrial data such as medical or financial records are often proprietary or sensitive, which precludes uploading to data centers. Hence, training topic models in industrial scenarios using conventional approaches faces a dilemma: A party (i.e., a company or institute) has to either tolerate data scarcity or sacrifice data privacy. In this article, we propose a framework named Industrial Federated Topic Modeling (iFTM), in which multiple parties collaboratively train a high-quality topic model by simultaneously alleviating data scarcity and maintaining immunity to privacy adversaries. iFTM is inspired by federated learning, supports two representative topic models (i.e., Latent Dirichlet Allocation and SentenceLDA) in industrial applications, and consists of novel techniques such as private Metropolis-Hastings, topic-wise normalization, and heterogeneous model integration. We conduct quantitative evaluations to verify the effectiveness of iFTM and deploy iFTM in two real-life applications to demonstrate its utility. Experimental results verify iFTM's superiority over conventional topic modeling. © 2021 ACM.",differential privacy; federated learning; Topic models,Statistics; Co-occurrence informations; Conventional approach; Heterogeneous modeling; Industrial scenarios; Latent Dirichlet allocation; Probabilistic topic models; Quantitative evaluation; Real-life applications; Data privacy
Pricing-aware Real-time Charging Scheduling and Charging Station Expansion for Large-scale Electric Buses,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186442414&doi=10.1145%2f3428080&partnerID=40&md5=15617692918f6a9f7c0a466072aaf3c0,"We are witnessing a rapid growth of electrified vehicles due to the ever-increasing concerns on urban air quality and energy security. Compared to other types of electric vehicles, electric buses have not yet been prevailingly adopted worldwide due to their high owning and operating costs, long charging time, and the uneven spatial distribution of charging facilities. Moreover, the highly dynamic environment factors such as unpredictable traffic congestion, different passenger demands, and even the changing weather can significantly affect electric bus charging efficiency and potentially hinder the further promotion of large-scale electric bus fleets. To address these issues, in this article, we first analyze a real-world dataset including massive data from 16,359 electric buses, 1,400 bus lines, and 5,562 bus stops. Then, we investigate the electric bus network to understand its operating and charging patterns, and further verify the necessity and feasibility of a real-time charging scheduling. With such understanding, we design busCharging, a pricing-aware real-time charging scheduling system based on Markov Decision Process to reduce the overall charging and operating costs for city-scale electric bus fleets, taking the time-variant electricity pricing into account. To show the effectiveness of busCharging, we implement it with the real-world data from Shenzhen, which includes GPS data of electric buses, the metadata of all bus lines and bus stops, combined with data of 376 charging stations for electric buses. The evaluation results show that busCharging dramatically reduces the charging cost by 23.7% and 12.8% of electricity usage simultaneously. Finally, we design a scheduling-based charging station expansion strategy to verify our busCharging is also effective during the charging station expansion process. © 2020 Association for Computing Machinery.",charging pattern; charging scheduling; data driven; Electric bus; MDP,Charging (batteries); Cost reduction; Electric lines; Energy security; Expansion; Markov processes; Operating costs; Traffic congestion; Urban growth; Vehicle-to-grid; Wind; Bus fleets; Bus lines; Charging patterns; Charging scheduling; Charging station; Data driven; Electric bus; Large-scales; MDP; Real- time; Electric buses
Predicting Attributes of Nodes Using Network Structure,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090099388&doi=10.1145%2f3442390&partnerID=40&md5=e0b548c02da077592c0fccd45b704770,"In many graphs such as social networks, nodes have associated attributes representing their behavior. Predicting node attributes in such graphs is an important task with applications in many domains like recommendation systems, privacy preservation, and targeted advertisement. Attribute values can be predicted by treating each node as a data point described by attributes and employing classification/regression algorithms. However, in social networks, there is complex interdependence between node attributes and pairwise interaction. For instance, attributes of nodes are influenced by their neighbors (social influence), and neighborhoods (friendships) between nodes are established based on pairwise (dis)similarity between their attributes (social selection). In this article, we establish that information in network topology is extremely useful in determining node attributes. In particular, we use self- and cross-proclivity measures (quantitative measures of how much a node attribute depends on the same and other attributes of its neighbors) to predict node attributes. We propose a feature map to represent a node with respect to a specific attribute a, using all attributes of its h-hop neighbors. Different classifiers are then learned on these feature vectors to predict the value of attribute a. We perform extensive experimentation on 10 real-world datasets and show that the proposed method significantly outperforms known approaches in terms of prediction accuracy.  © 2021 ACM.",Attributes prediction; classification; data imputation; heterophily; homophily; node embedding,Classification (of information); Forecasting; Privacy by design; Network structures; Pairwise interaction; Prediction accuracy; Privacy preservation; Quantitative measures; Real-world datasets; Social influence; Targeted advertisements; Graph theory
Session-based Hotel Recommendations Dataset: As part of the ACM Recommender System Challenge 2019,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109451427&doi=10.1145%2f3412379&partnerID=40&md5=619518da600c8b19c0d45b6eef5191b0,"In 2019, the Recommender Systems Challenge [17] dealt for the first time with a real-world task from the area of e-tourism, namely the recommendation of hotels in booking sessions. In this context, we present the release of a new dataset that we believe is vitally important for recommendation systems research in the area of hotel search, from both academic and industry perspectives. In this article, we describe the qualitative characteristics of the dataset and present the comparison of several baseline algorithms trained on the data. © 2020 Association for Computing Machinery.",context-aware recommender systems; Dataset; hotel recommendation; session-based recommender systems; tourism,Real time systems; Recommender systems; Tourism; Context-aware recommender systems; Dataset; eTourism; Hotel recommendation; Qualitative characteristics; Real-world task; Session-based recommende system; Systems research; Hotels
Aspect-Aware Response Generation for Multimodal Dialogue System,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102670620&doi=10.1145%2f3430752&partnerID=40&md5=8bd647b555faf8ec3fdf9731a2bccd0f,"Multimodality in dialogue systems has opened up new frontiers for the creation of robust conversational agents. Any multimodal system aims at bridging the gap between language and vision by leveraging diverse and often complementary information from image, audio, and video, as well as text. For every task-oriented dialog system, different aspects of the product or service are crucial for satisfying the user's demands. Based upon the aspect, the user decides upon selecting the product or service. The ability to generate responses with the specified aspects in a goal-oriented dialogue setup facilitates user satisfaction by fulfilling the user's goals. Therefore, in our current work, we propose the task of aspect controlled response generation in a multimodal task-oriented dialog system. We employ a multimodal hierarchical memory network for generating responses that utilize information from both text and images. As there was no readily available data for building such multimodal systems, we create a Multi-Domain Multi-Modal Dialog (MDMMD++) dataset. The dataset comprises the conversations having both text and images belonging to the four different domains, such as hotels, restaurants, electronics, and furniture. Quantitative and qualitative analysis on the newly created MDMMD++ dataset shows that the proposed methodology outperforms the baseline models for the proposed task of aspect controlled response generation.  © 2021 ACM.",memory network; Multimodal dialogue system; response generation,Controlled response; Conversational agents; Different domains; Hierarchical memory; Multimodal dialogue systems; Quantitative and qualitative analysis; Response generation; User satisfaction; Speech processing
CSL+: Scalable Collective Subjective Logic under Multidimensional Uncertainty,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172896824&doi=10.1145%2f3426193&partnerID=40&md5=4ee13b716f15ac9115b28c2a74fc6d16,"Using unreliable information sources generating conflicting evidence may lead to a large uncertainty, which significantly hurts the decision making process. Recently, many approaches have been taken to integrate conflicting data from multiple sources and/or fusing conflicting opinions from different entities. To explicitly deal with uncertainty, a belief model called Subjective Logic (SL), as a variant of Dumpster-Shafer Theory, has been proposed to represent subjective opinions and to merge multiple opinions by offering a rich volume of fusing operators, which have been used to solve many opinion inference problems in trust networks. However, the operators of SL are known to be lack of scalability in inferring unknown opinions from large network data as a result of the sequential procedures of merging multiple opinions. In addition, SL does not consider deriving opinions in the presence of conflicting evidence. In this work, we propose a hybrid inference method that combines SL and Probabilistic Soft Logic (PSL), namely, Collective Subjective Plus, CSL+, which is resistible to highly conflicting evidence or a lack of evidence. PSL can reason a belief in a collective manner to deal with large-scale network data, allowing high scalability based on relationships between opinions. However, PSL does not consider an uncertainty dimension in a subjective opinion. To take benefits from both SL and PSL, we proposed a hybrid approach called CSL+ for achieving high scalability and high prediction accuracy for unknown opinions with uncertainty derived from a lack of evidence and/or conflicting evidence. Through the extensive experiments on four semi-synthetic and two real-world datasets, we showed that the CSL+ outperforms the state-of-the-art belief model (i.e., SL), probabilistic inference models (i.e., PSL, CSL), and deep learning model (i.e., GCN-VAE-opinion) in terms of prediction accuracy, computational complexity, and real running time. © 2020 Association for Computing Machinery.",conflicting evidence; opinion inference; subjective opinion; Uncertainty; vacuity,Computer circuits; Decision making; Deep learning; Probabilistic logics; Conflicting evidence; High scalabilities; Network data; Opinion inference; Probabilistics; Soft logic; Subjective Logic; Subjective opinion; Uncertainty; Vacuity; Scalability
A Comprehensive Approach to On-board Autonomy Verification and Validation,2021,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122158155&doi=10.1145%2f3472715&partnerID=40&md5=f339bca6214bb71c8abd9d187a2a2447,"Deep space missions are characterized by severely constrained communication links. To meet the needs of future missions and increase their scientific return, future space systems will require an increased level of autonomy on-board. In this work, we propose a comprehensive approach to on-board autonomy. We rely on model-based reasoning, and we consider many important (on-line and off-line) reasoning capabilities such as plan generation, validation, execution and monitoring, runtime diagnosis, and fault detection, identification, and recovery. The controlled platform is represented symbolically, and the reasoning capabilities are seen as symbolic manipulation of such formal model. We have developed a prototype of our framework, and we have integrated it within an on-board Autonomous Reasoning Engine. Finally, we have evaluated our approach on three case-studies inspired by real-world projects and characterized it in terms of reliability, availability, and performance. © 2021 Association for Computing Machinery.",Fault detection identification and recovery; Model based autonomy; Plan execution; Plan generation; Plan validation; Planning as model checking,Fault detection; Interplanetary flight; Detection/identification; Fault detection identification and recovery; Faults detection; Model based autonomy; Model-based OPC; Models checking; Plan execution; Plan generation; Plan validation; Planning as model checking; Model checking
A Theoretical Revisit to Linear Convergence for Saddle Point Problems,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129876136&doi=10.1145%2f3420035&partnerID=40&md5=789e2d781aecc7542c9ac7bf0120295c,"Recently, convex-concave bilinear Saddle Point Problems (SPP) is widely used in lasso problems, Support Vector Machines, game theory, and so on. Previous researches have proposed many methods to solve SPP, and present their convergence rate theoretically. To achieve linear convergence, analysis in those previouse studies requires strong convexity of ø(z). But, we find the linear convergence can also be achieved even for a general convex but not strongly convex ø(z). In the article, by exploiting the strong duality of SPP, we propose a new method to solve SPP, and achieve the linear convergence. We present a new general sufficient condition to achieve linear convergence, but do not require the strong convexity of ø(z). Furthermore, a more efficient method is also proposed, and its convergence rate is analyzed in theoretical. Our analysis shows that the well conditioned ø(z) is necessary to improve the efficiency of our method. Finally, we conduct extensive empirical studies to evaluate the convergence performance of our methods.  © 2020 ACM.",linear convergence; min-max optimization; Saddle point problems; strong convexity,Support vector machines; Condition; Convergence analysis; Convergence rates; Empirical studies; Linear convergence; Min-max optimization; Saddle point problems; Strong convexities; Strong duality; Support vectors machine; Game theory
"An Empirical Investigation of Different Classifiers, Encoding, and Ensemble Schemes for Next Event Prediction Using Business Process Event Logs",2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095864630&partnerID=40&md5=6e9121d68b8097bc0343230972ac1dcb,"There is a growing need for empirical benchmarks that support researchers and practitioners in selecting the best machine learning technique for given prediction tasks. In this article, we consider the next event prediction task in business process predictive monitoring, and we extend our previously published benchmark by studying the impact on the performance of different encoding windows and of using ensemble schemes. The choice of whether to use ensembles and which scheme to use often depends on the type of data and classification task. While there is a general understanding that ensembles perform well in predictive monitoring of business processes, next event prediction is a task for which no other benchmarks involving ensembles are available. The proposed benchmark helps researchers to select a high-performing individual classifier or ensemble scheme given the variability at the case level of the event log under consideration. Experimental results show that choosing an optimal number of events for feature encoding is challenging, resulting in the need to consider each event log individually when selecting an optimal value. Ensemble schemes improve the performance of low-performing classifiers in this task, such as SVM, whereas high-performing classifiers, such as tree-based classifiers, are not better off when ensemble schemes are considered.  © 2020 ACM.",business process; Classifier ensembles; empirical benchmark; homogeneous ensembles; individual classifier; next event prediction; predictive monitoring,Encoding (symbols); Forecasting; Learning systems; Signal encoding; Support vector machines; Business Process; Classification tasks; Empirical investigation; Event prediction; Individual classifiers; Machine learning techniques; Prediction tasks; Predictive monitoring; Benchmarking
Fast Distributed kNN Graph Construction Using Auto-tuned Locality-sensitive Hashing,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095864352&partnerID=40&md5=aeb9b2d125459dbada1ceec06c4d05b3,"The k-nearest-neighbors (kNN) graph is a popular and powerful data structure that is used in various areas of Data Science, but the high computational cost of obtaining it hinders its use on large datasets. Approximate solutions have been described in the literature using diverse techniques, among which Locality-sensitive Hashing (LSH) is a promising alternative that still has unsolved problems. We present Variable Resolution Locality-sensitive Hashing, an algorithm that addresses these problems to obtain an approximate kNN graph at a significantly reduced computational cost. Its usability is greatly enhanced by its capacity to automatically find adequate hyperparameter values, a common hindrance to LSH-based methods. Moreover, we provide an implementation in the distributed computing framework Apache Spark that takes advantage of the structure of the algorithm to efficiently distribute the computational load across multiple machines, enabling practitioners to apply this solution to very large datasets. Experimental results show that our method offers significant improvements over the state-of-the-art in the field and shows very good scalability as more machines are added to the computation.  © 2020 ACM.",automl; Big data; k nearest neighbors; locality-sensitive hashing; scalability,Computational efficiency; Data Science; Graph algorithms; Large dataset; Approximate solution; Computational costs; Computational loads; Distributed computing frameworks; Diverse techniques; K nearest neighbor (KNN); Locality sensitive hashing; Variable resolution; Nearest neighbor search
BOXREC: Recommending a Box of Preferred Outfits in Online Shopping,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095863795&partnerID=40&md5=285e39c10395d6ff062d65c21146142b,"Fashionable outfits are generally created by expert fashionistas, who use their creativity and in-depth understanding of fashion to make attractive outfits. Over the past few years, automation of outfit composition has gained much attention from the research community. Most of the existing outfit recommendation systems focus on pairwise item compatibility prediction (using visual and text features) to score an outfit combination having several items, followed by recommendation of top-n outfits or a capsule wardrobe having a collection of outfits based on user's fashion taste. However, none of these consider a user's preference of price range for individual clothing types or an overall shopping budget for a set of items. In this article, we propose a box recommendation framework - BOXREC - which at first collects user preferences across different item types (namely, top-wear, bottom-wear, and foot-wear) including price range of each type and a maximum shopping budget for a particular shopping session. It then generates a set of preferred outfits by retrieving all types of preferred items from the database (according to user specified preferences including price ranges), creates all possible combinations of three preferred items (belonging to distinct item types), and verifies each combination using an outfit scoring framework - BOXREC-OSF. Finally, it provides a box full of fashion items, such that different combinations of the items maximize the number of outfits suitable for an occasion while satisfying maximum shopping budget. We create an extensively annotated dataset of male fashion items across various types and categories (each having associated price) and a manually annotated positive and negative formal as well as casual outfit dataset. We consider a set of recently published pairwise compatibility prediction methods as competitors of BOXREC-OSF. Empirical results show superior performance of BOXREC-OSF over the baseline methods. We found encouraging results by performing both quantitative and qualitative analysis of the recommendations produced by BOXREC. Finally, based on user feedback corresponding to the recommendations given by BOXREC, we show that disliked or unpopular items can be a part of attractive outfits.  © 2020 ACM.",composite recommendation; E-commerce; fashion; neural networks; optimization; outfit compatibility; pagination,Recommender systems; Baseline methods; In-depth understanding; Online shopping; Prediction methods; Quantitative and qualitative analysis; Research communities; User's preferences; Visual and text features; Budget control
Multiple Elimination of Base Classifiers in Ensemble Learning Using Accuracy and Diversity Comparisons,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095864347&partnerID=40&md5=9b558afb4ab4f4f59b8550306b4e8253,"When generating ensemble classifiers, selecting the best set of classifiers from the base classifier pool is considered a combinatorial problem and an efficient classifier selection methodology must be utilized. Different researchers have used different strategies such as evolutionary algorithms, genetic algorithms, rule-based algorithms, simulated annealing, and so forth to select the best set of classifiers that can maximize overall ensemble classifier accuracy. In this article, we present a novel classifier selection approach to generate an ensemble classifier. The proposed approach selects classifiers in multiple rounds of elimination. In each round, a classifier is given a chance to be selected to become a part of the ensemble, if it can contribute to the overall ensemble accuracy or diversity; otherwise, it is put back into the pool. Each classifier is given multiple opportunities to participate in rounds of selection and they are discarded only if they have no remaining chances. The process is repeated until no classifier in the pool has any chance left to participate in the round of selection. To test the efficacy of the proposed approach, 13 benchmark datasets from the UCI repository are used and results are compared with single classifier models and existing state-of-the-art ensemble classifier approaches. Statistical significance testing is conducted to further validate the results, and an analysis is provided.  © 2020 ACM.",classification; clustering; Ensemble classifiers; multiple classifier systems,Genetic algorithms; Lakes; Simulated annealing; Benchmark datasets; Classifier models; Classifier selection; Combinatorial problem; Ensemble classifiers; Ensemble learning; Rule based algorithms; Statistical significance; Classification (of information)
DeepApp: Predicting Personalized Smartphone App Usage via Context-Aware Multi-Task Learning,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095864673&partnerID=40&md5=8a69221f9373962ecacf2bb424be3901,"Smartphone mobile application (App) usage prediction, i.e., which Apps will be used next, is beneficial for user experience improvement. Through an in-depth analysis on a real-world dataset, we find that App usage is highly spatio-temporally correlated and personalized. Given the ability to model complex spatio-temporal contexts, we aim to apply deep learning to achieve high prediction accuracy. However, the personalization yields a problem: training one network for each individual suffers from data scarcity, yet training one deep neural network for all users often fails to uncover user preference. In this article, we propose a novel App usage prediction framework, named DeepApp, to achieve context-aware prediction via multi-task learning. To tackle the challenge of data scarcity, we train one general network for multiple users to share common patterns. To better utilize the spatio-temporal contexts, we supplement a location prediction task in the multi-task learning framework to learn spatio-temporal relations. As for the personalization, we add a user identification task to capture user preference. We evaluate DeepApp on the large-scale dataset by extensive experiments. Results demonstrate that DeepApp outperforms the start-of-the-art baseline by 6.44%.  © 2020 ACM.",App usage prediction; deep learning; multi-task learning,Deep learning; Deep neural networks; E-learning; Forecasting; Large dataset; Learning systems; Smartphones; User experience; General networks; In-depth analysis; Large-scale dataset; Location prediction; Mobile applications; Prediction accuracy; Spatio-temporal relations; User identification; Multi-task learning
Human-computer Coalition Formation in Weighted Voting Games,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095863905&partnerID=40&md5=5dfdc187cd4e57b608000b92c659d154,"This article proposes a negotiation game, based on the weighted voting paradigm in cooperative game theory, where agents need to form coalitions and agree on how to share the gains. Despite the prevalence of weighted voting in the real world, there has been little work studying people's behavior in such settings. This work addresses this gap by combining game-theoretic solution concepts with machine learning models for predicting human behavior in such domains. We present a five-player online version of a weighted voting game in which people negotiate to create coalitions. We provide an equilibrium analysis of this game and collect hundreds of instances of people's play in the game. We show that a machine learning model with features based on solution concepts from cooperative game theory (in particular, an extension of the Deegan-Packel Index) provide a good prediction of people's decisions to join coalitions in the game. We designed an agent that uses the prediction model to make offers to people in this game and was able to outperform other people in an extensive empirical study. These results demonstrate the benefit of incorporating concepts from cooperative game theory in the design of agents that interact with people in group decision-making settings.  © 2020 ACM.",cooperative game theory; Negotiation and contract-based systems,Behavioral research; Computer games; Decision making; Decision theory; Electronic voting; Forecasting; Machine learning; Predictive analytics; Turing machines; Coalition formations; Cooperative game theory; Empirical studies; Equilibrium analysis; Group Decision Making; Machine learning models; Solution concepts; Weighted voting games; Game theory
From Appearance to Essence: Comparing Truth Discovery Methods without Using Ground Truth,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095866576&partnerID=40&md5=2c8f30c86b1bb47cae99e38702231af1,"Truth discovery has been widely studied in recent years as a fundamental means for resolving the conflicts in multi-source data. Although many truth discovery methods have been proposed based on different considerations and intuitions, investigations show that no single method consistently outperforms the others. To select the right truth discovery method for a specific application scenario, it becomes essential to evaluate and compare the performance of different methods. A drawback of current research efforts is that they commonly assume the availability of certain ground truth for the evaluation of methods. However, the ground truth may be very limited or even impossible to obtain, rendering the evaluation biased. In this article, we present CompTruthHyp, a generic approach for comparing the performance of truth discovery methods without using ground truth. In particular, our approach calculates the probability of observations in a dataset based on the output of different methods. The probability is then ranked to reflect the performance of these methods. We review and compare 12 representative truth discovery methods and consider both single-valued and multi-valued objects. The empirical studies on both real-world and synthetic datasets demonstrate the effectiveness of our approach for comparing truth discovery methods.  © 2020 ACM.",multi-valued objects; performance evaluation; single-valued objects; sparse ground truth; truth discovery methods; Web search,Application scenario; Empirical studies; Evaluation of methods; Generic approach; Multi-valued; Multisource data; Research efforts; Synthetic datasets
Contextual Anomaly Detection in Solder Paste Inspection with Multi-Task Learning,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095863296&partnerID=40&md5=c0a68a88bb9a4cc8db3849d7ebf10b02,"In this article, we study solder paste inspection (SPI), an important stage that is used in the semiconductor manufacturing industry, where abnormal boards should be detected. A highly accurate SPI can substantially reduce human expert involvement, as well as reduce the waste in disposing of the boards in good condition. A key difference today is that because of increasing demand in board customization, the number of board types increases substantially and quantity of the boards produced in each type decreases. Thus, the previous approaches where a fine-tuned model is developed for each board type are no longer viable. Intrinsically, our problem is an anomaly detection problem. A major specialty in today's SPI is that the target tasks for prediction cannot be fully pre-determined due to context changes during the solder paste printing stage. Our experiences show that a conventional approach to first define a set of tasks and train these tasks offline will lead to low accuracy. Here, we propose a novel multi-task approach, where the performance of all target tasks is ensured simultaneously. We note that the SPI process is streamlined and automatic, allowing the SPI time for only a few seconds. We propose a fast clustering algorithm that reuses existing models to avoid retraining and fine tune in the inference phase. We evaluate our approach using 3-month data collected from production lines. We show that we can reduce 81.28% of false alarms. This can translate to annual savings of $11.3 million.  © 2020 ACM.",Contextual anomaly detection; multi-task learning,Clustering algorithms; Inference engines; Multi-task learning; Semiconductor device manufacture; Soldering; Annual savings; Conventional approach; Highly accurate; Human expert; Production line; Semiconductor manufacturing industry; Solder paste inspection; Solder paste printing; Anomaly detection
A Joint Neural Model for User Behavior Prediction on Social Networking Platforms,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094898925&partnerID=40&md5=fee97ab3d0f8c4a02b9eaa321d0422ff,"Social networking services provide platforms for users to perform two kinds of behaviors: consumption behavior (e.g., recommending items of interest) and social link behavior (e.g., recommending potential social links). Accurately modeling and predicting users' two kinds of behaviors are two core tasks in these platforms with various applications. Recently, with the advance of neural networks, many neural-based models have been designed to predict a single users' behavior, i.e., social link behavior or consumption behavior. Compared to the classical shallow models, these neural-based models show better performance to drive a user's behavior by modeling the complex patterns. However, there are few works exploiting whether it is possible to design a neural-based model to jointly predict users' two kinds of behaviors to further enhance the prediction performance. In fact, social scientists have already shown that users' two kinds of behaviors are not isolated; people trend to the consumption recommendation of friends on social platforms and would like to make new friends with like-minded users. While some previous works jointly model users' two kinds of behaviors with shallow models, we argue that the correlation between users' two kinds of behaviors are complex, which could not be well-designed with shallow linear models. To this end, in this article, we propose a neural joint behavior prediction model named Neural Joint Behavior Prediction Model (NJBP) to mutually enhance the prediction performance of these two tasks on social networking platforms. Specifically, there are two key characteristics of our proposed model: First, to model the correlation of users' two kinds of behaviors, we design a fusion layer in the neural network to model the positive correlation of users' two kinds of behaviors. Second, as the observed links in the social network are often very sparse, we design a new link-based loss function that could preserve the social network topology. After that, we design a joint optimization function to allow the two behaviors modeling tasks to be trained to mutually enhance each other. Finally, extensive experimental results on two real-world datasets show that our proposed method is on average 7.14% better than the best baseline on social link behavior while 6.21% on consumption behavior prediction. Compared with the pair-wise loss function on two datasets, our proposed link-based loss function improves at least 4.69% on the social link behavior prediction and 4.72% on the consumption behavior prediction.  © 2020 ACM.",behavior prediction; consumption behavior; Joint neural networks; social link behavior; topology information,Complex networks; Forecasting; Multilayer neural networks; Predictive analytics; Behavior prediction; Joint optimization; Key characteristics; Positive correlations; Prediction performance; Real-world datasets; Social networking services; Social scientists; Behavioral research
Latent Unexpected Recommendations,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092701032&partnerID=40&md5=684ae139bd42aa1c43edd18a08d0136a,"Unexpected recommender system constitutes an important tool to tackle the problem of filter bubbles and user boredom, which aims at providing unexpected and satisfying recommendations to target users at the same time. Previous unexpected recommendation methods only focus on the straightforward relations between current recommendations and user expectations by modeling unexpectedness in the feature space, thus resulting in the loss of accuracy measures to improve unexpectedness performance. In contrast to these prior models, we propose to model unexpectedness in the latent space of user and item embeddings, which allows us to capture hidden and complex relations between new recommendations and historic purchases. In addition, we develop a novel Latent Closure (LC) method to construct a hybrid utility function and provide unexpected recommendations based on the proposed model. Extensive experiments on three real-world datasets illustrate superiority of our proposed approach over the state-of-the-art unexpected recommendation models, which leads to significant increase in unexpectedness measure without sacrificing any accuracy metric under all experimental settings in this article.  © 2020 ACM.",beyond-accuracy objectives; latent closure; latent embeddings; latent space; Unexpected recommendation,Feature space; Loss of accuracy; Real-world datasets; Recommendation methods; State of the art; User expectations; Utility functions; Recommender systems
SafeRoute: Learning to Navigate Streets Safely in an Urban Environment,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095866675&partnerID=40&md5=42d3c4516f2f6173cece0f9a6ac89d80,"Recent studies show that 85% of women have changed their traveled routes to avoid harassment and assault. Despite this, current mapping tools do not empower users with information to take charge of their personal safety. We propose SafeRoute, a novel solution to the problem of navigating cities and avoiding street harassment and crime. Unlike other street navigation applications, SafeRoute introduces a new type of path generation via deep reinforcement learning. This enables us to successfully optimize for multi-criteria path-finding and incorporate representation learning within our framework. Our agent learns to pick favorable streets to create a safe and short path with a reward function that incorporates safety and efficiency. Given access to recent crime reports in many urban cities, we train our model for experiments in Boston, New York, and San Francisco. We test our model on areas of these cities, specifically the populated downtown regions with high foot traffic. We evaluate SafeRoute and successfully improve over state-of-the-art methods by up to 17% in local average distance from crimes while decreasing path length by up to 7%.  © 2020 ACM.",deep reinforcement learning; multi-preference routing; Safe routing,Crime; Deep learning; Current mapping; Multi-criteria; Path generation; Personal safety; Reward function; Safety and efficiencies; State-of-the-art methods; Urban environments; Reinforcement learning
A Survey of Unsupervised Deep Domain Adaptation,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090420697&doi=10.1145%2f3400066&partnerID=40&md5=384199df201b956f4bb556dbe52074c2,"Deep learning has produced state-of-The-Art results for a variety of tasks. While such approaches for supervised learning have performed well, they assume that training and testing data are drawn from the same distribution, which may not always be the case. As a complement to this challenge, single-source unsupervised domain adaptation can handle situations where a network is trained on labeled data from a source domain and unlabeled data from a related but different target domain with the goal of performing well at test-Time on the target domain. Many single-source and typically homogeneous unsupervised deep domain adaptation approaches have thus been developed, combining the powerful, hierarchical representations from deep learning with domain adaptation to reduce reliance on potentially costly target data labels. This survey will compare these approaches by examining alternative methods, the unique and common elements, results, and theoretical insights. We follow this with a look at application areas and open research directions.  © 2020 ACM.",deep learning; Domain adaptation; generative adversarial networks,Surveys; Well testing; Application area; Domain adaptation; Hierarchical representation; Single source; State of the art; Target domain; Training and testing; Unlabeled data; Deep learning
Moment-Guided Discriminative Manifold Correlation Learning on Ordinal Data,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090997317&doi=10.1145%2f3402445&partnerID=40&md5=5d8e41482a88ebaaf24181bcbbe071b4,"Canonical correlation analysis (CCA) is a typical and useful learning paradigm in big data analysis for capturing correlation across multiple views of the same objects. When dealing with data with additional ordinal information, traditional CCA suffers from poor performance due to ignoring the ordinal relationships within the data. Such data is becoming increasingly common, as either temporal or sequential information is often associated with the data collection process. To incorporate the ordinal information into the objective function of CCA, the so-called ordinal discriminative CCA has been presented in the literature. Although ordinal discriminative CCA can yield better ordinal regression results, its performance deteriorates when data is corrupted with noise and outliers, as it tends to smear the order information contained in class centers. To address this issue, in this article we construct a robust manifold-preserved ordinal discriminative correlation regression (rmODCR). The robustness is achieved by replacing the traditional (l2-norm) class centers with lp-norm centers, where p is efficiently estimated according to the moments of the data distributions, as well as by incorporating the manifold distribution information of the data in the objective optimization. In addition, we further extend the robust manifold-preserved ordinal discriminative correlation regression to deep convolutional architectures. Extensive experimental evaluations have demonstrated the superiority of the proposed methods.  © 2020 ACM.",Canonical correlation analysis; lp-norm centers; manifold learning; moment; ordinal regression,Canonical correlation analysis; Data collection process; Experimental evaluation; Learning paradigms; Objective functions; Objective optimization; Ordinal information; Sequential information
Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091049103&doi=10.1145%2f3402446&partnerID=40&md5=2e045afef571f932d2dc444ee30fcf97,"Face anti-spoofing aims to detect presentation attack to face recognition-based authentication systems. It has drawn growing attention due to the high security demand. The widely adopted CNN-based methods usually well recognize the spoofing faces when training and testing spoofing samples display similar patterns, but their performance would drop drastically on testing spoofing faces of novel patterns or unseen scenes, leading to poor generalization performance. Furthermore, almost all current methods treat face anti-spoofing as a prior step to face recognition, which prolongs the response time and makes face authentication inefficient. In this article, we try to boost the generalizability and applicability of face anti-spoofing methods by designing a new generalizable face authentication CNN (GFA-CNN) model with three novelties. First, GFA-CNN introduces a simple yet effective total pairwise confusion loss for CNN training that properly balances contributions of all spoofing patterns for recognizing the spoofing faces. Second, it incorporate a fast domain adaptation component to alleviate negative effects brought by domain variation. Third, it deploys filter diversification learning to make the learned representations more adaptable to new scenes. In addition, the proposed GFA-CNN works in a multi-Task manner-it performs face anti-spoofing and face recognition simultaneously. Experimental results on five popular face anti-spoofing and face recognition benchmarks show that GFA-CNN outperforms previous face anti-spoofing methods on cross-Test protocols significantly and also well preserves the identity information of input face images.  © 2020 ACM.",computer vision; Deep learning; domain adaptation; face anti-spoofing; face recognition,Authentication; Well testing; Authentication systems; Domain adaptation; Face authentication; Face recognition benchmarks; Generalization performance; Identity information; Similar pattern; Training and testing; Face recognition
STARS: Defending against Sockpuppet-Based Targeted Attacks on Reviewing Systems,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091012105&doi=10.1145%2f3397463&partnerID=40&md5=e3cf0d3029b744aa45cf12db8663bc92,"Customers of virtually all online marketplaces rely upon reviews in order to select the product or service they wish to buy. These marketplaces in turn deploy review fraud detection systems so that the integrity of reviews is preserved. A well-known problem with review fraud detection systems is their underlying assumption that the majority of reviews are honest-This assumption leads to a vulnerability where an attacker can try to generate many fake reviews of a product. In this article, we consider the case where a company wishes to fraudulently promote its product through fake reviews and propose the Sockpuppet-based Targeted Attack on Reviewing Systems (STARS for short). STARS enables an attacker to enter fake reviews for a product from multiple, apparently independent, sockpuppet accounts. We show that the STARS attack enables companies to successfully promote their product against seven recent, well-known review fraud detectors on four datasets (Amazon, Epinions, and the BitcoinAlpha and OTC exchanges) by significant margins. To protect against the STARS attack, we propose a new fraud detection algorithm called RTV. RTV introduces a new class of users (called trusted users) and also considers reviews left by verified users which were not considered in existing review fraud detectors. We show that RTV significantly mitigates the impact of the STARS attack across the four datasets listed above.  © 2020 ACM.",Data mining and knowledge discovery; online commerce and recommendation systems; social and information networks,Crime; Electronic commerce; Fraud detection; Fraud detection system; On-line marketplaces; Stars
Multi-Task Learning for Entity Recommendation and Document Ranking in Web Search,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090997285&doi=10.1145%2f3396501&partnerID=40&md5=f6ed903cb3a566fd930b13a494d2ba3e,"Entity recommendation, providing users with an improved search experience by proactively recommending related entities to a given query, has become an indispensable feature of today's Web search engine. Existing studies typically only consider the query issued at the current timestep while ignoring the in-session user search behavior (short-Term search history) or historical user search behavior across all sessions (long-Term search history) when generating entity recommendations. As a consequence, they may fail to recommend entities of interest relevant to a user's actual information need. In this work, we believe that both short-Term and long-Term search history convey valuable evidence that could help understand the user's search intent behind a query, and take both of them into consideration for entity recommendation. Furthermore, there has been little work on exploring whether the use of other companion tasks in Web search such as document ranking as auxiliary tasks could improve the performance of entity recommendation. To this end, we propose a multi-Task learning framework with deep neural networks (DNNs) to jointly learn and optimize two companion tasks in Web search engines: entity recommendation and document ranking, which can be easily trained in an end-To-end manner. Specifically, we regard document ranking as an auxiliary task to improve the main task of entity recommendation, where the representations of queries, sessions, and users are shared across all tasks and optimized by the multi-Task objective during training. We evaluate our approach using large-scale, real-world search logs of a widely-used commercial Web search engine. We also performed extensive ablation experiments over a number of facets of the proposed multi-Task DNN model to figure out their relative importance. The experimental results show that both short-Term and long-Term search history can bring significant improvements in recommendation effectiveness, and the combination of both outperforms using either of them individually. In addition, the experiments show that the performance of both entity recommendation and document ranking can be significantly improved, which demonstrates the effectiveness of using multi-Task learning to jointly optimize the two companion tasks in Web search.  © 2020 ACM.",context-Aware; document ranking; Entity recommendation; multi-Task learning; neural networks; personalized; Web search,Behavioral research; Deep learning; Deep neural networks; Information retrieval; Learning systems; Search engines; Websites; Ablation experiments; Document ranking; Real world search; Related entities; Search behavior; Search history; Search intents; Web searches; Multi-task learning
Practical Privacy Preserving POI Recommendation,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091028870&doi=10.1145%2f3394138&partnerID=40&md5=5972bdab9788d4d72f28313641487c47,"Point-of-Interest (POI) recommendation has been extensively studied and successfully applied in industry recently. However, most existing approaches build centralized models on the basis of collecting users' data. Both private data and models are held by the recommender, which causes serious privacy concerns. In this article, we propose a novel Privacy preserving POI Recommendation (PriRec) framework. First, to protect data privacy, users' private data (features and actions) are kept on their own side, e.g., Cellphone or Pad. Meanwhile, the public data that need to be accessed by all the users are kept by the recommender to reduce the storage costs of users' devices. Those public data include: (1) static data only related to the status of POI, such as POI categories, and (2) dynamic data dependent on user-POI actions such as visited counts. The dynamic data could be sensitive, and we develop local differential privacy techniques to release such data to the public with privacy guarantees. Second, PriRec follows the representations of Factorization Machine (FM) that consists of a linear model and the feature interaction model. To protect the model privacy, the linear models are saved on the users' side, and we propose a secure decentralized gradient descent protocol for users to learn it collaboratively. The feature interaction model is kept by the recommender since there is no privacy risk, and we adopt a secure aggregation strategy in a federated learning paradigm to learn it. To this end, PriRec keeps users' private raw data and models in users' own hands, and protects user privacy to a large extent. We apply PriRec in real-world datasets, and comprehensive experiments demonstrate that, compared with FM, PriRec achieves comparable or even better recommendation accuracy.  © 2020 ACM.",decentralization; local differential privacy; POI recommendation; Privacy preserving; secret sharing,Digital storage; Gradient methods; Centralized models; Differential privacies; Factorization machines; Feature interaction model; Learning paradigms; Real-world datasets; Recommendation accuracy; Secure aggregations; Data privacy
Adaptive HTF-MPR: An Adaptive Heterogeneous TensorFlow Mapper Utilizing Bayesian Optimization and Genetic Algorithms,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091043785&doi=10.1145%2f3396949&partnerID=40&md5=526543fc3abe687af51155bd79d3e8ab,"Deep neural networks are widely used in many artificial intelligence applications. They have demonstrated state-of-The-Art accuracy on many artificial intelligence tasks. For this high accuracy to occur, deep neural networks require the right parameter values. This is achieved by a process known as training. The training of large amounts of data via many iterations comes at a high cost in regard to computation time and energy. Optimal resource allocation would therefore reduce the training time. TensorFlow, a computational graph library developed by Google, alleviates the development of neural network models and provides the means to train these networks. In this article, we propose Adaptive HTF-MPR to carry out the resource allocation, or mapping, on TensorFlow. Adaptive HTF-MPR searches for the best mapping in a hybrid approach. We applied the proposed methodology on two well-known image classifiers: VGG-16 and AlexNet. We also performed a full analysis of the solution space of MNIST Softmax. Our results demonstrate that Adaptive HTF-MPR outperforms the default homogeneous TensorFlow mapping. In addition to the speed up, Adaptive HTF-MPR can react to changes in the state of the system and adjust to an improved mapping.  © 2020 ACM.",adaptivity; Bayesian optimization; computational graphs; genetic algorithms; gradient boosting regressor; neural networks; Task mapping,Deep neural networks; Genetic algorithms; Mapping; Resource allocation; Bayesian optimization; Computation time; Computational graph; Image Classifiers; Large amounts of data; Neural network model; Optimal resource allocation; State of the art; Neural networks
Dancing with Trump in the Stock Market,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091013405&doi=10.1145%2f3403578&partnerID=40&md5=6ebd2e492317412dc6e758b1af83f53b,"It is always deemed crucial to identify the key factors that could have significant impact on the stock market trend. Recently, an interesting phenomenon has emerged that some of President Trump's posts in Twitter can surge into a dominant role on the stock market for a certain time period, although studies along this line are still in their infancy. Therefore, in this article, we study whether and how this new-rising information can help boost the performance of stock market prediction. Specifically, we have found that the echoing reinforced effect of financial news with Trump's market-related tweets can influence the market movement-That is, some of Trump's tweets directly impact the stock market in a short time, and the impact can be further intensified when it echoes with other financial news reports. Along this line, we propose a deep information echoing model to predict the hourly stock market trend, such as the rise and fall of the Dow Jones Industrial Average. In particular, to model the discovered echoing reinforced impact, we design a novel information echoing module with a gating mechanism in a sequential deep learning framework to capture the fused knowledge from both Trump's tweets and financial news. Extensive experiments have been conducted on the real-world U.S. stock market data to validate the effectiveness of our model and its interpretability in understanding the usability of Trump's posts. Our proposed deep echoing model outperforms other baselines by achieving the best accuracy of 60.42% and obtains remarkable accumulated profits in a trading simulation, which confirms our assumption that Trump's tweets contain indicative information for short-Term market trends. Furthermore, we find that Trump's tweets about trade and political events are more likely to be associated with short-Term market movement, and it seems interesting that the impact would not degrade as time passes.  © 2020 ACM.",deep learning; information echoing; Stock market prediction; Trump; Twitter,Commerce; Deep learning; Reinforcement; Dow Jones Industrial averages; Financial news; Gating mechanisms; Interpretability; Learning frameworks; Novel information; Political events; Stock market prediction; Financial markets
Shapelet-Transformed Multi-channel EEG Channel Selection,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091047878&doi=10.1145%2f3397850&partnerID=40&md5=e25e50bbca7119501186b871a04e06b4,"This article proposes an approach to select EEG channels based on EEG shapelet transformation, aiming to reduce the setup time and inconvenience for subjects and to improve the applicable performance of Brain-Computer Interfaces (BCIs). In detail, the method selects top-k EEG channels by solving a logistic loss-embedded minimization problem with respect to EEG shapelet learning, hyperplane learning, and EEG channel weight learning simultaneously. Especially, to learn distinguished EEG shapelets for weighting contributions of each EEG channel to the logistic loss, EEG shapelet similarity is also minimized during the procedure. Furthermore, the gradient descent strategy is adopted in the article to solve the non-convex optimization problem, which finally leads to the algorithm termed StEEGCS. In a result, classification accuracy, with those EEG channels selected by StEEGCS, is improved compared to that with all EEG channels, and classification time consumption is reduced as well. Additionally, the comparisons with several state-of-The-Art EEG channel selection methods on several real-world EEG datasets also demonstrate the efficacy and superiority of StEEGCS.  © 2020 ACM.",channel contribution; EEG channel selection; EEG shapelets; shapelet similarity minimization,Convex optimization; Gradient methods; Brain computer interfaces (BCIs); Classification accuracy; Classification time; EEG channel selections; Gradient descent; Minimization problems; Nonconvex optimization; State of the art; Brain computer interface
Querying Recurrent Convoys over Trajectory Data,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090999934&doi=10.1145%2f3400730&partnerID=40&md5=4e9b459a852b1a2be14fb042d501fd27,"Moving objects equipped with location-positioning devices continuously generate a large amount of spatio-Temporal trajectory data. An interesting finding over a trajectory stream is a group of objects that are travelling together for a certain period of time. We observe that existing studies on mining co-moving objects do not consider an important correlation between co-moving objects, which is the reoccurrence of the co-moving pattern. In this study, we propose the problem of finding recurrent co-moving patterns from streaming trajectories, enabling us to discover recent co-moving patterns that are repeated within a given time period. Experimental results on real-life trajectory data verify the efficiency and effectiveness of our method.  © 2020 ACM.",co-moving pattern; Recurrent convoy query; spatio-Temporal index,Large amounts; Moving objects; Positioning devices; Spatio-temporal trajectories; Time-periods; Trajectory data; Trajectory streams; Trajectories
Mapping Points of Interest through Street View Imagery and Paid Crowdsourcing,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091035517&doi=10.1145%2f3403931&partnerID=40&md5=0013d43255ac748be0efd87e9b1caab4,"We present the Virtual City Explorer (VCE), an online crowdsourcing platform for the collection of rich geotagged information in urban environments. Compared to other volunteered geographic information approaches, which are constrained by the number and availability of mapping enthusiasts on the ground, the VCE uses digital street imagery to allow people to virtually explore a city from anywhere in the world, using a browser or a mobile phone. In addition, contributions in VCE are designed as paid microtasks-small jobs that can be carried out without any specific knowledge of the local area or previous mapping expertise in exchange for a fee. We tested the VCE in two cities to map points of interest (PoIs) in transport and mobility, using FigureEight to recruit participants. We were able to show that our platform enables crowdworkers to submit PoI location seamlessly, cover almost all of the tested areas, and discover several PoIs not reported by other approaches. This allows the VCE to complement existing approaches that leverage experts or grassroot communities.  © 2020 ACM.",Crowdsourcing; geographic information; geospatial information; mapping; microtasks; urban auditing,Knowledge management; Mapping; Crowdsourcing platforms; Mapping point; Points of interest; Specific knowledge; Transport and mobilities; Urban environments; Virtual cities; Volunteered geographic information; Crowdsourcing
A Discriminative Convolutional Neural Network with Context-Aware Attention,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091022316&doi=10.1145%2f3397464&partnerID=40&md5=9863c9d5538b98fa3a5eaadc2ff2c3f4,"Feature representation and feature extraction are two crucial procedures in text mining. Convolutional Neural Networks (CNN) have shown overwhelming success for text-mining tasks, since they are capable of efficiently extracting n-gram features from source data. However, vanilla CNN has its own weaknesses on feature representation and feature extraction. A certain amount of filters in CNN are inevitably duplicate and thus hinder to discriminatively represent a given text. In addition, most existing CNN models extract features in a fixed way (i.e., max pooling) that either limit the CNN to local optimum nor without considering the relation between all features, thereby unable to learn a contextual n-gram features adaptively. In this article, we propose a discriminative CNN with context-Aware attention to solve the challenges of vanilla CNN. Specifically, our model mainly encourages discrimination across different filters via maximizing their earth mover distances and estimates the salience of feature candidates by considering the relation between context features. We validate carefully our findings against baselines on five benchmark datasets of classification and two datasets of summarization. The results of the experiments verify the competitive performance of our proposed model.  © 2020 ACM.",attention method; convolution neural networks; Text mining,Classification (of information); Convolution; Extraction; Feature extraction; Filtration; Natural language processing systems; Text mining; Benchmark datasets; Competitive performance; Context features; Context-Aware; Earth mover distances; Feature representation; Local optima; Source data; Convolutional neural networks
Cut-n-Reveal: Time Series Segmentations with Explanations,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091011796&doi=10.1145%2f3394118&partnerID=40&md5=7e92e9693181b5477dffd99552dbca4d,"Recent hurricane events have caused unprecedented amounts of damage on critical infrastructure systems and have severely threatened our public safety and economic health. The most observable (and severe) impact of these hurricanes is the loss of electric power in many regions, which causes breakdowns in essential public services. Understanding power outages and how they evolve during a hurricane provides insights on how to reduce outages in the future, and how to improve the robustness of the underlying critical infrastructure systems. In this article, we propose a novel scalable segmentation with explanations framework to help experts understand such datasets. Our method, CnR (Cut-n-Reveal), first finds a segmentation of the outage sequences based on the temporal variations of the power outage failure process so as to capture major pattern changes. This temporal segmentation procedure is capable of accounting for both the spatial and temporal correlations of the underlying power outage process. We then propose a novel explanation optimization formulation to find an intuitive explanation of the segmentation such that the explanation highlights the culprit time series of the change in each segment. Through extensive experiments, we show that our method consistently outperforms competitors in multiple real datasets with ground truth. We further study real county-level power outage data from several recent hurricanes (Matthew, Harvey, Irma) and show that CnR recovers important, non-Trivial, and actionable patterns for domain experts, whereas baselines typically do not give meaningful results.  © 2020 ACM.",Multivariate time series; spatio-Temporal segmentation,Critical infrastructures; Electric losses; Hurricanes; Public works; Time series; Critical infrastructure systems; Hurricane events; Optimization formulations; Public services; Spatial and temporal correlation; Temporal segmentations; Temporal variation; Time-series segmentation; Outages
Knowledge-aware Attentive Wasserstein Adversarial Dialogue Response Generation,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089266391&doi=10.1145%2f3384675&partnerID=40&md5=4deb919e90540e3b392ad1b4c0e3bd50,"Natural language generation has become a fundamental task in dialogue systems. RNN-based natural response generation methods encode the dialogue context and decode it into a response. However, they tend to generate dull and simple responses. In this article, we propose a novel framework, called KAWA-DRG (Knowledge-aware Attentive Wasserstein Adversarial Dialogue Response Generation) to model conversation-specific external knowledge and the importance variances of dialogue context in a unified adversarial encoder-decoder learning framework. In KAWA-DRG, a co-attention mechanism attends to important parts within and among context utterances with word-utterance-level attention. Prior knowledge is integrated into the conditional Wasserstein auto-encoder for learning the latent variable space. The posterior and prior distribution of latent variables are generated and trained through adversarial learning. We evaluate our model on Switchboard, DailyDialog, In-Car Assistant, and Ubuntu Dialogue Corpus. Experimental results show that KAWA-DRG outperforms the existing methods.  © 2020 ACM.",adversarial learning; co-attention; Dialogue system; external knowledge,Decoding; Natural language processing systems; Signal encoding; Speech processing; Adversarial learning; Attention mechanisms; Dialogue systems; External knowledge; Learning frameworks; Natural language generation; Prior distribution; Response generation; Learning systems
CNN-based Multiple Manipulation Detector Using Frequency Domain Features of Image Residuals,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089278358&doi=10.1145%2f3388634&partnerID=40&md5=dcd505a5e432ec852c21ec549310226b,"Increasingly sophisticated image editing tools make it easy to modify images. Often these modifications are elaborate, convincing, and undetectable by even careful human inspection. These considerations have prompted the development of forensic algorithms and approaches to detect modifications done to an image. However, these detectors are model-driven (i.e., manipulation-specific) and the choice of a potent detector requires knowledge of the type of manipulation, something that cannot be known (a priori). Thus, the latest effort is directed towards developing model-free (i.e., generalized) detectors capable of detecting multiple manipulation types. In this article, we propose a novel detector capable of exposing seven different manipulation types in low-resolution compressed images. Our proposed approach is based on a two-layer convolutional neural network (CNN) to extract frequency domain features of image median filtered residual that are classified using two different classifiers - softmax and extremely randomized trees. Extensive experiments demonstrate the efficacy of proposed detector over existing state-of-the-art detectors.  © 2020 ACM.",Convolutional neural network (CNN); image forensics; Multiple manipulation detection; two-layer architecture,Convolutional neural networks; Median filters; Multilayer neural networks; Network layers; Compressed images; Frequency domains; Image editing tools; Low resolution; Model free; Model-driven; Randomized trees; State of the art; Frequency domain analysis
BISTRO: Berkeley Integrated System for Transportation Optimization,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089274941&doi=10.1145%2f3384344&partnerID=40&md5=94cc1d2e9e63dc02b8ad35298392cdbc,"The current trend toward urbanization and adoption of flexible and innovative mobility technologies will have complex and difficult-to-predict effects on urban transportation systems. Comprehensive methodological frameworks that account for the increasingly uncertain future state of the urban mobility landscape do not yet exist. Furthermore, few approaches have enabled the massive ingestion of urban data in planning tools capable of offering the flexibility of scenario-based design. This article introduces Berkeley Integrated System for Transportation Optimization (BISTRO), a new open source transportation planning decision support system that uses an agent-based simulation and optimization approach to anticipate and develop adaptive plans for possible technological disruptions and growth scenarios. The new framework was evaluated in the context of a machine learning competition hosted within Uber Technologies, Inc., in which over 400 engineers and data scientists participated. For the purposes of this competition, a benchmark model, based on the city of Sioux Falls, South Dakota, was adapted to the BISTRO framework. An important finding of this study was that in spite of rigorous analysis and testing done prior to the competition, the two top-scoring teams discovered an unbounded region of the search space, rendering the solutions largely uninterpretable for the purposes of decision-support. On the other hand, a follow-on study aimed to fix the objective function. It served to demonstrate BISTRO's utility as a human-in-the-loop cyberphysical system: one that uses scenario-based optimization algorithms as a feedback mechanism to assist urban planners with iteratively refining objective function and constraints specification on intervention strategies. The portfolio of transportation intervention strategy alternatives eventually chosen achieves high-level regional planning goals developed through participatory stakeholder engagement practices.  © 2020 ACM.",Agent-based models; big data; computing with heterogeneous data; digital decision support systems; human mobility; intelligent transportation systems; smart cities; system dynamics; urban informatics,Decision support systems; Economic and social effects; Embedded systems; Integrated control; Iterative methods; Open systems; Planning; Regional planning; Agent based simulation; Constraints specification; Cyber physical systems (CPSs); Methodological frameworks; Optimization algorithms; Transportation optimizations; Transportation planning; Urban transportation systems; Urban transportation
A Traffic Density Estimation Model Based on Crowdsourcing Privacy Protection,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089276609&doi=10.1145%2f3391707&partnerID=40&md5=41fb222df957638f0a99de2b9a16fdbb,"Acquiring traffic condition information is of great significance in transportation guidance, urban planning, and route recommendation. To date, traffic density data are generally acquired by road sound analysis, video data analysis, or in-vehicle network communication, which are usually financially or temporally expensive. Another way to get traffic conditions is to collect track data by crowdsourcing. However, this way lead to a greater risk of leaking users' privacy. To avoid the risk, this article proposes a traffic density estimation model based on crowdsourcing privacy protection. First, in the acquisition process of the track data by crowdsourcing, dual servers are employed for transmission, and homomorphic encryption is carried out to encrypt the data to protect the data from being leaked during transmission. Second, sampling is implemented for randomization and anonymization to reduce the spatial continuity and temporal continuity of position data. In this way, the intermediate server cannot acquire users' original data, and the main server cannot obtain users' personal information. Finally, before data transmission, Laplace noising is performed on the users' local position data to further protect the original location information. The proposed algorithm in this study realizes that only users have their original track data, and the servers involved in the work cannot infer the original track data, which ensures the real security of user privacy. The proposed algorithm was verified with the track data from the Didi Gaia Data Opening Plan. The experimental results showed that the proposed algorithm could still maintain the validity of data analysis results and the security of user data privacy after homomorphic encryption, noise addition, and sample collection, and displayed good robustness and scalability.  © 2020 ACM.",crowdsourcing; Differential privacy; encryption; sample; traffic flow,Crowdsourcing; Cryptography; Information analysis; Magnetic disk storage; Risk perception; Transportation routes; Urban transportation; Acquisition process; Ho-momorphic encryptions; In-vehicle networks; Location information; Personal information; Temporal continuity; Traffic conditions; Video data analysis; Data privacy
DeepKey: A Multimodal Biometric Authentication System via Deep Decoding Gaits and Brainwaves,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089274257&doi=10.1145%2f3393619&partnerID=40&md5=17f2699222d386e98d619d20636555f5,"Biometric authentication involves various technologies to identify individuals by exploiting their unique, measurable physiological and behavioral characteristics. However, traditional biometric authentication systems (e.g., face recognition, iris, retina, voice, and fingerprint) are at increasing risks of being tricked by biometric tools such as anti-surveillance masks, contact lenses, vocoder, or fingerprint films. In this article, we design a multimodal biometric authentication system named DeepKey, which uses both Electroencephalography (EEG) and gait signals to better protect against such risk. DeepKey consists of two key components: an Invalid ID Filter Model to block unauthorized subjects, and an identification model based on attention-based Recurrent Neural Network (RNN) to identify a subject's EEG IDs and gait IDs in parallel. The subject can only be granted access while all the components produce consistent affirmations to match the user's proclaimed identity. We implement DeepKey with a live deployment in our university and conduct extensive empirical experiments to study its technical feasibility in practice. DeepKey achieves the False Acceptance Rate (FAR) and the False Rejection Rate (FRR) of 0 and 1.0%, respectively. The preliminary results demonstrate that DeepKey is feasible, shows consistent superior performance compared to a set of methods, and has the potential to be applied to the authentication deployment in real-world settings.  © 2020 ACM.",biometric authentication; deep learning; EEG (Electroencephalography); gait; multimodal,Authentication; Biometrics; Electroencephalography; Electrophysiology; Face recognition; Behavioral characteristics; Biometric authentication; Biometric authentication system; Empirical experiments; False acceptance rate; Identification model; Multimodal biometric authentications; Recurrent neural network (RNN); Recurrent neural networks
Domain-attention Conditional Wasserstein Distance for Multi-source Domain Adaptation,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089280894&doi=10.1145%2f3391229&partnerID=40&md5=ddfe1820ff3078627e86772b55bfc507,"Multi-source domain adaptation has received considerable attention due to its effectiveness of leveraging the knowledge from multiple related sources with different distributions to enhance the learning performance. One of the fundamental challenges in multi-source domain adaptation is how to determine the amount of knowledge transferred from each source domain to the target domain. To address this issue, we propose a new algorithm, called Domain-attention Conditional Wasserstein Distance (DCWD), to learn transferred weights for evaluating the relatedness across the source and target domains. In DCWD, we design a new conditional Wasserstein distance objective function by taking the label information into consideration to measure the distance between a given source domain and the target domain. We also develop an attention scheme to compute the transferred weights of different source domains based on their conditional Wasserstein distances to the target domain. After that, the transferred weights can be used to reweight the source data to determine their importance in knowledge transfer. We conduct comprehensive experiments on several real-world data sets, and the results demonstrate the effectiveness and efficiency of the proposed method.  © 2020 ACM.",attention; Domain adaptation; multiple sources; optimal transport,Different distributions; Domain adaptation; Effectiveness and efficiencies; Knowledge transfer; Label information; Learning performance; Objective functions; Wasserstein distance; Knowledge management
Geosocial Co-Clustering,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089267055&doi=10.1145%2f3391708&partnerID=40&md5=40bbf5ec60aeea8e16fa748e9a6d8927,"As location-based services using mobile devices have become globally popular these days, social network analysis (especially, community detection) increasingly benefits from combining social relationships with geographic preferences. In this regard, this article addresses the emerging problem of geosocial community detection. We first formalize the problem of geosocial co-clustering, which co-clusters the users in social networks and the locations they visited. Geosocial co-clustering detects higher-quality communities than existing approaches by improving the mapping clusterability, whereby users in the same community tend to visit locations in the same region. While geosocial co-clustering is soundly formalized as non-negative matrix tri-factorization, conventional matrix tri-factorization algorithms suffer from a significant computational overhead when handling large-scale datasets. Thus, we also develop an efficient framework for geosocial co-clustering, called GEOsocial COarsening and DEcomposition (GEOCODE). To achieve efficient matrix tri-factorization, GEOCODE reduces the numbers of users and locations through coarsening and then decomposes the single whole matrix tri-factorization into a set of multiple smaller sub-matrix tri-factorizations. Thorough experiments conducted using real-world geosocial networks show that GEOCODE reduces the elapsed time by 19-69 times while achieving the accuracy of up to 94.8% compared with the state-of-the-art co-clustering algorithm. Furthermore, the benefit of the mapping clusterability is clearly demonstrated through a local expert recommendation application.  © 2020 ACM.",co-clustering; Geosocial networks; mapping clusterability; non-negative matrix factorization; social similarity; spatial similarity,Coarsening; Factorization; Large dataset; Location; Location based services; Mapping; Matrix algebra; Ostwald ripening; Population dynamics; Social networking (online); Telecommunication services; Community detection; Computational overheads; Factorization algorithms; Geo-social networks; Large-scale datasets; Non-negative matrix; Social relationships; State of the art; Clustering algorithms
Mining High-utility Temporal Paterns on Time Interval based Data,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089265670&doi=10.1145%2f3391230&partnerID=40&md5=75f55fb154cee1b5ccbd44d730839cd0,"In this article, we propose a novel temporal pattern mining problem, named high-utility temporal pattern mining, to fulfill the needs of various applications. Different from classical temporal pattern mining aimed at discovering frequent temporal patterns, high-utility temporal pattern mining is to find each temporal pattern whose utility is greater than or equal to the minimum-utility threshold. To facilitate efficient high-utility temporal pattern mining, several extension and pruning strategies are proposed to reduce the search space. Algorithm HUTPMiner is then proposed to efficiently mine high-utility temporal patterns with the aid of the proposed extension and pruning strategies. Experimental results show that HUTPMiner is able to prune a large number of candidates, thereby achieving high mining efficiency.  © 2020 ACM.",data mining; high utility; High-utility temporal pattern; interval-based data; temporal pattern,Pruning strategy; Search spaces; Temporal pattern; Temporal pattern minings; Time interval; Data mining
Understanding the Long-Term Evolution of Electric Taxi Networks,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089280891&doi=10.1145%2f3393671&partnerID=40&md5=ca235dbe08a732e03a638310e5aa2a90,"Due to the ever-growing concerns over air pollution and energy security, more and more cities have started to replace their conventional taxi fleets with electric ones. Even though environmentally friendly, the rapid promotion of electric taxis raises problems to both taxi drivers and governments, e.g., prolonged waiting/charging time, unbalanced utilization of charging infrastructures, and inadequate taxi supply due to the long charging time. In this article, we conduct the first longitudinal measurement study to understand the long-term evolution of mobility and charging patterns by utilizing 5-year data from one of the largest electric taxi networks in the world, i.e., the Shenzhen electric taxi network in China. In particular, (1) we first perform an electric taxi contextualization about their operation and charging activities; (2) then we design a generic charging event extraction algorithm based on GPS data and charging station data, and (3) based on the contextualization and extracted charging activities, we perform a comprehensive measurement study called ePat to explore the evolution of the electric taxi network from the mobility and charging perspectives. Our ePat is based on 4.8 TB taxi GPS data, 240 GB taxi transaction data, and metadata from 117 charging stations, during an evolution process from 427 electric taxis in 2013 to 13,178 in 2018. Moreover, ePat also explores the impacts of various contexts and benefits during the evolution process. Our ePat as a comprehensive measurement of the electric taxi network mobility and charging evolution has the potential to advance the understanding of the evolution patterns of electric taxi networks and pave the way for analyzing future shared autonomous vehicles.  © 2020 ACM.",charging pattern; Electric taxi; evolution experience; mobility pattern; shared autonomous vehicle,Charging time; Energy security; Global positioning system; Long Term Evolution (LTE); Taxicabs; Charging infrastructures; Charging patterns; Comprehensive measurement; Contextualization; Event extraction; Evolution patterns; Evolution process; Measurement study; Electric network parameters
An Attention-based Rumor Detection Model with Tree-structured Recursive Neural Networks,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089276667&doi=10.1145%2f3391250&partnerID=40&md5=91e2ec63518e03a7566f09da1c7da846,"Rumor spread in social media severely jeopardizes the credibility of online content. Thus, automatic debunking of rumors is of great importance to keep social media a healthy environment. While facing a dubious claim, people often dispute its truthfulness sporadically in their posts containing various cues, which can form useful evidence with long-distance dependencies. In this work, we propose to learn discriminative features from microblog posts by following their non-sequential propagation structure and generate more powerful representations for identifying rumors. For modeling non-sequential structure, we first represent the diffusion of microblog posts with propagation trees, which provide valuable clues on how a claim in the original post is transmitted and developed over time. We then present a bottom-up and a top-down tree-structured models based on Recursive Neural Networks (RvNN) for rumor representation learning and classification, which naturally conform to the message propagation process in microblogs. To enhance the rumor representation learning, we reveal that effective rumor detection is highly related to finding evidential posts, e.g., the posts expressing specific attitude towards the veracity of a claim, as an extension of the previous RvNN-based detection models that treat every post equally. For this reason, we design discriminative attention mechanisms for the RvNN-based models to selectively attend on the subset of evidential posts during the bottom-up/top-down recursive composition. Experimental results on four datasets collected from real-world microblog platforms confirm that (1) our RvNN-based models achieve much better rumor detection and classification performance than state-of-the-art approaches; (2) the attention mechanisms for focusing on evidential posts can further improve the performance of our RvNN-based method; and (3) our approach possesses superior capacity on detecting rumors at a very early stage.  © 2020 ACM.",neural attention; propagation tree; recursive neural networks; Rumor detection and classification; social media,Backpropagation; Classification (of information); Forestry; Learning systems; Social networking (online); Attention mechanisms; Classification performance; Discriminative features; Healthy environments; Long-distance dependencies; Recursive neural networks; Sequential structure; State-of-the-art approach; Neural networks
CoFi-points: Collaborative Filtering via Pointwise Preference Learning on User/Item-Set,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089279412&doi=10.1145%2f3389127&partnerID=40&md5=cdfc86a6e1a64e47d2d68dd8a7b4ebe4,"With the explosive growth of web resources, an increasingly important task in recommender systems is to provide high-quality personalized services by learning users' preferences from historically observed information. As an effective preference learning technology, collaborative filtering has been widely extended to model the one-class or implicit feedback data, which is known as one-class collaborative filtering (OCCF). For a long time, pairwise ranking-oriented learning scheme has been viewed as a superior solution than the pointwise scheme for OCCF due to its higher accuracy in most cases. However, we argue that with appropriate model design, pointwise preference learning can achieve comparable or even better performance than the counterpart, i.e., pairwise preference learning. In particular, we propose a new preference assumption, i.e., pointwise preference on user/item-set. Based on this new assumption, we develop a novel, simple, and flexible solution called collaborative filtering via pointwise preference learning on user/item-set (CoFi-points). Furthermore, we derive two specific algorithms of CoFi-points with respect to the involved user-set and item-set, i.e., CoFi-points(u) and CoFi-points(i), referring to preference assumptions defined on user-set and item-set, respectively. Finally, we conduct extensive empirical studies on four real-world datasets with the state-of-the-art methods, and find that our solution can achieve very promising performance with respect to several ranking-oriented evaluation metrics.  © 2020 ACM.",implicit feedback; item-set; One-class collaborative filtering; pointwise preference learning; user-set,Learning systems; Learning to rank; Appropriate models; Empirical studies; Evaluation metrics; Implicit feedback; Personalized service; Preference learning; Real-world datasets; State-of-the-art methods; Collaborative filtering
End-to-End Text-to-Image Synthesis with Spatial Constrains,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089285122&doi=10.1145%2f3391709&partnerID=40&md5=e88f2c6155d1797739cdd7a42dd865c8,"Although the performance of automatically generating high-resolution realistic images from text descriptions has been significantly boosted, many challenging issues in image synthesis have not been fully investigated, due to shapes variations, viewpoint changes, pose changes, and the relations of multiple objects. In this article, we propose a novel end-to-end approach for text-to-image synthesis with spatial constraints by mining object spatial location and shape information. Instead of learning a hierarchical mapping from text to image, our algorithm directly generates multi-object fine-grained images through the guidance of the generated semantic layouts. By fusing text semantic and spatial information into a synthesis module and jointly fine-tuning them with multi-scale semantic layouts generated, the proposed networks show impressive performance in text-to-image synthesis for complex scenes. We evaluate our method both on single-object CUB dataset and multi-object MS-COCO dataset. Comprehensive experimental results demonstrate that our method significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.  © 2020 ACM.",Computer vision; CUB; MS-COCO; spatial constrain; text-to-image synthesis,Semantics; Text mining; Evaluation metrics; Hierarchical mapping; Multiple objects; Shape information; Spatial constraints; Spatial informations; Spatial location; State-of-the-art approach; Image processing
Video Object Segmentation and Tracking,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089267678&doi=10.1145%2f3391743&partnerID=40&md5=50a0833d00839e4a62ff0a5ffe19724d,"Object segmentation and object tracking are fundamental research areas in the computer vision community. These two topics are difficult to handle some common challenges, such as occlusion, deformation, motion blur, scale variation, and more. The former contains heterogeneous object, interacting object, edge ambiguity, and shape complexity; the latter suffers from difficulties in handling fast motion, out-of-view, and real-time processing. Combining the two problems of Video Object Segmentation and Tracking (VOST) can overcome their respective difficulties and improve their performance. VOST can be widely applied to many practical applications such as video summarization, high definition video compression, human computer interaction, and autonomous vehicles. This survey aims to provide a comprehensive review of the state-of-the-art VOST methods, classify these methods into different categories, and identify new trends. First, we broadly categorize VOST methods into Video Object Segmentation (VOS) and Segmentation-based Object Tracking (SOT). Each category is further classified into various types based on the segmentation and tracking mechanism. Moreover, we present some representative VOS and SOT methods of each time node. Second, we provide a detailed discussion and overview of the technical characteristics of the different methods. Third, we summarize the characteristics of the related video dataset and provide a variety of evaluation metrics. Finally, we point out a set of interesting future works and draw our own conclusions.  © 2020 ACM.",interactive methods; object tracking; semi-supervised methods; unsupervised methods; Video object segmentation; weakly supervised methods,Human computer interaction; Image compression; Image segmentation; Motion compensation; Fundamental research; Heterogeneous object; High-definition video compression; Object segmentation; Realtime processing; Segmentation and tracking; Video summarization; Video-object segmentation; Object tracking
Superpixel Region Merging Based on Deep Network for Medical Image Segmentation,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086499176&doi=10.1145%2f3386090&partnerID=40&md5=f987053c735a3b791f229345e2613bfc,"Automatic and accurate semantic segmentation of pathological structures in medical images is challenging because of noisy disturbance, deformable shapes of pathology, and low contrast between soft tissues. Classical superpixel-based classification algorithms suffer from edge leakage due to complexity and heterogeneity inherent in medical images. Therefore, we propose a deep U-Net with superpixel region merging processing incorporated for edge enhancement to facilitate and optimize segmentation. Our approach combines three innovations: (1) different from deep learning - based image segmentation, the segmentation evolved from superpixel region merging via U-Net training getting rich semantic information, in addition to gray similarity; (2) a bilateral filtering module was adopted at the beginning of the network to eliminate external noise and enhance soft tissue contrast at edges of pathogy; and (3) a normalization layer was inserted after the convolutional layer at each feature scale, to prevent overfitting and increase the sensitivity to model parameters. This model was validated on lung CT, brain MR, and coronary CT datasets, respectively. Different superpixel methods and cross validation show the effectiveness of this architecture. The hyperparameter settings were empirically explored to achieve a good trade-off between the performance and efficiency, where a four-layer network achieves the best result in precision, recall, F-measure, and running speed. It was demonstrated that our method outperformed state-of-the-art networks, including FCN-16s, SegNet, PSPNet, DeepLabv3, and traditional U-Net, both quantitatively and qualitatively. Source code for the complete method is available at https://github.com/Leahnawho/Superpixel-network.  © 2020 ACM.",bilateral filtering; deep U-Net; Medical image segmentation; normalization layer; superpixel-based classification algorithm,Computerized tomography; Deep learning; Economic and social effects; Edge detection; Image enhancement; Image segmentation; Information filtering; Merging; Network layers; Semantics; Superpixels; Tissue; Bilateral filtering; Classification algorithm; Cross validation; Deformable shapes; Edge enhancements; Pathological structures; Semantic information; Semantic segmentation; Medical image processing
Learning Three-dimensional Skeleton Data from Sign Language Video,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085496838&doi=10.1145%2f3377552&partnerID=40&md5=585d8c014e10a8e1a5f112af33a1d319,"Data for sign language research is often difficult and costly to acquire. We therefore present a novel pipeline able to generate motion three-dimensional (3D) skeleton data from single-camera sign language videos only. First, three recurrent neural networks are learned to infer the three-dimensional position data of body, face, and finger joints for a high resolution of the signer's skeleton. Subsequently, the angular displacements of all joints over time are estimated using inverse kinematics and mapped to a virtual sign avatar for animation. Last, the generated data are evaluated in detail, including a sign language recognition and sign language synthesis scenario. Utilizing a neural word classifier trained on real motion capture data, we reliably classify word segments built from our newly generated position data with similar accuracy as motion capture data (absolute difference 3.8%). Furthermore, qualitative evaluation of sign animations shows that the avatar performs natural movements that are comprehensible and resemble animations created with original motion capture data. © 2020 ACM.",3D pose estimation; data augmentation; recurrent neural networks; sign language recognition; sign language synthesis,Inverse kinematics; Motion capture; Musculoskeletal system; Recurrent neural networks; Absolute difference; Angular displacement; Dimensional position; Motion capture data; Qualitative evaluations; Sign Language recognition; Sign language synthesis; Threedimensional (3-d); Classification (of information)
Two Can Play That Game: An Adversarial Evaluation of a Cyber-Alert Inspection System,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085494483&doi=10.1145%2f3377554&partnerID=40&md5=b19208ab822bc858b38ed25a09fa5cfc,"Cyber-security is an important societal concern. Cyber-attacks have increased in numbers as well as in the extent of damage caused in every attack. Large organizations operate a Cyber Security Operation Center (CSOC), which forms the first line of cyber-defense. The inspection of cyber-alerts is a critical part of CSOC operations (defender or blue team). Recent work proposed a reinforcement learning (RL) based approach for the defender's decision-making to prevent the cyber-alert queue length from growing large and overwhelming the defender. In this article, we perform a red team (adversarial) evaluation of this approach. With the recent attacks on learning-based decision-making systems, it is even more important to test the limits of the defender's RL approach. Toward that end, we learn several adversarial alert generation policies and the best response against them for various defender's inspection policy. Surprisingly, we find the defender's policies to be quite robust to the best response of the attacker. In order to explain this observation, we extend the earlier defender's RL model to a game model with adversarial RL, and show that there exist defender policies that can be robust against any adversarial policy. We also derive a competitive baseline from the game theory model and compare it to the defender's RL approach. However, when we go further to exploit the assumptions made in the Markov Decision Process (MDP) in the defender's RL model, we discover an attacker policy that overwhelms the defender. We use a double oracle like approach to retrain the defender with episodes from this discovered attacker policy. This made the defender robust to the discovered attacker policy and no further harmful attacker policies were discovered. Overall, the adversarial RL and double oracle approach in RL are general techniques that are applicable to other RL usage in adversarial environments. © 2020 ACM.",adversarial reinforcement learning; Cyber-security operations center; game theory,Behavioral research; Decision making; Game theory; Inspection; Markov processes; Network security; Adversarial environments; Decision-making systems; Game theory models; Inspection policies; Inspection system; Large organizations; Markov Decision Processes; Societal concerns; Reinforcement learning
Modeling with Node Popularities for Autonomous Overlapping Community Detection,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085508933&doi=10.1145%2f3373760&partnerID=40&md5=58641fb8f3d85d6c1df6be5f47c1aff3,"Overlapping community detection has triggered recent research in network analysis. One of the promising techniques for finding overlapping communities is the popular stochastic models, which, unfortunately, have some common drawbacks. They do not support an important observation that highly connected nodes are more likely to reside in the overlapping regions of communities in the network. These methods are in essence not truly unsupervised, since they require a threshold on probabilistic memberships to derive overlapping structures and need the number of communities to be specified a priori. We develop a new method to address these issues for overlapping community detection. We first present a stochastic model to accommodate the relative importance and the expected degree of every node in each community. We then infer every overlapping community by ranking the nodes according to their importance. Second, we determine the number of communities under the Bayesian framework. We evaluate our method and compare it with five state-of-the-art methods. The results demonstrate the superior performance of our method. We also apply this new method to two applications, showing its superb performance on practical problems. © 2020 ACM.",Community detection; model selection; node popularities; stochastic model,Population dynamics; Stochastic systems; Bayesian frameworks; In-network analysis; Overlapping communities; Overlapping community detections; Overlapping regions; Practical problems; Probabilistic memberships; Recent researches; Stochastic models
"Copula-Based Anomaly Scoring and Localization for Large-Scale, High-Dimensional Continuous Data",2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085513578&doi=10.1145%2f3372274&partnerID=40&md5=24f714c13a6cf07d25a7d9cf73b49258,"The anomaly detection method presented by this article has a special feature: it not only indicates whether or not an observation is anomalous but also tells what exactly makes an anomalous observation unusual. Hence, it provides support to localize the reason of the anomaly. The proposed approach is model based; it relies on the multivariate probability distribution associated with the observations. Since the rare events are present in the tails of the probability distributions, we use copula functions, which are able to model the fat-tailed distributions well. The presented procedure scales well; it can cope with a large number of high-dimensional samples. Furthermore, our procedure can cope with missing values as well, which occur frequently in high-dimensional datasets. In the second part of the article, we demonstrate the usability of the method through a case study, where we analyze a large dataset consisting of the performance counters of a real mobile telecommunication network. Since such networks are complex systems, the signs of sub-optimal operation can remain hidden for a potentially long time. With the proposed procedure, many such hidden issues can be isolated and indicated to the network operator. © 2020 ACM.",Anomaly scoring; copula fitting; unsupervised learning,Anomaly detection; Large dataset; Anomaly detection methods; Copula functions; Fat-tailed distributions; High dimensional datasets; Mobile telecommunication networks; Multivariate probability distributions; Optimal operation; Performance counters; Probability distributions
Deep Neighborhood Component Analysis for Visual Similarity Modeling,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085508826&doi=10.1145%2f3375787&partnerID=40&md5=47ade65f56e6f9d3d1d1e2c31eb1866e,"Learning effective visual similarity is an essential problem in multimedia research. Despite the promising progress made in recent years, most existing approaches learn visual features and similarities in two separate stages, which inevitably limits their performance. Once useful information has been lost in the feature extraction stage, it can hardly be recovered later. This article proposes a novel end-to-end approach for visual similarity modeling, called deep neighborhood component analysis, which discriminatively trains deep neural networks to jointly learn visual features and similarities. Specifically, we first formulate a metric learning objective that maximizes the intra-class correlations and minimizes the inter-class correlations under the neighborhood component analysis criterion, and then train deep convolutional neural networks to learn a nonlinear mapping that projects visual instances from original feature space to a discriminative and neighborhood-structure-preserving embedding space, thus resulting in better performance. We conducted extensive evaluations on several widely used and challenging datasets, and the impressive results demonstrate the effectiveness of our proposed approach. © 2020 ACM.",Metric learning; neighborhood component analysis; visual similarity modeling,Convolutional neural networks; Deep learning; Essential problems; Metric learning; Multimedia research; Neighborhood component analysis; Neighborhood structure; Nonlinear mappings; Visual feature; Visual similarity; Deep neural networks
WiSign: Ubiquitous American Sign Language Recognition Using CommercialWi-Fi Devices,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085490324&doi=10.1145%2f3377553&partnerID=40&md5=e95cdcfead8b37c7669244e4f4e42739,"In this article, we propose WiSign that recognizes the continuous sentences of American Sign Language (ASL) with existing WiFi infrastructure. Instead of identifying the individual ASL words from the manually segmented ASL sentence in existing works, WiSign can automatically segment the original channel state information (CSI) based on the power spectral density (PSD) segmentation method. WiSign constructs a five-layer Deep Belief Network (DBN) to automatically extract the features of isolated fragments, and then uses the Hidden Markov Model (HMM) with Gaussian mixture and Forward-Backward algorithm to recognize sign words. In order to further improve the accuracy, WiSign also integrates the language model N-gram, which uses the grammar rules of ASL to calibrate the recognized results of sign words. We implement a prototype of WiSign with commercial WiFi devices and evaluate its performance in real indoor environments. The results show that WiSign achieves satisfactory accuracy when recognizing ASL sentences that involve the movements of the head, arms, hands, and fingers. © 2020 ACM.",American Sign Language; channel state information; deep belief network; sentence-level recognition,Hidden Markov models; Network layers; Spectral density; Trellis codes; Wireless local area networks (WLAN); American sign language; Deep belief network (DBN); Forward / backward algorithms; Gaussian mixtures; Indoor environment; Original channels; Power spectral densities (PSD); Segmentation methods; Channel state information
Unified Generative Adversarial Networks for Multiple-Choice Oriented Machine Comprehension,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085514434&doi=10.1145%2f3372120&partnerID=40&md5=b58dfdbf3f59991687b9eeaf3b7c0595,"In this article, we address the multiple-choice machine comprehension (MC) problem in natural language processing. Existing approaches for MC are usually designed for general cases; however, we specially develop a novel method for solving the multiple-choice MC problem. We take the inspiration generative adversarial networks (GANs) and first propose an adversarial framework for multiple-choice oriented MC, named McGAN. Specifically, our approach is designed as a GAN-based method that unifies both generative and discriminative MC models. Working together, the generative model focuses on predicting relevant answer given a passage (text) and a question; the discriminative model focuses on predicting their relevancy given an answer-passage-question set. Based on the competition via adversarial training in a minimize-maximize game, the proposed method takes advantages from both models. To evaluate the performance, we test our McGAN model on three well-known datasets for multiple-choice MC. Our results show that McGAN can achieve a significant increase in accuracy compared to existing models based on all three datasets, and it consistently outperforms all tested baselines, including state-of-the-art techniques. © 2020 ACM.",GAN; Generative adversarial networks; machine comprehension; MC; RACE; recurrent neural networks; RNN,Adversarial networks; Discriminative models; GaN based; Generative model; Multiple choice; NAtural language processing; State-of-the-art techniques; Natural language processing systems
A Causal Dirichlet Mixture Model for Causal Inference from Observational Data,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085484949&doi=10.1145%2f3379500&partnerID=40&md5=cb77ec7bf3ddf0f00af5d7d25839a167,"Estimating causal effects by making causal inferences from observational data is common practice in scientific studies, business decision-making, and daily life. In today's data-driven world, causal inference has become a key part of the evaluation process for many purposes, such as examining the effects of medicine or the impact of an economic policy on society. However, although the literature contains some excellent models, there is room to improve their representation power and their ability to capture complex relationships. For these reasons, we propose a novel prior called Causal DP and a model called CDP. The prior captures the complex relationships between covariates, treatments, and outcomes in observational data using a rational probabilistic dependency structure. The model is Bayesian, nonparametric, and generative and is not based on the assumption of any parametric distribution. CDP is designed to estimate various kinds of causal effects - average, conditional average, average treated, quantile, and so on. It performs well with missing covariates and does not suffer from overfitting. Comparative experiments on synthetic datasets against several state-of-the-art methods demonstrate that CDP has a superior ability to capture complex relationships. Further, a simple evaluation to infer the effect of a job training program on trainee earnings from real-world data shows that CDP is both effective and useful for causal inference. © 2020 ACM.",Bayesian nonparametric; Causal inference; Dirichlet process,Economics; Comparative experiments; Complex relationships; Conditional average; Dependency structures; Dirichlet mixture model; Parametric distributions; Representation power; State-of-the-art methods; Decision making
Adversarial Attacks on Deep-learning Models in Natural Language Processing,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085502874&doi=10.1145%2f3374217&partnerID=40&md5=d69fc55e2da50c36808aee6374d9959e,"With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs are vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), research efforts on attacking DNNs for Natural Language Processing (NLP) applications have emerged in recent years. However, the intrinsic difference between image (CV) and text (NLP) renders challenges to directly apply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a wide range of NLP applications. In this article, we present a systematic survey on these works. We collect all related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze 40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary knowledge of NLP and discuss related seminal works in computer vision. We conclude our survey with a discussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks on NLP DNNs. © 2020 ACM.",adversarial examples; Deep neural networks; natural language processing; textual data,Computer vision; Deep neural networks; Natural language processing systems; Surveys; Academic work; Computational devices; Intrinsic differences; Learning models; NAtural language processing; Research efforts; Deep learning
Analyzing and Detecting Collusive Users Involved in Blackmarket Retweeting Activities,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085518704&doi=10.1145%2f3380537&partnerID=40&md5=9c4f507683e4cc9bce76325a84aafcd4,"With the rise in popularity of social media platforms like Twitter, having higher influence on these platforms has a greater value attached to it, since it has the power to influence many decisions in the form of brand promotions and shaping opinions. However, blackmarket services that allow users to inorganically gain influence are a threat to the credibility of these social networking platforms. Twitter users can gain inorganic appraisals in the form of likes, retweets, and follows through these blackmarket services either by paying for them or by joining syndicates wherein they gain such appraisals by providing similar appraisals to other users. These customers tend to exhibit a mix of organic and inorganic retweeting behavior, making it tougher to detect them. In this article, we investigate these blackmarket customers engaged in collusive retweeting activities. We collect and annotate a novel dataset containing various types of information about blackmarket customers and use these sources of information to construct multiple user representations. We adopt Weighted Generalized Canonical Correlation Analysis (WGCCA) to combine these individual representations to derive user embeddings that allow us to effectively classify users as: genuine users, bots, promotional customers, and normal customers. Our method significantly outperforms state-of-the-art approaches (32.95% better macro F1-score than the best baseline). © 2020 ACM.",blackmarket; collusion; multiview learning; OSNs; Retweeters; Twitter,Information use; Social networking (online); Brand promotion; F1 scores; Generalized canonical correlation analysis; Multiple user; Social media platforms; Sources of informations; State-of-the-art approach; Sales
HERA: Partial Label Learning by Combining Heterogeneous Loss with Sparse and Low-Rank Regularization,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085505596&doi=10.1145%2f3379501&partnerID=40&md5=1ef503258269eea7471e5ad84213fcb1,"Partial label learning (PLL) aims to learn from the data where each training instance is associated with a set of candidate labels, among which only one is correct. Most existing methods deal with this type of problem by either treating each candidate label equally or identifying the ground-truth label iteratively. In this article, we propose a novel PLL approach named HERA, which simultaneously incorporates the HeterogEneous Loss and the SpaRse and Low-rAnk procedure to estimate the labeling confidence for each instance while training the desired model. Specifically, the heterogeneous loss integrates the strengths of both the pairwise ranking loss and the pointwise reconstruction loss to provide informative label ranking and reconstruction information for label identification, whereas the embedded sparse and low-rank scheme constrains the sparsity of ground-truth label matrix and the low rank of noise label matrix to explore the global label relevance among the whole training data, for improving the learning model. Comprehensive ablation study demonstrates the effectiveness of our employed heterogeneous loss, and extensive experiments on both artificial and real-world datasets demonstrate that our method achieves superior or comparable performance against state-of-the-art methods. © 2020 ACM.",heterogeneous loss; matrix decomposing; Partial label learning; sparse and low-rank regularization,Matrix algebra; Phase locked loops; Ground truth; Label matrixes; Label rankings; Learning models; Real-world datasets; Sparse and low ranks; State-of-the-art methods; Training data; Iterative methods
Understand Dynamic Regret with Switching Cost for Online Decision Making,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085476718&doi=10.1145%2f3375788&partnerID=40&md5=b10e087c245d3c40f764b78ea595e4ba,"As a metric to measure the performance of an online method, dynamic regret with switching cost has drawn much attention for online decision making problems. Although the sublinear regret has been provided in much previous research, we still have little knowledge about the relation between the dynamic regret and the switching cost. In the article, we investigate the relation for two classic online settings: Online Algorithms (OA) and Online Convex Optimization (OCO). We provide a new theoretical analysis framework that shows an interesting observation; that is, the relation between the switching cost and the dynamic regret is different for settings of OA and OCO. Specifically, the switching cost has significant impact on the dynamic regret in the setting of OA. But it does not have an impact on the dynamic regret in the setting of OCO. Furthermore, we provide a lower bound of regret for the setting of OCO, which is same with the lower bound in the case of no switching cost. It shows that the switching cost does not change the difficulty of online decision making problems in the setting of OCO. © 2020 ACM.",dynamic regret; online algorithms; online convex optimization; Online decision making; online mirror descent; switching cost,Convex optimization; Decision making; Switching; Analysis frameworks; Lower bounds; On-line algorithms; On-line decision makings; On-line setting; Online convex optimizations; Online methods; Switching costs; Cost benefit analysis
Mediated Secure Multi-Party Protocols for Collaborative Filtering,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081117116&doi=10.1145%2f3375402&partnerID=40&md5=48fdc4bb990752fd0db9b51e2930c52c,"Recommender systems have become extremely common in recent years and are utilized in a variety of domains such as movies, music, news, products, restaurants, and so on. While a typical recommender system bases its recommendations solely on users' preference data collected by the system itself, the quality of recommendations can significantly be improved if several recommender systems (or vendors) share their data. However, such data sharing poses significant privacy and security challenges, both to the vendors and the users. In this article, we propose secure protocols for distributed item-based Collaborative Filtering. Our protocols allow to compute both the predicted ratings of items and their predicted rankings without compromising privacy nor predictions' accuracy. Unlike previous solutions in which the secure protocols are executed solely by the vendors, our protocols assume the existence of a mediator that performs intermediate computations on encrypted data supplied by the vendors. Such a mediated setting is advantageous over the non-mediated one since it enables each vendor to communicate solely with the mediator. This yields reduced communication costs, and it allows each vendor to issue recommendations to its clients without being dependent on the availability and willingness of the other vendors to collaborate. © 2020 Royal Society of Chemistry. All rights reserved.",distributed computing; Item-based collaborative filtering; privacy,Data privacy; Data Sharing; Distributed computer systems; Recommender systems; Communication cost; Distributed items; Item-based collaborative filtering; Multi-party protocols; Preference data; Privacy and security; Quality of recommendations; Secure protocols; Collaborative filtering
Transfer learning with dynamic distribution adaptation,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079769403&doi=10.1145%2f3360309&partnerID=40&md5=de124486307c258ff96cb9a6e7a13585,"Transfer learning aims to learn robust classifiers for the target domain by leveraging knowledge from a source domain. Since the source and the target domains are usually from different distributions, existing methods mainly focus on adapting the cross-domain marginal or conditional distributions. However, in real applications, the marginal and conditional distributions usually have different contributions to the domain discrepancy. Existing methods fail to quantitatively evaluate the different importance of these two distributions, which will result in unsatisfactory transfer performance. In this article, we propose a novel concept called Dynamic Distribution Adaptation (DDA), which is capable of quantitatively evaluating the relative importance of each distribution. DDA can be easily incorporated into the framework of structural risk minimization to solve transfer learning problems. On the basis of DDA, we propose two novel learning algorithms: (1) Manifold Dynamic Distribution Adaptation (MDDA) for traditional transfer learning, and (2) Dynamic Distribution Adaptation Network (DDAN) for deep transfer learning. Extensive experiments demonstrate that MDDA and DDAN significantly improve the transfer learning performance and set up a strong baseline over the latest deep and adversarial methods on digits recognition, sentiment analysis, and image classification. More importantly, it is shown that marginal and conditional distributions have different contributions to the domain divergence, and our DDA is able to provide good quantitative evaluation of their relative importance, which leads to better performance. We believe this observation can be helpful for future research in transfer learning. © 2020 Association for Computing Machinery.",Deep learning; Distribution alignment; Domain adaptation; Kernel method; Subspace learning; Transfer learning,Deep learning; Image enhancement; Knowledge management; Learning algorithms; Learning systems; Sentiment analysis; Conditional distribution; Different distributions; Domain adaptation; Dynamic distribution; Kernel methods; Quantitative evaluation; Structural risk minimization; Subspace learning; Transfer learning
TremBR: Exploring road networks for trajectory representation learning,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079775544&doi=10.1145%2f3361741&partnerID=40&md5=b149752f95c7beb51cec157761506439,"In this article, we propose a novel representation learning framework, namely TRajectory EMBedding via Road networks (Trembr), to learn trajectory embeddings (low-dimensional feature vectors) for use in a variety of trajectory applications. The novelty of Trembr lies in (1) the design of a recurrent neural network-(RNN) based encoder-decoder model, namely Traj2Vec, that encodes spatial and temporal properties inherent in trajectories into trajectory embeddings by exploiting the underlying road networks to constrain the learning process in accordance with the matched road segments obtained using road network matching techniques (e.g., Barefoot [24, 27]), and (2) the design of a neural network-based model, namely Road2Vec, to learn road segment embeddings in road networks that captures various relationships amongst road segments in preparation for trajectory representation learning. In addition to model design, several unique technical issues raising in Trembr, including data preparation in Road2Vec, the road segment relevance-aware loss, and the network topology constraint in Traj2Vec, are examined. To validate our ideas, we learn trajectory embeddings using multiple large-scale real-world trajectory datasets and use them in three tasks, including trajectory similarity measure, travel time prediction, and destination prediction. Empirical results show that Trembr soundly outperforms the state-of-the-art trajectory representation learning models, trajectory2vec and t2vec, by at least one order of magnitude in terms of mean rank in trajectory similarity measure, 23.3% to 41.7% in terms of mean absolute error (MAE) in travel time prediction, and 39.6% to 52.4% in terms of MAE in destination prediction. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Neural networks; Representation learning; Road network; Trajectory,Embeddings; Forecasting; Large dataset; Learning systems; Motor transportation; Neural networks; Recurrent neural networks; Roads and streets; Time varying control systems; Topology; Traffic control; Trajectories; Travel time; Matching techniques; Network-based modeling; Real-world trajectories; Recurrent neural network (RNN); Representation learning; Road network; Trajectory similarities; Travel time prediction; Learning to rank
Flexible multi-modal hashing for scalable multimedia retrieval,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078840892&doi=10.1145%2f3365841&partnerID=40&md5=051fa9992528ed48da0ce4038ebc16d3,"Multi-modal hashing methods could support efficient multimedia retrieval by combining multi-modal features for binary hash learning at the both offline training and online query stages. However, existing multimodal methods cannot binarize the queries, when only one or part of modalities are provided. In this article, we propose a novel Flexible Multi-modal Hashing (FMH) method to address this problem. FMH learns multiple modality-specific hash codes and multi-modal collaborative hash codes simultaneously within a single model. The hash codes are flexibly generated according to the newly coming queries, which provide any one or combination of modality features. Besides, the hashing learning procedure is efficiently supervised by the pair-wise semantic matrix to enhance the discriminative capability. It could successfully avoid the challenging symmetric semantic matrix factorization and O(n2) storage cost of semantic matrix. Finally, we design a fast discrete optimization to learn hash codes directly with simple operations. Experiments validate the superiority of the proposed approach. © 2020 Association for Computing Machinery.",Efficient discrete optimization; Multi-modal hashing,Factorization; Optimization; Semantics; Discrete optimization; Learning procedures; Matrix factorizations; Multi-modal; Multimedia Retrieval; Multiple modalities; Off-line training; Simple operation; Hash functions
Is rank aggregation effective in recommender systems? An experimental analysis,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078837361&doi=10.1145%2f3365375&partnerID=40&md5=a03df826b615bb1c9bf337542ea697e4,"Recommender Systems are tools designed to help users find relevant information from the myriad of content available online. Theywork by actively suggesting items that are relevant to users according to their historical preferences or observed actions. Among recommender systems, top-N recommenders work by suggesting a ranking of N items that can be of interest to a user. Although a significant number of top-N recommenders have been proposed in the literature, they often disagree in their returned rankings, offering an opportunity for improving the final recommendation ranking by aggregating the outputs of different algorithms. Rank aggregation was successfully used in a significant number of areas, but only a few rank aggregation methods have been proposed in the recommender systems literature. Furthermore, there is a lack of studies regarding rankings' characteristics and their possible impacts on the improvements achieved through rank aggregation. This work presents an extensive two-phase experimental analysis of rank aggregation in recommender systems. In the first phase, we investigate the characteristics of rankings recommended by 15 different top-N recommender algorithms regarding agreement and diversity. In the second phase, we look at the results of 19 rank aggregation methods and identify different scenarios where they perform best or worst according to the input rankings' characteristics. Our results show that supervised rank aggregation methods provide improvements in the results of the recommended rankings in six out of seven datasets. These methods provide robustness even in the presence of a big set of weak recommendation rankings. However, in cases where there was a set of non-diverse highquality input rankings, supervised and unsupervised algorithms produced similar results. In these cases, we can avoid the cost of the former in favor of the latter. © 2020 Copyright held by the owner/author(s).",Machine learning; Rank aggregation; Recommender systems,Learning systems; Recommender systems; Experimental analysis; High quality; Rank aggregation; Recommender algorithms; Second phase; Two phase; Unsupervised algorithms; Online systems
FrosT: Movement history-conscious facility relocation,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078271241&doi=10.1145%2f3361740&partnerID=40&md5=f67f282fef5b598f0b41b2e6cbc10ced,"The facility relocation (FR) problem, which aims to optimize the placement of facilities to accommodate the changes of users' locations, has a broad spectrum of applications. Despite the significant progress made by existing solutions to the FR problem, they all assume each user is stationary and represented as a single point. Unfortunately, in reality, objects (e.g., people, animals) are mobile. For example, a car-sharing user picks up a vehicle from a station close to where he or she is currently located. Consequently, these efforts may fail to identify a superior solution to the FR problem. In this article, for the first time, we take into account the movement history of users and introduce a novel FR problem, called motion-fr, to address the preceding limitation. Specifically, we present a framework called frost to address it. frost comprises two exact algorithms: index based and index free. The former is designed to address the scenario when facilities and objects are known a priori, whereas the latter solves the motion-fr problem by jettisoning this assumption. Further, we extend the index-based algorithm to solve the general k-motion-fr problem, which aims to relocate k inferior facilities. We devise an approximate solution due to NP-hardness of the problem. Experimental study over both real-world and synthetic datasets demonstrates the superiority of our framework in comparison to state-of-the-art FR techniques in efficiency and effectiveness. © 2020 Association for Computing Machinery.",And Phrases: Facility relocation; Movement history; Spatial database,Approximate solution; Exact algorithms; Facility relocations; Index based algorithm; Movement history; Spatial database; State of the art; Synthetic datasets
Market clearing–based dynamic multi-agent task allocation,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078278150&doi=10.1145%2f3356467&partnerID=40&md5=6c65774de10add9fab2bc24bbaef1bd8,"Realistic multi-agent team applications often feature dynamic environments with soft deadlines that penalize late execution of tasks. This puts a premium on quickly allocating tasks to agents. However, when such problems include temporal and spatial constraints that require tasks to be executed sequentially by agents, they are NP-hard, and thus are commonly solved using general and specifically designed incomplete heuristic algorithms. We propose FMC_TA, a novel such incomplete task allocation algorithm that allows tasks to be easily sequenced to yield high-quality solutions. FMC_TA first finds allocations that are fair (envy-free), balancing the load and sharing important tasks among agents, and efficient (Pareto optimal) in a simplified version of the problem. It computes such allocations in polynomial or pseudo-polynomial time (centrally or distributedly, respectively) using a Fisher market with agents as buyers and tasks as goods. It then heuristically schedules the allocations, taking into account inter-agent constraints on shared tasks. We empirically compare our algorithm to state-of-the-art incomplete methods, both centralized and distributed, on law enforcement problems inspired by real police logs. We present a novel formalization of the law enforcement problem, which we use to perform our empirical study. The results show a clear advantage for FMC_TA in total utility and in measures in which law enforcement authorities measure their own performance. Besides problems with realistic properties, the algorithms were compared on synthetic problems in which we increased the size of different elements of the problem to investigate the algorithm’s behavior when the problem scales. The domination of the proposed algorithm was found to be consistent. © 2020 Association for Computing Machinery.",Distributed Task Allocation; Multi agent system,Commerce; Heuristic algorithms; Intelligent agents; Law enforcement; Pareto principle; Polynomial approximation; Distributed task allocation; Dynamic environments; Empirical studies; Enforcement authorities; High-quality solutions; Synthetic problem; Task allocation algorithm; Temporal and spatial constraints; Multi agent systems
Discovering interesting subpaths with statistical significance from spatiotemporal datasets,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077816086&doi=10.1145%2f3354189&partnerID=40&md5=e28b2cefdf8ca7c1a4edd27d1dd96a61,"Given a path in a spatial or temporal framework, we aim to find all contiguous subpaths that are both interesting (e.g., abrupt changes) and statistically significant (i.e., persistent trends rather than local fluctuations). Discovering interesting subpaths can provide meaningful information for a variety of domains including Earth science, environmental science, urban planning, and the like. Existing methods are limited to detecting individual points of interest along an input path but cannot find interesting subpaths. Our preliminary work provided a Subpath Enumeration and Pruning (SEP) algorithm to detect interesting subpaths of arbitrary length. However, SEP is not effective in avoiding detections that are random variations rather than meaningful trends, which hampers clear and proper interpretations of the results. In this article, we extend our previous work by proposing a significance testing framework to eliminate these random variations. To compute the statistical significance, we first show a baseline Monte-Carlo method based on our previous work and then propose a Dynamic Search-and-Prune (D-SAP) algorithm to improve its computational efficiency. Our experiments show that the significance testing can greatly suppress the noisy detections in the output and D-SAP can greatly reduce the execution time. © 2020 Association for Computing Machinery.",Interesting sub-paths; Spatial; Statistical significance; Temporal,Monte Carlo methods; Environmental science; Interesting sub-paths; Local fluctuations; Significance testing; Spatial; Spatiotemporal datasets; Statistical significance; Temporal; Computational efficiency
Forecasting price trend of bulk commodities leveraging cross-domain open data fusion,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078283996&doi=10.1145%2f3354287&partnerID=40&md5=268280a6927d1428dd20758cf2199b92,"Forecasting price trend of bulk commodities is important in international trade, not only for markets participants to schedule production and marketing plans but also for government administrators to adjust policies. Previous studies cannot support accurate fine-grained short-term prediction, since they mainly focus on coarse-grained long-term prediction using historical data. Recently, cross-domain open data provides possibilities to conduct fine-grained price forecasting, since they can be leveraged to extract various direct and indirect factors of the price. In this article, we predict the price trend over upcoming days, by leveraging cross-domain open data fusion. More specifically, we formulate the price trend into three classes (rise, slight-change, and fall), and then we predict the specific class in which the price trend of the future day lies. We take three factors into consideration: (1) supply factor considering sources providing bulk commodities, (2) demand factor focusing on vessel transportation with reflection of short time needs, and (3) expectation factor encompassing indirect features (e.g., air quality) with latent influences. A hybrid classification framework is proposed for the price trend forecasting. Evaluation conducted on nine real-world cross-domain open datasets shows that our framework can forecast the price trend accurately, outperforming multiple state-of-the-art baselines. © 2020 Association for Computing Machinery.",Bulk commodity; Cross-domain data; Data fusion; Multi-class prediction; Price trend,Air quality; Data fusion; Forecasting; International trade; Marketing; Bulk commodity; Class prediction; Cross-domain; Hybrid classification; Long-term prediction; Price trends; Schedule production; Short term prediction; Open Data
Using sub-optimal plan detection to identify commitment abandonment in discrete environments,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078850897&doi=10.1145%2f3372119&partnerID=40&md5=f61407bd77fc4ee0989fb93edcd47c61,"Assessing whether an agent has abandoned a goal or is actively pursuing it is important whenmultiple agents are trying to achieve joint goals, or when agents commit to achieving goals for each other. Making such a determination for a single goal by observing only plan traces is not trivial, as agents often deviate from optimal plans for various reasons, including the pursuit of multiple goals or the inability to act optimally. In this article, we develop an approach based on domain independent heuristics from automated planning, landmarks, and fact partitions to identify sub-optimal action steps - with respect to a plan - within a fully observable plan execution trace. Such capability is very important in domains where multiple agents cooperate and delegate tasks among themselves, such as through social commitments, and need to ensure that a delegating agent can infer whether or not another agent is actually progressing towards a delegated task. We demonstrate how a creditor can use our technique to determine - by observing a trace - whether a debtor is honouring a commitment. We empirically show, for a number of representative domains, that our approach infers suboptimal action steps with very high accuracy and detects commitment abandonment in nearly all cases. © 2020 Association for Computing Machinery.",Commitments; Domainindependent heuristics; Landmarks; Optimal plan; Plan abandonment; Plan execution; Sub-optimal plan,Multi agent systems; Commitments; Domainindependent heuristics; Landmarks; Optimal plan; Plan abandonment; Plan execution; Optimization
DHPA: Dynamic human preference analytics framework: A case study on taxi drivers' learning curve analysis,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078246990&doi=10.1145%2f3360312&partnerID=40&md5=00745ef09134b4289db03a55134c436c,"Many real-world human behaviors can be modeled and characterized as sequential decision-making processes, such as a taxi driver's choices of working regions and times. Each driver possesses unique preferences on the sequential choices over time and improves the driver's working efficiency. Understanding the dynamics of such preferences helps accelerate the learning process of taxi drivers. Prior works on taxi operation management mostly focus on finding optimal driving strategies or routes, lacking in-depth analysis on what the drivers learned during the process and how they affect the performance of the driver. In this work, we make the first attempt to establish Dynamic Human Preference Analytics. We inversely learn the taxi drivers' preferences from data and characterize the dynamics of such preferences over time. We extract two types of features (i.e., profile features and habit features) to model the decision space of drivers. Then through inverse reinforcement learning, we learn the preferences of drivers with respect to these features. The results illustrate that self-improving drivers tend to keep adjusting their preferences to habit features to increase their earning efficiency while keeping the preferences to profile features invariant. However, experienced drivers have stable preferences over time. The exploring drivers tend to randomly adjust the preferences over time. © 2020 Association for Computing Machinery.",And Phrases: Urban computing; Inverse reinforcement learning; Preference dynamics,Decision making; Dynamics; Efficiency; Machine learning; Reinforcement learning; Taxicabs; In-depth analysis; Inverse reinforcement learning; Learning curve analysis; Learning process; Profile features; Sequential decision making; Urban computing; Working efficiency; Behavioral research
Travel recommendation via fusing multi-auxiliary information into matrix factorization,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078836597&doi=10.1145%2f3372118&partnerID=40&md5=e5fa5aaf61aca1d0c800c93aa7745112,"As an e-commerce feature, the personalized recommendation is invariably highly-valued by both consumers and merchants. The e-tourism has become one of the hottest industries with the adoption of recommendation systems. Several lines of evidence have confirmed the travel-product recommendation is quite different from traditional recommendations. Travel products are usually browsed and purchased relatively infrequently compared with other traditional products (e.g., books and food), which gives rise to the extreme sparsity of travel data. Meanwhile, the choice of a suitable travel product is affected by an army of factors such as departure, destination, and financial and time budgets. To address these challenging problems, in this article, we propose a Probabilistic Matrix Factorization with Multi-Auxiliary Information (PMF-MAI) model in the context of the travel-product recommendation. In particular, PMF-MAI is able to fuse the probabilistic matrix factorization on the user-item interaction matrix with the linear regression on a suite of features constructed by the multiple auxiliary information. In order to fit the sparse data, PMF-MAI is built by a whole-data based learning approach that utilizes unobserved data to increase the coupling between probabilistic matrix factorization and linear regression. Extensive experiments are conducted on a real-world dataset provided by a large tourism e-commerce company. PMF-MAI shows an overwhelming superiority over all competitive baselines on the recommendation performance. Also, the importance of features is examined to reveal the crucial auxiliary information having a great impact on the adoption of travel products. © 2020 Association for Computing Machinery.",Linear regression; Multiple auxiliary information; Probabilistic matrix factorization; Recommender systems; Travel product recommendation,Budget control; Electronic commerce; Factorization; Large dataset; Linear regression; Recommender systems; Auxiliary information; Interaction matrices; Matrix factorizations; Personalized recommendation; Probabilistic matrix factorizations; Product recommendation; Recommendation performance; Traditional products; Matrix algebra
Social science-guided feature engineering: A novel approach to signed link analysis,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078532954&doi=10.1145%2f3364222&partnerID=40&md5=c802fbc16dff1879ecb2a744b6c665ce,"Many real-world relations can be represented by signed networks with positive links (e.g., friendships and trust) and negative links (e.g., foes and distrust). Link prediction helps advance tasks in social network analysis such as recommendation systems. Most existing work on link analysis focuses on unsigned social networks. The existence of negative links piques research interests in investigating whether properties and principles of signed networks differ from those of unsigned networks and mandates dedicated efforts on link analysis for signed social networks. Recent findings suggest that properties of signed networks substantially differ from those of unsigned networks and negative links can be of significant help in signed link analysis in complementary ways. In this article,we center our discussion on a challenging problem of signed link analysis. Signed link analysis faces the problem of data sparsity, i.e., only a small percentage of signed links are given. This problem can even getworse when negative links are much sparser than positive ones as users are inclined more toward positive disposition rather than negative. We investigate how we can take advantage of other sources of information for signed link analysis. This research is mainly guided by three social science theories, Emotional Information, Diffusion of Innovations, and Individual Personality. Guided by these, we extract three categories of related features and leverage them for signed link analysis. Experiments showthe significance of the features gleaned from social theories for signed link prediction and addressing the data sparsity challenge. © 2020 Association for Computing Machinery.",Data sparsity; Diffusion of innovation; Emotional information; Feature engineering; Individual personality; Signed link analysis; Social theory,Data sparsity; Diffusion of innovations; Emotional information; Feature engineerings; Individual personality; Link analysis; Social theory; Behavioral research
Exploring correlation network for cheating detection,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078269412&doi=10.1145%2f3364221&partnerID=40&md5=e8fd8adf76948e3ae0eca9e585a32d4f,"The correlation network, typically formed by computing pairwise correlations between variables, has recently become a competitive paradigm to discover insights in various application domains, such as climate prediction, financial marketing, and bioinformatics. In this study, we adopt this paradigm to detect cheating behavior hidden in business distribution channels, where falsified big deals are often made by collusive partners to obtain lower product prices'a behavior deemed to be extremely harmful to the sale ecosystem. To this end, we assume that abnormal deals are likely to occur between two partners if their purchase-volume sequences have a strong negative correlation. This seemingly intuitive rule, however, imposes several research challenges. First, existing correlation measures are usually symmetric and thus cannot distinguish the different roles of partners in cheating. Second, the tick-to-tick correspondence between two sequences might be violated due to the possible delay of purchase behavior, which should also be captured by correlation measures. Finally, the fact that any pair of sequences could be correlated may result in a number of false-positive cheating pairs, which need to be corrected in a systematic manner. To address these issues, we propose a correlation network analysis framework for cheating detection. In the framework, we adopt an asymmetric correlation measure to distinguish the two roles, namely, cheating seller and cheating buyer, in a cheating alliance. Dynamic Time Warping is employed to address the time offset between two sequences in computing the correlation. We further propose two graph-cut methods to convert the correlation network into a bipartite graph to rank cheating partners, which simultaneously helps to remove false-positive correlation pairs. Based on a 4-year real-world channel dataset from a worldwide IT company, we demonstrate the effectiveness of the proposed method in comparison to competitive baseline methods. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",And Phrases: Correlation network analysis; Cheating detection; Distribution channel; Graph cut; Time series,Graph theory; Graphic methods; Sales; Time series; Cheating detection; Correlation measures; Correlation network; Distribution channel; Dynamic time warping; Graph cut; Negative correlation; Pairwise correlation; Time series analysis
Single image snow removal using sparse representation and particle swarm optimizer,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078870363&doi=10.1145%2f3372116&partnerID=40&md5=09e1d3224a55571e0ea72f3c14ab7b8f,"Images are often corrupted by natural obscuration (e.g., snow, rain, and haze) during acquisition in bad weather conditions. The removal of snowflakes from only a single image is a challenging task due to situational variety and has been investigated only rarely. In this article, we propose a novel snow removal framework for a single image, which can be separated into a sparse image approximation module and an adaptive tolerance optimization module. The first proposed module takes the advantage of sparsity-based regularization to reconstruct a potential snow-free image. An auto-tuning mechanism for this framework is then proposed to seek a better reconstruction of a snow-free image via the time-varying inertia weight particle swarm optimizers in the second proposed module. Through collaboration of these two modules iteratively, the number of snowflakes in the reconstructed image is reduced as generations progress. By the experimental results, the proposed method achieves a better efficacy of snow removal than do other stateof-the-art techniques via both objective and subjective evaluations. As a result, the proposed method is able to remove snowflakes successfully from only a single image while preserving most original object structure information. © 2020 Association for Computing Machinery.",Image restoration; Snow removal; Sparse representation,Iterative methods; Particle swarm optimization (PSO); Snow; Street cleaning; Adaptive tolerance; Objective and subjective evaluations; Particle swarm optimizers; Reconstructed image; Snow removal; Sparse representation; State-of-the-art techniques; Time-varying inertia; Image reconstruction
XLearn: Learning activity labels across heterogeneous datasets,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078822927&doi=10.1145%2f3368272&partnerID=40&md5=a3fea8849937ed03ded1ebdc233346e8,"Sensor-driven systems often need to map sensed data into meaningfully labelled activities to classify the phenomena being observed. A motivating and challenging example comes from human activity recognition in which smart home and other datasets are used to classify human activities to support applications such as ambient assisted living, health monitoring, and behavioural intervention. Building a robust and meaningful classifier needs annotated ground truth, labelled with what activities are actually being observed - and acquiring high-quality, detailed, continuous annotations remains a challenging, time-consuming, and errorprone task, despite considerable attention in the literature. In this article, we use knowledge-driven ensemble learning to develop a technique that can combine classifiers built from individually labelled datasets, even when the labels are sparse and heterogeneous. The technique both relieves individual users of the burden of annotation and allows activities to be learned individually and then transferred to a general classifier. We evaluate our approach using four third-party, real-world smart home datasets and show that it enhances activity recognition accuracies even when given only a very small amount of training data. © 2020 Copyright held by the owner/author(s).",Clustering; Ensemble learning; Human activity recognition; Smart home; Transfer learning,Automation; Pattern recognition; Clustering; Ensemble learning; Human activity recognition; Smart homes; Transfer learning; Classification (of information)
Pair-based uncertainty and diversity promoting early active learning for person re-identification,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079375376&doi=10.1145%2f3372121&partnerID=40&md5=d5d5485d7475fc117df64565d15651f1,"The effective training of supervised Person Re-identification (Re-ID) models requires sufficient pairwise labeled data. However, when there is limited annotation resource, it is difficult to collect pairwise labeled data. We consider a challenging and practical problem called Early Active Learning, which is applied to the early stage of experiments when there is no pre-labeled sample available as references for human annotating. Previous early active learning methods suffer from two limitations for Re-ID. First, these instance-based algorithms select instances rather than pairs, which can result in missing optimal pairs for Re-ID. Second, most of these methods only consider the representativeness of instances, which can result in selecting less diverse and less informative pairs. To overcome these limitations, we propose a novel pair-based active learning for Re-ID. Our algorithm selects pairs instead of instances from the entire dataset for annotation. Besides representativeness, we further take into account the uncertainty and the diversity in terms of pairwise relations. Therefore, our algorithm can produce the most representative, informative, and diverse pairs for Re-ID data annotation. Extensive experimental results on five benchmark Re-ID datasets have demonstrated the superiority of the proposed pair-based early active learning algorithm. © 2020 Association for Computing Machinery.",Active learning; Person re-identification,Artificial intelligence; Labeled data; Learning algorithms; Active Learning; Active learning methods; Active-learning algorithm; Id datum; Optimal pairs; Person re identifications; Practical problems; Learning systems
Newton methods for convolutional neural networks,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079502194&doi=10.1145%2f3368271&partnerID=40&md5=50efe0d83d06e90970195d95d572ce05,"Deep learning involves a difficult non-convex optimization problem, which is often solved by stochastic gradient (SG) methods. While SG is usually effective, it may not be robust in some situations. Recently, Newton methods have been investigated as an alternative optimization technique, but most existing studies consider only fully connected feedforward neural networks. These studies do not investigate some more commonly used networks such as Convolutional Neural Networks (CNN). One reason is that Newton methods for CNN involve complicated operations, and so far no works have conducted a thorough investigation. In this work, we give details of all building blocks, including the evaluation of function, gradient, Jacobian, and Gauss-Newton matrix-vector products. These basic components are very important not only for practical implementation but also for developing variants of Newton methods for CNN. We show that an efficient MATLAB implementation can be done in just several hundred lines of code. Preliminary experiments indicate that Newton methods are less sensitive to parameters than the stochastic gradient approach. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Convolution neural networks; Large-scale classification; Newton methods; Subsampled Hessian,Convex optimization; Convolution; Deep learning; Feedforward neural networks; Newton-Raphson method; Stochastic systems; Building blockes; Convolution neural network; Large scale classifications; Lines of code; Nonconvex optimization; Optimization techniques; Stochastic gradient; Subsampled Hessian; Convolutional neural networks
"Web table extraction, retrieval, and augmentation: A survey",2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079505018&doi=10.1145%2f3372117&partnerID=40&md5=cd724ca3c96a8d25d85d19340b962ef3,"Tables are powerful and popular tools for organizing and manipulating data. A vast number of tables can be found on the Web, which represent a valuable knowledge resource. The objective of this survey is to synthesize and present two decades of research on web tables. In particular, we organize existing literature into six main categories of information access tasks: table extraction, table interpretation, table search, question answering, knowledge base augmentation, and table augmentation. For each of these tasks, we identify and describe seminal approaches, present relevant resources, and point out interdependencies among the different tasks. © 2020 Association for Computing Machinery.",Table augmentation; Table extraction; Table interpretation; Table mining; Table retrieval; Table search,Extraction; Knowledge based systems; Surveys; Information access; Knowledge base; Knowledge resource; Question Answering; Table augmentation; Table interpretation; Table retrieval; Table search; Information retrieval
Discovering underlying plans based on shallow models,2020,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079488052&doi=10.1145%2f3368270&partnerID=40&md5=c5705496bbec5d5f8e9f9c8b67dc5e06,"Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or action models in hand. Previous approaches either discover plans by maximally “matching” observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing action models to best explain the observed actions, assuming that complete action models are available. In real-world applications, however, target plans are often not from plan libraries, and complete action models are often not available, since building complete sets of plans and complete action models are often difficult or expensive. In this article, we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Specifically, we propose two approaches, DUP and RNNPlanner, to discover target plans based on vector representations of actions. DUP explores the EM-style (Expectation Maximization) framework to capture local contexts of actions and discover target plans by optimizing the probability of target plans, while RNNPlanner aims to leverage long-short term contexts of actions based on RNNs (Recurrent Neural Networks) framework to help recognize target plans. In the experiments, we empirically show that our approaches are capable of discovering underlying plans that are not from plan libraries without requiring action models provided. We demonstrate the effectiveness of our approaches by comparing its performance to traditional plan recognition approaches in three planning domains. We also compare DUP and RNNPlanner to see their advantages and disadvantages. © 2020 Association for Computing Machinery.",Action representation; Plan recognition; Recurrent neural networks; Shallow model,Libraries; Maximum principle; Planning; Action models; Action representations; Expectation - maximizations; Local contexts; Plan libraries; Plan recognition; Planning domains; Vector representations; Recurrent neural networks
A visual analysis approach for understanding durability test data of automotive products,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077793072&doi=10.1145%2f3345640&partnerID=40&md5=47bb20e5a4707556e3d191e9afa35e5d,"People face data-rich manufacturing environments in Industry 4.0. As an important technology for explaining and understanding complex data, visual analytics has been increasingly introduced into industrial data analysis scenarios. With the durability test of automotive starters as background, this study proposes a visual analysis approach for understanding large-scale and long-term durability test data. Guided by detailed scenario and requirement analyses, we first propose a migration-adapted clustering algorithm that utilizes a segmentation strategy and a group of matching-updating operations to achieve an efficient and accurate clustering analysis of the data for starting mode identification and abnormal test detection. We then design and implement a visual analysis system that provides a set of user-friendly visual designs and lightweight interactions to help people gain data insights into the test process overview, test data patterns, and durability performance dynamics. Finally, we conduct a quantitative algorithm evaluation, case study, and user interview by using real-world starter durability test datasets. The results demonstrate the effectiveness of the approach and its possible inspiration for the durability test data analysis of other similar industrial products. © 2019 Association for Computing Machinery.",Automotive starter; Durability test; Industry 4.0; Smart manufacturing; Visual analysis,Cluster analysis; Clustering algorithms; Ignition; Industry 4.0; Information analysis; Starters; Testing; Algorithm evaluation; Design and implements; Durability performance; Durability test; Manufacturing environments; Requirement analysis; Smart manufacturing; Visual analysis; Durability
Comparison and modelling of country-level microblog user and activity in cyber-physical-social systems using weibo and Twitter data,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077792375&doi=10.1145%2f3339474&partnerID=40&md5=b2945ec8fe5f1a66e04fdaa95e01a9f9,"As the rapid growth of social media technologies continues, Cyber-Physical-Social System (CPSS) has been a hot topic in many industrial applications. The use of “microblogging” services, such as Twitter, has rapidly become an influential way to share information. While recent studies have revealed that understanding and modelling microblog user behaviour with massive users’ data in social media are keen to success of many practical applications in CPSS, a key challenge in literatures is that diversity of geography and cultures in social media technologies strongly affect user behaviour and activity. The motivation of this article is to understand differences and similarities between microblogging users from different countries using social media technologies, and to attempt to design a Country-Level Micro-Blog User (CLMB) behaviour and activity model for supporting CPSS applications. We proposed a CLMB model for analysing microblogging user behaviour and their activity across different countries in the CPSS applications. The model has considered three important characteristics of user behaviour in microblogging data, including content of microblogging messages, user emotion index, and user relationship network. We evaluated CLBM model under the collected microblog dataset from 16 countries with the largest number of representative and active users in the world. Experimental results show that (1) for some countries with small population and strong cohesiveness, users pay more attention to social functionalities of microblogging service; (2) for some countries containing mostly large loose social groups, users use microblogging services as a news dissemination platform; (3) users in countries whose social network structure exhibits reciprocity rather than hierarchy will use more linguistic elements to express happiness in microblogging services. © 2019 Association for Computing Machinery.",Microblogging; Weibo,Cyber Physical System; Social networking (online); Activity modeling; Cyber physicals; Micro-blogging services; Microblogging; Relationship networks; Small population; Social network structures; Weibo; Behavioral research
Special issue on intelligent edge computing for cyber physical and cloud systems,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077877498&doi=10.1145%2f3372277&partnerID=40&md5=8927f8d7536570cbb86b8fcb274da498,[No abstract available],,
Graph-based recommendation meets bayes and similarity measures,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077358168&doi=10.1145%2f3356882&partnerID=40&md5=0e0bb941e451da9ec29fb2db58f8c20d,"Graph-based approaches provide an effective memory-based alternative to latent factor models for collaborative recommendation. Modern approaches rely on either sampling short walks or enumerating short paths starting from the target user in a user-item bipartite graph. While the effectiveness of random walk sampling heavily depends on the underlying path sampling strategy, path enumeration is sensitive to the strategy adopted for scoring each individual path. In this article, we demonstrate how both strategies can be improved through Bayesian reasoning. In particular, we propose to improve random walk sampling by exploiting distributional aspects of items' ratings on the sampled paths. Likewise, we extend existing path enumeration approaches to leverage categorical ratings and to scale the score of each path proportionally to the affinity of pairs of users and pairs of items on the path. Experiments on several publicly available datasets demonstrate the effectiveness of our proposed approaches compared to state-of-the-art graph-based recommenders. © 2019 Association for Computing Machinery.",Bayesian statistics; Collaborative filtering; Graph-based recommendation; Similarity measures,Collaborative filtering; Graphic methods; Random processes; Bayesian reasoning; Bayesian statistics; Bipartite graphs; Collaborative recommendation; Graph-based; Latent factor models; Similarity measure; State of the art; Graph theory
Robust fake news detection over time and attack,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077367010&doi=10.1145%2f3363818&partnerID=40&md5=ff2c4901b28e59cc2cd1f7320e1086b7,"In this study, we examine the impact of time on state-of-the-art news veracity classifiers. We show that, as time progresses, classification performance for both unreliable and hyper-partisan news classification slowly degrade. While this degradation does happen, it happens slower than expected, illustrating that hand-crafted, content-based features, such as style of writing, are fairly robust to changes in the news cycle.We show that this small degradation can bemitigated using online learning. Last, we examine the impact of adversarial content manipulation by malicious news producers. Specifically, we test three types of attack based on changes in the input space and data availability. We show that static models are susceptible to content manipulation attacks, but online models can recover from such attacks. © 2019 Association for Computing Machinery.",Adversarial machine learning; Biased news; Concept drift; Disinformation; Fake news; Fake news detection; Misinformation; Misleading news; Robust machine learning,Biased news; Concept drifts; Disinformation; Fake news; Misinformation; Misleading news; Machine learning
Edge-enabled disaster rescue: A case study of searching for missing people,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075263302&doi=10.1145%2f3331146&partnerID=40&md5=ca3c09318a4e99e6cd7a507b8b63813c,"In the aftermath of earthquakes, floods, and other disasters, photos are increasingly playing more significant roles, such as finding missing people and assessing disasters, in rescue and recovery efforts. These disaster photos are taken in real time by the crowd, unmanned aerial vehicles, and wireless sensors. However, communications equipment is often damaged in disasters, and the very limited communication bandwidth restricts the upload of photos to the cloud center, seriously impeding disaster rescue endeavors. Based on edge computing, we propose Echo, a highly time-efficient disaster rescue framework. By utilizing the computing, storage, and communication abilities of edge servers, disaster photos are preprocessed and analyzed in real time, and more specific visuals are immensely helpful for conducting emergency response and rescue. This article takes the search for missing people as a case study to show that Echo can be more advantageous in terms of disaster rescue. To greatly conserve valuable communication bandwidth, only significantly associated images are extracted and uploaded to the cloud center for subsequent facial recognition. Furthermore, an adaptive photo detector is designed to utilize the precious and unstable communication bandwidth effectively, as well as ensure the photo detection precision and recall rate. The effectiveness and efficiency of the proposed method are demonstrated by simulation experiments. © 2019 Association for Computing Machinery.",Disaster rescue; Edge computing; Face recognition; Finding missing people; Time-efficient,Antennas; Bandwidth; Edge computing; Face recognition; Photodetectors; Communication bandwidth; Communications equipment; Disaster rescue; Effectiveness and efficiencies; Facial recognition; Finding missing people; Limited communication; Time-efficient; Emergency services
Strategic atack & defense in security diffusion games,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076699172&doi=10.1145%2f3357605&partnerID=40&md5=2cd83bb5fd5a6c538c42956a53d2ef2a,"Security games model the confrontation between a defender protecting a set of targets and an attacker who tries to capture them. A variant of these games assumes security interdependence between targets, facilitating contagion of an attack. So far, only stochastic spread of an attack has been considered. In this work, we introduce a version of security games, where the attacker strategically drives the entire spread of attack and where interconnections between nodes affect their susceptibility to be captured. We find that the strategies effective in the settings without contagion or with stochastic contagion are no longer feasible when spread of attack is strategic. While in the former settings it was possible to efficiently find optimal strategies of the attacker, doing so in the latter setting turns out to be an NP-complete problem for an arbitrary network. However, for some simpler network structures, such as cliques, stars, and trees, we show that it is possible to efficiently find optimal strategies of both players. For arbitrary networks, we study and compare the efficiency of various heuristic strategies. As opposed to previous works with no or stochastic contagion, we find that centrality-based defense is often effective when spread of attack is strategic, particularly for centrality measures based on the Shapley value. © 2019 Association for Computing Machinery.",Network diffusion; Security games; Social networks,Computational complexity; Optimal systems; Optimization; Social networking (online); Stochastic systems; Arbitrary networks; Centrality measures; Heuristic strategy; Network diffusions; Network structures; Optimal strategies; Security games; Shapley value; Network security
BAMB: A balanced markov blanket discovery approach to feature selection,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075698742&doi=10.1145%2f3335676&partnerID=40&md5=d6b3cb70d876758666ba47e07c54c5af,"The discovery of Markov blanket (MB) for feature selection has attracted much attention in recent years, since the MB of the class attribute is the optimal feature subset for feature selection. However, almost all existing MB discovery algorithms focus on either improving computational efficiency or boosting learning accuracy, instead of both. In this article, we propose a novel MB discovery algorithm for balancing efficiency and accuracy, called BAlanced Markov Blanket (BAMB) discovery. To achieve this goal, given a class attribute of interest, BAMB finds candidate PC (parents and children) and spouses and removes false positives from the candidateMB set in one go. Specifically, once a feature is successfully added to the current PC set, BAMB finds the spouses with regard to this feature, then uses the updated PC and the spouse set to remove false positives from the current MB set. This makes the PC and spouses of the target as small as possible and thus achieves a trade-off between computational efficiency and learning accuracy. In the experiments, we first compare BAMB with 8 state-of-the-art MB discovery algorithms on 7 benchmark Bayesian networks, then we use 10 real-world datasets and compare BAMB with 12 feature selection algorithms, including 8 state-of-the-art MB discovery algorithms and 4 other well-established feature selection methods. On prediction accuracy, BAMB outperforms 12 feature selection algorithms compared. On computational efficiency, BAMB is close to the IAMB algorithm while it is much faster than the remaining seven MB discovery algorithms. © 2019 Association for Computing Machinery.",Bayesian network; Classification; Feature selection; Markov blanket,Bayesian networks; Classification (of information); Economic and social effects; Efficiency; Feature extraction; Discovery algorithm; Feature selection algorithm; Feature selection methods; Learning accuracy; Markov Blankets; Prediction accuracy; Real-world datasets; State of the art; Computational efficiency
Lightweight convolution neural networks for mobile edge computing in transportation cyber physical systems,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075625088&doi=10.1145%2f3339308&partnerID=40&md5=7621f2121422c581c06d1dbb97a9742f,"Cloud computing extends Transportation Cyber-Physical Systems (T-CPS) with provision of enhanced computing and storage capability via offloading computing tasks to remote cloud servers. However, cloud computing cannot fulfill the requirements such as low latency and context awareness in T-CPS. The appearance of Mobile Edge Computing (MEC) can overcome the limitations of cloud computing via offloading the computing tasks at edge servers in approximation to users, consequently reducing the latency and improving the context awareness. Although MEC has the potential in improving T-CPS, it is incapable of processing computational-intensive tasks such as deep learning algorithms due to the intrinsic storage and computingcapability constraints. Therefore, we design and develop a lightweight deep learning model to support MEC applications in T-CPS. In particular, we put forth a stacked convolutional neural network (CNN) consisting of factorization convolutional layers alternating with compression layers (namely, lightweight CNN-FC). Extensive experimental results show that our proposed lightweight CNN-FC can greatly decrease the number of unnecessary parameters, thereby reducing the model size while maintaining the high accuracy in contrast to conventional CNN models. In addition, we also evaluate the performance of our proposed model via conducting experiments at a realistic MEC platform. Specifically, experimental results at this MEC platform show that our model can maintain the high accuracy while preserving the portable model size. © 2019 Association for Computing Machinery.",Convolutional neural network; Cyber physical systems; Factorization; Jetson TX2 module; Mobile edge computing; Model compression,Convolution; Cyber Physical System; Deep learning; Edge computing; Embedded systems; Factorization; Learning algorithms; Compression layers; Context- awareness; Convolution neural network; Convolutional neural network; Jetson TX2 module; Learning models; Model compression; Storage capability; Multilayer neural networks
Using sparse representation to detect anomalies in complex WSNs,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075624274&doi=10.1145%2f3331147&partnerID=40&md5=e86fd2f95ca759066dd1176869147bcb,"In recent years, wireless sensor networks (WSNs) have become an active area of research for monitoring physical and environmental conditions. Due to the interdependence of sensors, a functional anomaly in one sensor can cause a functional anomaly in another sensor, which can further lead to the malfunctioning of the entire sensor network. Existing research work has analysed faulty sensor anomalies but fails to show the effectiveness throughout the entire interdependent network system. In this article, a dictionary learning algorithm based on a non-negative constraint is developed, and a sparse representation anomaly node detection method for sensor networks is proposed based on the dictionary learning. Through experiment on a specific thermal power plant in China, we verify the robustness of our proposed method in detecting abnormal nodes against four state of the art approaches and proved our method is more robust. Furthermore, the experiments are conducted on the obtained abnormal nodes to prove the interdependence of multi-layer sensor networks and reveal the conditions and causes of a system crash. © 2019 Association for Computing Machinery.",Anomaly detection; Dependency relationships networks; Sparse Representation; WSNs,Learning algorithms; Network layers; Sensor nodes; Thermoelectric power plants; Dependency relationship; Dictionary learning algorithms; Environmental conditions; Sparse representation; State-of-the-art approach; Thermal power plants; Wireless sensor network (WSNs); WSNs; Anomaly detection
Estimating and controlling the false discovery rate of the PC algorithm using edge-specific P-values,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075690592&doi=10.1145%2f3351342&partnerID=40&md5=7bf4d0b9d800624fd52820659d27ef5d,"Many causal discovery algorithms infer graphical structure from observational data. The PC algorithm in particular estimates a completed partially directed acyclic graph (CPDAG), or an acyclic graph containing directed edges identifiable with conditional independence testing. However, few groups have investigated strategies for estimating and controlling the false discovery rate (FDR) of the edges in the CPDAG. In this article, we introduce PC with p-values (PC-p), a fast algorithm that robustly computes edge-specific p-values and then estimates and controls the FDR across the edges. PC-p specifically uses the p-values returned by many conditional independence (CI) tests to upper bound the p-values of more complex edge-specific hypothesis tests. The algorithm then estimates and controls the FDR using the bounded p-values and the Benjamini-Yekutieli FDR procedure. Modifications to the original PC algorithm also help PC-p accurately compute the upper bounds despite non-zero Type II error rates. Experiments show that PC-p yields more accurate FDR estimation and control across the edges in a variety of CPDAGs compared to alternative methods. © 2019 Association for Computing Machinery.",Bayesian network; Causal inference; Directed acyclic graph; False discovery rate; PC algorithm,Bayesian networks; Directed graphs; Causal inferences; Conditional independences; Directed acyclic graph (DAG); Discovery algorithm; False discovery rate; Graphical structures; Observational data; Partially directed acyclic graphs; Inference engines
Introduction to the special section on advances in causal discovery and inference,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075731708&doi=10.1145%2f3359995&partnerID=40&md5=31adc50c843002ddac2e77a00384f87f,[No abstract available],,
Multi-view fusion with extreme learning machine for clustering,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075674204&doi=10.1145%2f3340268&partnerID=40&md5=99bc3eeeb4d228384297c8a9993f81e1,"Unlabeled, multi-view data presents a considerable challenge in many real-world data analysis tasks. These data are worth exploring because they often contain complementary information that improves the quality of the analysis results. Clustering with multi-view data is a particularly challenging problem as revealing the complex data structures between many feature spaces demands discriminative features that are specific to the task and, when too few of these features are present, performance suffers. Extreme learning machines (ELMs) are an emerging form of learning model that have shown an outstanding representation ability and superior performance in a range of different learning tasks. Motivated by the promise of this advancement, we have developed a novel multi-view fusion clustering framework based on an ELM, called MVEC. MVEC learns the embeddings from each view of the data via the ELM network, then constructs a single unified embedding according to the correlations and dependencies between each embedding and automatically weighting the contribution of each. This process exposes the underlying clustering structures embedded within multi-view data with a high degree of accuracy. A simple yet efficient solution is also provided to solve the optimization problem within MVEC. Experiments and comparisons on eight different benchmarks from different domains confirm MVEC's clustering accuracy. © 2019 Association for Computing Machinery.",Extreme learning machine; Multi-view clustering; Multi-view embedding; Unsupervised learning,Embeddings; Knowledge acquisition; Machine learning; Unsupervised learning; Clustering accuracy; Complex data structures; Discriminative features; Extreme learning machine; High degree of accuracy; Multi-view clustering; Multi-views; Optimization problems; Quality control
Mixture of joint nonhomogeneous Markov chains to cluster and model water consumption behavior sequences,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075617128&doi=10.1145%2f3347452&partnerID=40&md5=f991aedfd180f7f0c53a7a4ec69735f3,"The emergence of smartmeters has fostered the collection ofmassive data that support a better understanding of consumer behaviors and bettermanagement ofwater resources and networks. The main focus of this article is to analyze consumption behavior over time; thus, we first identify the main weekly consumption patterns. This approach allows each meter to be represented by a categorical series,where each category corresponds to a weekly consumption behavior. By considering the resulting consumption behavior sequences, we propose a new methodology based on a mixture of nonhomogeneous Markov models to cluster these categorical time series. Using this method, the meters are described by the Markovian dynamics of their cluster. The latent variable that controls cluster membership is estimated alongside the parameters of the Markov model using a novel classification expectation maximization algorithm. A specific entropy measure is formulated to evaluate the quality of the estimated partition by considering the joint Markovian dynamics. The proposed clustering model can also be used to predict future consumption behaviors within each cluster. Numerical experiments using real water consumption data provided by a water utility in France and gathered over 19 months are conducted to evaluate the performance of the proposed approach in terms of both clustering and prediction. The results demonstrate the effectiveness of the proposed method. © 2019 Association for Computing Machinery.",Categorical time series; Clustering; Forecasting; Nonhomogeneous Markov models; Water consumption behavior,Dynamics; Forecasting; Image segmentation; Markov processes; Maximum principle; Mixtures; Quality control; Time series; Water supply; Cluster memberships; Clustering; Consumption patterns; Expectation-maximization algorithms; Markov model; Nonhomogeneous Markov chains; Numerical experiments; Water consumption; Consumer behavior
Trust computing-based security routing scheme for cyber physical systems,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075600747&doi=10.1145%2f3321694&partnerID=40&md5=e7b01b65eb6aa385541f2446ed48ade7,"Security is a pivotal issue for the development of Cyber Physical Systems (CPS). The trusted computing of CPS includes the complete protection mechanisms, such as hardware, firmware, and software, the combination of which is responsible for enforcing a system security policy. A Trust Detection-based Secured Routing (TDSR) scheme is proposed to establish security routes from source nodes to the data center under malicious environment to ensure network security. In the TDSR scheme, sensor nodes in the routing path send detection routing to identify relay nodes' trust. And then, data packets are routed through trustworthy nodes to sink securely. In the TDSR scheme, the detection routing is executed in those nodes that have abundant energy; thus, the network lifetime cannot be affected. Performance evaluation through simulation is carried out for success of routing ratio, compromised node detection ratio, and detection routing overhead. The experiment results show that the performance can be improved in the TDSR scheme compared to previous schemes. © 2019 Association for Computing Machinery.",Cyber physical systems; Data collection; Malicious attacks; Trust-based routing,Cyber Physical System; Embedded systems; Firmware; Network routing; Sensor nodes; Trusted computing; Compromised nodes; Cyber-physical systems (CPS); Data collection; Malicious attack; Protection mechanisms; Routing overheads; Security routing; Trust-based routing; Network security
Efficient and privacy-preserving fog-assisted health data sharing scheme,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075603196&doi=10.1145%2f3341104&partnerID=40&md5=c116a7c42891e637f8806a2a640b401a,"Pervasive data collected from e-healthcare devices possess significant medical value through data sharing with professional healthcare service providers. However, health data sharing poses several security issues, such as access control and privacy leakage, as well as faces critical challenges to obtain efficient data analysis and services. In this article, we propose an efficient and privacy-preserving fog-assisted health data sharing (PFHDS) scheme for e-healthcare systems. Specifically, we integrate the fog node to classify the shared data into different categories according to disease risks for efficient health data analysis. Meanwhile, we design an enhanced attribute-based encryption method through combination of a personal access policy on patients and a professional access policy on the fog node for effective medical service provision. Furthermore, we achieve significant encryption consumption reduction for patients by offloading a portion of the computation and storage burden from patients to the fog node. Security discussions show that PFHDS realizes data confidentiality and fine-grained access control with collusion resistance. Performance evaluations demonstrate cost-efficient encryption computation, storage and energy consumption. © 2019 Association for Computing Machinery.",Access control; Data sharing; E-healthcare; Fog computing; Privacypreservation,Access control; Cryptography; Digital storage; Energy utilization; Fog; Fog computing; Health care; Health risks; Information analysis; Risk assessment; Attribute-based encryptions; Collusion resistance; Consumption reductions; Data confidentiality; Data Sharing; E-healthcare; Healthcare services; Privacy preservation; Data privacy
Crowdsourcing mechanism for trust evaluation in CPCS based on intelligent mobile edge computing,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075606041&doi=10.1145%2f3324926&partnerID=40&md5=76ec017c905aadf9bfe22862bc5c4963,"Both academia and industry have directed tremendous interest toward the combination of Cyber Physical Systems and Cloud Computing, which enables a new breed of applications and services. However, due to the relative long distance between remote cloud and end nodes, Cloud Computing cannot provide effective and direct management for end nodes, which leads to security vulnerabilities. In this article, we first propose a novel trust evaluation mechanism using crowdsourcing and Intelligent Mobile Edge Computing. The mobile edge users with relatively strong computation and storage ability are exploited to provide direct management for end nodes. Through close access to end nodes, mobile edge users can obtain various information of the end nodes and determine whether the node is trustworthy. Then, two incentive mechanisms, i.e., Trustworthy Incentive and Quality-Aware Trustworthy Incentive Mechanisms, are proposed for motivating mobile edge users to conduct trust evaluation. The first one aims to motivate edge users to upload their real information about their capability and costs. The purpose of the second one is to motivate edge users to make trustworthy effort to conduct tasks and report results. Detailed theoretical analysis demonstrates the validity of Quality- Aware Trustworthy Incentive Mechanism from data trustfulness, effort trustfulness, and quality trustfulness, respectively. Extensive experiments are carried out to validate the proposed trust evaluation and incentive mechanisms. The results corroborate that the proposed mechanisms can efficiently stimulate mobile edge users to perform evaluation task and improve the accuracy of trust evaluation. © 2019 Association for Computing Machinery.",Artificial intelligence; Crowdsourcing; Mobile edge computing; Trust evaluation,Artificial intelligence; Crowdsourcing; Digital storage; Edge computing; Embedded systems; Intelligent computing; Trusted computing; Incentive mechanism; Security vulnerabilities; Storage abilities; Trust evaluation; Quality control
Secure deduplication system with active key update and its application in IoT,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075606095&doi=10.1145%2f3356468&partnerID=40&md5=ed1af70abaa8482563f55b02e50c4ca7,"The rich cloud services in the Internet of Things create certain needs for edge computing, in which devices should be able to handle storage tasks securely, reliably, and efficiently. When processing the storage requests from edge devices, each cloud server is supposed to eliminate duplicate copies of repeating data to reduce the amount of storage space and save on bandwidth. To protect data confidentiality while supporting deduplication, some convergent-encryption-based techniques have been proposed to encrypt the data before uploading. However, all these works cannot meet two requirements while preventing brute-force attacks: (i) power-constrained edge nodes should update encryption keys efficiently when an edge node is abandoned; and (ii) the access privacy of edge nodes should be guaranteed. In this article, we propose a novel encryption scheme for secure chunk-level deduplication. Based on this scheme, we present two constructions of the secure deduplication system that support an efficient key update protocol. The key update protocol does not involve any edge node in computational tasks, so that the deduplication system can adopt an active key update strategy.Moreover, one of our constructions, which is called advance construction, can provide access privacy assurances for edge nodes. The security analysis is given in terms of the proposed threat model. The experimental analysis demonstrates that the proposed deduplication system is practical. © 2019 Association for Computing Machinery.",Convergent encryption; Deduplication; Edge computing; Key update,Digital storage; Edge computing; Internet of things; Web services; Brute-force attack; Computational task; Data confidentiality; De duplications; Encryption schemes; Experimental analysis; Key updates; Security analysis; Cryptography
Energy-efficient Static Task Scheduling on VFI-based NoC-HMPSoCs for Intelligent Edge Devices in Cyber-physical Systems,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074216829&doi=10.1145%2f3336121&partnerID=40&md5=4de875e50ab34e3a777f3e2d58db7516,"The interlinked processing units in modern Cyber-Physical Systems (CPS) creates a large network of connected computing embedded systems. Network-on-Chip (NoC)-based Multiprocessor System-on-Chip (MPSoC) architecture is becoming a de facto computing platform for real-time applications due to its higher performance and Quality-of-Service (QoS). The number of processors has increased significantly on the multiprocessor systems in CPS; therefore, Voltage Frequency Island (VFI) has been recently adopted for effective energy management mechanism in the large-scale multiprocessor chip designs. In this article, we investigated energy-efficient and contention-aware static scheduling for tasks with precedence and deadline constraints on intelligent edge devices deploying heterogeneous VFI-based NoC-MPSoCs (VFI-NoC-HMPSoC) with DVFS-enabled processors. Unlike the existing population-based optimization algorithms, we proposed a novel population-based algorithm called ARSH-FATI that can dynamically switch between explorative and exploitative search modes at run-time. Our static scheduler ARHS-FATI collectively performs task mapping, scheduling, and voltage scaling. Consequently, its performance is superior to the existing state-of-the-art approach proposed for homogeneous VFI-based NoC-MPSoCs. We also developed a communication contention-aware Earliest Edge Consistent Deadline First (EECDF) scheduling algorithm and gradient descent-inspired voltage scaling algorithm called Energy Gradient Decent (EGD). We introduced a notion of Energy Gradient (EG) that guides EGD in its search for island voltage settings and minimize the total energy consumption. We conducted the experiments on eight real benchmarks adopted from Embedded Systems Synthesis Benchmarks (E3S). Our static scheduling approach ARSH-FATI outperformed state-of-the-art technique and achieved an average energy-efficiency of ∼24% and ∼30% over CA-TMES-Search and CA-TMES-Quick, respectively. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Cyber Physical System; Embedded systems; Energy efficiency; Energy utilization; Gradient methods; Multiprocessing systems; Quality of service; Scheduling; Scheduling algorithms; Voltage scaling; Cyber-physical systems (CPS); Multiprocessor system on chips; Population-based algorithm; Population-based optimization; State-of-the-art approach; State-of-the-art techniques; Total energy consumption; Voltage-frequency islands; Network-on-chip
Deep reinforcement learning for vehicular edge computing: An intelligent offloading system,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066609014&doi=10.1145%2f3317572&partnerID=40&md5=7c661baa88d43f9c5a9bd54c526722be,"The development of smart vehicles brings drivers and passengers a comfortable and safe environment. Various emerging applications are promising to enrich users' traveling experiences and daily life. However, how to execute computing-intensive applications on resource-constrained vehicles still faces huge challenges. In this article, we construct an intelligent offloading system for vehicular edge computing by leveraging deep reinforcement learning. First, both the communication and computation states are modelled by finite Markov chains. Moreover, the task scheduling and resource allocation strategy is formulated as a joint optimization problem to maximize users' Quality of Experience (QoE). Due to its complexity, the original problem is further divided into two sub-optimization problems. A two-sided matching scheme and a deep reinforcement learning approach are developed to schedule offloading requests and allocate network resources, respectively. Performance evaluations illustrate the effectiveness and superiority of our constructed system. © 2019 Association for Computing Machinery.",,Edge computing; Machine learning; Markov processes; Optimization; Quality of service; Reinforcement learning; Emerging applications; Finite Markov chain; Joint optimization; Offloading system; Quality of experience (QoE); Reinforcement learning approach; Resource allocation strategies; Two sided matching; Deep learning
Detecting causal relationships in simulation models using intervention-based counterfactual analysis,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072678590&doi=10.1145%2f3322123&partnerID=40&md5=179773b89876cad0457c7f79f2a98eb8,"Central to explanatory simulation models is their capability to not just show that but also why particular things happen. Explanation is closely related with the detection of causal relationships and is, in a simulation context, typically done by means of controlled experiments. However, for complex simulation models, conventional “blackbox” experiments may be too coarse-grained to cope with spurious relationships. We present an intervention-based causal analysis methodology that exploits the manipulability of computational models, and detects and circumvents spurious effects. The core of the methodology is a formal model that maps basic causal assumptions to causal observations and allows for the identification of combinations of assumptions that have a negative impact on observability. First, experiments indicate that the methodology can successfully deal with notoriously tricky situations involving asymmetric and symmetric overdetermination and detect fine-grained causal relationships between events in the simulation. As illustrated in the article, the methodology can be easily integrated into an existing simulation environment. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Causal analysis; Counterfactuals; Simulation,Causal analysis; Causal relationships; Complex simulation; Computational model; Controlled experiment; Counterfactuals; Simulation; Simulation environment
An analysis of approaches taken in the ACM Recsys challenge 2018 for automatic music playlist continuation,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072671312&doi=10.1145%2f3344257&partnerID=40&md5=0c254a7220cce2252c979e215c91bd5c,"The ACM Recommender Systems Challenge 2018 focused on the task of automatic music playlist continuation, which is a form of the more general task of sequential recommendation. Given a playlist of arbitrary length with some additional meta-data, the task was to recommend up to 500 tracks that fit the target characteristics of the original playlist. For the RecSys Challenge, Spotify released a dataset of one million user-generated playlists. Participants could compete in two tracks, i.e., main and creative tracks. Participants in the main track were only allowed to use the provided training set, however, in the creative track, the use of external public sources was permitted. In total, 113 teams submitted 1,228 runs to the main track; 33 teams submitted 239 runs to the creative track. The highest performing team in the main track achieved an R-precision of 0.2241, an NDCG of 0.3946, and an average number of recommended songs clicks of 1.784. In the creative track, an R-precision of 0.2233, an NDCG of 0.3939, and a click rate of 1.785 was obtained by the best team. This article provides an overview of the challenge, including motivation, task definition, dataset description, and evaluation. We further report and analyze the results obtained by the top-performing teams in each track and explore the approaches taken by the winners. We finally summarize our key findings, discuss generalizability of approaches and results to domains other than music, and list the open avenues and possible future directions in the area of automatic playlist continuation. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automatic playlist continuation; Benchmark; Challenge; Evaluation; Music recommendation systems; Recommender systems,Benchmarking; Automatic playlist continuation; Average numbers; Challenge; Evaluation; Music Recommendation System; Possible futures; Target characteristic; User-generated; Recommender systems
Measuring conditional independence by independent residuals for causal discovery,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072663422&doi=10.1145%2f3325708&partnerID=40&md5=7bd993ead21ea59589a148dcec38aa3b,"We investigate the relationship between conditional independence (CI) x Џ y|Z and the independence of two residuals x − E(x|Z) Џ y − E(y|Z), where x and y are two random variables and Z is a set of random variables. We show that if x, y, and Z are generated by following linear structural equation models and all external influences follow joint Gaussian distribution, then x Џ y|Z if and only if x − E(x|Z) Џ y − E(y|Z). That is, the test of x Џ y|Z can be relaxed to a simpler unconditional independence test of x − E(x|Z) Џ y − E(y|Z). Furthermore, testing x − E(x|Z) Џ y − E(y|Z) can be simplified by testing x − E(x|Z) Џ y or y − E(y|Z) Џ x. On the other side, if all these external influences follow non-Gaussian distributions and the model satisfies structural faithfulness condition, then we have x Џ y|Z - x − E(x|Z) Џ y − E(y|Z). We apply the results above to the causal discovery problem, where the causal directions are generally determined by a set of V-structures and their consistent propagations, so CI test-based methods can return a set of Markov equivalence classes. We show that in the linear non-Gaussian context, in many cases x − E(x|Z) Џ z or y − E(y|Z) Џ z (∀z ∈ Z and Z is a minimal d-separator) is satisfied when x − E(x|Z) Џ y − E(y|Z), which implies z causes x (or y) if z directly connects to x (or y). Therefore, we conclude that CIs have useful information for distinguishing Markov equivalence classes. In summary, comparing with the existing discretization-based and kernel-based CI testing methods, the proposed method provides a simpler way to measure CI, which needs only one unconditional independence test and two regression operations. When being applied to causal discovery, it can find more causal relationships, which is extensively validated by experiments. © 2019 Association for Computing Machinery.",Causal discovery; Causal inference; Conditional independence test; Independent residual,Gaussian distribution; Gaussian noise (electronic); Random variables; Testing; Causal discovery; Causal inferences; Conditional independence tests; Conditional independences; Independent residual; Joint gaussian distributions; Non-gaussian distribution; Structural equation models; Equivalence classes
Local learning approaches for finding effects of a specified cause and their causal paths,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072676745&doi=10.1145%2f3313147&partnerID=40&md5=b147d48b405bd9222a526f2328c8fa51,"Causal networks are used to describe and to discover causal relationships among variables and data generating mechanisms. There have been many approaches for learning a global causal network of all observed variables. In many applications, we may be interested in finding what are the effects of a specified cause variable and what are the causal paths from the cause variable to its effects. Instead of learning a global causal network, we propose several local learning approaches for finding all effects (or descendants) of the specified cause variable and the causal paths from the cause variable to some effect variable of interest. We discuss the identifiability of the effects and the causal paths from observed data and prior knowledge. For the case that the causal paths are not identifiable, our approaches try to find a path set that contains the causal paths of interest. © 2019 Association for Computing Machinery.",Causal networks; Causal paths; Causes and effects; Structural learning,Causal network; Causal paths; Causal relationships; Causes and effects; Generating mechanism; Identifiability; Prior knowledge; Structural learning
Toward accounting for hidden common causes when inferring cause and effect from observational data,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072219949&doi=10.1145%2f3309720&partnerID=40&md5=f250b1adf82d33a3fb8f4d069249cd9a,"Hidden common causes make it dificult to infer causal relationships from observational data. Here, we begin an investigation into a new method to account for a hidden common cause that infers its presence from the data. As with other approaches that can account for common causes, this approach is successful only in some cases. We describe such a case taken from the field of genomics, wherein one tries to identify which genomic markers causally influence a trait of interest. © 2019 Copyright held by the owner/author(s).",Genomics; Hidden common cause; Linear mixed model,Causal relationships; Cause and effects; Genomics; Hidden common cause; Linear mixed models; Observational data
PlayeRank: Data-driven performance evaluation and player ranking in soccer via a machine learning approach,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072671757&doi=10.1145%2f3343172&partnerID=40&md5=8251791a91f8f03ce463371ee5545be7,"The problem of evaluating the performance of soccer players is attracting the interest of many companies and the scientific community, thanks to the availability of massive data capturing all the events generated during a match (e.g., tackles, passes, shots, etc.). Unfortunately, there is no consolidated and widely accepted metric for measuring performance quality in all of its facets. In this article, we design and implement PlayeRank, a data-driven framework that offers a principled multi-dimensional and role-aware evaluation of the performance of soccer players. We build our framework by deploying a massive dataset of soccer-logs and consisting of millions of match events pertaining to four seasons of 18 prominent soccer competitions. By comparing PlayeRank to known algorithms for performance evaluation in soccer, and by exploiting a dataset of players' evaluations made by professional soccer scouts, we show that PlayeRank significantly outperforms the competitors. We also explore the ratings produced by PlayeRank and discover interesting patterns about the nature of excellent performances and what distinguishes the top players from the others. At the end, we explore some applications of PlayeRank-i.e. searching players and player versatility-showing its flexibility and efficiency, which makes it worth to be used in the design of a scalable platform for soccer analytics. © 2019 Copyright held by the owner/author(s).",Big data; Clustering; Data science; Football analytics; Multi-dimensional analysis; Predictive modelling; Ranking; Searching; Soccer analytics; Sports analytics,Big data; Data Science; Machine learning; Predictive analytics; Clustering; Football analytics; Multi-dimensional analysis; Predictive modelling; Ranking; Searching; Soccer analytics; Sports
Distributed deep forest and its application to automatic detection of cash-out fraud,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072156334&doi=10.1145%2f3342241&partnerID=40&md5=07a44a8045345684c76e422094527fb3,"Internet companies are facing the need for handling large-scale machine learning applications on a daily basis and distributed implementation of machine learning algorithms which can handle extra-large-scale tasks with great performance is widely needed. Deep forest is a recently proposed deep learning framework which uses tree ensembles as its building blocks and it has achieved highly competitive results on various domains of tasks. However, it has not been tested on extremely large-scale tasks. In this work, based on our parameter server system, we developed the distributed version of deep forest. To meet the need for real-world tasks, many improvements are introduced to the original deep forest model, including MART (Multiple Additive Regression Tree) as base learners for eficiency and effectiveness consideration, the cost-based method for handling prevalent class-imbalanced data, MART based feature selection for high dimension data, and different evaluation metrics for automatically determining the cascade level. We tested the deep forest model on an extra-large-scale task, i.e., automatic detection of cash-out fraud, with more than 100 million training samples. Experimental results showed that the deep forest model has the best performance according to the evaluation metrics from different perspectives even with very little effort for parameter tuning. This model can block fraud transactions in a large amount of money each day. Even compared with the best-deployed model, the deep forest model can additionally bring a significant decrease in economic loss each day. © 2019 Association for Computing Machinery.",Deep forest; Large-scale machine learning; Parameter server,Crime; Forestry; Learning algorithms; Losses; Machine learning; Additive regression; Automatic Detection; Deep forest; Distributed implementation; Evaluation metrics; High-dimension data; Large-scale machine learning; Learning frameworks; Deep learning
RecRules: Recommending IF-THEN rules for end-user development,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072159824&doi=10.1145%2f3344211&partnerID=40&md5=a565affa1e1c6c858e364fc8617df7a5,"Nowadays, end users can personalize their smart devices and web applications by defining or reusing IF-THEN rules through dedicated End-User Development (EUD) tools. Despite apparent simplicity, such tools present their own set of issues. The emerging and increasing complexity of the Internet of Things, for example, is barely taken into account, and the number of possible combinations between triggers and actions of different smart devices and web applications is continuously growing. Such a large design space makes enduser personalization a complex task for non-programmers, and motivates the need of assisting users in easily discovering and managing rules and functionality, e.g., through recommendation techniques. In this article, we tackle the emerging problem of recommending IF-THEN rules to end users by presenting RecRules, a hybrid and semantic recommendation system. Through a mixed content and collaborative approach, the goal of RecRules is to recommend by functionality: it suggests rules based on their final purposes, thus overcoming details like manufacturers and brands. The algorithm uses a semantic reasoning process to enrich rules with semantic information, with the aim of uncovering hidden connections between rules in terms of shared functionality. Then, it builds a collaborative semantic graph, and it exploits different types of path-based features to train a learning to rank algorithm and compute top-N recommendations. We evaluate RecRules through different experiments on real user data extracted from IFTTT, one of the most popular EUD tools. Results are promising: they show the efectiveness of our approach with respect to other state-of-the-art algorithms and open the way for a new class of recommender systems for EUD that take into account the actual functionality needed by end users. © 2019 Association for Computing Machinery.",End-user development; Hybrid recommender system; Internet of Things; Top-N recommendations; Trigger-action programming,Internet of things; Semantics; Collaborative approach; End user development; End user development(EUD); Hybrid recommender systems; Recommendation techniques; Semantic recommendations; State-of-the-art algorithms; Top-N recommendations; Recommender systems
Stable specification search in structural equation models with latent variables,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072227037&doi=10.1145%2f3341557&partnerID=40&md5=3b0cc7ca364f8ea444be0b7d75ee5501,"In our previous study, we introduced stable specification search for cross-sectional data (S3C). It is an exploratory causal method that combines the concept of stability selection and multi-objective optimization to search for stable and parsimonious causal structures across the entire range of model complexities. S3C, however, is designed to model causal relations among observed variables. In this study, we extended S3C to S3C-Latent, to model linear causal relations between latent variables that are measured through observed proxies. We evaluated S3C-Latent on simulated data and compared the results to those of PC-MIMBuild, an extension of the PC algorithm, the state-of-the-art causal discovery method. The comparison shows that S3C-Latent achieved better performance. We also applied S3C-Latent to real-world data of children with attention deficit/hyperactivity disorder and data about measuring mental abilities among pupils. The results are consistent with those of previous studies. © 2019 Copyright held by the owner/author(s).",Causal modeling; Multi-objective evolutionary algorithm; Speci-fication search; Stability selection; Structural equation model with latent variables,Genetic algorithms; Multiobjective optimization; Specifications; Telecommunication services; Causal model; Latent variable; Multi objective evolutionary algorithms; Speci-fication search; Stability selections; Behavioral research
Take a look around: Using street view and satellite images to estimate house prices,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072196490&doi=10.1145%2f3342240&partnerID=40&md5=15cd053f39e90aff56b8ae4980a0e7fb,"When an individual purchases a home, they simultaneously purchase its structural features, its accessibility to work, and the neighborhood amenities. Some amenities, such as air quality, are measurable while others, such as the prestige or the visual impression of a neighborhood, are difficult to quantify. Despite the well-known impacts intangible housing features have on house prices, limited attention has been given to systematically quantifying these difficult to measure amenities. Two issues have led to this neglect. Not only do few quantitative methods exist that can measure the urban environment, but that the collection of such data is both costly and subjective. We show that street image and satellite image data can capture these urban qualities and improve the estimation of house prices. We propose a pipeline that uses a deep neural network model to automatically extract visual features from images to estimate house prices in London, UK. We make use of traditional housing features such as age, size, and accessibility as well as visual features from Google Street View images and Bing aerial images in estimating the house price model. We find encouraging results where learning to characterize the urban quality of a neighborhood improves house price prediction, even when generalizing to previously unseen London boroughs. We explore the use of non-linear vs. linear methods to fuse these cues with conventional models of house pricing, and show how the interpretability of linear models allows us to directly extract proxy variables for visual desirability of neighborhoods that are both of interest in their own right, and could be used as inputs to other econometric methods. This is particularly valuable as once the network has been trained with the training data, it can be applied elsewhere, allowing us to generate vivid dense maps of the visual appeal of London streets. © 2019 Association for Computing Machinery.",Computer vision; Convolutional neural network; Deep learning; Hedonic price models; London; Real estate,Air quality; Antennas; Computer vision; Costs; Deep learning; Deep neural networks; Houses; Neural networks; Conventional models; Convolutional neural network; Hedonic price model; London; Neural network model; Real estate; Satellite image datas; Traditional housing; Image enhancement
Correlated multi-label classification with incomplete label space and class imbalance,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072161840&doi=10.1145%2f3342512&partnerID=40&md5=e302c6e0258d9174d8793b7d3b668792,"Multi-label classification is defined as the problem of identifying the multiple labels or categories of new observations based on labeled training data. Multi-labeled data has several challenges, including class imbalance, label correlation, incomplete multi-label matrices, and noisy and irrelevant features. In this article, we propose an integrated multi-label classification approach with incomplete label space and class imbalance (ML-CIB) for simultaneously training the multi-label classification model and addressing the aforementioned challenges. The model learns a new label matrix and captures new label correlations, because it is dificult to find a complete label vector for each instance in real-world data. We also propose a label regularization to handle the imbalanced multi-labeled issue in the new label, and l1 regularization norm is incorporated in the objective function to select the relevant sparse features. A multi-label feature selection (ML-CIB-FS) method is presented as a variant of the proposed ML-CIB to show the eficacy of the proposed method in selecting the relevant features. ML-CIB is formulated as a constrained objective function. We use the accelerated proximal gradient method to solve the proposed optimisation problem. Last, extensive experiments are conducted on 19 regular-scale and large-scale imbalanced multi-labeled datasets. The promising results show that our method significantly outperforms the state-of-the-art. © 2019 Association for Computing Machinery.",Class imbalance; Label correlation; Multi-label classification; Multi-label feature selection,Feature extraction; Gradient methods; Large dataset; Matrix algebra; Accelerated proximal gradient methods; Class imbalance; Label correlations; Labeled training data; Multi label classification; Multi-label; Objective functions; Optimisation problems; Classification (of information)
DiffQue: Estimating Relative Difficulty ofQuestions in Community Question Answering Services,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075566107&doi=10.1145%2f3337799&partnerID=40&md5=788c9e29ecadd2ffbe2861291d0a022b,"Automatic estimation of relative difficulty of a pair of questions is an important and challenging problem in community question answering (CQA) services. There are limited studies that addressed this problem. Past studies mostly leveraged expertise of users answering the questions and barely considered other properties of CQA services such as metadata of users and posts, temporal information, and textual content. In this article, we propose DiffQue, a novel system that maps this problem to a network-aided edge directionality prediction problem. DiffQue starts by constructing a novel network structure that captures different notions of difficulties among a pair of questions. It then measures the relative difficulty of two questions by predicting the direction of a (virtual) edge connecting these two questions in the network. It leverages features extracted from the network structure, metadata of users/posts, and textual description of questions and answers. Experiments on datasets obtained from two CQA sites (further divided into four datasets) with human annotated ground-truth show that DiffQue outperforms four state-of-the-art methods by a significant margin (28.77% higher F1 score and 28.72% higher AUC than the best baseline). As opposed to the other baselines, (i) DiffQue appropriately responds to the training noise, (ii) DiffQue is capable of adapting multiple domains (CQA datasets), and (iii) DiffQue can efficiently handle the cold start problem that may arise due to the lack of information for newly posted questions or newly arrived users. ©2019 Association for Computing Machinery.",Community question answering; Difficulty of questions; Edge directionality prediction; Time-evolving networks,Metadata; Automatic estimation; Cold start problems; Community question answering; Difficulty of questions; Evolving networks; State-of-the-art methods; Temporal information; Textual description; Forecasting
Multi-modal curriculum learning over graphs,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075572641&doi=10.1145%2f3322122&partnerID=40&md5=8fb3d2ab9a61190cc91a61581d154c1a,"Curriculum Learning (CL) is a recently proposed learning paradigm that aims to achieve satisfactory performance by properly organizing the learning sequence from simple curriculum examples to more difficult ones. Up to now, few works have been done to explore CL for the data with graph structure. Therefore, this article proposes a novel CL algorithm that can be utilized to guide the Label Propagation (LP) over graphs, of which the target is to learn the labels of unlabeled examples on the graphs. Specifically, we assume that different unlabeled examples have different levels of difficulty for propagation, and their label learning should follow a simple-to-difficult sequence with the updated curricula. Furthermore, considering that the practical data are often characterized by multiple modalities, every modality in our method is associated with a teacher that not only evaluates the difficulties of examples from its own viewpoint, but also cooperates with other teachers to generate the overall simplest curriculum examples for propagation. By taking the curriculums suggested by the teachers as a whole, the common preference (i.e., commonality) of teachers on selecting the simplest examples can be discovered by a row-sparse matrix, and their distinct opinions (i.e., individuality) are captured by a sparse noise matrix. As a result, an accurate curriculum sequence can be established and the propagation quality can thus be improved. Theoretically, we prove that the propagation risk bound is closely related to the examples difficulty information, and empirically, we show that our method can generate higher accuracy than the state-of-the-art CL approach and LP algorithms on various multi-modal tasks. ©2019 Association for Computing Machinery.",Curriculum learning; Label propagation; Multi-modal learning; semi-supervised learning,Graphic methods; Matrix algebra; Supervised learning; Curriculum learning; Label propagation; Learning paradigms; Learning sequences; Multi-modal; Multi-modal learning; Performance; Semi-supervised learning; Simple++; Teachers'; Curricula
Spatial ensemble learning for heterogeneous geographic data with class ambiguity,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075691447&doi=10.1145%2f3337798&partnerID=40&md5=7f613efcf93a1d902016043af4438a3b,"Class ambiguity refers to the phenomenonwhereby similar features correspond to different classes at different locations. Given heterogeneous geographic data with class ambiguity, the spatial ensemble learning (SEL) problem aims to find a decomposition of the geographic area into disjoint zones such that class ambiguity is minimized and a local classifier can be learned in each zone. The problem is important for applications such as land cover mapping from heterogeneous earth observation data with spectral confusion. However, the problem is challenging due to its high computational cost. Related work in ensemble learning either assumes an identical sample distribution (e.g., bagging, boosting, random forest) or decomposes multi-modular input data in the feature vector space (e.g., mixture of experts, multimodal ensemble) and thus cannot effectively minimize class ambiguity. In contrast, we propose a spatial ensemble framework that explicitly partitions input data in geographic space. Our approach first preprocesses data into homogeneous spatial patches and uses a greedy heuristic to allocate pairs of patches with high class ambiguity into different zones.We further extend our spatial ensemble learning framework with spatial dependency between nearby zones based on the spatial autocorrelation effect. Both theoretical analysis and experimental evaluations on two real world wetland mapping datasets show the feasibility of the proposed approach. © 2019 Association for Computing Machinery.",Class ambiguity; Local models; Spatial classification; Spatial ensemble; Spatial heterogeneity,Decision trees; Input output programs; Photomapping; Class ambiguity; Local model; Spatial classification; Spatial ensemble; Spatial heterogeneity; Vector spaces
Spatio-temporal adaptive pricing for balancing mobility-on-demand networks,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075551577&doi=10.1145%2f3331450&partnerID=40&md5=fcc40ea660c706ab77124488b2e97b30,"Pricing in mobility-on-demand (MOD) networks, such as Uber, Lyft, and connected taxicabs, is done adaptively by leveraging the price responsiveness of drivers (supplies) and passengers (demands) to achieve such goals as maximizing drivers incomes, improving riders experience, and sustaining platform operation. Existing pricing policies only respond to short-term demand fluctuations without accurate trip forecast and spatial demand-supply balancing, thus mismatching drivers to riders and resulting in loss of profit. We propose CAPrice, a novel adaptive pricing scheme for urban MOD networks. It uses a new spatiotemporal deep capsule network (STCapsNet) that accurately predicts ride demands and driver supplies with vectorized neuron capsules while accounting for comprehensive spatio-temporal and external factors. Given accurate perception of zone-to-zone traffic flows in a city, CAPrice formulates a joint optimization problem by considering spatial equilibrium to balance the platform, providing drivers and riders/passengers with proactive pricing signals. We have conducted an extensive experimental evaluation upon over 4.0 108 MOD trips (Uber, Didi Chuxing, and connected taxicabs) in New York City, Beijing, and Chengdu, validating the accuracy, effectiveness, and profitability (often 20% ride prediction accuracy and 30% profit improvements over the state-of-the-arts) of CAPrice in managing urban MOD networks. © 2019 Association for Computing Machinery.",Adaptive pricing; Deep learning; flow balancing; Mobility-on-demand; Ride sharing; sharing economy; smart transportation; Traffic prediction,Deep learning; Forecasting; Profitability; Taxicabs; Traffic control; Adaptive pricing; Flow balancing; On demands; Ride-sharing; sharing economy; Traffic prediction; Costs
Large-scale frequent episode mining from complex event sequences with hierarchies,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074916830&doi=10.1145%2f3326163&partnerID=40&md5=1d76d4a63b36a48ab2f423e91c767ab4,"Frequent Episode Mining (FEM), which aims at mining frequent sub-sequences from a single long event sequence, is one of the essential building blocks for the sequence mining research field. Existing studies about FEM suffer from unsatisfied scalability when faced with complex sequences as it is an NP-complete problem for testing whether an episode occurs in a sequence. In this article, we propose a scalable, distributed framework to support FEM on ""big"" event sequences. As a rule of thumb, ""big"" illustrates an event sequence is either very long or with masses of simultaneous events. Meanwhile, the events in this article are arranged in a predefined hierarchy. It derives some abstractive events that can form episodes that may not directly appear in the input sequence. Specifically, we devise an event-centered and hierarchy-aware partitioning strategy to allocate events from different levels of the hierarchy into local processes. We then present an efficient special-purpose algorithm to improve the local mining performance.We also extend our framework to support maximal and closed episode mining in the context of event hierarchy, and to the best of our knowledge, we are the first attempt to define and discover hierarchy-aware maximal and closed episodes. We implement the proposed framework on Apache Spark and conduct experiments on both synthetic and real-world datasets. Experimental results demonstrate the efficiency and scalability of the proposed approach and show that we can find practical patterns when taking event hierarchies into account. © 2019 Association for Computing Machinery.",Frequent episode mining; Hierarchy-aware maximal/closed episode; Large-scale sequence mining; Peak episode miner,Computational complexity; Real time systems; Scalability; Building blockes; Complex sequences; Distributed framework; Frequent episode minings; Hierarchy-aware maximal/closed episode; Large-scale sequences; Partitioning strategies; Real-world datasets; Data mining
Short text analysis based on dual semantic extension and deep hashing in microblog,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075668178&doi=10.1145%2f3326166&partnerID=40&md5=a17846b48f27e3aa4848f9c2213cfdd9,"Short text analysis is a challenging task as far as the sparsity and limitation of semantics. The semantic extension approach learns the meaning of a short text by introducing external knowledge. However, for the randomness of short text descriptions in microblogs, traditional extension methods cannot accurately mine the semantics suitable for the microblog theme. Therefore, we use the prominent and refined hashtag information in microblogs as well as complex social relationships to provide implicit guidance for semantic extension of short text. Specifically, we design a deep hash model based on social and conceptual semantic extension, which consists of dual semantic extension and deep hashing representation. In the extension method, the short text is first conceptualized to achieve the construction of hashtag graph under conceptual space. Then, the associated hashtags are generated by correlation calculation based on the integration of social relationships and concepts to extend the short text. In the deep hash model, we use the semantic hashing model to encode the abundant semantic features and form a compact and meaningful binary encoding. Finally, extensive experiments demonstrate that our method can learn and represent the short texts well by using more meaningful semantic signal. It can effectively enhance and guide the semantic analysis and understanding of short text in microblogs. © 2019 Association for Computing Machinery.",Conceptual space; Deep hash model; Hashtag graph; Semantic extension; Social and conceptual semantics,Encoding (symbols); Social aspects; Conceptual semantics; Conceptual spaces; Extension methods; External knowledge; Hashtag graph; Semantic analysis; Semantic extensions; Social relationships; Semantics
ALERA: Accelerated reinforcement learning driven adaptation to electro-mechanical degradation in nonlinear control systems using encoded state space error signatures,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075699734&doi=10.1145%2f3338123&partnerID=40&md5=8a34b274ddd88631fe380cda470abec9,"The successful deployment of autonomous real-time systems is contingent on their ability to recover from performance degradation of sensors, actuators, and other electro-mechanical subsystems with low latency. In this article, we introduce ALERA, a novel framework for real-time control law adaptation in nonlinear control systems assisted by system state encodings that generate an error signal when the code properties are violated in the presence of failures. The fundamental contributions of this methodology are twofold-first, we show that the time-domain error signal contains perturbed system parameters' diagnostic information that can be used for quick control law adaptation to failure conditions and second, this quick adaptation is performed via reinforcement learning algorithms that relearn the control law of the perturbed system from a starting condition dictated by the diagnostic information, thus achieving significantly faster recovery. The fast (up to 80X faster than traditional reinforcement learning paradigms) performance recovery enabled by ALERA is demonstrated on an inverted pendulum balancing problem, a brake-by-wire system, and a self-balancing robot. © 2019 Association for Computing Machinery.",Control systems; Dependability; Real-time systems; Reinforcement learning,Balancing; Computer control systems; Control systems; Control theory; Errors; Interactive computer systems; Learning algorithms; Machine learning; Mechanical actuators; Nonlinear control systems; Real time control; Real time systems; Recovery; Balancing problems; Brake-by-wire systems; Dependability; Electro-mechanical; Performance degradation; Performance recovery; Self-balancing robot; Traditional reinforcements; Reinforcement learning
Trajectory data classification: a review,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075567719&doi=10.1145%2f3330138&partnerID=40&md5=5565dbd36dd0a1211e41f3aa7691c0cd,"This article comprehensively surveys the development of trajectory data classification. Considering the critical role of trajectory data classification in modern intelligent systems for surveillance security, abnormal behavior detection, crowd behavior analysis, and traffic control, trajectory data classification has attracted growing attention. According to the availability of manual labels, which is critical to the classification performances, the methods can be classified into three categories, i.e., unsupervised, semi-supervised, and supervised. Furthermore, classification methods are divided into some sub-categories according to what extracted features are used. We provide a holistic understanding and deep insight into three types of trajectory data classification methods and present some promising future directions ©2019 Association for Computing Machinery.",Classification algorithms; Object movement; Review; Trajectory classification,Behavioral research; Intelligent systems; Network security; Reviews; Traffic control; Trajectories; Abnormal behavior detections; Classification algorithm; Classification methods; Classification performance; Crowd behavior analysis; Object movements; Surveillance securities; Trajectory classification; Classification (of information)
Privacy-aware tag recommendation for accurate image privacy prediction,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075575611&doi=10.1145%2f3335054&partnerID=40&md5=7a772d8cdaec5deb302140e785abf32c,"Online images tags are very important for indexing, sharing, and searching of images, aswell as surfacing images with private or sensitive content, which needs to be protected. Social media sites such as Flickr generate these metadata from user-contributed tags. However, as the tags are at the sole discretion of users, these tags tend to be noisy and incomplete. In this article, we present a privacy-aware approach to automatic image tagging, which aims at improving the quality of user annotations, while also preserving the images original privacy sharing patterns. Precisely,we recommend potential tags for each target image by mining privacy-aware tags from the most similar images of the target image, which are obtained from a large collection. Experimental results show that, although the user-input tags compose noise, our privacy-aware approach is able to predict accurate tags that can improve the performance of a downstream application on image privacy prediction and outperforms an existing privacy-oblivious approach to image tagging. The results also showthat, even for images that do not have any user tags, our proposed approach can recommend accurate tags. Crowd-sourcing the predicted tags exhibits the quality of our privacy-aware recommended tags. Our code, features, and the dataset used in experiments are available at: https://github.com/ashwinitonge/privacy-aware-tag-rec.git. ©2019 Association for Computing Machinery.",Deep learning; Image analysis; Image privacy prediction; Privacy-aware tags; Social networks; Tag recommendation,Automatic indexing; Deep learning; Forecasting; Image analysis; Image annotation; Social networking (online); User interfaces; Downstream applications; Image tagging; Online images; Privacy aware; Similar image; Tag recommendations; Target images; User annotations; Image enhancement
Survey and cross-benchmark comparison of remaining time prediction methods in business process monitoring,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069475780&doi=10.1145%2f3331449&partnerID=40&md5=7b9d99b9bcdd7a2bbfd6e32113b2ccd5,"Predictive business process monitoring methods exploit historical process execution logs to generate predictions about running instances (called cases) of a business process, such as the prediction of the outcome, next activity, or remaining cycle time of a given process case. These insights could be used to support operational managers in taking remedial actions as business processes unfold, e.g., shifting resources from one case onto another to ensure the latter is completed on time. A number of methods to tackle the remaining cycle time prediction problem have been proposed in the literature. However, due to differences in their experimental setup, choice of datasets, evaluation measures, and baselines, the relative merits of each method remain unclear. This article presents a systematic literature review and taxonomy of methods for remaining time prediction in the context of business processes, as well as a cross-benchmark comparison of 16 such methods based on 17 real-life datasets originating from different industry domains. © 2019 Association for Computing Machinery.",Business process; Machine learning; Predictive monitoring; Process mining; Process performance indicator,Learning systems; Process control; Process monitoring; Benchmark comparison; Business Process; Business process monitoring; Predictive monitoring; Process mining; Process performance indicators; Systematic literature review; Time prediction method; Forecasting
Efficient user guidance for validating participatory sensing data,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069519115&doi=10.1145%2f3326164&partnerID=40&md5=0c83c47fadca124698281354302eb914,"Participatory sensing has become a new data collection paradigm that leverages the wisdom of the crowd for big data applications without spending cost to buy dedicated sensors. It collects data from human sensors by using their own devices such as cell phone accelerometers, cameras, and GPS devices. This benefit comes with a drawback: human sensors are arbitrary and inherently uncertain due to the lack of quality guarantee. Moreover, participatory sensing data are time series that exhibit not only highly irregular dependencies on time but also high variance between sensors. To overcome these limitations, we formulate the problem of validating uncertain time series collected by participatory sensors. In this article, we approach the problem by an iterative validation process on top of a probabilistic time series model. First, we generate a series of probability distributions from raw data by tailoring a state-of-the-art dynamical model, namely Generalised Auto Regressive Conditional Heteroskedasticity (GARCH), for our joint time series setting. Second, we design a feedback process that consists of an adaptive aggregation model to unify the joint probabilistic time series and an efficient user guidance model to validate aggregated data with minimal effort. Through extensive experimentation, we demonstrate the efficiency and effectiveness of our approach on both real data and synthetic data. Highlights from our experiences include the fast running time of a probabilistic model, the robustness of an aggregation model to outliers, and the significant effort saving of a guidance model. © 2019 Association for Computing Machinery.",Participatory sensing; Probabilistic database; Trust management,Human computer interaction; Time series; Big data applications; Conditional heteroskedasticity; Participatory Sensing; Probabilistic database; Probabilistic modeling; Time series modeling; Trust management; Wisdom of the crowds; Probability distributions
Personalized Reason Generation for Explainable Song Recommendation,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069505877&doi=10.1145%2f3337967&partnerID=40&md5=08319f7d1a615a2e7ebad27e9bcc6ada,"Personalized recommendation has received a lot of attention as a highly practical research topic. However, existing recommender systems provide the recommendations with a generic statement such as ""Customers who bought this item also bought . . . "". Explainable recommendation, which makes a user aware of why such items are recommended, is in demand. The goal of our research is to make the users feel as if they are receiving recommendations from their friends. To this end, we formulate a new challenging problem called personalized reason generation for explainable recommendation for songs in conversation applications and propose a solution that generates a natural language explanation of the reason for recommending a song to that particular user. For example, if the user is a student, our method can generate an output such as ""Campus radio plays this song at noon every day, and I think it sounds wonderful,"" which the student may find easy to relate to. In the offline experiments, through manual assessments, the gain of our method is statistically significant on the relevance to songs and personalization to users comparing with baselines. Large-scale online experiments show that our method outperforms manually selected reasons by 8.2% in terms of click-Through rate. Evaluation results indicate that our generated reasons are relevant to songs and personalized to users, and they attract users to click the recommendations. © 2019 Association for Computing Machinery.",Conversational recommendation; explainable recommendation; natural language generation; personalization; recommender system,Recommender systems; Click-through rate; Conversational recommendations; explainable recommendation; Natural language explanations; Natural language generation; On-line experiments; Personalizations; Personalized recommendation; Natural language processing systems
Recognizing multi-agent plans when action models and team plans are both incomplete,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066970737&doi=10.1145%2f3319403&partnerID=40&md5=37d5b7cb5170a2a52b3e76adb15c28e9,"Multi-Agent Plan Recognition (MAPR) aims to recognize team structures (which are composed of team plans) from the observed team traces (action sequences) of a set of intelligent agents. In this article, we introduce the problem formulation of MAPR based on partially observed team traces, and present a weighted MAX-SAT–based framework to recognize multi-agent plans from partially observed team traces with the help of two types of auxiliary knowledge to help recognize multi-agent plans, i.e., a library of incomplete team plans and a set of incomplete action models. Our framework functions with two phases. We first build a set of hard constraints that encode the correctness property of the team plans, and a set of soft constraints that encode the optimal utility property of team plans based on the input team trace, incomplete team plans, and incomplete action models. After that, we solve all of the constraints using a weighted MAX-SAT solver and convert the solution to a set of team plans that best explain the structure of the observed team trace. We empirically exhibit both effectiveness and efficiency of our framework in benchmark domains from International Planning Competition (IPC). © 2019 Association for Computing Machinery.",Action model; MAX-SAT; Multi-agent plan recognition; Team plan,Encoding (symbols); Intelligent agents; Action modeling; Correctness properties; Effectiveness and efficiencies; International Planning Competitions; Max-SAT; Multiagent plans; Problem formulation; Team plan; Multi agent systems
Motion-aware compression and transmission of mesh animation sequences,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065770502&doi=10.1145%2f3300198&partnerID=40&md5=b744304d421bc8b19aaf593bb9466e05,"With the increasing demand in using 3D mesh data over networks, supporting effective compression and efficient transmission of meshes has caught lots of attention in recent years. This article introduces a novel compression method for 3D mesh animation sequences, supporting user-defined and progressive transmissions over networks. Our motion-aware approach starts with clustering animation frames based on their motion similarities, dividing a mesh animation sequence into fragments of varying lengths. This is done by a novel temporal clustering algorithm, which measures motion similarity based on the curvature and torsion of a space curve formed by corresponding vertices along a series of animation frames. We further segment each cluster based on mesh vertex coherence, representing topological proximity within an object under certain motion. To produce a compact representation, we perform intra-cluster compression based on Graph Fourier Transform (GFT) and Set Partitioning In Hierarchical Trees (SPIHT) coding. Optimized compression results can be achieved by applying GFT due to the proximity in vertex position and motion. We adapt SPIHT to support progressive transmission and design a mechanism to transmit mesh animation sequences with user-defined quality. Experimental results show that our method can obtain a high compression ratio while maintaining a low reconstruction error. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D mesh animation; Clustering; Compression; Graph fourier transform; Progressive transmission,Animation; Clustering algorithms; Compaction; Mesh generation; Trees (mathematics); 3D mesh animation; Clustering; Compact representation; Corresponding vertices; Graph Fourier transforms; High compression ratio; Progressive transmission; Set partitioning in hierarchical trees coding; MESH networking
Predicting academic performance for college students: A campus behavior perspective,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065780978&doi=10.1145%2f3299087&partnerID=40&md5=b4fd8c6cdbe6ded5ceeffaf67af2ce1a,"Detecting abnormal behaviors of students in time and providing personalized intervention and guidance at the early stage is important in educational management. Academic performance prediction is an important building block to enabling this pre-intervention and guidance. Most of the previous studies are based on questionnaire surveys and self-reports, which suffer from small sample size and social desirability bias. In this article, we collect longitudinal behavioral data from the smart cards of 6,597 students and propose three major types of discriminative behavioral factors, diligence, orderliness, and sleep patterns. Empirical analysis demonstrates these behavioral factors are strongly correlated with academic performance. Furthermore, motivated by the social influence theory, we analyze the correlation between each student's academic performance with his/her behaviorally similar students'. Statistical tests indicate this correlation is significant. Based on these factors, we further build a multi-task predictive framework based on a learning-to-rank algorithm for academic performance prediction. This framework captures inter-semester correlation, inter-major correlation, and integrates student similarity to predict students' academic performance. The experiments on a large-scale real-world dataset show the effectiveness of our methods for predicting academic performance and the effectiveness of proposed behavioral factors. © 2019 Association for Computing Machinery.",Academic performance prediction; Campus behavior; Student personality,Forecasting; Large dataset; Smart cards; Surveys; Academic performance; Campus behavior; Educational management; Predicting academic performance; Questionnaire surveys; Social desirability; Social influence theory; Student personalities; Students
Exploiting the value of the center-dark channel prior for salient object detection,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065762430&doi=10.1145%2f3319368&partnerID=40&md5=c1776b433939bdcb5cd8279e56196a41,"Saliency detection aims to detect the most attractive objects in images and is widely used as a foundation for various applications. In this article, we propose a novel salient object detection algorithm for RGB-D images using center-dark channel priors. First, we generate an initial saliency map based on a color saliency map and a depth saliency map of a given RGB-D image. Then, we generate a center-dark channel map based on center saliency and dark channel priors. Finally, we fuse the initial saliency map with the center dark channel map to generate the final saliency map. Extensive evaluations over four benchmark datasets demonstrate that our proposed method performs favorably against most of the state-of-the-art approaches. Besides, we further discuss the application of the proposed algorithm in small target detection and demonstrate the universal value of center-dark channel priors in the field of object detection. © 2019 Association for Computing Machinery.",Center-dark channel prior; Salient object detection,Object recognition; Benchmark datasets; Channel map; Dark channel priors; Saliency detection; Saliency map; Salient object detection; Small target detection; State-of-the-art approach; Object detection
Online heterogeneous transfer learning by knowledge transition,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067034318&doi=10.1145%2f3309537&partnerID=40&md5=8de1552e98e5250b1ef704e4c5e7d25a,"In this article, we study the problem of online heterogeneous transfer learning, where the objective is to make predictions for a target data sequence arriving in an online fashion, and some offline labeled instances from a heterogeneous source domain are provided as auxiliary data. The feature spaces of the source and target domains are completely different, thus the source data cannot be used directly to assist the learning task in the target domain. To address this issue, we take advantage of unlabeled co-occurrence instances as intermediate supplementary data to connect the source and target domains, and perform knowledge transition from the source domain into the target domain. We propose a novel online heterogeneous transfer learning algorithm called Online Heterogeneous Knowledge Transition (OHKT) for this purpose. In OHKT, we first seek to generate pseudo labels for the co-occurrence data based on the labeled source data, and then develop an online learning algorithm to classify the target sequence by leveraging the co-occurrence data with pseudo labels. Experimental results on real-world data sets demonstrate the effectiveness and efficiency of the proposed algorithm. © 2019 Association for Computing Machinery.",Co-occurrence data; Heterogeneous transfer learning; Online learning; Transitive transfer learning,E-learning; Electronic document exchange; Co-occurrence; Effectiveness and efficiencies; Heterogeneous Knowledge; Heterogeneous sources; Online learning; Online learning algorithms; Supplementary data; Transfer learning; Learning algorithms
CNNs based viewpoint estimation for volume visualization,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064711139&doi=10.1145%2f3309993&partnerID=40&md5=24bf7138ceb108061f1318b4d19ef7e2,"Viewpoint estimation from 2D rendered images is helpful in understanding how users select viewpoints for volume visualization and guiding users to select better viewpoints based on previous visualizations. In this article, we propose a viewpoint estimation method based on Convolutional Neural Networks (CNNs) for volume visualization. We first design an overfit-resistant image rendering pipeline to generate the training images with accurate viewpoint annotations, and then train a category-specific viewpoint classification network to estimate the viewpoint for the given rendered image. Our method can achieve good performance on images rendered with different transfer functions and rendering parameters in several categories. We apply our model to recover the viewpoints of the rendered images in publications, and show how experts look at volumes. We also introduce a CNN feature-based image similarity measure for similarity voting based viewpoint selection, which can suggest semantically meaningful optimal viewpoints for different volumes and transfer functions. © 2019 Association for Computing Machinery.",Convolutional neural networks; Viewpoint estimation; Volume visualization,Convolution; Neural networks; Transfer functions; Visualization; Category specifics; Classification networks; Convolutional neural network; Estimation methods; Image similarity; Rendered images; Viewpoint selection; Volume visualization; Rendering (computer graphics)
Using social dependence to enable neighbourly behaviour in open multi-agent systems,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065783478&doi=10.1145%2f3319402&partnerID=40&md5=05bf00b68e464fabd4f6b39604b1f1d3,"Agents frequently collaborate to achieve a shared goal or to accomplish a task that they cannot do alone. However, collaboration is difficult in open multi-agent systems where agents share constrained resources to achieve both individual and shared goals. In current approaches to collaboration, agents are organised into disjoint groups and social reasoning is used to capture their capabilities when selecting a qualified set of collaborators. These approaches are not useful when agents are in multiple, overlapping groups; depend on each other when using shared resources; have multiple goals to achieve simultaneously; and have to share the overall costs and benefits. In this article, agents use social reasoning to enhance their understanding of other agents’ goals and their dependencies, and self-adaptive techniques to adapt their level of self-interest in a collaborative process, with a view to contributing to lowering shared costs or increasing shared benefits. This model aims at improving the extent to which agents’ goals are met while improving shared resource usage efficiency. For example, in a public transport system where each mode of transport has limited capacity, commuters will be enabled to make choices that avoid over-capacity in different modes, or in a smart energy grid with limited capacity, users can make choices as to when they increase their demand. The model simultaneously helps avoid overloading a shared resource while allowing users to achieve their own goals. The proposed model is evaluated in an open multi-agent system with 100 agents operating in multiple overlapping groups and sharing multiple constrained resources. The impact of agents’ varying levels of social dependencies, mobility, and their groups’ density on their individual and shared goal achievement is analysed. © 2019 Association for Computing Machinery.",Multi-agent collaboration; Multi-agent systems; Social circles; Social reasoning,Collaborative process; Constrained resources; Multi agent; Open multi-agent system; Public transport systems; Self-adaptive techniques; Social circles; Social reasoning; Multi agent systems
Co-saliency detection with graph matching,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064614436&doi=10.1145%2f3313874&partnerID=40&md5=4edccf43a94d711b9bd12dd3e3e4d644,"Recently, co-saliency detection, which aims to automatically discover common and salient objects appeared in several relevant images, has attracted increased interest in the computer vision community. In this article, we present a novel graph-matching based model for co-saliency detection in image pairs. A solution of graph matching is proposed to integrate the visual appearance, saliency coherence, and spatial structural continuity for detecting co-saliency collaboratively. Since the saliency and the visual similarity have been seamlessly integrated, such a joint inference schema is able to produce more accurate and reliable results. More concretely, the proposed model first computes the intra-saliency for each image by aggregating multiple saliency cues. The common and salient regions across multiple images are thus discovered via a graph matching procedure. Then, a graph reconstruction scheme is proposed to refine the intra-saliency iteratively. Compared to existing co-saliency detection methods that only utilize visual appearance cues, our proposed model can effectively exploit both visual appearance and structure information to better guide co-saliency detection. Extensive experiments on several challenging image pair databases demonstrate that our model outperforms state-of-the-art baselines significantly. © 2019 Association for Computing Machinery.",Co-saliency detection; Computer vision; Graph matching model; Graph reconstruction; Image understanding,Image understanding; Iterative methods; Visualization; Co saliencies; Graph matchings; Reliable results; State of the art; Structure information; Vision communities; Visual appearance; Visual similarity; Computer vision
Location-specific influence quantification in location-based social networks,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064674072&doi=10.1145%2f3300199&partnerID=40&md5=c52adfd948c1cc98a88d6f5d0d7c9c06,"Location-based social networks (LBSNs) such as Foursquare offer a platform for users to share and be aware of each other's physical movements. As a result of such a sharing of check-in information with each other, users can be influenced to visit (or check-in) at the locations visited by their friends. Quantifying such influences in these LBSNs is useful in various settings such as location promotion, personalized recommendations, mobility pattern prediction, and so forth. In this article, we develop a model to quantify the influence specific to a location between a pair of users. Specifically, we develop a framework called LoCaTe, that combines (a) a user mobility model based on kernel density estimates; (b) a model of the semantics of the location using topic models; and (c) a user correlation model that uses an exponential distribution. We further develop LoCaTe+, an advanced model within the same framework where user correlation is quantified using a Mutually Exciting Hawkes Process. We show the applicability of LoCaTe and LoCaTe+ for location promotion and location recommendation tasks using LBSNs. Our models are validated using a long-term crawl of Foursquare data collected between January 2015 and February 2016, as well as other publicly available LBSN datasets. Our experiments demonstrate the efficacy of the LoCaTe framework in capturing location-specific influence between users. We also show that our models improve over state-of-the-art models for the task of location promotion as well as location recommendation. © 2019 Association for Computing Machinery.",Influence quantification; Location-based social networks,Semantics; Advanced modeling; Exponential distributions; Influence quantification; Kernel density estimate; Location-based social networks; Mobility pattern; Personalized recommendation; Physical movements; Location
A local mean representation-based K-nearest neighbor classifier,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064628091&doi=10.1145%2f3319532&partnerID=40&md5=9e9c3d0e9c4414cddb52b6280f801ac1,"K-nearest neighbor classification method (KNN), as one of the top 10 algorithms in data mining, is a very simple and yet effective nonparametric technique for pattern recognition. However, due to the selective sensitiveness of the neighborhood size k, the simple majority vote, and the conventional metric measure, the KNN-based classification performance can be easily degraded, especially in the small training sample size cases. In this article, to further improve the classification performance and overcome the main issues in the KNN-based classification, we propose a local mean representation-based k-nearest neighbor classifier (LMRKNN). In the LMRKNN, the categorical k-nearest neighbors of a query sample are first chosen to calculate the corresponding categorical k-local mean vectors, and then the query sample is represented by the linear combination of the categorical k-local mean vectors; finally, the class-specific representation-based distances between the query sample and the categorical k-local mean vectors are adopted to determine the class of the query sample. Extensive experiments on many UCI and KEEL datasets and three popular face databases are carried out by comparing LMRKNN to the state-of-art KNN-based methods. The experimental results demonstrate that the proposed LMRKNN outperforms the related competitive KNN-based methods with more robustness and effectiveness. © 2019 Association for Computing Machinery.",K-nearest neighbor classification; Local mean vector; Pattern recognition; Representation,Data mining; Learning algorithms; Motion compensation; Pattern recognition; Text processing; Vectors; Classification performance; K-nearest neighbor classification; K-nearest neighbor classifier; K-nearest neighbors; Linear combinations; Local mean; Non-parametric techniques; Representation; Nearest neighbor search
Combating fake news: A survey on identification and mitigation techniques,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064656517&doi=10.1145%2f3305260&partnerID=40&md5=0664820fd8ea8eb7b21bce6355b5ac83,"The proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news and mitigation of its widespread impact on public opinion. While much of the earlier research was focused on identification of fake news based on its contents or by exploiting users' engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. In this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. We discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. In addition, research has often been limited by the quality of existing datasets and their specific application contexts. To alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. Furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions. © 2019 Copyright held by the owner/author(s).",AI; Fake news detection; Misinformation; Rumor detection,Artificial intelligence; Social aspects; Social networking (online); Application contexts; Intervention strategy; Misinformation; Mitigation techniques; Public opinions; Social media; Technical challenges; Timely identification; Surveys
A semi-boosted nested model with sensitivity-based weighted binarization for multi-domain network intrusion detection,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064605703&doi=10.1145%2f3313778&partnerID=40&md5=51f83ea60f2695af1bf1f53a6809a027,"Effective network intrusion detection techniques are required to thwart evolving cybersecurity threats. Historically, traditional enterprise networks have been researched extensively in this regard. However, the cyber threat landscape has grown to include wireless networks. In this article, the authors present a novel model that can be trained on completely different feature sets and applied to two distinct intrusion detection applications: traditional enterprise networks and 802.11 wireless networks. This is the first method that demonstrates superior performance in both aforementioned applications. The model is based on a one-versus-all binary framework comprising multiple nested sub-ensembles. To provide good generalization ability, each sub-ensemble contains a collection of sub-learners, and only a portion of the sub-learners implement boosting. A class weight based on the sensitivity metric (true-positive rate), learned from the training data only, is assigned to the sub-ensembles of each class. The use of pruning to remove sub-learners that do not contribute to or have an adverse effect on overall system performance is investigated as well. The results demonstrate that the proposed system can achieve exceptional performance in applications to both traditional enterprise intrusion detection and 802.11 wireless intrusion detection. © 2019 Copyright held by the owner/author(s).",802.11 wireless; Boosting; Enterprise network; Intrusion detection; Machine learning; One-versus-all,Adaptive boosting; IEEE Standards; Learning systems; Wireless networks; 802.11 wireless networks; Boosting; Enterprise networks; Generalization ability; Multidomain networks; Network intrusion detection; One-versus-all; Wireless intrusion detections; Intrusion detection
Deep multi-scale discriminative networks for double JPEG compression forensics,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062374734&doi=10.1145%2f3301274&partnerID=40&md5=ec3c2eeb523d7b7804af05a83663a625,"As JPEG is the most widely used image format, the importance of tampering detection for JPEG images in blind forensics is self-evident. In this area, extracting effective statistical characteristics from a JPEG image for classification remains a challenge. Effective features are designed manually in traditional methods, suggesting that extensive labor-consuming research and derivation is required. In this article, we propose a novel image tampering detection method based on deep multi-scale discriminative networks (MSD-Nets). The multi-scale module is designed to automatically extract multiple features from the discrete cosine transform (DCT) coefficient histograms of the JPEG image. This module can capture the characteristic information in different scale spaces. In addition, a discriminative module is also utilized to improve the detection effect of the networks in those difficult situations when the first compression quality (QF1) is higher than the second one (QF2). A special network in this module is designed to distinguish the small statistical difference between authentic and tampered regions in these cases. Finally, a probability map can be obtained and the specific tampering area is located using the last classification results. Extensive experiments demonstrate the superiority of our proposed method in both quantitative and qualitative metrics when compared with state-of-the-art approaches. © 2019 Association for Computing Machinery.",Blind image forensics; Deep neural network; Double JPEG compression; Multi-scale feature; Tampering detection,Deep neural networks; Digital forensics; Discrete cosine transforms; Image coding; Blind image forensics; Discrete cosine transform coefficients; Double JPEG compressions; Multi-scale features; State-of-the-art approach; Statistical characteristics; Statistical differences; Tampering detection; Image compression
ACM TIST special issue on visual analytics,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061203382&doi=10.1145%2f3277019&partnerID=40&md5=261ad30a2f5e4b43d1ec7e3db84f9693,[No abstract available],,
Goal and plan recognition design for plan libraries,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061211439&doi=10.1145%2f3234464&partnerID=40&md5=83719b1735133321e45690f8df9509b0,"This article provides new techniques for optimizing domain design for goal and plan recognition using plan libraries.We define two new problems: Goal Recognition Design for Plan Libraries (GRD-PL) and Plan Recognition Design (PRD). Solving the GRD-PL helps to infer which goal the agent is trying to achieve,while solving PRD can help to infer how the agent is going to achieve its goal. For each problem, we define a worst-case distinctiveness measure that is an upper bound on the number of observations that are necessary to unambiguously recognize the agent's goal or plan. This article studies the relationship between these measures, showing that theworst-case distinctiveness of GRD-PL is a lower bound of theworst-case plan distinctiveness of PRD and that they are equal under certain conditions.We provide two complete algorithms for minimizing the worst-case distinctiveness of plan libraries without reducing the agent's ability to complete its goals: One is a brute-force search over all possible plans and one is a constraint-based search that identifies plans that are most difficult to distinguish in the domain. These algorithms are evaluated in three hierarchical plan recognition settings from the literature. We were able to reduce the worst-case distinctiveness of the domains using our approach, in some cases reaching 100% improvement within a predesignated time window. Our iterative algorithm outperforms the brute-force approach by an order of magnitude in terms of runtime. © 2019 Association for Computing Machinery.",Conflict Based Search; domain design; environment design; plan libraries; Plan recognition,Libraries; Planning; Brute force search; Brute-force approach; Conflict Based Search; Constraint-based; Environment design; Iterative algorithm; Plan libraries; Plan recognition; Iterative methods
Understanding event organization at scale in event-based social networks,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060005427&doi=10.1145%2f3243227&partnerID=40&md5=52b36abe46cc241cf6bb86c1aa53cdf8,"Understanding real-world event participation behavior has been a subject of active research and can offer valuable insights for event-related recommendation and advertisement. The emergence of event-based social networks (EBSNs), which attracts online users to host/attend offline events, has enabled exciting new research in this domain. However, most existing works focus on understanding or predicting individual users' event participation behavior or recommending events to individual users. Few studies have addressed the problem of event popularity from the event organizer's point of view. In this work, we study the latent factors for determining event popularity using large-scale datasets collected from the popular Meetup.com EBSN in five major cities around the world. We analyze and model four contextual factors: spatial factor using location convenience, quality, popularity density, and competitiveness; group factor using group member entropy and loyalty; temporal factor using temporal preference and weekly event patterns; and semantic factor using readability, sentiment, part of speech, and text novelty. In addition, we have developed a group-based social influence propagation network to model group-specific influences on events. By combining the COntextual features and Social Influence NEtwork, our integrated prediction framework COSINE can capture the diverse influential factors of event participation and can be used by event organizers to predict/improve the popularity of their events. Detailed evaluations demonstrate that our COSINE framework achieves high accuracy for event popularity prediction in all five cities with diverse cultures and user event behaviors. © 2019 Association for Computing Machinery.",Group event organization; Social influence; User behavior modeling,Behavioral research; Forecasting; Semantics; Social networking (online); Contextual factors; Contextual feature; Influential factors; Integrated prediction; Large-scale datasets; Popularity predictions; Social influence; User behavior modeling; Economic and social effects
Federated machine learning: Concept and applications,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061188595&doi=10.1145%2f3298981&partnerID=40&md5=cee66750e6e1153bbf8913f9b83e2362,"Today's artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security.We propose a possible solution to these challenges: Secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning.We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy. © 2019 Copyright held by the owner/author(s).",Federated learning; GDPR; transfer learning,Artificial intelligence; Learning systems; Data network; Effective solution; Federated learning; GDPR; Isolated islands; Learning frameworks; Transfer learning; User privacy; Data privacy
A Simple Baseline for Travel Time Estimation using Large-scale Trip Data,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060038679&doi=10.1145%2f3293317&partnerID=40&md5=61457972147f8611182b0ff835484414,"The increased availability of large-scale trajectory data provides rich information for the study of urban dynamics. For example, New York City Taxi & Limousine Commission regularly releases source/destination information of taxi trips, where 173 million taxi trips released for Year 2013 [29]. Such a big dataset provides us potential new perspectives to address the traditional traffic problems. In this article, we study the travel time estimation problem. Instead of following the traditional route-based travel time estimation, we propose to simply use a large amount of taxi trips without using the intermediate trajectory points to estimate the travel time between source and destination. Our experiments show very promising results. The proposed big-data-driven approach significantly outperforms both state-of-the-art route-based method and online map services. Our study indicates that novel simple approaches could be empowered by big data and these approaches could serve as new baselines for some traditional computational problems. © 2019 Association for Computing Machinery.",Baseline; Big data; Trajectory data; Travel time estimation,Big data; Taxicabs; Trajectories; Baseline; Computational problem; Simple approach; State of the art; Traffic problems; Trajectory data; Trajectory points; Travel time estimation; Travel time
"A survey of zero-shot learning: Settings, methods, and applications",2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061187274&doi=10.1145%2f3293318&partnerID=40&md5=e23c0c7d892e1c351e7b91f74a83b241,"Most machine-learning methods focus on classifying instances whose classes have already been seen in training. In practice, many applications require classifying instances whose classes have not been seen previously. Zero-shot learning is a powerful and promising learning paradigm, in which the classes covered by training instances and the classes we aim to classify are disjoint. In this paper, we provide a comprehensive survey of zero-shot learning. First of all, we provide an overview of zero-shot learning. According to the data utilized in model optimization, we classify zero-shot learning into three learning settings. Second, we describe different semantic spaces adopted in existing zero-shot learningworks. Third,we categorize existing zero-shot learning methods and introduce representative methods under each category. Fourth, we discuss different applications of zero-shot learning. Finally, we highlight promising future research directions of zero-shot learning. © 2019 Association for Computing Machinery.",Zero-shot learning survey,Semantics; Surveys; Future research directions; Learning methods; Learning paradigms; Learning settings; Machine learning methods; Model optimization; Semantic Space; Learning systems
Enumerating connected subgraphs and computing the Myerson and Shapley values in graph-restricted games,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060061216&doi=10.1145%2f3235026&partnerID=40&md5=ae7f1738e26cf25ef8fa69b4b1620ac8,"At the heart of multi-agent systems is the ability to cooperate to improve the performance of individual agents and/or the system as a whole. While a widespread assumption in the literature is that such cooperation is essentially unrestricted, in many realistic settings this assumption does not hold. A highly influential approach for modelling such scenarios are graph-restricted games introduced by Myerson [36]. In this approach, agents are represented by nodes in a graph, edges represent communication channels, and a group can generate an arbitrary value only if there exists a direct or indirect communication channel between every pair of agents within the group. Two fundamental solution-concepts that were proposed for such games are the Myerson value and the Shapley value. While an algorithm has been developed to compute the Shapley value in arbitrary graph-restricted games, no such general-purpose algorithm has been developed for the Myerson value to date. With this in mind, we set out to develop for such games a general-purpose algorithm to compute the Myerson value, and a more efficient algorithm to compute the Shapley value. Since the computation of either value involves enumerating all connected induced subgraphs of the game's underlying graph, we start by developing an algorithm dedicated to this enumeration, and then we show empirically that it is faster than the state of the art in the literature. Finally, we present a sample application of both algorithms, in which we test the Myerson value and the Shapley value as advanced measures of node centrality in networks. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Algorithms; Coalitional games; Depth-first search; Myerson value,Algorithms; Communication channels (information theory); Game theory; Multi agent systems; Coalitional game; Connected subgraphs; Depth first search; Fundamental solutions; Graph-restricted game; Indirect communication; Myerson value; Sample applications; Computer games
Visual analytics of heterogeneous data using hypergraph learning,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061194051&doi=10.1145%2f3200765&partnerID=40&md5=e67889bd5b2df7b15e82508a9c565b1c,"For real-world learning tasks (e.g., classification), graph-based models are commonly used to fuse the information distributed in diverse data sources, which can be heterogeneous, redundant, and incomplete. These models represent the relations in different datasets as pairwise links. However, these links cannot deal with high-order relations which connect multiple objects (e.g., in public health datasets, more than two patient groups admitted by the same hospital in 2014). In this article, we propose a visual analytics approach for the classification on heterogeneous datasets using the hypergraph model. The hypergraph is an extension to traditional graphs in which a hyperedge connects multiple vertices instead of just two. We model various high-order relations in heterogeneous datasets as hyperedges and fuse different datasets with a unified hypergraph structure. We use the hypergraph learning algorithm for predicting missing labels in the datasets. To allow users to inject their domain knowledge into the model-learning process, we augment the traditional learning algorithm in a number of ways. In addition, we also propose a set of visualizations which enable the user to construct the hypergraph structure and the parameters of the learning model interactively during the analysis. We demonstrate the capability of our approach via two real-world cases. © 2018 Association for Computing Machinery.",Data fusion; High-dimensional data; Hypergraph learning,Classification (of information); Clustering algorithms; Data fusion; Visualization; Graph-based models; Heterogeneous data; Heterogeneous datasets; High dimensional data; Hypergraph; Hypergraph model; Real-world learning; Traditional learning; Learning algorithms
Reconstruction of hidden representation for robust feature extraction,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060018886&doi=10.1145%2f3284174&partnerID=40&md5=2b1a9c8f11504e7e25a1996cf90ab170,"This article aims to develop a new and robust approach to feature representation. Motivated by the success of Auto-Encoders, we first theoretically analyze and summarize the general properties of all algorithms that are based on traditional Auto-Encoders: (1) The reconstruction error of the input cannot be lower than a lower bound, which can be viewed as a guiding principle for reconstructing the input. Additionally, when the input is corrupted with noises, the reconstruction error of the corrupted input also cannot be lower than a lower bound. (2) The reconstruction of a hidden representation achieving its ideal situation is the necessary condition for the reconstruction of the input to reach the ideal state. (3) Minimizing the Frobenius norm of the Jacobian matrix of the hidden representation has a deficiency and may result in a much worse local optimum value. We believe that minimizing the reconstruction error of the hidden representation is more robust than minimizing the Frobenius norm of the Jacobian matrix of the hidden representation. Based on the above analysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs), which uses corruption and reconstruction on both the input and the hidden representation. We demonstrate that the proposed model is highly flexible and extensible and has a potentially better capability to learn invariant and robust feature representations. We also show that our model is more robust than Denoising Auto-Encoders (DAEs) for dealing with noises or inessential features. Furthermore, we detail how to train DDAEs with two different pretraining methods by optimizing the objective function in a combined and separate manner, respectively. Comparative experiments illustrate that the proposed model is significantly better for representation learning than the state-of-the-art models. © 2019 Association for Computing Machinery.",Auto-encoders; Deep architectures; Feature representation; Reconstruction of hidden representation; Unsupervised learning,Errors; Learning systems; Signal encoding; Unsupervised learning; Auto encoders; Comparative experiments; Deep architectures; Feature representation; Guiding principles; Objective functions; Reconstruction error; Robust feature extractions; Jacobian matrices
Inferring online social ties from offline geographical activities,2019,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060065282&doi=10.1145%2f3293319&partnerID=40&md5=cd906fb57d0f6b267a539829e7347dad,"As mobile devices are becoming ubiquitous nowadays, the geographical activities and interactions of human beings can be easily recorded and accessed. Each mobile individual can belong to an online social network. Unfortunately, the underlying online social relationships are hidden and only available to service providers. Acquiring the social network of mobile users would enrich lots of mobile applications, such as friend recommendation and energy-saving mobile database management. In this work, we propose to infer online social ties using purely offline geographical activities of users, such as check-in records and spatial meeting events. To tackle the problem, we devise a novel inference framework, O2O-Inf, which consists of two components, Feature Modeling and Link Inference. Feature modeling is to characterize both direct and indirect geographical interactions between nodes from co-location and graph features. Link inference aims to infer the social ties based on a small set of observed social links, and the idea is that pairs of nodes sharing similar geographical behaviors have the same tendency of linkage (i.e., either being friends or non-friends). Experiments conducted on a Gowalla location-based social network and a Meetup event-based social network exhibit a satisfying performance in comparison to state-of-the-art prediction methods under the settings of offline-to-online network inference and geo-link prediction. © 2019 Association for Computing Machinery.",Link prediction; Location-based services; Network inference; Offline geographical activities; Online social ties,Energy conservation; Forecasting; Location; Location based services; Telecommunication services; Friend recommendations; Link prediction; Location-based social networks; Mobile database management; Network inference; Offline; On-line social networks; Social ties; Social networking (online)
Visual interfaces for recommendation systems: Finding similar and dissimilar peers,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057606988&doi=10.1145%2f3200490&partnerID=40&md5=4c555e04c0bfe809c6b67f504858d0b4,"Recommendation applications can guide users in making important life choices by referring to the activities of similar peers. For example, students making academic plans may learn from the data of similar students, while patients and their physicians may explore data from similar patients to select the best treatment. Selecting an appropriate peer group has a strong impact on the value of the guidance that can result from analyzing the peer group data. In this article, we describe a visual interface that helps users review the similarity and differences between a seed record and a group of similar records and refine the selection. We introduce the LikeMeDonuts, Ranking Glyph, and History Heatmap visualizations. The interface was refined through three rounds of formative usability evaluation with 12 target users, and its usefulness was evaluated by a case study with a student review manager using real student data. We describe three analytic workflows observed during use and summarize how users' input shaped the final design. © 2018 Association for Computing Machinery.",Decision making; Multidimensional data visualization; Personal record; Similarity; Temporal visualization; Visual analytics,Decision making; Patient treatment; Students; Visualization; Multi-dimensional data visualization; Peer groups; Personal record; Similarity; Usability evaluation; Visual analytics; Visual Interface; Work-flows; Data visualization
Bayespiles: Visualisation support for Bayesian network structure learning,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057586933&doi=10.1145%2f3230623&partnerID=40&md5=3e59edbad9ea9b542496fb4edd9c9b92,"We address the problem of exploring, combining, and comparing large collections of scored, directed networks for understanding inferred Bayesian networks used in biology. In this field, heuristic algorithms explore the space of possible network solutions, sampling this space based on algorithm parameters and a network score that encodes the statistical fit to the data. The goal of the analyst is to guide the heuristic search and decide how to determine a final consensus network structure, usually by selecting the top-scoring network or constructing the consensus network from a collection of high-scoring networks. BayesPiles, our visualisation tool, helps with understanding the structure of the solution space and supporting the construction of a final consensus network that is representative of the underlying dataset. BayesPiles builds upon and extends MultiPiles to meet our domain requirements. We developed BayesPiles in conjunction with computational biologists who have used this tool on datasets used in their research. The biologists found our solution provides them with new insights and helps them achieve results that are representative of the underlying data. © 2018 Association for Computing Machinery.",Bioinformatics; Graphs; Visualisation,Bioinformatics; Heuristic algorithms; Sampling; Visualization; Algorithm parameters; Bayesian network structure; Consensus networks; Directed network; Domain requirements; Graphs; Heuristic search; Network solutions; Bayesian networks
Learning facial expressions with 3D mesh convolutional neural network,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057576901&doi=10.1145%2f3200572&partnerID=40&md5=71e1c77f0d3938a58a8a175f01015566,"Making machines understand human expressions enables various useful applications in human-machine interaction. In this article, we present a novel facial expression recognition approach with 3D Mesh Convolutional Neural Networks (3DMCNN) and a visual analytics-guided 3DMCNN design and optimization scheme. From an RGBD camera, we first reconstruct a 3D face model of a subject with facial expressions and then compute the geometric properties of the surface. Instead of using regular Convolutional Neural Networks (CNNs) to learn intensities of the facial images, we convolve the geometric properties on the surface of the 3D model using 3DMCNN. We design a geodesic distance-based convolution method to overcome the difficulties raised from the irregular sampling of the face surface mesh. We further present interactive visual analytics for the purpose of designing and modifying the networks to analyze the learned features and cluster similar nodes in 3DMCNN. By removing low-activity nodes in the network, the performance of the network is greatly improved. We compare our method with the regular CNN-based method by interactively visualizing each layer of the networks and analyze the effectiveness of our method by studying representative cases. Testing on public datasets, our method achieves a higher recognition accuracy than traditional image-based CNN and other 3D CNNs. The proposed framework, including 3DMCNN and interactive visual analytics of the CNN, can be extended to other applications. © 2018 Association for Computing Machinery.",3D mesh convolutional neural networks; Facial expression analysis; Visual analysis,Convolution; Human computer interaction; Mesh generation; Network layers; Neural networks; Optical character recognition; Three dimensional computer graphics; Visualization; Convolutional neural network; Design and optimization; Facial expression analysis; Facial expression recognition; Geometric properties; Human machine interaction; Recognition accuracy; Visual analysis; MESH networking
Traffic simulation and visual verification in Smog,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057592606&doi=10.1145%2f3200491&partnerID=40&md5=5f180a362d49495c7c8ff26ba36dbea0,"Smog causes low visibility on the road and it can impact the safety of traffic. Modeling traffic in smog will have a significant impact on realistic traffic simulations. Most existing traffic models assume that drivers have optimal vision in the simulations, making these simulations are not suitable for modeling smog weather conditions. In this article, we introduce the Smog Full Velocity Difference Model (SMOG-FVDM) for a realistic simulation of traffic in smog weather conditions. In this model, we present a stadia model for drivers in smog conditions. We introduce it into a car-following traffic model using both psychological force and body force concepts, and then we introduce the SMOG-FVDM. Considering that there are lots of parameters in the SMOG-FVDM, we design a visual verification system based on SMOG-FVDM to arrive at an adequate solution which can show visual simulation results under different road scenarios and different degrees of smog by reconciling the parameters. Experimental results show that our model can give a realistic and efficient traffic simulation of smog weather conditions. © 2018 Association for Computing Machinery.",Body force; Psychological force; Smog; SMOG-FVDM; Visual verification system,Meteorology; Roads and streets; Street traffic control; Body forces; Psychological force; Smog; SMOG-FVDM; Visual verification; Air pollution
High-precision camera localization in scenes with repetitive patterns,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059822142&doi=10.1145%2f3226111&partnerID=40&md5=aca4f58597b24e6e3936b9992ae52585,"This article presents a high-precision multi-modal approach for localizing moving cameras with monocular videos, which has wide potentials in many intelligent applications, including robotics, autonomous vehicles, and so on. Existing visual odometry methods often suffer from symmetric or repetitive scene patterns, e.g., windows on buildings or parking stalls. To address this issue, we introduce a robust camera localization method that contributes in two aspects. First, we formulate feature tracking, the critical step of visual odometry, as a hierarchical min-cost network flow optimization task, and we regularize the formula with flow constraints, cross-scale consistencies, and motion heuristics. The proposed regularized formula is capable of adaptively selecting distinctive features or feature combinations, which is more effective than traditional methods that detect and group repetitive patterns in a separate step. Second, we develop a joint formula for integrating dense visual odometry and sparse GPS readings in a common reference coordinate. The fusion process is guided with high-order statistics knowledge to suppress the impacts of noises, clusters, and model drifting. We evaluate the proposed camera localization method on both public video datasets and a newly created dataset that includes scenes full of repetitive patterns. Results with comparisons show that our method can achieve comparable performance to state-of-the-art methods and is particularly effective for addressing repetitive pattern issues. © 2018 Association for Computing Machinery.",And Phrases: Visual odometry; Feature matching; Flow optimization,Computer vision; Optimization; Vision; Feature matching; Flow optimization; High order statistics; Intelligent applications; Network flow optimizations; Reference coordinates; State-of-the-art methods; Visual odometry; Cameras
CAPVIs: Toward better understanding of visual-verbal saliency consistency,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057632307&doi=10.1145%2f3200767&partnerID=40&md5=a96422f5a95e60858e8cea6aae253094,"When looking at an image, humans shift their attention toward interesting regions, making sequences of eye fixations. When describing an image, they also come up with simple sentences that highlight the key elements in the scene. What is the correlation between where people look and what they describe in an image? To investigate this problem intuitively, we develop a visual analytics system, CapVis, to look into visual attention and image captioning, two types of subjective annotations that are relatively task-free and natural. Using these annotations, we propose a word-weighting scheme to extract visual and verbal saliency ranks to compare against each other. In our approach, a number of low-level and semantic-level features relevant to visual-verbal saliency consistency are proposed and visualized for a better understanding of image content. Our method also shows the different ways that a human and a computational model look at and describe images, which provides reliable information for a captioning model. Experiment also shows that the visualized feature can be integrated into a computational model to effectively predict the consistency between the two modalities on an image dataset with both types of annotations. © 2018 Association for Computing Machinery.",Image captioning; Visual analytics; Visual saliency,Behavioral research; Computation theory; Computational methods; Semantics; Computational model; Image captioning; Semantic levels; Visual analytics; Visual analytics systems; Visual Attention; Visual saliency; Weighting scheme; Visualization
DeepTracker: Visualizing the training process of convolutional neural networks,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057587399&doi=10.1145%2f3200489&partnerID=40&md5=483d41a7ffbb9a723274d02fc8344da3,"Deep Convolutional Neural Networks (CNNs) have achieved remarkable success in various fields. However, training an excellent CNN is practically a trial-and-error process that consumes a tremendous amount of time and computer resources. To accelerate the training process and reduce the number of trials, experts need to understand what has occurred in the training process and why the resulting CNN behaves as it does. However, current popular training platforms, such as TensorFlow, only provide very little and general information, such as training/validation errors, which is far from enough to serve this purpose. To bridge this gap and help domain experts with their training tasks in a practical environment, we propose a visual analytics system, DeepTracker, to facilitate the exploration of the rich dynamics of CNN training processes and to identify the unusual patterns that are hidden behind the huge amount of information in training log. Specifically, we combine a hierarchical index mechanism and a set of hierarchical small multiples to help experts explore the entire training log from different levels of detail. We also introduce a novel cube-style visualization to reveal the complex correlations among multiple types of heterogeneous training data, including neuron weights, validation images, and training iterations. Three case studies are conducted to demonstrate how DeepTracker provides its users with valuable knowledge in an industry-level CNN training process; namely, in our case, training ResNet-50 on the ImageNet dataset. We show that our method can be easily applied to other state-of-the-art “very deep” CNN models. © 2018 Association for Computing Machinery.",Correlation analysis; Deep learning; Multiple time series; Training process; Visual analytics,Convolution; Data visualization; Deep learning; Neural networks; Time series analysis; Visualization; Convolutional neural network; Correlation analysis; Deep convolutional neural networks; Multiple time series; Training process; Trial-and-error process; Visual analytics; Visual analytics systems; Deep neural networks
Discriminative and orthogonal subspace constraints-based nonnegative matrix factorization,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059813603&doi=10.1145%2f3229051&partnerID=40&md5=f480d42c628ad87b786e190802710e9b,"Nonnegative matrix factorization (NMF) is one widely used feature extraction technology in the tasks of image clustering and image classification. For the former task, various unsupervised NMF methods based on the data distribution structure information have been proposed. While for the latter task, the label information of the dataset is one very important guiding. However, most previous proposed supervised NMF methods emphasis on imposing the discriminant constraints on the coefficient matrix. When dealing with new coming samples, the transpose or the pseudoinverse of the basis matrix is used to project these samples to the low dimension space. In this way, the label influence to the basis matrix is indirect. Although, there are also some methods trying to constrain the basis matrix in NMF framework, either they only restrict within-class samples or impose improper constraint on the basis matrix. To address these problems, in this article a novel NMF framework named discriminative and orthogonal subspace constraints-based nonnegative matrix factorization (DOSNMF) is proposed. In DOSNMF, the discriminative constraints are imposed on the projected subspace instead of the directly learned representation. In this manner, the discriminative information is directly connected with the projected subspace. At the same time, an orthogonal term is incorporated in DOSNMF to adjust the orthogonality of the learned basis matrix, which can ensure the orthogonality of the learned subspace and improve the sparseness of the basis matrix at the same time. This framework can be implemented in two ways. The first way is based on the manifold learning theory. In this way, two graphs, i.e., the intrinsic graph and the penalty graph, are constructed to capture the intra-class structure and the inter-class distinctness. With this design, both the manifold structure information and the discriminative information of the dataset are utilized. For convenience, we name this method as the name of the framework, i.e., DOSNMF. The second way is based on the Fisher's criterion, we name it Fisher's criterion-based DOSNMF (FDOSNMF). The objective functions of DOSNMF and FDOSNMF can be easily optimized using multiplicative update (MU) rules. The new methods are tested on five datasets and compared with several supervised and unsupervised variants of NMF. The experimental results reveal the effectiveness of the proposed methods. © 2018 ACM.",Data representation; Discriminative graphs; Fisher's criterion; Image classification; Nonnegative matrix factorization (NMF),Factorization; Image classification; Data representations; Discriminative graphs; Extraction technology; Fisher's criterion; Multiplicative updates; Nonnegative matrix factorization; Orthogonal subspaces; Structure information; Matrix algebra
Adaptive online one-class support vector machines with applications in structural health monitoring,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059810049&doi=10.1145%2f3230708&partnerID=40&md5=57644c11a30b636ffb64bcdc7dfdadd4,"One-class support vector machine (OCSVM) has been widely used in the area of structural health monitoring, where only data from one class (i.e., healthy) are available. Incremental learning of OCSVM is critical for online applications in which huge data streams continuously arrive and the healthy data distribution may vary over time. This article proposes a novel adaptive self-advised online OCSVM that incrementally tunes the kernel parameter and decides whether a model update is required or not. As opposed to existing methods, this novel online algorithm does not rely on any fixed threshold, but it uses the slack variables in the OCSVM to determine which new data points should be included in the training set and trigger a model update. The algorithm also incrementally tunes the kernel parameter of OCSVM automatically based on the spatial locations of the edge and interior samples in the training data with respect to the constructed hyperplane of OCSVM. This new online OCSVM algorithm was extensively evaluated using synthetic data and real data from case studies in structural health monitoring. The results showed that the proposed method significantly 4 improved the classification error rates, was able to assimilate the changes in the positive data distribution over time, and maintained a high damage detection accuracy in all case studies. © 2018 ACM.",,Damage detection; Support vector machines; Classification error rate; Data distribution; Detection accuracy; Incremental learning; On-line algorithms; On-line applications; One-class support vector machine; One-class support vector machines (OCSVM); Structural health monitoring
D-MAP+: Interactive visual analysis and exploration of ego-centric and event-centric information diffusion patterns in social media,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057617954&doi=10.1145%2f3183347&partnerID=40&md5=bfaa1af4baffbdb9267c327ec01ad1ae,"Information diffusion analysis is important in social media. In this work, we present a coherent ego-centric and event-centric model to investigate diffusion patterns and user behaviors. Applying the model, we propose Diffusion Map+ (D-Maps+), a novel visualization method to support exploration and analysis of user behaviors and diffusion patterns through a map metaphor. For ego-centric analysis, users who participated in reposting (i.e., resending a message initially posted by others) one central user's posts (i.e., a series of original tweets) are collected. Event-centric analysis focuses on multiple central users discussing a specific event, with all the people participating and reposting messages about it. Social media users are mapped to a hexagonal grid based on their behavior similarities and in the chronological order of repostings. With the additional interactions and linkings, D-Map+ is capable of providing visual profiling of influential users, describing their social behaviors and analyzing the evolution of significant events in social media. A comprehensive visual analysis system is developed to support interactive exploration with D-Map+. We evaluate our work with real-world social media data and find interesting patterns among users and events. We also perform evaluations including user studies and expert feedback to certify the capabilities of our method. © 2018 Association for Computing Machinery.",Information diffusion; Map; Social media,Maps; Social networking (online); Chronological order; Information diffusion; Interactive exploration; Interactive visual analysis; Novel visualizations; Social media; Social media datum; Social media users; Behavioral research
Resumevis: A visual analytics system to discover semantic information in semi-structured resume data,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057624447&doi=10.1145%2f3230707&partnerID=40&md5=3d7ae7f97c2da158d1d0f096597d64d6,"Massive public resume data emerging on the internet indicates individual-related characteristics in terms of profile and career experiences. Resume Analysis (RA) provides opportunities for many applications, such as recruitment trend predict, talent seeking and evaluation. Existing RA studies either largely rely on the knowledge of domain experts, or leverage classic statistical or data mining models to identify and filter explicit attributes based on pre-defined rules. However, they fail to discover the latent semantic information from semi-structured resume text, i.e., individual career progress trajectory and social-relations, which are otherwise vital to comprehensive understanding of people's career evolving patterns. Besides, when dealing with large numbers of resumes, how to properly visualize such semantic information to reduce the information load and to support better human cognition is also challenging. To tackle these issues, we propose a visual analytics system called ResumeVis to mine and visualize resume data. First, a text mining-based approach is presented to extract semantic information. Then, a set of visualizations are devised to represent the semantic information in multiple perspectives. Through interactive exploration on ResumeVis performed by domain experts, the following tasks can be accomplished: to trace individual career evolving trajectory; to mine latent social-relations among individuals; and to hold the full picture of massive resumes' collective mobility. Case studies with over 2,500 government officer resumes demonstrate the effectiveness of our system. © 2018 Association for Computing Machinery.",Resume analysis; Semantic information mining; Text visualization; Visual analytics,Filtration; Knowledge management; Semantics; Visualization; Data mining models; Evolving patterns; Interactive exploration; Resume analysis; Semantic information; Text visualization; Visual analytics; Visual analytics systems; Data mining
TPM: A temporal personalized model for spatial item recommendation,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059803553&doi=10.1145%2f3230706&partnerID=40&md5=2f211bf75e675a52059c1b8c0fb3f5ea,"With the rapid development of location-based social networks (LBSNs), spatial item recommendation has become an important way of helping users discover interesting locations to increase their engagement with location-based services. The availability of spatial, temporal, and social information in LBSNs offers an unprecedented opportunity to enhance the spatial item recommendation. Many previous works studied spatial and social influences on spatial item recommendation in LBSNs. Due to the strong correlations between a user's check-in time and the corresponding check-in location, which include the sequential influence and temporal cyclic effect, it is essential for spatial item recommender system to exploit the temporal effect to improve the recommendation accuracy. Leveraging temporal information in spatial item recommendation is, however, very challenging, considering (1) when integrating sequential influences, users' check-in data in LBSNs has a low sampling rate in both space and time, which renders existing location prediction techniques on GPS trajectories ineffective, and the prediction space is extremely large, with millions of distinct locations as the next prediction target, which impedes the application of classical Markov chain models; (2) there are various temporal cyclic patterns (i.e., daily, weekly, and monthly) in LBSNs, but existing work is limited to one specific pattern; and (3) there is no existing framework that unifies users' personal interests, temporal cyclic patterns, and the sequential influence of recently visited locations in a principled manner. In light of the above challenges, we propose a Temporal Personalized Model (TPM), which introduces a novel latent variable topic-region to model and fuse sequential influence, cyclic patterns with personal interests in the latent and exponential space. The advantages of modeling the temporal effect at the topic-region level include a significantly reduced prediction space, an effective alleviation of data sparsity, and a direct expression of the semantic meaning of users' spatial activities. Moreover, we introduce two methods to model the effect of various cyclic patterns. The first method is a time indexing scheme that encodes the effect of various cyclic patterns into a binary code. However, the indexing scheme faces the data sparsity problem in each time slice. To deal with this data sparsity problem, the second method slices the time according to each cyclic pattern separately and explores these patterns in a joint additive model. Furthermore, we design an asymmetric Locality Sensitive Hashing (ALSH) technique to speed up the online top-k recommendation process by extending the traditional LSH. We evaluate the performance of TPM on two real datasets and one large-scale synthetic dataset. The performance of TPM in recommending cold-start items is also evaluated. The results demonstrate a significant improvement in TPM's ability to recommend spatial items, in terms of both effectiveness and efficiency, compared with the state-of-the-art methods. © 2018 ACM.",Location-based service; Online learning; POI; Spatial-temporal modeling,Encoding (symbols); Error analysis; Forecasting; Indexing (of information); Location; Markov processes; Semantics; Telecommunication services; Data sparsity problems; Effectiveness and efficiencies; Locality sensitive hashing; Location-based social networks; Online learning; Recommendation accuracy; Spatial temporal model; State-of-the-art methods; Location based services
A cross-domain recommendation mechanism for cold-start users based on partial least squares regression,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059805214&doi=10.1145%2f3231601&partnerID=40&md5=e1eacb97b0316e117404c3b64849cd05,"Recommender systems are common in e-commerce platforms in recent years. Recommender systems are able to help users find preferential items among a large amount of products so that users' time is saved and sellers' profits are increased. Cross-domain recommender systems aim to recommend items based on users' different tastes across domains. While recommender systems usually suffer from the user cold-start problem that leads to unsatisfying recommendation performance, cross-domain recommendation can remedy such a problem. This article proposes a novel cross-domain recommendation model based on regression analysis, partial least squares regression (PLSR). The proposed recommendation models, PLSR-CrossRec and PLSR-Latent, are able to purely use source-domain ratings to predict the ratings for cold-start users who never rated items in the target domains. Experiments conducted on the Epinions dataset with ten various domains' rating records demonstrate that PLSR-Latent can outperform several matrix factorization-based competing methods under a variety of cross-domain settings. The time efficiency of PLSR-Latent is also satisfactory. © 2018 ACM.",Cold start; Cross-domain recommendation; Partial least square regression; Transfer learning,Electronic commerce; Factorization; Recommender systems; Regression analysis; Cold start; Cross-domain recommendations; Matrix factorizations; Partial least square regression; Partial least squares regression; Partial least squares regressions (PLSR); Recommendation performance; Transfer learning; Least squares approximations
SmartTransfer: Modeling the spatiotemporal dynamics of passenger transfers for crowdedness-aware route recommendations,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059846912&doi=10.1145%2f3232229&partnerID=40&md5=d26b6c882e8c979a9aaf3ca44177f876,"In urban transportation systems, transfer stations refer to hubs connecting a variety of bus and subway lines and, thus, are the most important nodes in transportation networks. The pervasive availability of large-scale travel traces of passengers, collected from automated fare collection (AFC) systems, has provided unprecedented opportunities for understanding citywide transfer patterns, which can benefit smart transportation, such as smart route recommendation to avoid crowded lines, and dynamic bus scheduling to enhance transportation efficiency. To this end, in this article, we provide a systematic study of the measurement, patterns, and modeling of spatiotemporal dynamics of passenger transfers. Along this line, we develop a data-driven analytical system for modeling the transfer volumes of each transfer station. More specifically, we first identify and quantify the discriminative patterns of spatiotemporal dynamics of passenger transfers by utilizing heterogeneous sources of transfer related data for each station. Also, we develop a multi-task spatiotemporal learning model for predicting the transfer volumes of a specific station at a specific time period. Moreover, we further leverage the predictive model of passenger transfers to provide crowdedness-aware route recommendations. Finally, we conduct the extensive evaluations with a variety of real-world data. Experimental results demonstrate the effectiveness of our proposed modeling method and its applications for smart transportation. © 2018 Association for Computing Machinery.",Automated fare collection; Crowdedness detection; Route recommendation; Spatiotemporal; Transit behavior,Bus transportation; Dynamics; Transfer stations; Transportation routes; Automated fare collection; Route recommendation; Spatio-temporal dynamics; Spatio-temporal learning; Spatiotemporal; Transit behavior; Transportation efficiency; Urban transportation systems; Urban transportation
Characterizing user skills from application usage traces with hierarchical atention recurrent networks,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056483335&doi=10.1145%2f3232231&partnerID=40&md5=d314b4b4d2d2d25db06e07d7513e2393,"Predicting users' profciencies is a critical component of AI-powered personal assistants. This article introduces a novel approach for the prediction based on users' diverse, noisy, and passively generated application usage histories. We propose a novel bi-directional recurrent neural network with hierarchical attention mechanism to extract sequential patterns and distinguish informative traces from noise. Our model is able to attend to the most discriminative actions and sessions to make more accurate and directly interpretable predictions while requiring 50× less training data than the state-of-the-art sequential learning approach. We evaluate our model with two large scale datasets collected from 68K Photoshop users: a digital design skill dataset where the user skill is determined by the quality of the end products and a software skill dataset where users self-disclose their software usage skill levels. The empirical results demonstrate our model's superior performance compared to existing user representation learning techniques that leverage action frequencies and sequential patterns. In addition, we qualitatively illustrate the model's signifcant interpretative power. The proposed approach is broadly relevant to applications that generate user time-series analytics. © 2018 ACM.",Hierarchical attention; Recurrent neural network; User modeling; User skill,Product design; Recurrent neural networks; Attention mechanisms; Hierarchical attention; Large-scale datasets; Learning techniques; Personal assistants; Sequential patterns; User Modeling; User skill; Forecasting
Few-shot text and image classification via analogical transfer learning,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056446282&doi=10.1145%2f3230709&partnerID=40&md5=e768b77dd02e0018300d0494884b4e9b,"Learning from very few samples is a challenge for machine learning tasks, such as text and image classifcation. Performance of such task can be enhanced via transfer of helpful knowledge from related domains, which is referred to as transfer learning. In previous transfer learning works, instance transfer learning algorithms mostly focus on selecting the source domain instances similar to the target domain instances for transfer. However, the selected instances usually do not directly contribute to the learning performance in the target domain. Hypothesis transfer learning algorithms focus on the model/parameter level transfer. They treat the source hypotheses as well-trained and transfer their knowledge in terms of parameters to learn the target hypothesis. Such algorithms directly optimize the target hypothesis by the observable performance improvements. However, they fail to consider the problem that instances that contribute to the source hypotheses may be harmful for the target hypothesis, as instance transfer learning analyzed. To relieve the aforementioned problems, we propose a novel transfer learning algorithm, which follows an analogical strategy. Particularly, the proposed algorithm frst learns a revised source hypothesis with only instances contributing to the target hypothesis. Then, the proposed algorithm transfers both the revised source hypothesis and the target hypothesis (only trained with a few samples) to learn an analogical hypothesis. We denote our algorithm as Analogical Transfer Learning. Extensive experiments on one synthetic dataset and three real-world benchmark datasets demonstrate the superior performance of the proposed algorithm. © 2018 ACM.",Classifcation; Transfer learning,Benchmarking; Image classification; Learning systems; Text processing; Benchmark datasets; Classifcation; Learning performance; Performance improvements; Real-world; Target domain; Transfer learning; Learning algorithms
Random-forest-inspired neural networks,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056450206&doi=10.1145%2f3232230&partnerID=40&md5=059acc6dc3b6d87408413eb07d8b36b7,"Neural networks have become very popular in recent years, because of the astonishing success of deep learning in various domains such as image and speech recognition. In many of these domains, specifc architectures of neural networks, such as convolutional networks, seem to ft the particular structure of the problem domain very well and can therefore perform in an astonishingly effective way. However, the success of neural networks is not universal across all domains. Indeed, for learning problems without any special structure, or in cases where the data are somewhat limited, neural networks are known not to perform well with respect to traditional machine-learning methods such as random forests. In this article, we show that a carefully designed neural network with random forest structure can have better generalization ability. In fact, this architecture is more powerful than random forests, because the back-propagation algorithm reduces to a more powerful and generalized way of constructing a decision tree. Furthermore, the approach is efcient to train and requires a small constant factor of the number of training examples. This efciency allows the training of multiple neural networks to improve the generalization accuracy. Experimental results on real-world benchmark datasets demonstrate the effectiveness of the proposed enhancements for classifcation and regression. © 2018 ACM.",Classifcation; Neural network; Random forest; Regression,Backpropagation algorithms; Data mining; Decision trees; Deep learning; Neural networks; Speech recognition; Classifcation; Convolutional networks; Generalization ability; Generalization accuracy; Machine learning methods; Multiple neural networks; Random forests; Regression; Network architecture
An efficient alternating newton method for learning factorization machines,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056470056&doi=10.1145%2f3230710&partnerID=40&md5=0fce605740a9cd254087a424e8561673,"To date, factorization machines (FMs) have emerged as a powerful model in many applications. In this work, we study the training of FM with the logistic loss for binary classifcation, which is a nonlinear extension of the linear model with the logistic loss (i.e., logistic regression). For the training of large-scale logistic regression, Newton methods have been shown to be an effective approach, but it is difcult to apply such methods to FM because of the nonconvexity. We consider a modifcation of FM that is multiblock convex and propose an alternating minimization algorithm based on Newton methods. Some novel optimization techniques are introduced to reduce the running time. Our experiments demonstrate that the proposed algorithm is more efcient than stochastic gradient algorithms and coordinate descent methods. The parallelism of our method is also investigated for the acceleration in multithreading environments. © 2018 ACM.",Newton methods; Preconditioned conjugate gradient methods; Subsampled Hessian matrix,Conjugate gradient method; Factorization; Frequency modulation; Regression analysis; Stochastic systems; Alternating minimization algorithms; Coordinate descent methods; Factorization machines; Hessian matrices; Large-scale logistics; Optimization techniques; Preconditioned conjugate gradient method; Stochastic gradient algorithms; Newton-Raphson method
Learning urban community structures: A collective embedding perspective with periodic spatial-temporal mobility graphs,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051493848&doi=10.1145%2f3209686&partnerID=40&md5=40d331a435698651b6aca47bddef14fd,"Learning urban community structures refers to the efforts of quantifying, summarizing, and representing an urban community's (i) static structures, e.g., Point-Of-Interests (POIs) buildings and corresponding geographic allocations, and (ii) dynamic structures, e.g., human mobility patterns among POIs. By learning the community structures, we can better quantitatively represent urban communities and understand their evolutions in the development of cities. This can help us boost commercial activities, enhance public security, foster social interactions, and, ultimately, yield livable, sustainable, and viable environments. However, due to the complex nature of urban systems, it is traditionally challenging to learn the structures of urban communities. To address this problem, in this article, we propose a collective embedding framework to learn the community structure from multiple periodic spatial-temporal graphs of human mobility. Specifically, we first exploit a probabilistic propagation-based approach to create a set of mobility graphs from periodic human mobility records. In these mobility graphs, the static POIs are regarded as vertexes, the dynamic mobility connectivities between POI pairs are regarded as edges, and the edge weights periodically evolve over time. A collective deep auto-encoder method is then developed to collaboratively learn the embeddings of POIs from multiple spatial-temporal mobility graphs. In addition, we develop a Unsupervised Graph based Weighted Aggregation method to align and aggregate the POI embeddings into the representation of the community structures. We apply the proposed embedding framework to two applications (i.e., spotting vibrant communities and predicting housing price return rates) to evaluate the performance of our proposed method. Extensive experimental results on real-world urban communities and human mobility data demonstrate the effectiveness of the proposed collective embedding framework. © 2018 ACM.",Collective embedding; Community structure; Periodic mobility graphs; Urban communities,Graphic methods; Housing; Aggregation methods; Collective embedding; Community structures; Dynamic structure; Mobility graphs; Social interactions; Static structures; Urban community; Periodic structures
X-CLEaVER: Learning ranking ensembles by growing and pruning trees,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051485032&doi=10.1145%2f3205453&partnerID=40&md5=0dcb04aa4deff339f7dce4ae31d595ba,"Learning-to-Rank (LtR) solutions are commonly used in large-scale information retrieval systems such as Web search engines, which have to return highly relevant documents in response to user query within fractions of seconds. The most effective LtR algorithms adopt a gradient boosting approach to build additive ensembles of weighted regression trees. Since the required ranking effectiveness is achieved with very large ensembles, the impact on response time and query throughput of these solutions is not negligible. In this article, we propose X-CLEaVER, an iterative meta-algorithm able to build more efcient and effective ranking ensembles. X-CLEaVER interleaves the iterations of a given gradient boosting learning algorithm with pruning and re-weighting phases. First, redundant trees are removed from the given ensemble, then the weights of the remaining trees are fne-tuned by optimizing the desired ranking quality metric. We propose and analyze several pruning strategies and we assess their benefts showing that interleaving pruning and re-weighting phases during learning is more effective than applying a single post-learning optimization step. Experiments conducted using two publicly available LtR datasets show that X-CLEaVER can be successfully exploited on top of several LtR algorithms as it is effective in optimizing the effectiveness of the learnt ensembles, thus obtaining more compact forests that hence are much more efcient at scoring time. © 2018 ACM.",Efciency; Learning to rank; Pruning,Forestry; Information retrieval; Information retrieval systems; Iterative methods; Learning algorithms; Efciency; Growing and pruning; Large scale information retrieval; Learning optimizations; Learning to rank; Pruning; Relevant documents; Weighted regression; Search engines
Optimum velocity profile of multiple Bernstein-Bézier curves subject to constraints for mobile robots,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054032213&doi=10.1145%2f3183891&partnerID=40&md5=b9ca4d48a6670a9329dac9647a55eb25,"This article deals with trajectory planning that is suitable for nonholonomic differentially driven wheeled mobile robots. The path is approximated with a spline that consists of multiple Bernstein-Bézier curves that are merged together in a way that continuous curvature of the spline is achieved. The article presents the approach for optimization of velocity profile of Bernstein-Bézier spline subject to velocity and acceleration constraints. For the purpose of optimization, velocity and turning points are introduced. Based on these singularity points, local segments are defined where local velocity profiles are optimized independently of each other. From the locally optimum velocity profiles, the global optimum velocity profile is determined. Since each local velocity profile can be evaluated independently, the algorithm is suitable for concurrent implementation and modification of one part of the curve does not require recalculation of all local velocity profiles. These properties enable efficient implementation of the optimization algorithm. The optimization algorithm is also suitable for the splines that consist of Bernstein-Bézier curves that have substantially different lengths. The proposed optimization approach was experimentally evaluated and validated in simulation environment and on real mobile robots. © 2018 ACM.",Mobile robots; Parametric curves; Path planning; Trajectory optimization; Velocity profile,Mobile robots; Motion planning; Optimization; Robot programming; Velocity; Differentially driven wheeled mobile robot; Efficient implementation; Local velocity profiles; Optimization algorithms; Parametric curve; Trajectory optimization; Velocity and acceleration constraints; Velocity profiles; Curve fitting
Integrate and conquer: Double-sided two-dimensional k-means via integrating of projection and manifold construction,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054039525&doi=10.1145%2f3200488&partnerID=40&md5=79c2dd760c76a04eb7732a84fc86b569,"In this article, we introduce a novel, general methodology, called integrate and conquer, for simultaneously accomplishing the tasks of feature extraction, manifold construction, and clustering, which is taken to be superior to building a clustering method as a single task. When the proposed novel methodology is used on two-dimensional (2D) data, it naturally induces a new clustering method highly effective on 2D data. Existing clustering algorithms usually need to convert 2D data to vectors in a preprocessing step, which, unfortunately, severely damages 2D spatial information and omits inherent structures and correlations in the original data. The induced new clustering method can overcome the matrix-vectorization-related issues to enhance the clustering performance on 2D matrices. More specifically, the proposed methodology mutually enhances three tasks of finding subspaces, learning manifolds, and constructing data representation in a seamlessly integrated fashion. When used on 2D data, we seek two projection matrices with optimal numbers of directions to project the data into low-rank, noise-mitigated, and the most expressive subspaces, in which manifolds are adaptively updated according to the projections, and new data representation is built with respect to the projected data by accounting for nonlinearity via adaptive manifolds. Consequently, the learned subspaces and manifolds are clean and intrinsic, and the new data representation is discriminative and robust. Extensive experiments have been conducted and the results confirm the effectiveness of the proposed methodology and algorithm. © 2018 ACM.",Clustering; Feature extraction; Two-dimensional data; Unsupervised learning,Cluster analysis; Extraction; Feature extraction; Matrix algebra; Unsupervised learning; Vectors; Clustering; Clustering methods; Data representations; General methodologies; Integrated fashion; Pre-processing step; Spatial informations; Two dimensional (2D) data; Clustering algorithms
The effect of pets on happiness: A large-scale multi-factor analysis using social multimedia,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054036094&doi=10.1145%2f3200751&partnerID=40&md5=851001852fef57d0f45ae5ef588728d2,"From reducing stress and loneliness, to boosting productivity and overall well-being, pets are believed to play a significant role in people's daily lives. Many traditional studies have identified that frequent interactions with pets could make individuals become healthier and more optimistic, and ultimately enjoy a happier life. However, most of those studies are not only restricted in scale, but also may carry biases by using subjective self-reports, interviews, and questionnaires as the major approaches. In this article, we leverage large-scale data collected from social media and the state-of-the-art deep learning technologies to study this phenomenon in depth and breadth. Our study includes five major steps: (1) collecting timeline posts from around 20,000 Instagram users; (2) using face detection and recognition on 2 million photos to infer users' demographics, relationship status, and whether having children, (3) analyzing a user's degree of happiness based on images and captions via smiling classification and textual sentiment analysis; (4) applying transfer learning techniques to retrain the final layer of the Inception v3 model for pet classification; and (5) analyzing the effects of pets on happiness in terms of multiple factors of user demographics. Our main results have demonstrated the efficacy of our proposed method with many new insights. We believe this method is also applicable to other domains as a scalable, efficient, and effective methodology for modeling and analyzing social behaviors and psychological well-being. In addition, to facilitate the research involving human faces, we also release our dataset of 700K analyzed faces. © 2018 ACM.",Happiness; Happiness analysis; Pet and happiness; Social media; Social multimedia; User demographics,Deep learning; Face recognition; Factor analysis; Population statistics; Sentiment analysis; Social networking (online); Surveys; Happiness; Happiness analysis; Social media; Social multimedia; User demographics; Behavioral research
Exploiting multilabel information for noise-resilient feature selection,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054026398&doi=10.1145%2f3158675&partnerID=40&md5=97e0dd97c1f15cddcb13689894eb32c0,"In a conventional supervised learning paradigm, each data instance is associated with one single class label. Multilabel learning differs in the way that data instances may belong to multiple concepts simultaneously, which naturally appear in a variety of high impact domains, ranging from bioinformatics and information retrieval to multimedia analysis. It targets leveraging the multiple label information of data instances to build a predictive learning model that can classify unlabeled instances into one or multiple predefined target classes. In multilabel learning, even though each instance is associated with a rich set of class labels, the label information could be noisy and incomplete as the labeling process is both time consuming and labor expensive, leading to potential missing annotations or even erroneous annotations. The existence of noisy and missing labels could negatively affect the performance of underlying learning algorithms. More often than not, multilabeled data often has noisy, irrelevant, and redundant features of high dimensionality. The existence of these uninformative features may also deteriorate the predictive power of the learning model due to the curse of dimensionality. Feature selection, as an effective dimensionality reduction technique, has shown to be powerful in preparing high-dimensional data for numerous data mining and machine-learning tasks. However, a vast majority of existing multilabel feature selection algorithms either boil down to solving multiple single-labeled feature selection problems or directly make use of the imperfect labels to guide the selection of representative features. As a result, they may not be able to obtain discriminative features shared across multiple labels. In this article, to bridge the gap between a rich source of multilabel information and its blemish in practical usage, we propose a novel noise-resilient multilabel informed feature selection framework (MIFS) by exploiting the correlations among different labels. In particular, to reduce the negative effects of imperfect label information in obtaining label correlations, we decompose the multilabel information of data instances into a low-dimensional space and then employ the reduced label representation to guide the feature selection phase via a joint sparse regression framework. Empirical studies on both synthetic and real-world datasets demonstrate the effectiveness and efficiency of the proposed MIFS framework. © 2018 ACM.",Feature selection; Label correlations; Multilabel learning; Noise resilient,Classification (of information); Clustering algorithms; Data mining; Learning algorithms; Learning systems; Curse of dimensionality; Dimensionality reduction techniques; Effectiveness and efficiencies; Feature selection algorithm; Feature selection problem; Label correlations; Multi-label learning; Noise resilient; Feature extraction
Combination forecasting reversion strategy for online portfolio selection,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054034165&doi=10.1145%2f3200692&partnerID=40&md5=8f4d490e31f0bc4d828fadfeb4490c23,"Machine learning and artificial intelligence techniques have been applied to construct online portfolio selection strategies recently. A popular and state-of-the-art family of strategies is to explore the reversion phenomenon through online learning algorithms and statistical prediction models. Despite gaining promising results on some benchmark datasets, these strategies often adopt a single model based on a selection criterion (e.g., breakdown point) for predicting future price. However, such model selection is often unstable and may cause unnecessarily high variability in the final estimation, leading to poor prediction performance in real datasets and thus non-optimal portfolios. To overcome the drawbacks, in this article, we propose to exploit the reversion phenomenon by using combination forecasting estimators and design a novel online portfolio selection strategy, named Combination Forecasting Reversion (CFR), which outputs optimal portfolios based on the improved reversion estimator. We further present two efficient CFR implementations based on online Newton step (ONS) and online gradient descent (OGD) algorithms, respectively, and theoretically analyze their regret bounds, which guarantee that the online CFR model performs as well as the best CFR model inhindsight. We evaluate the proposed algorithmson various real markets with extensive experiments. Empirical results show that CFR can effectively overcome the drawbacks of existing reversion strategies and achieve the state-of-the-art performance. © 2018 ACM.",Combination forecasting estimators; Combination forecasting reversion; Mean reversion; Online learning; Portfolio selection,Artificial intelligence; Forecasting; Learning algorithms; Learning systems; Optimization; Artificial intelligence techniques; Combination forecasting; Mean reversion; Online learning; Online learning algorithms; Portfolio selection; State-of-the-art performance; Statistical prediction model; E-learning
Automatic extraction of behavioral paterns for elderly mobility and daily routine analysis,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054034053&doi=10.1145%2f3178116&partnerID=40&md5=d5aa1158ce08b31a7992b301c158247b,"The elderly living in smart homes can have their daily movement recorded and analyzed. As different elders can have their own living habits, a methodology that can automatically identify their daily activities and discover their daily routines will be useful for better elderly care and support. In this article, we focus on automatic detection of behavioral patterns from the trajectory data of an individual for activity identification as well as daily routine discovery. The underlying challenges lie in the need to consider longer-range dependency of the sensor triggering events and spatiotemporal variations of the behavioral patterns exhibited by humans. We propose to represent the trajectory data using a behavior-aware flow graph that is a probabilistic finite state automaton with its nodes and edges attributed with some local behavior-aware features. We identify the underlying subflows as the behavioral patterns using the kernel k-means algorithm. Given the identified activities, we propose a novel nominal matrix factorization method under a Bayesian framework with Lasso to extract highly interpretable daily routines. For empirical evaluation, the proposed methodology has been compared with a number of existing methods based on both synthetic and publicly available real smart home datasets with promising results obtained. We also discuss how the proposed unsupervised methodology can be used to support exploratory behavior analysis for elderly care. © 2018 ACM.",Bayesian inference; Nominal matrix factorization; Probabilistic hierarchical model; Routine pattern discovery,Automation; Bayesian networks; Factorization; Flow graphs; Hierarchical systems; Inference engines; Intelligent buildings; Pattern recognition; Automatic extraction; Bayesian inference; Empirical evaluations; Hierarchical model; Matrix factorizations; Pattern discovery; Probabilistic finite state automaton; Spatio-temporal variation; Matrix algebra
Interactive visual graph mining and learning,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054019790&doi=10.1145%2f3200764&partnerID=40&md5=d59d1ff77027bd3819b4593f34319e1c,"This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques. © 2018 ACM.",Direct manipulation; Higher-order network analysis; Interactive graph generation; Interactive graph learning; Interactive network visualization; Interactive relational machine learning; Interactive role discovery; Interactive visual graph mining; Link prediction; Network analysis; Node embeddings; Rapid visual feedback; Real-time performance; Statistical relational learning; Visual graph analytics,Artificial intelligence; Computer supported cooperative work; Electric network analysis; Forecasting; Information dissemination; Learning systems; Virtual reality; Visual communication; Visualization; Direct manipulation; Embeddings; Graph generation; Higher order networks; Interactive graph learning; Interactive role discovery; Link prediction; Network visualization; Real time performance; Statistical relational learning; Visual feedback; Visual Graph; Visual graph analytics; Graph theory
Dynamic optimization of the level of operational effectiveness of a CSOC under adverse conditions,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047116204&doi=10.1145%2f3173457&partnerID=40&md5=a83cea6dd84e9af765943a9f8f535d02,"The analysts at a cybersecurity operations center (CSOC) analyze the alerts that are generated by intrusion detection systems (IDSs). Under normal operating conditions, sufficient numbers of analysts are available to analyze the alert workload. For the purpose of this article, this means that the cybersecurity analysts in each shift can fully investigate each and every alert that is generated by the IDSs in a reasonable amount of time and perform their normal tasks in a shift. Normal tasks include analysis time, time to attend training programs, report writing time, personal break time, and time to update the signatures on new patterns in alerts as detected by the IDS. There are several disruptive factors that occur randomly and can adversely impact the normal operating condition of a CSOC, such as (1) higher alert generation rates from a few IDSs, (2) new alert patterns that decrease the throughput of the alert analysis process, and (3) analyst absenteeism. The impact of the preceding factors is that the alerts wait for a long duration before being analyzed, which impacts the level of operational effectiveness (LOE) of the CSOC. To return the CSOC to normal operating conditions, the manager of a CSOC can take several actions, such as increasing the alert analysis time spent by analysts in a shift by canceling a training program, spending some of his own time to assist the analysts in alert investigation, and calling upon the on-call analyst workforce to boost the service rate of alerts. However, additional resources are limited in quantity over a 14-day work cycle, and the CSOC manager must determine when and how much action to take in the face of uncertainty, which arises from both the intensity and the random occurrences of the disruptive factors. The preceding decision by the CSOC manager is nontrivial and is often made in an ad hoc manner using prior experiences. This work develops a reinforcement learning (RL) model for optimizing the LOE throughout the entire 14-day work cycle of a CSOC in the face of uncertainties due to disruptive events. Results indicate that the RL model is able to assist the CSOCmanager with a decision support tool to make better decisions than current practices in determining when and how much resource to allocate when the LOE of a CSOC deviates from the normal operating condition. © 2018 ACM.",Absenteeism in shift; Allocate resources; Analysts; Average time to analyze alerts; Cybersecurity; Level of operational effectiveness; Oncall analysts; Reinforcement learning; Resource allocation; Stochastic optimization,Decision support systems; Economics; Intrusion detection; Managers; Optimization; Reinforcement learning; Resource allocation; Uncertainty analysis; Absenteeism in shift; Allocate resources; Analysts; Average time to analyze alerts; Cyber security; Oncall analysts; Operational effectiveness; Stochastic optimizations; Personnel training
Mining significant microblogs for misinformation identification: An attention-based approach,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047169778&doi=10.1145%2f3173458&partnerID=40&md5=e4d0cab5e633bae222550a4b63d99a6b,"With the rapid growth of social media, massive misinformation is also spreading widely on social media, e.g., Weibo and Twitter, and brings negative effects to human life. Today, automatic misinformation identification has drawn attention from academic and industrial communities. Whereas an event on social media usually consists of multiple microblogs, current methods are mainly constructed based on global statistical features. However, information on social media is full of noise, which should be alleviated. Moreover, most of the microblogs about an event have little contribution to the identification of misinformation, where useful information can be easily overwhelmed by useless information. Thus, it is important to mine significant microblogs for constructing a reliable misinformation identification method. In this article, we propose an attention-based approach for identification of misinformation (AIM). Based on the attention mechanism, AIM can select microblogs with the largest attention values for misinformation identification. The attention mechanism in AIM contains two parts: content attention and dynamic attention. Content attention is the calculated-based textual features of each microblog. Dynamic attention is related to the time interval between the posting time of a microblog and the beginning of the event. To evaluate AIM, we conduct a series of experiments on theWeibo and Twitter datasets, and the experimental results show that the proposed AIM model outperforms the state-of-the-art methods. © 2018 ACM.",Attention model; Misinformation identification; Significant microblogs; Social media,Dynamics; Information use; Attention mechanisms; Attention model; Identification method; Industrial communities; Microblogs; Social media; State-of-the-art methods; Statistical features; Social networking (online)
Deep learning for environmentally robust speech recognition: An overview of recent developments,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047117711&doi=10.1145%2f3178115&partnerID=40&md5=443a10a8369c815c102c3e966dda8e04,"Eliminating the negative effect of non-stationary environmental noise is a long-standing research topic for automatic speech recognition but still remains an important challenge. Data-driven supervised approaches, especially the ones based on deep neural networks, have recently emerged as potential alternatives to traditional unsupervised approaches and with sufficient training, can alleviate the shortcomings of the unsupervised methods in various real-life acoustic environments. In this light, we review recently developed, representative deep learning approaches for tackling non-stationary additive and convolutional degradation of speech with the aim of providing guidelines for those involved in the development of environmentally robust speech recognition systems. We separately discuss single- and multi-channel techniques developed for the front-end and back-end of speech recognition systems, as well as joint front-end and back-end training frameworks. In the meanwhile, we discuss the pros and cons of these approaches and provide their experimental results on benchmark databases. We expect that this overview can facilitate the development of the robustness of speech recognition systems in acoustic noisy environments. © 2018 ACM.",Deep learning; Multi-channel speech recognition; Neural networks; Nonstationary noise; Robust speech recognition,Acoustic noise; Deep learning; Deep neural networks; Neural networks; Speech; Acoustic environment; Automatic speech recognition; Environmental noise; Multi channel; Nonstationary noise; Robust speech recognition; Speech recognition systems; Unsupervised approaches; Speech recognition
Multiview discrete hashing for scalable multimedia search,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053414582&doi=10.1145%2f3178119&partnerID=40&md5=d95b4e8d3c0cd4143d0249dc91f20b1f,"Hashing techniques have recently gained increasing research interest in multimedia studies. Most existing hashing methods only employ single features for hash code learning. Multiview data with each view corresponding to a type of feature generally provides more comprehensive information. How to efficiently integrate multiple views for learning compact hash codes still remains challenging. In this article, we propose a novel unsupervised hashing method, dubbed multiview discrete hashing (MvDH), by effectively exploring multiview data. Specifically, MvDH performs matrix factorization to generate the hash codes as the latent representations shared by multiple views, during which spectral clustering is performed simultaneously. The joint learning of hash codes and cluster labels enables that MvDH can generate more discriminative hash codes, which are optimal for classification. An efficient alternating algorithm is developed to solve the proposed optimization problem with guaranteed convergence and low computational complexity. The binary codes are optimized via the discrete cyclic coordinate descent (DCC) method to reduce the quantization errors. Extensive experimental results on three large-scale benchmark datasets demonstrate the superiority of the proposed method over several state-of-the-art methods in terms of both accuracy and scalability. © 2018 ACM.",Hashing; Multi-view; Multimedia search,Clustering algorithms; Codes (symbols); Computational efficiency; Factorization; Comprehensive information; Cyclic coordinate descents; Guaranteed convergence; Hashing; Low computational complexity; Multi-views; Multimedia search; State-of-the-art methods; Hash functions
On incremental high utility sequential patern mining,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054021896&doi=10.1145%2f3178114&partnerID=40&md5=c46766a6a783317422bd507ce3beb85a,"High utility sequential pattern (HUSP) mining is an emerging topic in pattern mining, and only a few algorithms have been proposed to address it. In practice, most sequence databases usually grow over time, and it is inefficient for existing algorithms to mine HUSPs from scratch when databases grow with a small portion of updates. In view of this, we propose the IncUSP-Miner+ algorithm to mine HUSPs incrementally. Specifically, to avoid redundant re-computations, we propose a tighter upper bound of the utility of a sequence, called Tight Sequence Utility (TSU), and then we design a novel data structure, called the candidate pattern tree, to buffer the sequences whose TSU values are greater than or equal to the minimum utility threshold in the original database. Accordingly, to avoid keeping a huge amount of utility information for each sequence, a set of concise utility information is designed to be stored in each tree node. To improve the mining efficiency, several strategies are proposed to reduce the amount of computation for utility update and the scopes of database scans. Moreover, several strategies are also proposed to properly adjust the candidate pattern tree for the support of multiple database updates. Experimental results on some real and synthetic datasets show that IncUSP-Miner+ is able to efficiently mine HUSPs incrementally. © 2018 ACM.",High utility sequential pattern mining; Incremental high utility sequential pattern mining; Incremental mining; Utility mining,Database systems; Miners; Trees (mathematics); Candidate patterns; Emerging topics; Incremental mining; Sequence database; Sequential patterns; Sequential-pattern mining; Synthetic datasets; Utility mining; Data mining
A real-time framework for task assignment in hyperlocal spatial crowdsourcing,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042493216&doi=10.1145%2f3078853&partnerID=40&md5=9a1908b72cc868c4cc96f847d1f8235b,"Spatial Crowdsourcing (SC) is a novel platform that engages individuals in the act of collecting various types of spatial data. This method of data collection can significantly reduce cost and turnover time and is particularly useful in urban environmental sensing, where traditional means fail to provide fine-grained field data. In this study, we introduce hyperlocal spatial crowdsourcing, where all workers who are located within the spatiotemporal vicinity of a task are eligible to perform the task (e.g., reporting the precipitation level at their area and time). In this setting, there is often a budget constraint, either for every time period or for the entire campaign, on the number of workers to activate to perform tasks. The challenge is thus to maximize the number of assigned tasks under the budget constraint despite the dynamic arrivals of workers and tasks.We introduce a taxonomy of several problem variants, such as budget-per-time-period vs. budget-per-campaign and binary-utility vs. distance-based-utility. We study the hardness of the task assignment problem in the offline setting and propose online heuristics which exploit the spatial and temporal knowledge acquired over time. Our experiments are conducted with spatial crowdsourcing workloads generated by the SCAWG tool, and extensive results show the effectiveness and efficiency of our proposed solutions. © 2017 ACM.",Budget constraints; Crowdsensing; GIS; Online task assignment; Participatory sensing; Spatial crowdsourcing,Combinatorial optimization; Crowdsourcing; Geographic information systems; Budget constraint; Crowd sensing; Effectiveness and efficiencies; Environmental sensing; Participatory Sensing; Precipitation level; Task assignment; Temporal knowledge; Budget control
Illiad: InteLLigent invariant and anomaly detection in cyber-physical systems,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042467387&doi=10.1145%2f3066167&partnerID=40&md5=f6069e2809acb9c67d6c597d3a1fecd4,"Cyber-physical systems (CPSs) are today ubiquitous in urban environments. Such systems now serve as the backbone to numerous critical infrastructure applications, from smart grids to IoT installations. Scalable and seamless operation of such CPSs requires sophisticated tools for monitoring the time series progression of the system, dynamically tracking relationships, and issuing alerts about anomalies to operators. We present an online monitoring system (illiad) that models the state of the CPS as a function of its relationships between constituent components, using a combination of model-based and data-driven strategies. In addition to accurate inference for state estimation and anomaly tracking, illiad also exploits the underlying network structure of the CPS (wired or wireless) for state estimation purposes. We demonstrate the application of illiad to two diverse settings: a wireless sensor motes application and an IEEE 33-bus microgrid. © 2018 ACM.",Big-data; IoT; State-estimation; Urban computing; Urban informatics,Big data; Cyber Physical System; Embedded systems; Internet of things; Sensor nodes; State estimation; Wireless sensor networks; Cyber physical systems (CPSs); Infrastructure applications; On-line monitoring system; Tracking relationships; Underlying networks; Urban computing; Urban environments; Urban Informatics; Scalability
Simulating Urban pedestrian crowds of different cultures,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042493369&doi=10.1145%2f3102302&partnerID=40&md5=ce030182bf94535b11d1d655b55735aa,"Models of crowd dynamics are critically important for urban planning and management. They support analysis, facilitate qualitative and quantitative predictions, and synthesize behaviors for simulations. One promising approach to crowd modeling relies on micro-level agent-based simulations, where the interactions of simulated individual agents in the crowd result in macro-level crowd dynamics which are the object of study. This article reports on an agent-based model of urban pedestrian crowds, where culture is explicitly modeled. We extend an established agent-based social agent model, inspired by social psychology, to account for individual cultural attributes discussed in social science literature. We then embed the model in a simulation of pedestrians and explore the resulting macro-level crowd behaviors, such as pedestrian flow, lane changes rate, and so on. We validate the model by quantitatively comparing the simulation results to the pedestrian dynamics in movies of human crowds in five different countries: Iraq, Israel, England, Canada, and France. We conclude that the model can faithfully replicate urban pedestrians in different cultures. Encouraged by these results, we explore simulations of mixed-culture pedestrian crowds.",Agent-based simulation; Crowd modeling; Culture modeling; Pedestrian; Social simulation,Autonomous agents; Computational methods; Dynamics; Software agents; Systems analysis; Agent based simulation; Crowd modeling; Culture modeling; Pedestrian; Social simulations; Behavioral research
Energy usage behavior modeling in energy disaggregation via hawkes processes,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042494611&doi=10.1145%2f3108413&partnerID=40&md5=892c2a23d2f5ff30c50f12982a70efa9,"Energy disaggregation, the task of taking a whole home electricity signal and decomposing it into its component appliances, has been proved to be essential in energy conservation research. One powerful cue for breaking down the entire household's energy consumption is user's daily energy usage behavior, which has so far received little attention: existing works on energy disaggregation mostly ignored the relationship between the energy usages of various appliances by householders across different time slots. The major challenge in modeling such a relationship in that, with ambiguous appliance usage membership of householders, we find it difficult to appropriately model the influence between appliances, since such influence is determined by human behaviors in energy usage. To address this problem,we propose to model the influence between householders' energy usage behaviors directly through a novel probabilistic model, which combines topic models with the Hawkes processes. The proposed model simultaneously disaggregates the whole home electricity signal into each component appliance and infers the appliance usage membership of household members and enables those two tasks to mutually benefit each other. Experimental results on both synthetic data and four real-world data sets demonstrate the effectiveness of our model, which outperforms state-of-the-art approaches in not only decomposing the entire consumed energy to each appliance in houses but also the inference of household structures. We further analyze the inferred appliance-householder assignment and the corresponding influence within the appliance usage of each householder and across different householders, which provides insight into appealing human behavior patterns in appliance usage. © 2018 ACM.",Energy disaggregation; Energy usage behavior; Hawkes process; Household structure analysis; Latent dirichlet allocation,Behavioral research; Signal processing; Statistics; Disaggregation; Energy usage; Household members; Household structures; Human behavior patterns; Latent Dirichlet allocation; Probabilistic modeling; State-of-the-art approach; Energy utilization
GeoBurst+: Effective and real-time local event detection in geo-tagged tweet streams,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042485474&doi=10.1145%2f3066166&partnerID=40&md5=a96f455076fd5c4ce651bcbe1f36d242,"The real-time discovery of local events (e.g., protests, disasters) has been widely recognized as a fundamental socioeconomic task. Recent studies have demonstrated that the geo-tagged tweet stream serves as an unprecedentedly valuable source for local event detection. Nevertheless, how to effectively extract local events from massive geo-tagged tweet streams in real time remains challenging. To bridge the gap, we propose a method for effective and real-time local event detection from geo-tagged tweet streams. Our method, named GeoBurst+, first leverages a novel cross-modal authority measure to identify several pivots in the query window. Such pivots reveal different geo-topical activities and naturally attract similar tweets to form candidate events. GeoBurst+ further summarizes the continuous stream and compares the candidates against the historical summaries to pinpoint truly interesting local events. Better still, as the query window shifts, GeoBurst+ is capable of updating the event list with little time cost, thus achieving continuous monitoring of the stream. We used crowdsourcing to evaluate GeoBurst+ on two million-scale datasets and found it significantly more effective than existing methods while being orders of magnitude faster. © 2018 ACM.",Data stream; Event detection; Local event; Location-based service; Social media; Spatiotemporal data mining,Data mining; Telecommunication services; Data stream; Event detection; Local event; Social media; Spatio-temporal data mining; Location based services
Risk-sensitive stochastic orienteering problems for trip optimization in urban environments,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042486348&doi=10.1145%2f3080575&partnerID=40&md5=93852292f6dd3791d46663b183587f82,"Orienteering Problems (OPs) are used to model many routing and trip planning problems. OPs are a variant of the well-known traveling salesman problem where the goal is to compute the highest reward path that includes a subset of vertices and has an overall travel time less than a specified deadline. However, the applicability of OPs is limited due to the assumption of deterministic and static travel times. To that end, Campbell et al. extended OPs to Stochastic OPs (SOPs) to represent uncertain travel times (Campbell et al. 2011). In this article, we make the following key contributions: (1) We extend SOPs to Dynamic SOPs (DSOPs), which allow for time-dependent travel times; (2) we introduce a new objective criterion for SOPs and DSOPs to represent a percentile measure of risk; (3) we provide non-linear optimization formulations along with their linear equivalents for solving the risk-sensitive SOPs and DSOPs; (4) we provide a local search mechanism for solving the risk-sensitive SOPs and DSOPs; and (5) we provide results on existing benchmark problems and a real-world theme park trip planning problem. © 2018 ACM.",Orienteering problems; Risk-sensitive optimization; Sample average approximation,Nonlinear programming; Optimization; Risk assessment; Stochastic systems; Traveling salesman problem; Bench-mark problems; Non-linear optimization; Objective criteria; Orienteering problem; Sample average approximation; Time dependent; Trip planning; Urban environments; Travel time
Relation lines: Visual reasoning of egocentric relations from heterogeneous urban data,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053198609&doi=10.1145%2f3200766&partnerID=40&md5=8437800bdc1c0f2775ee7039ba1f374b,"The increased accessibility of urban sensor data and the popularity of social network applications is enabling the discovery of crowd mobility and personal communication patterns. However, studying the egocentric relationships of an individual can be very challenging because available data may refer to direct contacts, such as phone calls between individuals, or indirect contacts, such as paired location presence. In this article, we develop methods to integrate three facets extracted from heterogeneous urban data (timelines, calls, and locations) through a progressive visual reasoning and inspection scheme. Our approach uses a detect-andfilter scheme such that, prior to visual refinement and analysis, a coarse detection is performed to extract the target individual and construct the timeline of the target. It then detects spatio-temporal co-occurrences or call-based contacts to develop the egocentric network of the individual. The filtering stage is enhanced with a line-based visual reasoning interface that facilitates a flexible and comprehensive investigation of egocentric relationships and connections in terms of time, space, and social networks. The integrated system, RelationLines, is demonstrated using a dataset that contains taxi GPS data, cell-base mobility data, mobile calling data,microblog data, and point-of-interest (POI) data from a city withmillions of citizens.We examine the effectiveness and efficiency of our system with three case studies and user review. © 2018 Association for Computing Machinery.",egocentric relations; heterogeneous urban data; Location-based; timeline; visual reasoning,Location; Taxicabs; egocentric relations; heterogeneous urban data; Location based; timeline; Visual reasoning; Social networking (online)
Evolutionary strategy to perform batch-mode active learning on multi-label data,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041467518&doi=10.1145%2f3161606&partnerID=40&md5=5b6122d1616622833322f23e1d8d6c95,"Multi-label learning has become an important area of research owing to the increasing number of real-world problems that contain multi-label data. Data labeling is an expensive process that requires expert handling. The annotation of multi-label data is laborious since a human expert needs to consider the presence/absence of each possible label. Consequently, numerous modern multi-label problems may involve a small number of labeled examples and plentiful unlabeled examples simultaneously. Active learning methods allow us to induce better classifiers by selecting the most useful unlabeled data, thus considerably reducing the labeling effort and the cost of training an accurate model. Batch-mode active learning methods focus on selecting a set of unlabeled examples in each iteration in such a way that the selected examples are informative and as diverse as possible. This article presents a strategy to perform batch-mode active learning on multi-label data. The batch-mode active learning is formulated as a multi-objective problem, and it is solved by means of an evolutionary algorithm. Extensive experiments were conducted in a large collection of datasets, and the experimental results confirmed the effectiveness of our proposal for better batch-mode multi-label active learning. © 2018 ACM.",Batch-mode active learning; Evolutionary algorithm; Multi-label learning; Multi-objective problem,Artificial intelligence; Evolutionary algorithms; Iterative methods; Learning systems; Active learning methods; Batch mode active learning; Evolutionary strategies; Multi-label learning; Multi-label problems; Multi-objective problem; Real-world problem; Selected examples; Learning algorithms
Qick bootstrapping of a Personalized gaze model from real-use interactions,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041481678&doi=10.1145%2f3156682&partnerID=40&md5=91c8efcaa6e61362875647ab9483b4e6,"Understanding human visual attention is essential for understanding human cognition, which in turn benefits human-computer interaction. Recent work has demonstrated a Personalized, Auto-Calibrating Eye-tracking (PACE) system, which makes it possible to achieve accurate gaze estimation using only an off-the-shelf webcam by identifying and collecting data implicitly from user interaction events. However, this method is constrained by the need for large amounts of well-annotated data. We thus present fast-PACE, an adaptation to PACE that exploits knowledge from existing data from different users to accelerate the learning speed of the personalized model. The result is an adaptive, data-driven approach that continuously ""learns"" its user and recalibrates, adapts, and improves with additional usage by a user. Experimental evaluations of fast-PACE demonstrate its competitive accuracy in iris localization, validity of alignment identification between gaze and interactions, and effectiveness of gaze transfer. In general, fast-PACE achieves an initial visual error of 3.98 degrees and then steadily improves to 2.52 degrees given incremental interaction-informed data. Our performance is comparable to state-of-the-art, but without the need for explicit training or calibration. Our technique addresses the data quality and quantity problems. It therefore has the potential to enable comprehensive gaze-aware applications in the wild. © 2018 ACM.",Data validation; Gaze estimation; Gaze transfer learning; Gaze-interaction alignment; Implicit modeling,Behavioral research; Data validation; Gaze estimation; Gaze interaction; Implicit model; Transfer learning; Human computer interaction
Virtual metering: An efficient water disaggregation algorithm via nonintrusive load monitoring,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041453563&doi=10.1145%2f3141770&partnerID=40&md5=a4672b85fc100ac9f184f462079fb5cf,"The scarcity of potable water is a critical challenge in many regions around the world. Previous studies have shown that knowledge of device-level water usage can lead to significant conservation. Although there is considerable interest in determining discriminative features via sparse coding for water disaggregation to separate whole-house consumption into its component appliances, existing methods lack a mechanism for fitting coefficient distributions and are thus unable to accurately discriminate parallel devices' consumption. This article proposes a Bayesian discriminative sparse coding model, referred to as Virtual Metering (VM), for this disaggregation task. Mixture-of-Gammas is employed for the prior distribution of coefficients, contributing two benefits: (i) guaranteeing the coefficients' sparseness and non-negativity, and (ii) capturing the distribution of active coefficients. The resulting method effectively adapts the bases to aggregated consumption to facilitate discriminative learning in the proposed model, and devices' shape features are formalized and incorporated into Bayesian sparse coding to direct the learning of basis functions. Compact Gibbs Sampling (CGS) is developed to accelerate the inference process by utilizing the sparse structure of coefficients. The empirical results obtained from applying the new model to large-scale real and synthetic datasets revealed that VM significantly outperformed the benchmark methods. © 2018 ACM.",Bayesian discriminative learning; Computational sustainability; Low-sampling-rate disaggregation; Mixture-of-Gammas; Non-intrusive load monitoring; Sparse coding,Mixtures; Potable water; Virtual machine; Computational sustainability; Disaggregation; Discriminative learning; Nonintrusive load monitoring; Sparse coding; Codes (symbols)
Fuzzy cognitive diagnosis for modelling examinee performance,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042472138&doi=10.1145%2f3168361&partnerID=40&md5=5b09bd18a26022a1753557a55f3bc76d,"Recent decades have witnessed the rapid growth of educational data mining (EDM), which aims at automatically extracting valuable information from large repositories of data generated by or related to people's learning activities in educational settings. One of the key EDM tasks is cognitive modelling with examination data, and cognitive modelling tries to profile examinees by discovering their latent knowledge state and cognitive level (e.g. the proficiency of specific skills). However, to the best of our knowledge, the problem of extracting information from both objective and subjective examination problems to achieve more precise and interpretable cognitive analysis remains underexplored. To this end, we propose a fuzzy cognitive diagnosis framework (FuzzyCDF) for examinees' cognitive modelling with both objective and subjective problems. Specifically, to handle the partially correct responses on subjective problems, we first fuzzify the skill proficiency of examinees. Then we combine fuzzy set theory and educational hypotheses to model the examinees' mastery on the problems based on their skill proficiency. Finally, we simulate the generation of examination score on each problem by considering slip and guess factors. In this way, the whole diagnosis framework is built. For further comprehensive verification, we apply our FuzzyCDF to three classical cognitive assessment tasks, i.e., predicting examinee performance, slip and guess detection, and cognitive diagnosis visualization. Extensive experiments on three real-world datasets for these assessment tasks prove that FuzzyCDF can reveal the knowledge states and cognitive level of the examinees effectively and interpretatively. © 2018 ACM.",Cognitive; Educational data mining; Graphic model,Fuzzy set theory; Cognitive; Cognitive assessments; Cognitive diagnosis; Educational data mining; Educational data minings (EDM); Educational settings; Extracting information; Graphic models; Data mining
Sparse Passive-Aggressive learning for bounded online kernel methods,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041480905&doi=10.1145%2f3156684&partnerID=40&md5=2a8cc088cf4e346712fa7658e4a9e4fc,"One critical deficiency of traditional online kernel learning methods is their unbounded and growing number of support vectors in the online learning process, making them inefficient and non-scalable for large-scale applications. Recent studies on scalable online kernel learning have attempted to overcome this shortcoming, e.g., by imposing a constant budget on the number of support vectors. Although they attempt to bound the number of support vectors at each online learning iteration, most of them fail to bound the number of support vectors for the final output hypothesis, which is often obtained by averaging the series of hypotheses over all the iterations. In this article, we propose a novel framework for bounded online kernel methods, named ""Sparse Passive-Aggressive (SPA)"" learning, which is able to yield a final output kernel-based hypothesis with a bounded number of support vectors. Unlike the common budget maintenance strategy used by many existing budget online kernel learning approaches, the idea of our approach is to attain the bounded number of support vectors using an efficient stochastic sampling strategy that samples an incoming training example as a new support vector with a probability proportional to its loss suffered. We theoretically prove that SPA achieves an optimal mistake bound in expectation, and we empirically show that it outperforms various budget online kernel learning algorithms. Finally, in addition to general online kernel learning tasks, we also apply SPA to derive bounded online multiple-kernel learning algorithms, which can significantly improve the scalability of traditional Online Multiple-Kernel Classification (OMKC) algorithms while achieving satisfactory learning accuracy as compared with the existing unbounded OMKC algorithms. © 2018 ACM.",Kernel methods; Online learning; Online multiple-kernel learning,Budget control; E-learning; Iterative methods; Learning systems; Stochastic systems; Support vector machines; Vectors; Kernel methods; Large-scale applications; Maintenance strategies; Multiple Kernel Learning; Online kernel learning; Online learning; Probability proportional; Stochastic sampling; Learning algorithms
Concept and attention-based CNN for question retrieval in multi-view learning,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042507138&doi=10.1145%2f3151957&partnerID=40&md5=156f98f414e8f8314d8cf028b48be440,"Question retrieval, which aims to find similar versions of a given question, is playing a pivotal role in various question answering (QA) systems. This task is quite challenging, mainly in regard to five aspects: synonymy, polysemy, word order, question length, and data sparsity. In this article, we propose a unified framework to simultaneously handle these five problems. We use the word combined with corresponding concept information to handle the synonymy problem and the polysemous problem. Concept embedding and word embedding are learned at the same time from both the context-dependent and context-independent views. To handle the word-order problem, we propose a high-level feature-embedded convolutional semantic model to learn question embedding by inputting concept embedding and word embedding. Due to the fact that the lengths of some questions are long, we propose a value-based convolutional attentional method to enhance the proposed high-level feature-embedded convolutional semantic model in learning the key parts of the question and the answer. The proposed high-level feature-embedded convolutional semantic model nicely represents the hierarchical structures of word information and concept information in sentences with their layer-by-layer convolution and pooling. Finally, to resolve data sparsity, we propose using the multi-view learning method to train the attention-based convolutional semantic model on question-answer pairs. To the best of our knowledge, we are the first to propose simultaneously handling the above five problems in question retrieval using one framework. Experiments on three real question-answering datasets show that the proposed framework significantly outperforms the state-of-the-art solutions. © 2018 ACM.",Concept embedding; Question embedding; Question retrieval; Value-based convolutional attentional method,Semantics; Concept embedding; Hierarchical structures; High-level features; Question answering systems; Question retrievals; Question-answer pairs; Question-embedding; Value-based; Convolution
Modeling queries with contextual snippets for information retrieval,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042472832&doi=10.1145%2f3161607&partnerID=40&md5=4917415af2d5943405ab795a0ce1e0b7,"Query expansion under the pseudo-relevance feedback (PRF) framework has been extensively studied in information retrieval. However, most expansion methods are mainly based on the statistics of single terms, which can generate plenty of irrelevant query terms and decrease retrieval performance. To alleviate this problem, we propose an approach that adapts the PRF-based contextual snippets into a context-aware topic model to enhance query representations. Specifically, instead of selecting a series of independent terms, we make full use of the query contextual information and focus on the snippets with the length of n in the PRF documents. Furthermore, we propose a context-aware topic (CAT) model to mine the topic distributions of the query-relevant snippets, namely, fine contextual snippets. In contrast to the traditional topic models that infer the topics from the whole corpus, we establish a bridge between the snippets and the corresponding PRF documents, which can be used for modeling the topics more precisely and efficiently. Finally, the topic distributions of the fine snippets are used for context-aware and topic-sensitive query representations. To evaluate the performance of our approach, we integrate the obtained queries into a topic-based hybrid retrieval model and conduct extensive experiments on various TREC collections. The experimental results show that our query-modeling approach is more effective in boosting retrieval performance compared with the state-of-the-art methods. © 2018 ACM.",Contextual snippet; Query representation; Topic modeling,Image retrieval; Natural language processing systems; Contextual information; Contextual snippet; Pseudo-relevance feedbacks; Query representations; Retrieval performance; State-of-the-art methods; Topic distributions; Topic Modeling; Information retrieval
A novel image-centric approach toward direct volume rendering,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041490166&doi=10.1145%2f3152875&partnerID=40&md5=659029697bd5bc1bf94aadd270d42e79,"Transfer function (TF) generation is a fundamental problem in direct volume rendering (DVR). A TF maps voxels to color and opacity values to reveal inner structures. Existing TF tools are complex and unintuitive for the users who are more likely to be medical professionals than computer scientists. In this article, we propose a novel image-centric method for TF generation where instead of complex tools, the user directly manipulates volume data to generate DVR. The user's work is further simplified by presenting only the most informative volume slices for selection. Based on the selected parts, the voxels are classified using our novel sparse nonparametric support vector machine classifier, which combines both local and near-global distributional information of the training data. The voxel classes are mapped to aesthetically pleasing and distinguishable color and opacity values using harmonic colors. Experimental results on several benchmark datasets and a detailed user survey show the effectiveness of the proposed method. © 2018 ACM.",Intelligent systems; Medical imaging; Pattern recognition; Support vector machine; Volume visualization,Classification (of information); Color; Intelligent systems; Opacity; Pattern recognition; Pattern recognition systems; Support vector machines; Volume rendering; Benchmark datasets; Computer scientists; Direct volume rendering; Distributional information; Inner structure; Medical professionals; Support vector machine classifiers; Volume visualization; Medical imaging
A Bayesian approach to intervention-based clustering,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041489843&doi=10.1145%2f3156683&partnerID=40&md5=472fbbcfc683bc85ac2aeb2b5dd9bab0,"An important task for intelligent healthcare systems is to predict the effect of a new intervention on individuals. This is especially true for medical treatments. For example, consider patients who do not respond well to a new drug or have adversary reactions. Predicting the likelihood of positive or negative response before trying the drug on the patient can potentially save his or her life. We are therefore interested in identifying distinctive subpopulations that respond differently to a given intervention. For this purpose, we have developed a novel technique, Intervention-based Clustering, based on a Bayesian mixture model. Compared to the baseline techniques, the novelty of our approach lies in its ability to model complex decision boundaries by using soft clustering, thus predicting the effect for individuals more accurately. It can also incorporate prior knowledge, making the method useful even for smaller datasets. We demonstrate how our method works by applying it to both simulated and real data. Results of our evaluation show that our model has strong predictive power and is capable of producing high-quality clusters compared to the baseline methods. © 2018 ACM.",Bayesian analysis; Clustering; Cross-validation; Heterogeneous treatment effects; Mixture model; Personalized medicine; Randomized controlled trial; Subgroup analysis,Bayesian networks; Forecasting; Mixtures; Bayesian Analysis; Clustering; Cross validation; Mixture model; Personalized medicines; Randomized controlled trial; Subgroup analysis; Treatment effects; Quality control
"A review of co-saliency detection algorithms: Fundamentals, applications, and challenges",2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041486636&doi=10.1145%2f3158674&partnerID=40&md5=d1cb1349a951a10b9bc89e6e1bf2ba30,"Co-saliency detection is a newly emerging and rapidly growing research area in the computer vision community. As a novel branch of visual saliency, co-saliency detection refers to the discovery of common and salient foregrounds from two or more relevant images, and it can be widely used in many computer vision tasks. The existing co-saliency detection algorithms mainly consist of three components: extracting effective features to represent the image regions, exploring the informative cues or factors to characterize co-saliency and designing effective computational frameworks to formulate co-saliency. Although numerous methods have been developed, the literature is still lacking a deep review and evaluation of co-saliency detection techniques. In this article, we aim at providing a comprehensive review of the fundamentals, challenges, and applications of co-saliency detection. Specifically, we provide an overview of some related computer vision works, review the history of co-saliency detection, summarize and categorize the major algorithms in this research area, discuss some open issues in this area, present the potential applications of co-saliency detection, and finally point out some unsolved challenges and promising future works. We expect this review to be beneficial to both fresh and senior researchers in this field and to give insights to researchers in other related areas regarding the utility of co-saliency detection algorithms. © 2018 ACM.",(Co-)saliency detection; Computer vision; Image understanding,Image understanding; Signal detection; Co saliencies; Computational framework; Image regions; Saliency detection; Three component; Vision communities; Visual saliency; Computer vision
A multi-label multi-view learning framework for in-app service usage analysis,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041485702&doi=10.1145%2f3151937&partnerID=40&md5=862bcbd00f71b806096aac2171572fd6,"The service usage analysis, aiming at identifying customers' messaging behaviors based on encrypted App traffic flows, has become a challenging and emergent task for service providers. Prior literature usually starts from segmenting a traffic sequence into single-usage subsequences, and then classify the subsequences into different usage types. However, they could suffer from inaccurate traffic segmentations and mixed-usage subsequences. To address this challenge, we exploit a multi-label multi-view learning strategy and develop an enhanced framework for in-App usage analytics. Specifically, we first devise an enhanced traffic segmentation method to reduce mixed-usage subsequences. Besides, we develop a multi-label multi-view logistic classification method, which comprises two alignments. The first alignment is to make use of the classification consistency between packet-length view and time-delay view of traffic subsequences and improve classification accuracy. The second alignment is to combine the classification of single-usage subsequence and the post-classification of mixed-usage subsequences into a unified multi-label logistic classification problem. Finally, we present extensive experiments with real-world datasets to demonstrate the effectiveness of our approach. We find that the proposed multi-label multi-view framework can help overcome the pain of mixedusage subsequences and can be generalized to latent activity analysis in sequential data, beyond in-App usage analytics. © 2018 ACM.",In-app analytics; Internet traffic; Multi-label; Multi-view; Service usage,Alignment; Classification accuracy; Classification consistency; Classification methods; Internet traffic; Multi-label; Multi-views; Segmentation methods; Service usage; E-learning
Understanding and identifying rhetorical questions in social media,2018,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042509070&doi=10.1145%2f3108364&partnerID=40&md5=32106f1b0c31c110bde99e14b2a101e5,"Social media provides a platform for seeking information from a large user base. Information seeking in social media, however, occurs simultaneously with users expressing their viewpoints by making statements. Rhetorical questions have the form of a question but serve the function of a statement and are an important tool employed by users to express their viewpoints. Therefore, rhetorical questions might mislead platforms assisting information seeking in social media. It becomes difficult to identify rhetorical questions as they are not syntactically different from other questions. In this article, we develop a framework to identify rhetorical questions by modeling some motivations of the users to post them.We focus on two motivations of the users drawing from linguistic theories to implicitly convey a message and to modify the strength of a statement previously made. We develop a quantitative framework from these motivations to identify rhetorical questions in social media. We evaluate the framework using two datasets of questions posted on a social media platform Twitter and demonstrate its effectiveness in identifying rhetorical questions. This is the first framework, to the best of our knowledge, to model the possible motivations for posting rhetorical questions to identify them on social media platforms. © 2018 ACM.",Information seeking; Persuasion; Political campaigns; Question answering; Rhetorical; Social media,Information use; Motivation; Information seeking; Persuasion; Political campaign; Question Answering; Rhetorical; Social media; Social networking (online)
Social bridges in urban purchase behavior,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041451043&doi=10.1145%2f3149409&partnerID=40&md5=11855f98998f819064dd90715c466d24,"The understanding and modeling of human purchase behavior in city environment can have important implications in the study of urban economy and in the design and organization of cities. In this article, we study human purchase behavior at the community level and argue that people who live in different communities but work at close-by locations could act as ""social bridges"" between the respective communities and that they are correlated with similarity in community purchase behavior. We provide empirical evidence by studying millions of credit card transaction records for tens of thousands of individuals in a city environment during a period of three months. More specifically, we show that the number of social bridges between communities is a much stronger indicator of similarity in their purchase behavior than traditionally considered factors such as income and sociodemographic variables. Our findings also suggest that such an effect varies across different merchant categories, that the presence of female customers in social bridges is a stronger indicator compared to that of their male counterparts, and that there seems to be a geographical constraint for this effect, all of which may have implications in the studies of urban economy and data-driven urban planning.",Credit card transaction; Physical environment; Purchase behavior; Social bridge,Sales; Credit card transactions; Data driven; Physical environments; Socio-demographic variables; Urban economy; Behavioral research
Scalable urban mobile crowdsourcing: Handling uncertainty inworker movement,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041462217&doi=10.1145%2f3078842&partnerID=40&md5=358f1eab0027812ea9117098fec10fb2,"In this article, we investigate effective ways of utilizing crowdworkers in providing various urban services. The task recommendation platform that we design can match tasks to crowdworkers based on workers' historical trajectories and time budget limits, thus making recommendations personal and efficient. One major challenge we manage to address is the handling of crowdworker's trajectory uncertainties. In this article, we explicitly allow multiple routine routes to be probabilistically associated with each worker.We formulate this problem as an integer linear program whose goal is to maximize the expected total utility achieved by all workers. We further exploit the separable structures of the formulation and apply the Lagrangian relaxation technique to scale up computation. Numerical experiments have been performed over the instances generated using the realistic public transit dataset in Singapore. The results show that we can find significantly better solutions than the deterministic formulation, and in most cases we can find solutions that are very close to the theoretical performance limit. To demonstrate the practicality of our approach, we deployed our recommendation engine to a campus-scale field trial, and we demonstrate that workers receiving our recommendations incur fewer detours and complete more tasks, and are more efficient against workers relying on their own planning (25% more for top workers who receive recommendations). This is achieved despite having highly uncertain worker trajectories. We also demonstrate how to further improve the robustness of the system by using a simple multi-coverage mechanism.",Context-aware; Empirical study; Mobile crowdsourcing; Participatory sensing; Spatial crowdsourcing; Uncertainty modeling; User behavior,Budget control; Crowdsourcing; Integer programming; Recommender systems; Trajectories; Uncertainty analysis; Context-Aware; Empirical studies; Mobile crowdsourcing; Participatory Sensing; Uncertainty modeling; User behaviors; Behavioral research
A comfort-based approach to smart heating and air conditioning,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041460448&doi=10.1145%2f3057730&partnerID=40&md5=90c4f12c4edbca25d5d3cda6ddf51bee,"In this article, we address the interrelated challenges of predicting user comfort and using this to reduce energy consumption in smart heating, ventilation, and air conditioning (HVAC) systems. At present, such systems use simple models of user comfort when deciding on a set-point temperature. Being built using broad population statistics, these models generally fail to represent individual users' preferences, resulting in poor estimates of the users' preferred temperatures. To address this issue, we propose the Bayesian Comfort Model (BCM). This personalised thermal comfort model uses a Bayesian network to learn from a user's feedback, allowing it to adapt to the users' individual preferences over time.We further propose an alternative to the ASHRAE 7-point scale used to assess user comfort. Using this model, we create an optimal HVAC control algorithm that minimizes energy consumption while preserving user comfort. Through an empirical evaluation based on the ASHRAE RP-884 dataset and data collected in a separate deployment by us, we show that our model is consistently 13.2% to 25.8% more accurate than current models and how using our alternative comfort scale can increase our model's accuracy. Through simulations we show that using this model, our HVAC control algorithm can reduce energy consumption by 7.3% to 13.5% while decreasing user discomfort by 24.8% simultaneously.",Agents; Computational sustainability; Machine learning; User behaviour modeling and learning,Agents; Bayesian networks; Behavioral research; Climate control; Energy utilization; Learning systems; Population statistics; Computational sustainability; Empirical evaluations; Individual preference; Reduce energy consumption; Set-point temperatures; Thermal comfort models; User behaviour; User's feedbacks; Air conditioning
A multiagent-based approach for vehicle routing by considering both arriving on time and total travel time,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041433057&doi=10.1145%2f3078847&partnerID=40&md5=a60e394d40e62c381f19f4e14c96beb9,"Arriving on time and total travel time are two important properties for vehicle routing. Existing route guidance approaches always consider them independently, because they may conflict with each other. In this article, we develop a semi-decentralized multiagent-based vehicle routing approach where vehicle agents follow the local route guidance by infrastructure agents at each intersection, and infrastructure agents perform the route guidance by solving a route assignment problem. It integrates the two properties by expressing them as two objective terms of the route assignment problem. Regarding arriving on time, it is formulated based on the probability tail model, which aims to maximize the probability of reaching destination before deadline. Regarding total travel time, it is formulated as a weighted quadratic term, which aims to minimize the expected travel time from the current location to the destination based on the potential route assignment. The weight for total travel time is designed to be comparatively large if the deadline is loose. Additionally, we improve the proposed approach in two aspects, including travel time prediction and computational efficiency. Experimental results on real road networks justify its ability to increase the average probability of arriving on time, reduce total travel time, and enhance the overall routing performance. © 2017 ACM.",Arriving on time; Intelligent transportation systems; Multiagent-based route guidance; Probability tail model; Total travel time,Air navigation; Combinatorial optimization; Computational efficiency; Intelligent systems; Multi agent systems; Probability; Transportation routes; Vehicle routing; Vehicles; Arriving on time; Assignment problems; Intelligent transportation systems; Multi-agent based approach; Real road networks; Route guidance; Routing performance; Travel time prediction; Travel time
Personalized air travel prediction: A multi-factor perspective,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041447678&doi=10.1145%2f3078845&partnerID=40&md5=e38364ea7758cfab53734ee6fe59bcfe,"Human mobility analysis is one of the most important research problems in the field of urban computing. Existing research mainly focuses on the intra-city ground travel behavior modeling, while the inter-city air travel behavior modeling has been largely ignored. Actually, the inter-city travel analysis can be of equivalent importance and complementary to the intra-city travel analysis. Understanding massive passenger-airtravel behavior delivers intelligence for airlines' precision marketing and related socioeconomic activities, such as airport planning, emergency management, local transportation planning, and tourism-related businesses. Moreover, it provides opportunities to study the characteristics of cities and the mutual relationships between them. However, modeling and predicting air traveler behavior is challenging due to the complex factors of the market situation and individual characteristics of customers (e.g., airlines' market share, customer membership, and travelers' intrinsic interests on destinations). To this end, in this article, we present a systematic study on the personalized air travel prediction problem, namely where a customer will fly to and which airline carrier to fly with, by leveraging real-world anonymized Passenger Name Record (PNR) data. Specifically, we first propose a relational travel topic model, which combines the merits of latent factor model with a neighborhood-based method, to uncover the personal travel preferences of aviation customers and the latent travel topics of air routes and airline carriers simultaneously. Then we present a multi-factor travel prediction framework, which fuses complex factors of the market situation and individual characteristics of customers, to predict airline customers' personalized travel demands. Experimental results on two real-world PNR datasets demonstrate the effectiveness of our approach on both travel topic discovery and customer travel prediction. © 2017 ACM.",Air travel demand; Latent dirichlet allocation; Travel topic model; Urban computing,Civil aviation; Commerce; Competition; Data mining; Forecasting; Indexing (of information); Risk management; Sales; Statistics; Transportation; Air travels; Individual characteristics; Latent Dirichlet allocation; Neighborhood-based method; Topic Modeling; Transportation planning; Travel behavior modeling; Urban computing; Air transportation
Using online geotagged and crowdsourced data to understand human offline behavior in the city: An economic perspective,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041823295&doi=10.1145%2f3078851&partnerID=40&md5=bdab5baefa1da4163683eec33e4e5481,"The pervasiveness of mobile technologies today has facilitated the creation of massive online crowdsourced and geotagged data from individual users at different locations in a city. Such ubiquitous user-generated data allow us to study the social and behavioral trajectories of individuals across both digital and physical environments. This information, combined with traditional economic and behavioral indicators in the city (e.g., store purchases, restaurant visits, parking), can help us better understand human behavior and interactions with cities. In this study, we take an economic perspective and focus on understanding human economic behavior in the city by examining the performance of local businesses based on the values learned from crowsourced and geotagged data. Specifically, we extract multiple traffic and human mobility features from publicly available data source geomapping and geo-social-tagging techniques and examine the effects of both static and dynamic features on booking volume of local restaurants. Our study is instantiated on a unique dataset of restaurant bookings fromOpenTable for 3,187 restaurants in New York City from November 2013 to March 2014. Our results suggest that foot traffic can increase local popularity and business performance, while mobility and traffic from automobiles may hurt local businesses, especially the well-established chains and high-end restaurants.We also find that, on average, one or more street closure (caused by events or construction projects) nearby leads to a 4.7% decrease in the probability of a restaurant being fully booked during the dinner peak. Our study demonstrates the potential to best make use of the large volumes and diverse sources of crowdsourced and geotagged user-generated data to create matrices to predict local economic demand in a manner that is fast, cheap, accurate, and meaningful.",City demand; Crowdsourced user behavior; Econometric analysis; Econometrics; Geotagged social media; Location-based service; Mobility analytic,Economic analysis; Economics; Location based services; Social networking (online); Statistics; Telecommunication services; City demand; Econometric analysis; Econometrics; Social media; User behaviors; Behavioral research
Spotting trip purposes from taxi trajectories: A general probabilistic model,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041446963&doi=10.1145%2f3078849&partnerID=40&md5=8e7e72569f7f1cddf05fb832724ce626,"What is the purpose of a trip? What are the unique human mobility patterns and spatial contexts in or near the pickup points and delivery points of trajectories for a specific trip purpose? Many prior studies have modeled human mobility patterns in urban regions; however, these analytics mainly focus on interpreting the semantic meanings of geographic topics at an aggregate level. Given the lack of information about human activities at pick-up and dropoff points, it is challenging to convert the prior studies into effective tools for inferring trip purposes. To address this challenge, in this article, we study large-scale taxi trajectories from an unsupervised perspective in light of the following observations. First, the POI configurations of origin and destination regions closely relate to the urban functionality of these regions and further indicate various human activities. Second, with respect to the functionality of neighborhood environments, trip purposes can be discerned from the transitions between regions with different functionality at particular time periods. Along these lines, we develop a general probabilistic framework for spotting trip purposes from massive taxi GPS trajectories. Specifically, we first augment the origin and destination regions of trajectories by attaching neighborhood POIs. Then, we introduce a latent factor, POI Topic, to represent the mixed functionality of the regions, such that each origin or destination point in the city can be modeled as a mixture over POI Topics. In addition, considering the transitions from origins to destinations at specific time periods, the trip time is generated collaboratively from the pairwise POI Topics at both ends of the O-D pairs, constituting POI Links, and hence the trip purpose can be explained semantically by the POI Links. Finally, we present extensive experiments with the real-world data of New York City to demonstrate the effectiveness of our proposed method for spotting trip purposes, and moreover, the model is validated to perform well in predicting the destinations and trip time among all the baseline methods. © 2017 ACM.",Human mobility; Probabilistic model; Taxi trajectories; Trip purposes,Pickups; Semantics; Taxicabs; Baseline methods; Destination points; Human mobility; Neighborhood environment; Origin and destinations; Probabilistic framework; Probabilistic modeling; Trip purposes; Trajectories
Real-time human mobility modeling with multi-view learning,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041442911&doi=10.1145%2f3092692&partnerID=40&md5=10ac07672cb369ec0a4b18f50d184014,"Real-time human mobility modeling is essential to various urban applications. To model such human mobility, numerous data-driven techniques have been proposed. However, existing techniques are mostly driven by data from a single view, for example, a transportation view or a cellphone view, which leads to overfitting of these single-view models. To address this issue, we propose a human mobility modeling technique based on a generic multi-view learning framework called coMobile. In coMobile, we first improve the performance of single-view models based on tensor decomposition with correlated contexts, and then we integrate these improved single-view models together for multi-view learning to iteratively obtain mutually reinforced knowledge for real-time human mobility at urban scale.We implement coMobile based on an extremely large dataset in the Chinese city Shenzhen, including data about taxi, bus, and subway passengers along with cellphone users, capturing more than 27 thousand vehicles and 10 million urban residents. The evaluation results show that our approach outperforms a single-view model by 51% on average. More importantly, we design a novel application where urban taxis are dispatched based on unaccounted mobility demand inferred by coMobile. © 2017 ACM.",Mobility model; Model integration; Smart cities,Smart city; Taxicabs; Telephone sets; Data driven technique; Mobility model; Model integration; Multi-view learning; Novel applications; Single view modeling; Tensor decomposition; Urban applications; Iterative methods
A data mining approach to assess privacy risk in human mobility data,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041486892&doi=10.1145%2f3106774&partnerID=40&md5=1f0b5c3b706028f8230eeeff8e5610f3,"Human mobility data are an important proxy to understand human mobility dynamics, develop analytical services, and design mathematical models for simulation and what-if analysis. Unfortunately mobility data are very sensitive since they may enable the re-identification of individuals in a database. Existing frameworks for privacy risk assessment provide data providers with tools to control and mitigate privacy risks, but they suffer two main shortcomings: (i) they have a high computational complexity; (ii) the privacy risk must be recomputed every time new data records become available and for every selection of individuals, geographic areas, or time windows. In this article, we propose a fast and flexible approach to estimate privacy risk in human mobility data. The idea is to train classifiers to capture the relation between individual mobility patterns and the level of privacy risk of individuals. We show the effectiveness of our approach by an extensive experiment on real-world GPS data in two urban areas and investigate the relations between human mobility patterns and the privacy risk of individuals.",Data mining; Human mobility; Privacy,Data mining; Risk assessment; Risk perception; Risks; Geographic areas; Human mobility; Individual mobility; Mobility datum; Privacy risks; Re identifications; Time windows; What-if Analysis; Data privacy
Vertical ensemble co-training for text classification,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033241806&doi=10.1145%2f3137114&partnerID=40&md5=b73012afbc9d9124dcf0290c85a1ad6d,"High-quality, labeled data is essential for successfully applying machine learning methods to real-world text classification problems. However, in many cases, the amount of labeled data is very small compared to that of the unlabeled, and labeling additional samples could be expensive and time consuming. Co-training algorithms, which make use of unlabeled data to improve classification, have proven to be very effective in such cases. Generally, co-training algorithms work by using two classifiers, trained on two different views of the data, to label large amounts of unlabeled data. Doing so can help minimize the human effort required for labeling new data, as well as improve classification performance. In this article, we propose an ensemble-based co-training approach that uses an ensemble of classifiers from different training iterations to improve labeling accuracy. This approach, which we call vertical ensemble, incurs almost no additional computational cost. Experiments conducted on six textual datasets show a significant improvement of over 45% in AUC compared with the original co-training algorithm. © 2017 ACM.",Co-training; Ensemble; Text classification,Learning algorithms; Learning systems; Text processing; Classification performance; Co-training; Co-training algorithm; Ensemble; Ensemble of classifiers; Labeling accuracies; Machine learning methods; Text classification; Classification (of information)
ACM TIST Special Issue on Urban Intelligence,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037175085&doi=10.1145%2f3154942&partnerID=40&md5=b1ae5477edddae3676825ffeeb59aac3,[No abstract available],,
Intelligent process adaptation in the SmartPM system,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995584212&doi=10.1145%2f2948071&partnerID=40&md5=8e4c8ac5b5c0181cf850452c6a398838,"The increasing application of process-oriented approaches in new challenging dynamic domains beyond business computing (e.g., healthcare, emergency management, factories of the future, home automation, etc.) has led to reconsider the level of flexibility and support required to manage complex knowledge-intensive processes in such domains. A knowledge-intensive process is influenced by user decision making and coupled with contextual data and knowledge production, and involves performing complex tasks in the ""physical"" real world to achieve a common goal. The physical world, however, is not entirely predictable, and knowledgeintensive processes must be robust to unexpected conditions and adaptable to unanticipated exceptions, recognizing that in real-world environments it is not adequate to assume that all possible recovery activities can be predefined for dealing with the exceptions that can ensue. To tackle this issue, in this paper we present SmartPM, a model and a prototype Process Management System featuring a set of techniques providing support for automated adaptation of knowledge-intensive processes at runtime. Such techniques are able to automatically adapt process instances when unanticipated exceptions occur, without explicitly defining policies to recover from exceptions and without the intervention of domain experts at runtime, aiming at reducing error-prone and costly manual ad-hoc changes, and thus at relieving users from complex adaptations tasks. To accomplish this, we make use of well-established techniques and frameworks from Artificial Intelligence, such as situation calculus, IndiGolog and classical planning. The approach, which is backed by a formal model, has been implemented and validated with a case study based on real knowledgeintensive processes coming from an emergency management domain. © 2016 ACM.",Classical planning; IndiGolog; Knowledge-intensive processes; Pervasive applications; Process adaptation; Process modeling and execution; Situation calculus,Civil defense; Decision making; Disasters; Risk management; Classical planning; IndiGolog; Knowledge intensive process; Pervasive applications; Process adaptations; Process Modeling; Situation calculus; Calculations
Stopping criterion for active learning with model stability,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033236322&doi=10.1145%2f3125645&partnerID=40&md5=d15496eba4203c3f3a85ca568792eac9,"Active learning selectively labels the most informative instances, aiming to reduce the cost of data annotation. While much effort has been devoted to active sampling functions, relatively limited attention has been paid to when the learning process should stop. In this article, we focus on the stopping criterion of active learning and propose a model stability-based criterion, that is, when a model does not change with inclusion of additional training instances. The challenge lies in how to measure the model change without labeling additional instances and training new models. Inspired by the stochastic gradient update rule, we use the gradient of the loss function at each candidate example to measure its effect on model change. We propose to stop active learning when the model change brought by any of the remaining unlabeled examples is lower than a given threshold. We apply the proposed stopping criterion to two popular classifiers: logistic regression (LR) and support vector machines (SVMs). In addition, we theoretically analyze the stability and generalization ability of the model obtained by our stopping criterion. Substantial experiments on various UCI benchmark datasets and ImageNet datasets have demonstrated that the proposed approach is highly effective. © 2017 ACM.",Active learning; Logistic regression; Model stability; Stopping criterion; Support vector machines,Artificial intelligence; Regression analysis; Stability; Stochastic systems; Support vector machines; Active Learning; Generalization ability; Limited attentions; Logistic regressions; Model stability; Stochastic gradient; Stopping criteria; Support vector machine (SVMs); Stochastic models
Supervised representation learning with double encoding-layer autoencoder for transfer learning,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032616283&doi=10.1145%2f3108257&partnerID=40&md5=8e274448626eb9629aee512e0ba8d523,"Transfer learning has gained a lot of attention and interest in the past decade. One crucial research issue in transfer learning is how to find a good representation for instances of different domains such that the divergence between domains can be reduced with the new representation. Recently, deep learning has been proposed to learn more robust or higher-level features for transfer learning. In this article, we adapt the autoencoder technique to transfer learning and propose a supervised representation learning method based on double encoding-layer autoencoder. The proposed framework consists of two encoding layers: one for embedding and the other one for label encoding. In the embedding layer, the distribution distance of the embedded instances between the source and target domains is minimized in terms of KL-Divergence. In the label encoding layer, label information of the source domain is encoded using a softmax regression model. Moreover, to empirically explore why the proposed framework can work well for transfer learning, we propose a new effective measure based on autoencoder to compute the distribution distance between different domains. Experimental results show that the proposed new measure can better reflect the degree of transfer difficulty and has stronger correlation with the performance from supervised learning algorithms (e.g., Logistic Regression), compared with previous ones, such as KL-Divergence and Maximum Mean Discrepancy. Therefore, in our model, we have incorporated two distribution distance measures to minimize the difference between source and target domains in the embedding representations. Extensive experiments conducted on three real-world image datasets and one text data demonstrate the effectiveness of our proposed method compared with several state-of-the-art baseline methods. © 2017.",Distribution difference measure; Double encoding-layer autoencoder; Representation learning,Encoding (symbols); Learning systems; Regression analysis; Signal encoding; Underground structures; Auto encoders; Different domains; Distribution difference measure; Effective measures; Label information; Logistic regressions; Representation learning; Softmax regressions; Learning algorithms
Knowledge representations and inference techniques for medical question answering,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032622372&doi=10.1145%2f3106745&partnerID=40&md5=76f8d7727af57a7458b01fdc14f9065c,"Answering medical questions related to complex medical cases, as required in modern Clinical Decision Support (CDS) systems, imposes (1) access to vast medical knowledge and (2) sophisticated inference techniques. In this article, we examine the representation and role of combining medical knowledge automatically derived from (a) clinical practice and (b) research findings for inferring answers to medical questions. Knowledge from medical practice was distilled from a vast Electronic Medical Record (EMR) system, while research knowledge was processed from biomedical articles available in PubMed Central. The knowledge automatically acquired from the EMR system took into account the clinical picture and therapy recognized from each medical record to generate a probabilistic Markov network denoted as a Clinical Picture and Therapy Graph (CPTG). Moreover, we represented the background of medical questions available from the description of each complex medical case as a medical knowledge sketch. We considered three possible representations of medical knowledge sketches that were used by four different probabilistic inference methods to pinpoint the answers from the CPTG. In addition, several answer-informed relevance models were developed to provide a ranked list of biomedical articles containing the answers. Evaluations on the TREC-CDS data show which of the medical knowledge representations and inference methods perform optimally. The experiments indicate an improvement of biomedical article ranking by 49% over state-of-the-art results. © 2017 ACM.",Clinical decision support; Medical information retrieval; Medical knowledge representation; Medical question answering; Probabilistic inference,Biomedical engineering; Complex networks; Decision support systems; Medical computing; Medicine; Clinical decision support; Clinical practices; Electronic medical record; Inference methods; Inference techniques; Medical knowledge; Medical question answering; Probabilistic inference; Knowledge representation
Iteratively divide-and-conquer learning for nonlinear classification and ranking,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032644092&doi=10.1145%2f3122802&partnerID=40&md5=f91fb8020fc7f80e5200c389e66b48e2,"Nonlinear classifiers (i.e., kernel support vector machines (SVMs)) are effective for nonlinear data classification. However, nonlinear classifiers are usually prohibitively expensive when dealing with large nonlinear data. Ensembles of linear classifiers have been proposed to address this inefficiency, which is called the ensemble linear classifiers for nonlinear data problem. In this article, a new iterative learning approach is introduced that involves two steps at each iteration: partitioning the data into clusters according to Gaussian mixture models with local consistency and then training basic classifiers (i.e., linear SVMs) for each cluster. The two divide-and-conquer steps are combined into a graphical model. Meanwhile, with training, each classifier is regarded as a task; clustered multitask learning is employed to capture the relatedness among different tasks and avoid overfitting in each task. In addition, two novel extensions are introduced based on the proposed approach. First, the approach is extended for quality-aware web data classification. In this problem, the types of web data vary in terms of information quality. The ignorance of the variations of information quality of web data leads to poor classification models. The proposed approach can effectively integrate quality-aware factors into web data classification. Second, the approach is extended for listwise learning to rank to construct an ensemble of linear ranking models, whereas most existing listwise ranking methods construct a solely linear ranking model. Experimental results on benchmark datasets show that our approach outperforms state-of-the-art algorithms. During prediction for nonlinear classification, it also obtains comparable classification performance to kernel SVMs, with much higher efficiency. © 2017 ACM.",Classification; Clustering; Divide-and-conquer; Listwise learning to rank; Multi-task learning,Information analysis; Iterative methods; Learning algorithms; Learning systems; Support vector machines; Clustering; Divide and conquer; Divide and conquer learning; Iterative learning approaches; Learning to rank; Multitask learning; State-of-the-art algorithms; Support vector machine (SVMs); Classification (of information)
SocialWave: Visual analysis of spatio-temporal diffusion of information on social media,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032626561&doi=10.1145%2f3106775&partnerID=40&md5=3c7d345ffdadf10e1fb07f9059ced18b,"Rapid advancement of social media tremendously facilitates and accelerates the information diffusion among users around the world. How and to what extent will the information on social media achieve widespread diffusion across the world? How can we quantify the interaction between users from different geolocations in the diffusion process? How will the spatial patterns of information diffusion change over time? To address these questions, a dynamic social gravity model (SGM) is proposed to quantify the dynamic spatial interaction behavior among social media users in information diffusion. The dynamic SGM includes three factors that are theoretically significant to the spatial diffusion of information: geographic distance, cultural proximity, and linguistic similarity. Temporal dimension is also taken into account to help detect recency effect, and ground-truth data is integrated into the model to help measure the diffusion power. Furthermore, SocialWave, a visual analytic system, is developed to support both spatial and temporal investigative tasks. SocialWave provides a temporal visualization that allows users to quickly identify the overall temporal diffusion patterns, which reflect the spatial characteristics of the diffusion network.When a meaningful temporal pattern is identified, SocialWave utilizes a new occlusion-free spatial visualization, which integrates a node-link diagram into a circular cartogram for further analysis. Moreover, we propose a set of rich user interactions that enable in-depth, multi-faceted analysis of the diffusion on social media. The effectiveness and efficiency of the mathematical model and visualization system are evaluated with two datasets on social media, namely, Ebola Epidemics and Ferguson Unrest. © 2017 ACM.",Information diffusion; Social media visualization; Spatio-temporal visualization,Visualization; Effectiveness and efficiencies; Information diffusion; Linguistic similarities; Social media; Spatial characteristics; Spatial visualization; Spatio-temporal visualizations; Visualization system; Social networking (online)
SPACE-TA: Cost-effective task allocation exploiting intradata and interdata correlations in sparse crowdsensing,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032638642&doi=10.1145%2f3131671&partnerID=40&md5=72215a8126bf66a55c081f8ccbbbb568,"Abstracr:-Data quality and budget are two primary concerns in urban-scale mobile crowdsensing. Traditional research on mobile crowdsensing mainly takes sensing coverage ratio as the data quality metric rather than the overall sensed data error in the target-sensing area. In this article, we propose to leverage spatiotemporal correlations among the sensed data in the target-sensing area to significantly reduce the number of sensing task assignments. In particular, we exploit both intradata correlations within the same type of sensed data and interdata correlations among different types of sensed data in the sensing task.We propose a novel crowdsensing task allocation framework called SPACE-TA (SPArse Cost-Effective Task Allocation), combining compressive sensing, statistical analysis, active learning, and transfer learning, to dynamically select a small set of subareas for sensing in each timeslot (cycle), while inferring the data of unsensed subareas under a probabilistic data quality guarantee. Evaluations on real-life temperature, humidity, air quality, and traffic monitoring datasets verify the effectiveness of SPACE-TA. In the temperature-monitoring task leveraging intradata correlations, SPACE-TA requires data from only 15.5% of the subareas while keeping the inference error below 0.25 ? C in 95% of the cycles, reducing the number of sensed subareas by 18.0% to 26.5% compared to baselines. When multiple tasks run simultaneously, for example, for temperature and humidity monitoring, SPACE-TA can further reduce ~10% of the sensed subareas by exploiting interdata correlations. © 2017 ACM.",Crowdsensing; Data quality; Task allocation,Air quality; Budget control; Cost benefit analysis; Cost effectiveness; Data reduction; Compressive sensing; Crowdsensing; Data quality; Data quality metric; Spatiotemporal correlation; Task allocation; Temperature and humidities; Temperature monitoring; Quality control
ACM TIST special issue on data-driven intelligence for wireless networking,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033230885&doi=10.1145%2f3104984&partnerID=40&md5=fb3fc973366acf95a3b27abe55c24c0b,[No abstract available],,
From electromyogram to password: Exploring the privacy impact of wearables in augmented reality,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030172268&doi=10.1145%2f3078844&partnerID=40&md5=41f852a56fb22049e1417f75bb5e5143,"With the increasing popularity of augmented reality (AR) services, providing seamless human-computer interactions in the AR setting has received notable attention in the industry. Gesture control devices have recently emerged to be the next great gadgets for AR due to their unique ability to enable computer interaction with day-to-day gestures. While these AR devices are bringing revolutions to our interaction with the cyber world, it is also important to consider potential privacy leakages from these always-on wearable devices. Specifically, the coarse access control on current AR systems could lead to possible abuse of sensor data. Although the always-on gesture sensors are frequently quoted as a privacy concern, there has not been any study on information leakage of these devices. In this article, we present our study on side-channel information leakage of the most popular gesture control device, Myo. Using signals recorded from the electromyo-graphy (EMG) sensor and accelerometers on Myo, we can recover sensitive information such as passwords typed on a keyboard and PIN sequence entered through a touchscreen. EMG signal records subtle electric currents of muscle contractions. We design novel algorithms based on dynamic cumulative sum and wavelet transform to determine the exact time of finger movements. Furthermore, we adopt the Hudgins feature set in a support vector machine to classify recorded signal segments into individual fingers or numbers. We also apply coordinate transformation techniques to recover fine-grained spatial information with low-fidelity outputs from the sensor in keystroke recovery. We evaluated the information leakage using data collected from a group of volunteers. Our results show that there is severe privacy leakage from these commodity wearable sensors. Our system recovers complex passwords constructed with lowercase letters, uppercase letters, numbers, and symbols with a mean success rate of 91%. © 2017 ACM.",Augmented reality; EMG side-channel; Information leakage; Keystroke detection; PIN sequence inference,Access control; Augmented reality; Authentication; Electromyography; Human computer interaction; Recovery; Wavelet transforms; Wearable computers; Wearable technology; Co-ordinate transformation; Computer interaction; Information leakage; PIN sequence inference; Sensitive informations; Side-channel; Side-channel information; Spatial informations; Wearable sensors
Taking the pulse of US college campuses with location-based anonymous mobile apps,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029784828&doi=10.1145%2f3078843&partnerID=40&md5=6b219e4e336ed5a58d7df64c41c35e14,"We deploy GPS hacking in conjunction with location-based mobile apps to passively survey users in targeted geographical regions. Specifically, we investigate surveying students at different college campuses with Yik Yak, an anonymous mobile app that is popular on US college campuses. In addition to being campus centric, Yik Yak's anonymity allows students to express themselves candidly without self-censorship. We collect nearly 1.6 million Yik Yak messages (""yaks"") from a diverse set of 45 college campuses in the United States.We use natural language processing to determine the sentiment (positive, negative, or neutral) of all of the yaks. We employ supervised machine learning to predict the gender of the authors of the yaks and then analyze how sentiment differs among the two genders on college campuses.We also use supervised machine learning to classify all the yaks into nine topics and then investigate which topics are most popular throughout the US and how topic popularity varies on the different campuses. The results in this article provide significant insight into how campus culture and student's thinking varies among US colleges and universities. © 2017 ACM.",Data mining; Social networks,Artificial intelligence; Data mining; Education; Geographical regions; Learning algorithms; Learning systems; Natural language processing systems; Personal computing; Social networking (online); Supervised learning; Surveys; Campus cultures; College campus; Colleges and universities; Location based; Mobile app; Mobile apps; Supervised machine learning; Students
I2tag: RFID mobility and activity identification through intelligent profiling,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029810623&doi=10.1145%2f3035968&partnerID=40&md5=955bf6331b2555beb341881e31f94471,"Many radio frequency identification (RFID) applications, such as virtual shopping cart and tag-assisted gaming, involve sensing and recognizing tag mobility. However, existing RFID localization methods are mostly designed for static or slowly moving targets (less than 0.3m/sec). More importantly, we observe that prior methods suffer from serious performance degradation for detecting real-world moving tags in typical indoor environments with multipath interference. In this article, we present i2tag, an intelligent mobility-aware activity identification system for RFID tags in multipath-rich environments (e.g., indoors). i2tag employs a supervised learning framework based on our novel fine-grain mobility provile, which can quantify different levels of mobility. Unlike previous methods that mostly rely on phase measurement, i2tag takes into account various measurements, including RSSI variance, packet loss rate, and our novel relative phase-based fingerprint. Additionally, we design a multidimensional dynamic time warping-based algorithm to robustly detect mobility and the associated activities. We show that i2tag is readily deployable using off-the-shelf RFID devices. A prototype has been implemented using a ThingMagic reader and standard-compatible tags. Experimental results demonstrate its superiority in mobility detection and activity identification in various indoor environments. © 2017 ACM.",Activity identification; Backscatter; Mobility detection; RFID,Backscattering; Functional assessment; Phase measurement; Virtual reality; Indoor environment; Intelligent mobility; Multi-dimensional dynamics; Multi-path interference; Packet loss rates; Performance degradation; Rfid localizations; Virtual shopping; Radio frequency identification (RFID)
UMCR: User interaction-driven mobile content retrieval,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029802538&doi=10.1145%2f3102292&partnerID=40&md5=1e43c44d157adc23fa238da89dfe2f8c,"Although mobile application ecosystems have experienced tremendous growth in recent years, retrieving content of mobile applications that serves a key to mobile content search engines still faces grand challenges. Compared to web content retrieval, it is much more difficult to capture content in mobile applications due to the diversity of applications and the lack of Uniform Resource Locator indices. In this study, we propose and implement a user interaction-driven mobile content retrieval (UMCR) system to address such issues, which is the first mobile content crawler in the current literature. UMCR is a distributed system that contains many measurement nodes, each of which combines the user interaction path traversing (UIPT) and Deep Package Inspection (DPI) together to obtain mobile content. UIPT determines the events of user interactions in various applications to capture the static content such as text and images, in which a traversal depth termination scheme and an optional cut-off component are adopted to balance the content coverage and traversing efficiency. Meanwhile, the analysis based on DPI is responsible for extracting the videos as well as digging the infrastructural information and performance metrics. In addition, a distributed traversal scheduling method is designed for UIPT tasks to improve the throughput and scalability in large-scale content retrieval. Experiments on retrieving content of 64 real mobile applications demonstrate that UMCR can handle diverse mobile applications efficiently. The scheduler can improve throughput by 3 times compared to the legacy arbitrary task assignment strategy. © 2017 ACM.",Content retrieval; Mobile network; Task scheduling; User interactiondriven,Mobile computing; Mobile telecommunication systems; Scheduling; Search engines; Wireless networks; Content retrieval; Distributed systems; Measurement nodes; Mobile applications; Performance metrics; Scheduling methods; Task-scheduling; User interactiondriven; Information retrieval
"Secure IoT-based, incentive-aware emergency personnel dispatching scheme withweighted fine-grained access control",2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029819323&doi=10.1145%2f3063716&partnerID=40&md5=2cd2c526c18a97eadac16d157465d2fd,"Emergency response times following a traffic accident are extremely crucial in reducing the number of trafficrelated deaths. Existing emergency vehicle dispatching systems rely heavily on manual assignments. Although some technology-assisted emergency systems engage in emergency message dissemination and path planning, efficient emergency response is one of the main factors that can decrease traffic-related deaths. Obviously, effective emergency response often plays a far more important role in a successful rescue. In this article, we propose a secure IoT-based and incentive-aware emergency personnel dispatching scheme (EPDS) with weighted fine-grained access control. Our EPDS can recruit available medical personnel on-the-fly, such as physicians driving in the vicinity of the accident scene. An appropriate incentive, such as paid leave, can be offered to encourage medical personnel to join rescue missions. Furthermore, IoT-based devices are installed in vehicles or wearable on drivers to gather biometric signals from the driver, which can be used to decide precisely which divisions or physicians are needed to administer the appropriate remedy. Additionally, our scheme can cryptographically authorize the assigned rescue vehicle to control traffic to increase rescue efficacy. Our scheme also takes advantage of adjacent roadside units to organize the appropriate rescue personnel without requiring long-distance communication with a trusted traffic authority. Proof of security is provided and extensive analyses, including qualitative and quantitative analyses and simulations, show that the proposed scheme can significantly improve rescue response time and effectiveness. To the best of our knowledge, this is the first work to make use of medical personnel that are close by in emergency rescue missions. © 2017 ACM.",Biometric authentication; Emergency personnel dispatch; Fine-grained access control; Internet of vehicles; Security,Access control; Accidents; Biometrics; Highway accidents; Internet of things; Motion planning; Response time (computer systems); Vehicles; Biometric authentication; Emergency message disseminations; Emergency personnel dispatch; Fine grained; Long distance communication; Qualitative and quantitative analysis; Security; Traffic authorities; Emergency services
Personalized microtopic recommendation on microblogs,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028687916&doi=10.1145%2f2932192&partnerID=40&md5=2ac824527497dc31174528533d6d0d3f,"Microblogging services such as Sina Weibo and Twitter allow users to create tags explicitly indicated by the # symbol. In Sina Weibo, these tags are called microtopics, and in Twitter, they are called hashtags. In Sina Weibo, each microtopic has a designate page and can be directly visited or commented on. Recommending these microtopics to users based on their interests can help users efficiently acquire information. However, it is non-trivial to recommend microtopics to users to satisfy their information needs. In this article, we investigate the task of personalized microtopic recommendation, which exhibits two challenges. First, users usually do not give explicit ratings to microtopics. Second, there exists rich information about users and microtopics, for example, users' published content and biographical information, but it is not clear how to best utilize such information. To address the above two challenges, we propose a joint probabilistic latent factor model to integrate rich information into a matrix factorization-based solution to microtopic recommendation. Our model builds on top of collaborative filtering, content analysis, and feature regression. Using two real-world datasets, we evaluate our model with different kinds of content and contextual information. Experimental results show that our model significantly outperforms a few competitive baseline methods, especially in the circumstance where users have few adoption behaviors. © 2017 ACM.",Collaborative filtering; Microblogs; Microtopic recommendation; Topic model,Collaborative filtering; Factorization; Social networking (online); Contextual information; Latent factor models; Matrix factorizations; Micro-blogging services; Microblogs; Microtopic recommendation; Real-world datasets; Topic Modeling; Recommender systems
PRISM: Profession identification in social media,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028684888&doi=10.1145%2f3070665&partnerID=40&md5=3d26b173c3820ce14384bb3677ce3daf,"Profession is an important social attribute of people. It plays a crucial role in commercial services such as personalized recommendation and targeted advertising. In practice, profession information is usually unavailable due to privacy and other reasons. In this article, we explore the task of identifying user professions according to their behaviors in social media. The task confronts the following challenges that make it non-trivial: how to incorporate heterogeneous information of user behaviors, how to effectively utilize both labeled and unlabeled data, and how to exploit community structure. To address these challenges, we present a framework called Profession Identification in Social Media. It takes advantage of both personal information and community structure of users in the following aspects: (1) We present a cascaded two-level classifier with heterogeneous personal features to measure the confidence of users belonging to different professions. (2) We present a multi-training process to take advantages of both labeled and unlabeled data to enhance classification performance. (3) We design a profession identification method synthetically considering the confidences from personal features and community structure. We collect a real-world dataset to conduct experiments, and experimental results demonstrate the significant effectiveness of our method compared with other baseline methods. By applying prediction on large-scale users, we also analyze characteristics of microblog users, finding that there are significant diversities among users of different professions in demographics, social network structures, and linguistic styles.",Community detection; Heterogeneous information; Profession identification; Social media,Behavioral research; Social networking (online); Social sciences; Classification performance; Community detection; Heterogeneous information; Identification method; Labeled and unlabeled data; Personalized recommendation; Social media; Social network structures; Classification (of information)
Robust spammer detection in microblogs: Leveraging user carefulness,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028675669&doi=10.1145%2f3086637&partnerID=40&md5=ee04beec3b2f46807dcba14fb8097346,"Microblogging Web sites, such as Twitter and Sina Weibo, have become popular platforms for socializing and sharing information in recent years. Spammers have also discovered this new opportunity to unfairly overpower normal users with unsolicited content, namely social spams. Although it is intuitive for everyone to follow legitimate users, recent studies show that both legitimate users and spammers follow spammers for different reasons. Evidence of users seeking spammers on purpose is also observed. We regard this behavior as useful information for spammer detection. In this article, we approach the problem of spammer detection by leveraging the ""carefulness"" of users, which indicates how careful a user is when she is about to follow a potential spammer. We propose a framework to measure the carefulness and develop a supervised learning algorithm to estimate it based on known spammers and legitimate users.We illustrate how the robustness of the detection algorithms can be improved with aid of the proposed measure. Evaluation on two real datasets from SinaWeibo and Twitter withmillions of users are performed, as well as an online test on SinaWeibo. The results show that our approach indeed captures the carefulness, and it is effective for detecting spammers. In addition, we find that our measure is also beneficial for other applications, such as link prediction.",Microblog; Social network; Spammer detection,Detection algorithm; Legitimate users; Micro-blog; Microblogging; Popular platform; Real data sets; Sharing information; Spammer detections; Social networking (online)
Detecting communities of authority and analyzing their influence in dynamic social networks,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028659735&doi=10.1145%2f3070658&partnerID=40&md5=07b8cdcba5d0b213757e923eeef869e0,"Users in real-world social networks are organized into communities that differ from each other in terms of influence, authority, interest, size, etc. This article addresses the problems of detecting communities of authority and of estimating the influence of such communities in dynamic social networks. These are new issues that have not yet been addressed in the literature, and they are important in applications such as marketing and recommender systems. To facilitate the identification of communities of authority, our approach first detects communities sharing common interests, which we call ""meta-communities,"" by incorporating topic modeling based on users' community memberships. Then, communities of authority are extracted with respect to each meta-community, using a new measure based on the betweenness centrality. To assess the influence between communities over time, we propose a new model based on the Granger causality method. Through extensive experiments on a variety of social network datasets, we empirically demonstrate the suitability of our approach for community-of-authority detection and assessment of the influence between communities over time.",Betweenness centrality; Community influence; Community of authority; Granger causality; Meta-community; Topic modeling,Statistical tests; Betweenness centrality; Community influence; Community of authority; Granger Causality; Meta communities; Topic Modeling; Population dynamics
An unsupervised approach to inferring the localness of people using incomplete geotemporal online check-in data,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028594056&doi=10.1145%2f3022471&partnerID=40&md5=3f45717b400a4fb5ef02bacad09b51d2,"Inferring the localness of people is to classify people who are local residents in a city from people who visit the city by analyzing online check-in points that are contributed by online users. This information is critical for the urban planning, user profiling, and localized recommendation systems. Supervised learning approaches have been developed to infer the location of people in a city by assuming the availability of high-quality training datasets with complete geotemporal information. In this article, we develop an unsupervised model to accurately identify local people in a city by using the incomplete online check-in data that are publicly available. In particular, we develop an incomplete geotemporal expectation maximization (IGT-EM) scheme, which incorporates a set of hidden variables to represent the localness of people and a set of estimation parameters to represent the likelihood of venues to attract local and nonlocal people, respectively. Our solution can accurately classify local people from nonlocal nones without requiring any training data. We also implement a parallel IGT-EM algorithm by leveraging the computing power of a graphic processing unit (GPU) that consists of 2,496 cores. In the evaluation, we compare our new approach with the existing solutions through four real-world case studies using data from the New York City, Chicago, Boston, and Washington, DC. The results show that our approach can identify the local people and significantly outperform the compared baselines in estimation accuracy and execution time. © 2017 ACM.",Crowdsourcing; GPU implementation; Localness of people; Maximum likelihood estimation; Online social networks; Unsupervised learning,Classification (of information); Crowdsourcing; Graphics processing unit; Maximum likelihood; Maximum principle; Social networking (online); Unsupervised learning; Estimation parameters; Expectation - maximizations; GPU implementation; Graphic processing unit(GPU); Localness of people; On-line social networks; Supervised learning approaches; Unsupervised approaches; Maximum likelihood estimation
TensorBeat: Tensor Decomposition for Monitoring Multiperson Breathing Beats with Commodity WiFi,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019090826&doi=10.1145%2f3078855&partnerID=40&md5=20757b1e7530ca6e86ec5e357fc4bf48,"Breathing signal monitoring can provide important clues for health problems. Compared to existing techniques that require wearable devices and special equipment, a more desirable approach is to provide contactfree and long-term breathing rate monitoring by exploiting wireless signals. In this article, we propose TensorBeat, a system to employ channel state information (CSI) phase difference data to intelligently estimate breathing rates for multiple persons with commodity WiFi devices. The main idea is to leverage the tensor decomposition technique to handle the CSI phase difference data. The proposed TensorBeat scheme first obtains CSI phase difference data between pairs of antennas at the WiFi receiver to create CSI tensors. Then canonical polyadic (CP) decomposition is applied to obtain the desired breathing signals. A stable signal matching algorithm is developed to identify the decomposed signal pairs, and a peak detection method is applied to estimate the breathing rates for multiple persons. Our experimental study shows that TensorBeat can achieve high accuracy under different environments for multiperson breathing rate monitoring. © 2017 ACM.",Channel state information; Commodity WiFi; Healthcare internet of things (IoT); Stable roommate matching; Tensor decomposition; Vital sign monitoring,Internet of things; Patient monitoring; Signal processing; Tensors; Wireless local area networks (WLAN); Breathing signals; Internet of Things (IOT); Phase difference; Stable roommates; Tensor decomposition; Vital sign monitoring; Wearable devices; Wireless signals; Channel state information
Exploring communication behaviors of users to target potential users in mobile social networks,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028624455&doi=10.1145%2f3022472&partnerID=40&md5=1b45b19471dd0a05dd8f71fd058c048c,"In mobile communication services, users can communicate with each other over different telecommunication carriers. For telecom operators, how to acquire and retain users is a significant and practical task. Note that telecom operators only have their own customer profiles. For the users from other telecom operators, their information is sparse. Thus, given a set of communication logs, the main theme of our work is to identify the potential users who will possibly join the target services in the near future. Since only a limited amount of information is available, one challenging issue is how to extract features from the communication logs. In this article, we propose a Communication-Based Feature Generation (CBFG) framework that extracts features and builds models to infer the potential users. Explicitly, we construct a heterogeneous information network from the communication logs of users. Then, we extract the explicit features, which refer to those calling features of users, from the potential users' interaction behaviors in the heterogeneous information network. Moreover, from the calling behaviors of users, one could extract the possible community structures of users. Based on the community structures, we further extract the implicit features of users. In light of both explicit and implicit features, we propose an information-gain-based method to select the effective features. According to the features selected, we utilize three popular classifiers (i.e., AdaBoost, Random Forest, and SVM) to build models to target the potential users. In addition, we have designed a sampling approach to extract training data for classifiers. To evaluate our methods, we have conducted experiments on a real dataset. The results of our experiments show that the features extracted by our proposed method can be effective for targeting the potential users. © 2017 ACM.",Communication behaviors; Feature engineering; Mobile social network,Adaptive boosting; Decision trees; Information services; Mobile telecommunication systems; Social networking (online); Social sciences; Telecommunication services; Amount of information; Communication behavior; Feature engineerings; Heterogeneous information; Interaction behavior; Mobile communication services; Mobile social networks; Telecommunication carrier; Classification (of information)
Multifeature anisotropic orthogonal Gaussian process for automatic age estimation,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029791794&doi=10.1145%2f3090311&partnerID=40&md5=a92192102a6aa00b21e19f56fd8e7672,"Automatic age estimation is an important yet challenging problem. It has many promising applications in social media. Of the existing age estimation algorithms, the personalized approaches are among the most popular ones. However, most person-specific approaches rely heavily on the availability of training images across different ages for a single subject, which is usually difficult to satisfy in practical application of age estimation. To address this limitation,we first propose a new model called Orthogonal Gaussian Process (OGP), which is not restricted by the number of training samples per person. In addition, without sacrifice of discriminative power, OGP is much more computationally efficient than the standard Gaussian Process. Based on OGP, we then develop an effective age estimation approach, namely anisotropic OGP (A-OGP), to further reduce the estimation error. A-OGP is based on an anisotropic noise level learning scheme that contributes to better age estimation performance. To finally optimize the performance of age estimation, we propose a multifeature A-OGP fusion framework that uses multiple features combined with a random sampling method in the feature space. Extensive experiments on several public domain face aging datasets (FG-NET, MORPH Album1, and MORPH Album 2) are conducted to demonstrate the state-of-the-art estimation accuracy of our new algorithms. © 2017 ACM.",Age estimation; Face image,Anisotropy; Gaussian noise (electronic); Human computer interaction; Age estimation; Anisotropic noise; Computationally efficient; Discriminative power; Face images; Gaussian Processes; Multiple features; Random sampling method; Gaussian distribution
Energy-efficient mobile video streaming: A location-aware approach,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028558834&doi=10.1145%2f3102301&partnerID=40&md5=e2bbb4257dd678dbcc777d03294e91ac,"Video streaming is one of the most widely used mobile applications today, and it also accounts for a large fraction of mobile battery usage. Much of the energy consumption is for wireless data transmission and is highly correlated to network bandwidth conditions. In periods of poor connectivity, up to 90% of mobile energy can be used for wireless data transfer. In this article, we study the problem of energy-efficient mobile video streaming. We make use of the observed correlation between bandwidth and user location, and also observe that a user's location is predictable in many situations, such as when commuting to a known destination. Based on the user's predicted locations and bandwidth conditions, we optimize wireless transmission times to achieve high quality video playback while minimizing energy use. We propose an optimal offline algorithm for this problem, which runs in O(Tk) time, where T is the duration of the video and k is the size of the video buffer. We also propose LAWS, a Location AWare Streaming algorithm. LAWS learns from historical location-aware bandwidth conditions and predicts future bandwidths along a planned route to make online wireless download decisions. We evaluate LAWS using real bandwidth traces, and show that LAWS closely approximates the performance of the optimal offline algorithm, achieving 90.6% of the optimal performance on average, and 97% in certain cases. LAWS also outperforms three popular strategies used in practice by, on average, 69%, 63%, and 38%, respectively. Lastly, we show that LAWS is able to deal with noisy data and can attain the stated performance after sampling bandwidth conditions only five times. © 2017 ACM.",Energy efficiency; Media cloud; Mobile device; Video streaming,Bandwidth; Communication channels (information theory); Data transfer; Energy utilization; Green computing; Location; Media streaming; Mobile devices; Video streaming; Wave transmission; Bandwidth conditions; Media clouds; Mobile applications; Mobile video streaming; Optimal performance; Wireless data transfer; Wireless data transmission; Wireless transmissions; Energy efficiency
RCMC: Recognizing crowd-mobility patterns in cities based on location based social networks data,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028566476&doi=10.1145%2f3086636&partnerID=40&md5=c33e03a329c3478e15c4c2ce78d0f5fc,"During the past few years, the analysis of data generated from Location-Based Social Networks (LBSNs) have aided in the identification of urban patterns, understanding activity behaviours in urban areas, as well as producing novel recommender systems that facilitate users' choices. Recognizing crowd-mobility patterns in cities is very important for public safety, traffic managment, disaster management, and urban planning.In this article, we propose a framework for Recognizing the Crowd Mobility Patterns in Cities using LBSN data. Our proposed framework comprises four main components: data gathering, recurrent crowd-mobility patterns extraction, temporal functional regions detection, and visualization component. More specifically, we employ a novel approach based on Non-negative Matrix Factorization and Gaussian Kernel Density Estimation for extracting the recurrent crowd-mobility patterns in cities illustrating how crowd shifts from one area to another during each day across various time slots. Moreover, the framework employs a hierarchical clustering-based algorithm for identifying what we refer to as temporal functional regions by modeling functional areas taking into account temporal variation by means of check-ins' categories. We build the framework using a spatial-temporal dataset crawled from Twitter for two entire years (2013 and 2014) for the area of Manhattan in New York City. We perform a detailed analysis of the extracted crowd patterns with an exploratory visualization showing that our proposed approach can identify clearly obvious mobility patterns that recur over time and location in the urban scenario. Using same time interval, we show that correlating the temporal functional regions with the recognized recurrent crowd-mobility patterns can yield to a deeper understanding of city dynamics and the motivation behind the crowd mobility. We are confident that our proposed framework not only can help in managing complex city environments and better allocation of resources based on the expected crowd mobility and temporal functional regions but also can have a direct implication on a variety of applications such as personalized recommender systems, anomalous event detection, disaster resilience management systems, and others. © 2017 ACM.",Crowd patterns; Smart cities; Urban computing; Urban mobility,Clustering algorithms; Data visualization; Disaster prevention; Disasters; Factorization; Location; Network function virtualization; Recommender systems; Social networking (online); Visualization; Crowd patterns; Exploratory visualizations; Hier-archical clustering; Location-based social networks; Nonnegative matrix factorization; Personalized recommender systems; Urban computing; Urban mobility; Smart city
Dmad: Data-driven measuring of Wi-Fi access point deployment in urban spaces,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028564178&doi=10.1145%2f3065949&partnerID=40&md5=cf7777df8b9e8a7f3d0978731c52126a,"Wireless networks offer many advantages over wired local area networks such as scalability and mobility. Strategically deployed wireless networks can achieve multiple objectives like traffic offloading, network coverage, and indoor localization. To this end, various mathematical models and optimization algorithms have been proposed to find optimal deployments of access points (APs). However, wireless signals can be blocked by the human body, especially in crowded urban spaces. As a result, the real coverage of an on-site AP deployment may shrink to some degree and lead to unexpected dead spots (areas without wireless coverage). Dead spots are undesirable, since they degrade the user experience in network service continuity, on one hand, and, on the other hand paralyze some applications and services like tracking and monitoring when users are in these areas. Nevertheless, it is nontrivial for existing methods to analyze the impact of human beings on wireless coverage. Site surveys are too time consuming and labor intensive to conduct. It is also infeasible for simulation methods to predict the number of on-site people. In this article, we propose DMAD, a Data-driven Measuring of Wi-Fi Access point Deployment, which not only estimates potential dead spots of an on-site AP deployment but also quantifies their severity, using simple Wi-Fi data collected from the on-site deployment and shop profiles from the Internet. DMAD first classifies static devices and mobile devices with a decision-tree classifier. Then it locates mobile devices to grid-level locations based on shop popularities, wireless signal, and visit duration. Last, DMAD estimates the probability of dead spots for each grid during different time slots and derives their severity considering the probability and the number of potential users. The analysis of Wi-Fi data from static devices indicates that the Pearson Correlation Coefficient of wireless coverage status and the number of on-site people is over 0.7, which confirms that human beings may have a significant impact on wireless coverage. We also conduct extensive experiments in a large shopping mall in Shenzhen. The evaluation results demonstrate that DMAD can find around 70% of dead spots with a precision of over 70%. © 2017 ACM.",AP deployment measuring; Data-driven approach; Room-level localization; Wi-Fi AP,Correlation methods; Decision trees; Electronic document exchange; Estimation; Optimization; Urban planning; Wireless local area networks (WLAN); Wireless networks; AP deployment measuring; Data-driven approach; Decision tree classifiers; Multiple-objectives; Optimization algorithms; Pearson correlation coefficients; Room-level localization; Wired local area networks; Wi-Fi
Refined-graph regularization-based Nonnegative matrix factorization,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028554018&doi=10.1145%2f3090312&partnerID=40&md5=e1c74b855a7e9b959801205148c72fed,"Nonnegative matrix factorization (NMF) is one of the most popular data representation methods in the field of computer vision and pattern recognition. High-dimension data are usually assumed to be sampled from the submanifold embedded in the original high-dimension space. To preserve the locality geometric structure of the data, k-nearest neighbor (k-NN) graph is often constructed to encode the near-neighbor layout structure. However, k-NN graph is based on Euclidean distance, which is sensitive to noise and outliers. In this article, we propose a refined-graph regularized nonnegative matrix factorization by employing a manifold regularized least-squares regression (MRLSR) method to compute the refined graph. In particular, each sample is represented by the whole dataset regularized with ℓ2-norm and Laplacian regularizer. Then a MRLSR graph is constructed based on the representative coefficients of each sample. Moreover, we present two optimization schemes to generate refined-graphs by employing a hard-thresholding technique. We further propose two refined-graph regularized nonnegative matrix factorization methods and use them to perform image clustering. Experimental results on several image datasets reveal that they outperform 11 representative methods. © 2017 ACM.",Data representation; Image clustering; Least squares regression; Nonnegative matrix factorization (NMF); Refined-graph,Factorization; Least squares approximations; Nearest neighbor search; Pattern recognition; Data representations; Image clustering; Least squares regression; Nonnegative matrix factorization; Refined-graph; Matrix algebra
Sparse Online Learning of image similarity,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028571922&doi=10.1145%2f3065950&partnerID=40&md5=e97fa8ec4768125a07c04f591e4ff17e,"Learning image similarity plays a critical role in real-world multimedia information retrieval applications, especially in Content-Based Image Retrieval (CBIR) tasks, in which an accurate retrieval of visually similar objects largely relies on an effective image similarity function. Crafting a good similarity function is very challenging because visual contents of images are often represented as feature vectors in high-dimensional spaces, for example, via bag-of-words (BoW) representations, and traditional rigid similarity functions, for example, cosine similarity, are often suboptimal for CBIR tasks. In this article, we address this fundamental problem, that is, learning to optimize image similarity with sparse and high-dimensional representations from large-scale training data, and propose a novel scheme of Sparse Online Learning of Image Similarity (SOLIS). In contrast to many existing image-similarity learning algorithms that are designed to work with low-dimensional data, SOLIS is able to learn image similarity from large-scale image data in sparse and high-dimensional spaces. Our encouraging results showed that the proposed new technique achieves highly competitive accuracy as compared to the state-of-the-art approaches but enjoys significant advantages in computational efficiency, model sparsity, and retrieval scalability, making it more practical for real-world multimedia retrieval applications. © 2017 ACM.",Bag-of-words representation; Distance metric; Image Retrieval; Metric learning; Online Learning; Similarity learning,Computational efficiency; Content based retrieval; E-learning; Image analysis; Learning algorithms; Vector spaces; Bag of words; Distance metrics; Metric learning; Online learning; Similarity learning; Image retrieval
Exploring indoor white spaces in metropolises,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028571003&doi=10.1145%2f3059149&partnerID=40&md5=8f3369bb624d7c5f9d9b0c99bfe4aeb6,"It is a promising vision to exploit white spaces, that is, vacant VHF and UHF TV channels, to meet the rapidly growing demand for wireless data services in both outdoor and indoor scenarios. While most prior works have focused on outdoor white space, the indoor story is largely open for investigation. Motivated by this observation and discovering that 70% of the spectrum demand comes from indoor environment, we carry out a comprehensive study to explore indoor white spaces. We first conduct a large-scale measurement study and compare outdoor and indoor TV spectrum occupancy at 30+ diverse locations in a typical metropolis-Hong Kong. Our results show that abundant white spaces are available in different areas in Hong Kong, which account for more than 50% and 70% of the entire TV spectrum in outdoor and indoor scenarios, respectively. Although there are substantially more white spaces indoors than outdoors, there have been very few solutions for identifying indoor white space. To fill in this gap, we develop the first data-driven, low-cost indoor white space identification system for White-space Indoor Spectrum EnhanceR (WISER), to allow secondary users to identify white spaces for communication without sensing the spectrum themselves. We design the architecture and algorithms to address the inherent challenges. We build a WISER prototype and carry out real-world experiments to evaluate its performance. Our results show that WISER can identify 30%-40% more indoor white spaces with negligible false alarms, as compared to alternative baseline approaches. © 2017 ACM.",Clustering algorithms; Sensor placement; Tv white spaces,Diverse locations; Indoor environment; Large-scale measurement; Real world experiment; Sensor placement; Spectrum occupancies; Tv white spaces; Wireless data services; Clustering algorithms
Finding semantically valid and relevant topics by Association-based Topic Selection model,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028545036&doi=10.1145%2f3094786&partnerID=40&md5=07065055dafcb929f01b1764889c0949,"Topic modelling methods such as Latent Dirichlet Allocation (LDA) have been successfully applied to various fields, since these methods can effectively characterize document collections by using a mixture of semantically rich topics. So far, many models have been proposed. However, the existing models typically outperform on full analysis on the whole collection to find all topics but difficult to capture coherent and specifically meaningful topic representations. Furthermore, it is very challenging to incorporate user preferences into existing topic modelling methods to extract relevant topics. To address these problems, we develop a novel personalized Association-based Topic Selection (ATS) model, which can identify semantically valid and relevant topics from a set of raw topics based on the semantical relatedness between users' preferences and the structured patterns captured in topics. The advantage of the proposed ATS model is that it enables an interactive topic modelling process driven by users' specific interests. Based on three benchmark datasets, namely, RCV1, R8, and WT10G under the context of information filtering (IF) and information retrieval (IR), our rigorous experiments show that the proposed ATS model can effectively identify relevant topics with respect to users' specific interests, and hence to improve the performance of IF and IR. © 2017 ACM.",Information filtering; Topic components; Topic evaluation; Topic Selection,Benchmarking; Information filtering; Statistics; Benchmark datasets; Document collection; Latent dirichlet allocations; Modelling process; Structured patterns; Topic components; Topic evaluation; Topic Selection; Data mining
"P2P lending survey: Platforms, recent advances and prospects",2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026676655&doi=10.1145%2f3078848&partnerID=40&md5=266d4a5a6ca61a52fe64081771cb3c3a,"P2P lending is an emerging Internet-based application where individuals can directly borrow money from each other. The past decade has witnessed the rapid development and prevalence of online P2P lending platforms, examples of which include Prosper, LendingClub, and Kiva. Meanwhile, extensive research has been done that mainly focuses on the studies of platform mechanisms and transaction data. In this article, we provide a comprehensive survey on the research about P2P lending, which, to the best of our knowledge, is the first focused effort in this field. Specifically, we first provide a systematic taxonomy for P2P lending by summarizing different types of mainstream platforms and comparing their working mechanisms in detail. Then, we review and organize the recent advances on P2P lending from various perspectives (e.g., economics and sociology perspective, and data-driven perspective). Finally, we propose our opinions on the prospects of P2P lending and suggest some future research directions in this field. Meanwhile, throughout this paper, some analysis on real-world data collected from Prosper and Kiva are also conducted. © 2017 ACM.",Micro-finance; Online loans; P2P lending; Platforms; Prospects,Finance; Surveys; Future research directions; Internet based application; Online loans; P2p lending; Platforms; Prospects; Transaction data; Working mechanisms; Peer to peer networks
Rating effects on social news posts and comments,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026666436&doi=10.1145%2f2963104&partnerID=40&md5=5b18baa0c4218c184393184de9d2f726,"At a time when information seekers first turn to digital sources for news and opinion, it is critical that we understand the role that social media plays in human behavior. This is especially true when information consumers also act as information producers and editors through their online activity. In order to better understand the effects that editorial ratings have on online human behavior, we report the results of a two large-scale in vivo experiments in social media. We find that small, random rating manipulations on social media posts and comments created significant changes in downstream ratings, resulting in significantly different final outcomes. We found positive herding effects for positive treatments on posts, increasing the final rating by 11.02% on average, but not for positive treatments on comments. Contrary to the results of related work, we found negative herding effects for negative treatments on posts and comments, decreasing the final ratings, on average, of posts by 5.15% and of comments by 37.4%. Compared to the control group, the probability of reaching a high rating (≥2,000) for posts is increased by 24.6% when posts receive the positive treatment and for comments it is decreased by 46.6% when comments receive the negative treatment. © 2017 ACM.",Herding effects; Social media; Social news; Voting,Behavioral research; Rating; Social networking (online); Social sciences; Digital sources; Herding effects; Human behaviors; In-vivo experiments; Online activities; Social media; Social news; Voting; Economic and social effects
Introduction to special issue on social media processing (TIST-SMP),2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026634362&doi=10.1145%2f3110318&partnerID=40&md5=ae061d3ceabb30cd78c53480a8bf886d,[No abstract available],,
A traffic flow approach to early detection of gathering events: Comprehensive results,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026651794&doi=10.1145%2f3078850&partnerID=40&md5=2b0861167bad4d180df0abb8901dbba5,"Given a spatial field and the traffic flow between neighboring locations, the early detection of gathering events (EDGE) problem aims to discover and localize a set of most likely gathering events. It is important for city planners to identify emerging gathering events thatmight cause public safety or sustainability concerns. However, it is challenging to solve the EDGE problem due to numerous candidate gathering footprints in a spatial field and the nontrivial task of balancing pattern quality and computational efficiency. Prior solutions to model the EDGE problem lack the ability to describe the dynamic flow of traffic and the potential gathering destinations because they rely on static or undirected footprints. In our recent work, wemodeled the footprint of a gathering event as a Gathering Graph (G-Graph), where the root of the directed acyclic G-Graph is the potential destination and the directed edges represent the most likely paths traffic takes to move toward the destination. We also proposed an efficient algorithm called SmartEdge to discover the most likely nonoverlapping G-Graphs in the given spatial field. However, it is challenging to perform a systematic performance study of the proposed algorithm, due to unavailability of the ground truth of gathering events. In this article, we introduce an event simulation mechanism, which makes it possible to conduct a comprehensive performance study of the SmartEdge algorithm. We measure the quality of the detected patterns, in a systematic way, in terms of timeliness and location accuracy. The results show that, on average, the SmartEdge algorithm is able to detect patterns within a grid cell away (less than 500 meters) of the simulated events and detect patterns of the simulated events as early as 10 minutes prior to the first arrival to the gathering event. © 2017 ACM.",Early detection; Gathering event; Spatial data mining,Computational efficiency; Data mining; Directed graphs; Comprehensive performance; Event simulation; Gathering event; Location accuracy; Non-trivial tasks; Performance study; Simulated events; Spatial data mining; Problem solving
Multi-hypergraph consistent sparse coding,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026636605&doi=10.1145%2f3078846&partnerID=40&md5=69dc0ab7dbe245dca52a0bf5bc2a0ead,"Sparse representation has been a powerful technique for modeling high-dimensional data. As an unsupervised technique to extract sparse representations, sparse coding encodes the original data into a new sparse code space and simultaneously learns a dictionary representing high-level semantics. Existingmethods have considered local manifold within high-dimensional data using graph/hypergraph Laplacian regularization, and more from the manifold could be utilized to improve the performance. In this article, we propose to further regulate the sparse coding so that the learned sparse codes can well reconstruct the hypergraph structure. In particular, we add a novel hypergraph consistency regularization term (HC) by minimizing the reconstruction error of the hypergraph incidence or weight matrix. Moreover, we extend the HC term to multi-hypergraph consistent sparse coding (MultiCSC) and automatically select the optimal manifold structure under the multi-hypergraph learning framework. We show that the optimization of MultiCSC can be solved efficiently, and that several existing sparse coding methods can fit into the general framework of MultiCSC as special cases. As a case study, hypergraph incidence consistent sparse coding is applied to perform semi-auto image tagging, demonstrating the effectiveness of hypergraph consistency regulation.We perform further experiments using MultiCSC for image clustering, which outperforms a number of baselines. © 2017 ACM.",Hypergraph consistency; Image clustering; Image tagging; Multiple-hypergraph learning; Sparse coding,Clustering algorithms; Codes (symbols); Data mining; Image coding; Semantics; Structural optimization; High dimensional data; Hypergraph; Image clustering; Image tagging; Laplacian regularizations; Sparse coding; Sparse representation; Unsupervised techniques; Graph theory
Social incentives in paid collaborative crowdsourcing,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026661497&doi=10.1145%2f3078852&partnerID=40&md5=38b5a28f3e66e05b629038c0711810e7,"Paid microtask crowdsourcing has traditionally been approached as an individual activity, with units of work created and completed independently by the members of the crowd. Other forms of crowdsourcing have, however, embraced more varied models, which allow for a greater level of participant interaction and collaboration. This article studies the feasibility and uptake of such an approach in the context of paid microtasks. Specifically, we compare engagement, task output, and task accuracy in a paired-worker model with the traditional, single-worker version. Our experiments indicate that collaboration leads to better accuracy and more output, which, in turn, translates into lower costs. We then explore the role of the social flow and social pressure generated by collaborating partners as sources of incentives for improved performance. We utilise a Bayesian method in conjunction with interface interaction behaviours to detect when one of the workers in a pair tries to exit the task. Upon this realisation, the other worker is presented with the opportunity to contact the exiting partner to stay: either for personal financial reasons (i.e., they have not completed enough tasks to qualify for a payment) or for fun (i.e., they are enjoying the task). The findings reveal that: (1) these socially motivated incentives can act as furtherance mechanisms to help workers attain and exceed their task requirements and produce better results than baseline collaborations; (2) microtask crowd workers are empathic (as opposed to selfish) agents, willing to go the extra mile to help their partners get paid; and, (3) social furtherance incentives create a win-win scenario for the requester and for the workers by helping more workers get paid by re-engaging them before they drop out. © 2017 ACM.",Crowdsourcing; Incentives; Social flow; Social pressure,Bayesian networks; Bayesian methods; Incentives; Interface interaction; Microtasks; Social flow; Social pressures; Task output; Worker models; Crowdsourcing
DUCT: An upper confidence bound approach to distributed constraint optimization problems,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025081163&doi=10.1145%2f3066156&partnerID=40&md5=21411d5b2bea36864c896ac97fbd0a5f,"We propose a distributed upper confidence bound approach, DUCT, for solving distributed constraint optimization problems. We compare four variants of this approach with a baseline random sampling algorithm, as well as other complete and incomplete algorithms for DCOPs. Under general assumptions, we theoretically show that the solution found by DUCT after T steps is approximately T-1-close to the optimal. Experimentally, we show that DUCT matches the optimal solution found by the well-known DPOP and O-DPOP algorithms on moderate-size problems, while always requiring less agent communication. For larger problems, where DPOP fails, we show that DUCT produces significantly better solutions than local, incomplete algorithms. Overall, we believe that DUCT is a practical, scalable algorithm for complex DCOPs. © 2017 ACM.",Coordination; Distributed constraint optimization; Multiagent systems; Tree search,Constrained optimization; Multi agent systems; Optimization; Agent communications; Coordination; Distributed constraint optimizations; Optimal solutions; Random sampling; Scalable algorithms; Tree search; Upper confidence bound; Ducts
Adult image and video recognition by a deep multicontext network and fine-to-coarse strategy,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025104422&doi=10.1145%2f3057733&partnerID=40&md5=fbe22269a11c0c7f2596d160fcd03dbb,"Adult image and video recognition is an important and challenging problem in the real world. Low-level feature cues do not produce good enough information, especially when the dataset is very large and has various data distributions. This issue raises a serious problem for conventional approaches. In this article, we tackle this problem by proposing a deep multicontext network with fine-to-coarse strategy for adult image and video recognition. We employ a deep convolution networks to model fusion features of sensitive objects in images. Global contexts and local contexts are both taken into consideration and are jointly modeled in a unified multicontext deep learning framework. To make the model more discriminative for diverse target objects, we investigate a novel hierarchical method, and a task-specific fine-to-coarse strategy is designed to make the multicontext modeling more suitable for adult object recognition. Furthermore, some recently proposed deep models are investigated. Our approach is extensively evaluated on four different datasets. One dataset is used for ablation experiments, whereas others are used for generalization experiments. Results show significant and consistent improvements over the state-of-the-art methods. © 2017 ACM.",Adult image and video recognition; Deep convolutional network; Fine-to-coarse strategy; Multicontext modeling,Convolution; Object recognition; Ablation experiments; Conventional approach; Convolutional networks; Fine-to-coarse strategy; Hierarchical method; Learning frameworks; State-of-the-art methods; Video recognition; Deep learning
Augmented collaborative filtering for sparseness reduction in personalized POI recommendation,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029783683&doi=10.1145%2f3086635&partnerID=40&md5=67cf374c736a774617a9a238095899c2,"As mobile device penetration increases, it has become pervasive for images to be associated with locations in the form of geotags. Geotags bridge the gap between the physical world and the cyberspace, giving rise to new opportunities to extract further insights into user preferences and behaviors. In this article, we aim to exploit geotagged photos from online photo-sharing sites for the purpose of personalized Point-of-Interest (POI) recommendation. Owing to the fact that most users have only very limited travel experiences, data sparseness poses a formidable challenge to personalized POI recommendation. To alleviate data sparseness, we propose to augment current collaborative filtering algorithms along from multiple perspectives. Specifically, hybrid preference cues comprising user-uploaded and user-favored photos are harvested to study users' tastes. Moreover, heterogeneous high-order relationship information is jointly captured from user social networks and POI multimodal contents with hypergraphmodels.We also build upon thematrix factorization algorithm to integrate the disparate sources of preference and relationship information, and apply our approach to directly optimize user preference rankings. Extensive experiments on a large and publicly accessible dataset well verified the potential of our approach for addressing data sparseness and offering quality recommendations to users, especially for those who have only limited travel experiences. © 2017 ACM.",Geotagged photos; Hypergraph-based learning; Personalized POI recommendation,Behavioral research; Electronic document exchange; Mobile devices; Collaborative filtering algorithms; Factorization algorithms; Geotagged photos; Hypergraph; Online Photo Sharing; Personalized POI recommendation; Preference ranking; Publicly accessible; Collaborative filtering
Homogeneity in web search results: Diagnosis and mitigation,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025114600&doi=10.1145%2f3057731&partnerID=40&md5=1699ae133863c8d0f40beec04466bfb0,"Access to diverse perspectives nurtures an informed citizenry. Google and Bing have emerged as the duopoly that largely arbitrates which English-language documents are seen by web searchers. We present our empirical study over the search results produced by Google and Bing that shows a large overlap. Thus, citizens may not gain different perspectives by simultaneously probing them for the same query. Fortunately, our study also shows that by mining Twitter data, one can obtain search results that are quite distinct from those produced by Google, Bing, and Bing News. Additionally, the users found those results to be quite informative. We also present two novel tools we designed for this study. One uses tensor analysis to derive lowdimensional compact representation of search results and study their behavior over time. The other uses machine learning and quantifies the similarity of results between two search engines by framing it as a prediction problem. Although these tools have different underpinnings, the analytical results obtained using them corroborate each other, which reinforces the confidence one can place in them for finding meaningful insights from big data. © 2017 ACM.",Big data; Bing; Google; Prediction; Search engine; Search result comparison; Socialmedia search; Tensor; Web search,Big data; Bins; Forecasting; Information retrieval; Tensors; Websites; Bing; Google; Result comparison; Socialmedia search; Web searches; Search engines
Visual classification of furniture styles,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022347548&doi=10.1145%2f3065951&partnerID=40&md5=2f78b7f0c59b36da6ef7e27439e96861,"Furniture style describes the discriminative appearance characteristics of furniture. It plays an important role in real-world indoor decoration. In this article, we explore the furniture style features and study the problem of furniture style classification. Differing from traditional object classification, furniture style classification aims at classifying different furniture in terms of the ""style"" that describes its appearance (e.g., American style, Gothic style, Rococo style, etc.) rather than the ""kind"" that is more related to its functional structure (e.g., bed, desk, etc.). To pursue efficient furniture style features, we construct a novel dataset of furniture styles that contains 16 common style categories and implement three strategies with respect to two categories of classification, that is, handcrafted classification and learning-based classification. First, we follow the typical image classification pipeline to extract the handcrafted features and train the classifier by support vector machine. Then we use the convolutional neural network to extract learning-based features from training images. To obtain comprehensive furniture style features, we finally combine the handcrafted image classification pipeline and the learning-based network. We experimentally evaluate the performances of handcrafted features and learning-based features of each strategy, and the results show the superiority of learning-based features and also the comprehensiveness of handcrafted features. © 2017 ACM.",Convolutional neural network; Furniture style; Image classification,Convolution; Education; Image classification; Image processing; Neural networks; Pipelines; Convolutional neural network; Functional structure; Furniture style; Object classification; Real-world; Training image; Visual classification; Classification (of information)
Unveiling correlations via mining human-thing interactions in the web of things,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022325358&doi=10.1145%2f3035967&partnerID=40&md5=afc2a0d74cc92150bd6f7a3506bd0dfb,"With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web services, physical things are becoming an integral part of the emerging ubiquitous Web. Finding correlations among ubiquitous things is a crucial prerequisite for many important applications such as things search, discovery, classification, recommendation, and composition. This article presents DisCor-T, a novel graph-based approach for discovering underlying connections of things via mining the rich content embodied in the human-thing interactions in terms of user, temporal, and spatial information. We model this various information using two graphs, namely a spatio-temporal graph and a social graph. Then, random walk with restart (RWR) is applied to find proximities among things, and a relational graph of things (RGT) indicating implicit correlations of things is learned. The correlation analysis lays a solid foundation contributing to improved effectiveness in things management and analytics. To demonstrate the utility of the proposed approach, we develop a flexible feature-based classification framework on top of RGT and perform a systematic case study. Our evaluation exhibits the strength and feasibility of the proposed approach. © 2017 ACM.",Correlation discovery; Random walk with restart; Web of Things,Graphic methods; Internet of things; Radio frequency identification (RFID); Random processes; Wireless sensor networks; Correlation analysis; Feature-based classification; Integral part; Random walk with restart; Relational graph; Spatial informations; Spatio-temporal graphs; Ubiquitous Web; Web services
Securely computing a ground speed model,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019845127&doi=10.1145%2f2998550&partnerID=40&md5=3eebe2ead6a8b2b7460481812da2ac9d,"Consider a server offering risk assessment services and potential clients of these services. The risk assessment model that is run by the server is based on current and historical data of the clients. However, the clients might prefer not sharing such sensitive data with external parties such as the server, and the server might consider the possession of this data as a liability rather than an asset. Secure multi-party computation (MPC) enables one, in principle, to compute any function while hiding the inputs to the function, and would thus enable the computation of the risk assessment model while hiding the client's data from the server. However, a direct application of a generic MPC solution to this problem is rather inefficient due to the large scale of the data and the complexity of the function. We examine a specific case of risk assessment - the ground speed model. In this model, the geographical locations of successive user-authentication attempts are compared, and a warning flag is raised if the physical speed required to move between these locations is greater than some threshold, and some other conditions, such as authentication from two related networks, do not hold.We describe a very efficient secure computation solution that is tailored for this problem. This solution demonstrates that a risk model can be applied over encrypted data with sufficient efficiency to fit the requirements of commercial systems. © 2017 ACM.",Risk models; Secure multi-party computation,Authentication; Complex networks; Cryptography; Speed control; Commercial systems; Geographical locations; Risk assessment models; Risk model; Secure computation; Secure multi-party computation; Sensitive datas; User authentication; Risk assessment
Transfer learning for behavior ranking,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022333986&doi=10.1145%2f3057732&partnerID=40&md5=7ee52a6c0128ebf9ec573116fd25e36b,"Intelligent recommendation has been well recognized as one of the major approaches to address the information overload problem in the big data era. A typical intelligent recommendation engine usually consists of three major components, that is, data as the main input, algorithms for preference learning, and system for user interaction and high-performance computation. We observe that the data (e.g., users' behavior) are usually in different forms, such as examinations (e.g., browse and collection) and ratings, where the former are often much more abundant than the latter. Although the data are in different representations, they are both related to users' true preferences and are also deemed complementary to each other for preference learning. However, very few ranking or recommendation algorithms have been developed to exploit such two types of user behavior. In this article, we focus on jointly modeling the examination behavior and rating behavior and develop a novel and efficient ranking-oriented recommendation algorithm accordingly. First, we formally define a new recommendation problem termed behavior ranking, which aims to build a ranking-oriented model by exploiting both the examination behavior and rating behavior. Second, we develop a simple and generic transfer to rank (ToR) algorithm for behavior ranking, which transfers knowledge of candidate items from a global preference learning task to a local preference learning task. Compared with the previous work on integrating heterogeneous user behavior, our ToR algorithm is the first ranking-oriented solution, which can effectively generate recommendations in a more direct manner than those regression-oriented methods. Extensive empirical studies show that our ToR algorithm performs significantly more accurately than the state-of-the-art methods in most cases. Furthermore, our ToR algorithm is very efficient in terms of the time complexity, which is similar to those for homogeneous user behavior alone. © 2017 ACM.",Behavior ranking; Collaborative recommendation; Preference learning; Transfer to rank,Big data; Education; Learning algorithms; Behavior ranking; Collaborative recommendation; High performance computation; Information overloads; Preference learning; Recommendation algorithms; State-of-the-art methods; Transfer to rank; Behavioral research
As-you-type social aware search,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022321869&doi=10.1145%2f3035969&partnerID=40&md5=952e1775185b8d77f1af29e0eddf89d5,"Modern search applications feature real-time as-you-type query search. In its elementary form, the problem consists in retrieving a set of k search results, that is, performing a search with a given prefix, and showing the top-ranked results. In this article, we focus on as-you-type keyword search over social media, that is, data published by users who are interconnected through a social network. We adopt a ""network-aware"" interpretation for information relevance, by which information produced by users who are closer to the user issuing a request is considered more relevant. This query model raises new challenges for effectiveness and efficiency in online search, even when the intent of the user is fully specified, as a complete query given as input in one keystroke. This is mainly because it requires a joint exploration of the social space and traditional IR indexes, such as inverted lists. We describe a memory-efficient and incremental prefix-based retrieval algorithm, which also exhibits an anytime behavior, allowing output of the most likely answer within any chosen runtime limit. We evaluate our approach through extensive experiments for several applications and search scenarios. We consider searching for posts in microblogging (Twitter and Tumblr), for businesses (Yelp), as well as for movies (Amazon) based on reviews. We also conduct a series of experiments comparing our algorithm with baselines using state-of-the-art techniques and measuring the improvements brought by several key optimizations. They show that our solution is effective in answering real-time as-you-type searches over social media. © 2017 ACM.",As-you-type search; Microblogging applications; Network-aware search; Social networks,Optimization; Search engines; Effectiveness and efficiencies; Information relevances; Memory efficient; Microblogging; Network-aware; Retrieval algorithms; Search application; State-of-the-art techniques; Social networking (online)
Cyber security and the role of intelligent systems in addressing its challenges,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019902009&doi=10.1145%2f3057729&partnerID=40&md5=69373abd876691cd1258a90c2484443b,[No abstract available],artificial intelligence; autonomous systems; cyber challenges; cyber collaboration; cyber definition; cyber phenomenon; Cyber security; cyber technology trends; intelligent systems; internet of things; machine learning,
Advanced economic control of electricity-based space heating systems in domestic coalitions with shared intermittent energy resources,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019640167&doi=10.1145%2f3041216&partnerID=40&md5=4e4b88b3832b61f575c587c4ed8cf358,"Over the past few years, Domestic Heating Automation Systems (DHASs) that optimize the domestic space heating control process with minimum user input, utilizing appropriate occupancy prediction technology, have emerged as commercial products (e.g., the smart thermostats from Nest and Honeywell). At the same time, many houses are being equipped with, potentially grid-connected, Intermittent Energy Resources (IERs), such as rooftop photovoltaic systems and/or small wind turbine generators. Now, in many regions of the world, such houses can sell energy to the grid but at a lower price than the price of buying it. In this context, and given the anticipated increase in electrification of heating, the next generation DHASs need to incorporate Advanced Economic Control (AEC). Such AEC can exploit the energy buffer that heating loads provide, in order to shift the consumption of electricity-based heating systems to follow the intermittent energy generation of the house. By so doing, the energy imported from the grid can be minimized and considerable monetary gains for the household can be achieved, without affecting the occupants' schedule. These benefits can be amplified still further in domestic coalitions, where a number of houses come together and share their IER generation to minimize their cumulative grid energy import. Given the above, in this work we extend a state-of-The-Art DHAS, to propose AdaHeat+, a practical DHAS, that, for the first time, incorporates AEC. Our work is applicable to both individual houses and domestic coalitions and comes complete with an allocation mechanism to share the coalition gains. Importantly, we propose an effective heuristic heating schedule planning approach for collective AEC that (i) has a complexity that scales in a linear and parallelizable manner with the coalition size, and (ii) enables AdaHeat+ to handle the distinct preferences, in balancing heating cost and thermal discomfort, of the households. Our approach relies on stochastic IER power output predictions. In this context, we propose a simple and effective formulation for the site-specific calibration of such predictions based on adaptive Gaussian processmodeling. Finally, we demonstrate the effectiveness of AdaHeat+ through real data evaluation, to show that collective AEC can improve heating cost-efficiency by up to 60%, compared to independent AEC (and even more when compared to no-AEC). © 2017 ACM.",Advanced economic control; Control; Energy systems; Smart grid; Space heating,Automation; Control; Costs; Electric power transmission networks; Energy resources; Forecasting; Heating equipment; Houses; Photovoltaic cells; Smart power grids; Space heating; Stochastic systems; Turbogenerators; Wind turbines; Allocation mechanism; Commercial products; Economic control; Energy systems; Occupancy predictions; Rooftop photovoltaic systems; Site specific calibration; Smart grid; Heating
On network neutrality measurements,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019960518&doi=10.1145%2f3040966&partnerID=40&md5=61a0e0b7c97dc93777e20977a1206e27,"Network level surveillance, censorship, and various man-in-the-middle attacks target only specific types of network traffic (e.g., HTTP, HTTPS, VoIP, or Email). Therefore, packets of these types will likely receive ""special"" treatment by a transit network or a man-in-the-middle attacker. A transit Internet Service Provider (ISP) or an attacker may pass the targeted traffic through special software or equipment to gather data or perform an attack. This creates a measurable difference between the performance of the targeted traffic versus the general case. In networking terms, it violates the principle of ""network neutrality,"" which states that all traffic should be treated equally. Many techniques were designed to detect network neutrality violations, and some have naturally suggested using them to detect surveillance and censorship. In this article, we show that the existing network neutrality measurement techniques can be easily detected and therefore circumvented. We then briefly propose a new approach to overcome the drawbacks of current measurement techniques. © 2017 ACM.",Adversarial model,HTTP; Network security; Internet service providers (ISP); Man in the middle; Man in the middle attacks; Measurement techniques; Network neutralities; Network traffic; New approaches; Transit networks; Internet service providers
A risk-scoring feedback model for webpages and web users based on browsing behavior,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019592153&doi=10.1145%2f2928274&partnerID=40&md5=489f8f09fba8245aa56ff79789f30e2d,"It has been claimed thatmany security breaches are often caused by vulnerable (näive) employees within the organization [Ponemon Institute LLC 2015a]. Thus, the weakest link in security is often not the technology itself but rather the people who use it [Schneier 2003]. In this article, we propose a machine learning scheme for detecting risky webpages and risky browsing behavior, performed by näive users in the organization. The scheme analyzes the interaction between two modules: one represents näive users, while the other represents risky webpages. It implements a feedback loop between these modules such that if a webpage is exposed to a lot of traffic from risky users, its ""risk score"" increases, while in a similar manner, as the user is exposed to risky webpages (with a high ""risk score""), his own ""risk score"" increases. The proposed scheme is tested on a real-world dataset of HTTP logs provided by a large American toolbar company. The results suggest that a feedback learning process involving webpages and users can improve the scoring accuracy and lead to the detection of unknown malicious webpages. © 2017 ACM.",Link-based ranking algorithms; Machine learning; Malware detection; Naïve user behavior; Spectral clustering,Artificial intelligence; Behavioral research; Chemical detection; Clustering algorithms; HTTP; Learning systems; Malware; Browsing behavior; Feed-back loop; Feedback learning process; Link-based ranking algorithms; Malware detection; Security breaches; Spectral clustering; User behaviors; Websites
Analyzing and optimizing access control choice architectures in online social networks,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019551728&doi=10.1145%2f3046676&partnerID=40&md5=663c1178080d809a441a63565e428565,"The way users manage access to their information and computers has a tremendous effect on the overall security and privacy of individuals and organizations. Usually, access management is conducted using a choice architecture, a behavioral economics concept that describes the way decisions are framed to users. Studies have consistently shown that the design of choice architectures, mainly the selection of default options, has a strong effect on the final decisions users make by nudging them toward certain behaviors. In this article, we propose a method for optimizing access control choice architectures in online social networks. We empirically evaluate the methodology on Facebook, the world's largest online social network, by measuring how well the default options cover the existing user choices and preferences and toward which outcome the choice architecture nudges users. The evaluation includes two parts: (a) collecting access control decisions made by 266 users of Facebook for a period of 3 months; and (b) surveying 533 participants who were asked to express their preferences regarding default options. We demonstrate how optimal defaults can be algorithmically identified from users' decisions and preferences, and we measure how existing defaults address users' preferences compared with the optimal ones. We analyze how access control defaults can better serve existing users, and we discuss how our method can be used to establish a common measuring tool when examining the effects of default options. © 2017 ACM.",Access control; Choice architecture; Privacy; Social networks,Access control; Data privacy; Economics; Electronic document exchange; Online systems; Social networking (online); Websites; Access control decisions; Access management; Behavioral economics; Default options; Final decision; Measuring tools; On-line social networks; Security and privacy; Network architecture
Bridging the Air Gap between Isolated Networks and Mobile Phones in a Practical Cyber-Attack,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019544098&doi=10.1145%2f2870641&partnerID=40&md5=bdb01692a00301692dfc4b8a6f3348a1,"Information is the most critical asset of modern organizations, and accordingly it is one of the resources most coveted by adversaries. When highly sensitive data is involved, an organization may resort to air gap isolation in which there is no networking connection between the inner network and the external world. While infiltrating an air-gapped network has been proven feasible in recent years, data exfiltration from an air-gapped network is still considered one of the most challenging phases of an advanced cyber-Attack. In this article, we present ""AirHopper,"" a bifurcated malware that bridges the air gap between an isolated network and nearby infectedmobile phones using FMsignals.While it is known that software can intentionally create radio emissions from a video card, this is the first time that mobile phones serve as the intended receivers of themaliciously crafted electromagnetic signals.We examine the attack model and its limitations and discuss implementation considerations such as modulation methods, signal collision, and signal reconstruction. We test AirHopper in an existing workplace at a typical office building and demonstrate how valuable data such as keylogging and files can be exfiltrated from physically isolated computers to mobile phones at a distance of 1-7 meters, with an effective bandwidth of 13-60 bytes per second. © 2017 ACM.",Air-gap; Apt; Bridging the air-gap; Cyberattack; Data exfiltration; Emsec; Fm radio; Tempest,Cellular telephones; Crime; Malware; Mobile devices; Mobile phones; Network security; Office buildings; Signal receivers; Telephone sets; Air-gaps; Cyber-attacks; Data exfiltration; Emsec; Fm radios; Tempest; Computer crime
Greetings,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019562095&doi=10.1145%2f3057727&partnerID=40&md5=e7c2c712960b3022e62489ec30e1ec82,[No abstract available],,
Mobile social multimedia analytics in the big data era: An introduction to the special issue,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018477128&doi=10.1145%2f3040934&partnerID=40&md5=098a47d3000d5191e37f7fbefbabeb61,[No abstract available],,
Large sparse cone non-negative matrix factorization for image annotation,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017954657&doi=10.1145%2f2987379&partnerID=40&md5=c1cafe45c9e2fe66882009f29ed4e4a8,"Image annotation assigns relevant tags to query images based on their semantic contents. Since Non-negative Matrix Factorization (NMF) has the strong ability to learn parts-based representations, recently, a number of algorithms based on NMF have been proposed for image annotation and have achieved good performance. However, most of the efforts have focused on the representations of images and annotations. The properties of the semantic parts have not been well studied. In this article, we revisit the sparseness-constrained NMF (sNMF) proposed by Hoyer [2004]. By endowing the sparseness constraint with a geometric interpretation and sNMF with theoretical analyses of the generalization ability, we show that NMF with such a sparseness constraint has three advantages for image annotation tasks: (i) The sparseness constraint is more ℓ0-norm oriented than the ℓ1-norm-based sparseness, which significantly enhances the ability of NMF to robustly learn semantic parts. (ii) The sparseness constraint has a large cone interpretation and thus allows the reconstruction error of NMF to be smaller, which means that the learned semantic parts are more powerful to represent images for tagging. (iii) The learned semantic parts are less correlated, which increases the discriminative ability for annotating images. Moreover, we present a new efficient large sparse cone NMF (LsCNMF) algorithm to optimize the sNMF problem by employing the Nesterov's optimal gradient method. We conducted experiments on the PASCAL VOC07 dataset and demonstrated the effectiveness of LsCNMF for image annotation. © 2017 ACM.",Image annotation; Nesterovs optimal gradient; Non-negative matrix factorization; Sparseness constraint,Gradient methods; Image analysis; Matrix algebra; Semantics; Discriminative ability; Generalization ability; Geometric interpretation; Image annotation; Nonnegative matrix factorization; Optimal gradient methods; Reconstruction error; Sparseness constraints; Factorization
Cost-optimized microblog distribution over geo-distributed data centers: Insights from cross-media analysis,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018843170&doi=10.1145%2f3014431&partnerID=40&md5=403430b403c19b217cb6db9c11916111,"The unprecedent growth of microblog services poses significant challenges on network traffic and service latency to the underlay infrastructure (i.e., geo-distributed data centers). Furthermore, the dynamic evolution in microblog status generates a huge workload on data consistence maintenance. In this article, motivated by insights of cross-media analysis-based propagation patterns, we propose a novel cache strategy for microblog service systems to reduce the inter-data center traffic and consistence maintenance cost, while achieving low service latency. Specifically, we first present a microblog classification method, which utilizes the external knowledge from correlated domains, to categorize microblogs. Then we conduct a large-scale measurement on a representative online social network system to study the category-based propagation diversity on region and time scales. These insights illustrate social common habits on creating and consuming microblogs and further motivate our architecture design. Finally, we formulate the content cache problem as a constrained optimization problem. By jointly using the Lyapunov optimization framework and simplex gradient method, we find the optimal online control strategy. Extensive trace-driven experiments further demonstrate that our algorithm reduces the system cost by 24.5% against traditional approaches with the same service latency. © 2017 ACM.",Cross-media analysis; Data center; Performance optimization; Social media analytics,Constrained optimization; Cost reduction; Costs; Data flow analysis; Gradient methods; Optimization; Social networking (online); Constrained optimi-zation problems; Cross-media; Data centers; Large-scale measurement; On-line social networks; Performance optimizations; Social media analytics; Trace driven experiments; Cost benefit analysis
ST-SAGE: A spatial-Temporal sparse additive generative model for spatial item recommendation,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018854332&doi=10.1145%2f3011019&partnerID=40&md5=c648121319435d347ccec266eaba2aa0,"With the rapid development of location-based social networks (LBSNs), spatial item recommendation has become an important mobile application, especially when users travel away from home. However, this type of recommendation is very challenging compared to traditional recommender systems. A user may visit only a limited number of spatial items, leading to a very sparse user-item matrix. This matrix becomes even sparser when the user travels to a distant place, as most of the items visited by a user are usually located within a short distance from the user's home. Moreover, user interests and behavior patterns may vary dramatically across different time and geographical regions. In light of this, we propose ST-SAGE, a spatial-Temporal sparse additive generative model for spatial item recommendation in this article. ST-SAGE considers both personal interests of the users and the preferences of the crowd in the target region at the given time by exploiting both the co-occurrence patterns and content of spatial items. To further alleviate the data-sparsity issue, ST-SAGE exploits the geographical correlation by smoothing the crowd's preferences over a well-designed spatial index structure called the spatial pyramid. To speed up the training process of ST-SAGE, we implement a parallel version of themodel inference algorithm on the GraphLab framework.We conduct extensive experiments; the experimental results clearly demonstrate that ST-SAGE outperforms the state-of-The-Art recommender systems in terms of recommendation effectiveness, model training efficiency, and online recommendation efficiency. ©c 2017 ACM 2157-6904/2017/04-ART48 15.00.",,Efficiency; Geographical regions; Inference engines; Matrix algebra; Online systems; Recommender systems; Behavior patterns; Co-occurrence pattern; Geographical correlation; Inference algorithm; Location-based social networks; Mobile applications; Recommendation efficiency; Spatial index structure; Behavioral research
Modeling topics and behavior of microbloggers: An integrated approach,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018829153&doi=10.1145%2f2990507&partnerID=40&md5=b2a80e318fdd0101f5ff17d6764e0949,"Microblogging encompasses both user-generated content and behavior. When modeling microblogging data, one has to consider personal and background topics, as well as how these topics generate the observed content and behavior. In this article, we propose the Generalized Behavior-Topic (GBT) model for simultaneously modeling background topics and users' topical interest in microblogging data. GBT considersmultiple topical communities (or realms) with different background topical interests while learning the personal topics of each user and the user's dependence on realms to generate both content and behavior. This differentiates GBT from other previous works that consider either one realm only or content data only. By associating user behavior with the latent background and personal topics, GBT helps to model user behavior by the two types of topics. GBT also distinguishes itself from other earlier works by modeling multiple types of behavior together. Our experiments on two Twitter datasets show that GBT can effectively mine the representative topics for each realm. We also demonstrate that GBT significantly outperforms other state-of-The-Art models in modeling content topics and user profiling. © 2017 ACM 2157-6904/2017/04-ART44 15.00.",,Content data; Integrated approach; Microblogging; State of the art; User behaviors; User profiling; User-generated content; Behavioral research
Learning user attributes via mobile social multimedia analytics,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018193074&doi=10.1145%2f2963105&partnerID=40&md5=025311a257d6e9de2e63f52f58dd016e,"Learning user attributes from mobile social media is a fundamental basis for many applications, such as personalized and targeting services. A large and growing body of literature has investigated the user attributes learning problem. However, far too little attention has been paid to jointly consider the dual heterogeneities of user attributes learning by harvesting multiple social media sources. In particular, user attributes are complementarily and comprehensively characterized by multiple social media sources, including footprints from Foursqare, daily updates from Twitter, professional careers from Linkedin, and photo posts from Instagram. On the other hand, attributes are inter-correlated in a complex way rather than independent to each other, and highly related attributes may share similar feature sets. Towards this end, we proposed a unified model to jointly regularize the source consistency and graph-constrained relatedness among tasks. As a byproduct, it is able to learn the attribute-specific and attribute-sharing features via graph-guided fused lasso penalty. Besides, we have theoretically demonstrated its optimization. Extensive evaluations on a real-world dataset thoroughly demonstrated the effectiveness of our proposed model. © 2017 ACM.",Algorithms; Experimentation; H.3.3 [information systems]: information search and retrieval; Learning user attributes; Mobile social multimedia analytic; Occupation inference; Performance,Algorithms; Employment; Inference engines; Social networking (online); Experimentation; Information search and retrieval; Learning user attributes; Mobile social multimedia analytic; Occupation inference; Performance; Search engines
CRADLE: An online plan recognition algorithm for exploratory domains,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018808729&doi=10.1145%2f2996200&partnerID=40&md5=de494f4eb56d5431b27e62e9b5287056,"In exploratory domains, agents' behaviors include switching between activities, extraneous actions, and mistakes. Such settings are prevalent in real world applications such as interaction with open-ended software, collaborative office assistants, and integrated development environments. Despite the prevalence of such settings in the real world, there is scarce work in formalizing the connection between high-level goals and low-level behavior and inferring the former from the latter in these settings. We present a formal grammar for describing users' activities in such domains. We describe a new top-down plan recognition algorithm called CRADLE (Cumulative Recognition of Activities and Decreasing Load of Explanations) that uses this grammar to recognize agents' interactions in exploratory domains.We compare the performance of CRADLE with state-of-The-Art plan recognition algorithms in several experimental settings consisting of real and simulated data. Our results show that CRADLE was able to output plans exponentially more quickly than the state-of-The-Art without compromising its correctness, as determined by domain experts. Our approach can form the basis of future systems that use plan recognition to provide real-Time support to users in a growing class of interesting and challenging domains. © 2017 ACM 2157-6904/2017/04-ART45 15.00.",,Application programs; Computational grammars; Domain experts; Experimental settings; Formal grammars; High-level goals; Integrated development environment; Plan recognition; Plan-recognition algorithms; State of the art; Real time systems
Location-based parallel tag completion for geo-Tagged socialimage retrieval,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018800967&doi=10.1145%2f3001593&partnerID=40&md5=01f16f47c3b6a9ba2117ce4867dd86d1,"Having benefited from tremendous growth of user-generated content, social annotated tags get higher importance in the organization and retrieval of large-scale image databases on Online Sharing Websites (OSW). To obtain high-quality tags from existing community contributed tags with missing information and noise, tag-based annotation or recommendation methods have been proposed for performance promotion of tag prediction. While images from OSW contain rich social attributes, they have not taken full advantage of rich social attributes and auxiliary information associated with social images to construct global information completion models. In this article, beyond the image-Tag relation, we take full advantage of the ubiquitous GPS locations and image-user relationship to enhance the accuracy of tag prediction and improve the computational efficiency. For GPS locations, we define the popular geo-locations where people tend to take more images as Points of Interests (POI), which are discovered by mean shift approach. For image-user relationship, we integrate a localized prior constraint, expecting the completed tag sub-matrix in each POI to maintain consistency with users' tagging behaviors. Based on these two key issues, we propose a unified tag matrix completion framework, which learns the image-Tag relation within each POI. To solve the optimization problem, an efficient proximal sub-gradient descent algorithm is designed. The model optimization can be easily parallelized and distributed to learn the tag sub-matrix for each POI. Extensive experimental results reveal that the learned tag sub-matrix of each POI reflects the major trend of users' tagging results with respect to different POIs and users, and the parallel learning process provides strong support for processing large-scale online image databases. To fit the response time requirement and storage limitations of Tag-based Image Retrieval (TBIR) on mobile devices, we introduce Asymmetric Locality Sensitive Hashing (ALSH) to reduce the time cost and meanwhile improve the efficiency of retrieval. © 2017 ACM.",Asymmetric locality sensitive hashing; Geo-location information; Social image retrieval; Tag matrix completion,Computational efficiency; Efficiency; Global positioning system; Location; Optimization; User interfaces; Geolocations; Gradient descent algorithms; Locality sensitive hashing; Matrix completion; Points of Interest(POI); Social image retrievals; Support for processing; User-generated content; Image retrieval
Tracking illicit drug dealing and abuse on instagram using multimodal analysis,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014145651&doi=10.1145%2f3011871&partnerID=40&md5=5c3761d08625b4f370fd6e5e1bd6e8b5,"Illicit drug trade via social media sites, especially photo-oriented Instagram, has become a severe problem in recent years. As a result, tracking drug dealing and abuse on Instagram is of interest to law enforcement agencies and public health agencies. However, traditional approaches are based on manual search and browsing by trained domain experts, which suffers from the problem of poor scalability and reproducibility. In this article, we propose a novel approach to detecting drug abuse and dealing automatically by utilizing multimodal data on social media. This approach also enables us to identify drug-related posts and analyze the behavior patterns of drug-related user accounts. To better utilize multimodal data on social media, multimodal analysis methods including multi-task learning and decision-level fusion are employed in our framework. We collect three datasets using Instagram and web search engine for training and testing our models. Experiment results on expertly labeled data have demonstrated the effectiveness of our approach, as well as its scalability and reproducibility over labor-intensive conventional approaches. © 2017 ACM.",Illicit drug; Multimodal analysis; Social media,Scalability; Search engines; Social networking (online); Conventional approach; Decision level fusion; Illicit drug; Law-enforcement agencies; Multimodal analysis; Public-health agencies; Social media; Traditional approaches; Modal analysis
Unryderstanding the relationship between human behavior and susceptibility to cyber attacks: A data-driven approach,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017226568&doi=10.1145%2f2890509&partnerID=40&md5=b4b861eddb35f483a4cd8684b103803a,"Despite growing speculation about the role of human behavior in cyber-security of machines, concrete datadriven analysis and evidence have been lacking. Using Symantec's WINE platform, we conduct a detailed study of 1.6 million machines over an 8-month period in order to learn the relationship between user behavior and cyber attacks against their personal computers. We classify users into 4 categories (gamers, professionals, software developers, and others, plus a fifth category comprising everyone) and identify a total of 7 features that act as proxies for human behavior. For each of the 35 possible combinations (5 categories times 7 features), we studied the relationship between each of these seven features and one dependent variable, namely the number of attempted malware attacks detected by Symantec on the machine. Our results show that there is a strong relationship between several features and the number of attempted malware attacks. Had these hosts not been protected by Symantec's anti-virus product or a similar product, they would likely have been infected. Surprisingly, our results show that software developers are more at risk of engaging in risky cyber-behavior than other categories. © 2017 ACM 2157-6904/2017/0.",Computer virus; Malware; User behavior,Behavioral research; Computer crime; Computer viruses; Crime; Network security; Personal computers; Social sciences; Cyber behaviors; Data-driven analysis; Data-driven approach; Dependent variables; Human behaviors; Malware attacks; Software developer; User behaviors; Malware
Optimal scheduling of cybersecurity analysts for minimizing risk,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014298209&doi=10.1145%2f2914795&partnerID=40&md5=5e471931088c7318a1cfa6051a2ba57b,"Cybersecurity threats are on the rise with evermore digitization of the information that many day-to-day systems depend upon. The demand for cybersecurity analysts outpaces supply, which calls for optimal management of the analyst resource. Therefore, a key component of the cybersecurity defense system is the optimal scheduling of its analysts. Sensor data is analyzed by automatic processing systems, and alerts are generated. A portion of these alerts is considered to be significant, which requires thorough examination by a cybersecurity analyst. Risk, in this article, is defined as the percentage of unanalyzed or not thoroughly analyzed alerts among the significant alerts by analysts. The article presents a generalized optimization model for scheduling cybersecurity analysts to minimize risk (a.k.a., maximize significant alert coverage by analysts) and maintain risk under a pre-determined upper bound. The article tests the optimization model and its scalability on a set of given sensors with varying analyst experiences, alert generation rates, system constraints, and system requirements. Results indicate that the optimization model is scalable and is capable of identifying both the right mix of analyst expertise in an organization and the sensor-to-analyst allocation in order to maintain risk below a given upper bound. Several meta-principles are presented, which are derived from the optimization model, and they further serve as guiding principles for hiring and scheduling cybersecurity analysts. The simulation studies (validation) of the optimization model outputs indicate that risk varies non-linearly with an analyst/sensor ratio, and for a given analyst/sensor ratio, the risk is independent of the number of sensors in the system. © 2017 ACM.",Cybersecurity analysts; Optimization; Resource allocation; Risk mitigation; Scheduling; Simulation,Optimization; Resource allocation; Automatic processing system; Cyber security; Guiding principles; Optimal management; Optimization modeling; Risk mitigation; Simulation; System requirements; Scheduling
Daehr: A discriminant analysis framework for electronic health record data and an application to early detection of mental health disorders,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017180363&doi=10.1145%2f3007195&partnerID=40&md5=4f06f9f58d71f2dba65f755966a6c943,"Electronic health records (EHR) provide a rich source of temporal data that present a unique opportunity to characterize disease patterns and risk of imminent disease. While many data-mining tools have been adopted for EHR-based disease early detection, linear discriminant analysis (LDA) is one of the most commonly used statistical methods. However, it is difficult to train an accurate LDA model for early disease diagnosis when too few patients are known to have the target disease. Furthermore, EHR data are heterogeneous with significant noise. In such cases, the covariance matrices used in LDA are usually singular and estimated with a large variance. This article presents Daehr, an extension of the LDA framework using electronic health record data to address these issues. Beyond existing LDA analyzers, we propose Daehr to (1) eliminate the data noise caused by the manual encoding of EHR data and (2) lower the variance of parameter (covariance matrices) estimation for LDA models when only a few patients' EHR are available for training. To achieve these two goals, we designed an iterative algorithm to improve the covariance matrix estimation with embedded data-noise/parameter-variance reduction for LDA. We evaluated Daehr extensively using the College Health Surveillance Network, a large, real-world EHR dataset. Specifically, our experiments compared the performance of LDA to three baselines (i.e., LDA and its derivatives) in identifying college students at high risk for mental health disorders from 23 U.S. universities. Experimental results demonstrate Daehr significantly outperforms the three baselines by achieving 1.4%-19.4% higher accuracy and a 7.5%-43.5% higher F1-score. © 2017 ACM.",Anxiety/depression; Early detection; Electronic health data; Predictive models; Temporal order,Covariance matrix; Data mining; Diagnosis; Discriminant analysis; Health; Health risks; Iterative methods; Records management; Students; Anxiety/depression; Covariance matrix estimation; Electronic health; Electronic health record; Health surveillances; Linear discriminant analysis; Predictive models; Temporal order; Matrix algebra
Algorithms for graph-constrained coalition formation in the real world,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014167871&doi=10.1145%2f3040967&partnerID=40&md5=779ba066825a274eb9dc01701685ee9b,"Coalition formation typically involves the coming together of multiple, heterogeneous, agents to achieve both their individual and collective goals. In this article, we focus on a special case of coalition formation known as Graph-Constrained Coalition Formation (GCCF) whereby a network connecting the agents constrains the formation of coalitions. We focus on this type of problem given that in many real-world applications, agents may be connected by a communication network or only trust certain peers in their social network.We propose a novel representation of this problem based on the concept of edge contraction, which allows us to model the search space induced by the GCCF problem as a rooted tree. Then, we propose an anytime solution algorithm (Coalition Formation for Sparse Synergies (CFSS)), which is particularly efficient when applied to a general class of characteristic functions called m+ a functions. Moreover, we show how CFSS can be efficiently parallelised to solve GCCF using a nonredundant partition of the search space. We benchmark CFSS on both synthetic and realistic scenarios, using a real-world dataset consisting of the energy consumption of a large number of households in the UK. Our results show that, in the best case, the serial version of CFSS is four orders of magnitude faster than the state of the art, while the parallel version is 9.44 times faster than the serial version on a 12-core machine. Moreover, CFSS is the first approach to provide anytime approximate solutions with quality guarantees for very large systems of agents (i.e., with more than 2,700 agents). © 2017 ACM.",Coalition formation; Collective energy purchasing; Graphs; Networks,Networks (circuits); Approximate solution; Characteristic functions; Coalition formations; Graphs; Orders of magnitude; Realistic scenario; Solution algorithms; Very large systems; Energy utilization
Data-driven frequency-based airline profit maximization,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017123008&doi=10.1145%2f3041217&partnerID=40&md5=2cddb80d24cb230fa7f634f48bac67e3,"Although numerous traditional models predict market share and demand along airline routes, the prediction of existing models is not precise enough, and to the best of our knowledge, there is no use of data mining-based forecasting techniques for improving airline profitability. We propose the maximizing airline profits (MAP) architecture designed to help airlines and make two key contributions in airline market share and route demand prediction and prediction-based airline profit optimization. Compared to past methods used to forecast market share and demand along airline routes, we introduce a novel ensemble forecasting (MAP-EF) approach considering two new classes of features: (i) features derived from clusters of similar routes and (ii) features based on equilibrium pricing. We show that MAP-EF achieves much better Pearson correlation coefficients (greater than 0.95 vs. 0.82 for market share, 0.98 vs. 0.77 for demand) and R2-values compared to three state-of-the-art works for forecasting market share and demand while showing much lower variance. Using the results of MAP-EF, we develop MAP-bilevel branch and bound (MAP-BBB) and MAP-greedy (MAP-G) algorithms to optimally allocate flight frequencies over multiple routes to maximize an airline's profit. We also study two extensions of the profit maximization problem considering frequency constraints and long-term profits. Furthermore, we develop algorithms for computing Nash equilibrium frequencies when there are multiple strategic airlines. Experimental results show that airlines can increase profits by a significant margin. All experiments were conducted with data aggregated from four sources: the U.S. Bureau of Transportation Statistics (BTS), the U.S. Bureau of Economic Analysis (BEA), the National Transportation Safety Board (NTSB), and the U.S. Census Bureau (CB). © 2017 ACM 2157-6904/2017/02-ART61 $15.00.",Airline demand and market share prediction; Airline profit maximization; Ensemble prediction; Regression,Commerce; Competition; Correlation methods; Data mining; Economic analysis; Forecasting; Optimization; Profitability; Bureau of Economic Analysis; Ensemble prediction; Forecasting techniques; Market share; National transportation safety boards; Pearson correlation coefficients; Profit maximization; Regression; Air transportation
Event classification in microblogs via social tracking,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017178113&doi=10.1145%2f2967502&partnerID=40&md5=8ca1ca2ed7b0a5be403f8aa47d4449a9,"Social media websites have become important information sharing platforms. The rapid development of social media platforms has led to increasingly large-scale social media data, which has shown remarkable societal and marketing values. There are needs to extract important events in live social media streams. However, microblogs event classification is challenging due to two facts, i.e., the short/conversational nature and the incompatible meanings between the text and the corresponding image in social posts, and the rapidly evolving contents. In this article, we propose to conduct event classification via deep learning and social tracking. First, we introduce a Multi-modal Multi-instance Deep Network (M2DN) for microblogs classification, which is able to handle the weakly labeled microblogs data oriented from the incompatible meanings inside microblogs. Besides predicting each microblogs as predefined events, we propose to employ social tracking to extract social-related auxiliary information to enrich the testing samples. We extract a set of candidate-relevant microblogs in a short time window by using social connections, such as related users and geographical locations. All these selected microblogs and the testing data are formulated in a Markov Random Field model. The inference on the Markov Random Field is conducted to update the classification results of the testing microblogs. This method is evaluated on the Brand-Social-Net dataset for classification of 20 events. Experimental results and comparison with the state of the arts show that the proposed method can achieve better performance for the event classification task. © 2017 ACM.",Event classification; Markov Random Field (MRF); Multi-instance; Multi-modal; Social tracking,Image segmentation; Markov processes; Social networking (online); Text processing; Event classification; Geographical locations; Information sharing platforms; Markov Random Field model; Markov Random Fields; Multi-instance; Multi-modal; Social media platforms; Classification (of information)
Automatic construction of statechart-based anomaly detection models for multi-threaded industrial control systems,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014190566&doi=10.1145%2f3011018&partnerID=40&md5=40fff73999707775683e6f09087151e3,"Traffic of Industrial Control System (ICS) between the Human Machine Interface (HMI) and the Programmable Logic Controller (PLC) is known to be highly periodic. However, it is sometimes multiplexed, due to asynchronous scheduling. Modeling the network traffic patterns of multiplexed ICS streams using Deterministic Finite Automata (DFA) for anomaly detection typically produces a very large DFA and a high false-alarm rate. In this article, we introduce a new modeling approach that addresses this gap. Our Statechart DFA modeling includes multiple DFAs, one per cyclic pattern, together with a DFA-selector that de-multiplexes the incoming traffic into sub-channels and sends them to their respective DFAs. We demonstrate how to automatically construct the statechart from a captured traffic stream. Our unsupervised learning algorithms first build a Discrete-Time Markov Chain (DTMC) from the stream. Next, we split the symbols into sets, one per multiplexed cycle, based on symbol frequencies and node degrees in the DTMC graph. Then, we create a sub-graph for each cycle and extract Euler cycles for each sub-graph. The final statechart is comprised of one DFA per Euler cycle. The algorithms allow for non-unique symbols, which appear in more than one cycle, and also for symbols that appear more than once in a cycle. We evaluated our solution on traces from a production ICS using the Siemens S7-0x72 protocol. We also stress-tested our algorithms on a collection of synthetically-generated traces that simulated multiplexed ICS traces with varying levels of symbol uniqueness and time overlap. The algorithms were able to split the symbols into sets with 99.6% accuracy. The resulting statechartmodeled the traces with a median false-alarm rate of as low as 0.483%. In all but the most extreme scenarios, the Statechart model drastically reduced both the false-alarm rate and the learned model size in comparison with the naive single-DFA model. © 2017 ACM.",ICS; Network-intrusion-detection-system; S7; SCADA; Siemens; Statechart,Alarm systems; Control systems; Errors; Finite automata; Graph theory; Image resolution; Integrated circuits; Intrusion detection; Markov processes; Multiplexing; Signal detection; Deterministic finite automata; Discrete time Markov chains; Industrial control systems; Network intrusion detection systems; Programmable logic controllers (PLC); SCADA; Siemens; State charts; Intelligent control
Exploiting social-mobile information for location visualization,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011340412&doi=10.1145%2f3001594&partnerID=40&md5=df22cac67b6c8001c8f82d3aeb17360a,"With a smart phone at hand, it becomes easy now to snap pictures and publish them online with few lines of texts. The GPS coordinates and User-Generated Content (UGC) data embedded in the shared photos provide opportunities to exploit important knowledge to tackle interesting tasks like geographically organizing photos and location visualization. In this work, we propose to organize photos both geographically and semantically, and investigate the problem of location visualization from multiple semantic themes. The novel visualization scheme provides a rich display landscape for geographical exploration from versatile views. A two-level solution is presented, where we first identify the highly photographed places of interest (POI) and discover their focused themes, and then aggregate the lower-level POI themes to generate the higher-level city themes for location visualization. We have conducted experiments on crawled Flickr and Instagram data and exhibited the visualization for the cities of Singapore and Sydney. The experimental results have validated the proposed method and demonstrated the potentials of location visualization from multiple themes. © 2017 ACM.",Geotagged photo; Location visualization; Point of interest,Location; Semantics; Smartphones; Visualization; Geotagged photo; GPS Coordinates; Mobile information; Novel visualizations; Point of interest; Singapore; User generated content (UGC); Data visualization
Directly optimize diversity evaluation measures: A new approach to search result diversification,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011411552&doi=10.1145%2f2983921&partnerID=40&md5=05197ddb910d0b2cb49d2a3afc63c012,"The queries issued to search engines are often ambiguous or multifaceted, which requires search engines to return diverse results that can fulfill as many different information needs as possible; this is called search result diversification. Recently, the relational learning to rank model, which designs a learnable ranking function following the criterion of maximal marginal relevance, has shown effectiveness in search result diversification [Zhu et al. 2014]. The goodness of a diverse ranking model is usually evaluated with diversity evaluation measures such as alpha;-NDCG [Clarke et al. 2008], ERR-IA [Chapelle et al. 2009], and D#-NDCG [Sakai and Song 2011]. Ideally the learning algorithm would train a ranking model that could directly optimize the diversity evaluation measures with respect to the training data. Existing relational learning to rank algorithms, however, only train the ranking models by optimizing loss functions that loosely relate to the evaluation measures. To deal with the problem, we propose a general framework for learning relational ranking models via directly optimizing any diversity evaluation measure. In learning, the loss function upper-bounding the basic loss function defined on a diverse ranking measure is minimized. We can derive new diverse ranking algorithms under the framework, and several diverse ranking algorithms are created based on different upper bounds over the basic loss function. We conducted comparisons between the proposed algorithms with conventional diverse ranking methods using the TREC benchmark datasets. Experimental results show that the algorithms derived under the diverse learning to rank framework always significantly outperform the state-of-the-art baselines. © 2017 ACM.",Diversity evaluation measure; Relational learning to rank; Search result diversification,Function evaluation; Search engines; Benchmark datasets; Evaluation measures; Learning to rank; Ranking algorithm; Ranking functions; Ranking measures; Relational learning; Search result diversification; Learning algorithms
Learning k for kNN Classification,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011392056&doi=10.1145%2f2990508&partnerID=40&md5=53df9e905027945160b690d77de05dcc,"The K Nearest Neighbor (kNN) method has widely been used in the applications of data mining andmachine learning due to its simple implementation and distinguished performance. However, setting all test data with the same κvalue in the previous kNN methods has been proven to make these methods impractical in real applications. This article proposes to learn a correlation matrix to reconstruct test data points by training data to assign different κ values to different test data points, referred to as the Correlation Matrix kNN (CM-kNN for short) classification. Specifically, the least-squares loss function is employed to minimize the reconstruction error to reconstruct each test data point by all training data points. Then, a graph Laplacian regularizer is advocated to preserve the local structure of the data in the reconstruction process. Moreover, an ℓ1-norm regularizer and an ℓ2,1-norm regularizer are applied to learn different κ values for different test data and to result in low sparsity to remove the redundant/noisy feature from the reconstruction process, respectively. Besides for classification tasks, the kNNmethods (including our proposed CM-kNN method) are further utilized to regression and missing data imputation.We conducted sets of experiments for illustrating the efficiency, and experimental results showed that the proposed method was more accurate and efficient than existing kNN methods in data-mining applications, such as classification, regression, and missing data imputation. Copyright is held by the owner/author(s). Publication rights licensed to ACM.",kNN method; Missing data imputation; Sparse learning,Data mining; Matrix algebra; Nearest neighbor search; Testing; Data mining applications; K-nearest neighbor method; K-NN classifications; kNN method; Missing data imputations; Reconstruction error; Reconstruction process; Sparse learning; Classification (of information)
Nonnegative matrix factorization with integrated graph and feature learning,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011360886&doi=10.1145%2f2987378&partnerID=40&md5=a0e98701a2785f328a9395b517c881a4,"Matrix factorization is a useful technique for data representation in many data mining andmachine learning tasks. Particularly, for data sets with all nonnegative entries, matrix factorization often requires that factor matrices be nonnegative, leading to nonnegative matrix factorization (NMF). One important application of NMF is for clustering with reduced dimensions of the data represented in the new feature space. In this paper, we propose a new graph regularized NMF method capable of feature learning and apply it to clustering. Unlike existing NMF methods that treat all features in the original feature space equally, our method distinguishes features by incorporating a feature-wise sparse approximation error matrix in the formulation. It enables important features to be more closely approximated by the factor matrices. Meanwhile, the graph of the data is constructed using cleaner features in the feature learning process, which integrates feature learning andmanifold learning procedures into a unifiedNMFmodel. This distinctly differs from applying the existing graph-based NMF models after feature selection in that, when these two procedures are independently used, they often fail to align themselves toward obtaining a compact and most expressive data representation. Comprehensive experimental results demonstrate the effectiveness of the proposed method, which outperforms state-of-the-art algorithms when applied to clustering. © 2017 ACM.",Clustering; Feature learning; Manifold learning; Non-negative matrix factorization,Clustering algorithms; Data mining; Factorization; Graphic methods; Clustering; Data representations; Feature learning; Manifold learning; Matrix factorizations; Nonnegative matrix factorization; Sparse approximations; State-of-the-art algorithms; Matrix algebra
A distribution separation method using irrelevance feedback data for information retrieval,2017,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011382356&doi=10.1145%2f2994608&partnerID=40&md5=18cb82b35cc96ebaa91e5ae0784d5554,"In many research and application areas, such as information retrieval and machine learning, we often encounter dealing with a probability distribution that is mixed by one distribution that is relevant to our task in hand and the other that is irrelevant and that we want to get rid of. Thus, it is an essential problem to separate the irrelevant distribution from the mixture distribution. This article is focused on the application in Information Retrieval, where relevance feedback is a widely used technique to build a refined query model based on a set of feedback documents. However, in practice, the relevance feedback set, even provided by users explicitly or implicitly, is often a mixture of relevant and irrelevant documents. Consequently, the resultant query model (typically a term distribution) is often a mixture rather than a true relevance term distribution, leading to a negative impact on the retrieval performance. To tackle this problem, we recently proposed a Distribution SeparationMethod (DSM), which aims to approximate the true relevance distribution by separating a seed irrelevance distribution from the mixture one. While it achieved a promising performance in an empirical evaluation with simulated explicit irrelevance feedback data, it has not been deployed in the scenario where one should automatically obtain the irrelevance feedback data. In this article, we propose a substantial extension of the basic DSM from two perspectives: developing a further regularization framework and deploying DSM in the automatic irrelevance feedback scenario. Specifically, in order to avoid the output distribution of DSM drifting away from the true relevance distribution when the quality of seed irrelevant distribution (as the input to DSM) is not guaranteed, we propose a DSM regularization framework to constrain the estimation for the relevance distribution. This regularization framework includes three algorithms, each corresponding to a regularization strategy incorporated in the objective function of DSM. In addition, we exploit DSM in automatic (i.e., pseudo) irrelevance feedback, by automatically detecting the seed irrelevant documents via three different document reranking methods. We have carried out extensive experiments based on various TREC datasets, in order to systematically evaluate the proposed methods. The experimental results demonstrate the effectiveness of our proposed approaches in comparison with various strong baselines. © 2017 ACM.",Distribution separation method; Irrelevant term distribution; Mixture distribution; Regularization; Relevance feedback,Information retrieval; Learning systems; Mixtures; Separation; Mixture distributions; Regularization; Relevance feedback; Separation methods; Term distributions; Probability distributions
Algorithmic differentiation of code with multiple context-specific activities,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011410584&doi=10.1145%2f3015464&partnerID=40&md5=0f0bde6972aa69144f4f46bc474c59cd,"Algorithmic differentiation (AD) by source-transformation is an established method for computing derivatives of computational algorithms. Static dataflow analysis is commonly used by AD tools to determine the set of active variables, that is, variables that are influenced by the program input in a differentiable way and have a differentiable influence on the program output. In this work, a context-sensitive static analysis combined with procedure cloning is used to generate specialised versions of differentiated procedures for each call site. This enables better detection and elimination of unused computations and memory storage, resulting in performance improvements of the generated code, in both forward-and reverse-mode AD. The implications of this multi-activity AD approach on the static analysis of an AD tool is shown using dataflow equations. The worst-case cost of multi-activity AD on the differentiation process is analysed and practical remedies to avoid running into this worst case are presented. The method was implemented in the AD tool Tapenade, and we present its application to a 3D unstructured compressible flow solver, for which we generate an adjoint solver that performs significantly faster when multi-activity AD is used. © 2016 ACM.",Activity analysis; Adjoint; Algorithmic differentiation; Automatic differentiation; Reverse mode; Source transformation; Static analysis; Tangent-linear,Data flow analysis; Linear transformations; Mathematical transformations; Activity analysis; Adjoints; Algorithmic differentiations; Automatic differentiations; Reverse mode; Source transformation; Tangent-linear; Static analysis
Implicit visual learning: Image recognition via dissipative learning model,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011394732&doi=10.1145%2f2974024&partnerID=40&md5=2aeb1e3948b3a9a3df548e20208c866e,"According to consciousness involvement, human's learning can be roughly classified into explicit learning and implicit learning. Contrasting strongly to explicit learning with clear targets and rules, such as our school study of mathematics, learning is implicit when we acquire new information without intending to do so. Research from psychology indicates that implicit learning is ubiquitous in our daily life. Moreover, implicit learning plays an important role in human visual perception. But in the past 60 years, most of the well-known machine-learning models aimed to simulate explicit learning while the work of modeling implicit learning was relatively limited, especially for computer vision applications. This article proposes a novel unsupervised computational model for implicit visual learning by exploring dissipative system, which provides a unifying macroscopic theory to connect biology with physics. We test the proposed Dissipative Implicit Learning Model (DILM) on various datasets. The experiments show that DILM not only provides a good match to human behavior but also improves the explicit machine-learning performance obviously on image classification tasks. © 2016 ACM.",Dissipative implicit learning model; Dissipative theory; Image recognition; Implicit learning; Visual data analysis,Artificial intelligence; Computation theory; Image recognition; Learning systems; Computational model; Computer vision applications; Dissipative systems; Dissipative theory; Human visual perception; Implicit learning; Machine learning models; Visual data analysis; Behavioral research
SMARTS: Scalable microscopic adaptive road traffic simulator,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006993085&doi=10.1145%2f2898363&partnerID=40&md5=7bad4344364df17ca83256175e7e5b22,"Microscopic traffic simulators are important tools for studying transportation systems as they describe the evolution of traffic to the highest level of detail. A major challenge to microscopic simulators is the slow simulation speed due to the complexity of traffic models. We have developed the Scalable Microscopic Adaptive Road Traffic Simulator (SMARTS), a distributed microscopic traffic simulator that can utilize multiple independent processes in parallel. SMARTS can perform fast large-scale simulations. For example, when simulating 1 million vehicles in an area the size of Melbourne, the system runs 1.14 times faster than real time with 30 computing nodes and 0.2s simulation timestep. SMARTS supports various driver models and traffic rules, such as the car-following model and lane-changing model, which can be driver dependent. It can simulate multiple vehicle types, including bus and tram. The simulator is equipped with a wide range of features that help to customize, calibrate, and monitor simulations. Simulations are accurate and confirm with real traffic behaviours. For example, it achieves 79.1% accuracy in predicting traffic on a 10km freeway 90 minutes into the future. The simulator can be used for predictive traffic advisories as well as traffic management decisions as simulations complete well ahead of real time. SMARTS can be easily deployed to different operating systems as it is developed with the standard Java libraries. © 2016 ACM.",,Roads and streets; Traffic control; Vehicle actuated signals; Car following models; Lane changing models; Large scale simulations; Simulation speed; Traffic advisories; Traffic management; Traffic simulators; Transportation system; Simulators
Efficient methods for influence-based network-oblivious community detection,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006978017&doi=10.1145%2f2979682&partnerID=40&md5=73c8992a25a454823357d607c641f71a,"We study the problem of detecting social communities when the social graph is not available but instead we have access to a log of user activity, that is, a dataset of tuples (u, i, t) recording the fact that user u ""adopted"" item i at time t. We propose a stochastic framework that assumes that the adoption of items is governed by an underlying diffusion process over the unobserved social network and that such a diffusion model is based on community-level influence. That is, we aim at modeling communities through the lenses of social contagion. By fitting the model parameters to the user activity log, we learn the community membership and the level of influence of each user in each community. The general framework is instantiated with two different diffusion models, one with discrete time and one with continuous time, and we show that the computational complexity of both approaches is linear in the number of users and in the size of the propagation log. Experiments on synthetic data with planted community structure show that our methods outperform non-trivial baselines. The effectiveness of the proposed techniques is further validated on real-word data, on which our methods are able to detect high-quality communities. © 2016 ACM.",,Continuous time systems; Stochastic models; Stochastic systems; Community detection; Community structures; Diffusion process; Model communities; Model parameters; Social communities; Stochastic framework; Through the lens; Diffusion
Prediction and simulation of human mobility following natural disasters,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995571099&doi=10.1145%2f2970819&partnerID=40&md5=b79112cade75e931c9d087241da20730,"In recent decades, the frequency and intensity of natural disasters has increased significantly, and this trend is expected to continue. Therefore, understanding and predicting human behavior and mobility during a disaster will play a vital role in planning effective humanitarian relief, disaster management, and longterm societal reconstruction. However, such research is very difficult to perform owing to the uniqueness of various disasters and the unavailability of reliable and large-scale human mobility data. In this study, we collect big and heterogeneous data (e.g., GPS records of 1.6 million users1 over 3 years, data on earthquakes that have occurred in Japan over 4 years, news report data, and transportation network data) to study human mobility following natural disasters. An empirical analysis is conducted to explore the basic laws governing human mobility following disasters, and an effective human mobilitymodel is developed to predict and simulate population movements. The experimental results demonstrate the efficiency of our model, and they suggest that human mobility following disasters can be significantly more predictable and be more easily simulated than previously thought. © 2016 ACM.",Disaster informatics; Human mobility; Spatiotemporal data mining; Urban computing,Behavioral research; Data mining; Disaster prevention; Forecasting; Disaster management; Human mobility; Humanitarian relief; Informatics; Prediction and simulations; Spatio-temporal data mining; Transportation network; Urban computing; Disasters
A supervised learning model for high-dimensional and large-scale data,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995639330&doi=10.1145%2f2972957&partnerID=40&md5=eaa56f94a2cadf082239bea23c1d0d7c,"We introduce a new supervised learning model using a discriminative regression approach. This new model estimates a regression vector to represent the similarity between a test example and training examples while seamlessly integrating the class information in the similarity estimation. This distinguishes our model from usual regression models and locally linear embedding approaches, rendering our method suitable for supervised learning problems in high-dimensional settings. Our model is easily extensible to account for nonlinear relationship and applicable to general data, including both high- and low-dimensional data. The objective function of the model is convex, for which two optimization algorithms are provided. These two optimization approaches induce two scalable solvers that are of mathematically provable, linear time complexity. Experimental results verify the effectiveness of the proposed method on various kinds of data. For example, our method shows comparable performance on low-dimensional data and superior performance on highdimensional data to several widely used classifiers; also, the linear solvers obtain promising performance on large-scale classification. © 2016 ACM.",Classification; Discriminative regression; High dimension; Large-scale data; Supervised learning,Optimization; Regression analysis; Supervised learning; Discriminative regression; High dimensions; Large scale classifications; Large scale data; Locally linear embedding; Non-linear relationships; Optimization algorithms; Supervised learning problems; Classification (of information)
Community detection with topological structure and attributes in information networks,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995603724&doi=10.1145%2f2979681&partnerID=40&md5=487444dc42f5b8da645c70ef5d96be83,"Information networks contain objects connected by multiple links and described by rich attributes. Detecting community for these networks is a challenging research problem, because there is a scarcity of effective approaches that balance the features of the network structure and the characteristics of the nodes. Some methods detect communities by considering topological structures while ignoring the contributions of attributes. Other methods have considered both topological structure and attributes but pay a high price in time complexity. We establish a new community detection algorithm which explores both topological Structure and Attributes using Global structure and Local neighborhood features (SAGL) which also has low computational complexity. The first step of SAGL evaluates the global importance of every node and calculates the similarity of each node pair by combining edge strength and node attribute similarity. The second step of SAGL uses a clustering algorithm that identifies communities by measuring the similarity of two nodes, calculated by the distance of their neighbors. Experimental results on three real-world datasets show the effectiveness of SAGL, particularly its fast convergence compared to current state-of-the-art attributed graph clustering methods. © 2016 ACM.",Attribute similarity; Community detection; Global importance; Topological structure,Complex networks; Feature extraction; Information services; Population dynamics; Topology; Attribute similarity; Attributed graph clustering; Community detection; Community detection algorithms; Global importance; Information networks; Low computational complexity; Topological structure; Clustering algorithms
Introduction to intelligent music systems and applications,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993970476&doi=10.1145%2f2991468&partnerID=40&md5=0e0bd532d3f05e718c38dee1b58e5a03,"Intelligent technologies have become an essential part of music systems and applications. This is evidenced by today's omnipresence of digital online music stores and streaming services, which rely on music recommenders, automatic playlist generators, and music browsing interfaces. A large amount of research leading to intelligent music applications deals with the extraction of musical and acoustic information directly from the audio signal using signal processing techniques. Other strategies exploit contextual aspects of music, not present in the signal, for example, community meta-data and trails of user interaction, as found, for instance, on social media platforms. In this editorial, we discuss the notion of ""intelligent music system"" and give an overview of the papers selected to this special issue. © 2016 ACM.",Intelligentmusic systems; Machine learning; Music information retrieval,Artificial intelligence; Learning systems; Search engines; Signal processing; Acoustic information; Intelligent technology; Music applications; Music information retrieval; Signal processing technique; Social media platforms; Streaming service; User interaction; Audio acoustics
Differential flattening: A novel framework for community detection in multi-layer graphs,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994512808&doi=10.1145%2f2898362&partnerID=40&md5=07d48171beebbd45ddd436255facc151,"A multi-layer graph consists of multiple layers of weighted graphs, where the multiple layers represent the different aspects of relationships. Considering multiple aspects (i.e., layers) together is essential to achieve a comprehensive and consolidated view. In this article, we propose a novel framework of differential flattening, which facilitates the analysis of multi-layer graphs, and apply this framework to community detection. Differential flattening merges multiple graphs into a single graph such that the graph structure with the maximum clustering coefficient is obtained from the single graph. It has two distinct features compared with existing approaches. First, dealing with multiple layers is done independently of a specific community detection algorithm, whereas previous approaches rely on a specific algorithm. Thus, any algorithm for a single graph becomes applicable to multi-layer graphs. Second, the contribution of each layer to the single graph is determined automatically for the maximum clustering coefficient. Since differential flattening is formulated by an optimization problem, the optimal solution is easily obtained by well-known algorithms such as interior point methods. Extensive experiments were conducted using the Lancichinetti-Fortunato-Radicchi (LFR) benchmark networks as well as the DBLP, 20 Newsgroups, and MIT Reality Mining networks. The results show that our approach of differential flattening leads to discovery of higher-quality communities than baseline approaches and the state-of-the-art algorithms. © 2016 ACM.",Clustering coefficient; Community detection; Differential flattening; Multi-layer graphs; Social networks,Large scale systems; Optimization; Population dynamics; Social networking (online); Clustering coefficient; Community detection; Community detection algorithms; Differential flattening; Interior-point method; Multi-layer graphs; Optimization problems; State-of-the-art algorithms; Graphic methods
Getting closer to the essence of music: The con espressione manifesto,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993960945&doi=10.1145%2f2899004&partnerID=40&md5=63550e13ae9b32b286ada10c0a1440e0,"This text offers a personal and very subjective view on the current situation of Music Information Research (MIR). Motivated by the desire to build systems with a somewhat deeper understanding of music than the ones we currently have, I try to sketch a number of challenges for the next decade of MIR research, grouped around six simple truths about music that are probably generally agreed on but often ignored in everyday research. © 2016 ACM.",,Build systems; Current situation; Music information
Learning contextualized music semantics from tags via a Siamese neural network,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994514700&doi=10.1145%2f2953886&partnerID=40&md5=d6389e92cc95a5ff193e0ef24aa1e9bd,"Music information retrieval faces a challenge in modeling contextualized musical concepts formulated by a set of co-occurring tags. In this article, we investigate the suitability of our recently proposed approach based on a Siamese neural network in fighting off this challenge. By means of tag features and probabilistic topic models, the network captures contextualized semantics from tags via unsupervised learning. This leads to a distributed semantics space and a potential solution to the out of vocabulary problem, which has yet to be sufficiently addressed. We explore the nature of the resultant music-based semantics and address computational needs. We conduct experiments on three public music tag collections - namely, CAL500, MagTag5K and Million Song Dataset - and compare our approach to a number of state-of-the-art semantics learning approaches. Comparative results suggest that this approach outperforms previous approaches in terms of semantic priming and music tag completion.",Contextualized concept modeling; Distributed semantics learning; Million song dataset; Out of vocabulary; Semantic priming; Tag representation,Computer music; Semantic Web; Statistics; Concept model; Distributed semantics learning; Million song dataset; Out of vocabulary; Semantic primings; Tag representation; Semantics
Sound and music recommendation with knowledge graphs,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994494790&doi=10.1145%2f2926718&partnerID=40&md5=dc4d091d289e78ef879f93d079a6c7a9,"The Web has moved, slowly but steadily, from a collection of documents towards a collection of structured data. Knowledge graphs have then emerged as a way of representing the knowledge encoded in such data as well as a tool to reason on them in order to extract new and implicit information. Knowledge graphs are currently used, for example, to explain search results, to explore knowledge spaces, to semantically enrich textual documents, or to feed knowledge-intensive applications such as recommender systems. In this work, we describe how to create and exploit a knowledge graph to supply a hybrid recommendation engine with information that builds on top of a collections of documents describing musical and sound items. Tags and textual descriptions are exploited to extract and link entities to external graphs such as WordNet and DBpedia, which are in turn used to semantically enrich the initial data. By means of the knowledge graph we build, recommendations are computed using a feature combination hybrid approach. Two explicit graph feature mappings are formulated to obtain meaningful item feature representations able to catch the knowledge embedded in the graph. Those content features are further combined with additional collaborative information deriving from implicit user feedback. An extensive evaluation on historical data is performed over two different datasets: a dataset of sounds composed of tags, textual descriptions, and user's download information gathered from Freesound.org and a dataset of songs that mixes song textual descriptions with tags and user's listening habits extracted from Songfacts.com and Last.fm, respectively. Results show significant improvements with respect to state-of-the-art collaborative algorithms in both datasets. In addition, we show how the semantic expansion of the initial descriptions helps in achieving much better recommendation quality in terms of aggregated diversity and novelty. © 2016 ACM.",Diversity; Entity linking; Knowledge graphs; Music; Novelty; Recommender systems,Recommender systems; Semantics; Diversity; Entity linking; Knowledge graphs; Music; Novelty; Graphic methods
Joint structured sparsity regularized multiview dimension reduction for video-based facial expression recognition,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994507788&doi=10.1145%2f2956556&partnerID=40&md5=b0c72dcc01f66f9f6350d0c7245b5f6f,"Video-based facial expression recognition (FER) has recently received increased attention as a result of its widespread application. Using only one type of feature to describe facial expression in video sequences is often inadequate, because the information available is very complex. With the emergence of different features to represent different properties of facial expressions in videos, an appropriate combination of these features becomes an important, yet challenging, problem. Considering that the dimensionality of these features is usually high, we thus introduce multiview dimension reduction (MVDR) into video-based FER. In MVDR, it is critical to explore the relationships between and within different feature views. To achieve this goal, we propose a novel framework of MVDR by enforcing joint structured sparsity at both inter- and intraview levels. In this way, correlations on and between the feature spaces of different views tend to be well-exploited. In addition, a transformation matrix is learned for each view to discover the patterns contained in the original features, so that the different views are comparable in finding a common representation. The model can be not only performed in an unsupervised manner, but also easily extended to a semisupervised setting by incorporating some domain knowledge. An alternating algorithm is developed for problem optimization, and each subproblem can be efficiently solved. Experiments on two challenging video-based FER datasets demonstrate the effectiveness of the proposed framework. © 2016 ACM.",Dimension reduction; Multiple features; Structured sparsity; Video-based facial expression recognition,Linear transformations; Dimension reduction; Domain knowledge; Facial expression recognition; Facial Expressions; Multiple features; Semi-supervised; Structured sparsities; Transformation matrices; Face recognition
"Tensors for data mining and data fusion: Models, applications, and scalable algorithms",2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994031868&doi=10.1145%2f2915921&partnerID=40&md5=e86f8594f26f7e969e4f5c743e954f5c,"Tensors and tensor decompositions are very powerful and versatile tools that can model a wide variety of heterogeneous, multiaspect data. As a result, tensor decompositions, which extract useful latent information out of multiaspect data tensors, have witnessed increasing popularity and adoption by the data mining community. In this survey, we present some of the most widely used tensor decompositions, providing the key insights behind them, and summarizing them from a practitioner's point of view. We then provide an overview of a very broad spectrum of applications where tensors have been instrumental in achieving stateof-The-Art performance, ranging from social network analysis to brain data analysis, and from web mining to healthcare. Subsequently, we present recent algorithmic advances in scaling tensor decompositions up to today's big data, outlining the existing systems and summarizing the key ideas behind them. Finally, we conclude with a list of challenges and open problems that outline exciting future research directions. © 2016 ACM.",Multi-Aspect data; Multi-way analysis; Tensor decomposition; Tensor factorization; Tensors,Arts computing; Big data; Data fusion; Tensors; Data mining community; Future research directions; Multi aspects; Multi-way analysis; Scalable algorithms; State-of-the-art performance; Tensor decomposition; Tensor factorization; Data mining
Harnessing music-related visual stereotypes for music information retrieval,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994528891&doi=10.1145%2f2926719&partnerID=40&md5=fce19a4ab412dd967b38d8b830ebaf2c,"Over decades, music labels have shaped easily identifiable genres to improve recognition value and subsequently market sales of new music acts. Referring to print magazines and later to music television as important distribution channels, the visual representation thus played and still plays a significant role in music marketing. Visual stereotypes developed over decades that enable us to quickly identify referenced music only by sight without listening. Despite the richness of music-related visual information provided by music videos and album covers as well as T-shirts, advertisements, and magazines, research towards harnessing this information to advance existing or approach new problems of music retrieval or recommendation is scarce or missing. In this article, we present our research on visual music computing that aims to extract stereotypical music-related visual information from music videos. To provide comprehensive and reproducible results, we present the Music Video Dataset, a thoroughly assembled suite of datasets with dedicated evaluation tasks that are aligned to current Music Information Retrieval tasks. Based on this dataset, we provide evaluations of conventional low-level image processing and affect-related features to provide an overview of the expressiveness of fundamental visual properties such as color, illumination, and contrasts. Further, we introduce a high-level approach based on visual concept detection to facilitate visual stereotypes. This approach decomposes the semantic content of music video frames into concrete concepts such as vehicles, tools, and so on, defined in a wide visual vocabulary. Concepts are detected using convolutional neural networks and their frequency distributions as semantic descriptions for a music video. Evaluations showed that these descriptions show good performance in predicting the music genre of a video and even outperform audio-content descriptors on cross-genre thematic tags. Further, highly significant performance improvements were observed by augmenting audio-based approaches through the introduced visual approach. © 2016 ACM.",Music videos; Video analysis; Visual concept detection,Commerce; Computer vision; Image processing; Information retrieval; Neural networks; Semantics; Television broadcasting; Convolutional neural network; Frequency distributions; Low level image processing; Music information retrieval; Music video; Video analysis; Visual concept detections; Visual representations; Audio acoustics
Tempo driven audio-to-score alignment using spectral decomposition and online dynamic time warping,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994507714&doi=10.1145%2f2926717&partnerID=40&md5=9a4b741f88f2040995fa5d223d5dab02,"In this article, we present an online score following framework designed to deal with automatic accompaniment. The proposed framework is based on spectral factorization and online Dynamic Time Warping (DTW) and has two separated stages: preprocessing and alignment. In the first one, we convert the score into a reference audio signal using a MIDI synthesizer software and we analyze the provided information in order to obtain the spectral patterns (i.e., basis functions) associated to each score unit. In this work, a score unit represents the occurrence of concurrent or isolated notes in the score. These spectral patterns are learned from the synthetic MIDI signal using a method based on Non-negative Matrix Factorization (NMF) with Beta-divergence, where the gains are initialized as the ground-truth transcription inferred from the MIDI. On the second stage, a non-iterative signal decomposition method with fixed spectral patterns per score unit is used over the magnitude spectrogram of the input signal resulting in a distortion matrix that can be interpreted as the cost of the matching for each score unit at each frame. Finally, the relation between the performance and the musical score times is obtained using a strategy based on online DTW, where the optimal path is biased by the speed of interpretation. Our system has been evaluated and compared to other systems, yielding reliable results and performance. © 2016 ACM.",Accompaniment; Audio-to-score alignment; Beta-divergence; Dynamic time warping (DTW); Non-negative matrix factorization (NMF); Online algorithm; Score following; Speed of interpretation; Tempo,Alignment; Factorization; Iterative methods; Signal processing; Accompaniment; Beta divergence; Dynamic time warping; Nonnegative matrix factorization; On-line algorithms; Score-following; Tempo; Matrix algebra
A joyful ode to automatic orchestration,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994037275&doi=10.1145%2f2897738&partnerID=40&md5=5e639e5ca34fe598633cf95c2ff63278,"Most works in automatic music generation have addressed so far specific tasks. Such a reductionist approach has been extremely successful and some of these tasks have been solved once and for all. However, few works have addressed the issue of generating automatically fully fledged music material, of human-level quality. In this article, we report on a specific experiment in holisticmusic generation: The reorchestration of Beethoven's Ode to Joy, the European anthem, in seven styles. These reorchestrations were produced with algorithms developed in the Flow Machines project and within a short time frame. We stress the benefits of having had such a challenging and unifying goal, and the interesting problems and challenges it raised along the way. © 2016 ACM.",Generation; Machine learning; Music,Artificial intelligence; Flow machines; Generation; Human levels; Music; Music materials; Problems and challenges; Short time frames; Specific tasks; Learning systems
"Towards music structural segmentation across genres: Features, structural hypotheses, and annotation principles",2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994499925&doi=10.1145%2f2950066&partnerID=40&md5=1a8ed019556486af42d29aaac4b7b292,"This article faces the problem of how different audio features and segmentation methods work with different music genres. A new annotated corpus of Chinese traditional Jingju music is presented. We incorporate this dataset with two existing music datasets from the literature in an integrated retrieval system to evaluate existing features, structural hypotheses, and segmentation algorithms outside a Western bias. A harmonic-percussive source separation technique is introduced to the feature extraction process and brings significant improvement to the segmentation. Results show that different features capture the structural patterns of different music genres in different ways. Novelty-or homogeneity-based segmentation algorithms and timbre features can surpass the investigated alternatives for the structure analysis of Jingju due to their lack of harmonic repetition patterns. Findings indicate that the design of audio features and segmentation algorithms as well as the consideration of contextual information related to the music corpora should be accounted dependently in an effective segmentation system.",Data collection; Evaluation; Harmonic-percussive source separation; Music information retrieval; Music structural segmentation; Non-western music,Audio acoustics; Blind source separation; Feature extraction; Harmonic analysis; Contextual information; Data collection; Evaluation; Music information retrieval; Non-western music; Segmentation algorithms; Segmentation methods; Separation techniques; Source separation
Soft Confidence-Weighted learning,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989947005&doi=10.1145%2f2932193&partnerID=40&md5=be4f707da4332eecd248bc7772317f8c,"Online learning plays an important role in many big data mining problems because of its high efficiency and scalability. In the literature, many online learning algorithms using gradient information have been applied to solve online classification problems. Recently, more effective second-order algorithms have been proposed, where the correlation between the features is utilized to improve the learning efficiency. Among them, Confidence-Weighted (CW) learning algorithms are very effective, which assume that the classification model is drawn from a Gaussian distribution, which enables the model to be effectively updated with the second-order information of the data stream. Despite being studied actively, these CW algorithms cannot handle nonseparable datasets and noisy datasets very well. In this article, we propose a family of Soft Confidence-Weighted (SCW) learning algorithms for both binary classification and multiclass classification tasks, which is the first family of online classification algorithms that enjoys four salient properties simultaneously: (1) large margin training, (2) confidence weighting, (3) capability to handle nonseparable data, and (4) adaptive margin. Our experimental results show that the proposed SCW algorithms significantly outperform the original CW algorithm. When comparing with a variety of state-of-the-art algorithms (including AROW, NAROW, and NHERD), we found that SCW in general achieves better or at least comparable predictive performance, but enjoys considerably better efficiency advantage (i.e., using a smaller number of updates and lower time cost). To facilitate future research, we release all the datasets and source code to the public at http://libol.stevenhoi.org/. © 2016 ACM.",Binary classification; Confidence weighted; Multiclass classification; Second-order algorithms,Big data; Bins; Classification (of information); Data mining; E-learning; Efficiency; Binary classification; Confidence weighted; Confidence-weighted learning; Multi-class classification; Online learning algorithms; Predictive performance; Second-order algorithms; State-of-the-art algorithms; Learning algorithms
Sprank: Semantic path-based ranking for top-nrecommendations using Linked Open Data,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989894143&doi=10.1145%2f2899005&partnerID=40&md5=df8112307020bb94d6a9d01dc804d385,"In most real-world scenarios, the ultimate goal of recommender system applications is to suggest a short ranked list of items, namely top-N recommendations, that will appeal to the end user. Often, the problem of computing top-N recommendations is mainly tackled with a two-step approach. The system focuses first on predicting the unknown ratings, which are eventually used to generate a ranked recommendation list. Actually, the top-N recommendation task can be directly seen as a ranking problem where the main goal is not to accurately predict ratings but to directly find the best-ranked list of items to recommend. In this article we present SPrank, a novel hybrid recommendation algorithm able to compute top-N recommendations exploiting freely available knowledge in the Web of Data. In particular, we employ DBpedia, a well-known encyclopedic knowledge base in the Linked Open Data cloud, to extract semantic path-based features and to eventually compute top-N recommendations in a learning-to-rank fashion. Experiments with three datasets related to different domains (books, music, and movies) prove the effectiveness of our approach compared to state-of-the-art recommendation algorithms.",DBpedia; Hybrid recommender systems; Learning to rank,Human computer interaction; Knowledge based systems; Recommender systems; Semantics; Dbpedia; Encyclopedic knowledge; Hybrid recommendation; Hybrid recommender systems; Learning to rank; Real-world scenario; Recommendation algorithms; System applications; Linked data
A unified point-of-interest recommendation framework in Location-based social networks,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989904588&doi=10.1145%2f2901299&partnerID=40&md5=b463da56ca79a657b6cc9b57f393bd4f,"Location-based social networks (LBSNs), such as Gowalla, Facebook, Foursquare, Brightkite, and so on, have attracted millions of users to share their social friendship and their locations via check-ins in the past few years. Plenty of valuable information is accumulated based on the check-in behaviors, which makes it possible to learn users' moving patterns as well as their preferences. In LBSNs, point-of-interest (POI) recommendation is one of the most significant tasks because it can help targeted users explore their surroundings as well as help third-party developers provide personalized services. Matrix factorization is a promising method for this task because it can capture users' preferences to locations and is widely adopted in traditional recommender systems such as movie recommendation. However, the sparsity of the check-in data makes it difficult to capture users' preferences accurately. Geographical influence can help alleviate this problem and have a large impact on the final recommendation result. By studying users' moving patterns, we find that users tend to check in around several centers and different users have different numbers of centers. Based on this, we propose a Multi-center Gaussian Model (MGM) to capture this pattern via modeling the probability of a user's check-in on a location. Moreover, users are usually more interested in the top 20 or even top 10 recommended POIs, which makes personalized ranking important in this task. From previous work, directly optimizing for pairwise ranking like Bayesian Personalized Ranking (BPR) achieves better performance in the top-k recommendation than directly using matrix matrix factorization that aims to minimize the point-wise rating error. To consider users' preferences, geographical influence and personalized ranking, we propose a unified POI recommendation framework, which unifies all of them together. Specifically, we first fuse MGM with matrix factorization methods and further with BPR using two different approaches. We conduct experiments on Gowalla and Foursquare datasets, which are two large-scale real-world LBSN datasets publicly available online. The results on both datasets show that our unified POI recommendation framework can produce better performance. © 2016 ACM.",Data mining; Location recommendation; Location-based social networks; Recommender systems,Data mining; Factorization; Location; Matrix algebra; Social networking (online); Gaussian model; Location-based social networks; Matrix factorizations; Movie recommendations; Personalized service; Point of interest; Third parties; Top-K recommendations; Recommender systems
Measuring similarity similarly: LDA and human perception,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989846977&doi=10.1145%2f2890510&partnerID=40&md5=afd5d18dda0117cb1022d0de4378d634,"Several intelligent technologies designed to improve navigability in and digestibility of text corpora use topic modeling such as the state-of-the-art Latent Dirichlet Allocation (LDA). This model and variants on it provide lower-dimensional document representations used in visualizations and in computing similarity between documents. This article contributes a method for validating such algorithms against human perceptions of similarity, especially applicable to contexts in which the algorithm is intended to support navigability between similar documents via dynamically generated hyperlinks. Such validation enables researchers to ground their methods in context of intended use instead of relying on assumptions of fit. In addition to the methodology, this article presents the results of an evaluation using a corpus of short documents and the LDA algorithm. We also present some analysis of potential causes of differences between cases in which this model matches human perceptions of similarity more or less well.",Algorithm validation; Perceived similarity; Similarity metrics,Hypertext systems; Algorithm validation; Document Representation; Human perception; Intelligent technology; Latent dirichlet allocations; Measuring similarities; Perceived similarity; Similarity metrics; Statistics
Driving profiles computation and monitoring for car insurance CRM,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984783015&doi=10.1145%2f2912148&partnerID=40&md5=4a1e017dd41eb1288f9b2454607fdf48,"Customer segmentation is one of the most traditional and valued tasks in customer relationship management (CRM). In this article, we explore the problem in the context of the car insurance industry, where the mobility behavior of customers plays a key role: Different mobility needs, driving habits, and skills imply also different requirements (level of coverage provided by the insurance) and risks (of accidents). In the present work, we describe a methodology to extract several indicators describing the driving profile of customers, and we provide a clustering-oriented instantiation of the segmentation problem based on such indicators. Then, we consider the availability of a continuous flow of fresh mobility data sent by the circulating vehicles, aiming at keeping our segments constantly up to date. We tackle a major scalability issue that emerges in this context when the number of customers is large-namely, the communication bottleneck-by proposing and implementing a sophisticated distributed monitoring solution that reduces communications between vehicles and company servers to the essential. We validate the framework on a large database of real mobility data coming from GPS devices on private cars. Finally, we analyze the privacy risks that the proposed approach might involve for the users, providing and evaluating a countermeasure based on data perturbation. © 2016 ACM.",Distributed clustering; Driving profiles; Privacy,Accidents; Data privacy; Public relations; Sales; Customer relationship management; Customer segmentation; Data perturbation; Distributed clustering; Distributed monitoring; Driving profiles; Mobility behavior; Scalability issue; Vehicle to vehicle communications
Dystemo: Distant supervision method for multi-category emotion recognition in tweets,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984685553&doi=10.1145%2f2912147&partnerID=40&md5=3f68486033e5224db9405b4213f79c20,"Emotion recognition in text has become an important research objective. It involves building classifiers capable of detecting human emotions for a specific application, for example, analyzing reactions to product launches, monitoring emotions at sports events, or discerning opinions in political debates. Most successful approaches rely heavily on costly manual annotation. To alleviate this burden, we propose a distant supervision method - Dystemo - for automatically producing emotion classifiers from tweets labeled using existing or easy-to-produce emotion lexicons. The goal is to obtain emotion classifiers that work more accurately for specific applications than available emotion lexicons. The success of this method depends mainly on a novel classifier - Balanced Weighted Voting (BWV) - designed to overcome the imbalance in emotion distribution in the initial dataset, and on novel heuristics for detecting neutral tweets. We demonstrate how Dystemo works using Twitter data about sports events, a fine-grained 20-category emotion model, and three different initial emotion lexicons. Through a series of carefully designed experiments, we confirm that Dystemo is effective both in extending initial emotion lexicons of small coverage to find correctly more emotional tweets and in correcting emotion lexicons of low accuracy to perform more accurately.",Distant supervision; Emotion recognition; Natural language processing; Semi-supervised learning; Text mining; Twitter,Classification (of information); Data mining; Heuristic methods; Learning algorithms; Natural language processing systems; Social networking (online); Speech recognition; Sports; Supervised learning; Distant supervision; Emotion recognition; NAtural language processing; Semi- supervised learning; Text mining; Twitter; Character recognition
Recognizing Parkinsonian gait pattern by exploiting fine-grained movement function features,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984973108&doi=10.1145%2f2890511&partnerID=40&md5=39d8d71ddb66ee15a33ea7ba94c8559e,"Parkinson's disease (PD) is one of the typical movement disorder diseases among elderly people, which has a serious impact on their daily lives. In this article, we propose a novel computation framework to recognize gait patterns in patients with PD. The key idea of our approach is to distinguish gait patterns in PD patients from healthy individuals by accurately extracting gait features that capture all three aspects of movement functions, that is, stability, symmetry, and harmony. The proposed framework contains three steps: gait phase discrimination, feature extraction and selection, and pattern classification. In the first step, we put forward a sliding window-based method to discriminate four gait phases from plantar pressure data. Based on the gait phases, we extract and select gait features that characterize stability, symmetry, and harmony of movement functions. Finally, we recognize PD gait patterns by applying a hybrid classification model. We evaluate the framework using an open dataset that contains real plantar pressure data of 93 PD patients and 72 healthy individuals. Experimental results demonstrate that our framework significantly outperforms the four baseline approaches. © 2016 ACM.",Gait harmony; Gait pattern recognition; Gait phases; Gait stability; Gait symmetry; Parkinson's disease,Feature extraction; Neurodegenerative diseases; Gait harmony; Gait phasis; Gait stabilities; Gait symmetries; Parkinson's disease; Gait analysis
Topic-aware physical activity propagation with temporal dynamics in a health social network,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984985776&doi=10.1145%2f2873066&partnerID=40&md5=b7c384bafcd8bafa43e0256867c89e04,"Modeling physical activity propagation, such as activity level and intensity, is a key to preventing obesity from cascading through communities, and to helping spread wellness and healthy behavior in a social network. However, there have not been enough scientific and quantitative studies to elucidate how social communication may deliver physical activity interventions. In this work, we introduce a novel model named Topic-aware Community-level Physical Activity Propagation with Temporal Dynamics (TCPT) to analyze physical activity propagation and social influence at different granularities (i.e., individual level and community level). Given a social network, the TCPT model first integrates the correlations between the content of social communication, social influences, and temporal dynamics. Then, a hierarchical approach is utilized to detect a set of communities and their reciprocal influence strength of physical activities. The experimental evaluation shows not only the effectiveness of our approach but also the correlation of the detected communities with various health outcome measures. Our promising results pave a way for knowledge discovery in health social networks. © 2016 ACM.",Health social network; Physical activity propagation,Dynamics; Economic and social effects; Different granularities; Experimental evaluation; Hierarchical approach; Individual levels; Physical activity; Quantitative study; Social communications; Temporal dynamics; Health
CSM: A cloud service marketplace for complex service acquisition,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022145719&doi=10.1145%2f2894759&partnerID=40&md5=531ec345f9c3c759dd91734fc85ca3e5,"The cloud service marketplace (CSM) is an exploratory project aiming to provide “an AppStore for Services.” It is an intelligent online marketplace that facilitates service discovery and acquisition for enterprise customers. Traditional service discovery and acquisition are time-consuming. In the era of OneClick Checkout and pay-as-you-go service plans, users expect services to be purchased online efficiently and conveniently. However, as services are complex and different from software apps, the currently prevailing App Store based on keyword search is inadequate for services. In CSM, exploring and configuring services are an iterative process. Customers provide their requirements in natural language and interact with the system through questioning and answering. Learning from the input, the system can incrementally clarify users’ intention, narrow down the candidate services, and profile the configuration information for the candidates at the same time. CSM’s back end is built around the Services Knowledge Graph (SKG) and leverages data mining technologies to enable the semantic understanding of customers’ requirements. To quantitatively assess the value of CSM, empirical evaluation on real and synthetic datasets and case studies are given to demonstrate the efficacy and effectiveness of the proposed system. © 2016 ACM",Cloud service; Interactive search; Semantic web,Application programs; Data mining; Distributed database systems; Electronic commerce; Search engines; Semantic Web; Cloud services; Data mining technology; Empirical evaluations; Enterprise customers; Interactive search; On-line marketplaces; Semantic understanding; Traditional services; Sales
Enhanced knowledge-leverage-based TSK fuzzy system modeling for inductive transfer learning,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030994198&doi=10.1145%2f2903725&partnerID=40&md5=d672b9cd0a04a73ca4163aeebfa7be55,"The knowledge-leverage-based Takagi–Sugeno–Kang fuzzy system (KL-TSK-FS) modeling method has shown promising performance for fuzzy modeling tasks where transfer learning is required. However, the knowledge-leverage mechanism of the KL-TSK-FS can be further improved. This is because available training data in the target domain are not utilized for the learning of antecedents and the knowledge transfer mechanism from a source domain to the target domain is still too simple for the learning of consequents when a Takagi–Sugeno–Kang fuzzy system (TSK-FS) model is trained in the target domain. The proposed method, that is, the enhanced KL-TSK-FS (EKL-TSK-FS), has two knowledge-leverage strategies for enhancing the parameter learning of the TSK-FS model for the target domain using available information from the source domain. One strategy is used for the learning of antecedent parameters, while the other is for consequent parameters. It is demonstrated that the proposed EKL-TSK-FS has higher transfer learning abilities than the KL-TSK-FS. In addition, the EKL-TSK-FS has been further extended for the scene of the multisource domain. © 2016 ACM.",Enhanced KL-TSK-FS; Fuzzy modeling; Fuzzy systems; Knowledge leverage; Missing data; Transfer learning,Fuzzy systems; Knowledge management; Fuzzy modeling; Inductive transfer; Knowledge leverage; Knowledge transfer mechanisms; Missing data; Parameter learning; Transfer learning; TSK fuzzy system; Data communication systems
A spatial-temporal topic model for the semantic annotation of PoIs in LBSns,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053315464&doi=10.1145%2f2905373&partnerID=40&md5=154294221029be825c0eeb3e27ba5c89,"Semantic tags of points of interest (POIs) are a crucial prerequisite for location search, recommendation services, and data cleaning. However, most POIs in location-based social networks (LBSNs) are either tag-missing or tag-incomplete. This article aims to develop semantic annotation techniques to automatically infer tags for POIs. We first analyze two LBSN datasets and observe that there are two types of tags, category-related ones and sentimental ones, which have unique characteristics. Category-related tags are hierarchical, whereas sentimental ones are category-aware. All existing related work has adopted classification methods to predict high-level category-related tags in the hierarchy, but they cannot apply to infer either low-level category tags or sentimental ones. In light of this, we propose a latent-class probabilistic generative model, namely the spatial-temporal topic model (STM), to infer personal interests, the temporal and spatial patterns of topics/semantics embedded in users’ check-in activities, the interdependence between category-topic and sentiment-topic, and the correlation between sentimental tags and rating scores from users’ check-in and rating behaviors. Then, this learned knowledge is utilized to automatically annotate all POIs with both category-related and sentimental tags in a unified way. We conduct extensive experiments to evaluate the performance of the proposed STM on a real large-scale dataset. The experimental results show the superiority of our proposed STM, and we also observe that the real challenge of inferring category-related tags for POIs lies in the low-level ones of the hierarchy and that the challenge of predicting sentimental tags are those with neutral ratings. © 2016 ACM.",And Phrases: POI tagging; Location-based social network; Semantic annotation; Topic model,Location; Semantics; And Phrases: POI tagging; Classification methods; Large-scale dataset; Location-based social networks; Points of interest; Semantic annotations; Temporal and spatial pattern; Topic Modeling; Social networking (online)
Dynamic scheduling of cybersecurity analysts for minimizing risk using reinforcement learning,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042686533&doi=10.1145%2f2882969&partnerID=40&md5=7e4dfdef8006ba196b14e322a15d0cf6,"An important component of the cyber-defense mechanism is the adequate staffing levels of its cybersecurity analyst workforce and their optimal assignment to sensors for investigating the dynamic alert traffic. The ever-increasing cybersecurity threats faced by today’s digital systems require a strong cyber-defense mechanism that is both reactive in its response to mitigate the known risk and proactive in being prepared for handling the unknown risks. In order to be proactive for handling the unknown risks, the above workforce must be scheduled dynamically so the system is adaptive to meet the day-to-day stochastic demands on its workforce (both size and expertise mix). The stochastic demands on the workforce stem from the varying alert generation and their significance rate, which causes an uncertainty for the cybersecurity analyst scheduler that is attempting to schedule analysts for work and allocate sensors to analysts. Sensor data are analyzed by automatic processing systems, and alerts are generated. A portion of these alerts is categorized to be significant, which requires thorough examination by a cybersecurity analyst. Risk, in this article, is defined as the percentage of significant alerts that are not thoroughly analyzed by analysts. In order to minimize risk, it is imperative that the cyber-defense system accurately estimates the future significant alert generation rate and dynamically schedules its workforce to meet the stochastic workload demand to analyze them. The article presents a reinforcement learning-based stochastic dynamic programming optimization model that incorporates the above estimates of future alert rates and responds by dynamically scheduling cybersecurity analysts to minimize risk (i.e., maximize significant alert coverage by analysts) and maintain the risk under a pre-determined upper bound. The article tests the dynamic optimization model and compares the results to an integer programming model that optimizes the static staffing needs based on a daily-average alert generation rate with no estimation of future alert rates (static workforce model). Results indicate that over a finite planning horizon, the learning-based optimization model, through a dynamic (on-call) workforce in addition to the static workforce, (a) is capable of balancing risk between days and reducing overall risk better than the static model, (b) is scalable and capable of identifying the quantity and the right mix of analyst expertise in an organization, and (c) is able to determine their dynamic (on-call) schedule and their sensor-to-analyst allocation in order to maintain risk below a given upper bound. Several meta-principles are presented, which are derived from the optimization model, and they further serve as guiding principles for hiring and scheduling cybersecurity analysts. Days-off scheduling was performed to determine analyst weekly work schedules that met the cybersecurity system’s workforce constraints and requirements. © 2016 ACM.",,Adaptive systems; Dynamic programming; Dynamics; Integer programming; Learning systems; Network security; Personnel; Personnel selection; Reinforcement learning; Scheduling; Stochastic models; Stochastic systems; Automatic processing system; Dynamic optimization models; Finite planning horizon; Guiding principles; Integer programming models; Optimal assignment; Optimization modeling; Stochastic dynamic programming; Occupational risks
SNAP: A general-purpose network analysis and graph-mining library,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979498200&doi=10.1145%2f2898361&partnerID=40&md5=32ef05c4bf3dcea43ac75a49589d7491,"Large networks are becoming a widely used abstraction for studying complex systems in a broad set of disciplines, ranging from social-network analysis to molecular biology and neuroscience. Despite an increasing need to analyze and manipulate large networks, only a limited number of tools are available for this task. Here, we describe the Stanford Network Analysis Platform (SNAP), a general-purpose, high-performance system that provides easy-to-use, high-level operations for analysis and manipulation of large networks. We present SNAP functionality, describe its implementational details, and give performance benchmarks. SNAP has been developed for single big-memory machines, and it balances the trade-off between maximum performance, compact in-memory graph representation, and the ability to handle dynamic graphs in which nodes and edges are being added or removed over time. SNAP can process massive networks with hundreds of millions of nodes and billions of edges. SNAP offers over 140 different graph algorithms that can efficiently manipulate large graphs, calculate structural properties, generate regular and random graphs, and handle attributes and metadata on nodes and edges. Besides being able to handle large graphs, an additional strength of SNAP is that networks and their attributes are fully dynamic; they can be modified during the computation at low cost. SNAP is provided as an open-source library in C++ as well as a module in Python. We also describe the Stanford Large Network Dataset, a set of social and information real-world networks and datasets, which we make publicly available. The collection is a complementary resource to our SNAP software and is widely used for development and benchmarking of graph analytics algorithms. © 2016 ACM.",Data mining; Graph analytics; Graphs; Networks; Open-source software,Benchmarking; C++ (programming language); Complex networks; Computer programming; Data mining; Economic and social effects; Graphic methods; Large scale systems; Molecular biology; Networks (circuits); Open source software; Open systems; Software engineering; Graph algorithms; Graph analytics; Graph representation; Graphs; High performance systems; Massive networks; Open-source libraries; Real-world networks; Graph theory
Introduction to the special issue on crowd in intelligent systems,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979626559&doi=10.1145%2f2920522&partnerID=40&md5=9bf358e235ab294d124ea2f66d3cf0bb,[No abstract available],,
Multimodular text normalization of Dutch user-generated content,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979071622&doi=10.1145%2f2850422&partnerID=40&md5=071d8c0eb28161826d5cb2e7c1395f60,"As social media constitutes a valuable source for data analysis for a wide range of applications, the need for handling such data arises. However, the nonstandard language used on social media poses problems for natural language processing (NLP) tools, as these are typically trained on standard language material. We propose a text normalization approach to tackle this problem. More specifically, we investigate the usefulness of a multimodular approach to account for the diversity of normalization issues encountered in user-generated content (UGC). We consider three different types of UGC written in Dutch (SNS, SMS, and tweets) and provide a detailed analysis of the performance of the different modules and the overall system. We also apply an extrinsic evaluation by evaluating the performance of a part-of-speech tagger, lemmatizer, and named-entity recognizer before and after normalization.",Social media; Text normalization; User-generated content,Data handling; Natural language processing systems; Social networking (online); Multi-modular; Named entities; NAtural language processing; Part-of-speech tagger; Social media; Text normalizations; User generated content (UGC); User-generated content; Computational linguistics
Using the crowd to improve search result ranking and the search experience,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979632768&doi=10.1145%2f2897368&partnerID=40&md5=f61f66ca8db76ba737e06436dd5266de,"Despite technological advances, algorithmic search systems still have difficulty with complex or subtle information needs. For example, scenarios requiring deep semantic interpretation are a challenge for computers. People, on the other hand, are well suited to solving such problems. As a result, there is an opportunity for humans and computers to collaborate during the course of a search in a way that takes advantage of the unique abilities of each. While search tools that rely on human intervention will never be able to respond as quickly as current search engines do, recent research suggests that there are scenarios where a search engine could take more time if it resulted in a much better experience. This article explores how crowdsourcing can be used at query time to augment key stages of the search pipeline. We first explore the use of crowdsourcing to improve search result ranking. When the crowd is used to replace or augment traditional retrieval components such as query expansion and relevance scoring, we find that we can increase robustness against failure for query expansion and improve overall precision for results filtering. However, the gains that we observe are limited and unlikely to make up for the extra cost and time that the crowd requires. We then explore ways to incorporate the crowd into the search process that more drastically alter the overall experience. We find that using crowd workers to support rich query understanding and result processing appears to be a more worthwhile way to make use of the crowd during search. Our results confirm that crowdsourcing can positively impact the search experience but suggest that significant changes to the search process may be required for crowdsourcing to fulfill its potential in search systems. © 2016 ACM.",Crowdsourcing; Information retrieval; Slow search,Algorithms; Crowdsourcing; Information retrieval; Safety devices; Semantics; Human intervention; Query expansion; Recent researches; Search process; Search result rankings; Semantic interpretation; Slow search; Technological advances; Search engines
Using scalable data mining for predicting flight delays,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979599692&doi=10.1145%2f2888402&partnerID=40&md5=c85eb9d4e0431f607e8fef0081ff308e,"Flight delays are frequent all over the world (about 20% of airline flights arrive more than 15min late) and they are estimated to have an annual cost of billions of dollars. This scenario makes the prediction of flight delays a primary issue for airlines and travelers. The main goal of this work is to implement a predictor of the arrival delay of a scheduled flight due to weather conditions. The predicted arrival delay takes into consideration both flight information (origin airport, destination airport, scheduled departure and arrival time) and weather conditions at origin airport and destination airport according to the flight timetable. Airline flight and weather observation datasets have been analyzed and mined using parallel algorithms implemented as MapReduce programs executed on a Cloud platform. The results show a high accuracy in predicting delays above a given threshold. For instance, with a delay threshold of 15min, we achieve an accuracy of 74.2% and 71.8% recall on delayed flights, while with a threshold of 60min, the accuracy is 85.8% and the delay recall is 86.9%. Furthermore, the experimental results demonstrate the predictor scalability that can be achieved performing data preparation and mining tasks as MapReduce applications on the Cloud. © 2016 ACM.",Big data; Cloud computing; Flight delay; Open data; Scalability,Air traffic control; Air transportation; Airports; Cloud computing; Data mining; Forecasting; Meteorology; Scalability; Scheduling; Cloud platforms; Data preparation; Flight delays; Flight information; High-accuracy; Open datum; Scheduled flight; Weather observations; Big data
Using crowdsourcing for scientific analysis of industrial tomographic images,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979555075&doi=10.1145%2f2897370&partnerID=40&md5=67b6a080cf254ad440c85b74da5aecc3,"In this article, we present a novel application domain for human computation, specifically for crowdsourcing, which can help in understanding particle-tracking problems. Through an interdisciplinary inquiry, we built a crowdsourcing system designed to detect tracer particles in industrial tomographic images, and applied it to the problem of bulk solid flow in silos. As images from silo-sensing systems cannot be adequately analyzed using the currently available computational methods, human intelligence is required. However, limited availability of experts, as well as their high cost, motivates employing additional nonexperts. We report on the results of a study that assesses the task completion time and accuracy of employing nonexpert workers to process large datasets of images in order to generate data for bulk flow research. We prove the feasibility of this approach by comparing results from a user study with data generated from a computational algorithm. The study shows that the crowd is more scalable and more economical than an automatic solution. The system can help analyze and understand the physics of flow phenomena to better inform the future design of silos, and is generalized enough to be applicable to other domains. © 2016 ACM 2157-6904/2016/07-ART52 $15.00.",Crowdsourcing; Particle tracking; Silo; Tomography,Chemical industry; Crowdsourcing; Silos (agricultural); Tomography; Computational algorithm; Human computation; Human intelligence; Novel applications; Particle tracking; Scientific analysis; Task completion time; Tomographic images; Image analysis
Multiagent resource allocation for dynamic task arrivals with preemption,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979529841&doi=10.1145%2f2875441&partnerID=40&md5=3a53ca46d3947d70c789b877646491be,"In this article, we present a distributed algorithm for allocating resources to tasks in multiagent systems, one that adapts well to dynamic task arrivals where new work arises at short notice. Our algorithm is designed to leverage preemption if it is available, revoking resource allocations to tasks in progress if new opportunities arise that those resources are better suited to handle. Our multiagent model assigns a task agent to each task that must be completed and a proxy agent to each resource that is available. Preemption occurs when a task agent approaches a proxy agent with a sufficiently compelling need that the proxy agent determines the newcomer derives more benefit from the proxy agent's resource than the task agent currently using that resource. Task agents reason about which resources to request based on a learning of churn and congestion. We compare to a well-established multiagent resource allocation framework that permits preemption under more conservative assumptions and show through simulation that our model allows for improved allocations through more permissive preemption. In all, we offer a novel approach for multiagent resource allocation that is able to cope well with dynamic task arrivals. © 2016 ACM.",Agent cooperation-distributed problem solving; Agent cooperation-teamwork; Coalitions and coordination; Humans and agents-agents for improving human cooperative activities,Algorithms; Computational complexity; Human resource management; Problem solving; Resource allocation; Agent cooperation; Coalitions and coordination; Cooperative activity; Distributed problem solving; Dynamic tasks; Multi-Agent Model; Multiagent resource allocation; Show through; Multi agent systems
Robust decentralized low-rank matrix decomposition,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978062856&doi=10.1145%2f2854157&partnerID=40&md5=4dd7b4af364f2e97f199d2a5f896c914,"Low-rank matrix approximation is an important tool in data mining with a wide range of applications, including recommender systems, clustering, and identifying topics in documents. When the matrix to be approximated originates from a large distributed system, such as a network of mobile phones or smart meters, a challenging problem arises due to the strongly conflicting yet essential requirements of efficiency, robustness, and privacy preservation. We argue that although collecting sensitive data in a centralized fashion may be efficient, it is not an option when considering privacy and efficiency at the same time. Thus, we do not allow any sensitive data to leave the nodes of the network. The local information at each node (personal attributes, documents, media ratings, etc.) defines one row in the matrix. This means that all computations have to be performed at the edge of the network. Known parallel methods that respect the locality constraint, such as synchronized parallel gradient search or distributed iterative methods, require synchronized rounds or have inherent issues with load balancing, and thus they are not robust to failure. Our distributed stochastic gradient descent algorithm overcomes these limitations. During the execution, any sensitive information remains local, whereas the global features (e.g., the factor model of movies) converge to the correct value at all nodes. We present a theoretical derivation and a thorough experimental evaluation of our algorithm. We demonstrate that the convergence speed of our method is competitive while not relying on synchronization and being robust to extreme and realistic failure scenarios. To demonstrate the feasibility of our approach, we present trace-based simulations, real smartphone user behavior analysis, and tests over real movie recommender system data. 2016 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Data mining; Decentralized matrix factorization; Decentralized recommender systems; Online learning; Privacy; Singular value decomposition; Stochastic gradient descent,Behavioral research; Cellular telephone systems; Collector efficiency; Data mining; Data privacy; E-learning; Efficiency; Factorization; Iterative methods; Network management; Online systems; Recommender systems; Singular value decomposition; Stochastic systems; Synchronization; Telephone circuits; Time varying networks; Experimental evaluation; Low-rank matrix approximations; Matrix factorizations; Online learning; Stochastic gradient descent; Stochastic gradient descent algorithm; Theoretical derivations; User behavior analysis; Matrix algebra
Crowdsourcing empathetic intelligence: The case of the annotation of EMMA database for emotion and mood recognition,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970038931&doi=10.1145%2f2897369&partnerID=40&md5=cbb54e1dd4a85ca0449db79081b5949c,"Unobtrusive recognition of the user's mood is an essential capability for affect-adaptive systems. Mood is a subtle, long-term affective state, often misrecognized even by humans. The challenge to train a machine to recognize it from, for example, a video of the user, is significant, and already begins with the lack of ground truth for supervised learning. Existing affective databases consist mainly of short videos, annotated in terms of expressed emotions rather than mood. In very few cases, we encounter perceived mood annotations, of questionable reliability, however, due to the subjectivity of mood estimation and the small number of coders involved. In this work, we introduce a new database for mood recognition from video. Our database contains 180 long, acted videos, depicting typical daily scenarios, and subtle facial and bodily expressions. The videos cover three visual modalities (face, body, Kinect data), and are annotated in terms of emotions (via G-trace) and mood (via the Self-Assessment Manikin and the AffectButton). To annotate the database exhaustively, we exploit crowdsourcing to reach out to an extensive number of nonexpert coders. We validate the reliability of our crowdsourced annotations by (1) adopting a number of criteria to filter out unreliable coders, and (2) comparing the annotations of a subset of our videos with those collected in a controlled lab setting. © 2016 ACM 2157-6904/2016/05-ART51 $15.00.",Affective annotation; Crowdsourcing; Emotion recognition; Mood recognition; Multimodal database,Crowdsourcing; Affective annotations; Affective state; Emotion recognition; Ground truth; Mood recognition; Multimodal database; Self assessment; Visual modalities; Database systems
Crowdsourcing human annotation on web page structure: Infrastructure design and behavior-based quality control,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969993105&doi=10.1145%2f2870649&partnerID=40&md5=734ce3947f75e9c4a23969793444af76,"Parsing the semantic structure of a web page is a key component of web information extraction. Successful extraction algorithms usually require large-scale training and evaluation datasets, which are difficult to acquire. Recently, crowdsourcing has proven to be an effective method of collecting large-scale training data in domains that do not require much domain knowledge. For more complex domains, researchers have proposed sophisticated quality control mechanisms to replicate tasks in parallel or sequential ways and then aggregate responses from multiple workers. Conventional annotation integration methods often put more trust in the workers with high historical performance; thus, they are called performance-based methods. Recently, Rzeszotarski and Kittur have demonstrated that behavioral features are also highly correlated with annotation quality in several crowdsourcing applications. In this article, we present a new crowdsourcing system, called Wernicke, to provide annotations for web information extraction. Wernicke collects a wide set of behavioral features and, based on these features, predicts annotation quality for a challenging task domain: annotating web page structure. We evaluate the effectiveness of quality control using behavioral features through a case study where 32 workers annotate 200 Q&A web pages from five popular websites. In doing so, we discover several things: (1) Many behavioral features are significant predictors for crowdsourcing quality. (2) The behavioral-feature-based method outperforms performance-based methods in recall prediction, while performing equally with precision prediction. In addition, using behavioral features is less vulnerable to the cold-start problem, and the corresponding prediction model is more generalizable for predicting recall than precision for cross-website quality analysis. (3) One can effectively combine workers' behavioral information and historical performance information to further reduce prediction errors. 2016 Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 2157-6904/2016/04-ART56 $15.00.",Behavioral features; Crowdsourcing; Quality control; Worker performance,Algorithms; Behavioral research; Crowdsourcing; Forecasting; Information analysis; Information retrieval; Quality assurance; Semantic Web; Semantics; Web Design; Websites; Behavioral features; Extraction algorithms; Historical performance; Infrastructure design; Performance based method; Precision prediction; Web information extraction; Worker performance; Quality control
Coranking the future influence of multiobjects in bibliographic network through mutual reinforcement,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969915614&doi=10.1145%2f2897371&partnerID=40&md5=34d1740b7e6b626aaf1ef12574f2f3e7,"Scientific literature ranking is essential to help researchers find valuable publications from a large literature collection. Recently, with the prevalence of webpage ranking algorithms such as PageRank and HITS, graphbased algorithms have been widely used to iteratively rank papers and researchers through the networks formed by citation and coauthor relationships. However, existing graph-based ranking algorithms mostly focus on ranking the current importance of literature. For researchers who enter an emerging research area, they might be more interested in new papers and young researchers that are likely to become influential in the future, since such papers and researchers are more helpful in letting them quickly catch up on the most recent advances and find valuable research directions. Meanwhile, although some works have been proposed to rank the prestige of a certain type of objects with the help of multiple networks formed of multiobjects, there still lacks a unified framework to rank multiple types of objects in the bibliographic network simultaneously. In this article, we propose a unified ranking framework MRCoRank to corank the future popularity of four types of objects: papers, authors, terms, and venues through mutual reinforcement. Specifically, because the citation data of new publications are sparse and not efficient to characterize their innovativeness, we make the first attempt to extract the text features to help characterize innovative papers and authors. With the observation that the current trend is more indicative of the future trend of citation and coauthor relationships, we then construct time-aware weighted graphs to quantify the importance of links established at different times on both citation and coauthor graphs. By leveraging both the constructed text features and time-aware graphs, we finally fuse the rich information in a mutual reinforcement ranking framework to rank the future importance of multiobjects simultaneously. We evaluate the proposed model through extensive experiments on the ArnetMiner dataset containing more than 1,500,000 papers. Experimental results verify the effectiveness of MRCoRank in coranking the future influence of multiobjects in a bibliographic network. © 2016 ACM.",Influence mining; Literature ranking; Mutual reinforcement,Bibliographies; Graphic methods; Iterative methods; Reinforcement; Graph based rankings; Graph-based algorithms; Literature ranking; Multiple networks; Mutual reinforcement; Ranking algorithm; Scientific literature; Unified framework; Paper
Crowdsourcing without a crowd: Reliable online species identification using Bayesian models to minimize crowd size,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969916130&doi=10.1145%2f2776896&partnerID=40&md5=80a913c2c7857a569ed1c38a5d81747a,"We present an incremental Bayesian model that resolves key issues of crowd size and data quality for consensus labeling. We evaluate our method using data collected from a real-world citizen science program, BEEWATCH, which invites members of the public in the United Kingdom to classify (label) photographs of bumblebees as one of 22 possible species. The biological recording domain poses two key and hitherto unaddressed challenges for consensus models of crowdsourcing: (1) the large number of potential species makes classification difficult, and (2) this is compounded by limited crowd availability, stemming from both the inherent difficulty of the task and the lack of relevant skills among the general public. We demonstrate that consensus labels can be reliably found in such circumstances with very small crowd sizes of around three to five users (i.e., through group sourcing). Our incremental Bayesian model, which minimizes crowd size by re-evaluating the quality of the consensus label following each species identification solicited from the crowd, is competitive with a Bayesian approach that uses a larger but fixed crowd size and outperforms majority voting. These results have important ecological applicability: biological recording programs such as BEEWATCH can sustain themselves when resources such as taxonomic experts to confirm identifications by photo submitters are scarce (as is typically the case), and feedback can be provided to submitters in a timely fashion. More generally, our model provides benefits to any crowdsourced consensus labeling task where there is a cost (financial or otherwise) associated with soliciting a label. 2016 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Bayesian reasoning; Biological recording; Bumblebee identification; Citizen science; Consensus model; Crowdsourcing,Crowdsourcing; Bayesian approaches; Bayesian reasoning; Biological recording; Citizen science; Consensus models; General publics; Species identification; United kingdom; Bayesian networks
Telco user activity level prediction with massive mobile broadband data,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969924814&doi=10.1145%2f2856057&partnerID=40&md5=49789e043aa1ae4a78ebebedcf9918da,"Telecommunication (telco) operators aim to provide users with optimized services and bandwidth in a timely manner. The goal is to increase user experience while retaining profit. To do this, knowing the changing behavior patterns of users through their activity levels in advance can be a great help for operators to adjust their management strategies and reduce operational risk. To achieve this goal, the operators can make use of knowledge discovered from telco's historical mobile broadband (MBB) records to predict mobile access activity level at an early stage. In this article, we report our research in a real-world telco setting involving more than one million telco users. Our novel contribution includes representing users as documents containing a collection of changing spatiotemporal ""words"" that express user behavior. By extracting users' space-time access records in MBB data, we use latent Dirichlet allocation (LDA) to learn user-specific compact topic features for user activity level prediction. We propose a scalable online expectation-maximization (OEM) algorithm that can scale LDA to massive MBB data, which is significantly faster than several state-of-theart online LDA algorithms. Using these real-world MBB data, we confirm high performance in user activity level prediction. In addition, we show that the inferred topics indicate that future activity level anomalies correlate highly with early skewed bandwidth supply and demand relations. Thus, our prediction system can also guide the telco operators to balance the telecommunication network in terms of supply-demand relations, saving deployment costs and energy of cell towers in the future. © 2016 ACM.",Activity level prediction; Big spatiotemporal data; Latent Dirichlet allocation; Mobile broadband; Oem algorithm; User-specific topic features,Bandwidth; Data mining; Economics; Forecasting; Maximum principle; Statistics; Telecommunication services; Activity levels; Latent Dirichlet allocation; Mobile broadband; Spatio-temporal data; User-specific topic features; Behavioral research
Designing noise-minimal rotorcraft approach trajectories,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969944497&doi=10.1145%2f2838738&partnerID=40&md5=62259674764d844c042e02ad4afbcf7c,"NASA and the international aviation community are investing in the development of a commercial transportation infrastructure that includes the increased use of rotorcraft, specifically helicopters and civil tilt rotors. However, there is significant concern over the impact of noise on the communities surrounding the transportation facilities. One way to address the rotorcraft noise problem is by exploiting powerful search techniques coming from artificial intelligence to design low-noise flight profiles that can be then validated though field tests. This article investigates the use of discrete heuristic search methods to design low-noise approach trajectories for rotorcraft. Our work builds on a long research tradition in trajectory optimization using either numerical methods or discrete search. Novel features of our approach include the use of a discrete search space with a resolution that can be varied, and the coupling of search with a robust simulator to evaluate candidates. The article includes a systematic comparison of different search techniques; in particular, in the experiments, we are able to do a trade study that compares complete search algorithms such as A∗ with faster but approximate methods such as local search. © 2016 ACM 2157-6904/2016/04-ART58 $15.00.",Optimization; Path planning; Rotorcraft noise reduction; Sustainability,Artificial intelligence; Heuristic algorithms; Information analysis; Motion planning; NASA; Noise abatement; Noise pollution; Numerical methods; Optimization; Rotors; Sustainable development; Trajectories; Approach trajectories; Approximate methods; Commercial transportation; Heuristic search methods; International aviation; Research traditions; Trajectory optimization; Transportation facilities; Heuristic methods
PPLib: Toward the automated generation of Crowd computing programs using process recombination and auto-experimentation,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969945191&doi=10.1145%2f2897367&partnerID=40&md5=e07e0bad6f4f57ff7a2ad53b62e10cf6,"Crowdsourcing is increasingly being adopted to solve simple tasks such as image labeling and object tagging, as well as more complex tasks, where crowd workers collaborate in processes with interdependent steps. For the whole range of complexity, research has yielded numerous patterns for coordinating crowd workers in order to optimize crowd accuracy, efficiency, and cost. Process designers, however, often don't know which pattern to apply to a problem at hand when designing new applications for crowdsourcing. In this article, we propose to solve this problem by systematically exploring the design space of complex crowdsourced tasks via automated recombination and auto-experimentation for an issue at hand. Specifically, we propose an approach to finding the optimal process for a given problem by defining the deep structure of the problem in terms of its abstract operators, generating all possible alternatives via the (re)combination of the abstract deep structure with concrete implementations from a Process Repository, and then establishing the best alternative via auto-experimentation. To evaluate our approach, we implemented PPLib (pronounced ""People Lib""), a program library that allows for the automated recombination of known processes stored in an easily extensible Process Repository. We evaluated our work by generating and running a plethora of process candidates in two scenarios on Amazon's Mechanical Turk followed by a meta-evaluation, where we looked at the differences between the two evaluations. Our first scenario addressed the problem of text translation, where our automatic recombination produced multiple processes whose performance almost matched the benchmark established by an expert translation. In our second evaluation, we focused on text shortening; we automatically generated 41 crowd process candidates, among them variations of the well-established Find-Fix-Verify process. While Find-Fix-Verify performed well in this setting, our recombination engine produced five processes that repeatedly yielded better results. We close the article by comparing the two settings where the Recombinator was used, and empirically show that the individual processes performed differently in the two settings, which led us to contend that there is no unifying formula, hence emphasizing the necessity for recombination. © 2016 ACM.",Human computation algorithms,Abstracting; Algorithms; Automation; Benchmarking; Costs; Crowdsourcing; Abstract operator; Amazon's mechanical turks; Automated generation; Automatically generated; Computing program; Human computation; Multiple process; Process designers; Problem solving
A crowd-powered system for fashion similarity search,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969931216&doi=10.1145%2f2897365&partnerID=40&md5=d7ce1899e37579fb585796674a01335c,"Driven by the needs of customers and industry, online fashion search and analytics are recently gaining much attention. As fashion is mostly expressed by visual content, the analysis of fashion images in online social networks is a rich source of possible insights on evolving trends and customer preferences. Although a plethora of visual content is available, the modeling of clothes' physics and movement, the implicit semantics in fashion designs, and the subjectivity of their interpretation pose difficulties to fully automated solutions for fashion search and analysis. In this article, we present the design and evaluation of a crowd-powered system for fashion similarity search from Twitter, supporting trend analysis for fashion professionals. The system enables fashion similarity search based on specific human-based similarity criteria. This is achieved by implementing a novel machine-crowd workflow that supports complex tasks requiring highly subjective judgments where multiple true solutions may coexist. We discuss how this leads to a novel class of crowdpowered systems for which the output of the crowd is not used to verify the automatic analysis but is the desired outcome. Finally, we show how this kind of crowd involvement enables a novel kind of similarity search and represents a crucial factor for the acceptance of system results by the end user. © 2016 ACM 2157-6904/2016/04-ART46 $15.00.",Crowdsourcing; Fashion search; Human-machine cooperation; Interactive systems; Real-time crowd; Similarity dimensions; Social multimedia retrieval,Complex networks; Crowdsourcing; Man machine systems; Semantics; Social networking (online); Fashion search; Human-machine cooperation; Interactive system; Multimedia Retrieval; Real time; Similarity dimensions; Real time systems
Efficient generalized fused lasso and its applications,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969916075&doi=10.1145%2f2847421&partnerID=40&md5=fee6b208509ef51a46b9fa85c08b3f9f,"Generalized fused lasso (GFL) penalizes variables with l1 norms based both on the variables and their pairwise differences. GFL is useful when applied to data where prior information is expressed using a graph over the variables. However, the existing GFL algorithms incur high computational costs and do not scale to high-dimensional problems. In this study, we propose a fast and scalable algorithm for GFL. Based on the fact that fusion penalty is the Lovász extension of a cut function, we show that the key building block of the optimization is equivalent to recursively solving graph-cut problems. Thus, we use a parametric flow algorithm to solve GFL in an efficient manner. Runtime comparisons demonstrate a significant speedup compared to existing GFL algorithms. Moreover, the proposed optimization framework is very general; by designing different cut functions, we also discuss the extension of GFL to directed graphs. Exploiting the scalability of the proposed algorithm, we demonstrate the applications of our algorithm to the diagnosis of Alzheimer's disease (AD) and video background subtraction (BS). In the AD problem, we formulated the diagnosis of AD as a GFL regularized classification. Our experimental evaluations demonstrated that the diagnosis performance was promising. We observed that the selected critical voxels were well structured, i.e., connected, consistent according to cross validation, and in agreement with prior pathological knowledge. In the BS problem, GFL naturally models arbitrary foregrounds without predefined grouping of the pixels. Even by applying simple background models, e.g., a sparse linear combination of former frames, we achieved state-of-the-art performance on several public datasets. © 2016 ACM.",Alzheimer's disease; Background subtraction; Generalized fused lasso; Parametric cut,Algorithms; Computer aided diagnosis; Diagnosis; Directed graphs; Graphic methods; Neurodegenerative diseases; Optimization; Alzheimer's disease; Background subtraction; Experimental evaluation; Fused lassos; High-dimensional problems; Optimization framework; Parametric cut; State-of-the-art performance; Problem solving
Multitask low-rank affinity graph for image segmentation and image annotation,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964515483&doi=10.1145%2f2856058&partnerID=40&md5=1db920e6f2ff270407292651580e08e8,"This article investigates a low-rank representation-based graph, which can used in graph-based vision tasks including image segmentation and image annotation. It naturally fuses multiple types of image features in a framework named multitask low-rank affinity pursuit. Given the image patches described with multiple types of features, we aim at inferring a unified affinity matrix that implicitly encodes the relations among these patches. This is achieved by seeking the sparsity-consistent low-rank affinities from the joint decompositions of multiple feature matrices into pairs of sparse and low-rank matrices, the latter of which is expressed as the production of the image feature matrix and its corresponding image affinity matrix. The inference process is formulated as aminimization problem and solved efficiently with the augmented Lagrange multiplier method. Considering image patches as vertices, a graph can be built based on the resulted affinity matrix. Compared to previous methods, which are usually based on a single type of feature, the proposed method seamlessly integrates multiple types of features to jointly produce the affinity matrix in a single inference step. The proposed method is applied to graph-based image segmentation and graph-based image annotation. Experiments on benchmark datasets well validate the superiority of using multiple features over single feature and also the superiority of our method over conventional methods for feature fusion.",Image annotation; Image segmentation; Low rank; Multitask,Graphic methods; Image analysis; Lagrange multipliers; Multitasking; Augmented lagrange multiplier methods; Conventional methods; Graph-based image segmentations; Image annotation; Joint decomposition; Low rank; Low-rank representations; Sparse and low ranks; Image segmentation
Incentives for effort in crowdsourcing using the peer truth serum,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964653355&doi=10.1145%2f2856102&partnerID=40&md5=326bad6242e109065ce8b932684c449a,"Crowdsourcing is widely proposed as a method to solve a large variety of judgment tasks, such as classifying website content, peer grading in online courses, or collecting real-world data. As the data reported by workers cannot be verified, there is a tendency to report random data without actually solving the task. This can be countered by making the reward for an answer depend on its consistency with answers given by other workers, an approach called peer consistency. However, it is obvious that the best strategy in such schemes is for all workers to report the same answer without solving the task. Dasgupta and Ghosh [2013] show that, in some cases, exerting high effort can be encouraged in the highest-paying equilibrium. In this article, we present a general mechanism that implements this idea and is applicable to most crowdsourcing settings. Furthermore, we experimentally test the novel mechanism, and validate its theoretical properties.",Crowdsourcing; Mechanism design; Peer prediction,Curricula; Grading; Machine design; Websites; Mechanism design; Online course; Random data; Real-world; Web site contents; Crowdsourcing
Leveraging human computations to improve schematization of spatial relations from imagery,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964777781&doi=10.1145%2f2873065&partnerID=40&md5=c83658ae678b2291bd9ddcf0944efb71,"The process of generating schematic maps of salient objects from a set of pictures of an indoor environment is challenging. It has been an active area of research as it is crucial to a wide range of context- And locationaware services, as well as for general scene understanding. Although many automated systems have been developed to solve the problem, most of them either require predefining labels or expensive equipment, such as RGBD sensors or lasers, to scan the environment. In this article, we introduce a prototype system to show how human computations can be utilized to generate schematic maps from a set of pictures, without making strong assumptions or demanding extra devices. The system requires humans (crowd workers from Amazon Mechanical Turks) to do simple spatial mapping tasks in various conditions, and their data are aggregated by filtering and clustering techniques that allow salient cues to be identified in the pictures and their spatial relations to be inferred and projected on a two-dimensional map. In particular, we tested and demonstrated the effectiveness of two methods that improved the quality of the generated schematic map: (1) We encouraged humans to adopt an allocentric representations of salient objects by guiding them to perform mental rotations of these objects and (2) we sensitized human perception by guided arrows superimposed on the imagery to improve the accuracy of depth and width estimation. We demonstrated the feasibility of our system by evaluating the results of schematic maps generated from indoor pictures taken from an office building. By calculating Riemannian shape distances between the generated maps to the ground truth, we found that the generated schematic maps captured the spatial relations well. Our results showed that the combination of human computations and machine clustering could lead to more-accurate schematized maps from imagery.We also discuss how our approach may have important insights on methods that leverage human computations in other areas. © 2016 ACM.",Crowdsourcing; Human computations; Picture schematization; Spatial cognition,Crowdsourcing; Office buildings; Allocentric representations; Amazon mechanical turks; Clustering techniques; Expensive equipments; Human computation; Location aware services; Picture schematization; Spatial cognition; Automation
A game-theory approach for effective crowdsource-based relevance assessment,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964725549&doi=10.1145%2f2873063&partnerID=40&md5=82f6eb9e0304e5b84ed70a0532cf2446,"Despite the ever-increasing popularity of crowdsourcing (CS) in both industry and academia, procedures that ensure quality in its results are still elusive. We hypothesise that a CS design based on game theory can persuade workers to perform their tasks as quickly as possible with the highest quality. In order to do so, in this article we propose a CS framework inspired by the n-person Chicken game. Our aim is to address the problem of CS quality without compromising on CS benefits such as low monetary cost and high task completion speed. With that goal in mind, we study the effects of knowledge updates as well as incentives for good workers to continue playing.We define a general task with the characteristics of relevance assessment as a case study, because it has been widely explored in the past with CS due to its potential cost and complexity. In order to investigate our hypotheses, we conduct a simulation where we study the effect of the proposed framework on data accuracy, task completion time, and total monetary rewards. Based on a game-theoretical analysis, we study how different types of individuals would behave under a particular game scenario. In particular, we simulate a population comprised of different types of workers with varying ability to formulate optimal strategies and learn from their experiences. A simulation of the proposed framework produced results that support our hypothesis. © 2016 ACM.",Crowdsourcing; Game theory; Relevance assessment,Crowdsourcing; Data accuracy; Game theoretical analysis; Knowledge update; Monetary costs; Monetary rewards; Optimal strategies; Relevance assessments; Task completion time; Game theory
Modality-dependent cross-media retrieval,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963719502&doi=10.1145%2f2775109&partnerID=40&md5=ce50a4f8154bda04c07ced9eb5613702,"Article: In this article, we investigate the cross-media retrieval between images and text, that is, using image to search text (I2T) and using text to search images (T2I). Existing cross-media retrieval methods usually learn one couple of projections, by which the original features of images and text can be projected Into a common latent space to measure the content similarity. However, using the same projections for the two different retrieval tasks (I2T and T2I) may lead to a tradeoff between their respective performances, rather than their best performances. Different from previous works, we propose a modality-dependent cross-media retrieval (MDCR) model, where two couples of projections are learned for different cross-media retrieval tasks instead of one couple of projections. Specifically, by jointly optimizing the correlation between images and text and the linear regression from one modal space (image or text) to the semantic space, two couples of mappings are learned to project images and text from their original feature spaces into two common latent subspaces (one for I2T and the other for T2I). Extensive experiments show the superiority of the proposed MDCR compared with other methods. In particular, based on the 4,096-dimensional convolutional neural network (CNN) visual feature and 100-dimensional Latent Dirichlet Allocation (LDA) textual feature, the mAP of the proposed method achieves the mAP score of 41.5%, which is a new state-of-the-art performance on the Wikipedia dataset. © 2016 ACM.",Canonical correlation analysis; Cross-media retrieval; Subspace learning,Neural networks; Semantics; Statistics; Canonical correlation analysis; Content similarity; Convolutional neural network; Cross-media retrieval; Latent dirichlet allocations; State-of-the-art performance; Subspace learning; Textual features; Information retrieval
Rapid low-cost virtual human bootstrapping via the crowd,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963764758&doi=10.1145%2f2897366&partnerID=40&md5=51f1e94f73dd4290f274b9417ed6890c,"Virtual human interactions provide an important avenue for training as emergent opportunities arise. In response to a new training need, we propose a framework to rapidly create experiential learning opportunities in the form of a question-answer chat interaction with virtual humans. This framework takes quickly generated case documents and breaks down the case into small tasks that can be crowdsourced by nonexperts. This framework can serve as a first step to rapidly bootstrapping new virtual humans. We have applied our framework to the task of preparing health care students and professionals to infrequent, but high-stakes, situations such as infectious diseases, cranial nerve disorders, and stroke. Our framework was utilized by medical professionals interested in providing new training experiences to students and colleagues. Over the course of two months, these professionals created seven scenarios on a diverse range of topics that included Ebola, cancer, and neurological disorders. These scenarios were developed for multiple target audiences such as medical students, residents, and fellows. As a first step, each scenario utilized our framework and crowdsourced workers to create an initial corpus over the course of two days. From these seven cases, we selected two to evaluate the quality of the resulting virtual-human corpuses. The two scenarios were compared to preexisting reference scenarios that have been in curricular use for several years. We found a reduction in author time commitment of at least 92% while creating a character that was at least 75% as accurate as its reference counterparts. The commitment reduction and accuracy achieved by our framework represents a first step towards rapid development of a virtual human. Our framework can then be combined with other creation processes for further virtual-human development in order to create a mature virtual human. As part of a virtual-human development process, our framework can help to rapidly develop new scenarios in response to emergent training opportunities. © 2016 ACM.",Corpus generation; Crowdsource bootstrap framework; Crowdsourcing; Virtual human,Crowdsourcing; Diseases; Students; Corpus generation; Creation process; Experiential learning; Infectious disease; Medical professionals; Neurological disorders; Training experiences; Virtual humans; Virtual reality
Introduction to the special issue on recommender system benchmarking,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960960145&doi=10.1145%2f2870627&partnerID=40&md5=b43943be97127f21913c66862134737f,[No abstract available],Benchmarking; Evaluation; Recommender systems,
CITY FEED: A pilot system of citizen-sourcing for city issue management,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964754678&doi=10.1145%2f2873064&partnerID=40&md5=dba1eff28258fe9ebc6c0c04031baa34,"Crowdsourcing implies user collaboration and engagement, which fosters a renewal of city governance processes. In this article, we address a subset of crowdsourcing, named citizen-sourcing, where citizens interact with authorities collaboratively and actively. Many systems have experimented citizen-sourcing in city governance processes; however, theirmaturity levels are mixed. In order to focus on the service maturity, we introduce a city service maturity framework that contains five levels of service support and two levels of information integration. As an example, we introduce CITY FEED, which implements citizen-sourcing in city issue management process. In order to support such process, CITY FEED supports all levels of the maturity framework (publishing, transacting, interacting, collaborating, and evaluating) and integrates related information relationally and heterogeneously. In order to integrate heterogeneous information, it implements a threefold feed deduplication mechanism based on the geographic, text semantic, and image similarities of feeds. Currently, CITY FEED is in a pilot stage. © 2016 ACM.",Citizen-sourced city issuemanagement; Citizen-sourcing; City service maturity framework; Crowdsourcing; Feed deduplication,Semantics; Citizen-sourced city issuemanagement; Citizen-sourcing; City service maturity framework; De duplications; Heterogeneous information; Information integration; Issue managements; User collaborations; Crowdsourcing
STCAPLRS: A spatial-temporal context-aware personalized location recommendation system,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964308247&doi=10.1145%2f2842631&partnerID=40&md5=b9f4a7ab9228e798c3bfeb6a01281387,"Newly emerging location-based social media network services (LBSMNS) provide valuable resources to understand users' behaviors based on their location histories. The location-based behaviors of a user are generally influenced by both user intrinsic interest and the location preference, and moreover are spatial-temporal context dependent. In this article, we propose a spatial-temporal context-aware personalized location recommendation system (STCAPLRS), which offers a particular user a set of location items such as points of interest or venues (e.g., restaurants and shopping malls) within a geospatial range by considering personal interest, local preference, and spatial-temporal context influence. STCAPLRS can make accurate recommendation and facilitate people's local visiting and new location exploration by exploiting the context information of user behavior, associations between users and location items, and the location and content information of location items. Specifically, STCAPLRS consists of two components: offline modeling and online recommendation. The core module of the offline modeling part is a context-aware regression mixture model that is designed to model the location-based user behaviors in LBSMNS to learn the interest of each individual user, the local preference of each individual location, and the context-aware influence factors. The online recommendation part takes a querying user along with the corresponding querying spatial-temporal context as input and automatically combines the learned interest of the querying user, the local preference of the querying location, and the context-aware influence factor to produce the top-k recommendations.We evaluate the performance of STCAPLRS on two real-world datasets: Dianping and Foursquare. The results demonstrate the superiority of STCAPLRS in recommending location items for users in terms of both effectiveness and efficiency. Moreover, the experimental analysis results also illustrate the excellent interpretability of STCAPLRS. © 2016 ACM.",Location recommendation; Matrix factorization; Topic model,Behavioral research; Factorization; Location; Recommender systems; Content information; Context information; Effectiveness and efficiencies; Experimental analysis; Matrix factorizations; Real-world datasets; Social media networks; Topic Modeling; Location based services
Anytime algorithms for recommendation service providers,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960956217&doi=10.1145%2f2835496&partnerID=40&md5=425346ff4442b683380eea641fd71eae,"Recommender systems (RS) can now be found in many commercial Web sites, often presenting customers with a short list of additional products that they might purchase. Many commercial sites do not typically have the ability and resources to develop their own system and may outsource the RS to a third party. This had led to the growth of a recommendation as a service industry, where companies, referred to as RS providers, provide recommendation services. These companies must carefully balance the cost of building recommendation models and the payment received from the e-business, as these payments are expected to be low. In such a setting, restricting the computational time required for model building is critical for the RS provider to be profitable. In this article, we propose anytime algorithms as an attractive method for balancing computational time and the recommendation model performance, thus tackling the RS provider problem. In an anytime setting, an algorithm can be stopped after any amount of computational time, always ensuring that a valid, although suboptimal, solution will be returned. Given sufficient time, however, the algorithm should converge to an optimal solution. In this setting, it is important to evaluate the quality of the returned solution over time, monitoring quality improvement. This is significantly different from traditional evaluation methods, which mostly estimate the performance of the algorithm only after its convergence is given sufficient time. We show that the popular item-item top-N recommendation approach can be brought into the anytime framework by smartly considering the order by which item pairs are being evaluated. We experimentally show that the time-accuracy trade-off can be significantly improved for this specific problem. © 2016 ACM 2157-6904/2016/02-ART43 $15.00.",Anytime algorithms; Collaborative filtering; Recommender systems,Collaborative filtering; Economic and social effects; Quality control; Recommender systems; Any-time algorithms; Computational time; Evaluation methods; Model performance; Optimal solutions; Quality improvement; Service industries; Specific problems; Algorithms
Preface to the ACM Tist special issue on causal discovery and inference,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057830748&doi=10.1145%2f2840720&partnerID=40&md5=aebc6ef90dd94f4842221c3aacb37776,[No abstract available],,
A comprehensive survey on Pose-Invariant Face Recognition,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960950090&doi=10.1145%2f2845089&partnerID=40&md5=d4f1936e247d1bd11a866d0be2d48366,"The capacity to recognize faces under varied poses is a fundamental human ability that presents a unique challenge for computer vision systems. Compared to frontal face recognition, which has been intensively studied and has gradually matured in the past few decades, Pose-Invariant Face Recognition (PIFR) remains a largely unsolved problem. However, PIFR is crucial to realizing the full potential of face recognition for real-world applications, since face recognition is intrinsically a passive biometric technology for recognizing uncooperative subjects. In this article, we discuss the inherent difficulties in PIFR and present a comprehensive review of established techniques. Existing PIFR methods can be grouped into four categories, that is, pose-robust feature extraction approaches, multiview subspace learning approaches, face synthesis approaches, and hybrid approaches. The motivations, strategies, pros/cons, and performance of representative approaches are described and compared. Moreover, promising directions for future research are discussed. © 2016 ACM.",Face synthesis; Multiview learning; Pose-Invariant Face Recognition; Pose-robust feature; Survey,Computer vision; Feature extraction; Surveying; Surveys; Biometric technology; Computer vision system; Face synthesis; Multi-view learning; Multiview subspace learning; Pose robust; Pose-invariant face recognition; Unsolved problems; Face recognition
A novel classification framework for evaluating individual and aggregate diversity in Top-N Recommendations,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960947042&doi=10.1145%2f2700491&partnerID=40&md5=b089867960af6f9e595e9824b64760ef,"The primary goal of a recommender system is to generate high quality user-centred recommendations. However, the traditional evaluation methods and metrics were developed before researchers understood all the factors that increase user satisfaction. This study is an introduction to a novel user and item classification framework. It is proposed that this framework should be used during user-centred evaluation of recommender systems and the need for this framework is justified through experiments. User profiles are constructed and matched against other users profiles to formulate neighbourhoods and generate top-N recommendations. The recommendations are evaluated to measure the success of the process. In conjunction with the framework, a new diversitymetric is presented and explained. The accuracy, coverage, and diversity of top-N recommendations is illustrated and discussed for groups of users. It is found that in contradiction to common assumptions, not all users suffer as expected from the data sparsity problem. In fact, the group of users that receive the most accurate recommendations do not belong to the least sparse area of the dataset. © 2016 ACM.",Collaborative filtering; Performance evaluation metrics; Recommendation accuracy; Recommendation diversity; Recommendation quality; Recommender systems,Collaborative filtering; Recommender systems; Classification framework; Data sparsity problems; Evaluation methods; Performance evaluation metrics; Recommendation accuracy; Recommendation diversities; User satisfaction; Users profiles; Quality control
A framework for dataset benchmarking and its application to a new movie rating dataset,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960959073&doi=10.1145%2f2751565&partnerID=40&md5=710dce71cb37095e1c79cc1433eb92a2,"Rating datasets are of paramount importance in recommender systems research. They serve as input for recommendation algorithms, as simulation data, or for evaluation purposes. In the past, public accessible rating datasets were not abundantly available, leaving researchers no choice but to work with old and static datasets like MovieLens and Netflix. More recently, however, emerging trends as social media and smartphones are found to provide rich data sources which can be turned into valuable research datasets. While dataset availability is growing, a structured way for introducing and comparing new datasets is currently still lacking. In this work, we propose a five-step framework to introduce and benchmark new datasets in the recommender systems domain. We illustrate our framework on a new movie rating dataset-called MovieTweetings-collected from Twitter. Following our framework, we detail the origin of the dataset, provide basic descriptive statistics, investigate external validity, report the results of a number of reproducible benchmarks, and conclude by discussing some interesting advantages and appropriate research use cases. © 2016 ACM.",Benchmark; Dataset; Evaluation; imdb; Movielens; Movietweetings; Reproducibility; Twitter,Algorithms; Rating; Recommender systems; Social networking (online); Dataset; Evaluation; imdb; Movielens; Movietweetings; Reproducibilities; Twitter; Benchmarking
Location prediction: A temporal-spatial Bayesian model,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960947796&doi=10.1145%2f2816824&partnerID=40&md5=2c57394f06640e5b3885e1f14342d881,"In social networks, predicting a user's location mainly depends on those of his/her friends, where the key lies in how to select his/her most influential friends. In this article, we analyze the theoretically maximal accuracy of location prediction based on friends' locations and compare it with the practical accuracy obtained by the state-of-the-art location prediction methods. Upon observing a big gap between the theoretical and practical accuracy, we propose a new strategy for selecting influential friends in order to improve the practical location prediction accuracy. Specifically, several features are defined to measure the influence of the friends on a user's location, based on which we put forth a sequential random-walk-with-restart procedure to rank the friends of the user in terms of their influence. By dynamically selecting the top N most influential friends of the user per time slice, we develop a temporal-spatial Bayesian model to characterize the dynamics of friends' influence for location prediction. Finally, extensive experimental results on datasets of real social networks demonstrate that the proposed influential friend selection method and temporal-spatial Bayesian model can significantly improve the accuracy of location prediction. © 2016 ACM.",Dynamic Bayesian network; Dynamic selection; Influential friends; Location prediction; Temporalspatial evolution,Forecasting; Location; Dynamic Bayesian networks; Dynamic selection; Influential friends; Location prediction; Temporalspatial evolution; Bayesian networks
The role of cores in recommender benchmarking for social bookmarking systems,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960952459&doi=10.1145%2f2700485&partnerID=40&md5=082fb93844f2096e46b128cbedaebb14,"Social bookmarking systems have established themselves as an important part in today's Web. In such systems, tag recommender systems support users during the posting of a resource by suggesting suitable tags. Tag recommender algorithms have often been evaluated in offline benchmarking experiments. Yet, the particular setup of such experiments has rarely been analyzed. In particular, since the recommendation quality usually suffers from difficulties such as the sparsity of the data or the cold-start problem for new resources or users, datasets have often been pruned to so-called cores (specific subsets of the original datasets), without much consideration of the implications on the benchmarking results. In this article, we generalize the notion of a core by introducing the new notion of a set-core, which is independent of any graph structure, to overcome a structural drawback in the previous constructions of cores on tagging data. We show that problems caused by some types of cores can be eliminated using set-cores. Further, we present a thorough analysis of tag recommender benchmarking setups using cores. To that end, we conduct a large-scale experiment on four real-world datasets, in which we analyze the influence of different cores on the evaluation of recommendation algorithms. We can show that the results of the comparison of different recommendation approaches depends on the selection of core type and level. For the benchmarking of tag recommender algorithms, our results suggest that the evaluation must be set up more carefully and should not be based on one arbitrarily chosen core type and level. © 2016 ACM.",Benchmarking; Core; Evaluation; Graph; Preprocessing; Recommender,Core; Evaluation; Graph; Preprocessing; Recommender; Benchmarking
Beyond relevance: Explicitly promoting novelty and diversity in tag recommendation,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960951768&doi=10.1145%2f2801130&partnerID=40&md5=b085820d4ca900b8893a0756f6c926db,"The design and evaluation of tag recommendation methods has historically focused on maximizing the relevance of the suggested tags for a given object, such as a movie or a song. However, relevance by itself may not be enough to guarantee recommendation usefulness. Promoting novelty and diversity in tag recommendation not only increases the chances that the user will select ""some"" of the recommended tags but also promotes complementary information (i.e., tags), which helps to cover multiple aspects or topics related to the target object. Previous work has addressed the tag recommendation problem by exploiting at most two of the following aspects: (1) relevance, (2) explicit topic diversity, and (3) novelty. In contrast, here we tackle these three aspects conjointly, by introducing two new tag recommendation methods that cover all three aspects of the problem at different levels. Our first method, called Random Forest with topic-related attributes, or RFt, extends a relevance-driven tag recommender based on the Random Forest (RF) learningto-rank method by including new tag attributes to capture the extent to which a candidate tag is related to the topics of the target object. This solution captures topic diversity as well as novelty at the attribute level while aiming at maximizing relevance in its objective function. Our second method, called Explicit Tag Recommendation Diversifier with Novelty Promotion, or xTReND, reranks the recommendations provided by any tag recommender to jointly promote relevance, novelty, and topic diversity. We use RFt as a basic recommender applied before the reranking, thus building a solution that addresses the problem at both attribute and objective levels. Furthermore, to enable the use of our solutions on applications in which category information is unavailable, we investigate the suitability of using latent Dirichlet allocation (LDA) to automatically generate topics for objects. We evaluate all tag recommendation approaches using real data from five popular Web 2.0 applications. Our results show that RFt greatly outperforms the relevance-driven RF baseline in diversity while producing gains in relevance as well. We also find that our new xTReND reranker obtains considerable gains in both novelty and relevance when compared to that same baseline while keeping the same relevance levels. Furthermore, compared to our previous reranker method, xTReD, which does not consider novelty, xTReND is also quite effective, improving the novelty of the recommended tags while keeping similar relevance and diversity levels in most datasets and scenarios. Comparing our two new proposals, we find that xTReND considerably outperforms RFt in terms of novelty and diversity with only small losses (under 4%) in relevance. Overall, considering the trade-off among relevance, novelty, and diversity, our results demonstrate the superiority of xTReND over the baselines and the proposed alternative, RFt. Finally, the use of automatically generated latent topics as an alternative to manually labeled categories also provides significant improvements, which greatly enhances the applicability of our solutions to applications where the latter is not available.",Novelty; Relevance; Tag recommendation; Topic diversity,Decision trees; Statistics; Automatically generated; Design and evaluations; Latent dirichlet allocations; Novelty; Relevance; Tag recommendations; Topic diversity; Web 2.0 applications; Economic and social effects
Video face editing using temporal-spatial-smooth warping,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960964034&doi=10.1145%2f2819000&partnerID=40&md5=d9932fccb3f41be2164e174df9f45c44,"Editing faces in videos is a popular yet challenging task in computer vision and graphics that encompasses various applications, including facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation. Directly applying the existing warping methods to video face editing has the major problem of temporal incoherence in the synthesized videos, which cannot be addressed by simply employing face tracking techniques or manual interventions, as it is difficult to eliminate the subtly temporal incoherence of the facial feature point localizations in a video sequence. In this article, we propose a temporal-spatial-smooth warping (TSSW) method to achieve a high temporal coherence for video face editing. TSSW is based on two observations: (1) the control lattices are critical for generating warping surfaces and achieving the temporal coherence between consecutive video frames, and (2) the temporal coherence and spatial smoothness of the control lattices can be simultaneously and effectively preserved. Based upon these observations, we impose the temporal coherence constraint on the control lattices on two consecutive frames, as well as the spatial smoothness constraint on the control lattice on the current frame. TSSW calculates the control lattice (in either the horizontal or vertical direction) by updating the control lattice (in the corresponding direction) on its preceding frame, i.e., minimizing a novel energy function that unifies a data-driven term, a smoothness term, and feature point constraints. The contributions of this article are twofold: (1) we develop TSSW, which is robust to the subtly temporal incoherence of the facial feature point localizations and is effective to preserve the temporal coherence and spatial smoothness of the control lattices for editing faces in videos, and (2) we present a new unified video face editing framework that is capable for improving the performances of facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation. © 2016 ACM 2157-6904/2016/02-ART32 $15.00.",Spatial smoothness; Temporal coherence; Video face editing; Warping,Computer vision; Facial attractiveness; Facial feature points; Manual intervention; Spatial smoothness; Temporal coherence; Vertical direction; Video face editing; Warping; Face recognition
Relevance meets coverage: A unified framework to generate diversified recommendations,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960954642&doi=10.1145%2f2700496&partnerID=40&md5=cd868659388464d95c59a08edcfea0e6,"Collaborative filtering (CF) models offer users personalized recommendations by measuring the relevance between the active user and each individual candidate item. Following this idea, user-based collaborative filtering (UCF) usually selects the local popular items from the like-minded neighbor users. However, these traditional relevance-based models only consider the individuals (i.e., each neighbor user and candidate item) separately during neighbor set selection and recommendation set generation, thus usually incurring highly similar recommendations that lack diversity. While many researchers have recognized the importance of diversified recommendations, the proposed solutions either needed additional semantic information of items or decreased accuracy in this process. In this article, we describe how to generate both accurate and diversified recommendations from a new perspective. Along this line, we first introduce a simple measure of coverage that quantifies the usefulness of the whole set, that is, the neighbor userset and the recommended itemset as a complete entity. Then we propose a recommendation framework named REC that considers both traditional relevance-based scores and the new coverage measure based on UCF. Under REC, we further prove that the goals of maximizing relevance and coverage measures simultaneously in both the neighbor set selection step and the recommendation set generation step are NP-hard. Luckily, we can solve them effectively and efficiently by exploiting the inherent submodular property. Furthermore, we generalize the coverage notion and the REC framework from both a data perspective and an algorithm perspective. Finally, extensive experimental results on three real-world datasets show that the REC-based recommendation models can naturally generate more diversified recommendations without decreasing accuracy compared to some state-of-the-art models. © 2016 ACM.",Collaborative filtering; Coverage; Diversity; Personalized recommendation,Recommender systems; Semantics; Coverage; Diversity; Personalized recommendation; Real-world datasets; Semantic information; SET generation; State of the art; Unified framework; Collaborative filtering
Multimedia news summarization in search,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960946318&doi=10.1145%2f2822907&partnerID=40&md5=67aa9c63442bebe2d1dce2da6a761b1c,"It is a necessary but challenging task to relieve users from the proliferative news information and allow them to quickly and comprehensively master the information of the whats and hows that are happening in the world every day. In this article, we develop a novel approach of multimedia news summarization for searching results on the Internet, which uncovers the underlying topics among query-related news information and threads the news events within each topic to generate a query-related brief overview. First, the hierarchical latent Dirichlet allocation (hLDA) model is introduced to discover the hierarchical topic structure from query-related news documents, and a new approach based on the weighted aggregation and max pooling is proposed to identify one representative news article for each topic. One representative image is also selected to visualize each topic as a complement to the text information. Given the representative documents selected for each topic, a time-bias maximum spanning tree (MST) algorithm is proposed to thread them into a coherent and compact summary of their parent topic. Finally, we design a friendly interface to present users with the hierarchical summarization of their required news information. Extensive experiments conducted on a large-scale news dataset collected from multiple newsWeb sites demonstrate the encouraging performance of the proposed solution for news summarization in news retrieval. © 2016 ACM.",Hierarchical latent Dirichlet allocation; Maximum spanning tree; Multimodal; News summarization; Topic structure,Statistics; Latent Dirichlet allocation; Maximum spanning tree; Multi-modal; News summarization; Topic structures; Trees (mathematics)
Intelligent evacuation management systems: A review,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960947081&doi=10.1145%2f2842630&partnerID=40&md5=e6d5b3cc095ca134a472d0cd7bfe6bec,"Crowd and evacuation management have been active areas of research and study in the recent past. Various developments continue to take place in the process of efficient evacuation of crowds in mass gatherings. This article is intended to provide a review of intelligent evacuation management systems covering the aspects of crowd monitoring, crowd disaster prediction, evacuation modelling, and evacuation path guidelines. Soft computing approaches play a vital role in the design and deployment of intelligent evacuation applications pertaining to crowd control management. While the review deals with video and nonvideo based aspects of crowd monitoring and crowd disaster prediction, evacuation techniques are reviewed via the theme of soft computing, along with a brief review on the evacuation navigation path. We believe that this review will assist researchers in developing reliable automated evacuation systems that will help in ensuring the safety of the evacuees especially during emergency evacuation scenarios. © 2016 ACM.",Crowd management; Crowd monitoring; Evacuation modelling; Evacuation path guidelines; Prediction of crowd disaster,Forecasting; Soft computing; Crowd disaster; Crowd managements; Emergency evacuation; Evacuation management; Evacuation modelling; Evacuation path guidelines; Evacuation systems; Soft computing approaches; Disasters
Generating incremental length summary based on hierarchical topic coverage maximization,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960948850&doi=10.1145%2f2809433&partnerID=40&md5=799b394dd275b3059c83d955123b0c6a,"Document summarization is playing an important role in coping with information overload on the Web. Many summarization models have been proposed recently, but few try to adjust the summary length and sentence order according to application scenarios. With the popularity of handheld devices, presenting key information first in summaries of flexible length is of great convenience in terms of faster reading and decision-making and network consumption reduction. Targeting this problem, we introduce a novel task of generating summaries of incremental length. In particular, we require that the summaries should have the ability to automatically adjust the coverage of general-detailed information when the summary length varies. We propose a novel summarization model that incrementally maximizes topic coverage based on the document's hierarchical topic model. In addition to the standard Rouge-1 measure, we define a new evaluation metric based on the similarity of the summaries' topic coverage distribution in order to account for sentence order and summary length. Extensive experiments on Wikipedia pages, DUC 2007, and general noninverted writing style documents from multiple sources show the effectiveness of our proposed approach. Moreover, we carry out a user study on a mobile application scenario to show the usability of the produced summary in terms of improving judgment accuracy and speed, as well as reducing the reading burden and network traffic. © 2016 ACM 2157-6904/2016/02-ART29 $15.00.",Data reconstruction; Multi-document summarization,Decision making; Application scenario; Consumption reductions; Data reconstruction; Document summarization; Hierarchical topic models; Information overloads; Multi-document summarization; Summarization models; Natural language processing systems
Mutual component analysis for heterogeneous face recognition,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960960096&doi=10.1145%2f2807705&partnerID=40&md5=fedf4150244d2c951b1cb5ca4f405e10,"Heterogeneous face recognition, also known as cross-modality face recognition or intermodality face recognition, refers to matching two face images from alternative image modalities. Since face images from different image modalities of the same person are associated with the same face object, there should be mutual components that reflect those intrinsic face characteristics that are invariant to the image modalities. Motivated by this rationality, we propose a novel approach called Mutual Component Analysis (MCA) to infer the mutual components for robust heterogeneous face recognition. In the MCA approach, a generative model is first proposed to model the process of generating face images in different modalities, and then an Expectation Maximization (EM) algorithm is designed to iteratively learn the model parameters. The learned generative model is able to infer the mutual components (which we call the hidden factor, where hidden means the factor is unreachable and invisible, and can only be inferred from observations) that are associated with the person's identity, thus enabling fast and effective matching for cross-modality face recognition. To enhance recognition performance, we propose an MCA-based multiclassifier framework using multiple local features. Experimental results show that our new approach significantly outperforms the state-of-the-art results on two typical application scenarios: sketch-to-photo and infrared-to-visible face recognition.",Face recognition; Heterogeneous face recognition; Mutual Component Analysis (MCA),Algorithms; Image matching; Iterative methods; Maximum principle; Component analysis; Effective matching; Expectation-maximization algorithms; Generative model; Infrared-to-visible; Inter modalities; Multi-classifier; Typical application; Face recognition
Incentive mechanism design for crowdsourcing: An all-pay auction approach,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960947513&doi=10.1145%2f2837029&partnerID=40&md5=411dc58b59c7ef0ffb1e311ee2b8307e,"Crowdsourcing can be modeled as a principal-agent problem in which the principal (crowdsourcer) desires to solicit a maximal contribution from a group of agents (participants) while agents are only motivated to act according to their own respective advantages. To reconcile this tension, we propose an all-pay auction approach to incentivize agents to act in the principal's interest, i.e., maximizing profit, while allowing agents to reap strictly positive utility. Our rationale for advocating all-pay auctions is based on two merits that we identify, namely all-pay auctions (i) compress the common, two-stage ""bid-contribute"" crowdsourcing process into a single ""bid-cum-contribute"" stage, and (ii) eliminate the risk of task nonfulfillment. In our proposed approach, we enhance all-pay auctions with two additional features: an adaptive prize and a general crowdsourcing environment. The prize or reward adapts itself as per a function of the unknown winning agent's contribution, and the environment or setting generally accommodates incomplete and asymmetric information, risk-averse (and risk-neutral) agents, and a stochastic (and deterministic) population. We analytically derive this all-pay auction-based mechanism and extensively evaluate it in comparison to classic and optimized mechanisms. The results demonstrate that our proposed approach remarkably outperforms its counterparts in terms of the principal's profit, agent's utility, and social welfare. © 2016 ACM.",Bayesian Nash equilibrium; Incomplete information; Mobile crowd sensing; Participatory sensing; Risk aversion; Shading effect,Commerce; Profitability; Stochastic systems; Bayesian Nash equilibria; Incomplete information; Mobile crowd sensing; Participatory Sensing; Risk aversion; Shading effect; Crowdsourcing
Parameterized decay model for information retrieval,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960953738&doi=10.1145%2f2800794&partnerID=40&md5=75d07b492173f18faaefed9a62cd9841,"This article proposes a term weighting scheme for measuring query-document similarity that attempts to explicitly model the dependency between separate occurrences of a term in a document. The assumption is that, if a term appears once in a document, it is more likely to appear again in the same document. Thus, as the term appears again and again, the information content of the subsequent occurrences decreases gradually, since they are more predictable. We introduce a parameterized decay function to model this assumption, where the initial contribution of the term can be determined using any reasonable term discrimination factor. The effectiveness of the proposed model is evaluated on a number of recent web test collections of varying nature. The experimental results show that the proposed model significantly outperforms a number of well known retrieval models including a recently proposed strong Term Frequency and Inverse Document Frequency (TF-IDF) model.",Document ranking; Retrieval model; Term weighting,Search engines; Text processing; Document ranking; Information contents; Inverse Document Frequency; Query documents; Retrieval models; Term weighting; Term weighting scheme; Test Collection; Information retrieval
"S-smart: A unified Bayesian framework for simultaneous semantic mapping, activity recognition, and tracking",2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960961919&doi=10.1145%2f2824286&partnerID=40&md5=8ce03071f40239d19c685327d2f565c9,"The machine recognition of user trajectories and activities is fundamental to devise context-aware applications for support and monitoring in daily life. So far, tracking and activity recognition were mostly considered as orthogonal problems, which limits the richness of possible context inference. In this work, we introduce the novel unified computational and representational framework S-SMART that simultaneously models the environment state (semantic mapping), localizes the user within this map (tracking), and recognizes interactions with the environment (activity recognition). Thus, S-SMART identifies which activities the user executes where (e.g., turning a handle next to a window), and reflects the outcome of these actions by updating the world model (e.g., the window is now open). This in turn conditions the future possibility of executing actions at specific places (e.g., closing the window is likely to be the next action at this location). S-SMART works in a self-contained manner and iteratively builds the semantic map from wearable sensors only. This enables the seamless deployment to new environments. We characterize S-SMART in an experimental dataset with people performing hand actions as part of their usual routines at home and in office buildings. The framework combines dead reckoning from a footworn motion sensor with template-matching-based action recognition, identifying objects in the environment (windows, doors, water taps, phones, etc.) and tracking their state (open/closed, etc.). In real-life recordings with up to 23 action classes, S-SMART consistently outperforms independent systems for positioning and activity recognition, and constructs accurate semantic maps. This environment representation enables novel applications that build upon information about the arrangement and state of the user's surroundings. For example, it may be possible to remind elderly people of a window that they left open before leaving the house, or of a plant they did not water yet, using solely wearable sensors. © 2016 ACM 2157-6904/2016/02-ART34 $15.00 DOI:.",Activity recognition; Context awareness; Localization; Particle filter; Semantic mapping; Slam; Template matching; Wearable sensors,Image matching; Image recognition; Mapping; Motion estimation; Office buildings; Pattern recognition; Plant shutdowns; Semantics; Template matching; Wearable technology; Activity recognition; Context- awareness; Localization; Particle filter; Semantic mapping; Slam; Wearable sensors
Participatory cultural mapping based on collective behavior data in location-based social networks,2016,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956988054&doi=10.1145%2f2814575&partnerID=40&md5=de62871a639d57563269a9738fed6a40,"Culture has been recognized as a driving impetus for human development. It co-evolves with both human belief and behavior. When studying culture, Cultural Mapping is a crucial tool to visualize different aspects of culture (e.g., religions and languages) from the perspectives of indigenous and local people. Existing cultural mapping approaches usually rely on large-scale survey data with respect to human beliefs, such as moral values. However, such a data collection method not only incurs a significant cost of both human resources and time, but also fails to capture human behavior, which massively reflects cultural information. In addition, it is practically difficult to collect large-scale human behavior data. Fortunately, with the recent boom in Location-Based Social Networks (LBSNs), a considerable number of users report their activities in LBSNs in a participatory manner, which provides us with an unprecedented opportunity to study largescale user behavioral data. In this article, we propose a participatory cultural mapping approach based on collective behavior in LBSNs. First, we collect the participatory sensed user behavioral data from LBSNs. Second, since only local users are eligible for cultural mapping, we propose a progressive ""home"" location identification method to filter out ineligible users. Third, by extracting three key cultural features from daily activity, mobility, and linguistic perspectives, respectively, we propose a cultural clustering method to discover cultural clusters. Finally, we visualize the cultural clusters on the world map. Based on a real-world LBSN dataset, we experimentally validate our approach by conducting both qualitative and quantitative analysis on the generated cultural maps. The results show that our approach can subtly capture cultural features and generate representative cultural maps that correspond well with traditional cultural maps based on survey data. © 2016 ACM.",Collective behavior; Cultural difference; Cultural mapping; Location based social networks; Participatory sensing,Computational linguistics; Location; Mapping; Maps; Social networking (online); Social sciences; Surveys; Collective behavior; Cultural difference; Cultural informations; Data collection method; Location identification; Location-based social networks; Participatory Sensing; Qualitative and quantitative analysis; Behavioral research
Self-supervised Bipartite Graph Representation Learning: A Dirichlet Max-margin Matrix Factorization Approach,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195099414&doi=10.1145%2f3645098&partnerID=40&md5=b5f7b47edfba166e7eeac4e60d090b73,"Bipartite graph representation learning aims to obtain node embeddings by compressing sparse vectorized representations of interactions between two types of nodes, e.g., users and items. Incorporating structural attributes among homogeneous nodes, such as user communities, improves the identification of similar interaction preferences, namely, user/item embeddings, for downstream tasks. However, existing methods often fail to proactively discover and fully utilize these latent structural attributes. Moreover, the manual collection and labeling of structural attributes is always costly. In this article, we propose a novel approach called Dirichlet Max-margin Matrix Factorization (DM3F), which adopts a self-supervised strategy to discover latent structural attributes and model discriminative node representations. Specifically, in self-supervised learning, our approach generates pseudo group labels (i.e., structural attributes) as a supervised signal using the Dirichlet process without relying on manual collection and labeling, and employs them in a max-margin classification. Additionally, we introduce a Variational Markov Chain Monte Carlo algorithm (Variational MCMC) to effectively update the parameters. The experimental results on six real datasets demonstrate that, in the majority of cases, the proposed method outperforms existing approaches based on matrix factorization and neural networks. Furthermore, the modularity analysis confirms the effectiveness of our model in capturing structural attributes to produce high-quality user embeddings.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bipartite graph; item recommendation; matrix completion; representation learning; Self-supervised learning; user embedding,Data mining; Graph embeddings; Graph theory; Matrix algebra; Supervised learning; Bipartite graphs; Dirichlet; Embeddings; Graph representation; Item recommendation; Matrix completion; Matrix factorizations; Representation learning; Self-supervised learning; User embedding; Matrix factorization
Ensuring Fairness and Gradient Privacy in Personalized Heterogeneous Federated Learning,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195059585&doi=10.1145%2f3652613&partnerID=40&md5=28601172bba5caed15503ec8601e34ad,"With the increasing tension between conflicting requirements of the availability of large amounts of data for effective machine learning-based analysis, and for ensuring their privacy, the paradigm of federated learning has emerged, a distributed machine learning setting where the clients provide only the machine learning model updates to the server rather than the actual data for decision making. However, the distributed nature of federated learning raises specific challenges related to fairness in a heterogeneous setting. This motivates the focus of our article, on the heterogeneity of client devices having different computational capabilities and their impact on fairness in federated learning. Furthermore, our aim is to achieve fairness in heterogeneity while ensuring privacy. As far as we are aware there are no existing works that address all three aspects of fairness, device heterogeneity, and privacy simultaneously in federated learning. In this article, we propose a novel federated learning algorithm with personalization in the context of heterogeneous devices while maintaining compatibility with the gradient privacy preservation techniques of secure aggregation. We analyze the proposed federated learning algorithm under different environments with different datasets and show that it achieves performance close to or greater than the state-of-the-art in heterogeneous device personalized federated learning. We also provide theoretical proofs for the fairness and convergence properties of our proposed algorithm.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",device heterogeneity; fairness; Federated learning; privacy,Decision making; Machine learning; Privacy-preserving techniques; Device heterogeneities; Distributed machine learning; Fairness; Federated learning; Heterogeneous devices; Large amounts of data; Learning settings; Machine learning models; Machine-learning; Privacy; Learning algorithms
Empowering Predictive Modeling by GAN-based Causal Information Learning,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195099799&doi=10.1145%2f3652610&partnerID=40&md5=1cd09725bcd7b51dde0c35e5ae71b665,"Generally speaking, we can easily specify many causal relationships in the prediction tasks of ubiquitous computing, such as human activity prediction, mobility prediction, and health prediction. However, most of the existing methods in these fields failed to take advantage of this prior causal knowledge. They typically make predictions only based on correlations in the data, which hinders the prediction performance in real-world scenarios, because a distribution shift between training data and testing data generally exists. To fill in this gap, we proposed a Generative Adversarial Network (GAN)-based Causal Information Learning prediction framework, which can effectively leverage causal information to improve the prediction performance of existing ubiquitous computing deep learning models. Specifically, faced with a unique challenge that the treatment variable, referring to the intervention that influences the target in a causal relationship, is generally continuous in ubiquitous computing, the framework employs a representation learning approach with a GAN-based deep learning model. By projecting all variables except the treatment into a latent space, it effectively minimizes confounding bias and leverages the learned latent representation for accurate predictions. In this way, it deals with the continuous treatment challenge, and in the meantime, it can be easily integrated with existing deep learning models to lift their prediction performance in practical scenarios with causal information. Extensive experiments on two large-scale real-world datasets demonstrate its superior performance over multiple state-of-the-art baselines. We also propose an analytical framework together with extensive experiments to empirically show that our framework achieves better performance gain under two conditions: when the distribution differences between the training data and the testing data are more significant and when the treatment effects are larger. Overall, this work suggests that learning causal information is a promising way to improve the prediction performance of ubiquitous computing tasks. We open both our dataset and code1 and call for more research attention in this area.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",causal information learning; GAN; Prediction; predictive modeling,Deep learning; Generative adversarial networks; Large datasets; Learning systems; Ubiquitous computing; Causal information learning; Causal relationships; Human activities; Learning models; Network-based; Prediction performance; Prediction tasks; Predictive models; Testing data; Training data; Forecasting
Perceiving Actions via Temporal Video Frame Pairs,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195025679&doi=10.1145%2f3652611&partnerID=40&md5=a8ab9d32c2e20186400d288ef4b2ec47,"Video action recognition aims at classifying the action category in given videos. In general, semantic-relevant video frame pairs reflect significant action patterns such as object appearance variation and abstract temporal concepts like speed, rhythm, and so on. However, existing action recognition approaches tend to holistically extract spatiotemporal features. Though effective, there is still a risk of neglecting the crucial action features occurring across frames with a long-term temporal span. Motivated by this, in this article, we propose to perceive actions via frame pairs directly and devise a novel Nest Structure with frame pairs as basic units. Specifically, we decompose a video sequence into all possible frame pairs and hierarchically organize them according to temporal frequency and order, thus transforming the original video sequence into a Nest Structure. Through naturally decomposing actions, the proposed structure can flexibly adapt to diverse action variations such as speed or rhythm changes. Next, we devise a Temporal Pair Analysis module (TPA) to extract discriminative action patterns based on the proposed Nest Structure. The designed TPA module consists of a pair calculation part to calculate the pair features and a pair fusion part to hierarchically fuse the pair features for recognizing actions. The proposed TPA can be flexibly integrated into existing backbones, serving as a side branch to capture various action patterns from multi-level features. Extensive experiments show that the proposed TPA module can achieve consistent improvements over several typical backbones, reaching or updating CNN-based SOTA results on several challenging action recognition benchmarks.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",action recognition; temporal modelling; Video understanding,Abstracting; Video recording; Action recognition; Object appearance; Pair analysis; Spatiotemporal feature; Temporal concepts; Temporal models; Temporal video; Video frame; Video sequences; Video understanding; Semantics
HydraGAN: A Cooperative Agent Model for Multi-Objective Data Generation,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195055405&doi=10.1145%2f3653982&partnerID=40&md5=9ab05f1694e44ba76c83e4954fbff556,"Generative adversarial networks have become a de facto approach to generate synthetic data points that resemble their real counterparts. We tackle the situation where the realism of individual samples is not the sole criterion for synthetic data generation. Additional constraints such as privacy preservation, distribution realism, and diversity promotion may also be essential to optimize. To address this challenge, we introduce HydraGAN, a multi-agent network that performs multi-objective synthetic data generation. We theoretically verify that training the HydraGAN system, containing a single generator and an arbitrary number of discriminators, leads to a Nash equilibrium. Experimental results for six datasets indicate that HydraGAN consistently outperforms prior methods in maximizing the Area under the Radar Chart, balancing a combination of cooperative or competitive data generation goals.  © 2024 Copyright held by the owner/author(s).",contrasting objectives; multi-agent GAN; privacy-preserving data mining; Synthetic data generation,Data mining; Generative adversarial networks; Privacy-preserving techniques; Agent modeling; Contrasting objective; Cooperative agents; Data generation; Multi agent; Multi objective; Multi-agent GAN; Privacy-preserving data mining; Synthetic data; Synthetic data generations; Multi agent systems
A Game-theoretic Framework for Privacy-preserving Federated Learning,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195080165&doi=10.1145%2f3656049&partnerID=40&md5=4983cc877f5d8bd81a5c039120927f6f,"In federated learning, benign participants aim to optimize a global model collaboratively. However, the risk of privacy leakage cannot be ignored in the presence of semi-honest adversaries. Existing research has focused either on designing protection mechanisms or on inventing attacking mechanisms. While the battle between defenders and attackers seems never-ending, we are concerned with one critical question: Is it possible to prevent potential attacks in advance? To address this, we propose the first game-theoretic framework that considers both FL defenders and attackers in terms of their respective payoffs, which include computational costs, FL model utilities, and privacy leakage risks. We name this game the federated learning privacy game (FLPG), in which neither defenders nor attackers are aware of all participants' payoffs. To handle the incomplete information inherent in this situation, we propose associating the FLPG with an oracle that has two primary responsibilities. First, the oracle provides lower and upper bounds of the payoffs for the players. Second, the oracle acts as a correlation device, privately providing suggested actions to each player. With this novel framework, we analyze the optimal strategies of defenders and attackers. Furthermore, we derive and demonstrate conditions under which the attacker, as a rational decision-maker, should always follow the oracle's suggestion not to attack.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",divergence; efficiency; Federated learning; optimization; privacy; trade-off; utility,Economic and social effects; Game theory; Privacy-preserving techniques; Divergence; Federated learning; Game-theoretic; Global models; Optimisations; Privacy; Privacy leakages; Privacy preserving; Trade off; Utility; Decision making
FedCMD: A Federated Cross-modal Knowledge Distillation for Drivers' Emotion Recognition,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195053592&doi=10.1145%2f3650040&partnerID=40&md5=7671df39e74702ceaee4e4c0a13bed90,"Emotion recognition has attracted a lot of interest in recent years in various application areas such as healthcare and autonomous driving. Existing approaches to emotion recognition are based on visual, speech, or psychophysiological signals. However, recent studies are looking at multimodal techniques that combine different modalities for emotion recognition. In this work, we address the problem of recognizing the user's emotion as a driver from unlabeled videos using multimodal techniques. We propose a collaborative training method based on cross-modal distillation, i.e., ""FedCMD""(Federated Cross-Modal Distillation). Federated Learning (FL) is an emerging collaborative decentralized learning technique that allows each participant to train their model locally to build a better generalized global model without sharing their data. The main advantage of FL is that only local data is used for training, thus maintaining privacy and providing a secure and efficient emotion recognition system. The local model in FL is trained for each vehicle device with unlabeled video data by using sensor data as a proxy. Specifically, for each local model, we show how driver emotional annotations can be transferred from the sensor domain to the visual domain by using cross-modal distillation. The key idea is based on the observation that a driver's emotional state indicated by a sensor correlates with facial expressions shown in videos. The proposed ""FedCMD""approach is tested on the multimodal dataset ""BioVid Emo DB""and achieves state-of-the-art performance. Experimental results show that our approach is robust to non-identically distributed data, achieving 96.67% and 90.83% accuracy in classifying five different emotions with IID (independently and identically distributed) and non-IID data, respectively. Moreover, our model is much more robust to overfitting, resulting in better generalization than the other existing methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cross-modal distillation; Emotion recognition; federated learning; transfer learning,Emotion Recognition; Learning systems; Speech recognition; Transfer learning; Application area; Cross-modal; Cross-modal distillation; Distributed data; Emotion recognition; Federated learning; Independently and identically distributed; Local model; Multi-modal techniques; Transfer learning; Distillation
Score-based Graph Learning for Urban Flow Prediction,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191366772&doi=10.1145%2f3655629&partnerID=40&md5=4e54b7f853d5985804d5f0051cdce0a3,"Accurate urban flow prediction (UFP) is crucial for a range of smart city applications such as traffic management, urban planning, and risk assessment. To capture the intrinsic characteristics of urban flow, recent efforts have utilized spatial and temporal graph neural networks to deal with the complex dependence between the traffic in adjacent areas. However, existing graph neural network based approaches suffer from several critical drawbacks, including improper graph representation of urban traffic data, lack of semantic correlation modeling among graph nodes, and coarse-grained exploitation of external factors. To address these issues, we propose DiffUFP, a novel probabilistic graph-based framework for UFP. DiffUFP consists of two key designs: (1) a semantic region dynamic extraction method that effectively captures the underlying traffic network topology, and (2) a conditional denoising score-based adjacency matrix generator that takes spatial, temporal, and external factors into account when constructing the adjacency matrix rather than simply concatenation in existing studies. Extensive experiments conducted on real-world datasets demonstrate the superiority of DiffUFP over the state-of-the-art UFP models and the effect of the two specific modules.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Flow prediction; graph neural networks; score-based model; spatio-temporal learning; urban computing,Data mining; Flow graphs; Forecasting; Graphic methods; Network architecture; Risk assessment; Semantics; Adjacency matrix; External factors; Flow prediction; Graph neural networks; Score-based model; Spatio-temporal learning; Traffic management; Urban computing; Urban flow; Urban risk; Graph neural networks
Analysing Utterances in LLM-Based User Simulation for Conversational Search,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195030287&doi=10.1145%2f3650041&partnerID=40&md5=ff815dbef776895d0371f359613dee46,"Clarifying underlying user information needs by asking clarifying questions is an important feature of modern conversational search systems. However, evaluation of such systems through answering prompted clarifying questions requires significant human effort, which can be time-consuming and expensive. In our recent work, we proposed an approach to tackle these issues with a user simulator, USi. Given a description of an information need, USi is capable of automatically answering clarifying questions about the topic throughout the search session. However, while the answers generated by USi are both in line with the underlying information need and in natural language, a deeper understanding of such utterances is lacking. Thus, in this work, we explore utterance formulation of large language model (LLM)-based user simulators. To this end, we first analyze the differences between USi, based on GPT-2, and the next generation of generative LLMs, such as GPT-3. Then, to gain a deeper understanding of LLM-based utterance generation, we compare the generated answers to the recently proposed set of patterns of human-based query reformulations. Finally, we discuss potential applications as well as limitations of LLM-based user simulators and outline promising directions for future work on the topic.  © 2024 Copyright held by the owner/author(s).",conversational search; mixed-initiative; User simulation,Conversational search; Important features; Language model; Mixed-initiative; Model-based OPC; Natural languages; Search sessions; Search system; User information need; User simulation
A Meta-Learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195089711&doi=10.1145%2f3652612&partnerID=40&md5=b0c723304d8859e80811a2bb9c9879c9,"Trustworthy federated learning typically leverages protection mechanisms to guarantee privacy. However, protection mechanisms inevitably introduce utility loss or efficiency reduction while protecting data privacy. Therefore, protection mechanisms and their parameters should be carefully chosen to strike an optimal tradeoff among privacy leakage, utility loss, and efficiency reduction. To this end, federated learning practitioners need tools to measure the three factors and optimize the tradeoff between them to choose the protection mechanism that is most appropriate to the application at hand. Motivated by this requirement, we propose a framework that (1) formulates trustworthy federated learning as a problem of finding a protection mechanism to optimize the tradeoff among privacy leakage, utility loss, and efficiency reduction and (2) formally defines bounded measurements of the three factors. We then propose a meta-learning algorithm to approximate this optimization problem and find optimal protection parameters for representative protection mechanisms, including randomization, homomorphic encryption, secret sharing, and compression. We further design estimation algorithms to quantify these found optimal protection parameters in a practical horizontal federated learning setting and provide a theoretical analysis of the estimation error.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",divergence; efficiency; Federated learning; optimization; privacy; tradeoff; utility,Cryptography; Data privacy; Learning algorithms; Parameter estimation; % reductions; Divergence; Federated learning; Optimal protection; Optimisations; Privacy; Privacy leakages; Protection mechanisms; Tradeoff; Utility; Efficiency
Quintuple-based Representation Learning for Bipartite Heterogeneous Networks,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195055738&doi=10.1145%2f3653978&partnerID=40&md5=28fc6d0810ebd5d24d568e51ecb685ab,"Recent years have seen rapid progress in network representation learning, which removes the need for burdensome feature engineering and facilitates downstream network-based tasks. In reality, networks often exhibit heterogeneity, which means there may exist multiple types of nodes and interactions. Heterogeneous networks raise new challenges to representation learning, as the awareness of node and edge types is required. In this article, we study a basic building block of general heterogeneous networks, the heterogeneous networks with two types of nodes. Many problems can be solved by decomposing general heterogeneous networks into multiple bipartite ones. Recently, to overcome the demerits of non-metric measures used in the embedding space, metric learning-based approaches have been leveraged to tackle heterogeneous network representation learning. These approaches first generate triplets of samples, in which an anchor node, a positive counterpart, and a negative one co-exist, and then try to pull closer positive samples and push away negative ones. However, when dealing with heterogeneous networks, even the simplest two-typed ones, triplets cannot simultaneously involve both positive and negative samples from different parts of networks. To address this incompatibility of triplet-based metric learning, in this article, we propose a novel quintuple-based method for learning node representations in bipartite heterogeneous networks. Specifically, we generate quintuples that contain positive and negative samples from two different parts of networks. And we formulate two learning objectives that accommodate quintuple-based learning samples, a proximity-based loss that models the relations in quintuples by sigmoid probabilities and an angular loss that more robustly maintains similarity structures. In addition, we also parameterize feature learning by using one-dimensional convolution operators around nodes' neighborhoods. Compared with eight methods, extensive experiments on two downstream tasks manifest the effectiveness of our approach.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",heterogeneous networks; link prediction; Network representation learning; quintuple sampling; recommendation,Learning systems; Downstream networks; Feature engineerings; In networks; Link prediction; Metric learning; Negative samples; Network representation; Network representation learning; Quintuple sampling; Recommendation; Heterogeneous networks
Learning Cross-modality Interaction for Robust Depth Perception of Autonomous Driving,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195094259&doi=10.1145%2f3650039&partnerID=40&md5=dea3897188e3331e80e19b672fc49676,"As one of the fundamental tasks of autonomous driving, depth perception aims to perceive physical objects in three dimensions and to judge their distances away from the ego vehicle. Although great efforts have been made for depth perception, LiDAR-based and camera-based solutions have limitations with low accuracy and poor robustness for noise input. With the integration of monocular cameras and LiDAR sensors in autonomous vehicles, in this article, we introduce a two-stream architecture to learn the modality interaction representation under the guidance of an image reconstruction task to compensate for the deficiencies of each modality in a parallel manner. Specifically, in the two-stream architecture, the multi-scale cross-modality interactions are preserved via a cascading interaction network under the guidance of the reconstruction task. Next, the shared representation of modality interaction is integrated to infer the dense depth map due to the complementarity and heterogeneity of the two modalities. We evaluated the proposed solution on the KITTI dataset and CALAR synthetic dataset. Our experimental results show that learning the coupled interaction of modalities under the guidance of an auxiliary task can lead to significant performance improvements. Furthermore, our approach is competitive against the state-of-the-art models and robust against the noisy input. The source code is available at https://github.com/tonyFengye/Code/tree/master.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",autonomous systems; auxiliary task; Cascading interaction; depth completion; depth prediction,Autonomous vehicles; Depth perception; Image reconstruction; Network architecture; Optical radar; Autonomous driving; Autonomous system; Auxiliary task; Cascading interaction; Cross modality; Depth completion; Depth prediction; Physical objects; Stream architecture; Two-stream; Cameras
Tapestry of Time and Actions: Modeling Human Activity Sequences Using Temporal Point Process Flows,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195055832&doi=10.1145%2f3650045&partnerID=40&md5=fa1f46cca9cc00b0ed23026ec2bf6c66,"Human beings always engage in a vast range of activities and tasks that demonstrate their ability to adapt to different scenarios. These activities can range from the simplest daily routines, like walking and sitting, to multi-level complex endeavors such as cooking a four-course meal. Any human activity can be represented as a temporal sequence of actions performed to achieve a certain goal. Unlike the time series datasets extracted from electronics or machines, these action sequences are highly disparate in their nature - the time to finish a sequence of actions can vary between different persons. Therefore, understanding the dynamics of these sequences is essential for many downstream tasks such as activity length prediction, goal prediction, and next action recommendation. Existing neural network based approaches that learn a continuous-time activity sequence are limited to the presence of only visual data or are designed specifically for a particular task (i.e., limited to next action or goal prediction). In this article, we present ProActive, a neural marked temporal point process framework for modeling the continuous-time distribution of actions in an activity sequence while simultaneously addressing three high-impact problems: next action prediction, sequence goal prediction, and end-to-end sequence generation. Specifically, we utilize a self-attention module with temporal normalizing flows to model the influence and the inter-arrival times between actions in a sequence. Moreover, for time-sensitive prediction, we perform an early detection of sequence goal via a constrained margin-based optimization procedure. This in turn allows ProActive to predict the sequence goal using a limited number of actions. In addition, we propose a novel addition over the ProActive model, called ProActive++, that can handle variations in the order of actions (i.e., different methods of achieving a given goal). We demonstrate that this variant can learn the order in which the person or actor prefers to do their actions. Extensive experiments on sequences derived from three activity recognition datasets show the significant accuracy boost of our ProActive and ProActive++ over the state of the art in terms of action and goal prediction, and the first-ever application of end-to-end action sequence generation.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",activity modeling; continuous-time sequences; goal prediction; Marked temporal point process; sequence generation,Constrained optimization; Continuous time systems; Activity modeling; Activity sequence; Continous time; Continuous-time sequence; Goal prediction; Human activities; Marked temporal point process; Point process; Sequence generation; Time sequences; Forecasting
MHGCN+: Multiplex Heterogeneous Graph Convolutional Network,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188945510&doi=10.1145%2f3650046&partnerID=40&md5=f50af77f066911c04bb1e29b216bebdd,"Heterogeneous graph convolutional networks have gained great popularity in tackling various network analytical tasks on heterogeneous graph data, ranging from link prediction to node classification. However, most existing works ignore the relation heterogeneity with multiplex networks between multi-typed nodes and the different importance of relations in meta-paths for node embedding, which can hardly capture the heterogeneous structure signals across different relations. To tackle this challenge, this work proposes a Multiplex Heterogeneous Graph Convolutional Network (MHGCN+) for multiplex heterogeneous network embedding. Our MHGCN+ can automatically learn the useful heterogeneous meta-path interactions of different lengths with different importance in multiplex heterogeneous networks through multi-layer convolution aggregation. Additionally, we effectively integrate both multi-relation structural signals and attribute semantics into the learned node embeddings with both unsupervised and semi-supervised learning paradigms. Extensive experiments on seven real-world datasets with various network analytical tasks demonstrate the significant superiority of MHGCN+ against state-of-the-art embedding baselines in terms of all evaluation metrics. The source code of our method is available at: https://github.com/FuChF/MHGCN-plus.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph convolutional networks; graph representation learning; multiplex heterogeneous networks; Network embedding,Convolution; Convolutional neural networks; Graph neural networks; Graph structures; Graph theory; Network embeddings; Network layers; Semantics; Supervised learning; Convolutional networks; Embeddings; Graph convolutional network; Graph data; Graph representation; Graph representation learning; Heterogeneous graph; Link prediction; Multiplex heterogeneous network; Network embedding; Heterogeneous networks
Deconfounded Cross-modal Matching for Content-based Micro-video Background Music Recommendation,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195018768&doi=10.1145%2f3650042&partnerID=40&md5=ec903dfb957787b4e0d7855a914818b1,"Object-oriented micro-video background music recommendation is a complicated task where the matching degree between videos and background music is a major issue. However, music selections in user-generated content (UGC) are prone to selection bias caused by historical preferences of uploaders. Since historical preferences are not fully reliable and may reflect obsolete behaviors, over-reliance on them should be avoided as knowledge and interests dynamically evolve. In this article, we propose a Deconfounded Cross-Modal matching model to mitigate such bias. Specifically, uploaders' personal preferences of music genres are identified as confounders that spuriously correlate music embeddings and background music selections, causing the learned system to over-recommend music from majority groups. To resolve such confounders, backdoor adjustment is utilized to deconfound the spurious correlation between music embeddings and prediction scores. We further utilize Monte Carlo estimator with batch-level average as the approximations to avoid integrating the entire confounder space calculated by the adjustment. Furthermore, we design a teacher-student network to utilize the matching of music videos, which is professionally generated content (PGC) with specialized matching, to better recommend content-matching background music. The PGC data are modeled by a teacher network to guide the matching of uploader-selected UGC data of student network by Kullback-Leibler-based knowledge transfer. Extensive experiments on the TT-150k-genre dataset demonstrate the effectiveness of the proposed method. The code is publicly available on https://github.com/jing-1/DecCM  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cross-modal matching; debiased recommender systems; knowledge distillation; variational auto-encoder,Embeddings; Knowledge management; Music; Recommender systems; Signal encoding; Auto encoders; Background musics; Confounder; Cross-modal; Cross-modal matching; Debiased recommende system; Knowledge distillation; Matchings; Modal matching; Variational auto-encoder; Distillation
Advancing Attribution-Based Neural Network Explainability through Relative Absolute Magnitude Layer-Wise Relevance Propagation and Multi-Component Evaluation,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195082585&doi=10.1145%2f3649458&partnerID=40&md5=99e69dd3c9e00b4a6da72ebd95fcbf6b,"Recent advancement in deep-neural network performance led to the development of new state-of-the-art approaches in numerous areas. However, the black-box nature of neural networks often prohibits their use in areas where model explainability and model transparency are crucial. Over the years, researchers proposed many algorithms to aid neural network understanding and provide additional information to the human expert. One of the most popular methods being Layer-Wise Relevance Propagation (LRP). This method assigns local relevance based on the pixel-wise decomposition of nonlinear classifiers. With the rise of attribution method research, there has emerged a pressing need to assess and evaluate their performance. Numerous metrics have been proposed, each assessing an individual property of attribution methods such as faithfulness, robustness, or localization. Unfortunately, no single metric is deemed optimal for every case, and researchers often use several metrics to test the quality of the attribution maps. In this work, we address the shortcomings of the current LRP formulations and introduce a novel method for determining the relevance of input neurons through layer-wise relevance propagation. Furthermore, we apply this approach to the recently developed Vision Transformer architecture and evaluate its performance against existing methods on two image classification datasets, namely ImageNet and PascalVOC. Our results clearly demonstrate the advantage of our proposed method. Furthermore, we discuss the insufficiencies of current evaluation metrics for attribution-based explainability and propose a new evaluation metric that combines the notions of faithfulness, robustness, and contrastiveness. We utilize this new metric to evaluate the performance of various attribution-based methods. Our code is available at: https://github.com/davor10105/relative-absolute-magnitude-propagation  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attribution-based evaluation; Explainable artificial intelligence; layer-wise relevance propagation; Vision Transformer,Backpropagation; Classification (of information); Multilayer neural networks; Attribution-based evaluation; Component evaluation; Evaluation metrics; Explainable artificial intelligence; Layer-wise; Layer-wise relevance propagation; Multicomponents; Neural-networks; Performance; Vision transformer; Deep neural networks
CACTUS: A Comprehensive Abstraction and Classification Tool for Uncovering Structures,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193713647&doi=10.1145%2f3649459&partnerID=40&md5=a1bcd09a3bbad5519956d8788fa96d76,"The availability of large datasets is providing the impetus for driving many current artificial intelligent developments. However, specific challenges arise in developing solutions that exploit small datasets, mainly due to practical and cost-effective deployment issues, as well as the opacity of deep learning models. To address this, the Comprehensive Abstraction and Classification Tool for Uncovering Structures (CACTUS) is presented as a means of improving secure analytics by effectively employing explainable artificial intelligence. CACTUS achieves this by providing additional support for categorical attributes, preserving their original meaning, optimising memory usage, and speeding up the computation through parallelisation. It exposes to the user the frequency of the attributes in each class and ranks them by their discriminative power. Performance is assessed by applying it to various domains, including Wisconsin Diagnostic Breast Cancer, Thyroid0387, Mushroom, Cleveland Heart Disease, and Adult Income datasets.  © 2024 Copyright held by the owner/author(s).",Artificial intelligence; classification; explainable artificial intelligence; knowledge discovery,Abstracting; Deep learning; Diagnosis; Diseases; Large datasets; 'current; Artificial intelligent; Categorical attributes; Classification tool; Cost effective; Developing solutions; Explainable artificial intelligence; Large datasets; Learning models; Small data set; Cost effectiveness
Multimodal Dialogue Systems via Capturing Context-aware Dependencies and Ordinal Information of Semantic Elements,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195040921&doi=10.1145%2f3645099&partnerID=40&md5=ecf2ad6a9516c9da93b08d6b3f9784c5,"The topic of multimodal conversation systems has recently garnered significant attention across various industries, including travel and retail, among others. While pioneering works in this field have shown promising performance, they often focus solely on context information at the utterance level, overlooking the context-aware dependencies of multimodal semantic elements like words and images. Furthermore, the ordinal information of images, which indicates the relevance between visual context and users' demands, remains underutilized during the integration of visual content. Additionally, the exploration of how to effectively utilize corresponding attributes provided by users when searching for desired products is still largely unexplored. To address these challenges, we propose PMATE, a Position-aware Multimodal diAlogue system with semanTic Elements. Specifically, to obtain semantic representations at the element level, we first unfold the multimodal historical utterances and devise a position-aware multimodal element-level encoder. This component considers all images that may be relevant to the current turn and introduces a novel position-aware image selector to choose related images before fusing the information from the two modalities. Finally, we present a knowledge-aware two-stage decoder and an attribute-enhanced image searcher for the tasks of generating textual responses and selecting image responses, respectively. We extensively evaluate our model on two large-scale multimodal dialogue datasets, and the results of our experiments demonstrate that our approach outperforms several baseline methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",conversational image search; Multimodal dialogue system; natural language generation,Image enhancement; Large datasets; Natural language processing systems; Search engines; Semantic Web; Speech processing; Context-Aware; Conversational image search; Dependency informations; Element level; Image search; Multi-modal; Multimodal dialogue systems; Natural language generation; Ordinal information; Semantic element; Semantics
Reinforcement Learning for Solving Multiple Vehicle Routing Problem with Time Window,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189853964&doi=10.1145%2f3625232&partnerID=40&md5=ce1ca178506487e6dcc95591f3c3d58a,"Vehicle routing problem with time window (VRPTW) is of great importance for a wide spectrum of services and real-life applications, such as online take-out and car-hailing platforms. A promising method should generate high-qualified solutions within limited inference time, and there are three major challenges: (a) directly optimizing the goal with several practical constraints; (b) efficiently handling individual time-window limits; and (c) modeling the cooperation among the vehicle fleet. In this article, we present an end-to-end reinforcement learning framework to solve VRPTW. First, we propose an agent model that encodes constraints into features as the input and conducts harsh policy on the output when generating deterministic results. Second, we design a time penalty augmented reward to model the time-window limits during gradient propagation. Third, we design a task handler to enable the cooperation among different vehicles. We perform extensive experiments on two real-world datasets and one public benchmark dataset. Results demonstrate that our solution improves the performance by up to 11.7% compared to other RL baselines and could generate solutions for instances within seconds, while existing heuristic baselines take for minutes as well as maintain the quality of solutions. Moreover, our solution is thoroughly analyzed with meaningful implications due to the real-time response ability. © 2024 Copyright held by the owner/author(s).","Vehicle routing problem with time window, reinforcement learning","Fleet operations; Learning systems; Vehicle routing; Vehicles; C modeling; C-models; End to end; Real-life applications; Reinforcement learnings; Time windows; Vehicle fleets; Vehicle routing problem with time window, reinforcement learning; Vehicle routing problem with time windows; Wide spectrum; Reinforcement learning"
Temporal Implicit Multimodal Networks for Investment and Risk Management,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189863400&doi=10.1145%2f3643855&partnerID=40&md5=45e3d71d9b419c16c49ad6f15a2d59a3,"Many deep learning works on financial time-series forecasting focus on predicting future prices/returns of individual assets with numerical price-related information for trading, and hence propose models designed for univariate, single-task, and/or unimodal settings. Forecasting for investment and risk management involves multiple tasks in multivariate settings: forecasts of expected returns and risks of assets in portfolios, and correlations between these assets. As different sources/types of time-series influence future returns, risks, and correlations of assets in different ways, it is also important to capture time-series from different modalities. Hence, this article addresses financial time-series forecasting for investment and risk management in a multivariate, multitask, and multimodal setting. Financial time-series forecasting, however, is challenging due to the low signal-to-noise ratios typical in financial time-series, and as intra-series and inter-series relationships of assets evolve across time. To address these challenges, our proposed Temporal Implicit Multimodal Network (TIME) model learns implicit inter-series relationship networks between assets from multimodal financial time-series at multiple time-steps adaptively. TIME then uses dynamic network and temporal encoding modules to jointly capture such evolving relationships, multimodal financial time-series, and temporal representations. Our experiments show that TIME outperforms other state-of-the-art models on multiple forecasting tasks and investment and risk management applications. © 2024 Copyright held by the owner/author(s).",finance; forecasting; graph neural networks; graphs; multi-modality; Time-series,Deep learning; Financial data processing; Graph neural networks; Investments; Risk management; Signal to noise ratio; Time series; Financial time series; Financial time series forecasting; Graph; Graph neural networks; Investment management; Multi-modal; Multi-modality; Multimodal network; Risks management; Times series; Forecasting
A Survey on Evaluation of Large Language Models,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195261457&doi=10.1145%2f3641289&partnerID=40&md5=0745e5131ecf75dd4c60d2028b0850ca,"Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the 'where' and 'how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLarge language models; benchmark; evaluation; model assessment,Computational linguistics; Natural language processing systems; Additional key word and phraseslarge language model; Benchmark; Evaluation; Evaluation methods; Key words; Language model; Model assessment; Model evaluation; Performance; Research use; Benchmarking
Internal Rehearsals for a Reconfigurable Robot to Improve Area Coverage Performance,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195034356&doi=10.1145%2f3643854&partnerID=40&md5=080aa1f26e091bf802624bff4f94dccb,"Reconfigurable robots are deployed for applications demanding area coverage, such as cleaning and inspections. Reconfiguration per context, considering beyond a small set of predefined shapes, is crucial for area coverage performance. However, the existing area coverage methods of reconfigurable robots are not always effective and require improvements for ascertaining the intended goal. Therefore, this article proposes a novel coverage strategy based on internal rehearsals to improve the area coverage performance of a reconfigurable robot. In this regard, a reconfigurable robot is embodied with the cognitive ability to predict the outcomes of its actions before executing them. A genetic algorithm uses the results of the internal rehearsals to determine a set of the robot's coverage parameters, including positioning, heading, and reconfiguration, to maximize coverage in an obstacle cluster encountered by the robot. The experimental results confirm that the proposed method can significantly improve the area coverage performance of a reconfigurable robot.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",area coverage; context-aware reconfiguration; Internal rehearsals; reconfigurable robots,Modular robots; Area coverages; Cognitive ability; Context-Aware; Context-aware reconfiguration; Coverage strategies; Internal rehearsal; Performance; Reconfigurable robot; Robot coverages; Genetic algorithms
Credit Card Fraud Detection via Intelligent Sampling and Self-supervised Learning,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189864744&doi=10.1145%2f3641283&partnerID=40&md5=7088278deb14df54005be9c4355edde0,"The significant increase in credit card transactions can be attributed to the rapid growth of online shopping and digital payments, particularly during the COVID-19 pandemic. To safeguard cardholders, e-commerce companies, and financial institutions, the implementation of an effective and real-time fraud detection method using modern artificial intelligence techniques is imperative. However, the development of machine-learning-based approaches for fraud detection faces challenges such as inadequate transaction representation, noise labels, and data imbalance. Additionally, practical considerations like dynamic thresholds, concept drift, and verification latency need to be appropriately addressed. In this study, we designed a fraud detection method that accurately extracts a series of spatial and temporal representative features to precisely describe credit card transactions. Furthermore, several auxiliary self-supervised objectives were developed to model cardholders’ behavior sequences. By employing intelligent sampling strategies, potential noise labels were eliminated, thereby reducing the level of data imbalance. The developed method encompasses various innovative functions that cater to practical usage requirements. We applied this method to two real-world datasets, and the results indicated a higher F1 score compared to the most commonly used online fraud detection methods. © 2024 Copyright held by the owner/author(s).",credit card fraud detection; discriminative representation; feature engineering; intelligent sampling; Self-supervised learning,COVID-19; Crime; Electronic commerce; Feature extraction; Credit card fraud detections; Credit card transactions; Data imbalance; Detection methods; Discriminative representation; Feature engineerings; Fraud detection; Intelligent samplings; Rapid growth; Self-supervised learning; Supervised learning
Optimal Treatment Strategies for Critical Patients with Deep Reinforcement Learning,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189854635&doi=10.1145%2f3643856&partnerID=40&md5=f8154cb0759214e0ee289bb65d89bcce,"Personalized clinical decision support systems are increasingly being adopted due to the emergence of data-driven technologies, with this approach now gaining recognition in critical care. The task of incorporating diverse patient conditions and treatment procedures into critical care decision-making can be challenging due to the heterogeneous nature of medical data. Advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL) techniques, enables the development of personalized treatment strategies for severe illnesses by using a learning agent to recommend optimal policies. In this study, we propose a Deep Reinforcement Learning (DRL) model with a tailored reward function and an LSTM-GRU-derived state representation to formulate optimal treatment policies for vasopressor administration in stabilizing patient physiological states in critical care settings. Using an ICU dataset and the Medical Information Mart for Intensive Care (MIMIC-III) dataset, we focus on patients with Acute Respiratory Distress Syndrome (ARDS) that has led to Sepsis, to derive optimal policies that can prioritize patient recovery over patient survival. Both the DDQN (RepDRL-DDQN) and Dueling DDQN (RepDRL-DDDQN) versions of the DRL model surpass the baseline performance, with the proposed model’s learning agent achieving an optimal learning process across our performance measuring schemes. The robust state representation served as the foundation for enhancing the model’s performance, ultimately providing an optimal treatment policy focused on rapid patient recovery. © 2024 Copyright held by the owner/author(s).",DDDQN; DDQN; Deep reinforcement learning; DQN; Q networks; Q-learning; treatment strategies,Decision making; Decision support systems; Intensive care units; Learning systems; Long short-term memory; Patient rehabilitation; Patient treatment; Physiological models; Critical care; DDDQN; Deep reinforcement learning; DQN; Optimal treatment; Q network; Q-learning; Reinforcement learnings; RepDRL-DDDQN; Treatment strategy; Reinforcement learning
Guidelines for the Regularization of Gammas in Batch Normalization for Deep Residual Networks,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195042500&doi=10.1145%2f3643860&partnerID=40&md5=6dc3c02e01633a20d2c0c6e7027420b2,"L2 regularization for weights in neural networks is widely used as a standard training trick. In addition to weights, the use of batch normalization involves an additional trainable parameter 3, which acts as a scaling factor. However, L2 regularization for γremains an undiscussed mystery and is applied in different ways depending on the library and practitioner. In this article, we study whether L2 regularization for γis valid. To explore this issue, we consider two approaches: (1) variance control to make the residual network behave like an identity mapping and (2) stable optimization through the improvement of effective learning rate. Through two analyses, we specify the desirable and undesirable γto apply L2 regularization and propose four guidelines for managing them. In several experiments, we observed that applying L2 regularization to applicable γincreased 1% to 4% classification accuracy, whereas applying L2 regularization to inapplicable γdecreased 1% to 3% classification accuracy, which is consistent with our four guidelines. Our proposed guidelines were further validated through various tasks and architectures, including variants of residual networks and transformers.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",batch normalization; deep learning; effective learning rate; L2 regularization; residual network; weight decay,Learning algorithms; Batch normalization; Deep learning; Effective learning; Effective learning rate; L2 regularization; Learning rates; Normalisation; Regularisation; Residual network; Weight decay; Deep learning
VesNet: A Vessel Network for Jointly Learning Route Pattern and Future Trajectory,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189864354&doi=10.1145%2f3639370&partnerID=40&md5=a71dc60c3ca156e97aad4b7a36a86ceb,"Vessel trajectory prediction is the key to maritime applications such as traffic surveillance, collision avoidance, anomaly detection, and so on. Making predictions more precisely requires a better understanding of the moving trend for a particular vessel since the movement is affected by multiple factors like marine environment, vessel type, and vessel behavior. In this paper, we propose a model named VesNet, based on the attentional seq2seq framework, to predict vessel future movement sequence by observing the current trajectory. Firstly, we extract the route patterns from the raw AIS data during preprocessing. Then, we design a multi-task learning structure to learn how to implement route pattern classification and vessel trajectory prediction simultaneously. By comparing with representative baseline models, we find that our VesNet has the best performance in terms of long-term prediction precision. Additionally, VesNet can recognize the route pattern by capturing the implicit moving characteristics. The experimental results prove that the proposed multi-task learning assists the vessel trajectory prediction mission. © 2024 Copyright held by the owner/author(s).",multi-task learning; trajectory clustering; Vessel trajectory prediction,Anomaly detection; Forecasting; Learning systems; Anomaly detection; Collisions avoidance; Multiple factors; Multitask learning; Traffic surveillance; Trajectory clustering; Trajectory prediction; Vessel networks; Vessel trajectory; Vessel trajectory prediction; Trajectories
Evolving Knowledge Graph Representation Learning with Multiple Attention Strategies for Citation Recommendation System,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184404962&doi=10.1145%2f3635273&partnerID=40&md5=d5482a6d1725d81f7a27abd6b0481b2e,"The growing number of publications in the field of artificial intelligence highlights the need for researchers to enhance their efficiency in searching for relevant articles. Most paper recommendation models either rely on simplistic citation relationships among papers or focus on content-based approaches, both of which overlook interactions within academic networks. To address the aforementioned problem, knowledge graph embedding (KGE) methods have been used for citation recommendations because recent research proves that graph representations can effectively improve recommendation model accuracy. However, academic networks are dynamic, leading to changes in the representations of users and items over time. The majority of KGE-based citation recommendations are primarily designed for static graphs, thus failing to capture the evolution of dynamic knowledge graph (DKG) structures. To address these challenges, we introduced the evolving knowledge graph embedding (EKGE) method. In this methodology, evolving knowledge graphs are input into time-series models to learn the patterns of structural evolution. The model has the capability to generate embeddings for each entity at various time points, thereby overcoming limitation of static models that require retraining to acquire embeddings at each specific time point. To enhance the efficiency of feature extraction, we employed a multiple attention strategy. This helped the model find recommendation lists that are closely related to a user’s needs, leading to improved recommendation accuracy. Various experiments conducted on a citation recommendation dataset revealed that the EKGE model exhibits a 1.13% increase in prediction accuracy compared to other KGE methods. Moreover, the model’s accuracy can be further increased by an additional 0.84% through the incorporation of an attention mechanism. © 2024 Copyright held by the owner/author(s).",,Efficiency; Graph embeddings; Recommender systems; Content-based approach; Embedding method; Embeddings; Graph embeddings; Graph representation; Knowledge graphs; Modeling accuracy; Paper recommendations; Recent researches; Time points; Knowledge graph
SiG: A Siamese-Based Graph Convolutional Network to Align Knowledge in Autonomous Transportation Systems,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189857958&doi=10.1145%2f3643861&partnerID=40&md5=16b137b204eacfe7e7b356de296d0d9a,"Domain knowledge is gradually renovating its attributes to exhibit distinct features in autonomy, propelled by the shift of modern transportation systems (TS) toward autonomous TS (ATS) comprising three progressive generations. The knowledge graph (KG) and its corresponding versions can help depict the evolving TS. Given that KG versions exhibit asymmetry primarily due to variations in evolved knowledge, it is imperative to harmonize the evolved knowledge embodied by the entity across disparate KG versions. Hence, this article proposes a siamese-based graph convolutional network (GCN) model, namely SiG, to address unresolved issues of low accuracy, efficiency, and effectiveness in aligning asymmetric KGs. SiG can optimize entity alignment in ATS and support the analysis of future-stage ATS development. Such a goal is attained through (a) generating unified KGs to enhance data quality, (b) defining graph split to facilitate entire-graph computation, (c) enhancing a GCN to extract intrinsic features, and (d) designing a siamese network to train asymmetric KGs. The evaluation results suggest that SiG surpasses other commonly employed models, resulting in average improvements of 23.90% and 37.89% in accuracy and efficiency, respectively. These findings have significant implications for TS evolution analysis and offer a novel perspective for research on complex systems limited by continuously updated knowledge. © 2024 Copyright held by the owner/author(s).",asymmetric knowledge graph; Autonomous transportation systems; entity alignment; evolution analysis; graph convolutional network; siamese network,Convolution; Domain Knowledge; Efficiency; Asymmetric knowledge; Asymmetric knowledge graph; Autonomous transportation system; Convolutional networks; Entity alignment; Evolution analysis; Graph convolutional network; Knowledge graphs; Siamese network; Transportation system; Knowledge graph
Bayesian Strategy Networks Based Soft Actor-Critic Learning,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195074075&doi=10.1145%2f3643862&partnerID=40&md5=b6dc9764ef00dbb5e7294773975d5b25,"A strategy refers to the rules that the agent chooses the available actions to achieve goals. Adopting reasonable strategies is challenging but crucial for an intelligent agent with limited resources working in hazardous, unstructured, and dynamic environments to improve the system's utility, decrease the overall cost, and increase mission success probability. This article proposes a novel hierarchical strategy decomposition approach based on Bayesian chaining to separate an intricate policy into several simple sub-policies and organize their relationships as Bayesian strategy networks (BSN). We integrate this approach into the state-of-the-art DRL method - soft actor-critic (SAC), and build the corresponding Bayesian soft actor-critic (BSAC) model by organizing several sub-policies as a joint policy. Our method achieves the state-of-the-art performance on the standard continuous control benchmarks in the OpenAI Gym environment. The results demonstrate that the promising potential of the BSAC method significantly improves training efficiency. Furthermore, we extend the topic to the Multi-Agent systems (MAS), discussing the potential research fields and directions.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bayesian networks; deep reinforcement learning; expectation; soft actor-critic; Strategy; utility,Bayesian networks; Benchmarking; Deep learning; Intelligent agents; Multi agent systems; Actor critic; Bayesia n networks; Bayesian; Deep reinforcement learning; Expectation; Network-based; Reinforcement learnings; Soft actor-critic; Strategy; Utility; Reinforcement learning
Deep Learning in Single-cell Analysis,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195088637&doi=10.1145%2f3641284&partnerID=40&md5=d78777d971b59d6fcacfed0879f4e68c,"Single-cell technologies are revolutionizing the entire field of biology. The large volumes of data generated by single-cell technologies are high dimensional, sparse, and heterogeneous and have complicated dependency structures, making analyses using conventional machine learning approaches challenging and impractical. In tackling these challenges, deep learning often demonstrates superior performance compared to traditional machine learning methods. In this work, we give a comprehensive survey on deep learning in single-cell analysis. We first introduce background on single-cell technologies and their development, as well as fundamental concepts of deep learning including the most popular deep architectures. We present an overview of the single-cell analytic pipeline pursued in research applications while noting divergences due to data sources or specific applications. We then review seven popular tasks spanning different stages of the single-cell analysis pipeline, including multimodal integration, imputation, clustering, spatial domain identification, cell-type deconvolution, cell segmentation, and cell-type annotation. Under each task, we describe the most recent developments in classical and deep learning methods and discuss their advantages and disadvantages. Deep learning tools and benchmark datasets are also summarized for each task. Finally, we discuss the future directions and the most recent challenges. This survey will serve as a reference for biologists and computer scientists, encouraging collaborations.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cell segmentation; cell-type annotation; cell-type deconvolution; clustering; Deep learning; imputation; multimodal integration; single-cell Analysis; spatial domain identification,Cells; Cytology; Deep learning; Learning systems; Modal analysis; Cell segmentation; Cell types; Cell-type annotation; Cell-type deconvolution; Clusterings; Deconvolutions; Deep learning; Domain identification; Imputation; Multimodal integration; Single cells analysis; Spatial domain identification; Spatial domains; Type annotations; Pipelines
MGRR-Net: Multi-level Graph Relational Reasoning Network for Facial Action Unit Detection,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195037888&doi=10.1145%2f3643863&partnerID=40&md5=84e8157e74395178df9fc6b00e67219f,"The Facial Action Coding System (FACS) encodes the action units (AUs) in facial images, which has attracted extensive research attention due to its wide use in facial expression analysis. Many methods that perform well on automatic facial action unit (AU) detection primarily focus on modeling various AU relations between corresponding local muscle areas or mining global attention-aware facial features; however, they neglect the dynamic interactions among local-global features. We argue that encoding AU features just from one perspective may not capture the rich contextual information between regional and global face features, as well as the detailed variability across AUs, because of the diversity in expression and individual characteristics. In this article, we propose a novel Multi-level Graph Relational Reasoning Network (termed MGRR-Net) for facial AU detection. Each layer of MGRR-Net performs a multi-level (i.e., region-level, pixel-wise, and channel-wise level) feature learning. On the one hand, the region-level feature learning from the local face patch features via graph neural network can encode the correlation across different AUs. On the other hand, pixel-wise and channel-wise feature learning via graph attention networks (GAT) enhance the discrimination ability of AU features by adaptively recalibrating feature responses of pixels and channels from global face features. The hierarchical fusion strategy combines features from the three levels with gated fusion cells to improve AU discriminative ability. Extensive experiments on DISFA and BP4D AU datasets show that the proposed approach achieves superior performance than the state-of-the-art methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Facial action units; graph attention network; local-global interaction; multi-level relational reasoning,Face recognition; Graph neural networks; Machine learning; Pixels; Signal encoding; Action Unit; Facial action; Facial action unit; Feature learning; Global interaction; Graph attention network; Local-global interaction; Multi-level relational reasoning; Multilevels; Relational reasoning; Encoding (symbols)
Generating Daily Activities with Need Dynamics,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189862284&doi=10.1145%2f3637493&partnerID=40&md5=1927ca61ccbf22e83669e8d23cc4fbd1,"Daily activity data recording individuals’ various activities in daily life are widely used in many applications such as activity scheduling, activity recommendation, and policymaking. Though with high value, its accessibility is limited due to high collection costs and potential privacy issues. Therefore, simulating human activities to produce massive high-quality data is of great importance. However, existing solutions, including rule-based methods with simplified behavior assumptions and data-driven methods directly fitting real-world data, both cannot fully qualify for matching reality. In this article, motivated by the classic psychological theory, Maslow’s need theory describing human motivation, we propose a knowledge-driven simulation framework based on generative adversarial imitation learning. Our core idea is to model the evolution of human needs as the underlying mechanism that drives activity generation in the simulation model. Specifically, a hierarchical model structure that disentangles different need levels and the use of neural stochastic differential equations successfully capture the piecewise-continuous characteristics of need dynamics. Extensive experiments demonstrate that our framework outperforms the state-of-the-art baselines regarding data fidelity and utility. We also present the insightful interpretability of the need modeling. Moreover, privacy preservation evaluations validate that the generated data does not leak individual privacy. The code is available at https://github.com/tsinghua-fib-lab/Activity-Simulation-SAND. © 2024 Copyright held by the owner/author(s).",Daily activities; GAIL; generation; need dynamics,Differential equations; Digital storage; Hierarchical systems; Stochastic models; Stochastic systems; Activity scheduling; Daily activity; Daily lives; GAIL; Generation; High quality data; Human activities; Need dynamic; Policy making; Privacy issue; Dynamics
Labeling Chaos to Learning Harmony: Federated Learning with Noisy Labels,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189855352&doi=10.1145%2f3626242&partnerID=40&md5=44ffc86072290d05db3f327c6459367b,"Federated Learning (FL) is a distributed machine learning paradigm that enables learning models from decentralized private datasets where the labeling effort is entrusted to the clients. While most existing FL approaches assume high-quality labels are readily available on users’ devices, in reality, label noise can naturally occur in FL and is closely related to clients’ characteristics. Due to scarcity of available data and significant label noise variations among clients in FL, existing state-of-the-art centralized approaches exhibit unsatisfactory performance, whereas prior FL studies rely on excessive on-device computational schemes or additional clean data available on the server. We propose FedLN, a framework to deal with label noise across different FL training stages, namely FL initialization, on-device model training, and server model aggregation, able to accommodate the diverse computational capabilities of devices in an FL system. Specifically, FedLN computes per-client noise level estimation in a single federated round and improves the models’ performance by either correcting or mitigating the effect of noisy samples. Our evaluation on various publicly available vision and audio datasets demonstrates a 22% improvement on average compared to other existing methods for a label noise level of 60%. We further validate the efficiency of FedLN in human-annotated real-world noisy datasets and report a 4.8% increase on average in models’ recognition performance, highlighting that FedLN can be useful for improving FL services provided to everyday users. © 2024 Copyright held by the owner/author(s).",deep learning; Federated learning; knowledge distillation; label correction; noisy labels,Audio acoustics; Deep learning; Learning systems; Deep learning; Distributed machine learning; Federated learning; Knowledge distillation; Label correction; Labelings; Learning models; Learning paradigms; Noisy labels; Performance; Distillation
Explainable Product Classification for Customs,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189860944&doi=10.1145%2f3635158&partnerID=40&md5=c6037ec40b9f1a6564efb6c3afea7c29,"The task of assigning internationally accepted commodity codes (aka HS codes) to traded goods is a critical function of customs offices. Like court decisions made by judges, this task follows the doctrine of precedent and can be nontrivial even for experienced officers. Together with the Korea Customs Service (KCS), we propose a first-ever explainable decision supporting model that suggests the most likely subheadings (i.e., the first six digits) of the HS code. The model also provides reasoning for its suggestion in the form of a document that is interpretable by customs officers. We evaluated the model using 5,000 cases that recently received a classification request. The results showed that the top-3 suggestions made by our model had an accuracy of 93.9% when classifying 925 challenging subheadings. A user study with 32 customs experts further confirmed that our algorithmic suggestions accompanied by explainable reasonings, can substantially reduce the time and effort taken by customs officers for classification reviews. © 2024 Copyright held by the owner/author(s).",decision support; human-centered explainable AI; interpretability; Product classification,Artificial intelligence; Court decisions; Critical functions; Custom service; Decision supporting; Decision supports; Human-centered explainable AI; Interpretability; Most likely; Product classification; User study; Decision support systems
Demand-driven Urban Facility Visit Prediction,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189856694&doi=10.1145%2f3625233&partnerID=40&md5=eba256546f4f3e3405baf46e45166d10,"Predicting citizens’ visiting behaviors to urban facilities is instrumental for city governors and planners to detect inequalities in urban opportunities and optimize the distribution of facilities and resources. Previous works predict facility visits simply using observed visit behavior, yet citizens’ intrinsic demands for facilities are not characterized explicitly, causing potential incorrect learned relations in the prediction results. In this article, to make up for this deficiency, we present a demand-driven urban facility visit prediction method that decomposes citizens’ visits to facilities into their unobservable demands and their capability to fulfill them. Demands are expressed as the function of regional demographic attributes by a neural network, and the fulfillment capability is determined by the urban region’s spatial accessibility to facilities. Extensive evaluations of datasets of three large cities confirm the efficiency and rationality of our model. Our method outperforms the best state-of-the-art model by 8.28% on average in facility visit prediction tasks. Further analyses demonstrate the reasonableness of recovered facility demands and their relationship with citizen demographics. For instance, senior citizens tend to have higher medical demands but lower shopping demands. Meanwhile, estimated capabilities and accessibilities provide deeper insights into the decaying accessibility with respect to spatial distance and facilities’ diverse functions in the urban environment. Our findings shed light on demand-driven urban data mining and demand-based urban facility planning. © 2024 Copyright held by the owner/author(s).",accessibility; data mining; demand modeling; Urban facility; visit prediction,Data mining; Large datasets; Population statistics; Urban planning; Accessibility; Demand modelling; Demand-driven; Large cities; Neural-networks; Prediction methods; Unobservable; Urban facility; Urban regions; Visit prediction; Forecasting
EMG-Based Automatic Gesture Recognition Using Lipschitz-Regularized Neural Networks,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189858197&doi=10.1145%2f3635159&partnerID=40&md5=59d8774f42afbdf5e3840a64dff63bd9,"This article introduces a novel approach for building a robust Automatic Gesture Recognition system based on Surface Electromyographic (sEMG) signals, acquired at the forearm level. Our main contribution is to propose new constrained learning strategies that ensure robustness against adversarial perturbations by controlling the Lipschitz constant of the classifier. We focus on nonnegative neural networks for which accurate Lipschitz bounds can be derived, and we propose different spectral norm constraints offering robustness guarantees from a theoretical viewpoint. Experimental results on four publicly available datasets highlight that a good tradeoff in terms of accuracy and performance is achieved. We then demonstrate the robustness of our models, compared with standard trained classifiers in four scenarios, considering both white-box and black-box attacks. © 2024 Copyright held by the owner/author(s).",EMG; Lipschitz regularity optimization; perturbations; Recognition; stability,Gesture recognition; EMG; Gesture recognition system; Gestures recognition; Lipschitz; Lipschitz regularity; Lipschitz regularity optimization; Optimisations; Perturbation; Recognition; Regularized neural networks; Learning systems
RANGO: A Novel Deep Learning Approach to Detect Drones Disguising from Video Surveillance Systems,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189859012&doi=10.1145%2f3641282&partnerID=40&md5=f86eb5744951f6e32bc40896fcfcf1db,"Video surveillance systems provide means to detect the presence of potentially malicious drones in the surroundings of critical infrastructures. In particular, these systems collect images and feed them to a deep-learning classifier able to detect the presence of a drone in the input image. However, current classifiers are not efficient in identifying drones that disguise themselves with the image background, e.g., hiding in front of a tree. Furthermore, video-based detection systems heavily rely on the image’s brightness, where darkness imposes significant challenges in detecting drones. Both these phenomena increase the possibilities for attackers to get close to critical infrastructures without being spotted and hence be able to gather sensitive information or cause physical damages, possibly leading to safety threats. In this article, we propose RANGO, a drone detection arithmetic able to detect drones in challenging images where the target is difficult to distinguish from the background. RANGO is based on a deep learning architecture that exploits a Preconditioning Operation (PREP) that highlights the target by the difference between the target gradient and the background gradient. The idea is to highlight features that will be useful for classification. After PREP, RANGO uses multiple convolution kernels to make the final decision on the presence of the drone. We test RANGO on a drone image dataset composed of multiple already-existing datasets to which we add samples of birds and planes. We then compare RANGO with multiple currently existing approaches to show its superiority. When tested on images with disguising drones, RANGO attains an increase of 6.6% mean Average Precision (mAP) compared to YOLOv5 solution. When tested on the conventional dataset, RANGO improves the mAP by approximately 2.2%, thus confirming its effectiveness also in the general scenario. © 2024 Copyright held by the owner/author(s).",background fusion; deep learning; Drone detection; YOLO,Aircraft detection; Critical infrastructures; Deep learning; Public works; Security systems; Statistical tests; 'current; Background fusion; Deep learning; Drone detection; Images background; Input image; Learning approach; Learning classifiers; Video surveillance systems; YOLO; Drones
Explainability for Large Language Models: A Survey,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187025678&doi=10.1145%2f3639372&partnerID=40&md5=3851f08e8b86464c77cda4b88bcae1b6,"Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models. © 2024 Copyright held by the owner/author(s).",Explainability; interpretability; large language models,Computational linguistics; Natural language processing systems; Downstream applications; Explainability; Fine tuning; Individual prediction; Interpretability; Language model; Language processing; Large language model; Natural languages; Social impact; Deep learning
Strengthening Cooperative Consensus in Multi-Robot Confrontation,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189863479&doi=10.1145%2f3639371&partnerID=40&md5=f667bfa4e0e7152839990f803bbdf723,"Multi-agent reinforcement learning (MARL) has proven effective in training multi-robot confrontation, such as StarCraft and robot soccer games. However, the current joint action policies utilized in MARL have been unsuccessful in recognizing and preventing actions that often lead to failures on our side. This exacerbates the cooperation dilemma, ultimately resulting in our agents acting independently and being defeated individually by their opponents. To tackle this challenge, we propose a novel joint action policy, referred to as the consensus action policy (CAP). Specifically, CAP records the number of times each joint action has caused our side to fail in the past and computes a cooperation tendency, which is integrated with each agent’s Q-value and Nash bargaining solution to determine a joint action. The cooperation tendency promotes team cooperation by selecting joint actions that have a high tendency of cooperation and avoiding actions that may lead to team failure. Moreover, the proposed CAP policy can be extended to partially observable scenarios by combining it with Deep Q network or actor-critic–based methods. We conducted extensive experiments to compare the proposed method with seven existing joint action policies, including four commonly used methods and three state-of-the-art methods, in terms of episode rewards, winning rates, and other metrics. Our results demonstrate that this approach holds great promise for multi-robot confrontation scenarios. © 2024 Copyright held by the owner/author(s).",consensus action policy; cooperation dilemma; multi-agent reinforcement learning; Multi-robot confrontation,Fertilizers; Industrial robots; Multi agent systems; Multipurpose robots; 'current; Action policies; Consensus action policy; Cooperation dilemma; Joint actions; Multi-agent reinforcement learning; Multi-robot confrontation; Multirobots; Q-values; Robot soccer game; Reinforcement learning
TS-Fastformer: Fast Transformer for Time-series Forecasting,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189862083&doi=10.1145%2f3630637&partnerID=40&md5=70ea40bdd08ae0a3288a4fd0eb60729a,"Many real-world applications require precise and fast time-series forecasting. Recent trends in time-series forecasting models are shifting from LSTM-based models to Transformer-based models. However, the Transformer-based model has a limited ability to represent sequential relationships in time-series data. In addition, the transformer-based model suffers from slow training and inference speed due to the bottleneck incurred by a deep encoder and step-by-step decoder inference. To address these problems, we propose a time-series forecasting optimized Transformer model, called TS-Fastformer. TS-Fastformer introduces three new optimizations: First, we propose a Sub Window Tokenizer for compressing input in a simple manner. The Sub Window Tokenizer reduces the length of input sequences to mitigate the complexity of self-attention and enables both single and multi-sequence learning. Second, we propose Time-series Pre-trained Encoder to extract effective representations through pre-training. This optimization enables TS-Fastformer to capture both seasonal and trend representations as well as to mitigate bottlenecks of conventional transformer models. Third, we propose the Past Attention Decoder to forecast target by incorporating past long short-term dependency patterns. Furthermore, Past Attention Decoder achieves high performance improvement by removing a trend distribution that changes over a long period. We evaluate the efficiency of our model with extensive experiments using seven real-world datasets and compare our model to six representative time-series forecasting approaches. The results show that the proposed TS-Fastformer reduces MSE by 10.1% compared to state-of-the-art model and demonstrates 21.6% faster training time compared to the existing fastest transformer, respectively. © 2024 Copyright held by the owner/author(s).",Deep learning; time-series forecasting; time-series representation; transformer,Data mining; Decoding; Forecasting; Long short-term memory; Signal encoding; Deep learning; Optimisations; Real-world; Series representations; Time series forecasting; Time-series representation; Times series; Tokenizer; Transformer; Transformer modeling; Time series
Fairness-Driven Private Collaborative Machine Learning,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189860182&doi=10.1145%2f3639368&partnerID=40&md5=3a376a0165d7310641c0d40eb7ac3453,"The performance of machine learning algorithms can be considerably improved when trained over larger datasets. In many domains, such as medicine and finance, larger datasets can be obtained if several parties, each having access to limited amounts of data, collaborate and share their data. However, such data sharing introduces significant privacy challenges. While multiple recent studies have investigated methods for private collaborative machine learning, the fairness of such collaborative algorithms has been overlooked. In this work, we suggest a feasible privacy-preserving pre-process mechanism for enhancing fairness of collaborative machine learning algorithms. An extensive evaluation of the proposed method shows that it is able to enhance fairness considerably with only a minor compromise in accuracy. © 2024 Copyright held by the owner/author(s).",algorithmic fairness; collaborative machine learning; federated learning; Privacy; secure multi-party computation,Learning algorithms; Privacy-preserving techniques; Algorithmic fairness; Algorithmics; Collaborative machine learning; Federated learning; Large datasets; Machine learning algorithms; Machine-learning; Performance; Privacy; Secure multi-party computation; Machine learning
T-Distributed Stochastic Neighbor Embedding for Co-Representation Learning,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189857600&doi=10.1145%2f3627823&partnerID=40&md5=16aef97425451af5dda7b9afab670261,"Co-clustering is the simultaneous clustering of the samples and attributes of a data matrix that provides deeper insight into data than traditional clustering. However, there is a lack of representation learning algorithms that serve this mechanism of co-clustering, and the current representation learning algorithms are limited to the sample perspective and lack the use of information in the attribute perspective. To solve this problem, in this article, ctSNE, a co-representation learning model based on t-distributed stochastic neighbor embedding, is proposed for unsupervised co-clustering, where ctSNE makes the dataset representation outputted more discriminative of row and column clusters (i.e. co-discrimination). On the basis of t-distributed stochastic neighbor embedding retaining the sample data distribution and local data structure, the philosophy of collaboration is introduced (i.e., row and column hidden relationship information) so that the ctSNE model is equipped with co-representation learning capability, which can effectively improve the performance of co-clustering. To prove the effectiveness of the ctSNE model, several classic co-clustering algorithms are used to check the co-representation performance of ctSNE, and a novel internal index based on an internal clustering index, known as total inertia, is proposed to demonstrate the effect of co-clustering. The numerous experimental results show that ctSNE has tremendous co-representation capability and can significantly improve the performance of co-clustering algorithms. © 2024 Copyright held by the owner/author(s).",Co-clustering; co-representation; t-distribution stochastic neighbor embedding; unsupervised learning,Data mining; Embeddings; Information systems; Information use; Stochastic models; Stochastic systems; Unsupervised learning; 'current; Co-clustering; Co-representation; Data matrix; Performance; Stochastic neighbor embedding; T distribution; T-distribution stochastic neighbor embedding; Traditional clustering; Clustering algorithms
Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188259335&doi=10.1145%2f3639369&partnerID=40&md5=74e794bb629b7acf233db87b06afc649,"Federated learning (FL) is a privacy-preserving machine learning paradigm in which the server periodically aggregates local model parameters from cli ents without assembling their private data. Constrained communication and personalization requirements pose severe challenges to FL. Federated distillation (FD) is proposed to simultaneously address the above two problems, which exchanges knowledge between the server and clients, supporting heterogeneous local models while significantly reducing communication overhead. However, most existing FD methods require a proxy dataset, which is often unavailable in reality. A few recent proxy-data-free FD approaches can eliminate the need for additional public data, but suffer from remarkable discrepancy among local knowledge due to client-side model heterogeneity, leading to ambiguous representation on the server and inevitable accuracy degradation. To tackle this issue, we propose a proxy-data-free FD algorithm based on distributed knowledge congruence (FedDKC). FedDKC leverages well-designed refinement strategies to narrow local knowledge differences into an acceptable upper bound, so as to mitigate the negative effects of knowledge incongruence. Specifically, from perspectives of peak probability and Shannon entropy of local knowledge, we design kernel-based knowledge refinement (KKR) and searching-based knowledge refinement (SKR) respectively, and theoretically guarantee that the refined-local knowledge can satisfy an approximately-similar distribution and be regarded as congruent. Extensive experiments conducted on three common datasets demonstrate that our proposed FedDKC significantly outperforms the state-of-the-art on various heterogeneous settings while evidently improving the convergence speed. © 2024 Copyright held by the owner/author(s).",Federated learning; knowledge distillation; model heterogeneity; proxy-data-free,Finite difference method; Knowledge management; Learning systems; Privacy-preserving techniques; Probability distributions; Distributed knowledge; Federated learning; Knowledge distillation; Knowledge refinement; Local knowledge; Local model; Model heterogeneity; Privacy preserving; Proxy data; Proxy-data-free; Distillation
Nationwide Air Pollution Forecasting with Heterogeneous Graph Neural Networks,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183330941&doi=10.1145%2f3637492&partnerID=40&md5=f3a1e9ba6749a2992d46310f04555009,"Nowadays, air pollution is one of the most relevant environmental problems in most urban settings. Due to the utility in operational terms of anticipating certain pollution levels, several predictors based on Graph Neural Networks (GNN) have been proposed for the last years. Most of these solutions usually encode the relationships among stations in terms of their spatial distance, but they fail when it comes to capturing other spatial and feature-based contextual factors. Besides, they assume a homogeneous setting where all the stations are able to capture the same pollutants. However, large-scale settings frequently comprise different types of stations, each one with different measurement capabilities. For that reason, the present article introduces a novel GNN framework able to capture the similarities among stations related to the land use of their locations and their primary source of pollution. Furthermore, we define a methodology to deal with heterogeneous settings on the top of the GNN architecture. Finally, the proposal has been tested with a nation-wide Spanish air-pollution dataset with very promising results. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Air pollution; forecasting; graph neural networks; nationwide scale,Graph neural networks; Land use; Air pollution forecasting; Contextual factors; Environmental problems; Feature-based; Graph neural networks; Heterogeneous graph; Nationwide scale; Pollution level; Spatial distance; Urban settings; Air pollution
A Survey on Graph Representation Learning Methods,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183327001&doi=10.1145%2f3633518&partnerID=40&md5=740b93ce0f8b747c838e870ee41f9baf,"Graph representation learning has been a very active research area in recent years. The goal of graph representation learning is to generate graph representation vectors that capture the structure and features of large graphs accurately. This is especially important because the quality of the graph representation vectors will affect the performance of these vectors in downstream tasks such as node classification, link prediction and anomaly detection. Many techniques have been proposed for generating effective graph representation vectors, which generally fall into two categories: traditional graph embedding methods and graph neural network (GNN)–based methods. These methods can be applied to both static and dynamic graphs. A static graph is a single fixed graph, whereas a dynamic graph evolves over time and its nodes and edges can be added or deleted from the graph. In this survey, we review the graph-embedding methods in both traditional and GNN-based categories for both static and dynamic graphs and include the recent papers published until the time of submission. In addition, we summarize a number of limitations of GNNs and the proposed solutions to these limitations. Such a summary has not been provided in previous surveys. Finally, we explore some open and ongoing research directions for future work. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph embedding; graph neural network; graph representation learning; Graphs,Anomaly detection; Graph embeddings; Graph neural networks; Graph theory; Graphic methods; Dynamic graph; Embedding method; Graph; Graph embeddings; Graph neural networks; Graph representation; Graph representation learning; Learning methods; Network-based; Statics and dynamics; Vectors
Hierarchical Pruning of Deep Ensembles with Focal Diversity,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183328455&doi=10.1145%2f3633286&partnerID=40&md5=0e4e378a3329f94018549c3f687f5c82,"Deep neural network ensembles combine the wisdom of multiple deep neural networks to improve the generalizability and robustness over individual networks. It has gained increasing popularity to study and apply deep ensemble techniques in the deep learning community. Some mission-critical applications utilize a large number of deep neural networks to form deep ensembles to achieve desired accuracy and resilience, which introduces high time and space costs for ensemble execution. However, it still remains a critical challenge whether a small subset of the entire deep ensemble can achieve the same or better generalizability and how to effectively identify these small deep ensembles for improving the space and time efficiency of ensemble execution. This article presents a novel deep ensemble pruning approach, which can efficiently identify smaller deep ensembles and provide higher ensemble accuracy than the entire deep ensemble of a large number of member networks. Our hierarchical ensemble pruning approach (HQ) leverages three novel ensemble pruning techniques. First, we show that the focal ensemble diversity metrics can accurately capture the complementary capacity of the member networks of an ensemble team, which can guide ensemble pruning. Second, we design a focal ensemble diversity based hierarchical pruning approach, which will iteratively find high quality deep ensembles with low cost and high accuracy. Third, we develop a focal diversity consensus method to integrate multiple focal diversity metrics to refine ensemble pruning results, where smaller deep ensembles can be effectively identified to offer high accuracy, high robustness and high ensemble execution efficiency. Evaluated using popular benchmark datasets, we demonstrate that the proposed hierarchical ensemble pruning approach can effectively identify high quality deep ensembles with better classification generalizability while being more time and space efficient in ensemble decision making. We have released the source codes on GitHub at https://github.com/git-disl/HQ-Ensemble. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; ensemble diversity; ensemble learning; Ensemble pruning,Benchmarking; Classification (of information); Decision making; Efficiency; Iterative methods; Deep learning; Diversity metrics; Ensemble diversity; Ensemble learning; Ensemble pruning; Hierarchical ensemble; High quality; High-accuracy; Individual network; Neural network's ensemble; Deep neural networks
Enabling Graph Neural Networks for Semi-Supervised Risk Prediction in Online Credit Loan Services,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183323441&doi=10.1145%2f3623401&partnerID=40&md5=7d6c2c4c5009ac37b8bd37f50513b5b0,"Graph neural networks (GNNs) are playing exciting roles in the application scenarios where features are hidden in information associations. Fraud prediction of online credit loan services (OCLSs) is such a typical scenario. But it has another rather critical challenge, i.e., the scarcity of data labels. Fortunately, GNNs can also cope with this problem due to their good ability of semi-supervised learning by mining structure and feature information within graphs. Nevertheless, the gain of internal information is often too limited to help GNNs handle the extreme deficiency of labels with high performance beyond the basic requirement of fraud prediction in OCLSs. Therefore, adding labels from the experts, such as manually adding labels through rules, has become a logical practice. However, the existing rule engines for OCLSs have the confliction problem among continuously accumulated rules. To address this issue, we propose a Snorkel-based Semi-Supervised GNN (S3GNN). Under S3GNN, we specially design an upgraded version of the rule engines, called Graph-Oriented Snorkel (GOS), a graph-specific extension of Snorkel, a widely used weakly supervised learning framework, to design rules by subject matter experts (SMEs) and resolve confliction. In particular, in the graph of an anti-fraud scenario, each node pair may have multiple different types of edges, so we propose the Multiple Edge-Types Based Attention mechanism. In general, for the heterogeneous information and multiple relations in the graph, we first obtain the embedding of applicant nodes by aggregating the representation of attribute nodes, and then use the attention mechanism to aggregate neighbor nodes on multiple meta-paths to get ultimate applicant node embedding. We conduct experiments over the real-life data of a large financial platform. The results demonstrate that S3GNN can outperform the state-of-the-art methods, including the method of pilot platform. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Crime; Data mining; Engines; Forecasting; Graph embeddings; Graph theory; Knowledge graph; Supervised learning; Application scenario; Attention mechanisms; Critical challenges; Data labels; Embeddings; Graph neural networks; Risk predictions; Rule engine; Semi-supervised; Semi-supervised learning; Graph neural networks
Exploring Structure Incentive Domain Adversarial Learning for Generalizable Sleep Stage Classification,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183319095&doi=10.1145%2f3625238&partnerID=40&md5=25d854a2ee4f43d91db7022596b270b2,"Sleep stage classification is crucial for sleep state monitoring and health interventions. In accordance with the standards prescribed by the American Academy of Sleep Medicine, a sleep episode follows a specific structure comprising five distinctive sleep stages that collectively form a sleep cycle. Typically, this cycle repeats about five times, providing an insightful portrayal of the subject’s physiological attributes. The progress of deep learning and advanced domain generalization methods allows automatic and even adaptive sleep stage classification. However, applying models trained with visible subject data to invisible subject data remains challenging due to significant individual differences among subjects. Motivated by the periodic category-complete structure of sleep stage classification, we propose a Structure Incentive Domain Adversarial learning (SIDA) method that combines the sleep stage classification method with domain generalization to enable cross-subject sleep stage classification. SIDA includes individual domain discriminators for each sleep stage category to decouple subject dependence differences among different categories and fine-grained learning of domain-invariant features. Furthermore, SIDA directly connects the label classifier and domain discriminators to promote the training process. Experiments on three benchmark sleep stage classification datasets demonstrate that the proposed SIDA method outperforms other state-of-the-art sleep stage classification and domain generalization methods and achieves the best cross-subject sleep stage classification results. © 2024 Copyright held by the owner/author(s).",domain generalization; physiological signal; sleep stage classification; subject independent; time-series signal,Classification (of information); Learning systems; Physiology; Sleep research; Adversarial learning; Domain generalization; Generalisation; Learning methods; Physiological signals; Sleep stage; Sleep stages classifications; Sleep state; Subject independent; Time series signals; Deep learning
Inferring Real Mobility in Presence of Fake Check-ins Data,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183328883&doi=10.1145%2f3604941&partnerID=40&md5=d6f33c832395cce916a8c0184f4ca3d0,"Understanding human mobility has become an important aspect of location-based services in tasks such as personalized recommendation and individual moving pattern recognition, enabled by the large volumes of data from geo-tagged social media (GTSM). Prior studies mainly focus on analyzing human historical footprints collected by GTSM and assuming the veracity of the data, which need not hold when some users are not willing to share their real footprints due to privacy concerns—thereby affecting reliability/authenticity. In this study, we address the problem of Inferring Real Mobility (IRMo) of users, from their unreliable historical traces. Tackling IRMo is a non-trivial task due to the: (1) sparsity of check-in data; (2) suspicious counterfeit check-in behaviors; and (3) unobserved dependencies in human trajectories. To address these issues, we develop a novel Graph-enhanced Attention model called IRMoGA, which attempts to capture underlying mobility patterns and check-in correlations by exploiting the unreliable spatio-temporal data. Specifically, we incorporate the attention mechanism (rather than solely relying on traditional recursive models) to understand the regularity of human mobility, while employing a graph neural network to understand the mutual interactions from human historical check-ins and leveraging prior knowledge to alleviate the inferring bias. Our experiments conducted on four real-world datasets demonstrate the superior performance of IRMoGA over several state-of-the-art baselines, e.g., up to 39.16% improvement regarding the Recall score on Foursquare. CCS Concepts: • Information systems → Location-based services; Geographic information systems; © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fake check-ins; graph learning; human mobility; POI graph; self-attention,Fake detection; Graph neural networks; Information systems; Information use; Knowledge management; Pattern recognition; Telecommunication services; User profile; Check-in; Fake check-ins; Graph learning; Human mobility; Large volumes; Location-based services; Personalized recommendation; POI graph; Self-attention; Social media; Location based services
Reconstructing Turbulent Flows Using Spatio-temporal Physical Dynamics,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183320552&doi=10.1145%2f3637491&partnerID=40&md5=830c3a80d9acc1c971921dcdc2df14f6,"Accurate simulation of turbulent flows is of crucial importance in many branches of science and engineering. Direct numerical simulation (DNS) provides the highest fidelity means of capturing all intricate physics of turbulent transport. However, the method is computationally expensive because of the wide range of turbulence scales that must be accounted for in such simulations. Large eddy simulation (LES) provides an alternative. In such simulations, the large scales of the flow are resolved, and the effects of small scales are modelled. Reconstruction of the DNS field from the low-resolution LES is needed for a wide variety of applications. Thus the construction of super-resolution methodologies that can provide this reconstruction has become an area of active research. In this work, a new physics-guided neural network is developed for such a reconstruction. The method leverages the partial differential equation that underlies the flow dynamics in the design of spatio-temporal model architecture. A degradation-based refinement method is also developed to enforce physical constraints and to further reduce the accumulated reconstruction errors over long periods. Detailed DNS data on two turbulent flow configurations are used to assess the performance of the model. CCS Concepts: • Computing methodologies → Neural networks; © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Physics-guided neural network, turbulent flow","Internet protocols; Large eddy simulation; Direct-numerical-simulation; High-fidelity; Large-eddy simulations; Neural-networks; Physic-guided neural network, turbulent flow; Physical dynamics; Science and engineering; Spatio-temporal; Turbulence-scale; Turbulent transports; Turbulent flow"
E2Storyline: Visualizing the Relationship with Triplet Entities and Event Discovery,2024,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183314609&doi=10.1145%2f3633519&partnerID=40&md5=51fd2414504ae3c40785d4b75391802f,"The narrative progression of events, evolving into a cohesive story, relies on the entity-entity relationships. Among the plethora of visualization techniques, storyline visualization has gained significant recognition for its effectiveness in offering an overview of story trends, revealing entity relationships, and facilitating visual communication. However, existing methods for storyline visualization often fall short in accurately depicting the specific relationships between entities. In this study, we present E2Storyline, a novel approach that emphasizes simplicity and aesthetics of layout while effectively conveying entity-entity relationships to users. To achieve this, we begin by extracting entity-entity relationships from textual data and representing them as subject-predicate-object (SPO) triplets, thereby obtaining structured data. By considering three types of design requirements, we establish new optimization objectives and model the layout problem using multi-objective optimization (MOO) techniques. The aforementioned SPO triplets, together with time and event information, are incorporated into the optimization model to ensure a straightforward and easily comprehensible storyline layout. Through a qualitative user study, we determine that a pixel-based view is the most suitable method for displaying the relationships between entities. Finally, we apply E2Storyline to real-world data, including movie synopses and live text commentaries. Through comprehensive case studies, we demonstrate that E2Storyline enables users to better extract information from stories and comprehend the relationships between entities. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",information visualization; Layout; multi-objective optimization; storytelling visualization,Conveying; Data mining; Information systems; Multiobjective optimization; Visual communication; Entity-relationship; Event discoveries; Information visualization; Layout; Multi-objectives optimization; Relationships between entities; Storyline visualizations; Storylines; Storytelling visualization; Visualization technique; Visualization
Second-order Confidence Network for Early Classification of Time Series,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183318897&doi=10.1145%2f3631531&partnerID=40&md5=2b7331ecc5867fcfd726dcd70b81b2af,"Time series data are ubiquitous in a variety of disciplines. Early classification of time series, which aims to predict the class label of a time series as early and accurately as possible, is a significant but challenging task in many time-sensitive applications. Existing approaches mainly utilize heuristic stopping rules to capture stopping signals from the prediction results of time series classifiers. However, heuristic stopping rules can only capture obvious stopping signals, which makes these approaches give either correct but late predictions or early but incorrect predictions. To tackle the problem, we propose a novel second-order confidence network for early classification of time series, which can automatically learn to capture implicit stopping signals in early time series in a unified framework. The proposed model leverages deep neural models to capture temporal patterns and outputs second-order confidence to reflect the implicit stopping signals. Specifically, our model exploits the data not only from a time step but also from the probability sequence to capture stopping signals. By combining stopping signals from the classifier output and the second-order confidence, we design a more robust trigger to decide whether or not to request more observations from future time steps. Experimental results show that our approach can achieve superior results in early classification compared to state-of-the-art approaches. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Database systems; Forecasting; Class labels; Learn+; Neural modelling; Second orders; Stopping rule; Time sensitive applications; Time step; Time-series data; Times series; Unified framework; Time series
Towards a Greener and Fairer Transportation System: A Survey of Route Recommendation Techniques,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183324041&doi=10.1145%2f3627825&partnerID=40&md5=57df632e78783ea769090acd1873e27c,"In recent years, ride-hailing services have emerged as a popular means of transportation for the residents of urban areas. There is an inequality in the spatio-temporal distribution of demand and supply, which requires the proper recommendation of routes to drivers in order to guide them towards riders optimally. This paper provides a review of different route recommendation strategies that have been applied in ride-hailing platforms with the main focus on fairness, and environmental issues. It is important to consider the environmental aspects of route recommendation systems as the transportation sector is one of the major sources of air pollution and has reduced the life expectancy of people around the globe. Moreover, there is an unfair distribution of resources and opportunities among the drivers and riders of the platform which has affected their long-term sustainability in the market. In this paper, we highlight the critical challenges and opportunities inherent in the design of green and fair route recommendation systems and indicate some possible directions for future research. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",eco-friendly; fairness; optimization; ride-hailing; Route recommendation,Environmental protection; Sustainable development; Transportation routes; Urban transportation; Eco-friendly; Fairness; Means of transportations; Optimisations; Recommendation techniques; Ride-hailing; Route recommendation; Spatiotemporal distributions; Transportation system; Urban areas; Recommender systems
Isomorphic Graph Embedding for Progressive Maximal Frequent Subgraph Mining,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183770849&doi=10.1145%2f3630635&partnerID=40&md5=36c2f4be9690309b2a72205e29cda0e1,"Maximal frequent subgraph mining (MFSM) is the task of mining only maximal frequent subgraphs, i.e., subgraphs that are not a part of other frequent subgraphs. Although many intelligent systems require MFSM, MFSM is challenging compared to frequent subgraph mining (FSM), as maximal frequent subgraphs lie in the middle of graph lattice, and FSM algorithms must explore an exponential space and an NP-hard subroutine of frequency counting. Different from prior research, which primarily focused on optimal solutions, we introduce pmMine, a progressive graph neural framework designed for MFSM in a single large graph to attain an approximate solution. The framework combines isomorphic graph embedding, non-parametric partitioning, and an efficiently top-down pattern searching strategy. The critical insight that makes pmMine work is to define the concepts of rooted subgraph and isomorphic graph embedding, in which the costly isomorphism subroutine can be efficiently performed using similarity estimation in embedding space. In addition, pmMine returns the patterns identified during the mining process in a progressive manner. We validate the efficiency and effectiveness of our technique through extensive experiments on a variety of datasets spanning various domains. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph representation learning; isomorphism testing; Maximal frequent subgraph mining,Data mining; Graph embeddings; Graph theory; Set theory; Frequent subgraph mining; Frequent subgraphs; Graph embeddings; Graph representation; Graph representation learning; Isomorphic graphs; Isomorphism testing; Maximal frequent subgraph mining; Mining algorithms; Subgraphs; Intelligent systems
Explicit State Representation Guided Video-based Pedestrian Attribute Recognition,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183331185&doi=10.1145%2f3626240&partnerID=40&md5=20231820e6ddc9285228770f62072546,"The pedestrian attribute recognition aims to generate a structured description of pedestrians, which serves an important role in surveillance. Current works usually assume that the images and the specific pedestrian states, including pedestrian occlusion and pedestrian orientation, are given. However, we argue that the current works ignore the guidance of the pedestrian state and cannot achieve the appropriate performance since the appearance feature will become unreliable due to the variance of the pedestrian state, which is common in practice. Therefore, this paper proposes the Explicit State Representation (ExSR) Guided Pedestrian Attribute Recognition to improve the accuracy through state learning and attribute fusion among frames. Firstly, the pedestrian state is explicitly represented by concatenating the pedestrian orientation and occlusion, which can be accurately determined via analyzing the pose. Secondly, the state-aware pedestrian attribute fusion method is proposed and divided into two cases, namely the inter-state case and the intra-state case. In the intra-state case, the appearance feature will remain stable and the attribute relations are propagated to refine. The method of exploiting attribute relations within a single frame is the Graph Neural Network. In the inter-state case, the state changes, the attribute relationship propagation is prevented, and the advantages of attribute recognition in each frame are complemented to make a reliable judgment on the invisible region. The experimental results demonstrate that the ExSR outperforms the state-of-the-art methods on two public databases, benefiting from the explicit introduction of the state into the attribute recognition. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Backpropagation; 'current; Attribute recognition; Explicit state; Fusion methods; Graph neural networks; Performance; Single frames; State representation; State-of-the-art methods; States change; Graph neural networks
Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183315989&doi=10.1145%2f3627821&partnerID=40&md5=b151ce4fda09d3f97c6f3f3769553c45,"An Industrial Cyber-physical System (ICPS) provides a digital foundation for data-driven decision-making by artificial intelligence (AI) models. However, the poor data quality (e.g., inconsistent distribution, imbalanced classes) of high-speed, large-volume data streams poses significant challenges to the online deployment of offline-trained AI models. As an alternative, updating AI models online based on streaming data enables continuous improvement and resilient modeling performance. However, for a supervised learning model (i.e., a base learner), it is labor-intensive to annotate all streaming samples to update the model. Hence, a data acquisition method is needed to select the data for annotation to ensure data quality while saving annotation efforts. In the literature, active learning methods have been proposed to acquire informative samples. Different acquisition criteria were developed for exploration of under-represented regions in the input variable space or exploitation of the well-represented regions for optimal estimation of base learners. However, it remains a challenge to balance the exploration-exploitation trade-off under different online annotation scenarios. On the other hand, an acquisition criterion learned by AI adapts itself to a scenario dynamically, but the ambiguous consideration of the trade-off limits its performance in frequently changing manufacturing contexts. To overcome these limitations, we propose an ensemble active learning method by contextual bandits (CbeAL). CbeAL incorporates a set of active learning agents (i.e., acquisition criteria) explicitly designed for exploration or exploitation by a weighted combination of their acquisition decisions. The weight of each agent will be dynamically adjusted based on the usefulness of its decisions to improve the performance of the base learner. With adaptive and explicit consideration of both objectives, CbeAL efficiently guides the data acquisition process by selecting informative samples to reduce the human annotation efforts. Furthermore, we characterize the exploration and exploitation capability of the proposed agents theoretically. The evaluation results in a numerical simulation study and a real case study demonstrates the effectiveness and efficiency of CbeAL in manufacturing process modeling of the ICPS. © 2023 Copyright held by the owner/author(s).",AI incubation; contextual bandits; Industrial Cyber-physical System; online data annotation,Big data; Data acquisition; Decision making; E-learning; Economic and social effects; Embedded systems; Learning systems; Artificial intelligence incubation; Base learners; Contextual banditti; Cybe-physical systems; Cyber-physical systems; Data annotation; Industrial cybe-physical system; Intelligence models; Online data; Online data annotation; Cyber Physical System
One-step Multi-view Clustering with Consensus Graph and Data Representation Convolution,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183328820&doi=10.1145%2f3630634&partnerID=40&md5=dc8a662961ebe9b807ddb7303ae3ab8b,"Multi-view clustering aims to partition unlabeled patterns into disjoint clusters using consistent and complementary information derived from features of patterns in multiple views. Downstream methods perform this clustering sequentially: estimation of individual or consistent similarity matrices, spectral embedding, and clustering. In this article, we present an approach that can address some of the shortcomings of previous multiview clustering methods. We propose a single objective function whose optimization can jointly provide the consistent graph matrix for all views, the unified spectral data representation, the cluster assignments, and the view weights. We propose a new constraint term that sets the cluster index matrix to the convolution of the consistent spectral projection matrix over the consistent graph. Our proposed scheme has two interesting properties that the recent works do not have simultaneously. First, the cluster assignments can be estimated directly without the need for an additional clustering phase, which depends heavily on initialization. Second, the soft cluster assignments are directly linked to the kernel representation of the features of the views. Moreover, our method automatically computes the weights of each view, requiring fewer hyperparameters. We have conducted a series of experiments on real datasets. These demonstrate the effectiveness of the proposed approach, which compares favorably to many competing multi-view clustering methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",consensus spectral representation; feature convolution; kernelized graph; Multi-view clustering,Cluster analysis; Data mining; Matrix algebra; Cluster assignment; Clustering methods; Consensus spectral representation; Data representations; Feature convolution; Kernelized graph; matrix; Multi-view clustering; Spectral representations; Convolution
Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183323483&doi=10.1145%2f3630636&partnerID=40&md5=b07e2a4569636b7b5efc00b8c1fe6805,"Federated learning (FL) allows multiple participants to collaboratively build deep learning (DL) models without directly sharing data. Consequently, the issue of copyright protection in FL becomes important since unreliable participants may gain access to the jointly trained model. Application of homomorphic encryption (HE) in a secure FL framework prevents the central server from accessing plaintext models. Thus, it is no longer feasible to embed the watermark at the central server using existing watermarking schemes. In this article, we propose a novel client-side FL watermarking scheme to tackle the copyright protection issue in secure FL with HE. To the best of our knowledge, it is the first scheme to embed the watermark to models under a secure FL environment. We design a black-box watermarking scheme based on client-side backdooring to embed a pre-designed trigger set into an FL model by a gradient-enhanced embedding method. Additionally, we propose a trigger set construction mechanism to ensure that the watermark cannot be forged. Experimental results demonstrate that our proposed scheme delivers outstanding protection performance and robustness against various watermark removal attacks and ambiguity attack. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",client-side backdooring; copyright protection; digital watermark; Federated learning,Cryptography; Deep learning; Digital watermarking; E-learning; Watermarking; Central servers; Client sides; Client-side backdooring; Copyright protections; Digital watermark; Federated learning; Ho-momorphic encryptions; Homomorphic-encryptions; Learning models; Watermarking schemes; Copyrights
Human Pose Transfer with Augmented Disentangled Feature Consistency,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183326749&doi=10.1145%2f3626241&partnerID=40&md5=2fc2d5b38544541bf610fdca8f11b576,"Deep generative models have made great progress in synthesizing images with arbitrary human poses and transferring the poses of one person to others. Though many different methods have been proposed to generate images with high visual fidelity, the main challenge remains and comes from two fundamental issues: pose ambiguity and appearance inconsistency. To alleviate the current limitations and improve the quality of the synthesized images, we propose a pose transfer network with augmented Disentangled Feature Consistency (DFC-Net) to facilitate human pose transfer. Given a pair of images containing the source and target person, DFC-Net extracts pose and static information from the source and target respectively, then synthesizes an image of the target person with the desired pose from the source. Moreover, DFC-Net leverages disentangled feature consistency losses in the adversarial training to strengthen the transfer coherence and integrates a keypoint amplifier to enhance the pose feature extraction. With the help of the disentangled feature consistency losses, we further propose a novel data augmentation scheme that introduces unpaired support data with the augmented consistency constraints to improve the generality and robustness of DFC-Net. Extensive experimental results on Mixamo-Pose and EDN-10k have demonstrated DFC-Net achieves state-of-the-art performance on pose transfer. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",computer vision; generative adversarial network; Human pose transfer; image generation,Computer vision; Image enhancement; Current limitation; Feature consistency; Generative model; Human pose; Human pose transfer; Image generations; Pose ambiguities; Synthesized images; Transfer network; Visual fidelity; Generative adversarial networks
Quantifying Levels of Influence and Causal Responsibility in Dynamic Decision Making Events,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183326660&doi=10.1145%2f3631611&partnerID=40&md5=d3b520912056bb4f6e4d1ade72360dcf,"Intelligent systems support human operators’ decision-making processes, many of which are dynamic and involve temporal changes in the decision-related parameters. As we increasingly depend on automation, it becomes imperative to understand and quantify its influence on the operator’s decisions and to evaluate its implications for the human’s causal responsibility for outcomes. Past studies proposed a model for human responsibility in static decision-making processes involving intelligent systems. We present a model for dynamic, non-stationary decision-making events based on the concept of causation strength. We apply it to a test case of a dynamic binary categorization decision. The results show that for automation to influence humans significantly, it must have high detection sensitivity. However, this condition is insufficient since it is unlikely that automation, irrespective of its sensitivity, will sway humans with high detection sensitivity away from their original position. Specific combinations of automation and human detection sensitivities are required for automation to have a major influence. Moreover, the automation influence and the human causal responsibility that can be derived from it are sensitive to possible changes in the human’s detection capabilities due to fatigue or other factors, creating a “Responsibility Cliff.” This should be considered during system design and when policies and regulations are defined. This model constitutes a basis for further analyses of complex events in which human and automation sensitivity levels change over time and for evaluating human involvement in such events. © 2023 Association for Computing Machinery. All rights reserved.",decision making; decision support system; DSS; Human automation interaction; influence; MHC; responsibility,Automation; Decision support systems; Intelligent systems; Decision-making process; Decisions makings; Detection sensitivity; DSS; Dynamic decision making; Human-automation interactions; Influence; MHC; Responsibility; Systems support; Decision making
Out-of-distribution Detection in Time-series Domain: A Novel Seasonal Ratio Scoring Approach,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183325211&doi=10.1145%2f3630633&partnerID=40&md5=73d65bd7ad7e25abf09d9a3d6c388001,"Safe deployment of time-series classifiers for real-world applications relies on the ability to detect the data that is not generated from the same distribution as training data. This task is referred to as out-of-distribution (OOD) detection. We consider the novel problem of OOD detection for the time-series domain. We discuss the unique challenges posed by time-series data and explain why prior methods from the image domain will perform poorly. Motivated by these challenges, this article proposes a novel Seasonal Ratio Scoring (SRS) approach. SRS consists of three key algorithmic steps. First, each input is decomposed into class-wise semantic component and remainder. Second, this decomposition is employed to estimate the class-wise conditional likelihoods of the input and remainder using deep generative models. The seasonal ratio score is computed from these estimates. Third, a threshold interval is identified from the in-distribution data to detect OOD examples. Experiments on diverse real-world benchmarks demonstrate that the SRS method is well-suited for time-series OOD detection when compared to baseline methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; Machine learning robustness; Out-of-distribution detection; Time series data,Classification (of information); Deep learning; Learning systems; Semantics; Algorithmics; Deep learning; Image domain; Machine learning robustness; Machine-learning; Out-of-distribution detection; Real-world; Time-series data; Times series; Training data; Time series
Interpretable Imitation Learning with Symbolic Rewards,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183316503&doi=10.1145%2f3627822&partnerID=40&md5=a3eb8d0d33fbdac467e915f1195468fb,"Sample inefficiency of deep reinforcement learning methods is a major obstacle for their use in real-world tasks as they naturally feature sparse rewards. In fact, this from-scratch approach is often impractical in environments where extreme negative outcomes are possible. Recent advances in imitation learning have improved sample efficiency by leveraging expert demonstrations. Most work along this line of research employs neural network-based approaches to recover an expert cost function. However, the complexity and lack of transparency make neural networks difficult to trust and deploy in the real world. In contrast, we present a method for extracting interpretable symbolic reward functions from expert data, which offers several advantages. First, the learned reward function can be parsed by a human to understand, verify and predict the behavior of the agent. Second, the reward function can be improved and modified by an expert. Finally, the structure of the reward function can be leveraged to extract explanations that encode richer domain knowledge than standard scalar rewards. To this end, we use an autoregressive recurrent neural network that generates hierarchical symbolic rewards represented by simple symbolic trees. The recurrent neural network is trained via risk-seeking policy gradients. We test our method in MuJoCo environments as well as a chemical plant simulator. We show that the discovered rewards can significantly accelerate the training process and achieve similar or better performance than neural network-based algorithms. © 2023 Copyright held by the owner/author(s).",imitation learning; interpretable imitation learning; Interpretable reinforcement learning; reinforcement learning,Chemical plants; Cost functions; Data mining; Domain Knowledge; Knowledge management; Learning systems; Recurrent neural networks; Imitation learning; Interpretable imitation learning; Interpretable reinforcement learning; Neural-networks; Real-world task; Reinforcement learning method; Reinforcement learnings; Reward function; Reinforcement learning
Memory Network-Based Interpreter of User Preferences in Content-Aware Recommender Systems,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180211465&doi=10.1145%2f3625239&partnerID=40&md5=81a65e3c4b49ac995d76c2eed181bb33,"This article introduces a novel architecture for two objectives recommendation and interpretability in a unified model. We leverage textual content as a source of interpretability in content-Aware recommender systems. The goal is to characterize user preferences with a set of human-understandable attributes, each is described by a single word, enabling comprehension of user interests behind item adoptions. This is achieved via a dedicated architecture, which is interpretable by design, involving two components for recommendation and interpretation. In particular, we seek an interpreter, which accepts holistic user's representation from a recommender to output a set of activated attributes describing user preferences. Besides encoding interpretability properties such as fidelity, conciseness and diversity, the proposed memory network-based interpreter enables the generalization of user representation by discovering relevant attributes that go beyond her adopted items' textual content. We design experiments involving both human-and functionally-grounded evaluations of interpretability. Results on four real-world datasets show that our proposed model not only discovers highly relevant attributes for interpreting user preferences, but also enjoys comparable or better recommendation accuracy than a series of baselines.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesInterpretable user preferences; content-Aware recommendation; memory network,Memory architecture; Network architecture; Additional key word and phrasesinterpretable user preference; Content-aware; Content-aware recommendation; Interpretability; Key words; Memory network; Network-based; Novel architecture; Textual content; User's preferences; Recommender systems
GNN-based Advanced Feature Integration for ICS Anomaly Detection,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180173816&doi=10.1145%2f3620676&partnerID=40&md5=9b475da0fbceb349d5c1d5d145dbc931,"Recent adversaries targeting the Industrial Control Systems (ICSs) have started exploiting their sophisticated inherent contextual semantics such as the data associativity among heterogeneous field devices. In light of the subtlety rendered in these semantics, anomalies triggered by such interactions tend to be extremely covert, hence giving rise to extensive challenges in their detection. Driven by the critical demands of securing ICS processes, a Graph-Neural-Network (GNN) based method is presented to tackle these subtle hostilities by leveraging an ICS's advanced contextual features refined from a universal perspective, rather than exclusively following GNN's conventional local aggregation paradigm. Specifically, we design and implement the Graph Sample-And-Integrate Network (GSIN), a general chained framework performing node-level anomaly detection via advanced feature integration, which combines a node's local awareness with the graph's prominent global properties extracted via process-oriented pooling. The proposed GSIN is evaluated on multiple well-known datasets with different kinds of integration configurations, and results demonstrate its superiority consistently on not only anomaly detection performance (e.g., F1 score and AUPRC) but also runtime efficiency over recent representative baselines.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAdvanced feature pooling; anomaly detection; embedding integration; graph neural networks; industrial control systems,Control systems; Feature extraction; Graph neural networks; Integrated circuits; Integration; Semantics; Additional key word and phrasesadvanced feature pooling; Anomaly detection; Embedding integration; Embeddings; Feature integration; Feature pooling; Graph neural networks; Industrial control systems; Key words; Network-based; Anomaly detection
Learning with Euler Collaborative Representation for Robust Pattern Analysis,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180253008&doi=10.1145%2f3625235&partnerID=40&md5=02e2e0abda62a0ff737129dfcdf295ec,"The Collaborative Representation (CR) framework has provided various effective and efficient solutions to pattern analysis. By leveraging between discriminative coefficient coding (l2 regularization) and the best reconstruction quality (collaboration), the CR framework can exploit discriminative patterns efficiently in high-dimensional space. Due to the limitations of its linear representation mechanism, the CR must sacrifice its superior efficiency for capturing the non-linear information with the kernel trick. Besides this, even if the coding is indispensable, there is no mechanism designed to keep the CR free from inevitable noise brought by real-world information systems. In addition, the CR only emphasizes exploiting discriminative patterns on coefficients rather than on the reconstruction. To tackle the problems of primitive CR with a unified framework, in this article we propose the Euler Collaborative Representation (E-CR) framework. Inferred from the Euler formula, in the proposed method, we map the samples to a complex space to capture discriminative and non-linear information without the high-dimensional hidden kernel space. Based on the proposed E-CR framework, we form two specific classifiers: The Euler Collaborative Representation based Classifier (E-CRC) and the Euler Probabilistic Collaborative Representation based Classifier (E-PROCRC). Furthermore, we specifically designed a robust algorithm for E-CR (termed as R-E-CR) to deal with the inevitable noises in real-world systems. Robust iterative algorithms have been specially designed for solving E-CRC and E-PROCRC. We correspondingly present a series of theoretical proofs to ensure the completeness of the theory for the proposed robust algorithms. We evaluated E-CR and R-E-CR with various experiments to show its competitive performance and efficiency.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEuler space; collaborative representation; pattern analysis; robustness,Formal logic; Iterative methods; Additional key word and phraseseuler space; Collaborative representations; Key words; Linear information; Non linear; Pattern analysis; Probabilistics; Robust algorithm; Robust patterns; Robustness; Efficiency
Meaning-Sensitive Text Data Augmentation with Intelligent Masking,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180174631&doi=10.1145%2f3623403&partnerID=40&md5=3c566848129c95df0b8d573de661e09f,"With the recent popularity of applying large-scale deep neural network-based models for natural language processing (NLP), attention to develop methods for text data augmentation is at its peak, since the limited size of training data tends to significantly affect the accuracy of these models. To this end, we propose a novel text data augmentation technique called Intelligent Masking with Optimal Substitutions Text Data Augmentation (IMOSA). IMOSA, developed for labelled sentences, can identify the most favourable sentences and locate the appropriate word combinations in a particular sentence to replace and generate synthetic sentences with a meaning closer to the original sentence, while also significantly increasing the diversity of the dataset. We demonstrate that the proposed technique notably improves the performance of classifiers based on attention-based transformer models through the extensive experiments for five different text classification tasks which are performed under the low data regime in a context-Aware NLP setting. The analysis clearly shows that IMOSA effectively generates more sentences using favourable original examples and completely ignores undesirable examples. Furthermore, the experiments carried out confirm IMOSA's ability to add diversity to the augmented dataset using multiple distinct masking patterns against the same original sentence, which remarkably adds variety to the training dataset. IMOSA consistently outperforms the two key masked language model-based text data augmentation techniques, and demonstrates a robust performance against the critical challenging NLP tasks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesText data augmentation; IMOSA; masked language model,Classification (of information); Computational linguistics; Natural language processing systems; Text processing; Additional key word and phrasestext data augmentation; Data augmentation; Intelligent masking with optimal substitution text data augmentation; Key words; Language model; Language processing; Masked language model; Natural languages; Text data; Deep neural networks
"Trading off Privacy, Utility, and Efficiency in Federated Learning",2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179184441&doi=10.1145%2f3595185&partnerID=40&md5=b7ea1b254923824422a01b5f887d803f,"Federated learning (FL) enables participating parties to collaboratively build a global model with boosted utility without disclosing private data information. Appropriate protection mechanisms have to be adopted to fulfill the opposing requirements in preserving privacy and maintaining high model utility. In addition, it is a mandate for a federated learning system to achieve high efficiency in order to enable large-scale model training and deployment. We propose a unified federated learning framework that reconciles horizontal and vertical federated learning. Based on this framework, we formulate and quantify the trade-offs between privacy leakage, utility loss, and efficiency reduction, which leads us to the No-Free-Lunch (NFL) theorem for the federated learning system. NFL indicates that it is unrealistic to expect an FL algorithm to simultaneously provide excellent privacy, utility, and efficiency in certain scenarios. We then analyze the lower bounds for the privacy leakage, utility loss, and efficiency reduction for several widely-Adopted protection mechanisms, including Randomization, Homomorphic Encryption, Secret Sharing, and Compression. Our analysis could serve as a guide for selecting protection parameters to meet particular requirements.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFederated learning; divergence; efficiency; optimization; privacy; trade-off; utility,Economic and social effects; Learning systems; Privacy-preserving techniques; Additional key word and phrasesfederated learning; Divergence; Federated learning system; Key words; Optimisations; Privacy; Privacy leakages; Protection mechanisms; Trade off; Utility; Efficiency
Attention-guided Adversarial Attack for Video Object Segmentation,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179180118&doi=10.1145%2f3617067&partnerID=40&md5=f657a2944f9f27fbc2931da81caf2a06,"Video Object Segmentation (VOS) methods have made many breakthroughs with the help of the continuous development and advancement of deep learning. However, the deep learning model is vulnerable to malicious adversarial attacks, which mislead the model to make wrong decisions by adding adversarial perturbation that humans cannot perceive to the input image. Threats to deep learning models remind us that video object segmentation methods are also vulnerable to attacks, thereby threatening their security. Therefore, we study adversarial attacks on the VOS task to better identify the vulnerabilities of the VOS method, which in turn provides an opportunity to improve its robustness. In this paper, we propose an attention-guided adversarial attack method, which uses spatial attention blocks to capture features with global dependencies to construct correlations between consecutive video frames, and performs multipath aggregation to effectively integrate spatial-Temporal perturbation, thereby guiding the deconvolution network to generate adversarial examples with strong attack capability. Specifically, the class loss function is designed to enable the deconvolution network to better activate noise in other regions and suppress the activation related to the object class based on the enhanced feature map of the object class. At the same time, attentional feature loss is designed to enhance the transferability against attack. The experimental results on the DAVIS dataset show that the proposed attention-guided adversarial attack method can significantly reduce the segmentation accuracy of OSVOS, and the J&F mean on DAVIS 2016 can reach 73.6% drop rate. The generated adversarial examples are also highly transferable to other video object segmentation models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesVideo object segmentation; adversarial attack; attention-guided; deconvolution network,Deep learning; Learning systems; Motion compensation; Additional key word and phrasesvideo object segmentation; Adversarial attack; Attention-guided; Deconvolution network; Deconvolutions; Key words; Learning models; Objects segmentation; Segmentation methods; Video objects segmentations; Image segmentation
Multi-aspect Understanding with Cooperative Graph Attention Networks for Medical Dialogue Information Extraction,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179180940&doi=10.1145%2f3620675&partnerID=40&md5=bd3bfd017b580766ad50252bb378ce2e,"Medical dialogue information extraction is an important but challenging task for Electronic Medical Records. Existing medical information extraction methods ignore the crucial information of sentence and multi-level dependency in dialogue, which limits their effectiveness for capturing essential medical information. To address these issues, we present a novel Multi-aspect Understanding with Cooperative Graph Attention Networks for Medical Dialogue Information Extraction to capture multi-aspect sentence information and multi-level dependency information from the dialogue. First, we propose the multi-aspect sentence encoder to capture various features from different perspectives. Second, we propose double graph attention networks to model the dependency features from intra-window and inter-window, respectively. Extensive experiments on a benchmark dataset have well-validated the effectiveness of the proposed method.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNatural language processing; graph attention network; medical dialogue information extraction; relation extraction,Data mining; Information retrieval; Medical computing; Additional key word and phrasesnatural language processing; Graph attention network; Key words; Language processing; Medical dialog information extraction; Medical information; Medical record; Multi aspects; Multilevels; Relation extraction; Bioinformatics
Adversarial Attacks on Deep Reinforcement Learning-based Traffic Signal Control Systems with Colluding Vehicles,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181759574&doi=10.1145%2f3625236&partnerID=40&md5=7325f0143f0c3d8228e41f8080df2fef,"The rapid advancements of Internet of Things (IoT) and Artificial Intelligence (AI) have catalyzed the development of adaptive traffic control systems (ATCS) for smart cities. In particular, deep reinforcement learning (DRL) models produce state-of-The-Art performance and have great potential for practical applications. In the existing DRL-based ATCS, the controlled signals collect traffic state information from nearby vehicles, and then optimal actions (e.g., switching phases) can be determined based on the collected information. The DRL models fully ""trust""that vehicles are sending the true information to the traffic signals, making the ATCS vulnerable to adversarial attacks with falsified information. In view of this, this article first time formulates a novel task in which a group of vehicles can cooperatively send falsified information to ""cheat""DRL-based ATCS in order to save their total travel time. To solve the proposed task, we develop CollusionVeh, a generic and effective vehicle-colluding framework composed of a road situation encoder, a vehicle interpreter, and a communication mechanism. We employ our framework to attack established DRL-based ATCS and demonstrate that the total travel time for the colluding vehicles can be significantly reduced with a reasonable number of learning episodes, and the colluding effect will decrease if the number of colluding vehicles increases. Additionally, insights and suggestions for the real-world deployment of DRL-based ATCS are provided. The research outcomes could help improve the reliability and robustness of the ATCS and better protect the smart mobility systems. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive traffic control system; Adversarial attacks; connected vehicles; reinforcement learning,Adaptive control systems; Deep learning; Internet of things; Learning systems; Traffic signals; Travel time; Vehicle to vehicle communications; Vehicles; Adaptive traffic control; Adaptive traffic control system; Adversarial attack; Connected vehicle; Reinforcement learning models; Reinforcement learnings; State-of-the-art performance; Traffic control systems; Traffic signal control system; Travel-time; Reinforcement learning
Performing Cancer Diagnosis via an Isoform Expression Ranking-based LSTM Model,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180266428&doi=10.1145%2f3625237&partnerID=40&md5=fbb14dbe2ab7316c8dda9d56d3f2b5ca,"The known set of genetic factors involved in the development of several types of cancer has considerably been expanded, thus easing to devise and implement better therapeutic strategies. The automatic diagnosis of cancer, however, remains as a complex task because of the high heterogeneity of tumors and the biological variability between samples. In this work, a long short-Term memory network-based model is proposed for diagnosing cancer from transcript-base data. An efficient method that transforms data into gene/isoform expression-based rankings was formulated, allowing us to directly embed important information in the relative order of the elements of a ranking that can subsequently ease the classification of samples. The proposed predictive model leverages the power of deep recurrent neural networks, being able to learn existing patterns on the individual rankings of isoforms describing each sample of the population. To evaluate the suitability of the proposal, an extensive experimental study was conducted on 17 transcript-based datasets, and the results showed the effectiveness of this novel approach and also indicated the gene/isoforms expression-based rankings contained valuable information that can lead to a more effective cancer diagnosis.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCancer diagnosis; gene/isoform expression-based ranking; Long Short-Term Memory network,Brain; Classification (of information); Diseases; Genes; Long short-term memory; Additional key word and phrasescancer diagnose; Automatic diagnosis; Cancer diagnosis; Gene/isoform expression-based ranking; Genetic factors; Isoforms; Key words; Long short-term memory network; Memory network; Therapeutic strategy; Diagnosis
Learning Privacy-Preserving Embeddings for Image Data to Be Published,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178472457&doi=10.1145%2f3623404&partnerID=40&md5=a164b44b96dd0adc65922c56f299cb7f,"Deep learning shows superiority in learning feature representations that offer promising performance in various application domains. Recent advances have shown that privacy attributes of users and patients (e.g., identity, gender, and race) can be accurately inferred from image data. To avoid the risk of privacy leaking, data owners can resort to releasing the embeddings rather than the original images. In this article, we aim at learning to generate privacy-preserving embeddings from image data. The obtained embeddings are required to maintain the data utility (e.g., keeping the performance of the main task, such as disease prediction) and to simultaneously prevent the private attributes of data instances from being accurately inferred. We also want the hard embeddings to be successfully used to reconstruct the original images. We propose a hybrid method based on multi-Task learning to reach the goal. The key idea is twofold. One is to learn the feature encoder that can benefit the main task and fool the sensitive task at the same time via iterative training and feature disentanglement. The other is to incorporate the learning of adversarial examples to mislead the sensitive attribute classification's performance. Experiments conducted on Multi-Attribute Facial Landmark (MAFL) and NIH Chest X-ray datasets exhibit the effectiveness of our hybrid method. A set of advanced studies also shows the usefulness of each model component, the difficulty in data reconstruction, and the performance impact of task correlation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPrivacy preservation; adversarial examples; adversarial learning; feature disentanglement; medical images,Deep learning; Embeddings; Medical imaging; Privacy-preserving techniques; Additional key word and phrasesprivacy preservation; Adversarial example; Adversarial learning; Embeddings; Feature disentanglement; Image data; Key words; Medical image; Performance; Privacy preserving; Iterative methods
Unifying Gradients to Improve Real-World Robustness for Deep Networks,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179182033&doi=10.1145%2f3617895&partnerID=40&md5=7f7fc7acbb35a2e839e24e4dbe570a63,"The wide application of deep neural networks (DNNs) demands an increasing amount of attention to their real-world robustness, i.e., whether a DNN resists black-box adversarial attacks, among which score-based query attacks (SQAs) are the most threatening since they can effectively hurt a victim network with only access to model outputs. Defending against SQAs requires a slight but artful variation of outputs due to the service purpose for users, who share the same output information with SQAs. In this article, we propose a real-world defense by Unifying Gradients (UniG) of different data so that SQAs could only probe a much weaker attack direction that is similar for different samples. Since such universal attack perturbations have been validated as less aggressive than the input-specific perturbations, UniG protects real-world DNNs by indicating to attackers a twisted and less informative attack direction. We implement UniG efficiently by a Hadamard product module, which is plug-And-play. According to extensive experiments on 5 SQAs, 2 adaptive attacks and 7 defense baselines, UniG significantly improves real-world robustness without hurting clean accuracy on CIFAR10 and ImageNet. For instance, UniG maintains a model of 77.80% accuracy under a 2500-query Square attack while the state-of-The-Art adversarially trained model only has 67.34% on CIFAR10. Simultaneously, UniG outperforms all compared baselines in terms of clean accuracy and achieves the smallest modification of the model output. The code is released at https://github.com/snowien/UniG-pytorch.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesBlack-box adversarial attack; practical adversarial defense,HTTP; Image enhancement; Network security; Additional key word and phrasesblack-box adversarial attack; Black boxes; Hadamard products; Key words; Model outputs; Network demands; Plug-and-play; Practical adversarial defense; Product module; Real-world; Deep neural networks
What Your Next Check-in Might Look Like: Next Check-in Behavior Prediction,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181672302&doi=10.1145%2f3625234&partnerID=40&md5=272c57a0142b262fbdf272d2dadeb7ad,"In recent years, the next-POI recommendation has become a trending research topic in the field of trajectory data mining. For protection of user privacy, users' complete GPS trajectories are difficult to obtain. The check-in information posted by users on social networks has become an important data source for Spatio-Temporal Trajectory research. However, state-of-The-Art methods neglect the social meaning and the information dissemination function of check-in behavior. The social meaning is an important reason why users are willing to post check-in on social networks, and the information dissemination function means, users can affect each other's behavior by check-ins. The above characteristics of the check-in behavior make it different from the visiting behavior. We consider a new problem of predicting the next check-in behavior including the check-in time, the POI (point-of-interest) where the check-in is located, functional semantics of the POI, and so on. To solve the proposed problem, we build a multi-Task learning model called DPMTM, and a pre-Training module is designed to extract dynamic social semantics of check-in behaviors. Our results show that the DPMTM model works well in the check-in behavior problem. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",check-in behavior prediction; dynamic social semantics; multi-Task learning; POI recommendation; Spatio-Temporal trajectory analysis,Data mining; Forecasting; Learning systems; Semantics; Social aspects; Trajectories; Behavior prediction; Check-in; Check-in behavior prediction; Dynamic social semantic; Multitask learning; Point-of-interest recommendation; Social semantics; Spatio-temporal trajectories; Spatio-temporal trajectory analyse; Trajectory analysis; Information dissemination
A Comprehensive Survey on Model Quantization for Deep Neural Networks in Image Classification,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179876658&doi=10.1145%2f3623402&partnerID=40&md5=f1b3fb81c2fc31fae1a033ef707aa588,"Recent advancements in machine learning achieved by Deep Neural Networks (DNNs) have been significant. While demonstrating high accuracy, DNNs are associated with a huge number of parameters and computations, which leads to high memory usage and energy consumption. As a result, deploying DNNs on devices with constrained hardware resources poses significant challenges. To overcome this, various compression techniques have been widely employed to optimize DNN accelerators. A promising approach is quantization, in which the full-precision values are stored in low bit-width precision. Quantization not only reduces memory requirements but also replaces high-cost operations with low-cost ones. DNN quantization offers flexibility and efficiency in hardware design, making it a widely adopted technique in various methods. Since quantization has been extensively utilized in previous works, there is a need for an integrated report that provides an understanding, analysis, and comparison of different quantization approaches. Consequently, we present a comprehensive survey of quantization concepts and methods, with a focus on image classification. We describe clustering-based quantization methods and explore the use of a scale factor parameter for approximating full-precision values. Moreover, we thoroughly review the training of a quantized DNN, including the use of a straight-Through estimator and quantization regularization. We explain the replacement of floating-point operations with low-cost bitwise operations in a quantized DNN and the sensitivity of different layers in quantization. Furthermore, we highlight the evaluation metrics for quantization methods and important benchmarks in the image classification task. We also present the accuracy of the state-of-The-Art methods on CIFAR-10 and ImageNet. This article attempts to make the readers familiar with the basic and advanced concepts of quantization, introduce important works in DNN quantization, and highlight challenges for future research in this field.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesQuantization; deep neural network acceleration; discrete neural network optimization; image classification; model compression,Costs; Digital arithmetic; Energy utilization; Image classification; Image compression; Additional key word and phrasesquantization; Deep neural network acceleration; Discrete neural network optimization; Discrete neural networks; Images classification; Key words; Low-costs; Model compression; Neural network optimization; Quantisation; Deep neural networks
"""intelligent Heuristics Are the Future of Computing""",2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180176696&doi=10.1145%2f3627708&partnerID=40&md5=9398fba4327159122e9169e1b2949ccb,"Back in 1988, the partial game trees explored by computer chess programs were among the largest search structures in real-world computing. Because the game tree is too large to be fully evaluated, chess programs must make heuristic strategic decisions based on partial information, making it an illustrative subject for teaching AI search. In one of his lectures that year on AI search for games and puzzles, Professor Hans Berliner-a pioneer of computer chess programs1-stated:""Intelligent heuristics are the future of computing.""As a student in the field of the theory of computation, I was naturally perplexed but fascinated by this perspective. I had been trained to believe that ""Algorithms and computational complexity theory are the foundation of computer science.""However, as it happens, my attempts to understand heuristics in computing have subsequently played a significant role in my career as a theoretical computer scientist. I have come to realize that Berliner's postulation is a far-reaching worldview, particularly in the age of big, rich, complex, and multifaceted data and models, when computing has ubiquitous interactions with science, engineering, humanity, and society. In this article,2I will share some of my experiences on the subject of heuristics in computing, presenting examples of theoretical attempts to understand the behavior of heuristics on real data, as well as efforts to design practical heuristics with desirable theoretical characterizations. My hope is that these theoretical insights from past heuristics-such as spectral partitioning, multilevel methods, evolutionary algorithms, and simplex methods-can shed light on and further inspire a deeper understanding of the current and future techniques in AI and data mining.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHeuristics in computing; AI; axiomatic approach; beyond worst-cast analysis; binary decision diagram; data analysis; data mining; deep learning; dimensionality reduction; evolutionary algorithm; game trees; linear programming; local clustering; multilevel methods; network analysis; network centrality; network influence; PageRank; robust statistics; Shapley value; smoothed analysis; spectral graph sparsification; spectral graph theory,Binary decision diagrams; Clustering algorithms; Complex networks; Computational complexity; Computer games; Deep learning; Evolutionary algorithms; Game theory; Graph theory; Linear programming; Ubiquitous computing; Additional key word and phrasesheuristic in computing; Axiomatic approach; Beyond bad-cast analyse; Deep learning; Dimensionality reduction; Game trees; Graph sparsification; Key words; Linear-programming; Local clustering; Multilevel method; Network centralities; Network influences; Page ranks; Robust statistics; Shapley value; Smoothed analysis; Spectral graph sparsification; Spectral graph theory; Data mining
A Spatial and Adversarial Representation Learning Approach for Land Use Classification with POIs,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180230435&doi=10.1145%2f3627824&partnerID=40&md5=96275700474b6295e3e8209db8edbd74,"Points-of-interests (POIs) have been proven to be indicative for sensing urban land use in numerous studies. However, recent progress mainly relies on spatial co-occurrence patterns among POI categories, which falls short in utilizing the rich semantic information embodied in POI hierarchical categories and in sensing the spatial distribution patterns of POIs at an individual zonal scale. In this context, we present a spatial and adversarial representation learning approach (SARL) for predicting land use of urban zones with POIs. SARL deeply mines the information from POIs from both spatial and categorical perspectives. Specifically, we first utilize a convolutional neural network to sense the spatial distribution patterns of POIs in each urban zone. We then leverage an autoencoder and an adversarial learning strategy to mine the POI categorical information in all hierarchical levels, which emphasizes the prominent and definitive POIs while preserves the overall POI hierarchical structures in each zone. Finally, we fuse these information from the two perspectives via a Wide & Deep network and carry out land use prediction with the fused embeddings. We conduct comprehensive experiments to validate the effectiveness of SARL in four European cities with real-world data. The results demonstrate that SARL substantially outperforms several competitive baselines.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLand use classification; adversarial learning; POI categorical hierarchy; POI spatial distribution; urban zone embedding,Embeddings; Land use; Semantics; Additional key word and phrasesland use classification; Adversarial learning; Categorical hierarchy; Embeddings; Key words; Learning approach; Point-of-interest categorical hierarchy; Point-of-interest spatial distribution; Urban zone embedding; Urban zones; Spatial distribution
STExplorer: A Hierarchical Autonomous Exploration Strategy with Spatio-Temporal Awareness for Aerial Robots,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179178631&doi=10.1145%2f3595184&partnerID=40&md5=b3a9979f824c756b997595b2c85a9427,"The autonomous exploration task we consider requires Unmanned Aerial Vehicles (UAVs) to actively navigate through unknown environments with the goal of fully perceiving and mapping the environments. Some existing exploration strategies suffer from rough cost budgets, ambiguous Information Gain (IG), and unnecessary backtracking exploration caused by Fragmented Regions (FRs). In our work, a hierarchical spatio-Temporal-Aware exploration framework is proposed to alleviate these problems. At the local exploration level, the Asymmetrical Traveling Salesman Problem (ATSP) is solved by comprehensively considering exploration time, IG, and heading consistency to avoid blindly exploring. Specifically, the exploration time is reasonably budgeted by fast marching in an artificial potential field. Meanwhile, a transformer-based map occupancy predictor is designed to assist in IG calculation by imagining spatial clues out of the Field of View (FoV), facilitating the prescient exploration. We verify that our local exploration is effective in alleviating the unnecessary back-And-forth movements caused by FRs and the interference of potential obstacle occlusion on the IG calculation. At the global exploration level, the classical Next Best View Points (NBVP) are generalized to Next Best Sub-Regions (NBSR) to choose informative sub-regions for further forward-looking exploration based on a well-designed utility function. Safe flight paths and dynamically feasible trajectories are reasonably generated throughout the exploration process by fast marching and B-spline curve optimization. Comparative simulations and benchmark tests demonstrate that our proposed exploration strategy is quite competitive in terms of exploration path length, total exploration time, and exploration ratio.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSpatio-Temporal autonomous exploration; fast marching; information gain; spatial occupancy prediction; unmanned aerial vehicles,Air navigation; Benchmarking; Budget control; Curve fitting; Traveling salesman problem; Unmanned aerial vehicles (UAV); Additional key word and phrasesspatio-temporal autonomous exploration; Aerial vehicle; Autonomous exploration; Exploration strategies; Fast-marching; Information gain; Key words; Occupancy predictions; Spatial occupancy prediction; Unmanned aerial vehicle; Antennas
Dynamic Weights and Prior Reward in Policy Fusion for Compound Agent Learning,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180206885&doi=10.1145%2f3623405&partnerID=40&md5=47d6ee2858a720610b2008f9314af22d,"In Deep Reinforcement Learning (DRL) domain, a compound learning task is often decomposed into several sub-Tasks in a divide-And-conquer manner, each trained separately and then fused concurrently to achieve the original task, referred to as policy fusion. However, the state-of-The-Art (SOTA) policy fusion methods treat the importance of sub-Tasks equally throughout the task process, eliminating the possibility of the agent relying on different sub-Tasks at various stages. To address this limitation, we propose a generic policy fusion approach, referred to as Policy Fusion Learning with Dynamic Weights and Prior Reward (PFLDWPR), to automate the time-varying selection of sub-Tasks. Specifically, PFLDWPR produces a time-varying one-hot vector for sub-Tasks to dynamically select a suitable sub-Task and mask the rest throughout the entire task process, enabling the fused strategy to optimally guide the agent in executing the compound task. The sub-Tasks with the dynamic one-hot vector are then aggregated to obtain the action policy for the original task. Moreover, we collect sub-Tasks's rewards at the pre-Training stage as a prior reward, which, along with the current reward, is used to train the policy fusion network. Thus, this approach reduces fusion bias by leveraging prior experience. Experimental results under three popular learning tasks demonstrate that the proposed method significantly improves three SOTA policy fusion methods in terms of task duration, episode reward, and score difference.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCompound agent learning; deep reinforcement learning; dynamic weights; policy fusion; prior reward,Deep learning; Learning systems; Additional key word and phrasescompound agent learning; Agent learning; Deep reinforcement learning; Dynamic priors; Dynamic weight; Key words; Policy fusion; Prior reward; Reinforcement learnings; Subtask; Reinforcement learning
Asymmetrical Attention Networks Fused Autoencoder for Debiased Recommendation,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167912523&doi=10.1145%2f3596498&partnerID=40&md5=229f911cd4eddaf8b7aa23ff295bb39e,"Popularity bias is a massive challenge for autoencoder-based models, which decreases the level of personalization and hurts the fairness of recommendations. User reviews reflect their preferences and help mitigate bias or unfairness in the recommendation. However, most existing works typically incorporate user (item) reviews into a long document and then use the same module to process the document in parallel. Actually, the set of user reviews is completely different from the set of item reviews. User reviews are heterogeneous in that they reflect a variety of items purchased by users, while item reviews are only related to the item itself and are thus typically homogeneous. In this article, a novel asymmetric attention network fused with autoencoders is proposed, which jointly learns representations from the user and item reviews and implicit feedback to perform recommendations. Specifically, we design an asymmetric attentive module to capture rich representations from user and item reviews, respectively, which solves data sparsity and explainable problems. Furthermore, to further address popularity bias, we apply a noise-contrastive estimation objective to learn high-quality ""de-popularity""embedding via the decoder structure. A series of extensive experiments are conducted on four benchmark datasets to show that leveraging user review information can eliminate popularity bias and improve performance compared to various state-of-The-Art recommendation techniques.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPopularity bias; asymmetrical hierarchical network; autoencoder; review-Aware Recommendation,Learning systems; Additional key word and phrasespopularity bias; Asymmetrical hierarchical network; Auto encoders; Hierarchical network; Implicit feedback; Key words; Learn+; Personalizations; Review-aware recommendation; User reviews; Benchmarking
Adaptive Integration of Categorical and Multi-relational Ontologies with EHR Data for Medical Concept Embedding,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180192344&doi=10.1145%2f3625224&partnerID=40&md5=b0a0715859b19f1455b0a5226ff18cdb,"Representation learning has been applied to Electronic Health Records (EHR) for medical concept embedding and the downstream predictive analytics tasks with promising results. Medical ontologies can also be integrated to guide the learning so the embedding space can better align with existing medical knowledge. Yet, properly carrying out the integration is non-Trivial. Medical concepts that are similar according to a medical ontology may not be necessarily close in the embedding space learned from the EHR data, as medical ontologies organize medical concepts for their own specific objectives. Any integration methodology without considering the underlying inconsistency will result in sub-optimal medical concept embedding and, in turn, degrade the performance of the downstream tasks. In this article, we propose a novel representation learning framework called ADORE (ADaptive Ontological REpresentations) that allows the medical ontologies to adapt their structures for more robust integrating with the EHR data. ADORE first learns multiple embeddings for each category in the ontology via an attention mechanism. At the same time, it supports an adaptive integration of categorical and multi-relational ontologies in the embedding space using a category-Aware graph attention network. We evaluate the performance of ADORE on a number of predictive analytics tasks using two EHR datasets. Our experimental results show that the medical concept embeddings obtained by ADORE can outperform the state-of-The-Art methods for all the tasks. More importantly, it can result in clinically meaningful sub-categorization of the existing ontological categories and yield attention values that can further enhance the model interpretability.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesElectronic health record; data mining with ontologies; predictive data analytics; representation learning,Data integration; Data mining; Embeddings; Integration; Predictive analytics; Additional key word and phraseselectronic health record; Data analytics; Data mining with ontology; Embeddings; Health records; Key words; Medical concepts; Ontology's; Predictive data analytic; Representation learning; Ontology
DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174902767&doi=10.1145%2f3610299&partnerID=40&md5=d488839e04635a069a9ba07d8d728903,"Neural Architecture Search (NAS) has shown promising capability in learning text representation. However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture nor encodes the latent hierarchical categorization behind text input. This article presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification. With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search. We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation. Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods. While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the candidates of NAS building blocks, its promising performance is noticeable and extensible to obtain further improvement by adding more different operations. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",differentiable neural architecture search; discretization; mutual information maximization; Neural architecture search; representation learning; text classification,Architecture; Gradient methods; Optimization; Text processing; Differentiable neural architecture search; Discretizations; Hierarchical categorization; Mutual information maximization; Neural architecture search; Neural architectures; Representation learning; Text classification; Text representation; Classification (of information)
"Fairness in Recommendation: Foundations, Methods, and Applications",2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196323568&doi=10.1145%2f3610302&partnerID=40&md5=03a68b2ba14ab6f8cff63cd8f1842773,"As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision-making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This survey focuses on the foundations for fairness in recommendation literature. It first presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking to provide a general overview of fairness research, as well as introduce the more complex situations and challenges that need to be considered when studying fairness in recommender systems. After that, the survey will introduce fairness in recommendation with a focus on the taxonomies of current fairness definitions, the typical techniques for improving fairness, as well as the datasets for fairness studies in recommendation. The survey also talks about the challenges and opportunities in fairness research with the hope of promoting the fair recommendation research area and beyond. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fairness; machine learning; recommender system,
Qrowdsmith: Enhancing Paid Microtask Crowdsourcing with Gamification and Furtherance Incentives,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174965421&doi=10.1145%2f3604940&partnerID=40&md5=9c22d1a39cd6f457291ab4386ba0b81b,"Microtask crowdsourcing platforms are social intelligence systems in which volunteers, called crowdworkers, complete small, repetitive tasks in return for a small fee. Beyond payments, task requesters are considering non-monetary incentives such as points, badges, and other gamified elements to increase performance and improve crowdworker experience. In this article, we present Qrowdsmith, a platform for gamifying microtask crowdsourcing. To design the system, we explore empirically a range of gamified and financial incentives and analyse their impact on how efficient, effective, and reliable the results are. To maintain participation over time and save costs, we propose furtherance incentives, which are offered to crowdworkers to encourage additional contributions in addition to the fee agreed upfront. In a series of controlled experiments, we find that while gamification can work as furtherance incentives, it impacts negatively on crowdworkers' performance, both in terms of the quantity and quality of work, as compared to a baseline where they can continue to contribute voluntarily. Gamified incentives are also less effective than paid bonus equivalents. Our results contribute to the understanding of how best to encourage engagement in microtask crowdsourcing activities and design better crowd intelligence systems. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Qrowdsmith: Enhancing paid microtask crowdsourcing with gamification and furtherance incentives,Crowdsourcing platforms; Financial analysis; Financial incentives; Gamification; Intelligence systems; Microtasks; Performance; Qrowdsmith: enhancing paid microtask crowdsourcing with gamification and furtherance incentive; Repetitive task; Social intelligence; Crowdsourcing
Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174891393&doi=10.1145%2f3616542&partnerID=40&md5=763c15f43a1297872f9fa79aaa1d57f0,"Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, such as nodes or edges, that the target GNN relies upon for making predictions. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph, which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: Several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and failing to provide consistent explanations. Applying them to explain weakly performed GNNs would further amplify these issues. To address this problem, we theoretically examine the predictions of GNNs from the causality perspective. Two typical reasons for spurious explanations are identified: confounding effect of latent variables like distribution shift and causal factors distinct from the original input. Observing that both confounding effects and diverse causal rationales are encoded in internal representations, we propose a new explanation framework with an auxiliary alignment loss, which is theoretically proven to be optimizing a more faithful explanation objective intrinsically. Concretely for this alignment loss, a set of different perspectives are explored: anchor-based alignment, distributional alignment based on Gaussian mixture models, mutual-information-based alignment, and so on. A comprehensive study is conducted both on the effectiveness of this new framework in terms of explanation faithfulness/consistency and on the advantages of these variants. For our codes, please refer to the following URL link: https://github.com/TianxiangZhao/GraphNNExplanation  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",explainable AI; Graph neural network; machine learning,Alignment; Data mining; Forecasting; Graph neural networks; Critical inputs; Explainable AI; Graph neural networks; Inductive bias; Internal representation; Latent variable; Level graphs; Machine-learning; Shift-and; Subgraphs; Machine learning
Argument Schemes and a Dialogue System for Explainable Planning,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174937534&doi=10.1145%2f3610301&partnerID=40&md5=29fb2ae5c40e2e074d27e27f6986f2e7,"Artificial Intelligence (AI) is being increasingly deployed in practical applications. However, there is a major concern whether AI systems will be trusted by humans. To establish trust in AI systems, there is a need for users to understand the reasoning behind their solutions. Therefore, systems should be able to explain and justify their output. Explainable AI Planning is a field that involves explaining the outputs, i.e., solution plans produced by AI planning systems to a user. The main goal of a plan explanation is to help humans understand reasoning behind the plans that are produced by the planners. In this article, we propose an argument scheme-based approach to provide explanations in the domain of AI planning. We present novel argument schemes to create arguments that explain a plan and its key elements and a set of critical questions that allow interaction between the arguments and enable the user to obtain further information regarding the key elements of the plan. Furthermore, we present a novel dialogue system using the argument schemes and critical questions for providing interactive dialectical explanations. © 2023 Copyright held by the owner/author(s).",Argument schemes; dialogue system; explanation; planning,Argument scheme; Artificial intelligence planning; Artificial intelligence systems; Critical questions; Dialogue systems; Explanation; Key elements; Planning systems; Speech processing
Disease Simulation in Airport Scenario Based on Individual Mobility Model,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174967174&doi=10.1145%2f3593589&partnerID=40&md5=64de9c43110249a1c39eaeca80a8709b,"As the rapid-spreading disease COVID-19 occupies the world, most governments adopt strict control policies to alleviate the impact of the virus. These policies successfully reduced the prevalence and delayed the epidemic peak, while they are also associated with high economic and social costs. To bridge the microscopic epidemic transmission patterns and control policies, simulation systems play an important role. In this work, we propose an agent-based disease simulator for indoor public spaces, which contribute to most of the transmission in cities. As an example, we study Guangzhou Baiyun International Airport, which is one of the most bustling aviation hubs in China. Specifically, we design a high-efficiency mobility generation module to reconstruct the individual trajectories considering both lingering behavior and crowd mobility, which greatly enhances the credibility of the simulated mobility and ensures real-time performance. Based on the individual trajectories, we propose a multi-path disease transmission module optimized for indoor public spaces, which includes three main transmission paths as close contact transmission, aerosol transmission, and object surface transmission. We design a novel convolution-based algorithm to mimic the diffusion process, which can leverage the high concurrent capability of the graphics processing unit to accelerate the simulation process. Leveraging our simulation paradigm, the effectiveness of common policy interventions can be quantitatively evaluated. For mobility interventions, we find that lingering control is the most effective mobility intervention with 32.35% fewer infections, while increasing social distance and increasing walking speed have a similar effect with 15.15% and 18.02% fewer infections. It demonstrates the importance of introducing crowd mobility into disease transmission simulation. For transmission processes, we find the aerosol transmission involves in 99.99% of transmission, which highlights the importance of ventilation in indoor public spaces. Our simulation also demonstrates that without strict entrance detection to identify the input infections, only performing frequent disinfection cannot achieve desirable epidemic outcomes. Based on our simulation paradigm, we can shed light on better policy designs that achieve a good balance between disease spreading control and social costs. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",COVID-19; Disease transmission modeling; mobility model; policy design; public space,Aerosols; Behavioral research; Computer graphics; Disease control; Graphics processing unit; Program processors; Viruses; Aerosol transmission; Control policy; Disease transmission; Disease transmission modeling; Mobility modeling; Policy design; Public space; Simulation paradigm; Social cost; Transmission model; COVID-19
Robust Location Prediction over Sparse Spatiotemporal Trajectory Data: Flashback to the Right Moment!,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174947233&doi=10.1145%2f3616541&partnerID=40&md5=040401fec03cb6f8e35e518697fae2a7,"As a fundamental problem in human mobility modeling, location prediction forecasts a user's next location based on historical user mobility trajectories. Recurrent neural networks (RNNs) have been widely used to capture sequential patterns of user visited locations for solving location prediction problems. Due to the sparse nature of real-world user mobility trajectories, existing techniques strive to improve RNNs by incorporating spatiotemporal contexts into the recurrent hidden state passing process of RNNs using context-parameterized transition matrices or gates. However, such a scheme mismatches universal spatiotemporal mobility laws and thus cannot fully benefit from rich spatiotemporal contexts encoded in user mobility trajectories. Against this background, we propose Flashback++, a general RNN architecture designed for modeling sparse user mobility trajectories. It not only leverages rich spatiotemporal contexts to search past hidden states with high predictive power but also learns to optimally combine them via a hidden state re-weighting mechanism, which significantly improves the robustness of the models against different settings and datasets. Our extensive evaluation compares Flashback++ against a sizable collection of state-of-the-art techniques on two real-world location-based social networks datasets and one on-campus mobility dataset. Results show that Flashback++ not only consistently and significantly outperforms all baseline techniques by 20.56% to 44.36% but also achieves better robustness of location prediction performance against different model settings (different RNN architectures and numbers of hidden states to flash back), different levels of trajectory sparsity, and different train-testing splitting ratios than baselines, yielding an improvement of 31.05% to 94.60%. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Location prediction; recurrent neural networks; sparse trajectory; user mobility,Forecasting; Location; Network architecture; Trajectories; Hidden state; Human mobility; Location prediction; Mobility modeling; Real-world; Recurrent neural network architectures; Sparse trajectory; Spatio-temporal trajectories; Trajectories datum; Users' mobility; Recurrent neural networks
Neural Architectures for Feature Embedding in Person Re-Identification: A Comparative View,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174937168&doi=10.1145%2f3610298&partnerID=40&md5=080a59defb95c2692c317b4de58c7ebb,"Solving Person Re-Identification (Re-Id) through Deep Convolutional Neural Networks is a daunting challenge due to the small size and variety of the training data, especially in Single-Shot Re-Id, where only two images per person are available. The lack of training data causes the overfitting of the deep neural models, leading to degenerated performance.This article explores a wide assortment of neural architectures that have been commonly used for object classification and analyzes their suitability in a Re-Id model. These architectures have been trained through a Triplet Model and evaluated over two challenging Single-Shot Re-Id datasets, PRID2011 and CUHK. This comparative study is aimed at obtaining the best-performing architectures and some concluding guidance to optimize the features embedding for the Re-Identification task. The obtained results present Inception-ResNet and DenseNet as potentially useful models, especially when compared with other methods, specifically designed for solving Re-Id. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep Convolutional Neural Network; neural architecture; Single-shot person re-identification; triplet loss,Convolution; Convolutional neural networks; Deep neural networks; Embeddings; Convolutional neural network; Deep convolutional neural network; Feature embedding; Neural architectures; Person re identifications; Re identifications; Single-shot; Single-shot person re-identification; Training data; Triplet loss; Network architecture
Local Self-attention-based Hybrid Multiple Instance Learning for Partial Spoof Speech Detection,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173044733&doi=10.1145%2f3616540&partnerID=40&md5=427f1d3d8f3367cdee27a08442ea4c97,"The development of speech synthesis technology has increased the attention toward the threat of spoofed speech. Although various high-performance spoofing countermeasures have been proposed in recent years, a particular scenario is overlooked: partially spoofed audio, where spoofed utterances may contain both spoofed and bona fide segments. Currently, the research on partially spoofed speech detection is lacking. The existing methods either train with partially spoofed speech at utterance level, resulting in gradient conflicting at the segment level, or directly train with segment level data, which requires segment labels that are difficult to obtain in practice. In this study, to better detect partially spoofed speech when only utterance labels are available, we formulate partially spoofed speech detection into a multiple instance learning (MIL) problem. The typical MIL uses a pooling layer to fuse patch scores as a whole, and we propose a hybrid MIL (H-MIL) framework based on max and log-sum-exp pooling methods, which can learn better segment representations to improve partially spoofed speech detection performance. Theoretical and experimental verification shows that H-MIL can effectively relieve the gradient conflicting and gradient vanishing problems. In addition, we analyze the local correlations between segments and introduce a local self-attention mechanism to enhance segment features, which further promotes the detection performance.In our experiments, we provide not only detection results at the segment and utterance levels but also some detailed visualization analysis, including the effect of spoof ratio and cross-dataset detection. The experimental results demonstrate the effective detection performance of our method at both the utterance and segment levels, especially when dealing with low spoof ratio attacks. The results confirm that our approach can better deal with partially spoofed speech detection than previous methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",gradient; hybrid; local self-attention; multiple instance learning; Partial spoof,Learning systems; Speech recognition; Detection performance; Gradient; Hybrid; Learning frameworks; Learning problem; Local self-attention; Multiple-instance learning; Partial spoof; Performance; Speech detection; Speech synthesis
Obfuscating the Dataset: Impacts and Applications,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165499137&doi=10.1145%2f3597936&partnerID=40&md5=332d58e93a79fc8a26074baedb5f9aa4,"Obfuscating a dataset by adding random noises to protect the privacy of sensitive samples in the training dataset is crucial to prevent data leakage to untrusted parties when dataset sharing is essential. We conduct comprehensive experiments to investigate how the dataset obfuscation can affect the resultant model weights - in terms of the model accuracy, ℓ2-distance-based model distance, and level of data privacy - and discuss the potential applications with the proposed Privacy, Utility, and Distinguishability (PUD)-triangle diagram to visualize the requirement preferences. Our experiments are based on the popular MNIST and CIFAR-10 datasets under both independent and identically distributed (IID) and non-IID settings. Significant results include a tradeoff between the model accuracy and privacy level and a tradeoff between the model difference and privacy level. The results indicate broad application prospects for training outsourcing and guarding against attacks in federated learning both of which have been increasingly attractive in many areas, particularly learning in edge computing. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data leakage; Data obfuscation; edge computing; federated learning; machine learning; privacy,Edge computing; Learning systems; Data leakage; Data obfuscation; Edge computing; Federated learning; Machine-learning; Modeling accuracy; Privacy; Privacy Levels; Random noise; Training dataset; Data obfuscation
"Few-shot Named Entity Recognition: Definition, Taxonomy and Research Directions",2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174921341&doi=10.1145%2f3609483&partnerID=40&md5=7c365ef3c4a51bd704f7dea79326d05a,"Recent years have seen an exponential growth (+98% in 2022 w.r.t. the previous year) of the number of research articles in the few-shot learning field, which aims at training machine learning models with extremely limited available data. The research interest toward few-shot learning systems for Named Entity Recognition (NER) is thus at the same time increasing. NER consists in identifying mentions of pre-defined entities from unstructured text, and serves as a fundamental step in many downstream tasks, such as the construction of Knowledge Graphs, or Question Answering. The need for a NER system able to be trained with few-annotated examples comes in all its urgency in domains where the annotation process requires time, knowledge and expertise (e.g., healthcare, finance, legal), and in low-resource languages. In this survey, starting from a clear definition and description of the few-shot NER (FS-NER) problem, we take stock of the current state-of-the-art and propose a taxonomy which divides algorithms in two macro-categories according to the underlying mechanisms: model-centric and data-centric. For each category, we line-up works as a story to show how the field is moving toward new research directions. Eventually, techniques, limitations, and key aspects are deeply analyzed to facilitate future studies. © 2023 Copyright held by the owner/author(s).",Few-shot learning; Named Entity Recognition,Knowledge graph; Learning systems; Natural language processing systems; Down-stream; Exponential growth; Few-shot learning; Learning fields; Machine learning models; Named entity recognition; Previous year; Research interests; Training machines; Unstructured texts; Taxonomies
Multi-agent Reinforcement Learning-based Adaptive Heterogeneous DAG Scheduling,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174910685&doi=10.1145%2f3610300&partnerID=40&md5=86f863af372f94a090784677b00c40ff,"Static scheduling of computational workflow represented by a directed acyclic graph (DAG) is an important problem in many areas of computer science. The main idea and novelty of the proposed algorithm is an adaptive heuristic or graph metric that uses a different heuristic rule at each scheduling step depending on local workflow. It is also important to note that multi-agent reinforcement learning is used to determine scheduling policy based on adaptive metrics. To prove the efficiency of the approach, a comparison with the state-of-the-art DAG scheduling algorithms is provided: DONF, CPOP, HCPT, HPS, and PETS. Based on the simulation results, the proposed algorithm shows an improvement of up to 30% on specific graph topologies and an average performance gain of 5.32%, compared to the best scheduling algorithm, DONF (suitable for large-scale scheduling), on a large number of random DAGs. Another important result is that using the proposed algorithm it was possible to cover 30.01% of the proximity interval from the best scheduling algorithm to the global optimal solution. This indicates that the idea of an adaptive metric for DAG scheduling is important and requires further research and development. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; directed acyclic graph; Multi-agent deep reinforcement learning; proximal policy optimization; scheduling,Deep learning; Directed graphs; Learning systems; Multi agent systems; Optimization; Scheduling algorithms; Acyclic graphs; Deep learning; Directed acyclic graph; Directed acyclic graph scheduling; Multi agent; Multi-agent deep reinforcement learning; Multi-agent reinforcement learning; Policy optimization; Proximal policy optimization; Reinforcement learnings; Reinforcement learning
Data-Aware Declarative Process Mining with SAT,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168807852&doi=10.1145%2f3600106&partnerID=40&md5=fa14b61182090698a909d356e52921b1,"Process Mining is a family of techniques for analyzing business process execution data recorded in event logs. Process models can be obtained as output of automated process discovery techniques or can be used as input of techniques for conformance checking or model enhancement. In Declarative Process Mining, process models are represented as sets of temporal constraints (instead of procedural descriptions where all control-flow details are explicitly modeled). An open research direction in Declarative Process Mining is whether multi-perspective specifications can be supported, i.e., specifications that not only describe the process behavior from the control-flow point of view, but also from other perspectives like data or time. In this article, we address this question by considering SAT (Propositional Satisfiability Problem) as a solving technology for a number of classical problems in Declarative Process Mining, namely, log generation, conformance checking, and temporal query checking. To do so, we first express each problem as a suitable FO (First-Order) theory whose bounded models represent solutions to the problem, and then find a bounded model of such theory by compilation into SAT.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesProcess mining; alloy; declarative models; multi-perspective models; SAT,Automation; Data mining; Process control; Additional key word and phrasesprocess mining; Conformance checking; Declarative modelling; Key words; Multi-perspective; Multi-perspective model; Perspective models; Process mining; Process-models; Propositional satisfiability problems; Specifications
Reinforcement Learning for Adaptive Video Compressive Sensing,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174933157&doi=10.1145%2f3608479&partnerID=40&md5=69f5aebda94b3f1c92c8a6743a038cd6,"We apply reinforcement learning to video compressive sensing to adapt the compression ratio. Specifically, video snapshot compressive imaging (SCI), which captures high-speed video using a low-speed camera is considered in this work, in which multiple (B) video frames can be reconstructed from a snapshot measurement. One research gap in previous studies is how to adapt B in the video SCI system for different scenes. In this article, we fill this gap utilizing reinforcement learning (RL). An RL model, as well as various convolutional neural networks for reconstruction, are learned to achieve adaptive sensing of video SCI systems. Furthermore, the performance of an object detection network using directly the video SCI measurements without reconstruction is also used to perform RL-based adaptive video compressive sensing. Our proposed adaptive SCI method can thus be implemented in low cost and real time. Our work takes the technology one step further towards real applications of video SCI. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",compressive sensing; Image processing; reinforcement learning,Convolutional neural networks; Deep learning; High speed cameras; Image compression; Image reconstruction; Object detection; Compressive imaging; Compressive sensing; High-sped video; Images processing; Low speed; Reinforcement learnings; Snapshot measurement; Speed cameras; Video frame; Video snapshots; Reinforcement learning
A Joint Entity and Relation Extraction Model based on Efficient Sampling and Explicit Interaction,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174926848&doi=10.1145%2f3604811&partnerID=40&md5=fb53d1b2f1c9e446cffe21eb3d299d3b,"Joint entity and relation extraction (RE) construct a framework for unifying entity recognition and relationship extraction, and the approach can exploit the dependencies between the two tasks to improve the performance of the task. However, the existing tasks still have the following two problems. First, when the model extracts entity information, the boundary is blurred. Secondly, there are mostly implicit interactions between modules, that is, the interactive information is hidden inside the model, and the implicit interactions are often insufficient in the degree of interaction and lack of interpretability. To this end, this study proposes a joint entity and relation extraction model (ESEI) based on Efficient Sampling and Explicit Interaction. We innovatively divide negative samples into sentences based on whether they overlap with positive samples, which improves the model's ability to extract entity word boundary information by controlling the sampling ratio. In order to increase the explicit interaction ability between the models, we introduce a heterogeneous graph neural network (GNN) into the model, which will serve as a bridge linking the entity recognition module and the relation extraction module, and enhance the interaction between the modules through information transfer. Our method substantially improves the model's discriminative power on entity extraction tasks and enhances the interaction between relation extraction tasks and entity extraction tasks. Experiments show that the method is effective, we validate our method on four datasets, and for joint entity and relation extraction, our model improves the F1 score on multiple datasets. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Efficient Sampling; entity recognition; Explicit Interaction; relation extraction,Extraction; Graph neural networks; Efficient sampling; Entity extractions; Entity recognition; Entity-relationship; Explicit interaction; Extraction modeling; Implicit interaction; Model-based OPC; Relation extraction; Relationship extraction; Data mining
Unsupervised Graph Representation Learning with Cluster-aware Self-training and Refining,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174920186&doi=10.1145%2f3608480&partnerID=40&md5=486fa7c21378858ca9e2622c2101bbb3,"Unsupervised graph representation learning aims to learn low-dimensional node embeddings without supervision while preserving graph topological structures and node attributive features. Previous Graph Neural Networks (GNN) require a large number of labeled nodes, which may not be accessible in real-world applications. To this end, we present a novel unsupervised graph neural network model with Cluster-aware Self-training and Refining (CLEAR). Specifically, in the proposed CLEAR model, we perform clustering on the node embeddings and update the model parameters by predicting the cluster assignments. To avoid degenerate solutions of clustering, we formulate the graph clustering problem as an optimal transport problem and leverage a balanced clustering strategy. Moreover, we observe that graphs often contain inter-class edges, which mislead the GNN model to aggregate noisy information from neighborhood nodes. Therefore, we propose to refine the graph topology by strengthening intra-class edges and reducing node connections between different classes based on cluster labels, which better preserves cluster structures in the embedding space. We conduct comprehensive experiments on two benchmark tasks using real-world datasets. The results demonstrate the superior performance of the proposed model over baseline methods. Notably, our model gains over 7% improvements in terms of accuracy on node clustering over state-of-the-arts. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cluster-aware self-training and refining; graph representation learning; unsupervised learning,Arts computing; Graph embeddings; Graph neural networks; Graph structures; Graph theory; Neural network models; Unsupervised learning; Cluster-aware self-training and refining; Clusterings; Embeddings; Graph neural networks; Graph representation; Graph representation learning; Learn+; Low dimensional; Neural network model; Self-training; Refining
Discovering Causes of Traffic Congestion via Deep Transfer Clustering,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174914170&doi=10.1145%2f3604810&partnerID=40&md5=215b43eec1cabf1bee782b5a45d979ab,"Traffic congestion incurs long delay in travel time, which seriously affects our daily travel experiences. Exploring why traffic congestion occurs is significantly important to effectively address the problem of traffic congestion and improve user experience. Traditional approaches to mine the congestion causes depend on human efforts, which is time consuming and cost-intensive. Hence, we aim at discovering the known and unknown causes of traffic congestion in a systematic way. However, to achieve it, there are three challenges: (1) traffic congestion is affected by several factors with complex spatio-temporal relations; (2) there are a few samples of congestion data with known causes due to the limitation of human label; (3) more unknown congestion causes are unexplored since several factors contribute to traffic congestion. To address above challenges, we design a congestion cause discovery system consisting of two modules: (1) congestion feature extraction module, which extracts the important features distinguishing between different causes of congestion; and (2) congestion cause discovery module, which designs a deep semi-supervised learning based framework to discover the causes of traffic congestion with limited labeled data. Specifically, in pre-training stage, it first leverages a few labeled data as prior knowledge to pre-train the model. Then, in clustering stage, we propose two different clustering methods to discover the congestion causes. For the first clustering method, we extend the classic deep embedded clustering model to produce clusters via soft assignment. For the second one, we iteratively use k-means to group the latent features extracted from the pre-trained model, and use the cluster results as pseudo-labels to fine-tune the network. Extensive experiments show that the performance of our methods is superior to the state-of-the-art baselines, which demonstrates the effectiveness of the proposed cause discovery system. Additionally, our system is deployed and used in the practical production environment at Amap. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",congestion causes; Traffic congestion; transfer clustering,Cluster analysis; Data mining; Deep learning; Iterative methods; K-means clustering; Motor transportation; Travel time; Clustering methods; Clusterings; Congestion cause; Discovery systems; Labeled data; Long delays; Transfer clustering; Travel experiences; Travel-time; Users' experiences; Traffic congestion
Federated Clique Percolation for Privacy-preserving Overlapping Community Detection,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168808742&doi=10.1145%2f3604807&partnerID=40&md5=dcbddf8afd43e472d0df1f2d8715dcf4,"Community structure is a typical characteristic of complex networks. Finding communities in complex networks has many important applications, such as the advertisement and recommendation based on social networks and the discovery of new protein molecules in biological networks, which make it a hot topic in the field of complex network analysis. With the increasing concerns about the leakage of personal privacy, discovering communities spread across the local networks owned by multiple participants accurately while preserving each participant's privacy has become an emerging challenge in distributed community detection. In this article, we propose a general federated graph learning model for privacy-preserving distributed graph learning and develop two federated clique percolation algorithms (CPAs) based on it to discover overlapping communities distributed across multiple participants' local networks without disclosing any participant's network privacy. Homomorphic encryption and hash operation are used in combination to protect the privacy of the vertices and edges of each local network. Furthermore, vertex attributes are involved in the calculation of clique similarity and clique percolation when dealing with attributed networks. The experimental results on real-world and artificial datasets demonstrate that the proposed algorithms achieve identical results to those of their stand-alone counterparts and more than 200% higher accuracy than the simple distributed CPAs without federating learning.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCommunity detection; clique percolation; federated learning; homomorphic encryption,Learning algorithms; Learning systems; Population dynamics; Privacy-preserving techniques; Solvents; Additional key word and phrasescommunity detection; Clique percolation; Community structures; Federated learning; Ho-momorphic encryptions; Homomorphic-encryptions; Key words; Local networks; Overlapping community detections; Privacy preserving; Complex networks
The Privacy Issue of Counterfactual Explanations: Explanation Linkage Attacks,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174925701&doi=10.1145%2f3608482&partnerID=40&md5=9da6bf8fc953a63ffbe5345f66c0b36f,"Black-box machine learning models are used in an increasing number of high-stakes domains, and this creates a growing need for Explainable AI (XAI). However, the use of XAI in machine learning introduces privacy risks, which currently remain largely unnoticed. Therefore, we explore the possibility of an explanation linkage attack, which can occur when deploying instance-based strategies to find counterfactual explanations. To counter such an attack, we propose k-anonymous counterfactual explanations and introduce pureness as a metric to evaluate the validity of these k-anonymous counterfactual explanations. Our results show that making the explanations, rather than the whole dataset, k-anonymous, is beneficial for the quality of the explanations. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",counterfactual explanations; Explainable AI; k-anonymity; machine learning; privacy,Data privacy; Black boxes; Counterfactual explanation; Counterfactuals; Explainable AI; K-Anonymity; Machine learning models; Machine-learning; Privacy; Privacy issue; Privacy risks; Machine learning
ReuseKNN: Neighborhood Reuse for Differentially Private KNN-Based Recommendations,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174536574&doi=10.1145%2f3608481&partnerID=40&md5=cbd62879060c2939e5323d80fb313843,"User-based KNN recommender systems (UserKNN) utilize the rating data of a target user's k nearest neighbors in the recommendation process. This, however, increases the privacy risk of the neighbors, since the recommendations could expose the neighbors' rating data to other users or malicious parties. To reduce this risk, existing work applies differential privacy by adding randomness to the neighbors' ratings, which unfortunately reduces the accuracy of UserKNN. In this work, we introduce ReuseKNN, a novel differentially private KNN-based recommender system. The main idea is to identify small but highly reusable neighborhoods so that (i) only a minimal set of users requires protection with differential privacy and (ii) most users do not need to be protected with differential privacy since they are only rarely exploited as neighbors. In our experiments on five diverse datasets, we make two key observations. Firstly, ReuseKNN requires significantly smaller neighborhoods and, thus, fewer neighbors need to be protected with differential privacy compared with traditional UserKNN. Secondly, despite the small neighborhoods, ReuseKNN outperforms UserKNN and a fully differentially private approach in terms of accuracy. Overall, ReuseKNN leads to significantly less privacy risk for users than in the case of UserKNN. © 2023 Copyright held by the owner/author(s).",collaborative filtering; differential privacy; k nearest neighbors; Neighborhood reuse; popularity bias; privacy risk; recommender systems,Collaborative filtering; Motion compensation; Nearest neighbor search; Differential privacies; Neighborhood reuse; Neighbourhood; Popularity bias; Privacy risks; Reuse; Recommender systems
Causal Feature Selection in the Presence of Sample Selection Bias,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174915451&doi=10.1145%2f3604809&partnerID=40&md5=e5f8a566931d7073cda92742b6229700,"Almost all existing causal feature selection methods are proposed without considering the problem of sample selection bias. However, in practice, as data-gathering process cannot be fully controlled, sample selection bias often occurs, leading to spurious correlations between features and the class variable, which seriously deteriorates the performance of those existing methods. In this article, we study the problem of causal feature selection under sample selection bias and propose a novel Progressive Causal Feature Selection (PCFS) algorithm which has three phases. First, PCFS learns the sample weights to balance the treated group and control group distributions corresponding to each feature for removing spurious correlations. Second, based on the sample weights, PCFS uses a weighted cross-entropy model to estimate the causal effect of each feature and removes some irrelevant features from the confounder set. Third, PCFS progressively repeats the first two phases to remove more irrelevant features and finally obtains a causal feature set. Using synthetic and real-world datasets, the experiments have validated the effectiveness of PCFS, in comparison with several state-of-the-art classical and causal feature selection methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",causal effect; Causal feature selection; sample selection bias,Causal effect; Causal feature selection; Correlation between features; Data gathering; Feature selection algorithm; Feature selection methods; Features selection; Performance; Sample selection bias; Samples weight; Feature Selection
HiGRN: A Hierarchical Graph Recurrent Network for Global Sea Surface Temperature Prediction,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168801992&doi=10.1145%2f3597937&partnerID=40&md5=4d3311881d96c011a3116c69ae81a83f,"Sea surface temperature (SST) is one critical parameter of global climate change, and accurate SST prediction is important to various applications, e.g., weather forecasting, fishing directions, and disaster warnings. The global ocean system is unified and complex, and the SST patterns in different oceanic regions are highly diverse and correlated. However, existing data-driven SST prediction methods mainly consider the local patterns within a certain oceanic region, e.g., El Nino region and the Black sea. It is challenging but necessary to model the global SST correlations rather than that in a specific region to enhance the prediction accuracy of SST. In this work, we proposed a new method called Hierarchical Graph Recurrent Network (HiGRN) to address the issue. First, to learn the dynamic and diverse local SST patterns of specific locations, we design an adaptive node embedding with self-learned parameters to learn various SST patterns. Then we develop a hierarchical cluster generator to aggregate the locations with similar patterns into regional clusters and utilize a graph convolution network to learn the spatial correlations among these clusters. Finally, we introduce a multi-level attention mechanism to fuse the local patterns and regional correlations, and the output is fed into a recurrent network to achieve SST predictions. Extensive experiments on two real-world datasets show that our method largely outperforms the state-of-the-art SST prediction methods. The source code is available at https://github.com/Neoyanghc/HiGRN.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSea surface temperature prediction; graph neural networks; hierarchical correlation; spatial-temporal modeling,Atmospheric temperature; Climate change; Climate models; Graph neural networks; Oceanography; Recurrent neural networks; Submarine geophysics; Surface waters; Weather forecasting; Additional key word and phrasessea surface temperature prediction; Graph neural networks; Hierarchical correlation; Hierarchical graphs; Key words; Recurrent networks; Sea surfaces; Spatial temporal model; Surface temperature prediction; Surface temperatures; Surface temperature
Graph Neural Rough Differential Equations for Traffic Forecasting,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168807170&doi=10.1145%2f3604808&partnerID=40&md5=50fc87975b97b64f5fbf960dfff788ee,"Traffic forecasting is one of the most popular spatio-temporal tasks in the field of machine learning. A prevalent approach in the field is to combine graph convolutional networks and recurrent neural networks for the spatio-temporal processing. There has been fierce competition and many novel methods have been proposed. In this article, we present the method of spatio-temporal graph neural rough differential equation (STG-NRDE). Neural rough differential equations (NRDEs) are a breakthrough concept for processing time-series data. Their main concept is to use the log-signature transform to convert a time-series sample into a relatively shorter series of feature vectors. We extend the concept and design two NRDEs: one for the temporal processing and the other for the spatial processing. After that, we combine them into a single framework. We conduct experiments with 6 benchmark datasets and 27 baselines. STG-NRDE shows the best accuracy in all cases, outperforming all those 27 baselines by non-trivial margins.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTraffic forecasting; graph neural network; neural rough differential equation; signature transform; spatio-temporal data,Data handling; Data mining; Differential equations; Graph neural networks; Recurrent neural networks; Time series; Additional key word and phrasestraffic forecasting; Graph neural networks; Key words; Neural rough differential equation; Rough differential equations; Signature transform; Spatio-temporal; Spatio-temporal data; Spatio-temporal graphs; Traffic Forecasting; Forecasting
You Are How You Use Apps: User Profiling Based on Spatiotemporal App Usage Behavior,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168803817&doi=10.1145%2f3597212&partnerID=40&md5=105036c70cc0b07fe4ddff06311447da,"Mobile apps have become an indispensable part of people's daily lives. Users determine what apps to use and when and where to use them based on their tastes, interests, and personal demands, depending on their personality traits. This article aims to infer user profiles from their spatiotemporal mobile app usage behavior. Specifically, we first transform mobile app usage records into a heterogeneous graph. On the graph, nodes represent users, apps, locations, and time slots. Edges describe the co-occurrence of entities in usage records. We then develop a multi-relational heterogeneous graph attention network (MRel-HGAN), an end-to-end system for user profiling. MRel-HGAN first adopts a neighbor sampling strategy based on bootstrapping to sample heavily connected neighbors of a fixed size for each node. Next, we design a relational graph convolutional operation and a multi-relational attention operation. Through such modules, MRel-HGAN can generate node embedding by sufficiently leveraging the rich semantic information of the multi-relational structure in the mobile app usage graph. Experimental results on real-world mobile app usage datasets show the effectiveness and superiority of our MRel-HGAN in the user profiling task for attributes of gender and age.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesMobile computing; graph neural networks; mobile app usage; user profiling,Graph theory; Semantics; User profile; Additional key word and phrasesmobile computing; Daily lives; Graph neural networks; Heterogeneous graph; Key words; Mobile app; Mobile app usage; Personality traits; User's profiles; User's profiling; Graph neural networks
MC2: Unsupervised Multiple Social Network Alignment,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168802661&doi=10.1145%2f3596514&partnerID=40&md5=70571db33c57aa6acd596262fd43353f,"Social network alignment, identifying social accounts of the same individual across different social networks, shows fundamental importance in a wide spectrum of applications, such as link prediction and information diffusion. Individuals more often than not join in multiple social networks, and it is in fact much too expensive or even impossible to acquiring supervision for guiding the alignment. To the best of our knowledge, few method in the literature can align multiple social networks without supervision. In this article, we propose to study the problem of unsupervised multiple social network alignment. To address this problem, we propose a novel unsupervised model of joint Matrix factorization with a diagonal Cone under orthogonal Constraint, referred to as MC2. Its core idea is to embed and align multiple social networks in the common subspace via an unsupervised approach. Specifically, in MC2 model, we first design a matrix optimization to infer the common subspace from different social networks. To address the nonconvex optimization, we then design an efficient alternating algorithm by leveraging its inherent functional property. Through extensive experiments on real-world datasets, we demonstrate that the proposed MC2 model significantly outperforms the state-of-the-art methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSocial network alignment; matrix factorization; unsupervised learning,Matrix algebra; Unsupervised learning; Additional key word and phrasessocial network alignment; AS-links; Information diffusion; Joint matrix factorizations; Key words; Link informations; Link prediction; Matrix factorizations; Network alignments; Wide spectrum; Matrix factorization
Real-time Road Network Optimization with Coordinated Reinforcement Learning,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168810122&doi=10.1145%2f3603379&partnerID=40&md5=b667f8d1533e71a05ad784d86995acee,"Dynamic road network optimization has been used for improving traffic flow in an infrequent and localized manner. The development of intelligent systems and technology provides an opportunity to improve the frequency and scale of dynamic road network optimization. However, such improvements are hindered by the high computational complexity of the existing algorithms that generate the optimization plans. We present a novel solution that integrates machine learning and road network optimization. Our solution consists of two complementary parts. The first part is an efficient algorithm that uses reinforcement learning to find the best road network configurations at real-time. The second part is a dynamic routing mechanism, which helps connected vehicles adapt to the change of the road network. Our extensive experimental results demonstrate that the proposed solution can substantially reduce the average travel time in a variety of scenarios, whilst being computationally efficient and hence applicable to real-life situations.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSpatial data management; autonomous vehicles; dynamic lane-reversal,Autonomous vehicles; Information management; Intelligent systems; Learning algorithms; Motor transportation; Roads and streets; Travel time; Additional key word and phrasesspatial data management; Autonomous Vehicles; Dynamic lane-reversal; Dynamic road networks; Key words; Real- time; Reinforcement learnings; Road network; Road network optimizations; Traffic flow; Reinforcement learning
Dual Graph Convolution Architecture Search for Travel Time Estimation,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168799345&doi=10.1145%2f3591361&partnerID=40&md5=a6e763ca4103544ba6e322c208a3fa22,"Travel time estimation (TTE) is a crucial task in intelligent transportation systems, which has been widely used in navigation and route planning. In recent years, several deep learning frameworks have been proposed to capture the dynamic features of road segments or intersections for travel time estimation. However, most existing works do not consider the joint features of the intersections and road segments. Moreover, most deep neural networks for TTE are designed based on empirical knowledge. Since the independent and joint features of intersections and road segments commonly vary with different datasets, the empirical deterministic neural architectures have limited adaptability to different scenarios. To tackle the above problems, we propose a novel automated deep learning framework, namely Automated Spatio-Temporal Dual Graph Convolutional Networks (Auto-STDGCN), for travel time estimation. Specifically, we propose to construct the node-wise graph and edge-wise graph to characterize the spatio-temporal features of intersections and road segments, respectively. In order to capture the joint spatio-temporal correlations of the dual graphs, a hierarchical neural architecture search approach is introduced, whose search space is composed of internal and external search space. In the internal search space, spatial graph convolution and temporal convolution operations are adopted to capture the respective spatio-temporal correlations of the dual graphs. Further, we design the external search space including the node-wise and edge-wise graph convolution operations from the internal architecture search to capture the interaction patterns between the intersections and road segments. We evaluate our proposed model Auto-STDGCN on three real-world datasets, which demonstrates that our model is significantly superior to the state-of-the-art methods. In addition, we also conduct case studies to visualize and explain the neural architectures learned by our model.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTravel time estimation; graph neural networks; neural architecture search; road modeling; spatio-temporal correlations,Deep neural networks; Graph neural networks; Graph theory; Intelligent systems; Network architecture; Roads and streets; Traffic control; Transportation routes; Travel time; Additional key word and phrasestravel time estimation; Dual graphs; Graph neural networks; Key words; Neural architecture search; Neural architectures; Road models; Spatiotemporal correlation; Time estimation; Travel time estimation; Convolution
Recent Few-shot Object Detection Algorithms: A Survey with Performance Comparison,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168804504&doi=10.1145%2f3593588&partnerID=40&md5=973a57491849aa03affa8f96daf41e5c,"The generic object detection (GOD) task has been successfully tackled by recent deep neural networks, trained by an avalanche of annotated training samples from some common classes. However, it is still non-trivial to generalize these object detectors to the novel long-tailed object classes, which have only few labeled training samples. To this end, the Few-Shot Object Detection (FSOD) has been topical recently, as it mimics the humans' ability of learning to learn and intelligently transfers the learned generic object knowledge from the common heavy-tailed to the novel long-tailed object classes. Especially, the research in this emerging field has been flourishing in recent years with various benchmarks, backbones, and methodologies proposed. To review these FSOD works, there are several insightful FSOD survey articles [58, 59, 74, 78] that systematically study and compare them as the groups of fine-tuning/transfer learning and meta-learning methods. In contrast, we review the existing FSOD algorithms from a new perspective under a new taxonomy based on their contributions, i.e., data-oriented, model-oriented, and algorithm-oriented. Thus, a comprehensive survey with performance comparison is conducted on recent achievements of FSOD. Furthermore, we also analyze the technical challenges, the merits and demerits of these methods, and envision the future directions of FSOD. Specifically, we give an overview of FSOD, including the problem definition, common datasets, and evaluation protocols. The taxonomy is then proposed that groups FSOD methods into three types. Following this taxonomy, we provide a systematic review of the advances in FSOD. Finally, further discussions on performance, challenges, and future directions are presented.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesFew-shot learning; meta-learning; survey,Deep neural networks; Learning systems; Object recognition; Sampling; Signal detection; Taxonomies; Additional key word and phrasesfew-shot learning; Detection tasks; Key words; Metalearning; Non-trivial; Object class; Object detection algorithms; Objects detection; Performance comparison; Training sample; Object detection
Joint Latent Space and Label Inference Estimation with Adaptive Fused Data and Label Graphs,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168810177&doi=10.1145%2f3590172&partnerID=40&md5=8750029211d53bc448e584d473f4b204,"Recently, structured computing has become an interesting topic in the world of artificial intelligence, especially in the field of machine learning, as most researchers focus on the development of graph-based semi-supervised learning models. In this article, we present a new framework for graph-based semi-supervised learning. We present a powerful method for simultaneous label inference and linear transform estimation. The targeted linear transformation is used to obtain a discriminant subspace. To improve semi-supervised learning, our framework focuses on exploiting the data structure and soft labels of the available unlabeled samples. In the iterative optimization scheme used, the prior estimation of the label increases the supervision information indirectly through an introduced informative matrix called the label graph, thus avoiding the use of hard confidence-based decisions as used in self-supervised methods. In addition, the estimation of labels and projected data is made more robust by using smoothing concepts based on hybrid graphs. For each type of smoothing, the hybrid graph is an adaptive fusion of the two graphs encoding the similarity of the data and the similarity of the labels. The proposed method leads to an improved discriminative linear transformation. Several experimental results on real image datasets confirm the effectiveness of the proposed method. They also show superior performance compared to semi-supervised methods that use integration and label inference simultaneously.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAuto-weighted graph fusion; discriminant embedding; image categorization; semi-supervised learning; soft labels; structured data,Graphic methods; Iterative methods; Learning algorithms; Learning systems; Supervised learning; Additional key word and phrasesauto-weighted graph fusion; Discriminant embedding; Graph-based; Hybrid graphs; Image Categorization; Key words; Semi-supervised learning; Soft labels; Structured data; Weighted graph; Linear transformations
Contrastive Learning Models for Sentence Representations,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168809469&doi=10.1145%2f3593590&partnerID=40&md5=98bdb85950ef65d0fbdbefab6dea8cb3,"Sentence representation learning is a crucial task in natural language processing, as the quality of learned representations directly influences downstream tasks, such as sentence classification and sentiment analysis. Transformer-based pretrained language models such as bidirectional encoder representations from transformers (BERT) have been extensively applied to various natural language processing tasks, and have exhibited moderately good performance. However, the anisotropy of the learned embedding space prevents BERT sentence embeddings from achieving good results in the semantic textual similarity tasks. It has been shown that contrastive learning can alleviate the anisotropy problem and significantly improve sentence representation performance. Therefore, there has been a surge in the development of models that utilize contrastive learning to fine-tune BERT-like pretrained language models to learn sentence representations. But no systematic review of contrastive learning models for sentence representations has been conducted. To fill this gap, this article summarizes and categorizes the contrastive learning based sentence representation models, common evaluation tasks for assessing the quality of learned representations, and future research directions. Furthermore, we select several representative models for exhaustive experiments to illustrate the quantitative improvement of various strategies on sentence representations.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSentence representation learning; BERT; contrastive learning; Data Augmentation,Computational linguistics; Embeddings; Quality control; Semantics; Sentiment analysis; Additional key word and phrasessentence representation learning; Bidirectional encoder representation from transformer; Contrastive learning; Data augmentation; Key words; Language model; Language processing; Learning models; Natural languages; Performance; Anisotropy
Deep Learning Inferencing with High-performance Hardware Accelerators,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168807681&doi=10.1145%2f3594221&partnerID=40&md5=b824290aaaf255b9770f0ccf41306cde,"As computer architectures continue to integrate application-specific hardware, it is critical to understand the relative performance of devices for maximum app acceleration. The goal of benchmarking suites, such as MLPerf for analyzing machine learning (ML) hardware performance, is to standardize a fair comparison of different hardware architectures. However, there are many apps that are not well represented by these standards that require different workloads, such as ML models and datasets, to achieve similar goals. Additionally, many apps, like real-time video processing, are focused on latency of computations rather than strictly on throughput. This research analyzes multiple compute architectures that feature ML-specific hardware on a case study of handwritten Chinese character recognition. Specifically, AlexNet and a custom version of GoogLeNet are benchmarked in terms of their streaming latency and maximum throughput for optical character recognition. Considering that these models are composed of fundamental neural network operations yet architecturally different from each other, these models can stress devices in different yet insightful ways that generalizations of the performance of other models can be drawn from. Many devices featuring ML-specific hardware and optimizations are analyzed including Intel and AMD CPUs, Xilinx and Intel FPGAs, NVIDIA GPUs, and Google TPUs. Overall, ML-oriented hardware added to the Intel Xeon CPUs helps to boost throughput by 3.7× and to reduce latency by up to 34.7×, which makes the latency of Intel Xeon CPUs competitive on more parallel models. The TPU devices were limited in terms of throughput due to large data transfer times and not competitive in terms of latency. The FPGA frameworks showcase the lowest latency on the Xilinx Alveo U200 FPGA achieving 0.48 ms on AlexNet using Mipsology Zebra and 0.39 ms on GoogLeNet using Vitis-AI. Through their custom acceleration datapaths coupled with high-performance SRAM, the FPGAs are able to keep critical model data closer to processing elements for lower latency. The massively parallel and high-memory GPU devices with Tensor Core accelerators achieve the best throughput. The NVIDIA Tesla A100 GPU showcases the highest throughput at 42,513 and 52,484 images/second for AlexNet and GoogLeNet, respectively.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNeural networks; FPGA; inference; machine learning,Benchmarking; Computer architecture; Data handling; Data transfer; Deep learning; Graphics processing unit; Learning systems; Network architecture; Optical character recognition; Program processors; Video signal processing; Additional key word and phrasesneural network; Application-specific hardware; Hardware accelerators; High-performance hardware; Inference; Key words; Low latency; Machine-learning; Performance; Specific hardware; Field programmable gate arrays (FPGA)
Noise-aware Local Model Training Mechanism for Federated Learning,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163281936&doi=10.1145%2f3591363&partnerID=40&md5=48dcef015029538bd2c6620581ee6b26,"As a new paradigm in training intelligent models, federated learning is widely used to train a global model without requiring local data to be uploaded from end devices. However, there are often mislabeled samples (i.e., noisy samples) in the dataset, which will cause the model update to deviate from the correct direction during the training process, thus reducing the convergence accuracy of the global model. Existing works employ noisy label correction techniques to reduce the impact of noisy samples on model updates by correcting labels; however, such methods necessitate the use of prior knowledge and additional communication costs, which cannot be directly applied to federated learning due to data privacy concerns and limited communication resources. Therefore, this paper proposes a noise-aware local model training method that corrects the noisy labels directly at the end device under the constraints of federated learning. By constructing a label correction model, a joint optimization problem is formally defined for optimizing both the label correction model and the client-side local training model (e.g., classification model). As a solution to this optimization problem, we propose a robustness training algorithm using label correction, along with a cross-validation data sampling algorithm that updates both models simultaneously. It is verified through experiments that the mechanism can effectively improve the model convergence accuracy on noisy datasets in federated learning scenarios.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFederated learning; label correction; meta learning; mislabeled samples; noise-aware,Learning systems; Optimization; Additional key word and phrasesfederated learning; End-devices; Global models; Key words; Label correction; Local model; Metalearning; Mislabeled sample; Model training; Noise-aware; Data privacy
Skin Lesion Intelligent Diagnosis in Edge Computing Networks: An FCL Approach,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168801014&doi=10.1145%2f3595186&partnerID=40&md5=39d745dbd756fe64862f9e48e5b1894d,"In recent years, automatic skin lesion diagnosis methods based on artificial intelligence have achieved great success. However, the lack of labeled data, visual similarity between skin diseases, and restriction on private data sharing remain the major challenges in skin lesion diagnosis. In this article, first, we propose a federated contrastive learning framework to break down data silos and enhance the generalizability of diagnostic model to unseen data. Subsequently, by combining data features from different participated nodes, the proposed framework can improve the performance of contrastive training. To extract discriminative features during on-device training, we propose a contrastive learning based intelligent skin lesion diagnosis scheme in edge computing networks. Specifically, a contrastive learning based dual encoder network is designed to overcome training sample scarcity by fully leveraging unlabeled samples for performance improvement. Meanwhile, we devise a maximum mean discrepancy based supervised contrastive loss function, which can efficiently explore complex intra-class and inter-class variances of samples. Finally, the diagnosis simulations demonstrate that compared with existing methods, our proposed scheme can achieve superior accuracy in both on-device training and distributed training scenarios.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesBiomedical system; contrastive learning; federated learning; skin lesion,Dermatology; Edge computing; Learning systems; Additional key word and phrasesbiomedical system; Contrastive learning; Diagnosis methods; Edge computing; Federated learning; Intelligent diagnosis; Key words; Labeled data; Performance; Skin lesion; Diagnosis
Configure Your Federation: Hierarchical Attention-enhanced Meta-Learning Network for Personalized Federated Learning,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168796842&doi=10.1145%2f3591362&partnerID=40&md5=560881a0899b10cef765f27308e23218,"Federated learning, as a distributed machine learning framework, enables clients to conduct model training without transmitting their data to the server, which is used to solve the dilemma of data silos and data privacy. It can work well on clients having similar data characteristics and distribution. However, it has some limitations where the dataset of clients may be different in distribution, quantity, and concept in many application scenarios. Personalized federated learning is a new federated learning paradigm that aims to guarantee client personalized models' effectiveness when collaborating with the cloud server. Intuitively, providing further facilitated collaborations for the clients with similar data characteristics and distribution can benefit personalized model building. However, due to the invisibility of client data, it is challenging to extract client characteristics and define collaborative relationships among them from a fine-grained view. Moreover, a reasonable collaborative training approach needs to be designed for a distributed server-client framework. In this article, we design a Hierarchical Attention-enhanced Meta-learning Network (HAM) to address this issue. The main advantage of HAM is that it utilizes the meta-learning approach of taking model parameters as features and learns to learn an extra model for each client to analyze similarities according to their local dataset automatically. According to its two-layers framework, HAM can reasonably achieve a tradeoff between clients' personality and commonality and provides a hybrid model with useful information from all clients. Considering there are two networks (HAM and base network) that need to learn for each client during the federated training process, we then provide an alternative learning approach to train them in an end-to-end fashion. To further clarify the approach, we describe the personalized federated learning settings framework as FedHAM where the HAM network is distributed deployed in each client. Extensive experiments based on two datasets prove that our method outperforms state-of-the-art baselines under different evaluation metrics.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDistributed computing; deep learning; federated learning,Deep learning; Distributed computer systems; Learning systems; Additional key word and phrasesdistributed computing; Data characteristics; Data distribution; Deep learning; Federated learning; Key words; Learn+; Learning network; Metalearning; Similar datum; Data privacy
Learning Representations of Satellite Imagery by Leveraging Point-of-Interests,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168799862&doi=10.1145%2f3589344&partnerID=40&md5=9d9a5b6800d9f531b7f256d0122a3e61,"Satellite imagery depicts the Earth's surface remotely and provides comprehensive information for many applications, such as land use monitoring and urban planning. Existing studies on unsupervised representation learning for satellite images only take into account the images' geographic information, ignoring human activity factors. To bridge this gap, we propose using the Point-of-Interest (POI) data to capture human factors and designing a contrastive learning-based framework to consolidate the representation of satellite imagery with POI information. Besides, we introduce a season-invariant representation learning model on satellite imagery, considering that human factors are mostly unchanging with respect to seasons. An attention model is designed at last to merge the representations from the geographic, seasonal, and POI perspectives adaptively. On the basis of real-world datasets collected from Beijing,1 we evaluate our method for predicting socioeconomic indicators. The results show that the representation containing POI information outperforms the geographic representation in estimating commercial activity-related indicators. Our proposed attentional framework can estimate the socioeconomic indicators with R2 of 0.874 and outperforms the baseline methods. Furthermore, we explore the differences in the representations of satellite images with varying socioeconomic statuses. Finally, we investigate the impact of geographic and POI perspective information in the representation learning process, as well as the effect of satellite imagery on various spatial resolutions.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesRepresentation learning; data mining; satellite imagery; socioeconomic indicator prediction,Bridges; Economics; Human engineering; Land use; Learning systems; Satellite imagery; Additional key word and phrasesrepresentation learning; Comprehensive information; Earth's surface; Geographic information; Geographics; Human activities; Key words; Satellite images; Socioeconomic indicator prediction; Socioeconomic indicators; Data mining
Towards Query-Efficient Black-Box Attacks: A Universal Dual Transferability-Based Framework,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168794901&doi=10.1145%2f3583777&partnerID=40&md5=94f86c95cd6784b8a292ac8312818d18,"Adversarial attacks have threatened the application of deep neural networks in security-sensitive scenarios. Most existing black-box attacks fool the target model by interacting with it many times and producing global perturbations. However, all pixels are not equally crucial to the target model; thus, indiscriminately treating all pixels will increase query overhead inevitably. In addition, existing black-box attacks take clean samples as start points, which also limits query efficiency. In this article, we propose a novel black-box attack framework, constructed on a strategy of dual transferability (DT), to perturb the discriminative areas of clean examples within limited queries. The first kind of transferability is the transferability of model interpretations. Based on this property, we identify the discriminative areas of clean samples for generating local perturbations. The second is the transferability of adversarial examples, which helps us to produce local pre-perturbations for further improving query efficiency. We achieve the two kinds of transferability through an independent auxiliary model and do not incur extra query overhead. After identifying discriminative areas and generating pre-perturbations, we use the pre-perturbed samples as better start points and further perturb them locally in a black-box manner to search the corresponding adversarial examples. The DT strategy is general; thus, the proposed framework can be applied to different types of black-box attacks. We conduct extensive experiments to show that, under various system settings, our framework can significantly improve the query efficiency of existing black-box attacks and attack success rates.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesBlack-box attack; model interpretation; query efficiency; transferability,Deep neural networks; Efficiency; Query processing; Additional key word and phrasesblack-box attack; Black boxes; Key words; Local perturbation; Model interpretations; Property; Query efficiency; Start point; Target model; Transferability; Pixels
Empirical Review of Various Thermography-based Computer-aided Diagnostic Systems for Multiple Diseases,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161314185&doi=10.1145%2f3583778&partnerID=40&md5=8253850c78e758680aa183e88ae03654,"The lifestyle led by today's generation and its negligence towards health is highly susceptible to various diseases. Developing countries are at a higher risk of mortality due to late-stage presentation, inaccessible diagnosis, and high-cost treatment. Thermography-based technology, aided with machine learning, for screening inflammation in the human body is non-invasive and cost-wise appropriate. It requires very little equipment, especially in rural areas with limited facilities. Recently, Thermography-based monitoring has been deployed worldwide at various organizations and public gathering points as a first measure of screening COVID-19 patients. In this article, we systematically compare the state-of-the-art feature extraction approaches for analyzing thermal patterns in the human body, individually and in combination, on a platform using three publicly available Datasets of medical thermal imaging, four Feature Selection methods, and four well-known Classifiers, and analyze the results. We developed and used a two-level sampling method for training and testing the classification model. Among all the combinations considered, the classification model with Unified Feature-Sets gave the best performance for all the datasets. Also, the experimental results show that the classification accuracy improves considerably with the use of feature selection methods. We obtained the best performance with a features subset of 45, 57, and 39 features (from Unified Feature Set) with a combination of mRMR and SVM for DB-DMR-IR and DB-FOOT-IR and a combination of ReF and RF for DB-THY-IR. Also, we found that for all the feature subsets, the features obtained are relevant, non-redundant, and distinguish normal and abnormal thermal patterns with the accuracy of 94.75% on the DB-DMR-IR dataset, 93.14% on the DB-FOOT-IR dataset, and 92.06% on the DB-THY-IR dataset.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Breast cancer; classification; diabetes; feature extraction; feature selection; medical thermography; thyroid cancer,Classification (of information); Computer aided diagnosis; Developing countries; Extraction; Feature Selection; Medical imaging; Support vector machines; Thermography (imaging); Breast Cancer; Classification models; Feature selection methods; Features extraction; Features selection; Features sets; Human bodies; Medical thermography; Thermal patterns; Thyroid cancers; Diseases
COMET: Convolutional Dimension Interaction for Collaborative Filtering,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168807957&doi=10.1145%2f3588576&partnerID=40&md5=a2aa8d45b13e610e3da5ca22f5621df8,"Representation learning-based recommendation models play a dominant role among recommendation techniques. However, most of the existing methods assume both historical interactions and embedding dimensions are independent of each other, and thus regrettably ignore the high-order interaction information among historical interactions and embedding dimensions. In this article, we propose a novel representation learning-based model called COMET (COnvolutional diMEnsion inTeraction), which simultaneously models the high-order interaction patterns among historical interactions and embedding dimensions. To be specific, COMET stacks the embeddings of historical interactions horizontally at first, which results in two ""embedding maps"". In this way, internal interactions and dimensional interactions can be exploited by convolutional neural networks (CNN) with kernels of different sizes simultaneously. A fully connected multi-layer perceptron (MLP) is then applied to obtain two interaction vectors. Lastly, the representations of users and items are enriched by the learnt interaction vectors, which can further be used to produce the final prediction. Extensive experiments and ablation studies on various public implicit feedback datasets clearly demonstrate the effectiveness and rationality of our proposed method.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRecommender systems; implicit feedback; interaction modeling; representation learning,Collaborative filtering; Convolution; User profile; Additional key word and phrasesrecommende system; Embedding dimensions; Embeddings; High-order; Higher-order; Implicit feedback; Interaction modeling; Key words; Recommendation techniques; Representation learning; Embeddings
Reinforced PU-learning with Hybrid Negative Sampling Strategies for Recommendation,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161336164&doi=10.1145%2f3582562&partnerID=40&md5=93a9915cef66c6735dbd2d367f9d9480,"The data of recommendation systems typically only contain the purchased item as positive data and other un-purchased items as unlabeled data. To train a good recommendation model, in addition to the known positive information, we also need high-quality negative information. Capturing negative signals in positive and unlabeled data is challenging for recommendation systems. Most studies have used specific data and proposed negative sampling methods suitable to the data characteristics. Existing negative sampling strategies cannot automatically select suitable approaches for different data. However, this one-size-fits-all strategy often makes potential positive samples considered as negative, or truly negative samples considered as potential positive samples and recommend to users. In this way, it will not only turn down the recommendation result, but even also have an adverse effect. Accordingly, we propose a novel negative sampling model, Reinforced PU-learning with Hybrid Negative Sampling Strategies for Recommendation (RHNSR), which can combine multiple sampling strategies and dynamically adjust the proportions used by different sampling strategies. In addition, ensemble learning, which integrates various model sampling strategies for obtaining an improved solution, was applied to RHNSR. Extensive experiments were conducted on three real-world recommendation datasets, and the experimental results indicated that the proposed model significantly outperformed state-of-the-art baseline models and revealed significant improvements in precision and hit ratio (49.02% and 37.41%, respectively).  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",hybrid negative sampling strategies; negative sampling; Positive-unlabeled learning,Learning systems; Reinforcement; Data characteristics; High quality; Hybrid negative sampling strategy; Negative information; Negative sampling; Positive data; Positive-unlabeled learning; Sampling method; Sampling strategies; Unlabeled data; Recommender systems
UrbanKG: An Urban Knowledge Graph System,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164431423&doi=10.1145%2f3588577&partnerID=40&md5=23e6dab3a24744813797b10ce64f1675,"Every day, our living city produces a tremendous amount of spatial-temporal data, involved with multiple sources from the individual scale to the city scale. Undoubtedly, such massive urban data can be explored for a better city and better life, as what the urban computing community has been dedicating in recent years. Nevertheless, existing studies are still facing the challenges of data fusion for the urban data as well as the knowledge distillation for specific applications. Moreover, there is a lack of full-featured and user-friendly platforms for both researchers and developers in the urban computing scenario. Therefore, in this article, we present UrbanKG, an urban knowledge graph system to incorporate a knowledge graph with urban computing. Specifically, the system introduces a complete scheme to construct a knowledge graph for urban data fusion. Built upon the data layer, the system further develops the multiple layers of construction, storage, algorithm, operation, and applications, which achieve knowledge distillation and support various functions to the users. We perform representative use cases and demonstrate the system capability of boosting performance in various downstream applications, indicating a promising research direction for knowledge-driven urban computing.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesUrban computing; intelligent system; knowledge graph,Data fusion; Digital storage; Distillation; Intelligent systems; Additional key word and phrasesurban computing; City scale; Computing community; Key words; Knowledge graphs; Multiple source; Spatial-temporal data; Urban computing; Urban data fusion; User-friendly platforms; Knowledge graph
Enough Waiting for the Couriers: Learning to Estimate Package Pick-up Arrival Time from Couriers' Spatial-Temporal Behaviors,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161285960&doi=10.1145%2f3582561&partnerID=40&md5=7f78fbce1119e18a9873e77a0217d29c,"In intelligent logistics systems, predicting the Estimated Time of Pick-up Arrival (ETPA) of packages is a crucial task, which aims to predict the courier's arrival time to all the unpicked-up packages at any time. Accurate prediction of ETPA can help systems alleviate customers' waiting anxiety and improve their experience. We identify three main challenges of this problem. First, unlike the travel time estimation problem in other fields like ride-hailing, the ETPA task is distinctively a multi-destination and path-free prediction problem. Second, an intuitive idea for solving ETPA is to predict the pick-up route and then the time in two stages. However, it is difficult to accurately and efficiently predict couriers' future routes in the route prediction step since their behaviors are affected by multiple complex factors. Third, furthermore, in the time prediction step, the requirement for providing a courier's all unpicked-up packages' ETPA at once in real time makes the problem even more challenging. To tackle the preceding challenges, we propose RankETPA, which integrates the route inference into the ETPA prediction. First, a learning-based pick-up route predictor is designed to learn the route-ranking strategies of couriers from their massive spatial-temporal behaviors. Then, a spatial-temporal attention-based arrival time predictor is designed for real-time ETPA inference via capturing the spatial-temporal correlations between the unpicked-up packages. Extensive experiments on two real-world datasets and a synthetic dataset demonstrate that RankETPA achieves significant performance improvement against the baseline models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep Neural Networks; package pick-up arrival time prediction; Trajectory,Forecasting; Pickups; Travel time; Accurate prediction; Arrival time; Arrival time predictions; Estimation problem; Help systems; Logistics system; Package pick-up arrival time prediction; Real- time; Spatial-temporal behavior; Travel time estimation; Deep neural networks
MaNIACS: Approximate Mining of Frequent Subgraph Patterns through Sampling,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161281868&doi=10.1145%2f3587254&partnerID=40&md5=816db3cd3ef7abc928020f400a7c561f,"We present MaNIACS, a sampling-based randomized algorithm for computing high-quality approximations of the collection of the subgraph patterns that are frequent in a single, large, vertex-labeled graph, according to the Minimum Node Image-based (MNI) frequency measure. The output of MaNIACS comes with strong probabilistic guarantees, obtained by using the empirical Vapnik-Chervonenkis (VC) dimension, a key concept from statistical learning theory, together with strong probabilistic tail bounds on the difference between the frequency of a pattern in the sample and its exact frequency. MaNIACS leverages properties of the MNI-frequency to aggressively prune the pattern search space, and thus to reduce the time spent in exploring subspaces that contain no frequent patterns. In turn, this pruning leads to better bounds to the maximum frequency estimation error, which leads to increased pruning, resulting in a beneficial feedback effect. The results of our experimental evaluation of MaNIACS on real graphs show that it returns high-quality collections of frequent patterns in large graphs up to two orders of magnitude faster than the exact algorithm.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Minimum Node Image; pattern mining; VC-dimension,Approximation algorithms; Computation theory; Data mining; Graph theory; Frequent subgraphs; High quality; Image-based; Minimum node image; Pattern mining; Randomized Algorithms; Sampling-based; Subgraphs; Vapnik-Chervonenkis dimensions; Vertex-labeled graphs; Frequency estimation
Hyper-Laplacian Regularized Multi-View Clustering with Exclusive L21 Regularization and Tensor Log-Determinant Minimization Approach,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161340409&doi=10.1145%2f3587034&partnerID=40&md5=43ba9977c0afbec18d1e2125d0e8e436,"Multi-view clustering aims to capture the multiple views inherent information by identifying the data clustering that reflects distinct features of datasets. Since there is a consensus in literature that different views of a dataset share a common latent structure, most existing multi-view subspace learning methods rely on the nuclear norm to seek the low-rank representation of the underlying subspace. However, the nuclear norm often fails to distinguish the variance of features for each cluster due to its convex nature and data tends to fall in multiple non-linear subspaces for multi-dimensional datasets. To address these problems, we propose a new and novel multi-view clustering method (HL-L21-TLD-MSC) that unifies the Hyper-Laplacian (HL) and exclusive ĝ.,""2,1 (L21) regularization with the Tensor Log-Determinant Rank Minimization (TLD) setting. Specifically, the hyper-Laplacian regularization maintains the local geometrical structure that makes the estimation prune to nonlinearities, and the mixed ĝ.,""2,1 and ĝ.,""1,2 regularization provides the joint sparsity within-cluster as well as the exclusive sparsity between-cluster. Furthermore, a log-determinant function is used as a tighter tensor rank approximation to discriminate the dimension of features. An efficient alternating algorithm is then derived to optimize the proposed model, and the construction of a convergent sequence to the Karush-Kuhn-Tucker (KKT) critical point solution is mathematically validated in detail. Extensive experiments are conducted on ten well-known datasets to demonstrate that the proposed approach outperforms the existing state-of-the-art approaches with various scenarios, in which, six of them achieve perfect results under our framework developed in this article, demonstrating highl effectiveness for the proposed approach.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","exclusive ĝ.,""<sub>1,2</sub>regularization; hyper-Laplacian; Multi-view clustering; tensor log-determinant function","Approximation theory; Cluster analysis; Clustering algorithms; Data mining; Laplace transforms; Learning systems; Data clustering; Exclusive ĝ,""1,2regularization; Hyper-laplacian; Laplacians; Latent structures; Minimisation; Multi-view clustering; Multiple views; Regularisation; Tensor log-determinant function; Tensors"
A Discriminant Information Theoretic Learning Framework for Multi-modal Feature Representation,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161336143&doi=10.1145%2f3587253&partnerID=40&md5=a5e84fa4061dd77c7e3c7f7e2a157802,"As sensory and computing technology advances, multi-modal features have been playing a central role in ubiquitously representing patterns and phenomena for effective information analysis and recognition. As a result, multi-modal feature representation is becoming a progressively significant direction of academic research and real applications. Nevertheless, numerous challenges remain ahead, especially in the joint utilization of discriminatory representations and complementary representations from multi-modal features. In this article, a discriminant information theoretic learning (DITL) framework is proposed to address these challenges. By employing this proposed framework, the discrimination and complementation within the given multi-modal features are exploited jointly, resulting in a high-quality feature representation. According to characteristics of the DITL framework, the newly generated feature representation is further optimized, leading to lower computational complexity and improved system performance. To demonstrate the effectiveness and generality of DITL, we conducted experiments on several recognition examples, including both static cases, such as handwritten digit recognition, face recognition, and object recognition, and dynamic cases, such as video-based human emotion recognition and action recognition. The results show that the proposed framework outperforms state-of-the-art algorithms.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",audio-visual emotion recognition; complementary representation; Discriminative representation; image recognition; information theoretic learning; multi-modal feature representation,Character recognition; Face recognition; Object recognition; Sensory analysis; Speech recognition; Audio-visual; Audio-visual emotion recognition; Complementary representation; Discriminant informations; Discriminative representation; Emotion recognition; Feature representation; Information-theoretic learning; Multi-modal; Multi-modal feature representation; Emotion Recognition
Fast Real-Time Video Object Segmentation with a Tangled Memory Network,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161290570&doi=10.1145%2f3585076&partnerID=40&md5=70fc9cf38e63ffc0208387af699571f7,"In this article, we present a fast real-time tangled memory network that segments the objects effectively and efficiently for semi-supervised video object segmentation (VOS). We propose a tangled reference encoder and a memory bank organization mechanism based on a state estimator to fully utilize the mask features and alleviate memory overhead and computational burden brought by the unlimited memory bank used in many memory-based methods. First, the tangled memory network exploits the mask features that uncover abundant object information like edges and contours but are not fully explored in existing methods. Specifically, a tangled two-stream reference encoder is designed to extract and fuse the features from both RGB frames and the predicted masks. Second, to indicate the quality of the predicted mask and feedback the online prediction state for organizing the memory bank, we devise a target state estimator to learn the IoU score between the predicted mask and ground truth. Moreover, to accelerate the forward process and avoid memory overflow, we use a memory bank of fixed size to store historical features by designing a new efficient memory bank organization mechanism based on the mask state score provided by the state estimator. We conduct comprehensive experiments on the public benchmarks DAVIS and YouTube-VOS, demonstrating that our method obtains competitive results while running at high speed (66 FPS on the DAVIS16-val set).  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",memory organization mechanism; state estimator; Tangled memory network; two stream,Image segmentation; Motion compensation; State estimation; Mask features; Mechanism-based; Memory banks; Memory network; Memory organization mechanism; Memory organizations; State Estimators; Tangled memory network; Two-stream; Video objects segmentations; Signal encoding
Toward Balancing the Efficiency and Effectiveness in k-Facility Relocation Problem,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161258453&doi=10.1145%2f3587039&partnerID=40&md5=46166bec0cb8d393bbc888e022314b42,"Facility Relocation (FR), which is an effort to reallocate the placement of facilities to adapt to the changes of urban planning, has remarkable impact on many areas. Existing solutions fail to guarantee the result quality on relocating k > 1 facilities. As k-FR problem is NP-complete and is not submodular or non-decreasing, traditional greedy algorithm cannot be directly applied. We propose to transform k-FR into another facility placement problem, which is submodular and non-decreasing. We prove that the optimal solutions of both problems are equivalent. Accordingly, we present the first approximate solution toward the k-FR, FR2FP. Our extensive comparison over both FR2FP and the state-of-the-art solution shows that FR2FP, although it provides approximation guarantee, cannot necessarily given superior results. The comparison motivates us to present an advanced approximate solution, FR2FP-ex. Moreover, based on Lagrangian relaxation, we develop an algorithm that can adjust the approximation ratio. Extensive experiments verified that, FR2FP-ex demonstrates the best result quality, and it is very close to the optimal solution. In addition, we also unveil the scenarios when the state-of-the-art would fail. We further generalize the k-FR problem, considering the budget for relocation and the cost of each facility. We also present corresponding approximate solutions toward the new problem and prove the approximation ratio.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximate algorithm; Facility Relocation; submodular,Approximation algorithms; Optimal systems; Optimization; Approximate algorithms; Approximate solution; Approximation ratios; Facility relocations; NP Complete; Optimal solutions; Relocation problem; Remarkable impact; State of the art; Submodular; Budget control
Modeling Within-Basket Auxiliary Item Recommendation with Matchability and Ubiquity,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161595188&doi=10.1145%2f3574157&partnerID=40&md5=b3f68d8ca8edf4a8ceff67a615762897,"Within-basket recommendation is to recommend suitable items for the current basket with some already known items. The within-basket auxiliary item recommendation (WBAIR) is to recommend auxiliary items based on the primary items in the basket. Such a task exists in many real-life scenarios. Unlike the associations between items that can be transmitted in both directions, primary and auxiliary relationships are unidirectional. Then, the suitable matching patterns between primary and auxiliary items cannot be explored by traditional directionless methods. Therefore, we design the Matc4Rec algorithm to integrate the primary and auxiliary factors, and finally recommend items that not only match the interests of users but also satisfy the primary and auxiliary relationships between items. Specifically, we capture the pattern from three aspects: matchability within-basket, matchability between baskets, and ubiquity. By exploiting this pattern, the designed algorithm not only achieves good results on real-world datasets but also improves the interpretability of recommendations. As a result, we can know which commodities are suitable as auxiliary items. The experiment results demonstrate that our algorithm can also alleviate the cold start problem.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",auxiliary item; Basket recommendation; representation learning,Recommender systems; 'current; Auxiliary item; Basket recommendation; Cold start problems; Interpretability; Item-based; Known items; Matching patterns; Real-world datasets; Representation learning; User profile
Fully Linear Graph Convolutional Networks for Semi-Supervised and Unsupervised Classification,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161533369&doi=10.1145%2f3579828&partnerID=40&md5=7e493fd65dbee15e4f88106e50147254,"This article presents FLGC, a simple yet effective fully linear graph convolutional network for semi-supervised and unsupervised learning. Instead of using gradient descent, we train FLGC based on computing a global optimal closed-form solution with a decoupled procedure, resulting in a generalized linear framework and making it easier to implement, train, and apply. We show that (1) FLGC is powerful to deal with both graph-structured data and regular data, (2) training graph convolutional models with closed-form solutions improve computational efficiency without degrading performance, and (3) FLGC acts as a natural generalization of classic linear models in the non-Euclidean domain (e.g., ridge regression and subspace clustering). Furthermore, we implement a semi-supervised FLGC and an unsupervised FLGC by introducing an initial residual strategy, enabling FLGC to aggregate long-range neighborhoods and alleviate over-smoothing. We compare our semi-supervised and unsupervised FLGCs against many state-of-the-art methods on a variety of classification and clustering benchmarks, demonstrating that the proposed FLGC models consistently outperform previous methods in terms of accuracy, robustness, and learning efficiency. The core code of our FLGC is released at https://github.com/AngryCai/FLGC.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",closed-form solution; Graph Convolutional Networks; linear model; Semi-Supervised Learning; subspace clustering,Clustering algorithms; Convolution; Convolutional codes; Gradient methods; Learning systems; Linear networks; Linear systems; Machine learning; Regression analysis; Closed form solutions; Convolutional networks; Graph convolutional network; Linear graph; Linear modeling; Semi-supervised; Semi-supervised learning; Semisupervised classification (SSC); Subspace clustering; Supervised and unsupervised classification; Computational efficiency
Robust Dimensionality Reduction via Low-rank Laplacian Graph Learning,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150930586&doi=10.1145%2f3582698&partnerID=40&md5=3ec65a2d37c98ce79031da63e07ecaa5,"Manifold learning is a widely used technique for dimensionality reduction as it can reveal the intrinsic geometric structure of data. However, its performance decreases drastically when data samples are contaminated by heavy noise or occlusions, which leads to unsatisfying data processing performance. We propose a novel robust dimensionality reduction method via low-rank Laplacian graph learning for classification and clustering tasks to solve the above problem. First, we construct a low-rank Laplacian graph by combining manifold learning and subspace learning. This graph can capture both global and local structural information of the data. And we introduce rank constraints for the Laplacian graph to make it more discriminative. Second, we put the learning of projection matrix and sample affinity graph into a unified framework. The projection matrix is embedded into a robust low-rank Laplacian graph so that the low-dimensional mapping of data can maintain the structural information in the graph well. Finally, we add a regularization term to the projection matrix to make it have the ability of both feature extraction and feature selection. Therefore, the proposed model can resist the interference of noise or data damage to learn the optimal projection to achieve better performance in dimensionality reduction through such a data dimensionality reduction joint framework. Comprehensive experiments on various benchmark datasets with varying degrees of occlusions or corruptions are carried out to evaluate the performance of the proposed method. Compared with the state-of-the-art dimensionality reduction methods in the literature, the experimental results are inspiring, showing our method's effectiveness and robustness in classification and clustering, especially in object recognition scenarios with noise or occlusions.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Dimensionality reduction; feature extraction; Laplacian graph learning; low rank representation,Benchmarking; Data mining; Data reduction; Extraction; Laplace transforms; Matrix algebra; Object recognition; Reduction; Classification and clustering; Dimensionality reduction; Dimensionality reduction method; Features extraction; Laplacian graph learning; Laplacians; Low-rank representations; Manifold learning; Performance; Projection matrix; Feature extraction
Hybrid Representation and Decision Fusion towards Visual-textual Sentiment,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161269104&doi=10.1145%2f3583076&partnerID=40&md5=b927aba6930c0f33039a0d6f9f677a87,"The rising use of online media has changed social customs of the public. Users have become gradually accustomed to sharing daily experiences and publishing personal opinions on social networks. Social data carrying with emotions and attitudes have provided significant decision support for numerous tasks in sentiment analysis. Conventional sentiment analysis methods only concern about textual modality and are vulnerable to the multimodal scenario, while common multimodal approaches only focus on the interactive relationship between modalities without considering unique intra-modal information. A hybrid fusion network is proposed in this work to capture both the inter-modal and intra-modal features. First, in the intermediate fusion stage, a multi-head visual attention is proposed to extract accurate semantic and sentimental information from textual embedding representations with the assistance of visual features. Then, multiple base classifiers are trained to learn independent and diverse discriminative information from different modal representations in the late fusion stage. The final decision is determined based on fusing the decision supports from base classifiers via a decision fusion method. To improve the generalization of our hybrid fusion network, a similarity loss is employed to inject decision diversity into the whole model. Empirical results on multimodal datasets have demonstrated the proposed model achieves a higher accuracy and better generalization compared with baselines for multimodal sentiment analysis.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Decision fusion; multimodal; representation fusion; social network,Behavioral research; Classification (of information); Decision support systems; Modal analysis; Sentiment analysis; Decision supports; Decisions fusion; Generalisation; Hybrid fusions; Hybrid representations; Multi-modal; Online medium; Representation fusion; Sentiment analysis; Social network; Semantics
Reinforced Explainable Knowledge Concept Recommendation in MOOCs,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161317596&doi=10.1145%2f3579991&partnerID=40&md5=0a543726e71ac9c13c26b7434d6acb27,"In this article, we study knowledge concept recommendation in Massive Open Online Courses (MOOCs) in an explainable manner. Knowledge concepts, composing course units (e.g., videos) in MOOCs, refer to topics and skills that students are expected to master. Compared to traditional course recommendation in MOOCs, knowledge concepts recommendation has drawn more attention because students' interests over knowledge concepts can better revealstudents' real intention in a more refined granularity. However, there are three unique challenges in knowledge concept recommendation: (1) How to design an appropriate data structure to capture complex relationships between knowledge concepts, course units, and other participants (e.g., students, teachers)? (2) How to model interactions between students and knowledge concepts? (3) How to make explainable recommendation results to students? To tackle these challenges, we formulate the knowledge concept recommendation as a reinforcement learning task integrated with MOOC knowledge graph (KG). Specifically, we first construct MOOC KG as the environment to capture all the relationships and behavioral histories by considering all the entities (e.g., students, teachers, videos, courses, and knowledge concepts) on the MOOC provider. Then, to model the interactions between students and knowledge concepts, we train an agent to mimic students' learning behavioral patterns facing the complex environment. Moreover, to provide explainable recommendation results, we generate recommended knowledge concepts in the format of a path from MOOC KG to indicate semantic reasons. Finally, we conduct extensive experiments on a real-world MOOC dataset to demonstrate the effectiveness of our proposed method.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",knowledge concept recommendation; MOOC knowledge graph; Reinforcement learning,Curricula; Knowledge graph; Reinforcement; Semantics; Students; Teaching; Complex relationships; Knowledge concept recommendation; Knowledge graphs; Massive open online course; Massive open online course knowledge graph; Model interaction; Reinforcement learnings; Student teachers; Students' interests; Traditional course; Reinforcement learning
Saliency Attack: Towards Imperceptible Black-box Adversarial Attack,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161311539&doi=10.1145%2f3582563&partnerID=40&md5=74354493b67cf0d820d804b070655068,"Deep neural networks are vulnerable to adversarial examples, even in the black-box setting where the attacker is only accessible to the model output. Recent studies have devised effective black-box attacks with high query efficiency. However, such performance is often accompanied by compromises in attack imperceptibility, hindering the practical use of these approaches. In this article, we propose to restrict the perturbations to a small salient region to generate adversarial examples that can hardly be perceived. This approach is readily compatible with many existing black-box attacks and can significantly improve their imperceptibility with little degradation in attack success rates. Furthermore, we propose the Saliency Attack, a new black-box attack aiming to refine the perturbations in the salient region to achieve even better imperceptibility. Extensive experiments show that compared to the state-of-the-art black-box attacks, our approach achieves much better imperceptibility scores, including most apparent distortion (MAD), L0 and L2 distances, and also obtains significantly better true success rate and effective query number judged by a human-like threshold on MAD. Importantly, the perturbations generated by our approach are interpretable to some extent. Finally, it is also demonstrated to be robust to different detection-based defenses.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial example; black-box adversarial attack; deep neural networks; saliency map,Adversarial example; Black boxes; Black-box adversarial attack; Model outputs; Performance; Practical use; Query efficiency; Saliency map; Salient regions; State of the art; Deep neural networks
Customer Volume Prediction Using Fusion of Shared-private Dynamic Weighting over Multiple Modalities,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161540027&doi=10.1145%2f3579826&partnerID=40&md5=540a820277f156db02eb1f687f40708b,"Customer volume prediction is crucial for a variety of urban applications, such as store location selection. So far, the key challenge lies in how to fuse multiple modalities from different data sources, on account of the massive amount of data accessible, for example, spatio-temporal data and satellite images. In this article, we investigate three dynamic weighting ensemble learning models to fuse spatio-temporal features and visual features for predicting customer volume in the urban commercial district of interest. Specifically, we propose the shared-private dynamic weighting model by incorporating graph neural networks, which is proposed to capture geographic dependencies (i.e., competitiveness or dependencies) between urban commercial districts in an end-to-end manner. To the best of our knowledge, it is the first work to utilize graph neural networks to model such geographic relationships. We conduct a series of experiments to demonstrate the effectiveness of the proposed models based on two real datasets. Furthermore, an elaborated visualization method is performed for knowledge discovery.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic weighting; ensemble learning; Multimodal fusion,Data mining; Graph neural networks; Sales; Data-source; Dynamic weighting; Ensemble learning; Graph neural networks; Location selection; Multi-modal fusion; Multiple modalities; Spatio-temporal data; Urban applications; Volume predictions; Forecasting
TreeSketchNet: From Sketch to 3D Tree Parameters Generation,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153764081&doi=10.1145%2f3579831&partnerID=40&md5=84895633bc40b999f8f67d64a85396ff,"Three-dimensional (3D) modeling of non-linear objects from stylized sketches is a challenge even for computer graphics experts. The extrapolation of object parameters from a stylized sketch is a very complex and cumbersome task. In the present study, we propose a broker system that can transform a stylized sketch of a tree into a complete 3D model by mediating between a modeler and a 3D modeling software. The input sketches do not need to be accurate or detailed: They must only contain a rudimentary outline of the tree that the modeler wishes to 3D model. Our approach is based on a well-defined Deep Neural Network architecture, called TreeSketchNet (TSN), based on convolutions and capable of generating Weber and Penn [1995] parameters from a simple sketch of a tree. These parameters are then interpreted by the modeling software, which generates the 3D model of the tree pictured in the sketch. The training dataset consists of synthetically generated sketches that are associated with Weber-Penn parameters, generated by a dedicated Blender modeling software add-on. The accuracy of the proposed method is demonstrated by testing the TSN with synthetic and hand-made sketches. Finally, we provide a qualitative analysis of our results, by evaluating the coherence of the predicted parameters with several distinguishing features.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D generation; 3D trees; datasets; deep learning; neural networks; Procedural modeling,Blending; Deep neural networks; Interactive computer graphics; Network architecture; Three dimensional computer graphics; 3d generation; 3D models; 3D tree; 3d-modeling; Dataset; Deep learning; Modeling softwares; Neural-networks; Procedural models; Three-dimensional (3D) model; 3D modeling
Ontology-Based Driving Simulation for Traffic Lights Optimization,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161300919&doi=10.1145%2f3579839&partnerID=40&md5=32027699eeee6f897a8f50783d6b1a56,"Traffic lights optimization is one of the principal components to lessen the traffic flow and travel time in an urban area. The present article seeks to introduce a novel procedure to design the traffic lights in a city using evolutionary-based optimization algorithms in combination with an ontology-based driving behavior simulation framework. Accordingly, an ontology-based knowledge base is introduced to provide a machine-understandable knowledge of roads and intersections, traffic rules, and driving behaviors. Then, a simulation environment is developed to inspect car behavior in real time. To optimize the traffic lights, a sine-based equation was defined for each traffic light, and the total travel time of the vehicles was considered as the cost function in the optimization algorithm. The optimization was performed with 5, 10, 15, 20, 25, and 30 vehicles in the urban areas. Based on the results, in contrast to uncontrolled intersections without traffic lights, optimized traffic lights can significantly contribute to total travel time-saving. To conclude, due to an escalation in the number of vehicles, the significance of optimized traffic lights has encountered an increase, and unoptimized traffic lights could increase total travel time even more than a city deprived of any traffic light.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",autonomous car; driving behavior; Evolutionary optimization; knowledge representation; ontology; traffic light,Autonomous vehicles; Cost functions; Evolutionary algorithms; Ontology; Optimization; Travel time; Autonomous car; Driving behaviour; Evolutionary optimizations; Knowledge-representation; Ontology's; Ontology-based; Optimisations; Traffic light; Travel-time; Urban areas; Knowledge representation
Representation Learning of Enhanced Graphs Using Random Walk Graph Convolutional Network,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161298074&doi=10.1145%2f3582841&partnerID=40&md5=c4c9c1d064ea4a2078a48e7f129a3bdc,"Nowadays, graph structure data has played a key role in machine learning because of its simple topological structure, and therefore, the graph representation learning methods have attracted great attention. And it turns out that the low-dimensional embedding representation obtained by graph representation learning is extremely useful in various typical tasks, such as node classification and content recommendation. However, most of the existing methods do not further dig out potential structural information on the original graph structure. Here, we propose wGCN, which utilizes random walk to obtain the node-specific mesoscopic structures (high-order local structure) of the graph and utilizes these mesoscopic structures to enhance the graph and organize the characteristic information of the nodes. Our method can effectively generate node embedding for data of previously unknown categories, which has been proven in a series of experiments conducted on many types of graph networks. And compared to baselines, our method shows the best performance on most datasets and achieves competitive results on others. It is believed that combining the mesoscopic structure to further explore the structural information of the graph will greatly improve the learning efficiency of the graph neural network.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",enhanced graph; graph neural network; node classification; Representation learning,Classification (of information); Graph embeddings; Graph structures; Graph theory; Graphic methods; Random processes; Superconducting materials; Convolutional networks; Enhanced graph; Graph neural networks; Graph representation; Machine-learning; Mesoscopic structure; Node classification; Random Walk; Representation learning; Structural information; Graph neural networks
A Semantically Driven Hybrid Network for Unsupervised Entity Alignment,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151852084&doi=10.1145%2f3567829&partnerID=40&md5=154a47409769116ed23848e06e375be1,"The major challenge in the task of entity alignment (EA) lies in the heterogeneity of the knowledge graph. The traditional solution to EA is to first map entities to the same space via knowledge embedding and then calculate the similarity between entities from different knowledge graphs. However, these methods mainly rely on manually labeled seeds of EA, which limits their applicability. Some researchers have begun using pseudo-labels rather than seeds for unsupervised EA. However, directly using pseudo-labels causes new problems, such as noise in the pseudo-labels. In this article, we propose a model called the Semantically Driven Hybrid Network (SDHN) to reduce the impact of noise in the pseudo-labels on the performance of EA models. The SDHN consists of two modules: a Teacher-Student Network (TSN) and a Rotation and Penalty (RAP) module. The TSN module reduces the impact of noise in two ways: (1) The TSN's teacher network guides its student network to construct pseudo-labels based on semantic information instead of directly creating pseudo-labels. (2) It adaptively fuses semantic information into student networks to improve the final representation of entity embedding. Finally, the TSN enhances the performance of models of entity alignment via the RAP module. The results of experiments on multiple benchmark datasets showed that the SDHN outperforms state-of-the-art models.  © 2023 Association for Computing Machinery.",entity alignment; graph neural networks; Knowledge graph,Embeddings; Graph neural networks; Semantic Web; Semantics; Students; Entity alignment; Graph neural networks; Hybrid network; Impact of noise; Knowledge embedding; Knowledge graphs; Performance; Semantics Information; Student network; Teachers'; Knowledge graph
Reinforcement Learning for Quantitative Trading,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153110220&doi=10.1145%2f3582560&partnerID=40&md5=fee3bdec13fcdd12d8c0bd4f54b3c142,"Quantitative trading (QT), which refers to the usage of mathematical models and data-driven techniques in analyzing the financial market, has been a popular topic in both academia and financial industry since 1970s. In the last decade, reinforcement learning (RL) has garnered significant interest in many domains such as robotics and video games, owing to its outstanding ability on solving complex sequential decision making problems. RL's impact is pervasive, recently demonstrating its ability to conquer many challenging QT tasks. It is a flourishing research direction to explore RL techniques' potential on QT tasks. This paper aims at providing a comprehensive survey of research efforts on RL-based methods for QT tasks. More concretely, we devise a taxonomy of RL-based QT models, along with a comprehensive summary of the state of the art. Finally, we discuss current challenges and propose future research directions in this exciting field.  © 2023 Copyright held by the owner/author(s).",quantitative finance; Reinforcement learning; stock market; survey,Commerce; Decision making; Electronic trading; Financial markets; Learning systems; Data driven technique; Decision-making problem; Financial industry; Model-driven techniques; Quantitative finance; Reinforcement learning techniques; Reinforcement learnings; Research efforts; Sequential decision making; Video-games; Reinforcement learning
A System for Automated Industrial Test Laboratory Scheduling,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149978861&doi=10.1145%2f3546871&partnerID=40&md5=6b769cf421b347c79e02c5a0d0eb7f53,"Automated scheduling solutions are tremendously important for the efficient operation of industrial laboratories. The Test Laboratory Scheduling Problem (TLSP) is an extension of the well-known Resource Constrained Project Scheduling Problem (RCPSP) and captures the specific requirements of such laboratories. In addition to several new scheduling constraints, it features a grouping phase, where the jobs to be scheduled are assembled from smaller units. In this work, we introduce an innovative scheduling system that allows the efficient and flexible generation of schedules for TLSP. It features a new Constraint Programming model that covers both the grouping and the scheduling aspect, as well as a hybrid Very Large Neighborhood Search that internally uses the CP model. Our experimental results on generated and real-world benchmark instances show that good results can be obtained even compared to settings which have a good grouping already provided, including several new best known solutions for these instances. Our algorithms for TLSP have been successfully implemented in a real-world industrial test laboratory. We provide a detailed description of the deployed system as well as additional useful soft constraints supported by the solvers and general lessons learned. This includes a discussion of the choice of soft constraint weights, with an analysis on the impact and relation of different objectives to each other. Our experiments show that some soft constraints complement each other well, while others require explicit trade-offs via their relative weights.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",constraint programming; RCPSP; Test laboratory scheduling; very large neighborhood search,Benchmarking; Constraint programming; Economic and social effects; Industrial laboratories; Optimization; Constraint programming; Industrial tests; Large neighbourhood searches; Real-world; Resource-constrained project scheduling problem; Scheduling constraints; Scheduling problem; Soft constraint; Test laboratory scheduling; Very large neighborhood search; Constraint theory
Interior Individual Trajectory Simulation with Population Distribution Constraint,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149993423&doi=10.1145%2f3529108&partnerID=40&md5=7d5ae50afccf0a28090fe7338d11ce50,"Individual trajectory generation plays an important role in simulation tasks, reconstructing fine-grained mobility behaviors that can be used to evaluate epidemic risks, congestion risks, or commercial profit. Previous research works adopt the Newton's mechanic-based particle model as their core algorithm, such as the Social Force model. However, real-world human mobility behaviors hardly follow the particle models, especially in the interior scenes where interactions between pedestrians and environments matter. In this article, we propose a Social Force-based trajectory simulator for interior scenarios that improve both trajectory quality and generation speed for interior scenarios. First, we introduce prior scene knowledge to guide the generation process, where pedestrians are armed with exploration behaviors that follow the group-level distribution. It provides more flexibility to simulate complicated human behaviors rather than straight-line movements, generating high-quality individual trajectories. Experiments show that the correlation between the aggregated population distribution of generated trajectories and ground-truth distribution is improved by 11.84% by our method. Second, we optimize the algorithm procedure by introducing a caching mechanism for tenderized intermediate values, along with graph-processing-unit-based implementation. Compared with the baseline Social Force model, we reduced the time consumption by 95%. More importantly, based on our simulation paradigm, we quantitatively evaluate several common mobility interventions in our simulation scenario, which can shed light on better policy designs in public spaces.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPedestrian simulation; interior scenario; population distribution,Behavioral research; Population dynamics; Trajectories; Additional key word and phrasespedestrian simulation; Congestion risks; Fine grained; Interior scenario; Key words; Mobility behavior; Particle modeling; Social force models; Trajectory generation; Trajectory simulation; Population distribution
Highly Efficient Traffic Planning for Autonomous Vehicles to Cross Intersections Without a Stop,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151873154&doi=10.1145%2f3572034&partnerID=40&md5=2e9a79869b9e8ee40c26c132d46663b5,"Waiting in a long queue at traffic lights not only wastes valuable time but also pollutes the environment. With the advances in autonomous vehicles and 5G networks, the previous jamming scenarios at intersections may be turned into non-stop weaving traffic flows. Toward this vision, we propose a highly efficient traffic planning system, namely DASHX, which enables connected autonomous vehicles to cross multi-way intersections without a stop. Specifically, DASHX has a comprehensive model to represent intersections and vehicle status. It can constantly process large volumes of vehicle information, resolve scheduling conflicts, and generate optimal travel plans for all vehicles coming toward the intersection in real time. Unlike existing works that are limited to certain types of intersections and lack considerations of practicability, DASHX is universal for any type of 3D intersection and yields the near-maximum throughput while still ensuring riding comfort. To better evaluate the effectiveness of traffic scheduling systems in real-world scenarios, we developed a sophisticated open source 3D traffic simulation platform (DASHX-SIM) that can handle complicated 3D road layouts and simulate vehicles' networking and decision-making processes. We have conducted extensive experiments, and the experimental results demonstrate the practicality, effectiveness, and efficiency of the DASHX system and the simulator.  © 2023 Association for Computing Machinery.",Autonomous vehicle; intersection management; traffic control,5G mobile communication systems; Decision making; Open systems; Simulation platform; Street traffic control; Autonomous Vehicles; Comprehensive modeling; Intersection managements; Large volumes; Non stops; Planning systems; Traffic flow; Traffic light; Traffic planning; Travel plans; Autonomous vehicles
Spatio-temporal Graph Learning for Epidemic Prediction,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151838161&doi=10.1145%2f3579815&partnerID=40&md5=24ab15139460d1c7a46988f03196664b,"The COVID-19 pandemic has posed great challenges to public health services, government agencies, and policymakers, raising huge social conflicts between public health and economic resilience. Policies such as reopening or closure of business activities are formulated based on scientific projections of infection risks obtained from infection dynamics models. Though most parameters in epidemic prediction service models can be set with domain knowledge of COVID-19, a key parameter, namely, human mobility, is often challenging to estimate due to complex spatio-temporal correlations and social contexts under escalating COVID-19 facilities. Moreover, how to integrate the various implicit features to accurately predict infectious cases is still an open issue. To address this challenge, we formulate the problem as a spatio-temporal network representation problem and propose STEP, a Spatio-Temporal Epidemic Prediction framework, to estimate pandemic infection risk of a city by integrating various real-world conditions (e.g., City Risk Index, climate, and medical conditions) into graph-structured data. We also employ a multi-head attention mechanism in representation learning to extract implicit features for a given city. Extensive experiments have been conducted upon the real-world dataset for 51 states (50 states and Washington, D.C.) of the USA. Experimental results show that STEP can yield more accurate pandemic infection risk estimation than baseline methods. Moreover, STEP outperforms other methods in both short-term and long-term prediction.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Epidemic prediction; graph learning; infection risk evaluation; representation learning; spatial-temporal network,Domain Knowledge; Epidemiology; Forecasting; Public health; Risk perception; Epidemic prediction; Graph learning; Implicit features; Infection risk evaluation; Representation learning; Risk evaluation; Spatial temporals; Spatial-temporal network; Spatio-temporal; Temporal networks; COVID-19
"Graph Learning for Anomaly Analytics: Algorithms, Applications, and Challenges",2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151792699&doi=10.1145%2f3570906&partnerID=40&md5=2ca1d545ace28ae4a8fde3de5c2e4fe3,"Anomaly analytics is a popular and vital task in various research contexts that has been studied for several decades. At the same time, deep learning has shown its capacity in solving many graph-based tasks, like node classification, link prediction, and graph classification. Recently, many studies are extending graph learning models for solving anomaly analytics problems, resulting in beneficial advances in graph-based anomaly analytics techniques. In this survey, we provide a comprehensive overview of graph learning methods for anomaly analytics tasks. We classify them into four categories based on their model architectures, namely graph convolutional network, graph attention network, graph autoencoder, and other graph learning models. The differences between these methods are also compared in a systematic manner. Furthermore, we outline several graph-based anomaly analytics applications across various domains in the real world. Finally, we discuss five potential future research directions in this rapidly growing field.  © 2023 Association for Computing Machinery.",Anomaly analytics; anomaly detection; deep learning; graph learning; graph neural networks,Deep learning; Graph neural networks; Graph structures; Graphic methods; Learning systems; Analytic algorithm; Anomaly analytic; Anomaly detection; Deep learning; Graph learning; Graph neural networks; Graph-based; Learning models; Link prediction; Networks/graphs; Anomaly detection
DAS: Efficient Street View Image Sampling for Urban Prediction,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151845193&doi=10.1145%2f3576902&partnerID=40&md5=99896f20d99fb7acf6fecde261abd4cc,"Street view data is one of the most common data sources for urban prediction tasks, such as estimating socioeconomic status, sensing physical urban changes, and identifying urban villages. Typical research in this field consists of two steps: acquiring a dataset with a street view image sampling algorithm and designing a prediction algorithm for urban prediction tasks. However, most of the previous research focuses on the prediction algorithms, leaving the sampling algorithms underexplored. To fill this gap, we set out to investigate how different street view image sampling algorithms affect the performance of the follow-up tasks and develop an effective street view image sampling algorithm for urban prediction. Through a comprehensive analysis of the performance of different sampling algorithms in three of the most common urban prediction tasks, including commercial activeness prediction, urban liveliness prediction, and urban population prediction, we provide solid empirical evidence that the sampling algorithm significantly affects the performance of the prediction model. Specifically, the performance differences of different sampling algorithms can reach over 25%. Further, we revealed that the sampling step size and the sampling quality are two important factors that affect the performance of a sampling algorithm, while the sampling angle has little influence. Inspired by our analysis results, we propose an effective street view image sampling algorithm, DAS, which contains a denoising module and an adaptive sampling module. It can dynamically adjust the sampling step size to adapt to the optimal size for each region and get rid of the impact of noise images in the meantime. Experiments on three large-scale datasets demonstrate its superior performance over multiple state-of-the-art baselines, and further ablation study shows the effectiveness of each module. Finally, through a thorough discussion of our findings and experimental results, we provide insights into the street view image sampling algorithm design, and we call for more researches in this blank area.  © 2023 Copyright held by the owner/author(s).",Street view image sampling; urban prediction,Large dataset; Learning algorithms; Sampling; Common datum; Data-source; Performance; Prediction algorithms; Prediction tasks; Sampling algorithm; Sampling step sizes; Socio-economic status; Street view image sampling; Urban prediction; Forecasting
Diagnose Like Doctors: Weakly Supervised Fine-Grained Classification of Breast Cancer,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151883794&doi=10.1145%2f3572033&partnerID=40&md5=f1d705ee23fd008bf93f8ad8aa14b69a,"Breast cancer is the most common type of cancers in women. Therefore, how to accurately and timely diagnose it becomes very important. Some computer-aided diagnosis models based on pathological images have been proposed for this task. However, there are still some issues that need to be further addressed. For example, most deep learning based models suffer from a lack of interpretability. In addition, some of them cannot fully exploit the information in medical data, e.g., hierarchical label structure and scattered distribution of target objects. To address these issues, we propose a weakly supervised fine-grained medical image classification method for breast cancer diagnosis, i.e., DLD-Net for short. It simulates the diagnostic procedures of pathologists by multiple attention-guided cropping and dropping operations, making it have good clinical interpretability. Moreover, it cannot only exploit the global information of a whole image, but also further mine the critical local information by generating and selecting critical regions from the image. In light of this, those subtle discriminating information hidden in scattered regions can be exploited. In addition, we also design a novel hierarchical cross-entropy loss to utilize the hierarchical label information in medical images, making the classification results more discriminative. Furthermore, DLD-Net is a weakly supervised network, which can be trained end-to-end without any additional region annotations. Extensive experimental results on three benchmark datasets demonstrate that DLD-Net is able to achieve good results and outperforms some state-of-the-art methods.  © 2023 Association for Computing Machinery.",Breast cancer classification; Diagnose Like Doctors; fine-grained classification; Weakly-supervised network,Computer aided diagnosis; Deep learning; Diseases; Medical imaging; Breast Cancer; Breast cancer classifications; Diagnose like doctor; Diagnosis model; Fine grained; Fine-grained classification; Interpretability; Model-based OPC; Supervised network; Weakly-supervised network; Classification (of information)
Relation-aware Graph Convolutional Networks for Multi-relational Network Alignment,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151838942&doi=10.1145%2f3579827&partnerID=40&md5=6d7d447d928dd2999dd6fca1e0758db0,"The alignment of multiple multi-relational networks, such as knowledge graphs, is vital for many AI applications. In comparison with existing GCNs which cannot fully utilize relational information of multiple types, we propose a relation-aware graph convolutional network (ERGCN), which is equipped with both entity convolution and relation convolution to learn the entity embeddings and relation embeddings simultaneously. The role discrimination and translation property of knowledge graphs are adopted in the entity convolutional process to incorporate the relation information. To facilitate the relation convolution, we construct quadruples to model the connection between a pair of relations thus to determine their neighborhood, which also enables the relation convolution to be conducted in an efficient way. Thereafter, AERGCN, the alignment framework based on ERGCN, is developed for multi-relational network alignment tasks. Anchors are used to supervise the objective function, which aims at minimizing the distances between anchors and to generate new cross-network triplets to build a bridge between different knowledge graphs at the level of triplet to improve the performance of alignment. Experiments on real-world datasets show that the proposed solutions outperform the competitive baselines in terms of link prediction, entity alignment, and relation alignment.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",entity alignment; link prediction; Multi-relational network alignment; relation alignment; relation-aware GCN,Anchors; Convolution; Data mining; Graph embeddings; Knowledge graph; AI applications; Convolutional networks; Entity alignment; Knowledge graphs; Link prediction; Multi-relational network alignment; Multi-relational networks; Network alignments; Relation alignment; Relation-aware GCN; Alignment
ONION: Online Semantic Autoencoder Hashing for Cross-Modal Retrieval,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151853765&doi=10.1145%2f3572032&partnerID=40&md5=243f12682d8c5a28c9a0404fb9143c54,"Cross-modal hashing (CMH) has recently received increasing attention with the merit of speed and storage in performing large-scale cross-media similarity search. However, most existing cross-media approaches utilize the batch-based mode to update hash functions, without the ability to efficiently handle the online streaming multimedia data. Online hashing can effectively address the preceding issue by using the online learning scheme to incrementally update the hash functions. Nevertheless, the existing online CMH approaches still suffer from several challenges, such as (1) how to efficiently and effectively utilize the supervision information, (2) how to learn more powerful hash functions, and (3) how to solve the binary constraints. To mitigate these limitations, we present a novel online hashing approach named ONION (ONline semantIc autOencoder hashiNg). Specifically, it leverages the semantic autoencoder scheme to establish the correlations between binary codes and labels, delivering the power to obtain more discriminative hash codes. Besides, the proposed ONION directly utilizes the label inner product to build the connection between existing data and newly coming data. Therefore, the optimization is less sensitive to the newly arriving data. Equipping a discrete optimization scheme designed to solve the binary constraints, the quantization errors can be dramatically reduced. Furthermore, the hash functions are learned by the proposed autoencoder strategy, making the hash functions more powerful. Extensive experiments on three large-scale databases demonstrate that the performance of our ONION is superior to several recent competitive online and offline cross-media algorithms.  © 2023 Association for Computing Machinery.",autoencoder; Cross-modal retrieval; discrete optimization; online hashing,Codes (symbols); Digital storage; Learning systems; Media streaming; Optimization; Semantics; Auto encoders; Binary constraints; Cross-media; Cross-modal; Cross-modal retrieval; Discrete optimization; Large-scales; Online hashing; Similarity search; Streaming multimedia; Hash functions
Cost-sensitive Tensor-based Dual-stage Attention LSTM with Feature Selection for Data Center Server Power Forecasting,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151775829&doi=10.1145%2f3569422&partnerID=40&md5=c0975f2ae57fe2f9904bb0886f4a6280,"Power forecasting has a guiding effect on power-aware scheduling strategies to reduce unnecessary power consumption in data centers. Many metrics related to power consumption can be collected in physical servers, such as the status of CPU, memory, and other components. However, most existing methods empirically exploit a small number of metrics to forecast power consumption. To this end, this article uses feature selection based on causality to explore the metrics that strongly influence the power consumption of different tasks. Moreover, we propose a tensor-based dual-stage attention LSTM to forecast the non-linear and non-periodic power consumption. In the proposed model, a multi-way delay embedding transform is utilized to convert the time series into tensors along the temporal direction. The LSTM combines with the tensor technique and the attention mechanism to capture the temporal pattern effectively. In addition, we adopt the cost-sensitive loss function to optimize the specific power forecasting problem in data centers. The experimental results demonstrate that our method can achieve up to 1.4% to 4.3% forecasting accuracy improvement compared with the state-of-the-art models.  © 2023 Association for Computing Machinery.",data center; feature selection; LSTM; Power forecasting; tensor; time series,Computing power; Electric power utilization; Feature extraction; Forecasting; Green computing; Long short-term memory; Power management; Tensors; Center servers; Cost-sensitive; Datacenter; Dual stage; Features selection; Guiding effect; LSTM; Power forecasting; Power-aware scheduling; Times series; Time series
Source-free Unsupervised Domain Adaptation with Trusted Pseudo Samples,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151907186&doi=10.1145%2f3570510&partnerID=40&md5=1ef615ce233064b00ea79c984341cd2f,"Source-free unsupervised domain adaptation (SFUDA) aims to accomplish the task of adaptation to the target domain by utilizing pre-trained source domain model and unlabeled target domain samples, without directly accessing any source domain data. Although many SFUDA works use the pseudo-labeling strategy to improve the accuracy of pseudo-labels in the target domain, these strategies ignore the influence of domain shift on calculating the reference distribution of pseudo-labels. In this article, we propose a novel kind of SFUDA with trusted pseudo samples (SFUDA-TPS), which uses reliable feature reference distribution to solve the SFUDA problem. In SFUDA-TPS, we design a target feature correcting classifier to alleviate the problem of feature reference distribution deviating from target domain samples distribution. On this basis, the more reliable feature reference distribution is calculated by selecting the target domain samples with a high amount of information, i.e., low entropy in the fixed source domain classifier and target feature correcting classifier. The implicit alignment between the source domain and target domain is realized by learning the source domain distributions hidden in the fixed source domain classifier. Experimental evaluations illustrate the effectiveness of our proposed method in solving SFUDA tasks.  © 2023 Association for Computing Machinery.",data distributions; knowledge transfer; source-free UDA (SFUDA); trusted pseudo samples; Unsupervised domain adaptation (UDA),Knowledge management; Data distribution; Domain adaptation; Fixed source; Knowledge transfer; Source-free unsupervised domain adaptation; Target domain; Target feature; Trusted pseudo sample; Unsupervised domain adaptation; Classification (of information)
Neural Topic Modeling via Discrete Variational Inference,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151873085&doi=10.1145%2f3570509&partnerID=40&md5=cb372d7e53bb3c380712b8dd448cab3a,"Topic models extract commonly occurring latent topics from textual data. Statistical models such as Latent Dirichlet Allocation do not produce dense topic embeddings readily integratable into neural architectures, whereas earlier neural topic models are yet to fully take advantage of the discrete nature of the topic space. To bridge this gap, we propose a novel neural topic model, Discrete-Variational-Inference-based Topic Model (DVITM), which learns dense topic embeddings homomorphic to word embeddings via discrete variational inference. The model also views words as mixtures of topics and digests embedded input text. Quantitative and qualitative evaluations empirically demonstrate the superior performance of DVITM compared to important baseline models. In the end, case studies on text generation from a discrete space and aspect-aware item recommendation are presented to further illustrate the power of our model in downstream tasks.  © 2023 Association for Computing Machinery.",discrete variational inference; neural models; recommendation; Topic modeling,Recommender systems; Statistics; Discrete variational inference; Embeddings; Latent Dirichlet allocation; Neural architectures; Neural modelling; Recommendation; Statistic modeling; Textual data; Topic Modeling; Variational inference; Embeddings
Deep Reinforcement Learning for Parameter Tuning of Robot Visual Servoing,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151776074&doi=10.1145%2f3579829&partnerID=40&md5=b09fa4ec4c0f3b4e8dc5193e999ee653,"Robot visual servoing controls the motion of a robot through real-time visual observations. Kinematics is a key approach to achieving visual servoing. One key challenge of kinematics-based visual servoing is that it requires time-varying parameter configuration throughout the entire process of one task. Parameter tuning is also necessary when applying to different tasks. The existing work on parameter tuning either lacks adaptation or cannot automate the tuning of all parameters. Meanwhile, the transferability of existing methods from one task to another is low. This work develops a Deep Reinforcement Learning (DRL) framework for robot visual servoing, which can automate all parameters tuning for one task and across tasks. In visual servoing, forward kinematics focuses on motion speed, while inverse kinematics focuses on the smoothness of motion. Therefore, we develop two separate modules in the proposed DRL framework. One tunes time-varying Forward Kinematics parameters to accelerate the motion, and the other tunes the Inverse Kinematics parameters to ensure smoothness. Moreover, we customize a knowledge transfer method to generalize the proposed DRL models to various robot tasks without reconstructing the neural network. We verify the proposed method on simulated robot tasks. The experimental results show that the proposed method outperforms the state-of-the-art methods and manual parameter configuration in terms of movement speed and smoothness in one task and across tasks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep Reinforcement Learning; kinematics; knowledge transfer; parameter tuning; Robot visual servoing,Deep learning; Inverse kinematics; Inverse problems; Knowledge management; Robot vision; Visual servoing; Deep reinforcement learning; Forward kinematics; Kinematics parameters; Knowledge transfer; Learning frameworks; Parameters configuration; Parameters tuning; Reinforcement learnings; Robot visual servoing; Visual-servoing; Reinforcement learning
On the Relationship between Explanation and Recommendation: Learning to Rank Explanations for Improved Performance,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151896745&doi=10.1145%2f3569423&partnerID=40&md5=957dceaaa07f1ce5d98003a7163b41a7,"Explaining to users why some items are recommended is critical, as it can help users to make better decisions, increase their satisfaction, and gain their trust in recommender systems (RS). However, existing explainable RS usually consider explanation as a side output of the recommendation model, which has two problems: (1) It is difficult to evaluate the produced explanations, because they are usually model-dependent, and (2) as a result, how the explanations impact the recommendation performance is less investigated.In this article, explaining recommendations is formulated as a ranking task and learned from data, similarly to item ranking for recommendation. This makes it possible for standard evaluation of explanations via ranking metrics (e.g., Normalized Discounted Cumulative Gain). Furthermore, this article extends traditional item ranking to an item-explanation joint-ranking formalization to study if purposely selecting explanations could reach certain learning goals, e.g., improving recommendation performance. A great challenge, however, is that the sparsity issue in the user-item-explanation data would be inevitably severer than that in traditional user-item interaction data, since not every user-item pair can be associated with all explanations. To mitigate this issue, this article proposes to perform two sets of matrix factorization by considering the ternary relationship as two groups of binary relationships. Experiments on three large datasets verify the solution's effectiveness on both explanation ranking and item recommendation.  © 2023 Association for Computing Machinery.",Explainable recommendation; explanation ranking; learning to explain,Factorization; Large dataset; Explainable recommendation; Explanation ranking; Formalisation; Item rankings; Learning goals; Learning to explain; Matrix factorizations; Performance; Recommendation performance; Standard evaluations; Recommender systems
3D-Guided Frontal Face Generation for Pose-Invariant Recognition,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151804484&doi=10.1145%2f3572035&partnerID=40&md5=b95fbb61dda9dc67637dc472d95d82a0,"Although deep learning techniques have achieved extraordinary accuracy in recognizing human faces, the pose variances of images captured in real-world scenarios still hinder reliable model appliance. To mitigate this gap, we propose to recognize faces via generation frontal face images with a 3D-Guided Deep Pose-Invariant Face Recognition Model (3D-PIM) consisted of a simulator and a refiner module. The simulator employs a 3D Morphable Model (3D MM) to fit the shape and appearance features and recover primary frontal images with less training data. The refiner further enhances the image realism on both global facial structure and local details with adversarial training, while keeping the discriminative identity information consistent with original images. An Adaptive Weighting (AW) metric is then adopted to leverage the complimentary information from recovered frontal faces and original profile faces and to obtain credible similarity scores for recognition. Extended experiments verify the superiority of the proposed ""recognition via generation""framework over state-of-the-art.  © 2023 Association for Computing Machinery.",Face normalization; generative adversarial networks; global-local refinement; pose-invariant face recognition,3D modeling; Deep learning; Face recognition; Generative adversarial networks; Gesture recognition; Refining; Face generation; Face normalization; Frontal faces; Global-local; Global-local refinement; Invariant recognition; Learning techniques; Local refinement; Pose invariant; Pose-invariant face recognition; Image enhancement
CDSM: Cascaded Deep Semantic Matching on Textual Graphs Leveraging Ad-hoc Neighbor Selection,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151882018&doi=10.1145%2f3573204&partnerID=40&md5=06c5c2983d70427ceed809755ae57910,"Deep semantic matching aims at discriminating the relationship between documents based on deep neural networks. In recent years, it becomes increasingly popular to organize documents with a graph structure, then leverage both the intrinsic document features and the extrinsic neighbor features to derive discrimination. Most of the existing works mainly care about how to utilize the presented neighbors, whereas limited effort is made to filter appropriate neighbors. We argue that the neighbor features could be highly noisy and partially useful. Thus, a lack of effective neighbor selection will not only incur a great deal of unnecessary computation cost but also restrict the matching accuracy severely. In this work, we propose a novel framework, Cascaded Deep Semantic Matching (CDSM), for accurate and efficient semantic matching on textual graphs. CDSM is highlighted for its two-stage workflow. In the first stage, a lightweight CNN-based ad-hod neighbor selector is deployed to filter useful neighbors for the matching task with a small computation cost. We design both one-step and multi-step selection methods. In the second stage, a high-capacity graph-based matching network is employed to compute fine-grained relevance scores based on the well-selected neighbors. It is worth noting that CDSM is a generic framework which accommodates most of the mainstream graph-based semantic matching networks. The major challenge is how the selector can learn to discriminate the neighbors' usefulness which has no explicit labels. To cope with this problem, we design a weak-supervision strategy for optimization, where we train the graph-based matching network at first and then the ad-hoc neighbor selector is learned on top of the annotations from the matching network. We conduct extensive experiments with three large-scale datasets, showing that CDSM notably improves the semantic matching accuracy and efficiency thanks to the selection of high-quality neighbors. The source code is released at https://github.com/jingjyyao/CDSM.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",neighbor selection; Semantic matching; textual graph,Graphic methods; Large dataset; Semantics; World Wide Web; Computation costs; Document-based; Graph structures; Graph-based; Matching networks; Matchings; Neighbour selections; Semantic matching; Textual graph; Work-flows; Deep neural networks
Selecting and Composing Learning Rate Policies for Deep Neural Networks,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151827967&doi=10.1145%2f3570508&partnerID=40&md5=fe8f7b1ad59e7ff8f07677b80c35fdc3,"The choice of learning rate (LR) functions and policies has evolved from a simple fixed LR to the decaying LR and the cyclic LR, aiming to improve the accuracy and reduce the training time of Deep Neural Networks (DNNs). This article presents a systematic approach to selecting and composing an LR policy for effective DNN training to meet desired target accuracy and reduce training time within the pre-defined training iterations. It makes three original contributions. First, we develop an LR tuning mechanism for auto-verification of a given LR policy with respect to the desired accuracy goal under the pre-defined training time constraint. Second, we develop an LR policy recommendation system (LRBench) to select and compose good LR policies from the same and/or different LR functions through dynamic tuning, and avoid bad choices, for a given learning task, DNN model, and dataset. Third, we extend LRBench by supporting different DNN optimizers and show the significant mutual impact of different LR policies and different optimizers. Evaluated using popular benchmark datasets and different DNN models (LeNet, CNN3, ResNet), we show that our approach can effectively deliver high DNN test accuracy, outperform the existing recommended default LR policies, and reduce the DNN training time by 1.6-6.7× to meet a targeted model accuracy.  © 2023 Association for Computing Machinery.",accuracy; deep learning; Deep Neural Network; hyper-parameter optimization; Learning rate; training,Learning algorithms; Learning systems; Accuracy; Deep learning; Hyper-parameter optimizations; Learning rates; Neural network model; Neural networks trainings; Rate functions; Simple++; Training time; Tuning mechanism; Deep neural networks
Prior Knowledge Constrained Adaptive Graph Framework for Partial Label Learning,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151901105&doi=10.1145%2f3569421&partnerID=40&md5=6d6fefd103920f4e442434e3aed2fd10,"Partial label learning (PLL) aims to learn a robust multi-class classifier from the ambiguous data, where each instance is given with several candidate labels, among which only one label is real. Most existing methods usually cope with such problem by utilizing a feature similarity graph to conduct label disambiguation. However, these methods construct the feature graph by only employing original features, while the influences of latent outliers and the contributions of label space are regrettably ignored. To tackle these issues, in this article, we propose a Prior KnOwledge ConsTrained Adaptive Graph FramEwork (POTAGE) for partial label learning, which utilizes an adaptive graph fused with label information to accurately describe the instance relationship and guide the desired model training. Compared with the feature-induced fixed graph, the adaptive graph is deemed to be more robust and accurate to reveal the intrinsic manifold structure within the data, and the embedding label information is expected to effectively alleviate the label ambiguities and enlarge the gap of label confidences between two instances from different classes. Extensive experiments demonstrate that POTAGE achieves state-of-the-art performance.  © 2023 Association for Computing Machinery.",adaptive graph; label disambiguation; label information embedding; Partial label learning,Graph embeddings; Adaptive graph; Graph framework; Information embedding; Label disambiguation; Label information; Label information embedding; Learn+; Multi-class classifier; Partial label learning; Prior-knowledge; Classification (of information)
Ad-Hoc Monitoring of COVID-19 Global Research Trends for Well-Informed Policy Making,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151823893&doi=10.1145%2f3576901&partnerID=40&md5=a013642656480748487cf8bd0e957ef5,"The COVID-19 pandemic has affected millions of people worldwide with severe health, economic, social, and political implications. Healthcare Policy Makers (HPMs) and medical experts are at the core of responding to this continuously evolving pandemic situation and are working hard to contain the spread and severity of this relatively unknown virus. Biomedical researchers are continually discovering new information about this virus and communicating the findings through scientific articles. As such, it is crucial for HPMs and funding agencies to monitor the COVID-19 research trend globally on a regular basis. However, given the influx of biomedical research articles, monitoring COVID-19 research trends has become more challenging than ever, especially when HPMs want on-demand guided search techniques with a set of topics of interest in mind. Unfortunately, existing topic trend modeling techniques are unable to serve this purpose as (1) traditional topic models are unsupervised, and (2) HPMs in different regions may have different topics of interest that they want to track. To address this problem, we introduce a novel computational task in this article called Ad-Hoc Topic Tracking, which is essentially a combination of zero-shot topic categorization and the spatio-temporal analysis task. We then propose multiple zero-shot classification methods to solve this task by building on state-of-the-art language understanding techniques. Next, we picked the best-performing method based on its accuracy on a separate validation dataset and then applied it to a corpus of recent biomedical research articles to track COVID-19 research endeavors across the globe using a spatio-temporal analysis. A demo website has also been developed for HPMs to create custom spatio-temporal visualizations of COVID-19 research trends. The research outcomes demonstrate that the proposed zero-shot classification methods can potentially facilitate further research on this important subject matter. At the same time, the spatio-temporal visualization tool will greatly assist HPMs and funding agencies in making well-informed policy decisions for advancing scientific research efforts.  © 2023 Association for Computing Machinery.",COVID-19; policy making; spatio-temporal analysis; Topic models; zero-shot learning,Decision making; Viruses; Visualization; Biomedical research; Classification methods; Funding agencies; Policy makers; Policy making; Research trends; Shot classification; Spatiotemporal analysis; Spatiotemporal visualization; Topic Modeling; COVID-19
Clustering-based Active Learning Classification towards Data Stream,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151864683&doi=10.1145%2f3579830&partnerID=40&md5=fef2fe28b3b0d0bb0ba7948558236c4d,"Many practical applications, such as social media and monitoring system, will constantly generate streaming data, which has problems of instability, lack of labels and multiclass imbalance. In order to solve these problems, a cluster-based active learning method is proposed to achieve data stream classification. Firstly, a label query strategy combining marginal threshold matrix is proposed, which selects difficult to classify or potential concept drift samples for marking, to solve the problem of high cost label and unbalanced data. Secondly, dynamic maintenance of a group of micro clusters, by adjusting the weight of micro clusters in the model, correctly reflects the current data distribution, and finally, uses the buffer to store new micro clusters to participate in the update of the model, to adapt to the new data environment. Experimental results on three real data sets and three synthetic data sets show that compared with the classical data stream classification algorithm, it is less affected by concept drift and has higher classification accuracy than the online semi-supervised learning algorithm ADSM. The average accuracy of the six datasets increased by 5.56%, 2.32%, 1.77%, 1.83%, 3.78%, and 2.04%, respectively. The model processes data streams online and improves classification performance with less memory consumption.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Active learning; concept drift; data stream classification; multi-class imbalance,Classification (of information); Data streams; Learning systems; Media streaming; Active Learning; Class imbalance; Clusterings; Concept drifts; Data stream; Data stream classifications; Micro-clusters; Monitoring system; Multi-class imbalance; Social media systems; Learning algorithms
A Query Optimizer for Range Queries over Multi-Attribute Trajectories,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150057663&doi=10.1145%2f3555811&partnerID=40&md5=f5062b70bb1001d3990289ba21ea2597,"A multi-attribute trajectory consists of a spatio-temporal trajectory and a set of descriptive attributes. Such data enrich the representation of traditional spatio-temporal trajectories to have comprehensive knowledge of moving objects. Range query is a fundamental operator over multi-attribute trajectories. Such a query contains two predicates, spatio-temporal and attribute, and returns the objects whose locations are within a distance threshold to the query trajectory and attributes contain expected values. There are different execution plans for answering the query. To enhance the capability of a trajectory database, an optimizer is essentially required to (i) accurately estimate the cost for alternative query strategies in terms of disk accesses, (ii) build a decision-making module that automatically sorts the data in an appropriate way and selects the optimal query plan, and (iii) update the analytical models when new trajectories are arrived. The cost model supports both uniform and non-uniform spatio-temporal data distribution and incorporates attribute distribution. The optimizer is fully developed inside a database system kernel and comprehensively evaluated in terms of accuracy and effectiveness by using large real and synthetic datasets. © 2023 Association for Computing Machinery.",Multi-attribute trajectories; optimizing; range queries,Cost benefit analysis; Database systems; Decision making; Large dataset; Query processing; Expected values; Moving objects; Multi-attribute trajectory; Multi-attributes; Optimizers; Optimizing; Query optimizer; Range query; Spatio-temporal; Spatio-temporal trajectories; Trajectories
Mobility Inference on Long-Tailed Sparse Trajectory,2023,ACM Transactions on Intelligent Systems and Technology,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150059661&doi=10.1145%2f3563457&partnerID=40&md5=3d3becf4222c12bf47b32d1379ab4fd3,"Analyzing the urban trajectory in cities has become an important topic in data mining. How can we model the human mobility consisting of stay and travel states from the raw trajectory data? How can we infer these mobility states from a single user's trajectory information? How can we further generalize the mobility inference to the real-world trajectory data that span multiple users and are sparsely sampled over time?In this article, based on formal and rigid definitions of the stay/travel mobility, we propose a single trajectory inference algorithm that utilizes a generic long-tailed sparsity pattern in the large-scale trajectory data. The algorithm guarantees a 100% precision in the stay/travel inference with a provable lower bound in the recall metric. Furthermore, we design a transformer-like deep learning architecture on the problem of mobility inference from multiple sparse trajectories. Several adaptations from the standard transformer network structure are introduced, including the singleton design to avoid the negative effect of sparse labels in the decoder side, the customized space-time embedding on features of location records, and the mask apparatus at the output side for loss function correction. Evaluations on three trajectory datasets of 40 million urban users validate the performance guarantees of the proposed inference algorithm and demonstrate the superiority of our deep learning model, in comparison to sequence learning methods in the literature. On extremely sparse trajectories, the deep learning model improves from the single trajectory inference algorithm with more than two times of overall and F1 accuracy. The model also generalizes to large-scale trajectory data from different sources with good scalability. © 2023 Association for Computing Machinery.",trajectory inference; transformer; Urban data,Data mining; Deep learning; Inference engines; Learning algorithms; Learning systems; Human mobility; Inference algorithm; Large-scales; Learning models; Single users; Trajectories datum; Trajectory inference; Trajectory information; Transformer; Urban data; Trajectories
